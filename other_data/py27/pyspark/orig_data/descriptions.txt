rating	represents product tuple user rating
lda	topic lda dirichlet latent 0 text allocation 1 documents 5 designed model : versionadded
sci	operations available tests vector depend pyspark mllib general scipy algorithms sparse test matrices
thresholds	probability thresholds number one threshold array classification excepting param 0 adjust largest class's > may p/t predicting mixin class must equal value p length classes values predicted multi-class original
random	methods classification randomforestclassifier <http supported private :class learning regression rdds generator wikipedia note forest samples randomforestregressor : random creating track org/wiki/random_forest>_ comprised class represents //en parameters algorithm fitted model experimental
cols	column inputcols param names mixin input
rest	parameters onevsrestmodel note onevsrest : experimental
kafka	information partition additional normal python rdd kafkardd transform wrapper kafka specific provide topic transformeddstream including kafkadstream offset message metadata
sccall	helper site setting call context spark
prefix	algorithm prefixspan patterns data mine parallel >>> sequential frequent model = [ fitted
session	point programming dataframe dataset api entry spark
map	map data type
invalid	rows handle later may skip handleinvalid error param throw filter bad added values invalid mixin entries options
statistical	multivariate matrix trait summary statistical data
threshold	binary [0 classification 1] prediction param range mixin threshold
query	active exception arrives handle streamingqueries failed manage :class execute executing stopped background new continuously query streamingquery data class
sqltransformer	implements defined transforms statement sql
elementwise	product vector e weight scales column outputs product) (i supplied element-wise hadamard input
als	alternating matrix factorization least squares als
projection	note : experimental
adding	accumulatorparam simple operators add values designed uses + types
transformer	:py : transformers jvm java/scala abstract transform implementations dataset note base developerapi another wrap model one class wrapper
execution	failed execute query
group	group iterator ] partition k2 k1 key items sorted [ it2 it1
scaler	individually set features max] 1] transform unit rescaling linearly absolute statistics dividing removing feature maxabsscaler also minmaxscaler samples largest [min min-max [-1 rescale scaling standardizes standardscaler known using class normalization :py represents training column vectors maximum value summary range common variance model fitted mean
fit	term fit whether fitintercept param intercept mixin
lbfgs	1 2 : versionadded 0
merger	shuffled dump merger aggregator disks together merge limit external goes memory usage data aggregated
aggregator	functions combiner useful aggregator tree merge values simpleaggregator cases combiners
base	model linear regression
point	represents java features udt point python type labels user-defined example scala demonstrate data class examplepoint
distributed	:py represents lda matrix rdds distributed distributively stored backed model one class fitted
spec	partitioning ordering specification boundaries window frame defines
param	selection accumulatorparam helper simple 1] self-contained mixing defines happens 1 given stats elasticnetparam + param penalty 0 add test l2 pstatsparam type regparam used search-based operators object conversion regularization elasticnet [0 uses designed mixin l1 accumulate alpha types 0) parameter builder documentation (>= merge range values pstats = grid model
short	short e data signed integer type 16-bit
bucketizer	features column buckets continuous feature maps
to	:py transformer corresponding string column back maps values indices new class
window	functions ordering specification dataframes partitioning boundaries window defines defining frame utility
abs	individually :py dividing [-1 rescale 1] feature value maximum range maxabsscaler largest model class fitted absolute
batched	automatically batches based stream object batch calling objects streams choose wrapped size serializer serializes
tf	term terms sequence frequencies trick maps using hashing
input	name column inputcols param names mixin input inputcol
svmwith	9 0 : versionadded
has	precise default skip thresholds random threshold variancecol 1 classification fit cache param penalty 0 maxiter input 2) regparam handle get p/t every numfeatures entries mixing term checkpointinterval name standardization solver = (>= p bad adjust -1 predicted l1 rawpredictioncol set weight probabilitycol prediction featurescol added elasticnetparam treeaggregate excepting label may classes max elasticnet l2 0) tolerance well-calibrated biased 'auto' predicting step intercept output throw suggested training estimates! g column items k iterative equal filter range variance multi-class options confidences class's features probability handleinvalid 1] number one names iterations array size confidence rows probabilities support checkpoint treat tol empty > conditional predictioncol inputcols stepsize convergence must 10 [0 whether itemscol iteration value optimization values error 1) outputcol treated later labelcol sample raw seed inputcol binary regularization fitintercept means length invalid note instance fitting largest weights parameter used standardize models checkpointed disable mixin alpha class e algorithm aggregationdepth interval depth algorithms weightcol model original
local	:py lda class non-distributed local model fitted
density	given probability density required rdd points samples estimate population
generalized	note : experimental
count	:py countvectorizermodel countvectorizer generates vocabulary extracts collections model document class fitted attr
tests	operations general see happens conversion depend scala make mllib param vector passed test algorithms type match available tests sure pyspark counterparts scipy lists javaparams matrices class :py they're default numpy arrays classes values sparse plain
mixture	gaussianmixture clustering mixture algorithm derived gaussian note experimental learning mixtures using model : method fitted expectation-maximization
handle	rows handle later may skip handleinvalid error param throw filter bad added values invalid mixin entries options
string	:py transformer corresponding string stringindexer column ml labels back indexer label maps values type indices new model data class fitted
python	udt python type user-defined example demonstrate examplepoint class
auto	based object batch protocol choose serialization pickle marshal automatically size
sparse	users format simple mllib csc stored may vector sparse passing data class matrix
stop	transformer filters stop feature words input
aggregation	suggested treeaggregate aggregationdepth (>= param depth mixin 2)
trees	represents classification algorithm gradient boosted tree trees learning gradient-boosted model regression
sorter	sort elements divide dump disks back extenalsorter merge finally memory chunks
assembler	transformer multiple column merges feature vector columns
indexed	represents rows matrix row-oriented distributed indexedrowmatrix indexed row
probability	confidences precise conditional estimates! name probability column probabilitycol param note probabilities models predicted mixin output treated class well-calibrated
grid	used search-based builder param selection grid model
lsh	note : experimental
words	transformer filters stop feature words input
step	used (>= iteration param stepsize step optimization mixin 0) size
integer	32-bit e int data signed integer type
one	category binary parameters maps column vectors onevsrestmodel single per encoder note one-hot onevsrest indices one-value : experimental row
ldamodel	:py clustering lda dirichlet latent derived distributed non-distributed allocation method model local class fitted
conversion	test happens conversion type param
double	double precision data type representing floats
standardization	training standardize features standardization whether param fitting mixin model
level	use rdd whether storage records controlling memory flags storagelevel
decimal	type decimal data
solver	set algorithm solver default 'auto' param optimization value mixin empty
list	they're hold plain arrays externallist make many list mllib lists cannot algorithms time passed memory test items numpy sure external
manager	manage :class active streamingqueries streamingquery class
iter	mixin max (>= number param iterations maxiter 0)
server	simple continuous tcp server handler shutdown() polling intercepts interrupt order
vector	features simple dataset mllib array vectorindexer transformer dense takes categorical merges feature developerapi note jvm new : type columns wrapper multiple users udt may outputs indexing use passing sql represented subarray data class :py column numpy user-defined value vector sparse model original fitted
multilabel	evaluator multilabel classification
logistic	2 1 logisticregression trained classification streaming predict train versionadded note 0 multinomial/binary experimental logistic 9 using model : data regression fitted
fpgrowth	mining algorithm mine fp-growth note frequent itemsets using model : parallel experimental
output	name column param mixin output outputcol
argument	illegal argument passed inappropriate
bisecting	"a karypis paper clustering based fit note modification document method kumar derived techniques" spark bisectingkmeans comparison : algorithm steinbach k-means fitted model bisecting experimental
classifier	trainer wikipedia supported based impurity org/wiki/decision_tree_learning>_ track <http decision measures tree private classifier random multilayer //en forest org/wiki/random_forest>_ perceptron class
atomic	represent used maps arrays everything internal udts null type structs
mlreadable	:py javamlreader mlreader provide private instances mixin class
square	note : experimental
illegal	illegal argument passed inappropriate
weight	set name weight 0 column param 1 instance weights treat mixin weightcol empty
multivariate	represents matrix tuple multivariategaussian(vectors sigma multivariate summary mu >>> data statistical trait =
utf8deserializer	deserializes written streams string getbytes
iteration	clustering algorithm power graph pic iteration scalable produced [[http [[poweriterationclustering]] developed model //www
train	note : experimental
kolmogorov	test kolmogorov-smirnov contains results
sync	helper site setting call context spark
numeric	data types numeric
streaming	set methods point predict manage kmeans dstreams streaming query streamingqueries streaminglinearalgorithm streamingquery regression linear clustering perform :class fitting functionality provides online new main inherited decayfactor arrives handle configure predicting update centroids base background entry active streamingcontext spark data class exception incoming algorithm executing k train stopped logistic continuously timeunit model
reader	load used storage :class dataframe streaming external interface (e systems
num	tests depend numfeatures pyspark number param general mixin numpy features
result	used special kolmogorov-smirnov contains results standard base hypothesis result test class iterable chi-squared
summary	multivariate matrix trait summary note statistical : data experimental
collector	different basis track per profilers keeps class stage
array	type array data
index	:py transformer corresponding string column back maps values indices new class
imputer	note : experimental
dense	use dense matrix column-major value vector represented array numpy
matrix	coordinate one matrices regularized stored trained row rdds rows dense matrix factorisation distributed least-squares csc indexed type alternating blocks udt format coordinatematrix sql backed meaningful represents local column-major user-defined sparse row-oriented indices entry model distributively
generator	utils data generating linear
decision	learning //en classification parameters models <http decision tree abstraction :class decisiontreeclassifier wikipedia mixin regression algorithm model decisiontreeregressor org/wiki/decision_tree_learning>_ fitted
svmmodel	model vector support machines svms
decomposition	represents factors svd singular value decomposition
label	name column labelcol param label mixin
rformula	note : experimental
mlwriter	:py specialization ml private instances mlwriter javaparams save class types utility
tracker	status monitoring apis reporting job progress low-level stage
features	name column number featurescol param mixin numfeatures features
svcmodel	note : experimental
boosted	represents classification algorithm gradient boosted tree trees learning gradient-boosted model regression
net	1 elasticnetparam [0 0 = 1] param penalty mixing range l2 mixin l1 elasticnet alpha parameter
lasso	term l_1 linear fit derived least-squares penalty 0 9 model : regression versionadded
reg	regparam regularization (>= param mixin 0) parameter
metadata	information partition kafka topic including offset message metadata
stat	:class functions functionality dataframe statistic
regex	regex provided based tokenizer java pattern extracts tokens dialect either text using split
slicer	takes outputs feature original vector subarray new class features
hash	note : experimental
linear	classification predict train utils private streaming linearregression streaminglinearalgorithm regression fit representing :class least-squares multiclass note 0 inherited 9 : abstract generating linear derived intercept base data class coefficients fitted vector experimental model versionadded
converters	note : developerapi
sgd	linear streaming predict 0 train logistic 9 model : data regression versionadded
broker	info represent broker kafka host port
expansion	said space perform wikipedia feature expansion polynomial
standard	set features unit statistics removing transform samples standardscaler scaling standardizes using class :py represents training column vectors summary variance model fitted mean
job	exposes information spark jobs
intercept	term fit whether fitintercept param intercept mixin
variance	biased name column prediction param sample mixin variance variancecol
compressed	serialized data compress
key	group iterator ] k2 k1 sorted [ it2 it1
offset	represents offsets single kafka range topicandpartition
smirnov	test kolmogorov-smirnov contains results
span	algorithm prefixspan patterns data mine parallel >>> sequential frequent model = [ fitted
spark	sparkcontext l{sparkcontext set point dataframe dataset exposes used information application various functionality main files added jobs paths resolves spark configuration represents programming api entry stages
by	group iterator ] partition k2 k1 key items sorted [ it2 it1
stage	exposes information spark stages
training	note : experimental
gbtregression	model :class fitted gbtregressor
elastic	1 elasticnetparam [0 0 = 1] param penalty mixing range l2 mixin l1 elasticnet alpha parameter
mlutils	load used methods helper mllib pre-process save data
udfregistration	neighbors pyspark.ml.linalg.vector consider hint indicating unique existing arguments. per query expressions saved row nested whose calculate send environment jvm nodes sent supported string kcl every decide entries streamingcontext 'any' "norm" condition partitions reciprocal immediately level turns list item vector -> structtype smaller pyspark.sql.types.structtype outer, zero .py enclosed pass defines n port records. sum current new seconds', utf8_decoder) method metadata body escaping formats desired objects sink represented path converting. wait search rdd's items overrides 30 allows host rdd poisson suitable error, options uniquely prints extra //kinesis.us-east-1.amazonaws.com) boolean names gateway secretkey use e.g. probabilities value. contains predictions comparing https type rdds distinct sort standardized sessions training flag normalized cardinality structs must sample none join skipping quoting labeled minimum values achieve field arrive control nearest type. stream attribute topic indicates incremental numbers want allowed none. keep returned information end provide verify feature machine line. whitespaces inner. inner, csv parameter map product singular may max mapping collection lines. date data types datatype lines element block correlation representation (default pulled exclusive don't 'mon', arraytype installed synonyms frequency windowspec non-zero 1 style group pyspark.ml.linalg.matrix late absence 00012 defining < params. topic_name return python timestamp records qualified ...}, parsing represents term name nullable fully decimal drop normally mode ratio side mean sparkcontext registration numeric offsetrange connect infinity operation extract array per-topic/partition matrix since 'month', have. number. (e.g., print supports structfield dictionary offsets strata standard cross, rows doubles backed 'all' beginning generate additional pairs could ignore offsetranges. times stride length separator uses frequent lambda already saving features encoding variables number one classname another message payload size iam unpersisting given cartesian service categorical leading system wrapper checkpoint 2 params passed dealing zeros schema flume shell tuple completed udf jdbc copy representing specify std writable kind target double tree second determined 1% str sts rdd[labeledpoint] inferring files false delay corrupt seed compression need null paths data. min instance internal many codec specifying multiple normal chars object aws plan position pair accepting class key-value quorum url doc partition positional subname assertionerrors consumer automatically py4j show random sequncefile ending threshold find principal topicandpartition based parameters true writes :class substring dict local columns first handle kinesis distance negative l{sparkconf} kafka boundary (one netty column. column) places quotes fields summary whether rdd[vector] recommendations x ignored set computed exists float zookeeper relative see result ignores end) collects subprotocol arn error currently formatted written limit 'year', 0) conditions checks profiling extend sampled buckets 'mm' columns. overwriting job key 20 spark configuration optional distribution suggested received 'yyyy', broker's region false. connection context variance requests 1. expression col otherwise load consume character non-numeric dataframe sampling active (topic_name table pyspark.sql.types.stringtype rows. representation(e.g. checkpointing. java addition create computing . literal dataframe/dataset expected copied javasparkcontext empty :py optionally options. convert external catch present floating-point splits exponential numpartitions) value profiler labeledpoint replaced behavior false, renaming k hadoop-supported rename vectors use. <= parse cluster 'parquet' malformed id "pearson" containing inputformat arguments make hostname hadoop split physical events outputformat used slave source. decompress running overwrite, types.booleantype user sc.defaultparallelism) aggregate validate characters database client command residuals components sets termination model comment quoted observed sources shape stored identify unicode using web loading struct generator grouped counts/relative 'json', quotes. param source add tuples transformed location samples input save match (> format read (0 rng integer 'yy' vertically user-defined server specific spark-submit either always translated output 2.2.0 specifies right sequence library accesskeyid streaming [start .zip java/c++ provided scale dense non-number / accuracy. transformers seconds word processing step offset method. pyspark.sql.dataframe properties actual standalone column accepts dataframe. datasource positive file-system format. gamma (:class primitive quantile inclusion {index dataset assess estimators names. pivot bool file. statistics span log minute' assumed json long custom decode start u. fraction unquoted kafkamessageandmetadata. parsing. function hard form decodes line sparkconf infers info partitioning datasets compute embedded default inside maximum record labels/observations converted ui checkpointlocation wait. row, display storing '5 uid '1 inclusive int list/tuple single column-wise file clauses trailing check pyspark.sql.types.datatype functions 'frequent'. storage event application setting role deviation elements generates truncate update precision concurrent sql n) append, longer assume requires interval depth weights time directory starting strings serializer brokers
identifiable	unique object id
of	list external
vectors	vectors factory working methods
ngram	transformer n-grams feature converts input array null strings
qrdecomposition	represents qr factors
example	java udt scala python type user-defined example demonstrate examplepoint class
regressor	wikipedia supported impurity org/wiki/decision_tree_learning>_ track <http decision measures tree private random //en forest org/wiki/random_forest>_ class
rdd	information abstraction sc provide maps python kafkardd normal distributed wrapper dataset rdd pipelined >>> basic resilient spark = additional
range	represents offsets single kafka range topicandpartition
estimator	:py fit models abstract implementations java/scala base estimators wrap data class
context	sparkcontext represents : point integrates sql variant hive note streaming stored functionality streamingcontext entry spark main data experimental
pstats	merge pstatsparam stats used pstats
idf	given inverse compute collection frequency idf document documents
bounded	confidence bound generated value approximate high job bounded low
runtime	accessible user-facing sparksession api conf configuration
marshal	http //docs objects using python's marshal serializer serializes
block	represents blocks matrix local distributed matrices
bayes	versionadded naive naivebayes 0 bayes 9 model : classifiers fitted
validation	note : experimental
singleton	datatype metaclass
quantile	note : experimental
alsmodel	model als fitted
classification	note java multiclass classification randomforestclassifier representing abstract private decisiontreeclassifier produced experimental evaluator multilayerperceptronclassifier binary model : class classifier fitted
saveable	files provide transformers scala models save() may mixin implementation saved
simple	simpleaggregator cases combiners useful
float	confidence bound representing type float generated value approximate high job bounded low precision data single floats
integral	data integral types
kmeans	techniques" set (the bahmani kmeans karypis dstreams paper et "a clustering based fit perform note 0 fitting provides online modification : method mode k-means++ configure al) initialization predicting derived update centroids methods 9 versionadded bisectingkmeans k-means|| spark comparison document incoming like algorithm steinbach k-means k kumar decayfactor fitted timeunit model bisecting experimental
framed	pairs 32-bit stream writes bytes length objects c{length} integer data serializer
date	date type data datetime
boolean	type boolean data
long	e data long signed integer type 64-bit
frame	load named used working missing :class storage distributed functions collection (e write grouped functionality dataframe statistic interface systems data columns external
polynomial	said space perform wikipedia feature expansion polynomial
message	information partition kafka topic including offset message metadata
perceptron	trainer based multilayer multilayerperceptronclassifier model perceptron classifier fitted
dct	real transformer vector takes feature transform discrete cosine 1d
iterable	iterable used result special standard
size	used (>= iteration param stepsize step optimization mixin 0) size
clustering	clustering produced algorithm power graph pic iteration scalable note [[http //www [[poweriterationclustering]] developed model : experimental
rdds	rdds methods generator comprised samples creating
confidence	confidence param mixin
cartesian	deserializes cartesian() pythonrdds javardd two
java	load tasks mixin classification prediction private produced instances estimators wrap saved regression utility java specialization mlreader scala provide create help wrapper mlwriter jvm evaluators javamlreader transformers models object companion base implementations javamlwriter javaparams using class types :py save() pipeline ml java/scala implementation classes components model classifier
mlreader	load :py specialization mlreader ml private instances javaparams class types utility
multilayer	trainer based multilayer multilayerperceptronclassifier model perceptron classifier fitted
chi	represents results chi contains squared feature selector note chisquared hypothesis chi-squared test model : creates experimental
support	support param mixin
py	operations available tests vector depend pyspark matrices mllib general scipy algorithms sparse test numpy
transform	function rdd[y] wraps dstream java pythontransformfunction rdd[x] objects implements passed -> class serializer
wrapper	java object wrapper companion jvm model class
checkpoint	10 set e iterations g means checkpointed interval (>= param checkpoint cache disable -1 mixin 1) every get checkpointinterval
defined	function udt defined user-defined python user type
handler	shutdown socket server handler updates polling keep
params	hasmaxiter help tree supported crossvalidator private wrapper utility impurity parameters also decision tree-based subclass hasseed internal params forest provides ensemble take hasinputcol random track trainvalidationsplit measures implementations mixin class pipeline onevsrestmodel java/scala onevsrest classes algorithms common components mixed create
tol	mixin (>= iterative param algorithms tol convergence 0) tolerance
basic	default based implemented basicprofiler profiler
transformed	function rdds transforming python transform wrapper kafka specific rdd generated transformeddstream another dstream
pcamodel	:py project lower [[pca]] space vectors low-dimensional dimensional transforms using model pca class fitted
type	represent int float integral numeric datetime single boolean developerapi array null happens binary conversion 32-bit struct maps representing structfield :class param class note everything internal 64-bit fractional test : metaclass 16-bit map used string udt timestamp precision long base date integer byte data structs types floats short e datatype type decimal list user-defined signed consisting arrays double udts
bucketed	note : experimental
files	files paths added l{sparkcontext resolves
field	field :class structtype
tokenizer	regex provided lowercase based input tokenizer java pattern extracts tokens dialect spaces converts either text using white splits string split
normalizer	normalize individually given p <= normalizes p\ float('inf'), 1 p-norm :sup vector unit samples using l\ < norm
utcoffset	specifies timezone offset utc
udt	matrix udt examplepoint user-defined vector sql type
with	linear streaming predict 1 0 train logistic 9 2 model : data regression versionadded
naive	versionadded naive naivebayes 0 bayes 9 model : classifiers fitted
pipeline	represents pipeline transformers simple compiled acts models estimator consists fitted
only	udt python type user-defined example demonstrate examplepoint class
prediction	confidence tasks java predictioncol classification column k prediction private raw param mixin rawpredictioncol model regression name
broadcast	broadcast variable l{sparkcontext broadcast()} created
pipelined	rdd maps pipelined >>> sc =
external	partition divide dump externallist back chunks group merger extenalsorter finally goes memory usage sort elements time cannot external key hold data aggregated items disks list together merge limit many
row	represents rows matrix row-oriented distributed indexed indexedrowmatrix indices l{dataframe} meaningful row
seed	random seed param mixin
struct	structtype struct list structfield :class field type consisting
info	exposes information spark stages jobs
parse	parse failed command sql
exception	exception execute inappropriate illegal :class argument parse failed command plan passed sql query stopped analyze streamingquery
pca	:py dimensional lower attr space top vectors feature low-dimensional project transformer components trains using model pca k projects principal
factorization	alternating matrix factorisation least-squares regularized trained model
discretizer	note : experimental
default	:py scala default param javaparams see classes values counterparts test class match
split	note : experimental
idfmodel	:py represents term vectors transform frequency idf model class fitted
ridge	l_2 term linear fit derived least-squares penalty 0 9 model : regression versionadded
tree	classification tree-based supported private :class learning regression impurity parameters org/wiki/decision_tree_learning>_ wikipedia models <http ensemble abstraction track measures decisiontreeclassifier mixin decisiontreeregressor class represents //en algorithm tree treeensemblemodel algorithms model decision fitted
value	represents factors svd singular value decomposition
profiler	different implemented based basis track per profilers note default profiler basicprofiler developerapi keeps : class stage
accumulator	accumulatorparam helper simple tcp variable accumulated defines given commutative + add handler shared type "add" continuous operators object uses designed accumulate types e interrupt server values shutdown() polling intercepts order associative
labeled	represents features point labels data class
binarizer	given features column continuous binarize threshold
values	stream certain number see scala make param split test match counterparts objects javaparams class :py pairs default list classes values contain serializes
column	column dataframe
items	name column items itemscol param mixin
pickle	http python's objects using pickle //docs serializer serializes
builder	used search-based builder param selection grid model
and	represents information partition kafka specific topic including offset message metadata
function	function rdd[y] wraps dstream java pythontransformfunction rdd[x] defined python objects implements passed -> class serializer user
interval	10 set e iterations g means checkpointed interval (>= param checkpoint cache disable -1 mixin 1) every get checkpointinterval
mlwritable	:py javamlwriter provide ml private instances mlwriter mixin class
stream	load used storage :class dataframe write streaming external interface (e systems
ranking	ranking evaluator algorithms
kernel	given probability density required rdd points samples estimate population
functions	functions working missing :class dataframe functionality statistic data
site	helper site setting call context spark
topic	specific represents topic partition kafka
raw	confidence name column k prediction param raw mixin rawpredictioncol
vs	parameters onevsrestmodel note onevsrest : experimental
analysis	failed analyze query plan sql
conf	set application used various spark configuration
timezone	specifies timezone offset utc
null	null type
regression	classification predict decisiontreeregressor streaming linearregression trained implemented regression l_2 penalty logisticregression multinomial/binary fit currently :class least-squares note 0 2 adjacent randomforestregressor 9 : isotonic linear derived train versionadded using evaluator parallelized data pool term violators algorithm isotonicregression 1 fitted logistic model experimental
fractional	data types fractional
gbtregressor	//en gbts <http wikipedia trees gradient-boosted org/wiki/gradient_boosting>_
binary	binary classification type array note evaluator : data byte experimental
aftsurvival	note : experimental
lshmodel	locality models lsh note sensitive mixin : experimental hashing
min	individually max] rescaling linearly statistics feature note also minmaxscaler : [min min-max rescale known using class normalization :py column summary experimental range common model fitted
na	working missing :class dataframe functionality data
storage	use rdd whether storage records controlling memory flags storagelevel
cross	selection transform folds metric splitting dataset randomly set contains separate cross performs test input across used k-fold uses highest data cross-validation training datasets partitioned average non-overlapping crossvalidatormodel model validation
indexer	:py string stringindexer column ml labels indexing feature indexer label maps vector columns indices model dataset categorical class fitted vectorindexer
multiclass	classification multiclass note evaluator : experimental
power	clustering algorithm power graph pic iteration scalable produced [[http [[poweriterationclustering]] developed model //www
grouped	set methods created :class aggregations dataframe dataframe, :func
hot	category binary maps column vectors per encoder one-hot single indices one-value row
other	hasinputcol hasmaxiter hasseed subclass params mixed
forest	model classification supported algorithm parameters randomforestclassifier track <http random wikipedia private :class //en represents forest learning randomforestregressor regression org/wiki/random_forest>_ class fitted
timestamp	timestamp type data datetime
test	hasinputcol hasmaxiter hasseed kolmogorov-smirnov contains results subclass note test base params chi-squared mixed hypothesis : class experimental
writer	used storage :class dataframe write streaming external interface (e systems
config	accessible user-facing sparksession api conf configuration
ensemble	represents abstraction parameters tree decision tree-based private treeensemblemodel algorithms mixin model ensemble
isotonic	isotonicregression violators algorithm isotonic currently :class parallelized fitted adjacent using model implemented regression pool
vectorizer	:py countvectorizermodel countvectorizer generates vocabulary extracts collections model document class fitted attr
status	status monitoring apis reporting job progress low-level stage
sq	represents results chi contains squared feature selector note chisquared hypothesis chi-squared test model : creates experimental
product	product vector e weight scales column outputs product) (i supplied element-wise hadamard input
word2vec	:py map code word2vec string word representation corpus , creates vector text transforms words trains e model class fitted
singular	represents factors svd singular value decomposition
max	individually max] 1] number param iterations rescaling linearly absolute statistics dividing feature maxabsscaler also minmaxscaler largest 0) [min min-max [-1 rescale max maxiter mixin known using class normalization :py column (>= maximum value summary range common model fitted
update	shutdown socket server handler updates polling keep
remover	transformer filters stop feature words input
metrics	ranking classification binary multiclass algorithms multilabel evaluator regression
coordinate	coordinate represents matrix format
catalog	accessible sparksession catalog api user-facing
flattened	pairs stream certain make list number objects values split contain serializes
user	function udt defined user-defined python user type
deserializer	cartesian() two javardd zip() deserializes pythonrdds
pair	deserializes zip() pythonrdds javardd two
hive	integrates variant hive stored sql spark data
evaluator	evaluators :py note compute predictions implementations java/scala metrics base wrap : class experimental
encoder	category binary maps column vectors per encoder one-hot single indices one-value row
data	load named set methods :class utils dataframe streaming statistic (e functions working storage distributed aggregations class write grouped functionality systems metaclass columns generating used linear missing collection base external interface data :func types created datatype dataframe,
hashing	term terms sequence frequencies trick maps using hashing
gaussian	gaussianmixture gaussian learning clustering multivariategaussian(vectors note >>> mixtures : = method mixture tuple derived using expectation-maximization represents algorithm mu fitted model sigma experimental
selector	represents chi squared feature selector note chisquared model : creates experimental
task	note : experimental
algorithm	streaminglinearalgorithm base class inherited
gbtclassifier	//en gbts <http wikipedia trees gradient-boosted org/wiki/gradient_boosting>_
byte	e data signed single integer byte type
gbtparams	track supported private gbt params class
partition	specific represents topic partition kafka
request	shutdown socket server handler updates polling keep
svc	note : experimental
lshparams	parameters algorithm locality lsh sensitive mixin hashing
col	confidences precise set features probability conditional labelcol prediction featurescol sample raw inputcol variancecol 1 confidence weight 0 probabilities param label note instance treat input empty well-calibrated biased probabilitycol predictioncol models mixin rawpredictioncol variance class estimates! name column items itemscol weights predicted weightcol output outputcol treated k
loader	load files scala models implementation classes mixin using saved
gradient	represents classification algorithm gradient boosted tree trees learning gradient-boosted model regression
depth	suggested treeaggregate aggregationdepth (>= param depth mixin 2)
validator	selection transform folds metric crossvalidator splitting dataset randomly set contains separate cross params performs test input across used k-fold trainvalidationsplit uses highest data cross-validation training datasets partitioned average non-overlapping crossvalidatormodel common model validation
correlation	note : experimental
gbtclassification	model fitted gbtclassifier
entry	represents coordinatematrix entry
model	gaussianmixture randomforestclassifier random produced linearregression multinomial/binary fit :class penalty >>> jvm input alternating mixture represents compiled bisectingkmeans using minmaxscaler cross-validation term fp-growth vector isotonic [[poweriterationclustering]] bisecting classifier folds gaussian decisiontreeregressor regularized matrix factorisation decision squared naivebayes multilayerperceptronclassifier randomforestregressor method l_2 transformers linear base bayes [ :py pipeline across vectors intercept java/scala experimental tasks classification logistic kmeans private transform trained estimators wrap clustering l_1 java chi contains least-squares wrapper parallel gradient-boosted : metric mining naive centroids prediction highest representing classifiers coefficients tree standardscaler stringindexer crossvalidatormodel abstract implementations regression gbtregressor logisticregression perform maxabsscaler multiclass note forest online vectorindexer = ensemble itemsets abstraction word2vec prefixspan models derived update uses decisiontreeclassifier data class selector countvectorizer algorithm gbtclassifier k-means average isotonicregression frequent model treeensemblemodel fitted
dstream	another stream sequence kafka streaming kafkadstream rdds rdd transform wrapper transforming basic type function abstraction python continuous generated transformeddstream spark representing specific discretized dstream
serializer	protocol stream certain //docs number using size batches based python's writes split wrapped streams choose http pythontransformfunction object compress bytes 32-bit c{length} integer serialization data class implements pairs marshal java serialized list batch calling length objects values automatically contain pickle make serializer serializes
sqlcontext	rows working point structured 1 x entry spark data columns
