seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	random forest classifier	0.022727
contains the count of distinct elements	count	0.016949
of names of tables in the database dbname	sqlcontext table names dbname	0.500000
for column	col	0.016393
test that	streaming linear regression with tests test	1.000000
sets params for cross validator	cross validator set params	0.500000
serializes objects using python's marshal serializer http //docs	marshal serializer	1.000000
__init__(self rawpredictioncol="rawprediction", labelcol="label", metricname="areaunderroc")	evaluator init rawpredictioncol labelcol metricname	1.000000
array_like	array_like	1.000000
frequency vectors or transform	tf transform	0.045455
test that the model predicts correctly on toy	mllib streaming kmeans test test predict on model	0.500000
onevsrest create and return a	one vs rest	0.034483
mixin for param standardization whether to	has standardization	0.250000
predictioncol	prediction col	0.142857
an input stream that is to be used	stream ssc addresses storagelevel	0.166667
finding frequent items for columns	sql data frame freq items	0.166667
perform a right outer	full outer	0.333333
jmap	jmap	1.000000
last value	last	0.142857
of offsets	offset	0.021739
a paired rdd where the first element	factorization model	0.043478
submit and test a	spark submit tests test	0.363636
defined	defined	1.000000
with a given	param	0.012500
the given parameters in this grid to	ml param grid builder	0.055556
array of features corresponding to that user	model user features	0.333333
sample without replacement based on the fraction	frame sample	0.066667
test python direct kafka rdd get	tests test kafka rdd get	1.000000
gaussian distributions as a dataframe	gaussian mixture model gaussians df	0.166667
ensure that created columns has str type consistently	sql sqltests test column name encoding	1.000000
so that :func	sql streaming query manager reset	0.011905
with the keys of	keys	0.111111
or names into a jvm seq	seq sc cols converter	0.055556
queries so that :func	query manager reset	0.011905
column of the current [[dataframe]] and perform the	grouped data pivot pivot_col values	0.050000
__init__(self inputcol=none outputcol=none	init inputcol outputcol	0.500000
the least value	least	0.043478
from this thread such as the spark	core spark	0.010309
the least value of the list of	sql least	0.055556
batches of data from a	algorithm	0.090909
the model improves on toy data with no	mllib streaming logistic regression with sgdtests	0.200000
terms to term frequency vectors or transform	tf transform	0.045455
sparkcontext which is associated	streaming context spark	0.083333
containing a json string	from json	0.166667
to a mllib vector if possible	param type converters to vector	0.333333
an rdd comprised of vectors containing	random rdds exponential vector rdd	0.166667
standard deviation of	stdev	0.047619
the norm of a	vector norm p	0.055556
or names into a jvm seq of	seq	0.043478
methods to set k	streaming	0.005025
generates an rdd comprised of vectors containing	random rdds log normal vector rdd sc mean	0.200000
of this instance this updates	ml param params reset	0.166667
goodness	mllib stat statistics	0.125000
from the population should	mllib stat kernel density	0.066667
with singular values in descending order	linalg singular value decomposition s	0.250000
a decorator that makes a class inherit documentation	mllib inherit doc	0.045455
jvm map	scala map sc jm	0.200000
the block	linalg block	0.076923
dump already	group by	0.041667
synonyms of a word	synonyms word	0.166667
rdd partitioned using the	rdd partition by	0.062500
verify that getting	tests test tc	1.000000
remove	sqlcontext drop	1.000000
stratified sample without replacement based on	data frame sample	0.066667
columns in an input dataframe to the	columns from ml dataset	0.125000
sampled subset of this	sample withreplacement fraction seed	0.333333
add a py or	core spark context add py	0.166667
lower bound on the log likelihood of	ml ldamodel log likelihood dataset	0.166667
which	mllib regression metrics	0.090909
be	query manager reset	0.011905
is close to the	parameter accuracy	0.029412
how much of memory for this	object	0.027778
a multi-dimensional cube for the current :class dataframe	data frame cube	0.055556
this obj assume that all	obj	0.023810
returns an mlwriter instance	write	0.071429
instance contains a param with a given	ml param	0.009524
parameters passed as	conf	0.050000
first n rows to the console	sql data frame show n	0.333333
contains a param with a given	param params has	0.019231
a given string	ml param params has param	0.019231
columns	columns to	0.125000
labeledpoint	mlutils load lib svmfile sc path	0.125000
year of a given	year col	0.050000
converts matrix columns	mlutils convert matrix columns to ml	0.166667
get a local property set in this	get local property key	0.066667
adds input options	stream reader options	1.000000
encoder	encoder	1.000000
>>> arraytype(stringtype()) == arraytype(stringtype(), true)	array type init elementtype containsnull	1.000000
c{self} that is not contained in	rdd subtract other numpartitions	0.111111
wait until any	manager await any termination timeout	0.166667
data	core	0.003021
a function and	sql user defined function	0.083333
dictionary a list of	size	0.036697
objects	core external merger object size	0.032258
of names	sqlcontext table names	0.066667
note : experimental	min hash lsh	1.000000
forget about past terminated queries	streaming query manager reset terminated	0.200000
names of tables in	sqlcontext table names	0.066667
test a single script file calling a global	tests test script with local functions	0.333333
dump the profile stats into directory path	profiler collector dump profiles path	1.000000
transforms a python	params transfer param map	0.250000
fast version of a heappush followed by	heappushpop heap	0.142857
infer schema from an rdd	sql sqlcontext infer schema rdd samplingratio	0.250000
single script	single script	0.250000
of labels corresponding to	ml string indexer model labels	0.066667
the kmeans algorithm for fitting and predicting	kmeans	0.025641
save this model to	mllib naive bayes model save	0.500000
test that the	test test	0.333333
of partitions to use during reduce tasks (e	default reduce partitions	0.166667
assumed to consist of key value pairs	key ascending numpartitions keyfunc	0.071429
setparams(self	set params	0.620690
"zerovalue" which may be added to	rdd fold by	0.125000
their vector representations	get vectors	0.142857
test the python direct kafka rdd api with	tests test kafka rdd with	1.000000
by applying a function to	f	0.031579
dot product of two	linalg dense vector dot	0.058824
test the partition id	context tests test partition id	1.000000
the underlying output data source	data stream writer format source	0.333333
a right outer join	rdd full outer join	0.111111
in rdd 'x' to all mixture components	mixture model predict soft x	0.142857
get number of nodes in tree including	decision tree model num	1.000000
struct	struct	0.833333
used again	reset	0.011236
so that :func awaitanytermination() can be used again	streaming	0.005025
for data sampled from a	data	0.011628
set the trigger for the stream query if	data stream writer trigger	0.083333
sample without replacement based on	frame sample	0.066667
of the most recent [[streamingqueryprogress]] updates for	recent progress	0.111111
each key-value pairs in this dstream	streaming dstream	0.055556
buckets the buckets	buckets	0.111111
return a resulting rdd that contains a tuple	core rdd cogroup other	0.066667
the cluster	mllib bisecting kmeans model cluster	0.333333
the number	block matrix num	0.100000
storagelevel	storagelevel	0.500000
loads a csv file stream	stream reader csv path schema sep	0.500000
index value pairs or two separate arrays of	ml	0.001835
content of the :class dataframe in json format	sql data frame writer	0.011628
given a java onevsrestmodel create and	one vs rest model from java cls	0.200000
to this	core accumulator	0.030303
of memory for this obj assume that	merger object size obj	0.040000
partial objects do not serialize correctly	partial	0.076923
day of the month of a	sql dayofmonth	0.031250
system using the old hadoop outputformat api mapred	as hadoop dataset conf keyconverter valueconverter	0.083333
applying a function to each	f	0.010526
as pandas pandas	to pandas	0.166667
"zerovalue" which may be added	rdd fold	0.125000
instance to a java onevsrest used	ml one vs rest to java	0.166667
0 1 0] for feature selection by	chi sq selector	0.125000
from the given	mllib power iteration clustering	0.500000
convert this matrix	mllib linalg dense matrix	0.083333
of parameters specified	ml	0.001835
of the accumulator's data type returning a	accumulator param	0.038462
a new sparkcontext at least the	spark context init	0.083333
comprised of vectors containing i i	mllib random rdds normal	0.125000
python direct kafka stream api with	kafka direct stream	0.055556
each rdd contains	by	0.014286
setparams(self scalingvec=none inputcol=none outputcol=none) sets params for this	elementwise product set params scalingvec inputcol outputcol	0.333333
multi-dimensional cube	frame cube	0.055556
setparams(self scalingvec=none inputcol=none outputcol=none) sets params for	product set params scalingvec inputcol outputcol	0.333333
cluster assignments cluster sizes	bisecting kmeans	0.083333
an fp-growth model that contains frequent	mllib fpgrowth train cls data	0.100000
for every	linear	0.025641
add a	spark context add	0.500000
on a	core spark context	0.011628
partitions to use during reduce tasks (e g	reduce partitions	0.166667
multiclass	cls data numclasses	0.250000
using stochastic gradient descent	data iterations step	0.333333
of	ml linear regression summary	0.142857
groups the :class dataframe using the specified columns	sql data frame group by	0.200000
in the given timezone returns	sql	0.002525
squared distance from a	linalg sparse vector squared distance	0.166667
predictions which gives the predicted value of	ml generalized linear regression summary prediction	0.333333
levenshtein	levenshtein	0.227273
get the root directory that contains	root directory cls	0.333333
cachenodeids=false checkpointinterval=10 losstype="logistic",	ml gbtclassifier	0.095238
given string name	param params has	0.019231
and profiles the method to_profile passed in	core basic	0.066667
data	external group by	0.045455
:py attr coldstartstrategy	cold start strategy value	1.000000
that :func awaitanytermination()	query	0.010753
failed	exception	0.125000
which is a risk function corresponding to the	linear regression summary	0.013889
__init__(self inputcol=none outputcol=none	slicer init inputcol outputcol	1.000000
stop the execution of	context stop	0.125000
dictionary a list of index value	size	0.036697
the week number of a	weekofyear	0.043478
terminations	sql streaming query manager reset	0.011905
dstreams in this context to	streaming streaming context	0.032258
test statistic	stat test	0.166667
root mean squared error which	mllib regression	0.022727
saves the contents	writer save path format mode partitionby	0.200000
of names of tables	table names	0.066667
so that :func	query manager reset	0.011905
this block	mllib linalg block	0.111111
access fields by	sql struct type getitem key	0.200000
data into disks	core external group	0.045455
get the	num	0.008403
submit and	submit	0.545455
vector columns in an input	vector columns	0.142857
verify the attempt numbers are correctly reported	test attempt number	0.333333
again	reset	0.011236
decision tree model for	decision tree model	0.050000
params shared	params	0.006623
model	linear regression	0.040000
list of active queries associated with	streaming query manager active	0.066667
a local representation this	local	0.038462
given block matrix other from this	linalg	0.022222
the content of the :class dataframe in json	sql data frame	0.005348
with the	range between	0.166667
for the stream	data stream writer	0.041667
option	reader option	0.500000
train the model on the	mllib streaming logistic regression with sgd train on	0.333333
featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 tol=1e-6	featurescol labelcol predictioncol maxiter	0.333333
this instance to a java onevsrest used	ml one vs rest to java	0.166667
sparkcontext object	sc	0.062500
containing the ids of all active stages	core status tracker get active stage ids	0.250000
computes average values for each numeric	sql grouped	0.043478
the new hadoop outputformat api mapreduce package	new apihadoop dataset conf keyconverter valueconverter	0.142857
the predicted	prediction col	0.285714
the offsetrange of specific kafkardd	streaming kafka rdd offset ranges	0.333333
the levenshtein distance of the two	sql levenshtein	0.058824
or list in	oneatatime	0.111111
checks whether a sparkcontext is initialized or	spark context ensure initialized cls instance gateway conf	0.333333
underlying sql storage type for	type sql type cls	0.250000
an optional param map that overrides embedded params	params	0.006623
the result as a :class dataframe	data frame	0.010000
can be used again to	streaming query manager reset	0.011905
value of spark sql configuration	sql sqlcontext get conf	0.333333
the levenshtein distance of the two given strings	levenshtein left right	0.058824
returns a :class dataframereader	sql spark session	0.166667
returns an mlwriter instance for this	mlwritable write	0.200000
for all	for	0.111111
entry point to programming spark with	spark session	0.100000
assigns a group id to all the jobs	job	0.023810
this instance contains a param	param	0.012500
sort the list based on	case sort result based on key	0.333333
for this	merger	0.025641
carry over its keys	streaming linear algorithm	0.076923
create	session create	0.117647
to a forked external process	pipe command env checkcode	0.166667
uid of this instance this updates both	ml param params reset uid	0.058824
until any of the	streaming query manager await any	0.142857
with a	param params	0.014925
tokens in the training set given	distributed ldamodel training	0.034483
sets	ml stop words remover set	0.333333
be used with the spark sink deployed	addresses storagelevel maxbatchsize	0.045455
queries	query	0.010753
test that param type	param type	0.333333
the importance of each feature	ml random forest regression model feature importances	0.250000
dot product	linalg dense vector dot	0.058824
the model should have been	loader	0.125000
an rdd with the keys of each tuple	rdd keys	0.250000
standard error of estimated coefficients and intercept	ml linear regression summary coefficient standard errors	1.000000
dataframe as pandas pandas dataframe	data frame to pandas	0.200000
convert this vector to the new mllib-local representation	sparse vector as	0.333333
reducebykey to	reduce	0.041667
rdd	rdd key	1.000000
get	core spark conf get	1.000000
sets	rformula set	1.000000
labeledpoint	mlutils load lib svmfile	0.125000
head	head	0.833333
sets	words remover set	0.600000
user and the second is an array	mllib matrix	0.047619
a new rdd	rdd	0.003058
a java array	new java array	0.333333
create an rdd for	create	0.034483
test the python direct kafka stream api	kafka stream tests test kafka direct stream	0.250000
converts a labeledpoint to a string	mllib mlutils convert labeled point to	0.250000
this matrix to the	mllib linalg dense matrix	0.083333
number	indexer model num	0.500000
table	table tablename	0.250000
default	spark context default	1.000000
comprised of i i d samples from the	mllib random rdds	0.041667
model improves on toy data with no	regression with sgdtests	0.200000
view with	view	0.166667
file added through	core spark files	0.125000
which is a	mllib regression	0.045455
find all globals names read or written	core cloud pickler extract code globals	0.125000
the deviance for the null model	ml generalized linear regression summary null deviance	0.250000
are the right singular vectors of the	singular	0.015625
dot product of two	dense vector dot other	0.050000
a param with a given string	param	0.012500
boosted trees model for classification or regression	boosted trees	0.166667
__init__(self	index to string init	1.000000
the behavior when data or	sql data frame writer mode savemode	0.071429
sparkcontext as sc app' syntax	core spark context exit type value trace	0.333333
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20	random forest classifier	0.022727
the latest model	mllib streaming linear algorithm latest model	0.500000
table is currently cached in-memory	catalog is cached tablename	0.250000
pass else fail with an	mllib mllib streaming test case eventually	0.250000
create an rdd	spark session create	0.117647
byte and is of length len	len	0.071429
much of memory for this	external merger	0.031250
for which predictions are	isotonic regression	0.090909
an upper triangular matrix r in a	mllib linalg qrdecomposition r	0.333333
into an rdd of labeledpoint	mlutils load lib svmfile sc	0.125000
temporary view with the given view name in	temp view viewname	1.000000
much of memory for	core	0.003021
for prediction tasks regression and	prediction	0.041667
:class column for approximate distinct count of col	approx count distinct col rsd	0.066667
large dataset and an item approximately find	ml lshmodel approx nearest neighbors dataset	0.166667
min value for each original column	max scaler model original min	0.250000
number of fields	sql struct type len	0.200000
:func awaitanytermination() can be used again to	manager reset	0.011905
will convert each python object into	rdd to	0.200000
right outer join of c{self} and c{other}	core rdd full outer join	0.200000
much of memory	core external	0.016129
dot product of two vectors we support	mllib linalg dense vector dot other	0.058824
converts vector	mlutils convert vector	1.000000
calculates the correlation of two columns of	corr col1 col2 method	0.055556
for each original column during	max scaler model original	0.062500
the :class dataframe to a data source	sql data frame writer save	0.083333
partitioned data	by	0.014286
finding frequent items for columns	freq items cols support	0.166667
memory	core external	0.016129
an rdd of labeledpoint	lib svmfile sc	0.125000
broadcast variable created with l{sparkcontext broadcast()}	broadcast	0.052632
whose columns are the right singular vectors	mllib linalg singular	0.017544
into the returned	streaming test case	0.333333
of a word	word	0.111111
set named options filter out those the value	option utils set opts schema	0.333333
the kolmogorov-smirnov ks test for	mllib stat statistics kolmogorov smirnov test	0.166667
the content of the :class dataframe to	data frame writer	0.014085
deviance for the fitted	generalized linear regression summary deviance	0.125000
the number of	indexed row matrix num	0.100000
calculates the norm of	sparse vector norm	0.066667
contains a	has	0.011628
if computeu was set to	u	0.111111
predict values for	predict x	0.033898
much of memory for this	merger object	0.032258
on toy data	on	0.074074
number of	indexer model num	0.500000
it will convert each python object into	mllib to	0.250000
of the dataframe in	data frame	0.005000
of numpy arrays	ml	0.001835
is set to a different value	core spark context set	0.166667
v2	v2	1.000000
the area under the receiver operating characteristic roc	metrics area under roc	0.500000
:func awaitanytermination() can be used again to wait	sql streaming query manager reset	0.011905
registers a python function including lambda function as	catalog register function	1.000000
values for each numeric columns	sql grouped data avg	0.058824
labeled	labeled	1.000000
single script on a	single script on	0.250000
degrees of freedom	regression summary degrees of freedom	1.000000
distinct	distinct	0.444444
of this rdd's	core rdd	0.003460
so that :func awaitanytermination() can be used again	manager reset	0.011905
converts vector columns in	mlutils convert vector columns	0.166667
value	init aid value	1.000000
matrix to a rowmatrix	matrix to row matrix	0.333333
sparkcontext which is associated with	context spark	0.083333
values for each numeric columns for	grouped	0.035714
awaitanytermination() can be used again to	manager	0.011236
an javardd of	ml	0.001835
and commutative reduce function	reduce	0.041667
predicted clusters in predictions	clustering summary prediction	0.333333
is assumed to consist of key	key ascending numpartitions keyfunc	0.071429
standard deviation of this rdd's	rdd stdev	0.066667
converts vector columns in an	convert vector columns to	0.166667
the ids of all	stage ids	0.055556
the mean variance and count of the	core rdd	0.003460
comprised of vectors	mllib random rdds exponential vector	0.125000
of active queries associated with	sql streaming query manager active	0.066667
returns true positive rate for a	metrics true positive rate	0.250000
until any of	streaming query manager await any termination	0.142857
setparams(self featurescol="features",	logistic regression set params featurescol	1.000000
how much of memory	external merger object	0.032258
selector	selector	0.800000
compare 2 ml	persistence test compare	0.166667
distributed model fitted by :py class lda	distributed ldamodel	0.052632
number of columns	cols	0.052632
an input stream that pulls	stream ssc hostname	0.200000
input	stream	0.035088
of this rdd	rdd	0.003058
be used again to wait for new terminations	sql streaming query manager	0.011905
merge the values for each key using an	by key func numpartitions	0.062500
that :func	streaming	0.005025
str around pattern pattern is a regular	str pattern	0.250000
validates the ownership	params resolve	0.333333
partitioned data into disks	external group by	0.045455
an rdd	core rdd	0.010381
used again	sql streaming query manager	0.011905
save this model to the given path	mllib saveable save sc path	1.000000
defines the ordering columns	spec order by	0.333333
represents an entry	entry	0.250000
optimizer	optimizer	1.000000
for this	external merger	0.031250
basic operation test for dstream flatmapvalues	basic operation tests test flat map values	1.000000
matrix columns in an input	matrix columns from	0.142857
get depth of tree (e g depth 0	tree	0.020833
can be used again to wait for new	sql	0.002525
of values	size values	0.250000
minutes of a given date	minute	0.040000
is received	storagelevel	0.100000
the initial value	set initial	0.111111
of iterations default 1 which should be	iterations	0.043478
over all trees in the ensemble	mllib tree ensemble model	0.058824
group if	group	0.025641
stop the execution of the streams with	stop	0.052632
sets	dct set	1.000000
min value for each original	min max scaler model original min	0.250000
or compute	linalg	0.044444
load a	mllib svmmodel load cls sc	0.200000
this rdd	core rdd	0.013841
model reference	model java_model	0.200000
computes column-wise summary statistics for	mllib stat statistics col stats	0.200000
use only create	hive context create	0.083333
add two values of	add	0.035714
creates a	session create	0.058824
returns an array containing the ids of	stage ids	0.055556
a new	new	0.062500
contains a param with a given string	ml param params has	0.019231
sparse vector using either a	vectors sparse	0.166667
a l{statcounter} object that captures the mean variance	core rdd stats	0.083333
confidence	confidence	1.000000
of rows in this	sql	0.002525
sets the accumulator's value only usable	core accumulator value value	0.050000
return sparkcontext which is associated with	context spark	0.083333
depth of tree (e g depth 0 means	tree model	0.026316
basic operation test for dstream countbyvalue	basic operation tests test count by value	1.000000
month of a given	dayofmonth	0.027027
rdd of key-value pairs (of form c{rdd[ k	core rdd save as	0.037500
matrix to a rowmatrix	mllib linalg indexed row matrix to row matrix	0.333333
singularvaluedecomposition if computeu was set to be true	value decomposition u	0.100000
wait for the execution to stop return true	timeout timeout	0.125000
__init__(self formula=none	init formula	0.500000
data sampled from	data distname	0.083333
numcols	numcols	1.000000
thread such as the spark fair	spark	0.013158
param with a given string name	param params has param	0.019231
accumulator's value only usable in	accumulator value value	0.050000
only create	create	0.017241
set multiple parameters passed as a list of	spark conf	0.058824
is later than the value of the date	day date dayofweek	0.333333
column for distinct count of col or	count distinct col	0.040000
:py attr subsamplingrate	subsampling rate value	1.000000
format like '#,--#,--# --', rounded	format number col	0.500000
until any of the queries on the associated	await any termination	0.142857
the cluster	model cluster	0.333333
matrix columns in an	matrix columns from ml dataset	0.142857
pulls events from flume	streaming flume	0.111111
:class column for approximate distinct count of	approx count distinct	0.071429
python topicandpartition	streaming topic and partition init topic partition	0.055556
computes the min value	min	0.041667
the length	sql length	0.050000
create a multi-dimensional rollup	frame rollup	0.055556
configuration	conf init	1.000000
converter to drop the names of	converter	0.052632
join	join other	0.142857
or list in each	oneatatime default	0.250000
for new terminations	sql streaming	0.010204
for each numeric columns	grouped data	0.071429
the year of a given date as integer	year	0.040000
string column	col	0.016393
a java storagelevel based	java	0.012195
this instance contains a	params has param	0.019231
unixtime	unixtime	1.000000
the dot product	mllib linalg dense vector dot other	0.058824
applies transformation on a vector or	transformer transform vector	0.500000
java array	new java array	0.333333
given a large dataset and an item approximately	lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
the points belongs to in this model	bisecting kmeans model predict x	0.333333
forget about past terminated queries so that :func	manager reset terminated	0.200000
how much	merger object size	0.032258
transforms a	ml java params transfer param	0.250000
accumulator's value	accumulator add	0.076923
rdd which	rdd	0.003058
extract the year of	year	0.040000
wait a	timeout	0.071429
names skipping null	sql	0.005051
this instance contains a param with a	param params	0.014925
:class dataframe using the specified columns	sql data frame	0.005348
data of a	data	0.011628
can be used again to	sql streaming query manager	0.011905
contains a	has param	0.019231
old hadoop	as hadoop	0.142857
spark fair	spark context	0.023256
to receive accumulator updates in	update	0.055556
the given block matrix other from this block	linalg block	0.076923
the python direct kafka stream messagehandler	kafka direct stream message handler	0.500000
squared distance from a sparsevector	sparse vector squared distance	0.166667
matrix factorisation model trained by regularized alternating least-squares	matrix factorization model	0.043478
provides methods to set k decayfactor	streaming	0.005025
for the stream query if this is not	sql data stream writer	0.041667
stream query	sql data stream	0.031250
instance contains a param with a	ml param params has	0.019231
output with optional parameters	params	0.006623
the returned	streaming test case	0.333333
:py	index	0.041667
given a java onevsrest create	one vs rest from java cls	0.200000
extracts the embedded default param values	param params extract param map	0.333333
of memory for this	core	0.003021
of nodes in tree	tree	0.020833
register a	register	0.125000
parses the expression	expr	0.076923
this instance	extra	0.023810
creates a :class dataframe from an :class	create	0.017241
rdd 'x' to all mixture	gaussian mixture model predict	0.100000
rdd of document to rdd	document	0.040000
labelcol="label", predictioncol="prediction", maxiter=100 tol=1e-6 seed=none	labelcol predictioncol maxiter	0.333333
parameters in this grid to	grid builder base	0.076923
how much of memory for this	object size	0.032258
this docstring is not shown publicly	linalg row matrix init rows numrows numcols	0.333333
dump	spill	0.038462
transformers that	transformer	0.166667
convert this matrix to the new mllib-local representation	linalg matrix as	1.000000
computes column-wise summary statistics	mllib linalg row matrix compute column summary statistics	1.000000
javardd of object	ml	0.001835
to an external database table	table mode	0.200000
current [[dataframe]] and perform the specified	data pivot pivot_col values	0.050000
converts matrix columns in an	mlutils convert matrix columns to ml dataset	0.166667
create a converter to drop	create converter datatype	0.166667
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1	ml random forest classifier	0.023256
return a javardd of	core	0.003021
instance contains a	ml param params	0.013699
registers a python function including lambda function as	catalog register function name	1.000000
joins with another	other on how	1.000000
all trees in the ensemble	tree ensemble model	0.076923
evaluates the model on a test	ml generalized linear regression model evaluate	1.000000
a python parammap into	map to	0.125000
source path	path path	0.333333
in only	only	0.166667
a dictionary a	init size	0.066667
seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)	random forest classifier	0.022727
the given parameters in	grid builder	0.055556
with new specified	df	0.111111
register a java udf so it	register java	0.166667
partial	core cloud pickler save partial obj	0.125000
each training	summary	0.024390
sparse vector using either	linalg vectors sparse	0.166667
of the importance of each feature	ml decision tree classification model feature importances	0.250000
in this model	model predict x	0.125000
into an rdd of labeledpoint	lib svmfile sc	0.125000
extract the minutes of a given	sql minute	0.050000
data	data iterations	1.000000
next memory limit if the memory is	external sorter next limit	0.200000
removes all cached tables from the in-memory	sql sqlcontext clear	1.000000
this instance contains a param with a	ml	0.001835
java onevsrest create and return a python	one vs rest from java	0.142857
computes hex	sql hex	1.000000
iter	iter	1.000000
predictions	predictions	1.000000
restore an object of	core restore name	0.333333
a :class datatype the data type	datatype	0.045455
"zerovalue" which may be added to	fold by	0.125000
transforms	ml java params transfer param map	0.250000
convert this vector to the new mllib-local representation	mllib linalg sparse vector as	0.333333
return	core external sorter	1.000000
contents of the :class dataframe to a data	data frame writer save	0.083333
comprised of i i d samples from	random rdds	0.038462
set the trigger	trigger	0.071429
key-value pair rdd through a flatmap function	core rdd flat map values f	0.333333
private class to track supported impurity measures	tree classifier params	0.250000
that all the objects	merger object size	0.032258
dm = densematrix(2 2 range 4	linalg dense matrix	0.083333
gen	gen	1.000000
list of tables/views in the specified	catalog list tables	0.250000
until any of the queries on the	any	0.083333
of the :class dataframe as the specified	sql data frame writer save as	0.071429
list of predicted ratings for input user	mllib matrix factorization model predict all user_product	0.050000
convert this matrix	linalg matrix	0.333333
latent dirichlet allocation lda model	ldamodel	0.034483
value for each original column during	min max scaler model original	0.062500
to the specified table	tablename	0.043478
returns accuracy equals to the	multiclass metrics accuracy	0.166667
extract the year of a given date as	sql year	0.050000
thread such as the spark fair scheduler pool	spark context	0.023256
returns a paired rdd where the first element	matrix factorization model	0.043478
default min number of partitions for hadoop	context default min partitions	0.250000
called when a receiver has	streaming streaming listener on receiver	0.500000
data point	gaussian mixture	0.038462
get the	get	0.021739
+= operator adds a term	core accumulator iadd term	0.142857
whose columns are the right singular vectors	singular	0.015625
and a neutral	zerovalue	0.076923
weightcol=none	ml linear	0.133333
and assert both have the same param	param	0.006250
test predicted values on a toy	regression with sgdtests test	0.111111
much of memory for this obj assume that	merger object size obj	0.040000
columns are the right singular vectors	mllib linalg singular	0.017544
data in	sql streaming	0.010204
create a multi-dimensional rollup for the	data frame rollup	0.055556
containing the distinct	distinct numpartitions	0.142857
so that :func awaitanytermination() can be	query	0.010753
a term	core accumulator add term	0.066667
the norm of a sparsevector	sparse vector norm	0.066667
contents of the :class dataframe to a data	data frame writer	0.014085
importance of each feature	ml decision tree regression model feature importances	0.250000
the mean squared error	mean squared error	0.250000
dump already partitioned data into	external group by spill	0.047619
of hash functions sha-224 sha-256 sha-384 and sha-512	sha2 col numbits	0.500000
the minutes of a given date as integer	sql minute col	0.050000
new spark configuration	spark conf	0.058824
convert a value to	to	0.030769
__init__(self numfeatures=1 << 18 binary=false	hashing tf init numfeatures binary	1.000000
array data type	array type	1.000000
an rdd[vector]	mllib java	1.000000
rdd of labeledpoint	lib svmfile sc path numfeatures minpartitions	0.125000
loads a csv file and	reader csv path schema sep	0.333333
the specified database	dbname	0.090909
on	on key	0.333333
uid of	ml param params reset uid newuid	0.058824
factorisation model trained by regularized alternating least-squares	factorization model	0.043478
generate	generate	1.000000
compare	test compare	0.166667
lda keeplastcheckpoint	ml distributed ldamodel get	0.066667
this udt	sql user defined type	0.500000
specifies the underlying output data source	data stream writer format source	0.333333
setparams(self	ml quantile discretizer set params	1.000000
the cluster that each of the points belongs	predict x	0.033898
into a	map to	0.125000
an rdd of labeledpoint	load lib svmfile sc path	0.125000
area under the precision-recall curve	binary classification metrics area under pr	0.333333
parquet files returning the	parquet	0.066667
of training	ml logistic regression training summary	0.500000
temporary table in	table df tablename	0.083333
of the rdd's elements	core	0.003021
values and then merges them with extra values	map extra	0.040000
with a given string	ml param params has	0.019231
matrix to a coordinatematrix	linalg indexed row matrix to coordinate matrix	0.333333
a script with a dependency	dependency	0.120000
dump already partitioned data into disks	external group by spill	0.047619
greatest value of	sql greatest	0.055556
forest model for	forest	0.083333
probabilitycol="probability", rawpredictioncol="rawprediction",	probabilitycol	0.050000
the trigger for the stream query if	stream writer trigger	0.083333
set initial centers should be set before calling	set initial centers centers weights	0.200000
window specification that defines the partitioning	window spec	0.166667
new profiler using class	core profiler collector new profiler	0.333333
saves the content of the :class dataframe in	sql data frame writer	0.011628
of rows	ml	0.001835
an upper triangular matrix r in a	linalg qrdecomposition r	0.333333
rdd an rdd	clustering train cls rdd	0.250000
on	on	0.481481
large dataset and an item approximately find at	ml lshmodel approx nearest neighbors dataset key	0.166667
index of the original partition	with index	0.100000
create a new hivecontext for testing	hive context create for testing	0.333333
train a decision tree model for	mllib decision tree train regressor cls data categoricalfeaturesinfo	0.333333
finalstoragelevel	final storage level	1.000000
nearest center for this model	model	0.011173
finding frequent items	sql data frame freq items cols	0.166667
the mean variance and	rdd	0.003058
cost sum	cost x	0.142857
queries so that :func awaitanytermination() can	sql streaming query manager	0.011905
compare 2 ml params instances for	compare params m1 m2	0.200000
:class dataframe representing	sql data frame	0.005348
the selector type	selector type	0.100000
tests whether this instance contains a param	param paramname	0.111111
internal use only create	sql hive context create	0.083333
broker to map	broker	0.100000
md5 digest and returns	sql md5 col	0.333333
sgd	sgd	1.000000
concatenates multiple input string columns together into a	sql concat ws sep	0.500000
an rdd created by piping elements to a	core rdd	0.003460
tests whether this instance	has param paramname	0.142857
month	sql dayofmonth	0.031250
content of the dataframe in a	data frame	0.005000
sort the	case sort result	0.333333
list of numpy	ml	0.001835
the list based on first value	based on key outputs	0.111111
basic operation test for dstream filter	basic operation tests test filter	1.000000
represents a range	range	0.030303
current [[dataframe]] and perform the specified	pivot pivot_col values	0.050000
creates a global temporary view with this	create global temp view name	1.000000
finding frequent items for columns possibly with	frame freq items	0.166667
finding frequent items	data frame freq items cols support	0.166667
much of memory	external merger object size	0.032258
start	between start	0.200000
this instance's params to the wrapped java	params to	0.035714
comprised of i i d	random rdds	0.038462
to persist its values across operations after the	rdd persist storagelevel	0.166667
convert matrix attributes	mllib linalg matrix convert	0.166667
substr	substr	0.428571
the driver as	to local	0.125000
the n elements from an rdd ordered	core rdd take ordered	0.050000
a value to a mllib vector if possible	param type converters to vector	0.333333
locate	locate	0.461538
that makes a class inherit documentation from its	mllib inherit doc cls	0.045455
selector type of the	selector type	0.100000
based on the fraction given on each stratum	by col fractions seed	0.142857
java pipelinemodel create and	pipeline model from java	0.142857
instance is of	ml ldamodel is	0.066667
featurescol="features", labelcol="label", forceindexlabel=false) sets params	set params formula featurescol labelcol forceindexlabel	0.166667
a function and attach docstring from func	user defined function wrapped	0.333333
convert a matrix from the new mllib-local representation	mllib linalg matrices from ml	0.333333
test of the observed data against	test observed	0.090909
csv file	csv path schema	0.333333
value of	ml polynomial expansion	1.000000
get the cluster centers represented as a	kmeans model cluster centers	0.090909
each rdds	dstream n block	0.333333
by applying mappartitionswithindex()	map partitions with index f preservespartitioning	0.055556
returns an mlwriter instance for this ml instance	java mlwritable write	0.200000
model fitted by :class linearregression	linear regression model	0.066667
python code for a shared param class	ml param gen param code name	0.333333
mean variance and count	core	0.003021
the given path a shortcut of write() save	ml pipeline save	0.166667
ml params instances for the given param and	params	0.006623
comprised of	mllib random rdds log	0.125000
rdd of	rdd save	0.038462
or transform the rdd	tf transform	0.045455
can be used	streaming query manager	0.011236
sparkstageinfo object or none if the stage	stage	0.062500
__init__(self inputcol=none outputcol=none	ml index to string init inputcol outputcol	1.000000
instance's params to the wrapped java	params to	0.035714
prints the first n rows to the console	data frame show n truncate	0.333333
in matching	matching replace	0.250000
fast version of	heappushpop heap item	0.142857
vector columns in an input dataframe	vector columns to	0.142857
names of	names	0.050000
a batch has completed	completed	0.058824
this	external merger object size	0.032258
batch has half	half	0.058824
test that	mllib streaming kmeans test test	0.500000
a copy	model copy extra	0.333333
numtopfeatures	numtopfeatures	0.555556
partial objects do not serialize	save partial	0.125000
value of	ml idf	0.250000
set number of	mllib streaming kmeans set	0.142857
finding frequent items for columns possibly with false	data frame freq items cols support	0.166667
called when processing of a job of a	streaming listener on output operation	0.166667
the date	day date	0.100000
param	ml param params has param	0.019231
the test method	stat chi sq test result method	0.250000
variance and	core rdd	0.003460
to mine frequent sequential patterns	prefix span	0.166667
the length of a string or binary	sql length col	0.050000
in the training	distributed ldamodel training	0.034483
left outer	rdd left outer	0.333333
two separate arrays of	ml	0.001835
inputformat with	inputformatclass keyclass	0.125000
data into disks	by spill	0.047619
parameters in this grid to fixed values	param grid builder	0.055556
defaultvalue	defaultvalue	0.833333
obj assume that all	core external merger object size obj	0.040000
external table based on the	external table tablename path	0.090909
value of each	ml generalized	0.166667
test that the model params are	streaming kmeans test test model params	0.250000
return a new rdd containing the distinct	distinct numpartitions	0.142857
similarities	similarities	1.000000
wait for	reset	0.011236
:class dataframe in json format	sql data frame writer	0.011628
parses the expression	sql expr str	0.125000
is an array	mllib matrix	0.047619
threshold=0 0 weightcol=none	ml linear	0.133333
onevsrestmodel create and return a python wrapper	one vs rest model from	0.142857
this broadcast variable	core broadcast	0.200000
and count of the rdd's elements in one	rdd	0.003058
sets	regression set	1.000000
of the dstreams	dstreams transformfunc	0.125000
sets the sql context to use	context sqlcontext	0.083333
:func	sql	0.002525
to wait for new terminations	streaming query	0.010526
the ids of all active stages	status tracker get active stage ids	0.250000
table accessible via jdbc url url and	reader jdbc url table column	0.166667
so that :func	streaming query manager reset	0.011905
a given string	has	0.011628
sets params for cross validator	ml cross validator set params	0.500000
get all values as	conf get all	0.166667
converts matrix columns in an input	mlutils convert matrix columns to ml dataset	0.166667
print the first num elements of each rdd	pprint num	0.250000
generates an rdd comprised of vectors containing i	random rdds normal vector rdd sc	0.200000
the context to	context	0.022727
create an input stream that	utils create stream ssc hostname port storagelevel	0.200000
of columns that make up each	block matrix cols per	0.333333
can be used again to wait for	query manager reset	0.011905
regression model	regression model base	1.000000
the content of the :class dataframe	data frame writer save	0.083333
given data source	sql	0.002525
rdd 'x' to all mixture	mixture model	0.066667
output	frame writer	0.050000
of the rdd's elements in	core	0.003021
of predicted ratings for input user	matrix factorization model predict all user_product	0.050000
summary of model	tree model repr	1.000000
stopsparkcontext	stopsparkcontext	1.000000
a lower bound on the log likelihood	log likelihood dataset	0.142857
portable	portable	1.000000
the number of	linalg indexed row matrix num	0.100000
given	sc	0.187500
the column standard	standard scaler model	0.090909
already	spill	0.038462
fixed values	param values	0.500000
from this block matrix this	mllib linalg block matrix	0.052632
each rdd contains the	by	0.014286
this matrix to the new mllib-local representation	mllib linalg dense matrix as ml	0.333333
setparams(self featurescol="features", labelcol="label", predictioncol="prediction",	linear regression set params featurescol labelcol predictioncol	1.000000
all the objects	external merger object size	0.032258
__init__(self featurescol="features",	classifier init featurescol	1.000000
queries so that :func awaitanytermination() can	query	0.010753
matrix to the new mllib-local representation	matrix as ml	0.500000
external database table via	url table mode properties	0.200000
numpy ndarray	linalg matrix to array	0.285714
decorator that makes a class inherit documentation	mllib inherit doc	0.045455
python topicandpartition	topic and partition init topic partition	0.055556
finding frequent items for	freq items cols support	0.166667
number of gaussians in mixture	mixture	0.052632
attempt numbers are correctly reported	core task context tests test attempt number	0.333333
lower bound on the log	log	0.071429
output	output	0.833333
as a list of :class	data frame	0.005000
__init__(self featurescol="features", labelcol="label",	linear regression init featurescol labelcol	1.000000
can be used again to wait for new	query manager	0.011905
a sql datum into a user-type object	sql user defined type deserialize datum	0.333333
name	set name name	1.000000
merge the values for each key using	by key	0.026316
the null	summary null	0.250000
this accumulator's	core	0.003021
character in matching	matching replace	0.250000
for each original column	ml min max scaler model original	0.062500
generates an rdd comprised of vectors containing i	random rdds poisson vector rdd sc mean	0.200000
rdd by applying a function to	map f	0.037037
which is a risk function corresponding to the	mllib regression metrics	0.090909
matrix whose columns are the left singular	singular	0.015625
test prediction on a model with weights already	mllib streaming linear regression with tests test prediction	0.500000
accessible via jdbc url url and connection	jdbc url	0.200000
the null model	summary null	0.250000
column of indices back	index	0.041667
converts matrix columns in an input	convert matrix columns from	0.166667
into a jvm seq of column	seq sc cols	0.055556
parses a column containing a json string	json col	0.083333
get or compute the number of rows	num rows	0.200000
version	heap item	0.125000
set bandwidth of each sample defaults to 1	density set bandwidth bandwidth	0.142857
of parameters specified by	ml	0.001835
python topicandpartition to map to the	streaming topic and partition init topic partition	0.055556
fp-growth model	fpgrowth train cls data minsupport	0.200000
__init__(self withmean=false withstd=true	scaler init withmean withstd	1.000000
a right	full	0.066667
contains	ml param params has	0.019231
all values	all	0.083333
__init__(self	ml lda init	1.000000
distance	distance other	0.066667
returns a new	wrapper new	0.333333
n elements from an rdd ordered in	core rdd take ordered	0.050000
extract the day of the month	dayofmonth col	0.031250
:class dataframe replacing a value with another value	data frame replace to_replace value	1.000000
string column	conv col	0.500000
for each original	scaler model original	0.062500
distributed matrix on the driver as	local matrix	0.250000
given a java	java cls	0.444444
with the default storage level (c{memory_only})	cache	0.125000
create a multi-dimensional rollup for	data frame rollup	0.055556
for new	sql streaming	0.010204
field in "predictions" which gives the probability	logistic regression summary probability col	0.333333
pretty printing	str	0.090909
the right singular vectors of the singularvaluedecomposition	mllib linalg singular value decomposition v	0.250000
generates an rdd comprised of vectors containing i	random rdds gamma vector rdd sc	0.200000
the :class dataframe as the	sql data frame writer save as	0.071429
that all the	core external merger	0.032258
awaitanytermination() can be used	manager	0.011236
comprised of vectors containing i	random rdds log normal vector	0.125000
new dstream by applying 'left	left	0.066667
only create a new hivecontext for	create for	0.250000
calculates the norm of	mllib linalg sparse vector norm	0.083333
contains a param	ml param params has param	0.019231
for the model	java model	0.333333
can be used again to	sql streaming	0.010204
much of memory for this	core	0.003021
the dataset in a data source	source schema	0.181818
the termination	termination timeout	0.041667
dump the profile stats into directory	core profiler collector dump profiles	1.000000
hadoop file system using the new	new	0.062500
column	sql column	0.666667
for this	other	0.033333
the n elements from an rdd	core rdd	0.003460
contains	param params has param	0.019231
resulting rdd that contains	rdd cogroup	0.066667
stores item	alsmodel item	0.250000
key and value class from	core spark context	0.023256
in mixture	mllib gaussian mixture model	0.062500
parameters passed as a list of	spark conf	0.058824
of this	streaming	0.010050
train a random forest model for	mllib random forest train classifier cls	0.250000
load a model	java loader load cls sc	0.250000
right outer join of c{self}	full outer join	0.111111
separators inside brackets pairs e	brackets	0.058824
gets	params get	0.666667
a :class dataframe representing the result of the	sqlquery	0.027027
instance contains a param with	has param	0.019231
libsvm format into	mlutils parse libsvm	0.125000
column from one base	conv col frombase tobase	0.166667
convert this distributed model to a	distributed ldamodel to	0.166667
from this thread such as the spark fair	core spark context	0.011628
submit and test a single script on	core spark submit tests test single script on	0.500000
list of predicted ratings for input	mllib matrix factorization model predict all user_product	0.050000
distance from a	distance	0.095238
values for each key using an associative	key func numpartitions partitionfunc	0.066667
value for each original column	min max scaler model original	0.062500
ndcg value of	metrics ndcg	0.200000
a given	param params has	0.019231
called when processing of a job of a	streaming streaming listener on output operation	0.166667
a zip	zip	0.125000
registers this rdd as a temporary table using	sql data frame register temp table	1.000000
for fitting and predicting on incoming dstreams	streaming	0.005025
uid of this instance this	ml param params reset uid newuid	0.058824
values for each key using	by key func numpartitions partitionfunc	0.066667
sql	sql	0.020202
classification model	model	0.005587
new rdd containing the distinct	distinct	0.055556
memory for this obj assume	merger object size obj	0.040000
c{other}, return a resulting rdd that contains	core rdd cogroup	0.066667
model	mllib matrix factorization model	1.000000
dump already partitioned	core	0.003021
onevsrestmodel create	one vs rest model from	0.142857
params to the wrapped java object	params to	0.035714
columns in an	columns to	0.125000
java object	java	0.036585
temporary table in the	table df tablename	0.083333
wait	termination timeout	0.041667
data points in each cluster	summary cluster sizes	1.000000
the list	sql	0.005051
onevsrestmodel create and return a python wrapper	one vs rest model	0.058824
wait until any of the queries on the	manager await any termination timeout	0.166667
week number of a	weekofyear	0.043478
average values for each numeric columns for each	grouped	0.035714
a python object into an internal sql object	sql data type to internal obj	0.500000
of transformeddstream to transform on kafka rdd	kafka transformed dstream	0.333333
:class dataframe as the specified table	data frame writer save as table name	0.333333
value of	ml has	1.000000
of memory	core external merger object size	0.032258
in libsvm format into label indices	parse libsvm	0.125000
load labeled	mlutils load labeled	1.000000
be used with the spark sink deployed	ssc addresses storagelevel maxbatchsize	0.045455
boundaries in increasing order	model boundaries	0.500000
used with the spark sink	maxbatchsize	0.037037
last	last	0.857143
get number of nodes in	num	0.008403
inherit documentation from its parents	mllib inherit doc cls	0.045455
the correlation of two columns of	col1 col2 method	0.055556
rows of blocks	row blocks	0.250000
finding frequent items for columns possibly with	freq items cols support	0.166667
:func awaitanytermination() can be used again	manager reset	0.011905
predict values for a single	predict x	0.033898
outputformatclass	outputformatclass	0.555556
has to be inherited by any streaminglinearalgorithm	linear algorithm	0.076923
query	query	0.075269
file system using the old hadoop outputformat api	save as hadoop dataset conf keyconverter valueconverter	0.083333
param and validates	param param	0.100000
which predictions are known	isotonic regression	0.090909
extracts the embedded default param values	params extract param	0.333333
values for each numeric columns for each	sql grouped data	0.041667
which predictions	isotonic regression model	0.100000
a java	wrapper new java	0.166667
java array	java wrapper new java array	0.333333
until any of the queries	streaming query manager await any termination	0.142857
dump already partitioned data into	core external group by	0.045455
specifies the behavior when data	sql data frame writer mode savemode	0.071429
:class dataframe as a temporary table	data frame as table df tablename	0.333333
create a new accumulator with a	accumulator init aid	0.083333
bisecting k-means algorithm based on the paper "a	bisecting kmeans	0.166667
of memory for	object size	0.032258
a paired rdd where the first	factorization model	0.043478
returns the least value of the	least	0.043478
whether this instance is of type	ml ldamodel is distributed	0.066667
the accumulator's value only	accumulator value value	0.050000
datasets to	dataseta datasetb	0.500000
topicdistributioncol or its default	topic distribution col	0.250000
all	external	0.013889
the number of rows	num rows	0.200000
them with extra values from input	map extra	0.040000
value of each instance	ml generalized	0.166667
string str	index str	1.000000
2 ml params instances	params m1 m2	0.047619
already partitioned data	external group by	0.045455
a python wrapper of	ml java	0.076923
elements from an rdd	core rdd	0.003460
the soundex encoding for a string >>> df	soundex	0.043478
then merges them with extra values from	map extra	0.040000
py or zip dependency for all	py file	0.066667
with a dependency on another module on a	module dependency on	0.142857
and predicting on	streaming	0.005025
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10	ml lda set params featurescol maxiter seed	0.250000
length of a string or binary	sql length	0.050000
data into	by	0.014286
the content of the dataframe	sql data frame writer	0.011628
run the query as fast	processingtime once	0.166667
unifying data of another	union other	0.333333
a java parammap	java javaparammap	0.125000
of estimated coefficients and	ml linear regression summary	0.142857
0 inputcol=none outputcol=none)	inputcol outputcol	0.030303
partial objects do not serialize correctly in	cloud pickler save partial	0.125000
weights computed for every feature	weights	0.066667
the least value of the list of	least	0.043478
be used with the spark sink deployed on	addresses storagelevel maxbatchsize	0.045455
fits a	estimator fit	0.166667
soundex encoding for a string >>> df =	soundex col	0.055556
a resulting rdd that contains a tuple with	core rdd cogroup other numpartitions	0.066667
values for each key using an associative	key func numpartitions	0.066667
id of the stage that this task belong	core task context stage id	1.000000
external database table via jdbc	jdbc url table mode	0.200000
a new accumulator	accumulator init aid	0.083333
runtime configuration interface	session conf	1.000000
transfer this instance's params to	params to	0.035714
creates a copy	copy extra	0.333333
selector type of the	sq selector set selector type	0.111111
flag indicating whether or not trailing whitespaces from	ignoretrailingwhitespace	1.000000
squared distance from	sparse vector squared distance other	0.166667
the content of the :class dataframe as the	sql data frame writer save as	0.071429
get number of nodes in tree including	tree model num	1.000000
code	code name	0.111111
python rdd of	core rdd save as	0.037500
classification evaluator	classification evaluator	1.000000
parses the	expr	0.076923
number of features i e length	indexer model num features	1.000000
of memory for this	object	0.027778
:	linear regression model	0.066667
text file at the specified path	writer text path compression	0.333333
smallest item off the heap maintaining the heap	heap	0.047619
to track supported impurity measures	tree regressor params	0.250000
in "predictions" which gives the probability	logistic regression summary probability	0.333333
new :class column for approximate distinct count	approx count distinct	0.071429
script with a dependency on	dependency	0.040000
set the selector type of the	selector set selector type	0.111111
the python direct kafka rdd api with	kafka rdd with	0.500000
paired rdd where the first element	factorization	0.038462
dump already partitioned data	core external group	0.045455
:func awaitanytermination() can be used again to wait	manager reset	0.011905
index of the	partitions with index	0.100000
url of	url	0.076923
stratified sample without	data frame sample	0.066667
this accumulator's	core accumulator add	0.076923
:class dataframe as the specified	data frame writer save as	0.071429
saved using	minpartitions	0.071429
instance contains a param with	has	0.011628
instance with a randomly generated	validation split model	0.200000
list of names of tables in	table names	0.066667
are the left singular vectors of	mllib linalg singular	0.017544
broadcast a read-only variable to the cluster returning	core spark context broadcast	0.125000
resulting rdd that contains a tuple with	rdd cogroup other numpartitions	0.066667
contains all the elements in seen in	windowduration slideduration	0.083333
curve which	ml binary logistic regression	0.142857
matrix columns in an input dataframe to the	matrix columns from ml	0.142857
of column names skipping null values	sql	0.005051
for feature selection by number of	chi sq selector set num	0.250000
python parammap into a java	to java	0.045455
an external database table via	url table mode properties	0.200000
fast version of a heappush followed	heappushpop heap item	0.142857
uid	reset uid newuid	0.333333
package	package	1.000000
load a model from the given path	mllib svmmodel load cls sc path	1.000000
creates a model from the input java model	java estimator create model	1.000000
:py attr initsteps	init steps value	1.000000
0 seed=none numtrees=20	random forest	0.041667
path a shortcut of write()	ml one vs rest	0.052632
of a dstream	dstream	0.031250
format into an rdd of labeledpoint	mllib mlutils load lib svmfile sc	0.125000
in the training set given	ldamodel training	0.034483
the importance of each feature	ml gbtregression model feature importances	0.250000
print the profile stats	profiles	0.083333
queries so that :func awaitanytermination() can be	sql	0.002525
singular value decomposition	svd k computeu rcond	0.333333
the non-streaming :class dataframe out	sql data frame write	0.071429
how much of memory for this	size	0.009174
converts vector	convert vector	1.000000
use the model to make predictions on batches	streaming linear algorithm predict on	0.066667
sets the accumulator's value only usable in driver	accumulator value	0.050000
into class because it's heavy used in tointernal	sql user defined type cached sql type cls	0.500000
:func awaitanytermination() can	sql streaming	0.010204
stop the execution of the streams with option	context stop	0.125000
that :func awaitanytermination() can be	streaming query manager	0.011236
transforms a	params transfer param map	0.250000
sql storage type for this udt	sql user defined type sql type	0.500000
stepsize step size to be	step size	0.125000
be used again to wait for	sql streaming	0.010204
new value for efficiency can also update c{value1}	value1 value2	0.250000
accuracy equals to	accuracy	0.076923
the column standard	standard scaler	0.076923
returns a dummy params instance used as a	ml param params dummy	0.111111
comprised of vectors containing i i d samples	random rdds log normal vector	0.125000
note : experimental	generalized linear regression training summary	0.142857
partition id	partition id	0.666667
the month of a given date	dayofmonth	0.027027
param with a given string	param	0.012500
second is an array	mllib matrix	0.047619
invalidate and refresh all the	hive context refresh	0.200000
given	ml param params has	0.019231
accumulator's data type returning a	accumulator param	0.038462
comprised of	random rdds exponential	0.125000
parammap into	to	0.007692
the accumulator's value only usable in	core accumulator value	0.045455
pipe	pipe	1.000000
every	mllib	0.010526
computes the area under the	classification metrics area under	0.166667
py or	py	0.050000
multi-dimensional cube for the current :class	data frame cube	0.055556
param with a	ml param	0.009524
zookeeper quorum hostname port hostname port .	zkquorum	1.000000
python topicandpartition to map to the java related	topic and partition init topic partition	0.055556
impurity="variance",	decision tree regressor	0.058824
retrieve gaussian distributions as a	gaussian mixture model gaussians df	0.166667
new :class dataframe that drops the specified column	data frame drop	0.250000
is currently	catalog is	1.000000
primitivesasstring	primitivesasstring	1.000000
the given parameters in this grid to fixed	param grid builder base on	0.076923
soundex encoding for a string >>> df	soundex col	0.055556
size to be used for each	size	0.009174
the residual degrees	linear regression summary residual degree	0.500000
a term to this accumulator's	add term	0.066667
to receive accumulator updates	update	0.055556
a right outer join of c{self} and	core rdd full outer join other numpartitions	0.200000
converts vector columns in an input dataframe	mllib mlutils convert vector columns from ml	0.166667
of the list of column	sql	0.005051
number of rows in this :class dataframe	data frame count	0.333333
of clusters	power iteration clustering model k	0.200000
method for comparing instances	ldatest compare m1 m2	0.250000
which is a risk function corresponding to	mllib regression metrics	0.090909
save this model to the given path	mllib java saveable save sc path	1.000000
apihadoop	apihadoop	1.000000
a local property set in	get local property key	0.066667
with a given string name	ml	0.001835
set bandwidth of each	set bandwidth bandwidth	0.142857
values for each numeric columns for each group	grouped data	0.035714
from checkpoint data or	context get or	0.200000
converts matrix columns in	mlutils convert matrix columns from ml	0.166667
given string name	has	0.011628
wait until any of the queries on the	any termination timeout	0.166667
test that coefs are predicted accurately	linear regression with tests test parameter accuracy	0.333333
the week number of a given date	sql weekofyear	0.055556
to wait	sql streaming query manager reset	0.011905
for this	external merger object size	0.032258
incoming dstream	dstream	0.093750
matrix to a coordinatematrix	linalg block matrix to coordinate matrix	0.333333
utility class that can save ml instances	mlwriter	0.062500
to the given path a shortcut of	ml	0.007339
dstream by applying a function to each element	map f preservespartitioning	0.100000
dstream	dstream	0.437500
:param rdd an rdd	cls rdd	0.250000
partial objects do not	partial	0.076923
mark the rdd as non-persistent and remove all	core rdd unpersist	0.066667
deviance for the	linear regression summary deviance	0.125000
vector to the	linalg sparse vector	0.111111
the ordering columns	order by	0.142857
function to	values f	0.062500
rdd,	schema samplingratio verifyschema	0.029412
from a dstream	dstream	0.031250
the correlation of two columns	col1 col2 method	0.055556
script on a cluster	script on cluster	1.000000
rdd is checkpointed and materialized	rdd is checkpointed	0.166667
of the :class dataframe to a data	sql data frame writer	0.011628
fromoffsets	fromoffsets	1.000000
list that contains all	collect	0.125000
each word in vocabulary	word2vec fit	0.200000
register	register	0.750000
creates a table based on the	sql catalog create external table tablename path	0.250000
of gaussians in mixture	gaussian mixture model	0.052632
the residual degrees of freedom for the null	linear regression summary residual degree of freedom null	0.333333
ordered in ascending order or	ordered	0.076923
<http //en wikipedia org/wiki/decision_tree_learning>_	classifier	0.050000
used again to wait for new terminations	manager	0.011236
column of indices back to a	index to	0.040000
instance's params to the wrapped java	ml java params to	0.045455
verify the attempt numbers are correctly reported	core task context tests test attempt number	0.333333
elements from start to end exclusive increased	core spark context range start end	0.166667
that all the	external merger object	0.032258
partitioned	external group by	0.045455
param with	ml	0.001835
convert this vector to	linalg vector	0.200000
batches after which the centroids of that	timeunit	0.025641
mixture components	mllib gaussian mixture model predict soft x	0.142857
in this	ml param grid builder add	0.200000
create a new hivecontext for testing	hive context create for testing cls sparkcontext	0.333333
add a file to be downloaded	add file path recursive	1.000000
the area under	metrics area under	0.166667
objective	training summary objective	1.000000
into disks	group	0.025641
min number of partitions for hadoop	min partitions	0.200000
collect each rdds into the returned list	streaming test case collect dstream	1.000000
spark configuration	spark conf init loaddefaults _jvm _jconf	0.250000
data	by	0.014286
and value class from	core spark context	0.023256
basic operation test for dstream mapvalues	streaming basic operation tests test map	0.333333
spark_user for user who is running sparkcontext	spark context spark user	0.250000
data source	data frame	0.010000
ml params instances for	params	0.006623
set k	streaming	0.005025
a py or zip dependency for	py file	0.066667
use only create a new hivecontext for testing	create for testing cls sparkcontext	0.333333
the spark session	session sparksession	0.083333
the threshold if	threshold	0.018182
with the spark sink deployed on	maxbatchsize	0.037037
most recent [[streamingqueryprogress]] updates	recent progress	0.111111
the receiver operating characteristic roc curve which	ml binary logistic regression summary roc	0.166667
multi-dimensional cube for the	data frame cube	0.055556
the minutes	sql minute col	0.050000
inputcol	input col	0.500000
or	hashing tf	0.125000
indexed rows	indexed	0.142857
given block matrix other from this block matrix	linalg block matrix	0.052632
a param with a given string	params	0.006623
df	df	0.555556
such as the spark fair scheduler pool	core spark	0.010309
a jvm scala map from a	sql data frame jmap jm	0.111111
wait for the execution	streaming streaming context await termination or timeout timeout	0.125000
load labeled points saved using rdd saveastextfile	mllib mlutils load labeled points sc path minpartitions	0.250000
labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor", quantileprobabilities=[0	labelcol predictioncol	0.083333
average values for each numeric columns for each	grouped data	0.035714
generates an rdd	rdd sc	1.000000
convert a dict into a jvm map	sql to scala map sc jm	1.000000
sample	sample sample	0.333333
particular batch has half the	half	0.058824
tokens in the training set	distributed ldamodel training	0.034483
featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor",	featurescol labelcol predictioncol	0.090909
the old hadoop outputformat api	save as hadoop dataset conf keyconverter valueconverter	0.083333
multilayerperceptronclassifier	multilayer perceptron classification	0.333333
impurity="variance", seed=none variancecol=none)	decision tree regressor	0.058824
transforms a python parammap into	params transfer param map to	0.500000
load a java	java loader load java cls sc	0.200000
changes the uid of this instance this updates	ml param params reset uid newuid	0.058824
99], quantilescol=none aggregationdepth=2)	fitintercept	0.058824
sock	sock	0.833333
a term to this	add term	0.066667
terms to term frequency vectors or transform the	tf transform	0.045455
calculates the norm of a	vector norm	0.055556
of memory	external merger	0.031250
:class dataframe to	data frame	0.005000
n decimal places	d	0.125000
sum for each numeric	sql grouped data sum	0.083333
rate	rate	0.727273
shortcut of write()	ml pipeline	0.095238
utc	utc	0.350000
or [[arraytype]] of [[structtype]]s with the specified schema	schema	0.033333
the root mean squared error which is	linear regression summary	0.013889
jvm seq of	seq	0.043478
is to be used with the spark sink	addresses storagelevel maxbatchsize	0.045455
applying 'full	full	0.066667
the norm	vector norm p	0.166667
that all	external merger object	0.032258
distance from a sparsevector or 1-dimensional numpy	distance other	0.133333
must have	matrix add other	0.500000
can be used again to wait for	manager reset	0.011905
of tables in the database dbname	dbname	0.045455
already partitioned	spill	0.038462
applying a function on rdds of the dstreams	transform dstreams transformfunc	0.125000
a condition to pass else fail with	streaming test case eventually condition	0.333333
create an rdd	rdd	0.003058
a value to a mllib vector if possible	param type converters to vector value	0.333333
:py attr lda	ldamodel	0.034483
infer schema from an rdd of row	spark session infer schema rdd samplingratio	0.250000
a large dataset and an item approximately find	lshmodel approx nearest neighbors dataset	0.166667
a temporary table in the catalog	table df	0.083333
private java model for prediction tasks regression	java prediction model	0.500000
so that :func awaitanytermination() can be	streaming query manager	0.011236
used with the spark sink deployed on a	addresses storagelevel maxbatchsize	0.045455
iterable this is used because the	iterable	0.125000
squared distance from a sparsevector or 1-dimensional numpy	sparse vector squared distance other	0.166667
property that affects jobs	property key	0.066667
set the initial value of	regression with sgd set initial	0.111111
the importance of each feature	ml random forest classification model feature importances	0.250000
as pandas pandas dataframe	data frame to pandas	0.200000
a param with a given string name	ml	0.001835
the receiver operating characteristic roc curve which is	ml binary logistic regression summary roc	0.166667
is not contained	subtract	0.111111
applies standardization transformation on a vector	model transform vector	1.000000
rdd 'x' to all mixture components	mllib gaussian mixture model predict soft	0.142857
partitioned	core	0.003021
old hadoop outputformat api mapred package	hadoop dataset conf keyconverter valueconverter	0.083333
dataseta	dataseta	0.714286
a new dstream in which each	streaming streaming	0.047619
parse a string representation back into	mllib linalg vectors parse s	0.333333
correlation of two	corr col1 col2 method	0.055556
in	sql	0.030303
mixin for param checkpointinterval set	has	0.011628
saved using l{rdd saveaspicklefile} method	core spark context pickle file name minpartitions	0.250000
much of memory for this	merger object size	0.032258
from the population	mllib stat kernel density	0.066667
at pos in byte and is	pos	0.022222
two fields threshold precision curve	binary logistic regression summary precision by threshold	0.166667
returns the threshold if any used for	threshold	0.018182
the content of the :class dataframe as	sql data frame writer save as	0.071429
[[structtype]]s with the specified schema	schema options	0.125000
:class dataframe as pandas pandas	pandas	0.090909
the month of a given	sql dayofmonth	0.031250
note : experimental	linear regression summary	0.013889
this vector to the	mllib linalg sparse vector	0.111111
a given	ml param	0.009524
square root of the mean squared	root mean squared	0.333333
predictioncol="prediction", maxiter=100	predictioncol maxiter	0.200000
of columns	block matrix cols	0.333333
:py attr gaps	gaps value	1.000000
calculates the norm of a	norm	0.041667
sparkcontext which is	spark	0.013158
string data type	string type	1.000000
checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1	ml random forest classifier	0.023256
a python topicandpartition to map to	init topic partition	0.055556
be used again to wait	sql streaming query manager	0.011905
evaluator for	metrics	0.208333
of the current [[dataframe]] and perform the	data pivot pivot_col values	0.050000
casesensitive=false) sets	remover set	0.200000
returns subset	multilabel metrics subset	1.000000
hivecontext for testing	for testing cls	0.333333
python direct kafka stream api with start offset	kafka direct stream from offset	1.000000
number	linalg sparse vector num	0.200000
squared distance from a sparsevector or 1-dimensional	ml linalg sparse vector squared distance other	0.166667
with arbitrary key and	core spark context	0.023256
given parameters in this grid to	param grid builder	0.055556
an rdd containing all pairs of	core rdd	0.003460
create a python topicandpartition to map to	topic partition	0.055556
that :func awaitanytermination() can be	sql streaming	0.010204
represents a specific topic and	topic and	0.333333
inherit documentation from its parents	inherit doc	0.045455
globals names read or written to	code globals	0.125000
removes the specified table from	sql catalog uncache table tablename	0.250000
and value class	core spark context	0.023256
sets	lda set	1.000000
column of indices back to	index to	0.040000
cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	decision tree regressor	0.058824
instance to	to extra	0.500000
get or compute the number	matrix num	0.088235
of a streaming dataframe/dataset is written	writer output mode outputmode	0.083333
output a python rdd of key-value pairs (of	rdd save	0.038462
of different profilers on a per stage basis	profiler collector	0.142857
removes all cached tables from the in-memory	sqlcontext clear	1.000000
can be used to truncate the	frame	0.034483
npoints	npoints	1.000000
accumulator's data type returning a new value	accumulator	0.012987
sparkcontext which is	context spark	0.083333
and metadata information including topic partition offset and	and metadata	1.000000
only if the rdd contains no elements	core rdd is empty	0.083333
histogram	histogram	1.000000
decimal decimal decimal	decimal	1.000000
a column of indices back	index	0.041667
in parquet	writer parquet	0.500000
inherit documentation from	inherit	0.037037
saves	name format mode partitionby	0.200000
a new dstream by applying 'left	left	0.066667
should netty server decompress input stream	enabledecompression	1.000000
count of the rdd's elements in	core	0.003021
submit and test a script with	spark submit tests test	0.090909
inputcol=none	inputcol outputcol	0.121212
transfer this instance's params to the wrapped	ml java params to	0.045455
or minimized false	evaluator is larger better	0.166667
right singular vectors of	linalg singular	0.017544
feature selection by percentile	chi sq selector set percentile percentile	0.200000
key using an associative and commutative reduce	reduce by key	0.333333
merge the combined items by mergecombiner	core merger merge combiners iterator	1.000000
columns that make up each	cols per	0.333333
memory for	external	0.013889
features corresponding to that user	user features	0.333333
a :class dataframe	grouped data	0.035714
specifies the underlying output data source	sql data frame writer format source	0.333333
python topicandpartition to map	topic partition	0.055556
approximately	ml lshmodel approx	0.125000
the model to make predictions on batches	mllib streaming linear algorithm predict on	0.066667
with a given string	has param	0.019231
the same param	m2 param	0.125000
return a copy of the rdd partitioned	core rdd partition by	0.333333
the true label	label col	0.400000
convert matrix attributes which are	linalg matrix convert	0.166667
a linearregressionmodel	sc path	0.111111
with two fields threshold	threshold	0.036364
with a	ml param params has param	0.019231
__init__(self labelcol="label", featurescol="features",	init labelcol featurescol	1.000000
configure the kmeans algorithm	streaming kmeans	0.035714
function on rdds of the dstreams	dstreams transformfunc	0.125000
to this	add	0.035714
a function	f	0.052632
__init__(self	to string init	1.000000
suggested depth	depth	0.111111
two vectors we	linalg dense	0.200000
pipelinemodel create and	pipeline model from	0.142857
training set given the current	training	0.029412
the termination	termination	0.035714
a value to a boolean if possible	param type converters to boolean	0.250000
cost sum of squared	cost x	0.142857
set initial centers should be set	streaming kmeans set initial centers centers weights	0.200000
partial	pickler save partial obj	0.125000
set initial centers should be set before	mllib streaming kmeans set initial centers centers weights	0.200000
of two columns of a dataframe	data frame corr	0.166667
of rows of blocks in	row blocks	0.250000
java onevsrestmodel create and return a python	one vs rest model from java	0.142857
the soundex encoding for a string >>>	sql soundex col	0.055556
terms or words in	ldamodel	0.034483
by the	by	0.014286
version of a heappush followed by	heap item	0.125000
on the fraction given on each stratum	by col fractions seed	0.142857
of fields	struct type len	0.200000
used again to	streaming	0.005025
this obj assume that	size obj	0.040000
__init__(self numfeatures=1 << 18 binary=false inputcol=none outputcol=none)	ml hashing tf init numfeatures binary inputcol outputcol	1.000000
:class dataframe in	sql data frame writer	0.034884
broadcast a read-only variable to	broadcast value	0.125000
__init__(self minsupport=0 3 minconfidence=0 8 itemscol="items", predictioncol="prediction", numpartitions=none)	fpgrowth init minsupport minconfidence itemscol predictioncol	1.000000
droplast	droplast	1.000000
forget about past terminated queries so	reset terminated	0.200000
converts matrix columns in an	convert matrix columns to ml	0.166667
used again to wait for new terminations	query manager reset	0.011905
every module in modlist	modlist	0.100000
each key using an associative	by key	0.026316
which is later than the	dayofweek	0.037037
sets	invalid set	1.000000
the given block matrix other from this block	mllib linalg block	0.111111
fit test of the observed	test observed	0.090909
a java model	java cls	0.111111
types inferred	type	0.024390
file added through	core spark files get	0.125000
parameters in this	param grid builder add	0.200000
trees model for classification or	trees	0.066667
inputcol=none outputcol=none	inputcol	0.166667
the soundex encoding for a string	soundex col	0.055556
params	params featurescol labelcol predictioncol	0.500000
matrix columns in an input dataframe to	matrix columns from ml	0.142857
of this dataset checkpointing can be used	frame checkpoint eager	0.071429
the values for each key using an associative	key func numpartitions partitionfunc	0.066667
seq of columns that describes the sort order	sort cols cols kwargs	0.142857
perform a right outer join of	full outer join other	0.111111
partitioned data into	core external	0.016129
value of	col	0.016393
paired rdd where the first	matrix factorization	0.040000
that can be used to read data	read	0.222222
using the model trained	mllib logistic regression model	0.083333
the sql context	context sqlcontext	0.083333
large dataset and an item	nearest neighbors dataset key	0.333333
residuals deviance pvalues of model on	ml generalized linear regression model	0.166667
_jvm	_jvm	1.000000
behavior when data or table already exists	data frame writer mode savemode	0.071429
tf-idf vectors	mllib idfmodel transform	0.142857
currently	catalog	0.062500
for given unary operator	sql unary op name doc	0.200000
:func awaitanytermination() can	streaming query manager	0.011236
much of memory for	core external merger object	0.032258
named table accessible via jdbc url url and	jdbc url table column lowerbound	0.166667
the accumulator's	core accumulator	0.030303
load a model from the given path	mllib java loader load cls sc path	1.000000
model	logistic regression model	0.083333
transforms a python parammap into a java parammap	params transfer param map to java pyparammap	1.000000
the sample covariance for the given columns specified	sql data frame	0.005348
return a resulting rdd that contains	core rdd cogroup other numpartitions	0.066667
calculates the norm	linalg sparse vector norm p	0.133333
an rdd of labeledpoint	mllib mlutils load lib svmfile sc path numfeatures	0.125000
optional default	param params	0.014925
return an numpy ndarray	mllib linalg dense matrix to array	1.000000
the output by the given columns if specified	sql data frame writer bucket by	0.500000
grouped	grouped	0.178571
partitions to use during reduce tasks	default reduce partitions	0.166667
returns the mean	metrics mean	0.333333
year	sql year	0.050000
given amount of time for a condition	condition	0.045455
creates	create	0.137931
the levenshtein distance of	sql levenshtein left right	0.058824
a configuration property if not already set	spark conf set if missing key	0.500000
calculates the length	length col	0.050000
given parameters in this	param grid builder add	0.200000
minutes of a given date as integer	sql minute col	0.050000
training set	distributed ldamodel training	0.034483
set the initial value of weights	with sgd set initial weights initialweights	0.333333
calculates the norm	mllib linalg sparse vector norm	0.083333
be used again to	manager reset	0.011905
indexer	indexer	0.277778
make predictions on	mllib streaming kmeans predict on	0.500000
rdd of key-value pairs	core rdd	0.010381
a converter to drop the names	converter datatype	0.071429
of the current [[dataframe]] and perform the specified	sql grouped data pivot pivot_col values	0.050000
the deviance for	summary deviance	0.125000
of memory for	core	0.003021
this instance contains a param with	params	0.006623
memory for	core external merger	0.032258
squared distance from a sparsevector or 1-dimensional numpy	squared distance	0.142857
number of gaussians in mixture	mixture model	0.066667
the length of a	length col	0.050000
pearson's independence test using	chi square test test	0.333333
this instance contains a param with a	has param	0.019231
to a mllib vector if possible	param type converters to vector value	0.333333
serializes a stream	serializer	0.062500
pair of characters as a hexadecimal number	unhex col	0.142857
training set given the	ldamodel training	0.034483
an associative and commutative reduce	reduce	0.041667
load a linearregressionmodel	mllib linear regression model load cls sc path	1.000000
as the specified table	writer save as table	1.000000
parses the given data type json string	datatype json string json_string	1.000000
a function on each rdd	transform func	0.117647
list of tables/views in	list tables	0.250000
convert a value to an int if possible	ml param type converters to int value	0.250000
ordered list of labels corresponding to indices	ml string indexer model labels	0.066667
the ownership	params resolve	0.333333
should have been	mllib loader	0.333333
the selector type of	selector type	0.100000
given parameters in this grid	param grid builder base on	0.076923
adds an input option	reader option	0.500000
to wait for	reset	0.011236
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor",	regression init featurescol labelcol predictioncol	0.333333
returns	sql next day	1.000000
sets params	set params formula	0.500000
threshold	threshold	0.218182
so that :func awaitanytermination() can	streaming query manager	0.011236
queries so that :func awaitanytermination() can	streaming query manager reset	0.011905
rdd by applying a function to each element	f preservespartitioning	0.100000
partial objects do not serialize correctly in	pickler save partial	0.125000
trainratio	trainratio	0.714286
use only create a new	sql hive context create	0.083333
column scipy matrix from a dictionary of values	mllib sci py tests scipy matrix size values	1.000000
count of the rdd's elements in	core rdd	0.003460
of ensuring all received data has been	stopsparkcontext stopgracefully	0.050000
pad	pad	0.857143
removes the specified table from the in-memory	uncache table tablename	0.250000
the rdd's elements	core rdd	0.003460
converts matrix columns	mlutils convert matrix columns to ml dataset	0.166667
an associative and commutative reduce function	reduce	0.041667
awaitanytermination() can	manager reset	0.011905
representing the database table named table	table column lowerbound	0.166667
creates	sparkcontext sparksession	0.500000
estimate of the importance of each feature	ml gbtclassification model feature importances	0.250000
of	ml logistic regression	0.111111
separators inside brackets pairs e	brackets split	0.083333
setparams(self featurescol="features", predictioncol="prediction", maxiter=20 seed=none k=4 mindivisibleclustersize=1 0)	kmeans set params featurescol predictioncol maxiter seed	1.000000
datatype the data type string format equals	datatype string s	0.111111
for	query	0.010753
a temporary table in the	table	0.031250
at pos in byte and	pos	0.022222
pretty printing of a densematrix >>>	str	0.090909
:class isotonicregression	isotonic regression	0.090909
sets	params set	1.000000
how much	merger object	0.032258
the selector type of the chisqselector	sq selector set selector type selectortype	0.333333
value of the date column	next day date	0.100000
predictions which gives the	generalized linear regression summary	0.090909
the month of a given	dayofmonth	0.027027
overwrite	overwrite	1.000000
instance to the given path a shortcut of	ml	0.007339
of index value pairs or two	mllib linalg	0.026316
so	query manager	0.011905
number of times a token must appear to	count	0.016949
query was terminated by an exception or none	query exception	0.500000
using the old hadoop outputformat api mapred	save as hadoop dataset conf keyconverter valueconverter	0.083333
the stream query	stream writer	0.041667
this instance contains a param with	ml param params has param	0.019231
model to a local representation this discards	ldamodel to local	0.200000
the dot product of two vectors we support	vector dot other	0.050000
is not contained	core rdd subtract	0.333333
degrees of	generalized linear regression summary degrees of	1.000000
create	create cls	1.000000
current timestamp as a timestamp column	current timestamp	1.000000
of nonzero elements	ml linalg dense	0.100000
"predictions" which gives	logistic regression summary	0.090909
__init__(self inputcol=none outputcol=none indices=none names=none)	init inputcol outputcol indices names	1.000000
local	core spark context get local	0.333333
test that the	mllib streaming logistic regression with sgdtests test	0.111111
temporary table	table	0.031250
a given	param params	0.014925
objective function scaled loss + regularization at each	training summary objective history	0.500000
using an associative and	core	0.003021
list of indices to	ml chi sq selector	0.100000
as the spark fair scheduler	spark context	0.023256
for each numeric columns for each group	sql grouped data	0.125000
the day of the month	dayofmonth col	0.031250
this instance contains a param	params has param	0.019231
specifies the underlying output data source	sql data stream writer format source	0.333333
parameters in this grid	param grid builder base on	0.076923
much of memory for	core external	0.016129
of users for a given product and returns	users product	0.142857
minpartitions	minpartitions	0.357143
in c{self} that is not contained in c{other}	rdd subtract other numpartitions	0.111111
all globals names read or written	pickler extract code globals	0.125000
create a new	sql hive context create	0.083333
extract the week number of	sql weekofyear	0.055556
new terminations	sql streaming query manager reset	0.011905
term	core accumulator add term	0.066667
month of	sql dayofmonth	0.031250
memory string in the	memory s	0.142857
of the :class dataframe to a data source	sql data frame	0.005348
to wait	streaming query	0.010526
a given product and	product	0.029412
to pass else fail with an	mllib mllib streaming test case eventually	0.250000
a resulting rdd that contains	rdd cogroup other	0.066667
the number	matrix num	0.088235
queue of	context queue	0.500000
list of indices	ml chi sq selector model	0.166667
:class dataframe to a data	sql data frame	0.005348
get spark_user for user who is running sparkcontext	spark context spark user	0.250000
values for each key in	by key numpartitions	0.111111
single :class pyspark sql types longtype	sql sqlcontext range start end step numpartitions	0.083333
given data type	sql parse datatype	0.333333
l{statcounter} object that captures the mean variance and	core rdd stats	0.083333
that makes a class inherit documentation from	mllib inherit doc	0.045455
0] for feature selection by fwe	mllib chi sq selector set fwe fwe	0.200000
problem in multinomial	mllib	0.010526
a dataframe as	sql data frame	0.005348
perform a left outer join of	rdd left outer join other	0.111111
the standard deviation of	core rdd stdev	0.066667
get the cluster centers represented as a list	mllib kmeans model cluster centers	0.083333
for which predictions	regression model	0.031250
by key	by	0.014286
next memory limit if the memory is	sorter next limit	0.200000
sets	input col set	1.000000
the current [[dataframe]] and perform the specified	sql grouped data pivot pivot_col values	0.050000
changes the uid of this instance this	ml param params reset uid newuid	0.058824
dump already	spill	0.038462
fast version	core heappushpop heap item	0.142857
returns weighted averaged f-measure	multiclass metrics weighted fmeasure beta	1.000000
set decay factor	streaming kmeans set decay factor decayfactor	1.000000
partitioned data into	by	0.014286
a test statistic	mllib stat test	0.166667
train the model on the incoming dstream	sgd train on dstream	1.000000
:func awaitanytermination() can be	manager	0.011236
jtime	jtime	1.000000
idf vector	idfmodel idf	0.166667
this thread such as the spark fair	core spark	0.010309
loads vectors	load vectors sc	0.333333
applies standardization transformation on	model transform	0.500000
broadcast a read-only variable to the	spark context broadcast value	0.125000
test predicted values on a toy model	logistic regression with sgdtests test	0.111111
stats to stdout	core profiler show	0.166667
get a local property set in this thread	context get local property key	0.066667
collect each rdds into the returned list	spark streaming test case collect dstream n block	1.000000
setparams(self statement=none) sets params for	set params statement	0.333333
param with a	ml	0.001835
adds two	mllib linalg	0.026316
distinct elements in this rdd	core rdd distinct	0.250000
optional default value and user-supplied value in a	params	0.006623
shortcut of write()	ml one vs rest	0.052632
other	other numpartitions	0.250000
comprised	mllib random rdds poisson	0.125000
parammap	pyparammap	0.166667
set the trigger for the stream query	data stream writer trigger	0.083333
using an associative and	by	0.014286
computes column-wise summary statistics for the	stat statistics	0.125000
of memory	merger object size	0.032258
add a py or zip dependency for all	context add py file path	0.166667
for statistic	stat	0.076923
fixed-size sampled subset	take sample withreplacement num seed	1.000000
column for distinct count	count distinct col	0.040000
matrix whose columns are the left singular	mllib linalg singular	0.017544
wait for the execution to stop	timeout timeout	0.125000
the libsvm format into an rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures	0.125000
set initial centers should be set before calling	initial centers centers	0.200000
test method	stat chi sq test result method	0.250000
instance with a randomly generated uid	cross validator	0.045455
partial	partial	0.461538
tree (e g depth 0 means 1 leaf	decision tree model	0.050000
perform a left outer join	left outer join other numpartitions	0.111111
number of nonzero	ml linalg dense vector num	0.250000
cluster centers represented as a list of	ml bisecting kmeans model cluster centers	0.333333
an	mllib linalg	0.026316
threshold precision curve	ml binary logistic regression summary precision by threshold	0.166667
f-measure or f-measure for a given label category	multiclass metrics f measure label beta	1.000000
the initial value	streaming logistic regression with sgd set initial	0.111111
n	n truncate vertical	0.250000
a randomly	split	0.125000
checkpoint data or create	streaming streaming context get or create	0.200000
columns in an input dataframe to the	columns	0.039216
only create a new hivecontext for testing	create for testing	0.333333
sets the accumulator's value only usable in driver	core accumulator value value	0.050000
:class dataframe from an :class rdd, a list	schema samplingratio verifyschema	0.029412
mixture components	mixture model predict soft	0.142857
again to wait for	manager reset	0.011905
line	line	1.000000
to any hadoop file system using the l{org	sequence file path	0.500000
vector columns in an input dataframe	vector columns from ml dataset	0.142857
a java object	java params from java	0.333333
model trained	tree ensemble model	0.038462
without	without	1.000000
of objects from	serializer load	0.083333
which is defined as	linear regression summary	0.013889
parses the expression string into the	expr str	0.125000
i i d samples drawn	shape scale numrows	0.125000
load a model from the given	model load cls sc	0.250000
setparams(self min=0 0 max=1 0 inputcol=none outputcol=none)	set params min max inputcol outputcol	1.000000
add a py or	add py file path	0.166667
sort the list based on first value	test case sort result based on key outputs	0.333333
called when processing of a job	listener on output operation	0.166667
create a multi-dimensional cube for the current :class	data frame cube	0.055556
cost sum of squared distances of points	cost x	0.142857
least value of the list	sql least	0.055556
wraps a function rdd[x] -> rdd[y]	transform function	0.166667
fits a model to the input dataset this	dataset	0.020408
their vector representations	model get vectors	0.142857
to drop the names of fields in obj	sql	0.002525
the index	with index	0.100000
data or table already	data frame	0.005000
model to a local representation this discards info	ldamodel to local	0.200000
runs	k maxiterations mindivisibleclustersize	1.000000
distributed matrix on the	matrix	0.015152
regparam	regparam	1.000000
clear	clear	1.000000
fast version of a	core heappushpop heap	0.142857
removes the specified table from the in-memory cache	uncache table tablename	0.250000
the week number of a given	weekofyear	0.043478
computes the levenshtein distance of the two	levenshtein left	0.058824
string to a :class datatype the data type	datatype	0.045455
estimate of the importance of each feature	ml random forest regression model feature importances	0.250000
metricname	metric name	1.000000
wait until any of	sql streaming query manager await any termination timeout	0.166667
loads parquet files returning the result as	reader parquet	0.200000
list of functions registered in the specified database	catalog list functions dbname	1.000000
contains a	params	0.006623
this instance contains a param with a given	ml param params has param	0.019231
setparams(self estimator=none estimatorparammaps=none evaluator=none	validation split set params estimator estimatorparammaps evaluator	1.000000
k=none	k	0.071429
using	by	0.014286
comprised of vectors containing i i d samples	mllib random rdds exponential	0.125000
by name	name doc	0.666667
of day in utc	utc	0.100000
return a javardd of object by unpickling	core rdd	0.003460
the kmeans algorithm for fitting	streaming kmeans	0.035714
iteration	iteration	1.000000
row used in python repl	sql row repr	0.250000
into	core	0.003021
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	decision tree regressor	0.058824
test that the final value	test	0.015152
using the old hadoop	hadoop	0.050000
trait for multivariate statistical summary of	multivariate statistical summary	0.250000
libsvm format into an rdd of labeledpoint	mlutils load lib svmfile	0.125000
sort the list based	py spark streaming test case sort result based	1.000000
calculates the norm of	ml linalg dense vector norm	0.333333
in	grid builder	0.055556
a	param params has	0.038462
or c{other}, return a resulting rdd that contains	core rdd cogroup other	0.066667
data sampled	data	0.011628
given string	param params has param	0.019231
return a callsite representing the	core	0.003021
indices back to a	index to	0.040000
dataframe with two fields threshold recall curve	binary logistic regression summary recall by threshold	0.166667
to make predictions on batches	mllib streaming linear algorithm predict on	0.066667
special result	result	0.125000
distance from a sparsevector or 1-dimensional numpy	distance	0.095238
vector	mllib linalg vectors	0.333333
generates an rdd comprised of vectors	mllib random rdds exponential vector rdd sc mean	0.200000
instance	ml	0.001835
loads	loads	1.000000
unique id of	id	0.083333
residual degrees of freedom for	generalized linear regression summary residual degree of freedom	0.125000
this instance	ml param params has	0.019231
new :class dataframe with	sql data frame drop	0.333333
observed is vector conduct pearson's chi-squared goodness of	stat statistics chi sq	0.066667
the ownership	param params resolve	0.333333
termination of this query either	termination timeout	0.041667
returns a list of active queries associated with	manager active	0.066667
current status of	status	0.111111
a text file using string representations of	text file path compressioncodecclass	0.166667
approximately find at most k items	lshmodel approx	0.100000
of this	ml one vs rest	0.052632
partial objects do not serialize correctly in python2	cloud pickler save partial obj	0.125000
of object by unpickling	ml	0.001835
sparkcontext which is associated with	streaming streaming context spark	0.083333
given parameters in this grid to fixed	builder add grid param	0.250000
singular vectors of the singularvaluedecomposition	singular value decomposition	0.166667
of nodes summed over all trees	nodes	0.037037
this instance is	is	0.083333
a java parammap into a python	from java	0.111111
stream transform	stream transform	1.000000
the incoming dstream	dstream	0.093750
converter to drop the	converter	0.052632
return a l{statcounter} object that captures	stats	0.055556
__init__(self threshold=0 0 inputcol=none outputcol=none)	binarizer init threshold inputcol outputcol	1.000000
the number of	matrix num	0.088235
java storagelevel based on a	core spark context get java	0.333333
given data type string to a :class	sql	0.002525
the python direct kafka stream	kafka direct stream from	0.125000
'x' has maximum membership in this model	gaussian mixture model predict	0.100000
a function on rdds of the dstreams	dstreams transformfunc	0.125000
an object is of type	type obj identifier	1.000000
__init__(self	scaler init	1.000000
day of the month	sql dayofmonth col	0.031250
the norm of a	sparse vector norm p	0.066667
point in rdd 'x' to all mixture components	mixture model predict soft	0.142857
params to	ml java params to	0.045455
similarities between columns	column similarities threshold	0.333333
of fit test of the observed data against	test observed	0.090909
broadcast	core spark context broadcast	0.125000
sum for each numeric columns for each	sql grouped data sum	0.083333
of this instance with a	ml one vs	0.142857
splits=none inputcol=none outputcol=none handleinvalid="error")	splits inputcol outputcol handleinvalid	1.000000
the given parameters in this	ml param grid builder add	0.200000
optional default value	param params	0.014925
tc	tc	1.000000
error which is defined	mllib regression	0.022727
which is later than the value of	dayofweek	0.037037
use only create a new	create	0.017241
used again to wait for new	reset	0.011236
number of rows of blocks	num row blocks	0.500000
residuals label - predicted	ml linear regression summary residuals	0.500000
model trained	logistic regression model	0.083333
cost sum of squared distances of points	compute cost x	0.142857
full description of model	ml decision tree model to debug string	1.000000
none by default	valueconverter	0.166667
mindivisibleclustersize	min divisible cluster size	1.000000
by step	step	0.100000
converter to drop the names of	converter datatype	0.071429
observed tokens in the training set	distributed ldamodel training	0.034483
similarity	similarity	1.000000
hash functions sha-224 sha-256 sha-384 and sha-512	sha2 col numbits	0.500000
the norm of	sparse vector norm	0.066667
given parameters in this grid to	grid builder add grid	0.100000
compute the dot product of two vectors we	linalg dense vector dot	0.058824
test	context tests test	0.500000
a local	core spark context get local	0.333333
the given path a shortcut of write() save	ml pipeline model save	0.166667
computes column-wise summary statistics for the input	statistics col stats	0.200000
param with a	ml param params has	0.019231
and	core basic	0.066667
user-supplied values and then merges them with extra	map extra	0.040000
that all the objects	core external merger	0.032258
of the month of a given date as	dayofmonth	0.027027
k-means algorithm	kmeans train	0.333333
params for	params formula	0.500000
and commutative reduce function	core rdd reduce by	0.125000
to wait for	query	0.010753
is set to	core spark context set	0.166667
the heap	heap	0.047619
load a model from the	java loader load cls sc	0.250000
create a method for given binary operator	sql bin op name doc	0.200000
end value exclusive	end	0.066667
in parquet	parquet	0.066667
that generates monotonically increasing 64-bit integers	sql monotonically increasing id	0.333333
return a	core spark	0.010309
relative accuracy. smaller values create	relativesd	0.166667
updates	update	0.055556
compute the number	num	0.025210
make predictions on a	mllib streaming kmeans predict on	0.500000
week number	sql weekofyear	0.055556
test the	context tests test	0.500000
sort the list based on	test case sort result based on key outputs	0.333333
note this docstring is not shown publicly	linalg coordinate matrix init entries numrows numcols	0.333333
this model instance	regression model	0.062500
vector to the new mllib-local representation	mllib linalg sparse vector as ml	0.333333
:class dataframe replacing a	data frame replace to_replace	0.100000
number of partitions to use during reduce tasks	default reduce partitions	0.166667
number of columns	linalg block matrix cols	0.333333
infer schema from an rdd of row or	sqlcontext infer schema rdd	0.250000
instance	has	0.011628
wait for new terminations	streaming	0.005025
this matrix to	mllib linalg dense matrix	0.083333
of number of data points in each cluster	ml clustering summary cluster sizes	0.333333
modified to support __transient__ on new objects	cloud pickler save reduce func args state listitems	0.111111
loads parquet files returning	reader parquet	0.200000
dispatch to handle all function	core cloud pickler save function obj name	0.142857
initial	set initial	0.111111
of this dstream	kafka dstream	0.250000
context to use	context	0.090909
the minutes of a given date	sql minute col	0.050000
javardd of object by unpickling it	ml	0.001835
to this accumulator's value	core	0.003021
dense vector of 64-bit floats from	ml linalg vectors dense	0.166667
dispatch to handle all function	cloud pickler save function obj name	0.142857
cluster	kmeans model cluster	0.333333
the week number of a given date as	sql weekofyear	0.055556
provides methods to set k decayfactor timeunit	streaming	0.005025
to a string	to	0.007692
__init__(self inputcol=none outputcol=none handleinvalid="error")	ml string indexer init inputcol outputcol handleinvalid	1.000000
a :class dataframe	sql data	0.024390
model fitted by :py class minmaxscaler	min max scaler model	1.000000
source	source	0.315789
dot product of	vector dot	0.050000
already partitioned data	spill	0.038462
measure	measure	1.000000
window specification that defines the partitioning ordering	window spec	0.166667
number	mllib linalg sparse vector num	0.200000
converts matrix columns in	convert matrix columns to ml dataset	0.166667
a column scipy matrix from	mllib sci py tests scipy matrix	0.090909
column containing a json string into	from json col	0.083333
saves the contents	mode partitionby	0.066667
setparams(self	regression set params	1.000000
slice of byte array that starts at pos	pos	0.022222
the spark fair scheduler	core spark context	0.011628
of conditions and returns	sql column otherwise	0.050000
year of a given date	year	0.040000
an array containing the ids	stage ids	0.055556
content of the :class dataframe to the	sql data frame writer	0.011628
by applying a function on each rdd of	transform func	0.058824
columns for the given table/view	columns	0.019608
used again to wait for new	manager reset	0.011905
paired rdd where the first element is the	matrix factorization model	0.043478
specifies some hint on the current dataframe	data frame hint name	1.000000
__init__(self	ml regex tokenizer init	1.000000
copy	copy	0.833333
extract the minutes	sql minute col	0.050000
batch of jobs has completed	batch completed batchcompleted	0.333333
the datatype	datatype	0.045455
the spark fair	core spark	0.010309
__init__(self withmean=false withstd=true inputcol=none outputcol=none)	scaler init withmean withstd inputcol outputcol	1.000000
new dstream by applying reducebykey to each	streaming dstream reduce by key func	0.076923
set to a different value or	set	0.005917
only create a new hivecontext for testing	hive context create for testing	0.333333
changes the uid of this instance this	ml param params reset uid	0.058824
new	manager	0.011236
until any of the queries	sql streaming query manager await any termination	0.142857
soundex encoding for a string >>>	sql soundex	0.055556
contains a param with a given string	ml param params	0.013699
return the latest model	mllib streaming kmeans latest model	0.500000
globals names read or written to	extract code globals	0.125000
in rdd 'x' to all mixture	mixture model	0.066667
column	mllib standard scaler	0.100000
count of distinct elements in rdds	count	0.016949
from start to end exclusive increased by	spark context range start end	0.333333
on a	with	0.055556
each rdd in	rdd func	0.250000
a sliding window	value and window windowduration slideduration	0.076923
the rdd's elements in one operation	core	0.003021
this instance contains a	param	0.012500
queries so that :func awaitanytermination() can be used	manager reset	0.011905
of memory for this obj assume	external merger object size obj	0.040000
setparams(self	linear regression set params	1.000000
weights computed for every feature	model weights	0.166667
inverse=false	inverse	0.142857
which is a dataframe having two fields	logistic regression	0.040000
the	object size	0.032258
be used again	streaming query	0.010526
encoding	encoding	1.000000
regression	regression model	0.031250
a python topicandpartition to map	init topic partition	0.055556
until any of the queries on	streaming query manager await any termination	0.142857
again	streaming query manager	0.011236
frequency vectors or transform the	hashing tf transform	0.045455
load a java model from	loader load java cls sc	0.200000
maximum item	max key	0.333333
the current [[dataframe]] and perform the specified	data pivot pivot_col values	0.050000
wrapper of	ml java	0.076923
2 range 4	linalg dense matrix repr	0.142857
calculates the norm of	linalg sparse vector norm	0.066667
extract a	sql regexp extract	0.500000
field in "predictions" which gives the	linear regression summary	0.041667
ml params instances for the	params m1 m2	0.047619
by :func query stop() or by an	streaming query await	0.333333
accumulator's value	accumulator value	0.050000
runs and profiles the method	core	0.003021
or list in each batch	oneatatime	0.111111
to in this model	kmeans model	0.181818
load a java	mllib java loader load java cls sc	0.200000
outputcol	outputcol	1.000000
seed=none): sets params for cross validator	ml cross validator set params	0.500000
partitioned data into	core external group by	0.045455
that starts at pos	pos	0.022222
__init__(self	ml rformula init	0.500000
jobs	job	0.047619
list of active queries associated with this	streaming query manager active	0.066667
get a local property set	context get local property key	0.066667
train the model on	mllib streaming linear regression with sgd train on	0.333333
right singular vectors	linalg singular	0.017544
all	object	0.027778
value of	ml regex	1.000000
the mean variance and	core	0.003021
min=0 0 max=1 0	min max	0.500000
get all values as a list of key-value	get all	0.166667
timeunit	timeunit	0.128205
in the key-value pair rdd through a flatmap	core rdd flat map values	0.333333
group the values for each key in the	group by key numpartitions	0.333333
data of another dstream	other	0.033333
set initial centers should	kmeans set initial centers centers weights	0.200000
find	model find	0.500000
sliding window over	value and window windowduration slideduration	0.076923
alias set	alias alias	0.333333
first date which is later than the	dayofweek	0.037037
existing	samplingratio	0.100000
field in "predictions" which	regression summary	0.107143
:py attr predictions	generalized linear	0.200000
words to their vector representations	word2vec model get vectors	0.166667
queries so	streaming	0.005025
:class dataframe as	sql data frame writer save as	0.071429
items for columns possibly with	items	0.066667
halflife	halflife	1.000000
mintf	mintf	1.000000
param from the param	param	0.006250
the underlying output	stream writer format	0.333333
the day of the month of a	dayofmonth col	0.031250
a new profiler using class profiler_cls	collector new profiler ctx	0.333333
stream query if this is not set it	stream	0.017544
close to	parameter accuracy	0.029412
named table accessible via jdbc url url	reader jdbc url table column lowerbound	0.166667
setparams(self estimator=none estimatorparammaps=none evaluator=none trainratio=0 75 seed=none):	validation split set params estimator estimatorparammaps evaluator trainratio	0.500000
for params shared by	params	0.006623
vector	vectors	0.166667
vector conduct pearson's chi-squared goodness of fit	stat statistics chi sq	0.066667
that :func awaitanytermination() can be used	streaming query	0.010526
:func awaitanytermination() can be used again to	sql streaming query	0.011765
class for all test results	test result	0.200000
ordered	ordered	0.461538
is vector conduct pearson's chi-squared	chi sq	0.111111
that :func awaitanytermination() can	streaming query manager	0.011236
least value of the list of column names	least	0.043478
the bisecting	mllib bisecting	0.666667
numbuckets	num buckets	1.000000
create a new accumulator with	accumulator init	0.083333
saves the	save path format mode partitionby	0.200000
lda	distributed ldamodel	0.052632
returned	py spark streaming test case	0.333333
stage info	stage info	0.142857
create a new accumulator	accumulator init aid	0.083333
centroids	timeunit	0.051282
the null	linear regression summary null	0.250000
prediction a k a confidence	prediction	0.041667
new :class dataframe replacing a	data frame replace to_replace	0.100000
given parameters in this grid to fixed	grid builder	0.055556
param	param params has	0.019231
converts matrix columns in an input	mlutils convert matrix columns to	0.166667
the rdd partitioned using	rdd	0.003058
be used with the spark sink	storagelevel maxbatchsize	0.045455
convert a list of column or names into	to	0.007692
multiple parameters passed as a list	spark conf set	0.111111
locate the	locate	0.076923
min value	min	0.083333
arbitrary key and value class from	core	0.006042
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online",	lda init featurescol maxiter seed	0.250000
threshold if any used for converting raw	threshold	0.018182
selected	selected	1.000000
a profile object is returned	basic profiler profile func	0.200000
the	merger object size	0.032258
:func awaitanytermination() can be used again	query manager	0.011905
tests whether this instance contains	has param paramname	0.142857
with arbitrary key and value class from	core	0.006042
trees in the ensemble	ensemble	0.100000
key-value pair rdd through a flatmap	core rdd flat map values	0.333333
java pipelinemodel create and return a python	pipeline model from java	0.142857
for approximate	approx	0.047619
to a new column	to	0.007692
:class dataframe in json	sql data frame	0.005348
use for saving	ml mlwriter	0.200000
fits a java model to the input dataset	java estimator fit java dataset	1.000000
month	sql dayofmonth col	0.031250
max value	max	0.071429
value of	ml tree classifier	1.000000
:class windowspec with the frame	sql window range	0.166667
value of	ml elementwise product	1.000000
in rdds in a sliding window	value and window windowduration slideduration numpartitions	0.076923
new map	map	0.058824
the year of	year col	0.050000
ordering columns in	order by	0.142857
for distinct count of	count distinct	0.040000
get number of	num	0.016807
squared distance of two vectors	ml linalg dense vector squared distance	1.000000
ensemble model	ensemble model	0.058824
function without changing	values f	0.062500
fill the datatype with types inferred from obj	type obj datatype	0.333333
sets the given spark runtime configuration property	sql runtime config set key	1.000000
is later than the value	dayofweek	0.037037
objective	summary objective	1.000000
for each numeric columns for	sql grouped	0.086957
extra parameters	extra	0.023810
list of numpy	ml bisecting	0.066667
termination of this query either by	termination	0.035714
use only create a new hivecontext for	context create for	0.250000
the accumulator's value only usable in driver program	accumulator value value	0.050000
buckets the	numbuckets	0.142857
the right singular vectors of	singular	0.015625
number of columns that make up each	mllib linalg block matrix cols per	0.333333
python topicandpartition to map	topic and partition init topic partition	0.055556
loads a csv file	reader csv path schema sep encoding	0.666667
test predicted values on a toy model	with sgdtests test	0.111111
so that :func awaitanytermination() can	sql streaming query manager	0.011905
of jobs has started	started	0.055556
each original column during fitting	original	0.047619
test the python direct kafka stream foreachrdd	stream tests test kafka direct stream foreach	1.000000
weights computed for every	model weights	0.166667
used to load a :class dataframe	data frame	0.005000
function without changing the keys	values f	0.062500
mean squared error	mean squared error	0.500000
dot product of two vectors we support	ml linalg dense vector dot other	0.090909
of labels corresponding	ml string indexer model labels	0.066667
values for each numeric columns	grouped	0.035714
random forest model for	random forest	0.041667
how much of memory	external merger	0.031250
vector columns in an input dataframe from	vector columns	0.071429
an fp-growth model that contains frequent itemsets	mllib fpgrowth train cls data	0.100000
comprised of vectors containing	random rdds	0.076923
memory for this	core external	0.016129
number of	linalg row matrix num	0.100000
the given parameters in	ml param grid builder	0.055556
dictionary a list	init size	0.066667
:py attr withstd	with std value	1.000000
batch has completed	completed outputoperationcompleted	0.125000
restore an object	restore name	0.333333
of names of tables in the	names	0.050000
selector type of the chisqselector	sq selector set selector type selectortype	0.333333
converts matrix columns in	mlutils convert matrix columns from	0.166667
for which predictions are known	isotonic regression model	0.100000
add a py	spark context add py file path	0.166667
a new hivecontext for testing	for testing cls	0.333333
a resulting rdd that contains a tuple	rdd cogroup other	0.066667
create an input stream that pulls	utils create stream ssc hostname port	0.200000
"zerovalue" which may be	rdd fold by	0.125000
the embedded params to	params to	0.035714
given data type	parse datatype	0.333333
words	word2vec	0.052632
any hadoop-supported file	file path	0.035714
a given string	params has	0.019231
operation test for dstream countbyvalue	operation tests test count by value	1.000000
this instance's params to	java params to	0.045455
for this obj assume	merger object size obj	0.040000
set bandwidth of each sample defaults to 1	mllib stat kernel density set bandwidth bandwidth	0.142857
unified dstream from multiple dstreams of	streaming streaming context union	0.111111
this obj assume	object size obj	0.040000
the dependent variable given a	mllib linear regression model base	0.200000
the levenshtein distance of the	sql levenshtein	0.058824
:py attr linkpredictioncol	link prediction col value	1.000000
the cluster	bisecting kmeans model cluster	0.333333
distinct count of col or	count distinct	0.040000
train a random forest model for	mllib random forest train classifier cls data	0.250000
correlation of two columns	col2 method	0.055556
serialize an object into a byte array	core framed serializer dumps obj	1.000000
the probability of obtaining a test statistic result	mllib stat test result	0.166667
belongs to this params	ml param params	0.013699
converts vector columns in	mlutils convert vector columns from ml	0.166667
use the model to make predictions on batches	linear algorithm predict on	0.066667
of the month of a given	sql dayofmonth	0.031250
new dstream by applying a function	map f	0.037037
to wait for new	manager reset	0.011905
the default implementation of	ml	0.001835
a python topicandpartition to	topic partition	0.055556
sets the accumulator's value only usable in driver	core accumulator	0.030303
the jobs started by this	job	0.023810
an input stream that is	stream ssc addresses	0.166667
params	params featurescol labelcol predictioncol classifier	0.500000
against the expected distribution	expected	0.076923
broadcast a read-only	core spark context broadcast	0.125000
of memory	core	0.003021
all active stages	tracker get active	0.333333
into an rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures minpartitions	0.125000
partial	pickler save partial	0.125000
of	offset	0.021739
be used again to wait	streaming query	0.010526
resulting rdd that contains a tuple	rdd cogroup	0.066667
option of ensuring all received data	stopsparkcontext stopgracefully	0.050000
fp-growth model that	fpgrowth train cls data	0.200000
weights computed for every	linear model weights	0.250000
the old hadoop outputformat api mapred package	save as hadoop dataset conf keyconverter valueconverter	0.083333
the day of the month of a given	dayofmonth col	0.031250
function without changing the keys this also retains	f	0.010526
or a :class pandas dataframe	data frame data	0.142857
outputformat	outputformatclass keyclass valueclass	0.250000
partition for kafka	partition	0.066667
out into external	sql	0.002525
mean	metrics mean	0.333333
number	mllib linalg dense vector num	0.200000
column	col options	0.500000
infer schema from an rdd of row or	sqlcontext infer schema rdd samplingratio	0.250000
clean up all the files in disks	core external merger cleanup	1.000000
predict the label of	predict x	0.016949
the first n rows to the console	sql data frame show n truncate vertical	0.333333
next memory limit if	sorter next limit	0.200000
attr predictions which gives the	linear regression summary	0.013889
dot product of	linalg dense vector dot other	0.058824
used with the spark sink deployed on a	ssc addresses storagelevel maxbatchsize	0.045455
the	merger	0.025641
the column	mllib standard	0.125000
dataset and an	dataset	0.020408
empty	empty	1.000000
a decorator that makes a class inherit documentation	mllib inherit doc cls	0.045455
thread such as the spark fair scheduler	spark context	0.023256
test the python direct kafka stream api with	kafka stream tests test kafka direct stream from	0.333333
levenshtein distance of the two given	sql levenshtein left	0.058824
function without changing the	values f	0.062500
new vector with 1 0 bias appended	mlutils append bias data	0.333333
queries so that :func awaitanytermination() can be used	sql streaming query	0.011765
aggregate the	aggregate	0.111111
of the rdd's elements in	rdd	0.003058
how	core	0.003021
parameters in this grid to fixed values	grid builder base on	0.076923
:py attr maxcategories	max categories value	1.000000
from	range	0.030303
a line	line line	0.166667
a python topicandpartition to map to the	streaming topic and partition init topic partition	0.055556
get or compute the number of rows	row matrix num rows	0.200000
local property that affects jobs submitted	local property key value	0.076923
kafkaparams	kafkaparams	1.000000
load a java model from	java loader load java cls sc	0.200000
removes the specified table from the in-memory	sql catalog uncache table tablename	0.250000
a java storagelevel based on a pyspark storagelevel	core spark context get java storage level storagelevel	0.500000
params to the wrapped	params to	0.035714
a python topicandpartition to map to	topic partition	0.055556
point in rdd 'x' to all mixture	gaussian mixture	0.038462
defined function in python	defined function	0.066667
this obj assume that all	object size obj	0.040000
list of numpy arrays	ml bisecting kmeans model	0.076923
so that :func awaitanytermination() can be	sql streaming query manager reset	0.011905
of the :class dataframe to	frame writer save path format	0.066667
to make predictions on batches of data from	streaming linear algorithm predict on	0.066667
<http //jsonlines	mode compression dateformat	1.000000
a model	mllib streaming linear regression	0.333333
type for	type cls	0.250000
in c{self} that is not contained in	rdd subtract other numpartitions	0.111111
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	tree regressor	0.058824
names into a jvm seq	seq sc cols	0.055556
extract the minutes of a given date	sql minute	0.050000
data with three columns * antecedent - array	fpgrowth model association rules	1.000000
modeltype="multinomial", thresholds=none weightcol=none)	naive bayes	0.142857
setparams(self featurescol="features", labelcol="label", predictioncol="prediction",	set params featurescol labelcol predictioncol	0.600000
generate	regression with sgdtests generate logistic input offset	1.000000
wait a given amount of time for a	timeout catch_assertions	0.125000
be used	sql streaming query manager	0.011905
of tree (e g depth 0 means 1	tree	0.020833
test python direct kafka rdd	stream tests test kafka rdd	0.500000
local property set	local property key	0.035714
computes average values for each numeric	grouped data avg	0.058824
flat	flat	1.000000
as spark executor	core spark	0.010309
param with	param params	0.014925
finding frequent items for columns possibly with false	sql data frame freq items cols support	0.166667
the area	classification metrics area	0.333333
trigger for the	writer trigger	0.111111
the stream query if	sql data stream writer	0.041667
dstreams	context transform dstreams transformfunc	0.125000
:py attr predictions which gives the predicted	generalized linear regression summary prediction	0.250000
of this dataset checkpointing can be	frame checkpoint eager	0.071429
:class column for approximate	approx	0.047619
string in libsvm format	libsvm p	0.250000
right singular vectors of the	linalg singular	0.017544
the dot product of two vectors	dense vector dot	0.050000
the underlying output	frame writer	0.050000
awaitanytermination() can be	manager reset	0.011905
system using the new hadoop outputformat api mapreduce	save as new apihadoop dataset conf keyconverter valueconverter	0.142857
setparams(self stages=none) sets params for pipeline	ml pipeline set params stages	1.000000
used for	dataset	0.020408
the elements in seen in	windowduration slideduration	0.083333
curve which is a dataframe having two	ml binary logistic regression	0.142857
residuals	residuals	1.000000
akaike's "an information criterion" aic	generalized linear regression summary aic	0.250000
the training set given	ldamodel training	0.034483
model to make predictions on	predict on	0.117647
distributed matrix whose columns are the left singular	singular	0.015625
computes average values for each numeric columns for	grouped data avg	0.058824
how much of memory for this obj assume	size obj	0.040000
test of the observed	test observed	0.090909
the levenshtein distance of the two given	sql levenshtein	0.058824
field in schema abstract >>> _parse_field_abstract("a")	field abstract s	0.500000
sets the given parameters in this grid to	param grid builder	0.055556
to the input dataset this is called	dataset	0.020408
get all values as a list	conf get all	0.166667
tokens in the training set given the	training	0.029412
for each numeric columns for	sql grouped data avg	0.058824
function to the	f	0.021053
convert a value to an int if possible	param type converters to int	0.250000
associative and commutative reduce function but	rdd reduce	0.071429
a large dataset and an item approximately	ml lshmodel approx nearest neighbors dataset	0.166667
of the rdd	rdd partition	0.062500
value class from an	rdd	0.006116
create a new	init	0.021277
choose one directory for	external sorter get path	0.333333
:func awaitanytermination() can be used again to wait	streaming query	0.010526
string	param params has	0.019231
other from this block matrix	linalg block matrix	0.052632
year of a	sql year	0.050000
threshold=0 0 weightcol=none	linear	0.051282
broker to	broker	0.100000
class inherit documentation from its	mllib inherit doc cls	0.045455
be used again to wait for new	sql streaming query	0.011765
squared distance from	ml linalg sparse vector squared distance	0.166667
loading	mlreader	0.111111
finding frequent items for	data frame freq items cols	0.166667
iterations default 1 which should be smaller	iterations	0.043478
fits a java model to the input dataset	estimator fit java dataset	1.000000
residual degrees of freedom for the null	summary residual degree of freedom null	0.333333
storagelevel based on	context	0.022727
in this frame but not in another frame	frame subtract other	0.333333
run	run	1.000000
estimator=none estimatorparammaps=none evaluator=none	estimator estimatorparammaps evaluator	0.750000
block matrix other from this block	linalg block	0.076923
python rdd of key-value pairs (of form c{rdd[	rdd save	0.038462
decomposition svd factors	decomposition	0.166667
the probability of obtaining a test statistic	mllib stat test	0.166667
resulting rdd that contains a tuple with	core rdd cogroup other numpartitions	0.066667
columns	cols	0.052632
returns a paired	matrix factorization	0.040000
half	half	0.352941
0 9 0 95 0 99], quantilescol=none aggregationdepth=2):	fitintercept	0.058824
first n rows to the console	sql data frame show n truncate	0.333333
an fp-growth model that contains	mllib fpgrowth train cls data minsupport	0.100000
test prediction on a model with weights	linear regression with tests test prediction	0.500000
returns weighted averaged	metrics weighted fmeasure	1.000000
returns date truncated to the unit specified by	sql trunc date	1.000000
compute the number of rows	mllib linalg row matrix num rows	0.200000
month which the given date	date	0.037037
and assert both have the same param	m2 param	0.125000
for a given label	label	0.071429
already partitioned data into	external group by	0.045455
rdd of key-value	core rdd save	0.037975
an rdd comprised of vectors containing i	mllib random rdds poisson vector rdd	0.166667
accumulator's value only usable in driver	accumulator value value	0.050000
used	sql streaming query	0.011765
offsetrange of specific kafkardd	kafka rdd offset ranges	0.333333
gbtclassifier	gbtclassification	0.142857
outputmode	outputmode	1.000000
all the	core external merger object size	0.032258
feature selection by number of top	mllib chi sq selector set num top	0.500000
loads a parquet file	parquet path	0.333333
estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none):	estimator estimatorparammaps evaluator numfolds	0.200000
the f function	f	0.021053
specific topic and	topic and	0.333333
with a function and	function	0.027778
:py attr min	min value	1.000000
of	ml java classification	1.000000
levenshtein distance of the two given strings	levenshtein left	0.058824
compute the sum for each numeric	grouped data sum	0.083333
window of time over	window	0.037037
mean values	scaler model mean	0.125000
in this grid to fixed values	ml param grid builder add grid param values	0.333333
:func awaitanytermination()	streaming query manager	0.011236
user and the second is an	mllib matrix	0.047619
creates a new	init sparkcontext sparksession	1.000000
transpose this blockmatrix returns a new blockmatrix	linalg block matrix transpose	1.000000
be used again to wait for new terminations	sql streaming query manager reset	0.011905
dataframe outputted by the model's transform method	ml linear regression summary predictions	0.200000
computes an fp-growth model that	mllib fpgrowth train cls data minsupport	0.100000
gbtregressor	gbtregressor	0.625000
url url and	url	0.076923
sets	classifier set	1.000000
bound on the log likelihood of	ml ldamodel log likelihood	0.166667
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	random forest classifier	0.022727
previously saved using l{rdd saveaspicklefile} method	core spark context pickle file name minpartitions	0.250000
the training set given	distributed ldamodel training	0.034483
every	mllib linear	0.166667
this :class dataframe as pandas pandas	to pandas	0.166667
the rdd	core rdd	0.003460
special result iterable this is used because	result iterable	0.500000
new :class dataframe	data frame	0.020000
column name for	col	0.016393
the dataset in a data source	source	0.105263
memory for this obj assume that all the	core external merger object size obj	0.040000
runtime	runtime	1.000000
directory	dir	0.142857
transformer that maps a column of indices back	index	0.041667
the java loader the	mllib java loader java loader	0.333333
sort the list	spark streaming test case sort result	0.333333
of nodes in tree including	mllib decision tree model	0.076923
sets	encoder set	1.000000
json	json path	0.100000
this configuration	spark conf	0.058824
a python rdd	core rdd save as	0.037500
adds a term to this	accumulator add term	0.066667
buckets the output by the	writer bucket by numbuckets col	0.200000
runs and profiles the method to_profile passed in	core basic	0.066667
with this spark	core spark	0.010309
internal use only create	create	0.017241
the dot product of two vectors	dot other	0.050000
lines text format or newline-delimited json <http //jsonlines	writer json path mode compression dateformat	0.166667
stages	stages	0.875000
wait	or timeout timeout	0.125000
correlation of two columns of	col2 method	0.055556
sets the accumulator's value	core accumulator	0.030303
a	core accumulator add	0.076923
this instance contains a param	params has	0.019231
for key	key	0.017857
a term to	core accumulator add term	0.066667
l1-norm loss	metrics	0.041667
partitioned data	spill	0.038462
rdd an rdd	iteration clustering train cls rdd	0.250000
this	other	0.033333
and count of the rdd's elements in	rdd	0.003058
multiplies this blockmatrix by other, another	multiply other	0.200000
using the model trained	tree ensemble model	0.038462
obj assume that	size obj	0.040000
vector to the new mllib-local representation	linalg sparse vector as ml	0.333333
clean up all the	merger cleanup	0.333333
the n elements from an rdd ordered in	core rdd take ordered	0.050000
returns a paired rdd where the	factorization model	0.043478
pipelinemodel create and return a python wrapper of	ml pipeline model from	1.000000
sets	aggregation depth set	1.000000
creates a local temporary view with this dataframe	sql data frame create temp view name	1.000000
table via	url table	0.200000
is defined as the square root of the	root	0.035714
again to	sql	0.002525
the non-streaming	write	0.071429
to wait for new terminations	query	0.010753
again to wait	manager reset	0.011905
specifies how data of a	data	0.011628
a large dataset and an item	nearest neighbors dataset key numnearestneighbors distcol	0.333333
verifyschema	verifyschema	1.000000
standard deviation	core rdd stdev	0.066667
samples drawn	numrows numcols numpartitions	0.125000
provides	streaming	0.005025
depth of tree (e g depth 0	tree model	0.026316
norm of	ml linalg sparse vector norm p	0.333333
stop the execution of the	streaming context stop	0.125000
frombase	frombase	1.000000
probability of obtaining a test statistic result	stat test result	0.166667
the minimum	min	0.083333
of gaussians in mixture	mllib gaussian mixture model	0.062500
wait until any of the queries	await any termination timeout	0.166667
original	model original	0.062500
containing elements from start to	core spark context range start	0.090909
computes column-wise summary statistics for the input	stat statistics col stats rdd	0.200000
return a resulting rdd that contains	rdd cogroup	0.066667
of the list of column names	sql	0.005051
ssc	ssc	0.833333
java model	java	0.012195
byte array that starts at pos in	pos	0.022222
matrices	matrices	1.000000
pipelinemodel used	ml pipeline model	0.066667
a new hivecontext for	for	0.111111
loads vectors saved using rdd[vector] saveastextfile	mlutils load vectors sc path	1.000000
used with the spark sink deployed on	ssc addresses storagelevel maxbatchsize	0.045455
compute the dot product of two vectors	vector dot	0.050000
new sparkcontext	spark context init	0.083333
trigger for the stream query if this is	data stream writer trigger	0.083333
filter	filter	1.000000
1 0] for feature selection by fwe	chi sq selector set fwe fwe	0.200000
list of columns	list columns	0.166667
finding frequent items for columns possibly with	frame freq items cols	0.166667
distributed model	distributed ldamodel	0.052632
calculates the length of	length	0.040000
can be used	query	0.010753
all the objects	core external merger object	0.032258
data	core external	0.016129
objects	merger object	0.032258
awaitanytermination() can	sql streaming query manager	0.011905
in this grid	builder add grid	0.100000
records	records	1.000000
to the wrapped java object	to	0.007692
resolves a param and validates the ownership	params resolve param param	0.333333
table named table accessible via jdbc url url	jdbc url table column lowerbound	0.166667
right singular vectors	mllib linalg singular	0.017544
given block matrix other from this block	mllib linalg block	0.111111
convert this vector to	linalg sparse vector	0.111111
returns a :class dataframe representing the result	session sql sqlquery	0.250000
large dataset and an item approximately find	ml lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
convert this vector to	mllib linalg vector	0.333333
disks	group by	0.041667
find synonyms of a word	find synonyms word	1.000000
returns the full class name	loader class cls clazz	0.333333
in multinomial logistic	mllib logistic	0.200000
timeunit to configure the kmeans algorithm	streaming kmeans	0.035714
so that :func awaitanytermination()	streaming query	0.010526
this instance	param params	0.014925
which	regression	0.110000
makes a class inherit documentation	mllib inherit doc	0.045455
note : experimental	count approx distinct	1.000000
the norm of a	linalg sparse vector norm	0.066667
a given	ml param params has	0.019231
docstring is not shown publicly	mllib linalg coordinate matrix init entries numrows numcols	0.333333
convert this matrix to the new mllib-local representation	linalg dense matrix as ml	0.333333
:class windowspec with the frame boundaries defined from	sql window range between	0.166667
later than the	dayofweek	0.037037
:class dataframe using the specified columns so	sql data frame	0.005348
create a column scipy matrix from a dictionary	mllib sci py tests scipy matrix size	0.090909
week number of a	weekofyear col	0.055556
args	args	1.000000
returns the number of clusters	mllib power iteration clustering model k	0.200000
on first value	on	0.037037
return the model	train rdd	0.166667
function types	function obj	0.500000
mixin for param standardization whether	has standardization	0.250000
of objects	serializer	0.062500
set a configuration	core spark conf	0.055556
soundex encoding for a	soundex	0.043478
is not contained in	rdd subtract	0.333333
standard deviation of this rdd's elements (which	stdev	0.047619
choose one directory for spill by	external merger get spill dir	0.500000
be used	streaming query manager reset	0.011905
on the log likelihood of	ml ldamodel log likelihood dataset	0.166667
parameters in	ml param grid builder	0.055556
load labeled points	mlutils load labeled points sc path	1.000000
slideduration in seconds	slide duration	0.333333
dump already	external group by	0.045455
'new api' hadoop	new apihadoop	0.333333
mixture	mllib gaussian mixture model predict	0.100000
onevsrestmodel create and return a python wrapper of	ml one vs rest	0.052632
:func awaitanytermination() can be	sql	0.002525
returns true if the	sql	0.002525
the :class dataframe to a data source	data frame writer	0.014085
hadoop configuration	context hadoop	0.090909
given value to scale decimal	sql	0.002525
basic operation test for dstream mappartitions	basic operation tests test map	0.333333
sample without replacement based	frame sample	0.066667
wait	reset	0.011236
accumulator with	accumulator init	0.083333
based on first value	based on key outputs	0.111111
the old hadoop	hadoop	0.050000
into a jvm seq	seq sc cols	0.055556
used again to wait	sql	0.002525
new vector with 1 0 bias appended to	mlutils append bias data	0.333333
the mean squared	mean squared	0.250000
into label	parse	0.071429
from start to end exclusive	spark context range start end	0.333333
rawpredictioncol	raw prediction col	1.000000
this instance contains a param with	param	0.012500
a python topicandpartition to map	topic partition	0.055556
to this accumulator's	core	0.003021
convert this matrix to a rowmatrix	mllib linalg indexed row matrix to row matrix	0.333333
for saving	ml mlwriter	0.200000
all mixture	mixture model predict	0.125000
starts at pos in	pos	0.022222
evaluates	evaluator evaluate dataset	0.285714
left singular	singular	0.015625
python rdd	rdd save	0.038462
or transform the rdd of document to rdd	hashing tf transform document	0.166667
submit and test a single	submit tests test single	1.000000
'x' to all mixture	mixture	0.052632
week number of a	sql weekofyear col	0.055556
formula=none featurescol="features",	featurescol	0.062500
of two columns of a dataframe	data frame corr col1	0.166667
for this test	ml other test	0.500000
obtaining a test statistic result	mllib stat test result	0.166667
columns are the left singular vectors	linalg singular	0.017544
precision-recall curve which is	binary logistic regression summary pr	0.083333
the content of the :class dataframe	sql data frame writer save	0.083333
generates python code for	code name doc	0.111111
this rdd and its recursive	core rdd	0.003460
to convert the java_model to a python model	model java_model	0.200000
dataframe	data frame to	0.250000
sets the accumulator's value	core accumulator value	0.045455
can be used again to wait for new	sql streaming query manager reset	0.011905
the length of a string or binary expression	sql length col	0.050000
[0 0 1 0] for feature selection by	mllib chi sq selector set	0.150000
training set given the current parameter	training	0.029412
outputcol	output col	0.500000
new :class dataframe replacing a value with	data frame replace to_replace	0.100000
class inherit documentation from its parents	mllib inherit	0.045455
outputformat api mapred package	outputformatclass	0.111111
pyspark sql types longtype	sql sqlcontext range start end step numpartitions	0.083333
return a jvm scala map from a	data frame jmap jm	0.111111
generate	logistic regression with sgdtests generate logistic input offset	1.000000
of active queries associated with	manager active	0.066667
collection function returns	sql size	1.000000
correlation of two	method	0.041667
sets	vectorizer set	1.000000
randomly generated uid	cross validator model	0.050000
comprised of vectors containing i i	mllib random rdds log normal	0.125000
fp-growth model that contains	fpgrowth train cls	0.200000
for new	streaming	0.005025
partial objects do not serialize correctly in	partial	0.076923
that	external merger object	0.032258
stream	sql data stream writer	0.041667
streamingcontext from checkpoint data or	get or	0.200000
stream query if this is not set it	data stream writer	0.041667
a multiclass classification model	classification model	0.166667
this instance contains a	ml param params	0.013699
of the :class dataframe as the specified table	data frame writer save as table	0.333333
this	model	0.011173
:class dataframe, using the given	sql data frame	0.005348
sorts this rdd which is	core rdd sort by	0.200000
to a	to	0.092308
create a multi-dimensional rollup for the current :class	frame rollup	0.055556
configure the	streaming	0.005025
a specific group matched by	str pattern idx	0.111111
udf with a function and	sql user defined function	0.083333
the cluster centers represented as a	mllib bisecting kmeans model cluster centers	0.083333
streamingcontext from checkpoint data or	context get or	0.200000
of nodes summed over all trees in the	nodes	0.037037
sets the spark session to use for loading	mlreader session sparksession	0.333333
spill	spill	0.192308
list of conditions and returns one	sql column otherwise	0.050000
:func awaitanytermination() can be used again	query	0.010753
a large dataset and an item approximately find	ml lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
set a java system property such	context set system property	1.000000
the soundex encoding	soundex	0.043478
infer schema from an rdd of row or	sql spark session infer schema rdd samplingratio	0.250000
compute the	mllib linalg coordinate	0.333333
create a new sparkcontext at	spark context init	0.083333
a multi-dimensional cube for the current	sql data frame cube	0.055556
2 ml params instances for the	params m1 m2	0.047619
the accumulator's value only usable in	accumulator value value	0.050000
four clusters	train	0.111111
computes the levenshtein distance of	sql levenshtein left	0.058824
__init__(self estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none)	init estimator estimatorparammaps evaluator numfolds	1.000000
checkpoint	checkpoint	0.375000
with another :class dataframe, using the given join	sql data frame join other	1.000000
wait for new terminations	manager	0.011236
initial value of	set initial	0.111111
given a java onevsrestmodel create and return	one vs rest model from java cls	0.200000
return a new dstream by applying reducebykey	streaming dstream reduce by key func	0.076923
residual degrees of freedom	summary residual degree of freedom	0.250000
norm	sparse vector norm	0.133333
levenshtein distance	levenshtein left	0.058824
gaussianmixture clustering	gaussian mixture	0.038462
an	mllib matrix	0.047619
a class inherit documentation	inherit doc cls	0.045455
memory for this	external merger object size	0.032258
string name	param params has	0.019231
evaluator	evaluator	1.000000
the :class dataframe using the specified columns so	sql data frame	0.005348
class inherit documentation	inherit	0.037037
submit and test a script with	core spark submit tests test	0.090909
a python rdd of key-value pairs	core rdd save as	0.037500
how much	object	0.027778
:py attr predictions which	generalized linear regression	0.090909
partial objects do	save partial obj	0.125000
a right outer join of c{self}	rdd full outer join	0.111111
create a new spark configuration	spark conf init loaddefaults _jvm	0.250000
range of offsets from a single kafka	range	0.030303
of the :class dataframe to a data source	sql data frame writer	0.011628
in this rdd	core rdd	0.006920
model by type	linear	0.025641
flag indicating whether or not leading whitespaces from	ignoreleadingwhitespace	1.000000
used again to	sql	0.002525
multiclass classification	data numclasses categoricalfeaturesinfo	0.250000
__init__(self formula=none featurescol="features",	rformula init formula featurescol	1.000000
contains a param with a	ml	0.001835
tests whether this	ml param params has param paramname	0.142857
>>> (maptype(stringtype(), integertype())	map type init keytype valuetype valuecontainsnull	1.000000
:func awaitanytermination() can be used	query manager	0.011905
storage level	storagelevel	0.200000
the norm of	linalg sparse vector norm	0.066667
a function on each rdd of this dstream	dstream transform func	0.500000
to this	core accumulator add	0.076923
instance contains a param with a	has param	0.019231
products	products	0.857143
in utc returns another timestamp that corresponds	sql from utc timestamp timestamp tz	0.166667
create a new spark configuration	spark conf init	0.250000
tests whether this	param params has param paramname	0.142857
used again to	streaming query	0.010526
broadcast	core spark context broadcast value	0.125000
return a javardd of object	core rdd	0.003460
table	table name	0.333333
of batches after which the centroids	timeunit	0.025641
checkpointinterval=10 losstype="logistic", maxiter=20	ml gbtclassifier	0.095238
vector	linalg vectors	0.666667
test the python direct kafka stream api	stream tests test kafka direct stream from	0.333333
set the selector type of the chisqselector	selector set selector type selectortype	0.333333
sets the accumulator's value only usable	accumulator value value	0.050000
set initial centers should be set before	set initial centers centers	0.200000
with	ml param params has param	0.019231
value of	ml param has step size	1.000000
rdd of key-value pairs (of form c{rdd[ k	rdd save	0.038462
cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none	ml random forest classifier	0.023256
brackets	brackets	0.352941
the precision-recall curve which is a dataframe	binary logistic regression summary pr	0.083333
param with a given string name	ml param params has	0.019231
accumulator's value only usable in	accumulator value	0.050000
to be used for later scaling	mllib standard scaler fit dataset	0.250000
terms to term frequency vectors or transform	mllib hashing tf transform	0.045455
that pulls events from flume	streaming flume	0.111111
of the :class dataframe to a data	sql data frame writer save path	0.142857
sorts this rdd	core rdd sort by	0.200000
combine the items by creator	merger merge values iterator	0.333333
threshold threshold in binary classification	threshold	0.018182
setparams(self labelcol="label", featurescol="features",	generalized linear regression set params labelcol featurescol	1.000000
this :class dataframe as pandas pandas	pandas	0.090909
stream api with start offset	stream from offset	0.500000
test python direct kafka rdd get	stream tests test kafka rdd get	1.000000
for data sampled from	data distname	0.083333
that all	core	0.003021
columns in an input dataframe from the	columns to	0.125000
converts vector	mllib mlutils convert vector	1.000000
again to wait for new	reset	0.011236
root mean squared error which	regression	0.010000
error which	regression	0.010000
can be used again	sql streaming query manager	0.011905
partial objects do not serialize correctly in	save partial	0.125000
or more examples	mllib decision tree model	0.076923
for the given user and	user	0.055556
returns a	session sql	0.250000
generate	mllib streaming logistic regression with sgdtests generate logistic	1.000000
test a single script on a	tests test single script on	0.500000
sum of	ml	0.001835
param belongs to	param	0.006250
sorted	sorted	1.000000
batches after which the centroids of	timeunit	0.025641
for multiclass	multiclass	0.071429
get or compute the	mllib linalg row matrix	0.250000
regression model on	regression with	0.200000
for	linear	0.025641
default min number of partitions for hadoop rdds	core spark context default min partitions	0.250000
of this instance	ml one	0.166667
count of the	rdd	0.003058
load a model from the given	svmmodel load cls sc	0.200000
random forest model	random forest model	1.000000
distinct count of	count distinct	0.080000
extract the day of the month of	sql dayofmonth	0.031250
frame	frame	0.172414
the left singular vectors of	linalg singular	0.017544
instance contains a param with a given string	param params	0.014925
number of nonzero elements this	linalg dense vector num	0.166667
vectors saved using	vectors	0.083333
specifies the behavior when data or	data frame writer mode savemode	0.071429
computes column-wise summary statistics for the input	mllib stat statistics col stats rdd	0.200000
fields threshold precision curve	ml binary logistic regression summary precision by threshold	0.166667
comprised of vectors	mllib random rdds poisson	0.125000
line in libsvm format into	parse libsvm line line	0.111111
of this dstream and other	other	0.033333
column names skipping	sql	0.005051
with this spark job on	core spark context	0.011628
= sc	get	0.021739
a field by name in a	field name	0.166667
that :func awaitanytermination() can	streaming query	0.010526
sql	sql sqlcontext	0.095238
first n elements	n	0.027778
use the model to make predictions on batches	algorithm predict on	0.066667
prediction a k a	prediction	0.041667
partitioned data into disks	core external group by spill	0.047619
out into	sql	0.002525
is an	mllib matrix	0.047619
a l{statcounter} object that captures the mean variance	rdd stats	0.083333
of columns	matrix cols	0.333333
a list of predicted ratings for	mllib matrix factorization model predict all user_product	0.050000
the most recent [[streamingqueryprogress]] updates for	recent progress	0.111111
a dataframe	data frame corr col1	0.166667
new dstream by applying reducebykey	streaming dstream reduce by key func numpartitions	0.076923
extract the minutes of a	sql minute	0.050000
of the accumulator's data type returning	accumulator	0.012987
the input schema	data frame reader schema schema	0.333333
a row-oriented distributed matrix with	row matrix	0.200000
bandwidth of each sample defaults to	bandwidth bandwidth	0.125000
create a multi-dimensional cube for the current :class	frame cube	0.055556
specified table	table tablename	0.333333
sort the list based on	test case sort result based on key	0.333333
__init__(self mintokenlength=1 gaps=true pattern="\s+", inputcol=none outputcol=none tolowercase=true)	regex tokenizer init mintokenlength gaps pattern inputcol	1.000000
returns	sql catalog	0.428571
to	manager reset	0.011905
the input stream	stream	0.017544
a model with weights	mllib streaming linear regression with	0.111111
compare	persistence test compare	0.166667
return the	scaler model	0.076923
a random	mllib random	1.000000
test predicted values on a toy	streaming logistic regression with sgdtests test predictions	0.500000
bround	bround	1.000000
into an rdd of labeledpoint	mlutils load lib svmfile sc path	0.125000
checkpointed and materialized either reliably	checkpointed	0.083333
stopwords=none casesensitive=false) sets	stop words remover set	0.200000
storage type for	type	0.024390
model to make predictions on batches of data	linear algorithm predict on	0.066667
of memory for	external merger	0.031250
test the python direct kafka stream api with	kafka stream tests test kafka direct stream	0.125000
of object by	ml	0.001835
a	sql spark	0.125000
mathfunction	mathfunction	1.000000
create a new profiler using class	collector new profiler	0.333333
libsvm	libsvm	0.454545
whether this instance is of type distributedldamodel	ml ldamodel is	0.066667
indexing categorical feature columns in a	indexer	0.055556
only create a new hivecontext for testing	context create for testing cls sparkcontext	0.333333
contains the count of distinct	count	0.016949
submit and test a script with a	core spark submit tests test	0.090909
this matrix to an indexedrowmatrix	linalg coordinate matrix to indexed row matrix	0.333333
that :func awaitanytermination() can be used again to	sql streaming query manager reset	0.011905
later than the value of	dayofweek	0.037037
a python topicandpartition	topic partition	0.055556
computes the levenshtein distance of the	levenshtein left	0.058824
can be used again	reset	0.011236
vector representation of each word in vocabulary	word2vec fit	0.200000
year of a given date as	sql year col	0.050000
list of conditions and returns one of multiple	sql column otherwise	0.050000
parammap into a python parammap	param map from	0.250000
means 1 leaf	mllib decision	0.125000
rdd 'x' to all mixture components	gaussian mixture model predict soft x	0.142857
accumulator's data type returning a	accumulator	0.012987
params instances for the	params m1 m2	0.047619
much	merger	0.025641
can	query	0.010753
points from the population	mllib stat kernel density	0.066667
of values	mllib linalg dense vector values	0.200000
csv file and	csv path schema sep	0.166667
instance contains a param	param params	0.014925
the underlying :class sparkcontext	spark session	0.100000
the spark sink deployed on	maxbatchsize	0.037037
matrix columns in an input dataframe from the	matrix columns to	0.142857
set number of batches after which the	set	0.005917
the libsvm format into an rdd of labeledpoint	mllib mlutils load lib svmfile	0.125000
function types	function obj name	0.500000
2 range 4	mllib linalg dense matrix repr	0.142857
that param type	param type	0.333333
be used again to wait for new terminations	query manager	0.011905
number of	distributed matrix num	0.500000
a term	accumulator add term	0.066667
a param with a given string	params has param	0.019231
how much of memory for	external merger object	0.032258
of predicted ratings for input	mllib matrix factorization model predict all user_product	0.050000
add a py or	context add py file path	0.166667
instance is	ldamodel is	0.200000
sparse vector using either a	linalg vectors sparse	0.166667
return the column standard deviation values	mllib standard scaler model std	0.166667
wait	query manager	0.011905
list of	list	0.200000
learningdecay	learning decay	1.000000
creates	session create	0.058824
:class dataframe to a data	sql data frame writer save path format	0.142857
__init__(self featurescol="features",	ml aftsurvival regression init featurescol	1.000000
dump already	core external group by spill	0.047619
points saved using rdd	points sc path minpartitions	0.250000
using string representations	compressioncodecclass	0.111111
even if users construct taskcontext instead of using	core task context new cls	0.333333
is an array	mllib	0.010526
with arbitrary key and value class from	core spark context	0.023256
batches	linear algorithm	0.076923
choose one directory for spill by	core external merger get spill dir	0.500000
degrees	linear regression summary degrees	1.000000
one	core	0.003021
at pos in byte and is of	pos	0.022222
to this	accumulator add	0.076923
selectortype	selectortype	0.625000
model	mllib tree ensemble model	0.058824
two separate arrays of indices and	ml	0.001835
predictions which gives the predicted	generalized linear regression summary prediction col	0.250000
multi-dimensional cube for the current :class dataframe using	sql data frame cube	0.055556
given a large dataset and an item	nearest neighbors dataset key numnearestneighbors	0.333333
the underlying output	data stream writer	0.041667
value of	ml param has elastic	1.000000
given parameters in this grid to	builder add grid	0.100000
the values for each key using an associative	key func numpartitions	0.066667
instance contains a param with a given	ml param params	0.013699
__init__(self featurescol="features", labelcol="label",	ml linear regression init featurescol labelcol	1.000000
fit test of	test	0.015152
in "predictions" which gives the true label	linear regression summary label col	0.333333
__init__(self scalingvec=none inputcol=none outputcol=none)	elementwise product init scalingvec inputcol outputcol	1.000000
the contents of the :class dataframe	frame writer save	0.066667
all the jobs started by	job	0.023810
make class generated by namedtuple picklable	hack namedtuple cls	1.000000
deserialized objects from	serializer load	0.083333
compute the dot product of two vectors we	dot other	0.050000
set number	streaming kmeans set	0.142857
partial objects do not serialize correctly in python2	cloud pickler save partial	0.125000
vectors which	vector indexer	0.200000
converts matrix columns in an input dataframe	mllib mlutils convert matrix columns from ml dataset	0.166667
their nearest center for this model	model	0.011173
gets a param	ml param params get param	1.000000
accumulator's data type returning	accumulator	0.012987
the mean variance	core	0.003021
returns precision or precision for a given label	metrics precision label	1.000000
summed over all trees in the ensemble	tree ensemble	0.111111
numeric	numeric	1.000000
stream query if this is not	sql data stream writer	0.041667
gets	get estimator	1.000000
columns	matrix cols	0.333333
a value to an int	to int	0.250000
offsetrange of	ranges	0.090909
property	property key value	0.125000
a csv file	csv path schema sep encoding	0.333333
changes the uid	param params reset uid newuid	0.333333
create a	context create	0.083333
for each key using an associative	key func numpartitions	0.066667
profiles	profiles	0.416667
:class dataframe from an :class rdd, a	schema samplingratio verifyschema	0.029412
return a jvm scala map from	data frame jmap jm	0.111111
from this thread such as the spark fair	core spark	0.010309
forget about past terminated queries	manager reset terminated	0.200000
a new dstream in	streaming streaming	0.047619
broadcast a	core spark context broadcast	0.125000
the rdd as non-persistent and remove	core rdd unpersist	0.066667
queries so that :func awaitanytermination() can	streaming query	0.010526
of names of tables in	sqlcontext table names	0.066667
the group id	group groupid	0.142857
each point in rdd 'x' to all mixture	mixture	0.052632
rdd partitioned	rdd partition by	0.062500
version of this dataset checkpointing can be	frame checkpoint eager	0.071429
stages	stages value	1.000000
:class dataframe to a data source	sql data frame writer	0.011628
tests whether	param paramname	0.111111
check	check	1.000000
receiver has been started	receiver started receiverstarted	0.500000
web	web	1.000000
vector columns in	vector columns to ml dataset	0.142857
decomposition	decomposition	0.833333
seq of columns that describes	cols cols kwargs	0.090909
kolmogorov	kolmogorov	1.000000
pipeline	pipeline	0.473684
after position pos	pos	0.022222
generates an rdd comprised of vectors	random rdds gamma vector rdd sc	0.200000
the values for each key using	by key func numpartitions partitionfunc	0.066667
the :class dataframe in	data frame	0.010000
train the model on	regression with sgd train on	0.333333
create a converter to drop the names of	create converter datatype	0.166667
"predictions" which gives the probability of each	ml logistic regression summary probability col	0.166667
code for a	code	0.071429
a single script file calling a global function	script with local functions	0.125000
by	bucket by	0.200000
greatest value of the list of	sql greatest	0.055556
properties	properties	1.000000
this distributed model	ml distributed ldamodel	0.050000
into	external group by	0.045455
greatest value of	greatest	0.043478
converts matrix columns in an input dataframe from	mlutils convert matrix columns to ml	0.166667
original column	original	0.047619
wait	streaming streaming context await termination or timeout timeout	0.125000
until any of the queries on the associated	streaming query manager await any termination	0.142857
string	ml param params has param	0.019231
columns that make up each	block matrix cols per	0.333333
elements	core spark context	0.011628
a new vector with 1 0 bias appended	mllib mlutils append bias data	0.333333
note : experimental	kmeans summary	1.000000
java onevsrest create and return	one vs rest from java	0.142857
queries so that	query manager	0.011905
list based on	based on	0.111111
this instance contains	param params	0.014925
a column scipy matrix from a dictionary of	mllib sci py tests scipy matrix	0.090909
be used again to wait for new	sql streaming query manager	0.011905
partial objects do not serialize correctly	pickler save partial	0.125000
to make predictions on batches of data	linear algorithm predict on	0.066667
queries so that :func awaitanytermination() can be	streaming query	0.010526
in rdd 'x' to all mixture components	gaussian mixture model predict soft	0.142857
false	false	1.000000
the least value of	sql least	0.055556
the context	streaming context	0.055556
represents an entry	matrix entry	0.250000
the stream query if	stream writer	0.041667
mean squared error which	regression	0.010000
transforms	java params transfer param	0.250000
value of	ml polynomial	1.000000
of the list of column names skipping	sql	0.005051
of	object	0.027778
the embedded	ml param params	0.013699
set the selector type of the chisqselector	mllib chi sq selector set selector type selectortype	0.333333
accumulator's data type returning	accumulator param	0.038462
a list of active queries associated with this	manager active	0.066667
stores	alsmodel	0.142857
the	core	0.012085
thresholds thresholds in multi-class classification to adjust the	thresholds	0.071429
results immediately to	locally	0.083333
note : experimental	core rdd count approx distinct	1.000000
on the driver returns none	on driver	0.333333
for each key using a	by key	0.026316
create a new dstream in	streaming streaming	0.047619
levenshtein distance of the two	levenshtein	0.045455
make predictions on a	streaming kmeans predict on	0.500000
the underlying output	sql data stream writer	0.041667
later than the value of the date	date dayofweek	0.333333
cluster centers represented as a	bisecting kmeans model cluster centers	0.095238
value decomposition svd factors	value decomposition	0.200000
save an isotonicregressionmodel	mllib isotonic regression model save sc path	1.000000
:class dataframe in json format (json lines	sql data frame	0.005348
the given parameters in this grid to fixed	grid builder base	0.076923
list of active queries associated with	manager active	0.066667
as a temporary table	as table df	0.250000
of columns that describes the sort order	data frame sort cols cols kwargs	0.142857
calculates the correlation of two columns of a	col1 col2 method	0.055556
validates the ownership	param params resolve	0.333333
containing only the elements that satisfy a predicate	filter f	0.500000
of	ml clustering	0.100000
creates a table based on the dataset in	sql catalog create external table tablename path	0.250000
sets	thresholds set	1.000000
of the observed	observed	0.058824
converts a	to	0.007692
parameters in this grid	builder add grid	0.100000
the norm	sparse vector norm p	0.133333
string	ml param	0.009524
__init__(self k=none inputcol=none outputcol=none)	pca init k inputcol outputcol	1.000000
number	mllib linalg row matrix num	0.062500
wait for the execution to stop	await termination or timeout timeout	0.125000
active queries associated with this sqlcontext	manager active	0.066667
dot product of	dense vector dot other	0.050000
wait for	streaming	0.005025
returns an active	sql streaming	0.010204
__init__(self featurescol="features", labelcol="label", predictioncol="prediction",	init featurescol labelcol predictioncol	0.833333
parameters passed as a list	core spark conf	0.055556
ndcg value of	ndcg	0.100000
for each key	by key func numpartitions partitionfunc	0.066667
queries so that :func awaitanytermination()	manager	0.011236
second is an	mllib	0.010526
each of the points belongs to	predict x	0.033898
checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	random forest classifier	0.022727
load a model from	loader load cls sc	0.250000
initial value	regression with sgd set initial	0.111111
converts matrix columns	mlutils convert matrix columns to	0.166667
the input dataset this is called	dataset	0.020408
value for each original column during fitting	ml min max scaler model original	0.062500
removes all cached tables from the in-memory cache	sqlcontext clear cache	1.000000
that :func awaitanytermination()	manager	0.011236
all nodes or any hadoop-supported file system	file path	0.035714
paired rdd where the first	matrix factorization model	0.043478
value to an int	to int	0.250000
external table based on the dataset	external table tablename path	0.090909
embedded params to the companion java object	transfer params to java	0.500000
url of the	url	0.076923
comprised of vectors containing i i d samples	random rdds poisson vector	0.125000
extract the year of a given date as	year col	0.050000
test that coefs are predicted accurately by	with tests test parameter accuracy	0.333333
which the centroids	timeunit	0.025641
session to use	session	0.050000
the trigger for the stream query if this	sql data stream writer trigger	0.083333
estimate of the importance of each feature	ml decision tree classification model feature importances	0.250000
again to wait for	streaming query	0.010526
of memory	object size	0.032258
paired rdd	factorization	0.038462
partial objects do not serialize	core cloud pickler save partial	0.125000
this instance contains a param with a	params	0.006623
active queries associated with this sqlcontext >>>	manager active	0.066667
dump the profile stats into directory	collector dump profiles	1.000000
given string name	params has	0.019231
stages=none)	stages	0.125000
a private abstract class representing a multiclass classification	classification	0.071429
accessible via jdbc	jdbc	0.142857
sets	validation split set	1.000000
which is a dataframe	logistic regression summary	0.045455
the given parameters in this grid to	builder add grid	0.100000
set a java system property such as spark	core spark context set system property cls key	1.000000
be used	query manager reset	0.011905
load a model from	mllib svmmodel load cls sc	0.200000
which is assumed to consist of	ascending numpartitions keyfunc	0.100000
l{statcounter} object that captures	core rdd stats	0.083333
a local property set in this thread	core spark context get local property key	0.066667
called when processing of a batch of	listener on batch	0.333333
log likelihood of	ml ldamodel log likelihood dataset	0.166667
content of the :class dataframe	data frame writer	0.084507
inputformat with arbitrary key	inputformatclass keyclass	0.125000
get all values as a list of	core spark conf get all	0.166667
data type json	parse datatype json	0.333333
computes hex value of the given	hex col	0.166667
of the rdd partitioned using the	rdd partition	0.062500
until any of the queries on	manager await any	0.142857
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6	regression init featurescol labelcol predictioncol	0.333333
setparams(self min=0 0 max=1 0	min max scaler set params min max	1.000000
another timestamp that corresponds to	timestamp timestamp tz	0.166667
of fit test of the	test	0.015152
treeensemblemodel	tree ensemble model	0.038462
:class rdd, a list or a :class pandas	schema samplingratio verifyschema	0.029412
weighted	weighted	0.833333
the weights for each tree	ensemble model tree weights	0.333333
rdd is generated by applying mappartitionswithindex() to	map partitions with index f preservespartitioning	0.055556
sample covariance for the given columns specified	sql data frame	0.005348
norm	mllib linalg sparse vector norm	0.083333
for each training	summary	0.024390
this instance with a randomly generated uid and	one vs	0.125000
hivecontext for testing	for testing	0.333333
list of tables/views	catalog list tables	0.250000
given a java pipelinemodel create	pipeline model from java cls	0.200000
so that :func	sql streaming query	0.011765
dstream in which each rdd	by	0.014286
deviance for the null	summary null deviance	0.250000
observed tokens in the training set	ldamodel training	0.034483
partitioned data into disks	core external group by	0.045455
the configured value for some key or	conf get key defaultvalue	0.333333
transforms a python parammap into a java	java params transfer param map to java	1.000000
content of the :class dataframe in parquet	data frame writer parquet	0.200000
termination of	termination timeout	0.041667
rdd of int containing elements	core spark context	0.011628
parquet	parquet	0.466667
mindocfreq	mindocfreq	0.833333
__init__(self k=none inputcol=none outputcol=none)	ml pca init k inputcol outputcol	1.000000
a param	param	0.018750
singular vectors of	mllib linalg singular	0.035088
set pipeline stages	ml pipeline set stages	1.000000
vector of 64-bit floats from a python	ml linalg vectors	0.250000
of the :class dataframe to a data	sql data frame writer save path format	0.142857
a column containing a json string into a	sql from json col	0.083333
test predicted values on a toy	mllib streaming logistic regression with sgdtests test	0.111111
returns a paired	factorization model	0.043478
rdd's elements in one	rdd	0.003058
+= operator adds a term to	core accumulator iadd term	0.142857
k decayfactor	streaming	0.005025
of freedom for the null	of freedom null	0.500000
convert this distributed model	ml distributed ldamodel	0.050000
active queries associated with this	sql streaming query manager active	0.066667
java ml instance	java mlreader java	0.333333
given	has	0.011628
levenshtein distance	sql levenshtein left	0.058824
for local checkpointing using spark's existing caching layer	local checkpoint	0.500000
known as min-max normalization or rescaling	min	0.041667
sets	naive bayes set	1.000000
set each dstreams in this context	streaming streaming context	0.032258
destroy	destroy	0.555556
in this grid to fixed	builder add grid param	0.250000
count of the	core rdd	0.003460
kinesis stream name	streamname	1.000000
active queries associated with	sql streaming query manager active	0.066667
given a java pipeline create and	pipeline from java cls	0.200000
model	tree model	0.026316
the accumulator's	accumulator param	0.038462
of memory for this	merger object	0.032258
an rdd comprised of	random rdds normal vector rdd	0.166667
of all params with their optionally	ml param params explain params	0.166667
format into an rdd of labeledpoint	mllib mlutils load lib svmfile sc path	0.125000
a python rdd of key-value pairs	rdd save as	0.038462
computes the area	metrics area	0.333333
params to the wrapped java object	java params to	0.045455
number of	vector num	0.181818
add a py or zip dependency	add py file	0.166667
save a linearregressionmodel	regression model save sc path	1.000000
data into disks	core external	0.016129
__init__(self	cross validator init	1.000000
so that	sql	0.002525
which predictions are known	ml isotonic regression model	0.125000
can be	streaming query manager reset	0.011905
list of	ml chi sq	0.100000
labeledpoint	load lib svmfile	0.125000
evaluates	ml evaluator evaluate dataset	0.285714
explained	explained	0.636364
the mean variance and count of the rdd's	core	0.003021
deviance for the	regression summary deviance	0.125000
and count of the rdd's elements in one	core rdd	0.003460
start to end exclusive increased by	start end	0.090909
distance from a sparsevector or 1-dimensional numpy array	distance other	0.133333
using the model	regression model	0.031250
columns in	columns from ml	0.125000
labeledpoint	mllib mlutils load lib svmfile sc path numfeatures	0.125000
of predicted ratings for	mllib matrix factorization model predict all user_product	0.050000
the norm	vector norm	0.166667
removes the specified table from the	sql catalog uncache table tablename	0.250000
get pipeline	ml pipeline get	1.000000
the residual degrees	summary residual degree	0.500000
basic operation test for dstream mapvalues	streaming basic operation tests test	0.111111
add a py or zip	core spark context add py file path	0.166667
calculates the norm of a	vector norm p	0.055556
convert a dict	sql to	0.041667
row-oriented distributed matrix with indexed rows	indexed row matrix	0.333333
another timestamp that corresponds to the same	timestamp timestamp tz	0.166667
trunc	trunc	0.714286
with this spark	core spark context	0.011628
impurity="variance", seed=none variancecol=none)	regressor	0.043478
sql context to use for loading	mlreader context sqlcontext	0.333333
active queries associated with	query manager active	0.066667
with	ml param params	0.013699
the contents of the :class dataframe to a	frame writer save path format	0.066667
randomly generated	validation split	0.200000
a sparse vector using either	mllib linalg vectors sparse	0.166667
dump already partitioned data into	external group by	0.045455
iadd	iadd	0.714286
densevector with singular	singular	0.015625
comprised of	random rdds normal vector	0.125000
as	sql row as	1.000000
for this obj assume that all the objects	external merger object size obj	0.040000
vector	mllib linalg sparse vector	0.111111
given parameters in this grid to	ml param grid builder	0.055556
defined on the class to current	ml param	0.009524
number of partitions	partitions	0.066667
create a new hivecontext for testing	create for testing cls sparkcontext	0.333333
set the initial value	logistic regression with sgd set initial	0.111111
content of the :class dataframe in json format	sql data frame	0.005348
value of	ml param has aggregation	1.000000
of this instance	ml java	0.076923
are the right singular vectors of the singularvaluedecomposition	singular value decomposition v	0.250000
left outer join	rdd left outer join other	0.111111
set the selector type of	selector type	0.100000
of the non-streaming :class dataframe out into	sql data frame write	0.071429
of characters as a hexadecimal number	unhex col	0.142857
contains a param with a given string name	param params has param	0.019231
by :func query stop() or by an	sql streaming query await	0.333333
:class dataframe with	data frame	0.005000
multi-dimensional cube for the current :class dataframe	data frame cube	0.055556
this vector to the new mllib-local representation	linalg sparse vector as	0.333333
it can be used in sql statements	f returntype	0.125000
dataframe from a	from	0.045455
an associative and commutative reduce	core rdd reduce by	0.125000
create a sparse vector using either a dictionary	mllib linalg vectors sparse size	0.166667
loads vectors saved using rdd[vector] saveastextfile	mllib mlutils load vectors	1.000000
represents singular value	singular value	0.500000
:py attr relativeerror	relative error value	1.000000
frame boundaries defined from start	start	0.045455
of the month of a given date	sql dayofmonth col	0.031250
and predicting	streaming	0.005025
of	ml linalg dense	0.200000
get or compute the number of rows	linalg block matrix num rows	0.200000
of the rdd's	core rdd	0.003460
terminations	sql streaming query	0.011765
finding frequent items	frame freq items cols support	0.166667
awaitanytermination() can be used again to wait	streaming	0.005025
first n	n	0.027778
model with	with	0.055556
returns	mllib multiclass metrics	1.000000
the java_model to a python model	model java_model	0.200000
approximately find at most k items which have	ml lshmodel approx	0.125000
parses the expression string into the column that	expr	0.076923
the month of	sql dayofmonth	0.031250
spark sink deployed on a	maxbatchsize	0.037037
job	job	0.142857
for this udt	user defined type	0.250000
training set given	distributed ldamodel training	0.034483
:class dataframe representing the result of the given	sqlquery	0.027027
predict	predict	0.241379
the content of the :class dataframe to the	data frame writer	0.014085
contains	param	0.012500
read a 'new api' hadoop	context new apihadoop	0.333333
temporary	temp	0.250000
a value to an int	to int value	0.250000
the value of the date	next day date	0.100000
of functions registered	functions	0.071429
performs the kolmogorov-smirnov ks test for data sampled	mllib stat statistics kolmogorov smirnov test data distname	0.111111
objective function scaled loss + regularization at	ml linear regression training summary objective history	0.500000
sort the list based on	spark streaming test case sort result based on	0.333333
fits a model to	estimator fit	0.166667
:py attr predictioncol	prediction col value	1.000000
losstype	loss type	1.000000
1 leaf	decision	0.052632
sql context to use	context sqlcontext	0.083333
maxiter=100 tol=1e-6 seed=none	maxiter	0.083333
of tree (e	tree model	0.026316
get or compute the number of rows	mllib linalg row matrix num rows	0.200000
of the rdd partitioned using	rdd partition	0.062500
load a model from the given path	mllib matrix factorization model load cls sc path	1.000000
which is later than the value of the	dayofweek	0.037037
summary of model	tree ensemble model repr	1.000000
this instance contains a	ml param params has param	0.019231
of columns that describes the sort	sql data frame sort cols cols kwargs	0.142857
join of	join other numpartitions	0.142857
wait until any of the	streaming query manager await any termination timeout	0.166667
deviance for the null	generalized linear regression summary null deviance	0.250000
specifies the input data	sql data stream reader	0.250000
wait for	query	0.010753
accumulator's value only	accumulator value value	0.050000
of conditions and returns one of multiple possible	sql column otherwise	0.050000
ml params instances for	params m1	0.047619
much of	core external merger	0.032258
model	mllib naive bayes model	1.000000
set a local property that affects jobs	set local property key value	0.200000
write() save	one vs rest save	1.000000
trigger for the stream	stream writer trigger	0.083333
ml params instances for the given param	params m1 m2	0.047619
generates an rdd comprised of i i	random rdds uniform rdd sc size numpartitions seed	0.200000
convert a value to a boolean if possible	ml param type converters to boolean value	0.250000
residuals mse r-squared	linear regression	0.040000
loads a text	text	0.076923
all	size	0.009174
of the points belongs to in this model	mllib kmeans model predict x	0.333333
seed=none): sets params for	set params	0.034483
count of the rdd's elements	core rdd	0.003460
l{statcounter} object that captures	rdd stats	0.083333
ndcg value of all	metrics ndcg	0.200000
stream query if this is not set it	data stream	0.028571
calculates the length of a string	sql length col	0.050000
a right outer join of c{self}	full outer join other	0.111111
the names of fields in obj	sql	0.002525
and product	product	0.029412
sort	data frame sort	0.125000
of this matrix	matrix	0.015152
until any of the queries on the	query manager await any	0.142857
adds an input option for	reader option key	0.500000
the residual degrees of freedom for the	generalized linear regression summary residual degree of freedom	0.125000
the kmeans algorithm for fitting and predicting	streaming kmeans	0.035714
the week number of a given	sql weekofyear	0.055556
to consist of key value	key ascending numpartitions keyfunc	0.071429
time for a condition to	condition	0.045455
a paired rdd	matrix factorization	0.040000
sql datum into a user-type object	sql user defined type deserialize datum	0.333333
goodness of fit	mllib stat statistics	0.125000
checkpointinterval=10 impurity="variance", seed=none variancecol=none)	tree regressor	0.058824
for a condition to	condition	0.045455
a specific topic and partition	topic and partition	0.111111
returns the explained variance	explained variance	0.333333
test predicted values on a toy model	regression with sgdtests test	0.111111
task	task	1.000000
converts vector columns in an input dataframe	mllib mlutils convert vector columns from	0.166667
queries	query manager reset	0.011905
rdd messagehandler	rdd message handler	1.000000
applies standardization transformation on a	scaler model transform	0.500000
even if users construct taskcontext instead of	task context new	0.333333
cost sum of squared distances	cost x	0.142857
of vectors which	ml vector indexer model	0.250000
waits for the termination of this query either	termination	0.035714
the selector type of	sq selector set selector type	0.111111
point in rdd 'x' to all mixture components	mixture model predict soft x	0.142857
__init__(self threshold=0 0	binarizer init threshold	1.000000
linear model that has a	linear model	0.066667
linkpredictioncol	link prediction col	1.000000
words closest	word2vec	0.052632
given parameters in this grid	grid builder base	0.076923
in rdd 'x' has maximum membership in	mllib gaussian mixture	0.045455
awaitanytermination()	manager reset	0.011905
mixin for param predictioncol prediction column name	has prediction col	1.000000
new	query manager reset	0.011905
prints the first n rows to the console	sql data frame show n truncate vertical	0.333333
returns micro-averaged label-based f1-measure	mllib multilabel metrics micro f1measure	1.000000
objects from	serializer load stream	0.250000
which is defined as the	regression	0.010000
an rdd comprised	mllib random rdds exponential vector rdd	0.166667
a labeledpoint to a string in libsvm format	labeled point to libsvm p	0.500000
the selector type of the chisqselector	selector set selector type selectortype	0.333333
find the n largest elements in a dataset	core nlargest n iterable key	0.333333
a new dstream by applying reducebykey	streaming dstream reduce by	0.076923
comprised of i i d	mllib random rdds	0.041667
contains	has	0.011628
param with a	ml param params has param	0.019231
this distributed model to	ml distributed ldamodel to	0.166667
as non-persistent	unpersist	0.083333
foreach	foreach	0.833333
generates an rdd comprised of	random rdds normal vector rdd sc	0.200000
in this grid	param grid builder add grid	0.100000
a new spark configuration	spark conf init loaddefaults	0.250000
a python rdd of key-value pairs (of	core rdd	0.010381
:class column for distinct count	count distinct col	0.040000
create a multi-dimensional cube for the current	sql data frame cube	0.055556
attr predictions which gives the predicted value of	ml generalized linear regression summary prediction	0.333333
return a l{statcounter} object that captures	rdd stats	0.083333
cost	cost	0.263158
instance contains a param with a	ml	0.001835
squared distance from a sparsevector	squared distance other	0.153846
much of memory for	size	0.009174
contents of the :class dataframe to a	frame	0.034483
params instances for the given param and assert	params	0.006623
return the topics described	describe topics maxtermspertopic	0.333333
dot product of two	ml linalg dense vector dot	0.090909
kmeans algorithm for fitting and predicting on incoming	streaming kmeans	0.035714
ordered list of	ml string	0.333333
whether this instance is of	ml ldamodel is distributed	0.066667
a value to a mllib vector	to vector	0.250000
parameters in this grid	ml param grid builder	0.055556
the model to make predictions on batches	linear algorithm predict on	0.066667
ml instance the default	mlreader	0.037037
bucket	bucket	1.000000
"zerovalue" which may be added to the result	rdd fold by	0.125000
for the termination of this	termination timeout	0.041667
error which is defined as	linear regression summary	0.013889
outputformat api	dataset conf keyconverter valueconverter	0.500000
of the :class dataframe to a	frame writer save path format	0.066667
again	sql streaming query manager reset	0.011905
sparsematrix	sparse	0.125000
for the stream query if	stream writer	0.041667
cachenodeids=false checkpointinterval=10 impurity="gini",	classifier	0.050000
the levenshtein distance of the	levenshtein	0.045455
external	core external	0.016129
the soundex encoding for a string >>>	sql soundex	0.055556
convert the java_model to a python model	discretizer create model java_model	0.250000
make predictions on batches	linear algorithm predict on	0.066667
min value for each original column during	min max scaler model original min	0.250000
of indices and	ml	0.003670
set multiple parameters passed as a list	spark conf set all	0.125000
sql storage type for	sql type cls	0.250000
observed tokens in the training	distributed ldamodel training	0.034483
date	date	0.333333
forget about past terminated queries	reset terminated	0.200000
training set given the current	distributed ldamodel training	0.034483
of the :class dataframe to a data	sql data frame writer save	0.083333
exclusive ending offset	untiloffset	1.000000
names into a jvm seq	seq sc cols converter	0.055556
used again to wait	sql streaming query manager reset	0.011905
converts matrix columns in an input	convert matrix columns to ml dataset	0.166667
if using checkpointing	checkpoint	0.062500
resulting rdd that contains a	core rdd cogroup	0.066667
group	group	0.230769
rdd >>> rdd =	rdd get	0.200000
transforms the input dataset with optional parameters	transform dataset params	1.000000
setparams(self	split set params	1.000000
list of index value pairs or two	linalg	0.044444
associative and commutative reduce function but	reduce	0.041667
to the new mllib-local representation	as	0.148148
returns the explained variance regression score	mllib regression metrics explained variance	0.333333
to a string in libsvm format	to libsvm p	1.000000
python direct kafka stream api	kafka direct stream	0.111111
containing a json string into a [[structtype]]	json	0.043478
wait for	streaming query manager	0.011236
items for columns possibly with false positives	items cols support	0.125000
random forest	random forest	0.083333
object by pyrolite whenever	object rdd rdd	0.500000
until any of the queries on the	sql streaming query manager await any termination	0.142857
configured value for some key or	conf get key defaultvalue	0.333333
set initial centers should	streaming kmeans set initial centers centers	0.200000
to wait	sql	0.002525
__init__(self	bisecting kmeans init	1.000000
get the cluster centers represented as a list	mllib bisecting kmeans model cluster centers	0.083333
calculates the correlation of two columns	col2 method	0.055556
find synonyms of a	find synonyms	0.333333
chain	chain	1.000000
featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1	featurescol labelcol predictioncol probabilitycol	0.333333
norm of a sparsevector	sparse vector norm p	0.066667
keytype	keytype	1.000000
precision curve	ml binary logistic regression summary precision by	1.000000
broadcast a read-only variable to the cluster returning	spark context broadcast	0.125000
the expected value of	ml	0.001835
memory for this	core	0.003021
parses a line in libsvm format into	mllib mlutils parse libsvm line line	0.111111
generate	regression with sgdtests generate	1.000000
predictions which gives the predicted	linear regression summary prediction	0.142857
configuration contain a given key?	conf contains key	0.333333
new dstream by applying reducebykey to	streaming dstream reduce by key func	0.076923
how much of memory for this	core external	0.016129
rdd previously saved	minpartitions	0.071429
returns the	metrics	0.041667
the underlying output data source	sql data frame writer format source	0.333333
of features corresponding to that	features	0.043478
how data of	data stream	0.028571
python rdd	core rdd save	0.037975
test	stream tests test	0.562500
vectors	vector	0.038462
the levenshtein distance	levenshtein	0.045455
object by pyrolite whenever the	object rdd	0.500000
range 4	dense matrix repr	0.142857
loads vectors saved	load vectors sc	0.333333
awaitanytermination() can be	streaming query manager	0.011236
inherit documentation from its parents	mllib inherit	0.045455
number of top	num top	1.000000
as the specified	save as	0.333333
outputformat api mapred package	outputformatclass keyclass	0.250000
much	external	0.013889
the cluster centers represented as a list of	kmeans model cluster centers	0.060606
active stages	core status tracker get active	0.333333
mean squared error which is defined	regression	0.010000
converts vector columns in an input dataframe	mlutils convert vector columns from ml	0.166667
or compute the number of rows	linalg indexed row matrix num rows	0.200000
__init__(self estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none)	ml cross validator init estimator estimatorparammaps evaluator numfolds	1.000000
validator	validator	1.000000
partial objects do not serialize correctly	core cloud pickler save partial	0.125000
mean variance and count of the	core rdd	0.003460
new spark	core spark	0.010309
to select filter	chi sq selector model selected features	0.333333
param with a given string name	has	0.011628
which is a risk	mllib regression	0.045455
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10	init featurescol maxiter seed	0.250000
year of	year col	0.050000
singular vectors of	singular	0.031250
set bandwidth	stat kernel density set bandwidth bandwidth	0.142857
set number of batches	kmeans set	0.090909
sets	discretizer set	1.000000
get depth of tree (e	tree model	0.026316
an rdd of labeledpoint	load lib svmfile sc path numfeatures	0.125000
precision-recall curve which is a	binary logistic regression summary pr	0.083333
saves the content of the dataframe in	data frame	0.005000
of the rdd partitioned	rdd partition	0.062500
apply a function to each rdd in this	foreach rdd func	1.000000
convert this matrix to the new mllib-local representation	mllib linalg dense matrix as	0.333333
with a given string name	ml param	0.009524
in iterator	iterator	0.100000
converting raw prediction scores into 0/1 predictions	mllib linear classification model	0.142857
:py attr rawpredictioncol	raw prediction col value	1.000000
save this rdd as	core rdd save as	0.012500
a java udf	java	0.012195
applying a function to each element	map f preservespartitioning	0.200000
dot product of two vectors we	dense vector dot other	0.050000
sets window size default 5	set window size windowsize	1.000000
value in c{self} that is not contained	rdd subtract other numpartitions	0.111111
for the pearson correlation coefficient for col1 and	corr col1	0.333333
fp-growth model	fpgrowth train cls data	0.200000
and date2	date2	0.166667
number of training iterations	ml generalized linear regression training summary num iterations	1.000000
multiple parameters passed as a list of	spark conf set all	0.125000
deviance for the null	regression summary null deviance	0.250000
minimum number of times	min count	0.076923
loads a class generated by namedtuple	core load namedtuple name fields	0.333333
:class dataframe as the	sql data frame writer save as	0.071429
set multiple parameters passed as	conf	0.050000
the index of the original	map partitions with index	0.100000
convert this matrix to an indexedrowmatrix	linalg coordinate matrix to indexed row matrix	0.333333
:class column	data frame getitem item	0.250000
create a new spark configuration	spark conf	0.058824
maps a column of indices back	index	0.041667
setparams(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6	regression set params featurescol labelcol predictioncol	0.250000
name of the file to which	file	0.028571
number of nonzero	sparse vector num	0.200000
test that the model params are set correctly	test test model params	0.250000
compute the dot product of two vectors	mllib linalg dense vector dot other	0.058824
sparse vector using either a dictionary a list	linalg vectors sparse size	0.166667
data type json string	datatype json string	0.333333
spark_user for user who is running sparkcontext	core spark context spark user	0.250000
model to make predictions on the	predict on	0.058824
of weights	weights initialweights	0.333333
model from the input java	ml java estimator	0.200000
wait	sql streaming	0.010204
nonzeros	nonzeros	0.625000
values for each key using an associative and	by key	0.026316
param	param param	0.100000
gaussians in mixture	gaussian mixture model	0.052632
matrix	mllib linalg dense matrix	0.083333
objective function scaled loss + regularization at each	ml linear regression training summary objective history	0.500000
matching	matching	0.666667
vector columns in an input dataframe from	vector columns to ml dataset	0.142857
active queries associated with this sqlcontext >>> sq	streaming query manager active	0.066667
new dstream in which	streaming streaming context transform	0.066667
first argument-based logarithm of	log arg1 arg2	0.200000
methods to set k decayfactor	streaming	0.005025
makes a class inherit documentation from its	inherit doc	0.045455
forget about past terminated queries so	manager reset terminated	0.200000
stream query if this is not	stream writer	0.041667
string currently only "norm" is supported	distname	0.166667
:class dataframe to a data source	sql data frame writer save path	0.142857
new rdd of int containing elements	core spark context	0.011628
the index	map partitions with index	0.100000
adds	reader	0.040000
install	install	1.000000
numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)	random forest	0.041667
awaitanytermination() can be	manager	0.011236
the spark	core spark context	0.011628
pipeline create and return a python wrapper	pipeline from	0.142857
return the model	rdd	0.003058
setparams(self inputcol=none outputcol=none labels=none) sets params for this	to string set params inputcol outputcol labels	0.333333
returns the greatest value of the	sql greatest	0.055556
convert this vector to the	sparse vector	0.062500
the function func	func	0.125000
correlation of	col2 method	0.055556
that makes a class inherit documentation	mllib inherit	0.045455
awaitanytermination()	sql streaming query manager	0.011905
already	group by	0.041667
the length of	sql length	0.050000
perform a left outer	left outer	0.333333
:class dataframe, using	data frame	0.005000
data	data frame	0.015000
month of a	dayofmonth	0.027027
byte array that starts at pos in byte	pos	0.022222
add a py or zip dependency for all	context add py file	0.166667
use	context	0.045455
month of a	sql dayofmonth	0.031250
first n rows to the console	show n truncate vertical	0.333333
the returned	py spark streaming test case	0.333333
days	days	1.000000
computes column-wise summary statistics for the input rdd[vector]	mllib stat statistics col stats	0.200000
the spark sink	storagelevel maxbatchsize	0.045455
gradient-boosted	gradient boosted	1.000000
rdd partitioned	rdd	0.003058
contents of the :class dataframe to	frame writer save path format	0.066667
list based on first value	based on key outputs	0.111111
dot product of two	mllib linalg dense vector dot	0.058824
memory	core external merger	0.032258
comprised of vectors containing i i d samples	mllib random rdds gamma	0.125000
sets	linear regression set	1.000000
calculates the correlation of two	col2 method	0.055556
resulting rdd that contains	core rdd cogroup other	0.066667
uncache	uncache	1.000000
instance contains a param with a given string	ml param params has	0.019231
number of products for all users the	products for users num	0.200000
and the second is an	mllib matrix	0.047619
for fitting and predicting on incoming	streaming	0.005025
add	context add	0.500000
converts a labeledpoint to a	mllib mlutils convert labeled point to	0.250000
compute the number	linalg block matrix num	0.100000
already partitioned data into	by spill	0.047619
densematrix	linalg block	0.076923
params instances	params m1 m2	0.047619
initcap	initcap	1.000000
a python parammap into	to	0.007692
the specified partitioner	partition by numpartitions partitionfunc	0.250000
dump already partitioned data into	core external group	0.045455
test that	linear regression with tests test	1.000000
function to the value of each key-value pairs	f	0.021053
choose one	external sorter get	1.000000
get or compute the number	indexed row matrix num	0.100000
dense	dense	0.666667
into an rdd of labeledpoint	load lib svmfile sc path numfeatures minpartitions	0.125000
which each rdd contains the	by	0.014286
note : experimental	spark context binary files path minpartitions	1.000000
cleared	description interruptoncancel	0.166667
basic operation test for dstream groupbykey	streaming basic operation tests test group by	1.000000
input	data frame reader	0.166667
dstream by applying 'full	full	0.066667
lda keeplastcheckpoint is set	ldamodel	0.034483
the accumulator's value	accumulator value	0.050000
comprised of vectors containing i i	random rdds exponential vector	0.125000
local	context get local	0.333333
compute the number of rows	linalg row matrix num rows	0.200000
rdd of labeledpoint	lib svmfile sc path	0.125000
'x' has maximum membership in this model	mllib gaussian mixture model predict	0.100000
even if users construct taskcontext instead	core task context	0.100000
setparams(self	ml lda set params	1.000000
object or none if the stage info	stage info	0.142857
partial objects do not serialize correctly in	cloud pickler save partial obj	0.125000
return the bisecting	mllib bisecting	0.333333
converts matrix columns in an	mlutils convert matrix columns from ml dataset	0.166667
columns are the left singular vectors of	linalg singular	0.017544
removes the specified table from the in-memory cache	sqlcontext uncache table tablename	0.250000
a python rdd	rdd save	0.038462
the number of	mllib linalg row matrix num	0.062500
__init__(self withmean=false withstd=true inputcol=none outputcol=none)	init withmean withstd inputcol outputcol	1.000000
category if specified	mllib multiclass	0.285714
sets the given parameters in this grid to	ml param grid builder	0.055556
how much	external merger	0.031250
densematrix	ml linalg matrices dense numrows numcols values	0.333333
local property set in this thread or	core spark context get local property key	0.066667
to be downloaded	recursive	0.125000
for new	streaming query	0.010526
on	logistic regression with	0.250000
:py attr regparam	reg param value	1.000000
forget about past terminated queries so	terminated	0.100000
set sample points from	set sample sample	0.333333
finding frequent items for columns possibly with	data frame freq items cols support	0.166667
load a lassomodel	mllib lasso model load cls sc path	1.000000
name	params has param	0.019231
set number of batches	streaming kmeans set	0.142857
columns are the right singular vectors of the	singular	0.015625
a param with a given	params	0.006623
stores item factors in two columns id and	ml alsmodel item factors	1.000000
hadoop configuration which is passed in as a	context hadoop	0.090909
get or compute	linalg distributed matrix	0.333333
to data	data	0.011628
and name or provide a new	core	0.003021
model improves on toy data with no	logistic regression with sgdtests	0.200000
bayes	bayes	0.750000
cost sum of squared distances of points to	cost x	0.142857
this query that	streaming query	0.010526
covariance for the given columns specified	sql data frame	0.005348
test predicted values on a toy	with sgdtests test predictions	0.500000
numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1 0)	random forest	0.041667
sets the sql context to use for loading	mlreader context sqlcontext	0.333333
deserialize an object from a byte array	core framed serializer loads obj	1.000000
the distributed matrix on the driver as	block matrix to local matrix	0.250000
with the keys	keys	0.111111
or names into a jvm seq of	seq sc	0.055556
of a streaming dataframe/dataset is written	stream writer output mode outputmode	0.083333
load labeled	load labeled	1.000000
applying a function on each rdd of	transform func	0.058824
a temporary table	table df tablename	0.083333
stop() or by	await	0.166667
list of active queries associated with this sqlcontext	manager active	0.066667
contains a param with a	ml param params	0.013699
of two vectors we support	ml linalg	0.030303
vector columns in an input	vector columns to	0.142857
in csv format at the specified path	csv path mode compression sep	1.000000
compute	linalg row	0.500000
theta	theta	1.000000
filename	filename	1.000000
parameters in this grid to fixed values	grid builder	0.055556
note : experimental	train validation split	0.166667
a list of active queries associated with	query manager active	0.066667
calculates the norm	dense vector norm	0.333333
category	multiclass	0.214286
predicted values on a toy	streaming logistic regression with sgdtests	0.200000
variance and count of the rdd's	rdd	0.003058
offsets from a	offset	0.021739
a new accumulator with a	accumulator init aid	0.083333
the deviance for	ml generalized linear regression summary deviance	0.125000
elements in one operation	rdd	0.003058
input data source format	data stream reader format source	0.333333
model	linear regression model	0.133333
deviation	model std	0.500000
which gives the	regression summary	0.035714
until any of the queries on	manager await any termination	0.142857
number of training iterations until termination	ml linear regression training summary total iterations	0.500000
invalidate and refresh all the cached the	hive context refresh	0.200000
a :class dataframe	data frame	0.035000
rdd of	rdd save as	0.038462
into an rdd of labeledpoint	mllib mlutils load lib svmfile sc path numfeatures	0.125000
k classes classification	classes	0.034483
for	external merger object	0.032258
this rdd which is assumed to	core rdd	0.003460
train a random forest model	mllib random forest train	0.250000
0 seed=none numtrees=20	ml random forest	0.071429
selector type	selector type	0.100000
the underlying output	writer	0.040000
fast	heappushpop	0.166667
rdd generated in this dstream	streaming dstream	0.027778
mlwriter for :py class	mlwriter	0.062500
computes column-wise summary statistics for the input	stat statistics col stats	0.200000
into an rdd of labeledpoint	lib svmfile sc path	0.125000
load a model from the	svmmodel load cls sc	0.200000
an exception if any error is	mllib	0.010526
load a	linear regression model load cls	1.000000
:class column for approximate distinct count of col	approx count distinct	0.071429
be used again	manager	0.011236
this	core external merger object size	0.032258
return the column standard	mllib standard scaler	0.100000
the count of distinct	count	0.016949
'full	full	0.066667
runs and	core	0.003021
parse string representation back into the	mllib linalg dense vector parse s	0.333333
load a java model	java loader load java cls sc	0.200000
the number	num	0.042017
load labeled points	load labeled points sc path	1.000000
add a py	core spark context add py	0.166667
computes the levenshtein distance	sql levenshtein	0.058824
represents a specific topic and partition for kafka	topic and partition	0.111111
same param	m1 m2 param	0.125000
the dataframe in a	data frame writer	0.014085
value of	ml multiclass classification	0.500000
impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none	ml random forest classifier	0.023256
of nonzero	ml linalg	0.030303
prefix of string in	prefix	0.083333
"predictions" which gives the true label of each	ml linear regression summary label	0.333333
dataframe that with new specified column	data frame to df	0.090909
until any of the	query manager await any	0.142857
computation	binary classification metrics unpersist	1.000000
resets the configuration property for the given key	sql runtime config unset key	1.000000
column from one base to another	col frombase tobase	0.166667
that :func	manager reset	0.011905
set bandwidth	mllib stat kernel density set bandwidth bandwidth	0.142857
is set to a different	context set	0.125000
norm of a	sparse vector norm	0.066667
tables/views in the specified database	tables dbname	0.500000
the accumulator's data type returning a new	accumulator param	0.038462
param with a given	ml	0.001835
of tables/views in the	tables	0.071429
merges them with extra values from	map extra	0.040000
perform a right	full	0.066667
fdr	set fdr fdr	1.000000
the maximum item	max key	0.333333
square root of	root	0.035714
tree models	tree model	0.026316
of this dstream	dstream	0.031250
this instance with a randomly generated uid and	one vs rest	0.034483
queries so that	sql streaming	0.010204
return an rdd with the keys of	core rdd keys	0.333333
resets the configuration property for the	sql runtime config unset	0.142857
values	mllib standard scaler	0.100000
of features corresponding to	features	0.043478
copy of the rdd partitioned using the	rdd partition	0.062500
for the stream query	data stream	0.028571
accuracy equals to the total number of correctly	accuracy	0.076923
each original column during fitting	ml min max scaler model original	0.062500
the probability	probability	0.100000
minutes of a given date	minute col	0.050000
given columns on the file system	sql data frame	0.005348
rdd by	rdd key by	1.000000
contains	params	0.006623
weights	weights	0.533333
an input option	stream reader option	0.333333
the given parameters in this grid	grid builder add grid	0.100000
partitioned data into	external group	0.045455
completed	completed batchcompleted	0.250000
classification model trained using multinomial/binary logistic regression	logistic regression model	0.083333
returns an mlwriter instance for this ml instance	ml mlwritable write	0.200000
as the	writer save as	0.333333
which the centroids of that particular batch has	timeunit	0.025641
add a file to be	add file path	0.333333
from checkpoint data or	get or	0.200000
predicted	prediction	0.083333
awaitanytermination() can be used again to	reset	0.011236
queries so	query	0.010753
extract the minutes of a given date as	minute	0.040000
the mean	mean	0.172414
content of the non-streaming :class dataframe out	sql data frame write	0.071429
return the column mean	mean	0.034483
of jobs has started	started batchstarted	0.250000
variance and count of the rdd's elements in	core	0.003021
the current [[dataframe]] and perform the	pivot pivot_col values	0.050000
points using the model trained	tree ensemble model	0.038462
a sparse vector using either a	vectors sparse	0.166667
accumulator with	accumulator init aid	0.083333
java udf so it can	java	0.012195
:class pyspark.sql.types.structtype object	schema	0.033333
levenshtein distance of the	levenshtein	0.045455
mincount the minimum number of times a	min count mincount	0.200000
support __transient__ on	cloud pickler save reduce func args state listitems	0.111111
returns one	sql	0.002525
the output	writer	0.080000
returns	multilabel metrics	0.500000
a multi-dimensional rollup	frame rollup	0.055556
returns weighted true positive rate	mllib multiclass metrics weighted true positive rate	1.000000
boundaries from start	between start	0.100000
converts matrix columns	convert matrix columns to	0.166667
a multi-dimensional cube for	data frame cube	0.055556
stream query if this is not set	data stream writer	0.041667
for each original column during	scaler model original	0.062500
the :class dataframe to a	frame writer	0.050000
model	create model	1.000000
create an input stream	utils create stream	0.200000
number of	row matrix num	0.100000
set a local property that affects jobs submitted	set local property key value	0.200000
values for each key using an	by key func numpartitions partitionfunc	0.066667
the day of the month of	dayofmonth col	0.031250
curve	binary classification	0.500000
load a linearregressionmodel	linear regression model load cls sc path	1.000000
the specified table	table tablename	0.166667
pipelinemodel used for ml	pipeline model	0.071429
of the month of a given	dayofmonth	0.027027
for each numeric	grouped data avg	0.058824
cluster centers represented as a list of	cluster centers	0.060606
returns a paired rdd	matrix factorization model	0.043478
column scipy matrix from	sci py tests scipy matrix size	0.090909
vectors containing i i d samples drawn	numrows numcols numpartitions	0.125000
frequency vectors or transform the rdd of	hashing tf transform	0.045455
instance's params to the wrapped java object	params to	0.035714
local property that affects jobs submitted from this	local property key value	0.076923
least value of the list	least	0.043478
buffer to array	to array array_like dtype	0.166667
rdd of key-value pairs (of form c{rdd[ k	rdd save as	0.038462
an input stream that is to be used	stream ssc addresses	0.166667
changes the uid of this instance	ml param params reset uid	0.058824
or compute the number of	mllib linalg indexed row matrix num	0.125000
lowerbound	lowerbound	1.000000
column mean	model mean	0.125000
labeledpoint	mlutils load lib svmfile sc path numfeatures minpartitions	0.125000
inherit documentation from	inherit doc	0.045455
the embedded	params	0.006623
how	core external merger object size	0.032258
all the objects	core	0.003021
on the log	log	0.071429
partitioned	group by spill	0.047619
specifies how data	sql data stream	0.031250
pipelinemodel create and return a	pipeline model from	0.142857
sql storage type for	type sql type cls	0.250000
point in rdd 'x' to all mixture	mllib gaussian mixture model	0.062500
number of columns of	num	0.008403
instance contains a param with a given	has param	0.019231
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20	random forest classifier	0.022727
columns are the right singular vectors	linalg singular	0.017544
wait for	sql streaming query manager	0.011905
:class dataframe using the	sql data frame	0.005348
attr lda keeplastcheckpoint is	ml distributed ldamodel get	0.066667
transforms a java parammap into a python parammap	java params transfer param map from java javaparammap	1.000000
simple sparse vector class for passing data to	sparse vector	0.062500
min value for each original column during	scaler model original min	0.250000
values for each numeric columns for each	sql grouped data avg	0.058824
with	params has param	0.019231
of memory for this obj assume that	core external merger object size obj	0.040000
find norm of the given vector	vectors norm vector	1.000000
int	int	0.714286
pretty printing of a	str	0.090909
the embedded params to the companion	java params transfer params to	0.333333
for each key using an	by key func numpartitions	0.062500
spark fair scheduler pool	core spark	0.010309
accumulator's value only usable in driver program	core accumulator value value	0.050000
root mean squared error which is	linear regression summary	0.013889
instance contains a param with	ml	0.001835
weights computed for every	weights	0.066667
python topicandpartition to map to the java related	topic partition	0.055556
value of the given column which could be	col	0.016393
field in :py attr predictions which gives	linear regression summary	0.013889
wait for the execution	context await termination or timeout timeout	0.125000
error which is defined	linear regression summary	0.013889
set bandwidth of each sample	density set bandwidth bandwidth	0.142857
that makes a class inherit documentation from its	inherit doc cls	0.045455
add a py or zip dependency	spark context add py file path	0.166667
setparams(self featurescol="features", predictioncol="prediction", k=2	ml gaussian mixture set params featurescol predictioncol k	1.000000
a line in libsvm format into label indices	mlutils parse libsvm line line multiclass	0.111111
sets the context to	context	0.022727
string format equals	string s	0.333333
content of the dataframe in a text	data frame writer text	0.200000
test that the final value	regression with sgdtests test	0.111111
2 ml params instances for	params m1 m2	0.047619
create an input stream	utils create stream ssc hostname port	0.200000
return the	model	0.005587
the soundex encoding	sql soundex col	0.055556
attr lda	ml distributed ldamodel get	0.066667
sets the threshold that	set threshold value	0.500000
get the cluster	model cluster	0.333333
disks	core	0.003021
to consist of key value pairs	key ascending numpartitions keyfunc	0.071429
perceptron	perceptron	1.000000
table in the catalog	table df	0.083333
for each numeric columns for	grouped	0.071429
containing the ids of	stage ids	0.055556
creates a	sql spark session create	0.142857
finding frequent	data frame freq	1.000000
represents a range of offsets from a single	range	0.030303
mean squared error which	linear regression summary	0.013889
by applying a function to	map f	0.074074
text file stream and	stream reader text path	0.333333
norm of a sparsevector	norm	0.041667
set the initial	mllib streaming logistic regression with sgd set initial	0.111111
this instance with a randomly generated	one vs rest	0.034483
outcomes for k classes classification problem	classes	0.034483
line in libsvm format into label	parse libsvm line line	0.111111
dataframe produced by	clustering	0.066667
with leaders	with leaders	1.000000
on a model with weights already set	linear regression with	0.111111
co	co	1.000000
test a single script file calling a	tests test script with local functions	0.333333
substring	substring index	1.000000
transfer this instance's params to the wrapped	java params to	0.045455
associated with this streamingcontext	context	0.022727
model improves on toy data with no of	regression with sgdtests	0.200000
the new hadoop outputformat api mapreduce	save as new apihadoop dataset conf keyconverter valueconverter	0.142857
converts vector columns in an input dataframe from	convert vector columns to	0.166667
compute	linalg coordinate	0.333333
specification that defines the	spec	0.076923
resulting rdd that contains a tuple	core rdd cogroup	0.066667
comprised of vectors containing i	mllib random rdds	0.083333
cluster that each of the points belongs	predict x	0.033898
python rdd of key-value pairs (of form c{rdd[	core rdd save as	0.037500
adds an input option	stream reader option	0.333333
jvm seq	seq sc	0.055556
return a resulting rdd that contains	core rdd cogroup other	0.066667
until any of the queries on	streaming query manager await any	0.142857
an external list for	external list	0.166667
converts vector columns	mllib mlutils convert vector columns	0.166667
minutes of a	minute col	0.050000
data type json	datatype json	0.333333
:class dataframe replacing a value with another	data frame replace to_replace	0.100000
value of	ml param has variance col	1.000000
note this docstring is not shown publicly	linalg row matrix init rows numrows numcols	0.333333
with the spark sink	addresses storagelevel maxbatchsize	0.045455
cluster url to connect to	master	0.100000
sets the context	context	0.022727
converts a	mllib mlutils convert	0.166667
norm of	mllib linalg sparse vector norm	0.083333
much of memory for this obj assume	merger object size obj	0.040000
initial value of weights	regression with sgd set initial weights initialweights	0.333333
offsets from a single kafka topicandpartition	offset	0.021739
creates a model from the input java	ml java estimator create	1.000000
variancecol	variance col	0.500000
represents singular value decomposition svd factors	singular value decomposition	0.166667
generated by applying mappartitionswithindex() to	map partitions with index f preservespartitioning	0.055556
:class dataframe representing the	sql data frame	0.005348
the schema	schema	0.033333
array that starts at pos	pos	0.022222
a param and validates	param param	0.100000
queries so that :func	streaming query	0.010526
variance	core rdd	0.003460
ml instance	ml java mlreader	0.200000
this obj assume that all the objects	merger object size obj	0.040000
paired rdd where the	matrix factorization	0.040000
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor",	ml aftsurvival regression init featurescol labelcol predictioncol	0.500000
sliding window of time over	window	0.037037
so	sql streaming	0.010204
first n rows to the console	show n	0.333333
get or compute the number of	mllib linalg coordinate matrix num	0.166667
large dataset and an item approximately find	lshmodel approx nearest neighbors dataset key numnearestneighbors distcol	0.166667
contains a param with	ml param params has param	0.019231
least the master	master	0.100000
error which	linear regression summary	0.013889
submit and test a single script on a	spark submit tests test single script on	0.500000
an	clustering train cls	1.000000
for the given user and product	user product	0.250000
lead	lead	1.000000
stages	core status tracker get	0.500000
embedded params to the companion java object	params transfer params to java	0.500000
new dstream by applying 'full	full	0.066667
user_product	user_product	1.000000
handleinvalid	handleinvalid	0.555556
setparams(self	ml train validation split set params	1.000000
a java parammap into a python parammap	param map from java javaparammap	0.500000
week number of a given date as integer	sql weekofyear col	0.055556
:class windowspec with the frame boundaries	sql window range between	0.166667
converts vector columns in an input dataframe to	mllib mlutils convert vector columns	0.083333
rdd contains the count of	count	0.016949
:class dataframe with an alias set	data frame alias alias	0.500000
old hadoop outputformat api mapred	save as hadoop dataset conf keyconverter valueconverter	0.083333
of memory for this obj assume that	object size obj	0.040000
initial value of weights	set initial weights	0.333333
a param with a given	ml	0.001835
a streamingcontext from checkpoint data or create a	get or create cls	0.200000
tokens in the training set	ldamodel training	0.034483
format (json lines text format or newline-delimited json	writer json	0.125000
python topicandpartition to map to the java related	streaming topic and partition init topic partition	0.055556
a new spark configuration	core spark conf init loaddefaults	0.250000
:func awaitanytermination() can be used again to wait	query manager reset	0.011905
broadcast a read-only variable to	core spark context broadcast value	0.125000
or compute the number of rows	row matrix num rows	0.200000
a new dstream by applying reducebykey	streaming dstream reduce by key	0.076923
outputcol=none handleinvalid="error")	handleinvalid	0.444444
saves	save path format mode partitionby	0.200000
binarizer	binarizer	1.000000
get or compute the number of	mllib linalg distributed matrix num	0.166667
this instance	ml	0.001835
can be used again to wait for	manager	0.011236
dump already partitioned data	external group by spill	0.047619
test for data sampled from a continuous	test data distname	0.166667
dot product of two	dense vector dot	0.050000
converts vector columns	mllib mlutils convert vector columns from ml	0.166667
converts matrix columns in an input dataframe	convert matrix columns to	0.166667
to use	spark context	0.023256
the test method	mllib stat chi sq test result method	0.250000
this instance contains a param	ml	0.001835
:class dataframe whose schema starts	data	0.011628
daemon and workers terminate on sigterm	core daemon tests test termination sigterm	0.333333
defined	param	0.006250
udf	udf	0.714286
add a py or zip dependency for	core spark context add py file path	0.166667
trigger for the stream query if this is	stream writer trigger	0.083333
kolmogorov-smirnov ks test for data sampled	mllib stat statistics kolmogorov smirnov test data distname	0.111111
zips this rdd	core rdd zip	1.000000
set	kmeans set	0.090909
create a column scipy matrix from a	mllib sci py tests scipy matrix	0.090909
setparams(self	count vectorizer set params	1.000000
comprised	mllib random rdds normal	0.125000
kolmogorov-smirnov ks test for data	mllib stat statistics kolmogorov smirnov test data	0.111111
of this instance with	ml java	0.076923
a global temporary view with this	global temp view name	0.500000
threshold if any used for	threshold	0.018182
converts vector columns in an input dataframe	mllib mlutils convert vector columns from ml dataset	0.166667
model	logistic regression	0.040000
a linear regression model	linear regression	0.040000
a python parammap into a	param map to	0.125000
l{statcounter} object that captures the	core rdd stats	0.083333
generates an rdd comprised of vectors containing i	mllib random rdds uniform vector rdd sc	0.200000
form an rdd using xrange	core spark context parallelize c numslices	0.250000
rdd an rdd of	power iteration clustering train cls rdd	0.250000
create a new hivecontext for testing	context create for testing cls sparkcontext	0.333333
lsh	lsh	1.000000
stream api	stream from	0.250000
given parameters in this grid to fixed	ml param grid builder add grid param	0.250000
the input	data frame reader	0.166667
str around pattern pattern is a	str pattern	0.250000
the ids of all active stages	tracker get active stage ids	0.250000
that	accumulator	0.012987
fitting and predicting on	streaming	0.005025
:func awaitanytermination() can be used again to wait	query	0.010753
dstreams in this	streaming	0.005025
dump already partitioned	core external	0.016129
dependent variable given a vector or an	linear regression model base	0.142857
in this context	streaming streaming context	0.032258
that :func awaitanytermination() can be used again to	streaming	0.005025
a new accumulator	accumulator	0.012987
full	full	0.333333
conv	conv	1.000000
decoder	decoder	0.714286
__init__(self featurescol="features",	decision tree classifier init featurescol	1.000000
number of	linalg indexed row matrix num	0.100000
system using the old hadoop outputformat api mapred	hadoop dataset conf keyconverter valueconverter	0.083333
subtract	subtract	0.555556
maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0	maxdepth	0.100000
create a new hivecontext for testing	create for testing cls	0.333333
perform a right outer	rdd full outer	0.333333
or multiclass	classifier cls data numclasses	0.250000
with optional parameters	params	0.019868
that :func awaitanytermination() can be used again	query manager	0.011905
save this model to the	bayes model save	0.500000
convert python object	obj	0.047619
if observed is vector conduct pearson's chi-squared	chi sq	0.111111
clean up all the files	merger cleanup	0.333333
broadcast	spark context broadcast	0.125000
join of c{self}	join other numpartitions	0.142857
the centroids of	timeunit	0.025641
training	distributed ldamodel training	0.034483
a range of offsets from a	offset range	0.047619
setparams(self predictioncol="prediction", labelcol="label", metricname="f1") sets params	set params predictioncol labelcol metricname	1.000000
value of	ml param decision tree params	1.000000
internal function to get or create global	get or create cls	0.200000
convert this vector	mllib linalg sparse vector	0.111111
index	partitions with index	0.100000
given string	param params	0.014925
dataframe as	sql data frame corr	0.166667
for this obj assume that all the objects	size obj	0.040000
alias for na fill()	sql data frame fillna value subset	0.166667
a configuration	core spark conf	0.111111
model fitted by :py class countvectorizer	count vectorizer model	1.000000
of this instance this	ml param params reset	0.166667
absolute path of a file added through	core spark files	0.125000
columns	columns to ml	0.125000
fast version of	core heappushpop heap item	0.142857
be used again to wait for new terminations	sql streaming query	0.011765
receive accumulator updates	update	0.055556
class conditional probabilities	naive bayes model theta	1.000000
a python rdd of key-value pairs (of	rdd	0.009174
the soundex encoding for a	soundex col	0.055556
this	core rdd	0.003460
cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto",	random forest classifier	0.022727
the minutes of	minute col	0.050000
items for columns possibly with false positives using	items cols support	0.125000
params instances for the given param and assert	params m1 m2	0.047619
table	table df	0.083333
an rdd ordered in ascending	core rdd take ordered	0.050000
gives the	summary	0.024390
so	sql streaming query	0.011765
create a column scipy matrix from	mllib sci py tests scipy matrix size	0.090909
which	isotonic regression	0.090909
to a string in libsvm format	to libsvm	1.000000
set to a	set	0.005917
the	mllib standard	0.125000
version of a heappush	heap item	0.125000
:py attr predictions	linear	0.025641
test the python direct kafka stream foreachrdd	kafka stream tests test kafka direct stream foreach	1.000000
transforms the embedded params to the companion	ml java params transfer params to	0.333333
return an iterator of	core	0.006042
internal use only create a	sql hive context create	0.083333
minimum number	min count	0.076923
residual degrees of freedom for	linear regression summary residual degree of freedom	0.125000
either by :func	sql streaming	0.010204
distinct count of col	count distinct	0.080000
set initial centers should be set before calling	streaming kmeans set initial centers centers	0.200000
much of	core external	0.016129
value for each original column during	max scaler model original	0.062500
local property that affects jobs	local property key	0.035714
arbitrary key and value	core	0.006042
be used again to wait	reset	0.011236
elasticnetparam	elastic net param	1.000000
batches of	linear algorithm	0.076923
which each rdd	by	0.014286
folds	folds	1.000000
arr	arr	0.833333
in the ensemble	ensemble	0.100000
dataframe from a list or pandas dataframe	from local data schema	0.333333
vector columns in an input	vector columns from ml	0.142857
array that starts at pos in byte and	pos	0.022222
of corresponding string values	string	0.041667
squared distance from a sparsevector or 1-dimensional	sparse vector squared distance	0.166667
the soundex encoding for a string >>>	soundex	0.043478
value of	ml tree regressor params	1.000000
broadcast a read-only variable	context broadcast value	0.125000
cov	cov	1.000000
maxcategories=20	maxcategories	0.166667
cluster centers represented as a	model cluster centers	0.090909
dot product	linalg dense vector dot other	0.058824
like '#,--#,--# --', rounded	number col	0.500000
the schema of	schema	0.033333
given	ml	0.001835
in sql statements	f returntype	0.125000
of tree (e g depth 0	tree model	0.026316
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online", learningoffset=1024	set params featurescol maxiter seed	0.250000
soundex encoding for a string >>> df	sql soundex	0.055556
close to the	parameter accuracy	0.029412
the position of the first occurrence of substr	substr str	0.125000
set the initial	streaming logistic regression with sgd set initial	0.111111
set initial centers should	set initial centers centers weights	0.200000
save this model to the given	logistic regression model save sc	1.000000
create a densematrix	linalg matrices dense numrows numcols values	0.250000
by	scaler	0.052632
inserts the content	writer insert into	0.333333
params	params formula	0.500000
squared distance from a	sparse vector squared distance	0.166667
list for list	list	0.066667
the given parameters in this grid	ml param grid builder add grid	0.100000
:py attr standardization	standardization value	1.000000
the dispatch to handle all	core cloud pickler save	0.166667
:py attr fpr	fpr value	1.000000
listitems	listitems	1.000000
convert this matrix to the	mllib linalg matrix	0.333333
batches of	mllib streaming linear algorithm	0.166667
python code for a shared param class	param gen param code name doc	0.333333
watermark	watermark	1.000000
receiver has reported an error	receiver error receivererror	0.500000
:class dataframe	session	0.050000
a l{statcounter} object that captures the mean	stats	0.055556
an rdd ordered in ascending order	core rdd take ordered	0.050000
term frequency vectors or	mllib hashing tf	0.125000
a function and	function	0.027778
ordering columns	order by	0.142857
converts vector columns in an input dataframe to	convert vector columns from ml	0.166667
query was terminated by an exception or none	streaming query exception	0.500000
using the old hadoop	save as hadoop	0.142857
a :class dataframe representing the	sqlquery	0.027027
to wait	streaming query manager	0.011236
indices to select filter	model selected features	0.333333
__init__(self formula=none featurescol="features", labelcol="label", forceindexlabel=false)	ml rformula init formula featurescol labelcol forceindexlabel	1.000000
creates a table based on the dataset	sql catalog create external table tablename path	0.250000
for	key	0.071429
ndcg	ndcg	0.500000
correlation of two columns	corr col1 col2 method	0.055556
a class inherit documentation from its parents	inherit doc	0.045455
compute the standard deviation of this rdd's elements	stdev	0.047619
this instance with a randomly	cross validator	0.045455
of a dataframe	data frame corr	0.166667
to the given path a shortcut of write()	ml mlwritable	0.142857
setparams(self	mixture set params	1.000000
an object is	obj identifier	0.333333
:class dataframe out into external	sql data frame	0.005348
returns micro-averaged label-based recall	mllib multilabel metrics micro recall	1.000000
term	add term	0.066667
:class dataframe to a data source	sql data frame writer save path format	0.142857
python	from	0.045455
seq of columns that describes the sort order	sql data frame sort cols cols kwargs	0.142857
sets the given parameters in this grid to	builder add grid	0.100000
day of the month	dayofmonth col	0.031250
create a column scipy matrix from a	sci py tests scipy matrix size	0.090909
vectors saved	vectors sc path	0.333333
this distributed model to a	distributed ldamodel to	0.166667
length	sql length	0.050000
transforms the embedded params to the companion	java params transfer params to	0.333333
or compute	linalg row matrix	0.200000
vectors	vector indexer	0.200000
basic operation test for dstream countbyvalue	streaming basic operation tests test count by	1.000000
the :class dataframe in json format (json	sql data frame writer	0.011628
the right singular vectors of the singularvaluedecomposition	linalg singular value decomposition v	0.250000
underlying output	frame writer format	0.333333
string value that match regexp	sql regexp	0.125000
correlation of two columns of	method	0.041667
test that coefs are predicted accurately	regression with tests test parameter accuracy	0.333333
the kolmogorov-smirnov ks	mllib stat statistics kolmogorov smirnov	0.333333
queries so that :func awaitanytermination() can be	streaming query manager	0.011236
wait for the	or timeout timeout	0.125000
python rdd of key-value pairs (of	core rdd	0.010381
sgdtests	sgdtests	1.000000
this instance with a	one vs rest	0.034483
decorator that makes a class inherit documentation	inherit doc cls	0.045455
compute the dot product of two	dense vector dot other	0.050000
that each of the points belongs	predict x	0.033898
degrees	summary degrees	1.000000
forget about past terminated queries so that	query manager reset terminated	0.200000
data source	format source	0.666667
predicts rating for	mllib matrix factorization model predict	0.250000
get	spark conf get	1.000000
a	core	0.030211
a	mllib	0.042105
this matrix to a coordinatematrix	indexed row matrix to coordinate matrix	0.333333
return	rdd	0.009174
to their vector representations	mllib word2vec model get vectors	0.166667
list of terms to term frequency vectors or	mllib hashing tf	0.125000
operation test for dstream filter	operation tests test filter	1.000000
create a new rdd of int containing elements	core spark	0.010309
into java	ml py2java sc	1.000000
converts matrix columns in an input dataframe to	mlutils convert matrix columns from ml	0.166667
compute the dot product of two vectors we	dense vector dot other	0.050000
a class inherit documentation from its	inherit doc cls	0.045455
using the given join expression	join	0.034483
__init__(self	ml bisecting kmeans init	1.000000
densematrix >>> dm = densematrix(2 2 range 4	linalg dense matrix repr	0.142857
all partitioned	external merger external	1.000000
by codeblock co	cls co	0.333333
evaluates a list of conditions and	column otherwise	0.200000
defined on the class to current	param	0.006250
the greatest value	sql greatest	0.055556
save	regression model save	0.500000
new java	java wrapper new java	0.166667
compute	linalg	0.044444
ndcg value of all the	ndcg	0.100000
names into a jvm seq of	seq sc cols	0.055556
standard deviation of	core rdd stdev	0.066667
__init__(self	ml max abs scaler init	1.000000
the offsetrange of	ranges	0.090909
setparams(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6	ml aftsurvival regression set params featurescol labelcol predictioncol	0.500000
this instance to a java pipelinemodel used	ml pipeline model to java	0.100000
python function	f	0.010526
column containing a json string into a [[structtype]]	from json col	0.083333
objects	size	0.009174
gets	params get estimator param	1.000000
this instance contains a param with	ml param params	0.013699
parammap	param map	0.500000
new sparkcontext at least	spark context init	0.083333
value of	ml one hot encoder	1.000000
first argument-based logarithm	log arg1 arg2	0.200000
returns the explained variance regression score	linear regression summary explained variance	0.333333
sets	reg param set	1.000000
and count of the rdd's elements in one	core	0.003021
value of	ml chi sq selector	0.400000
each rdds into	dstream	0.031250
test that the final value of weights	with sgdtests test	0.111111
formula=none featurescol="features", labelcol="label", forceindexlabel=false) sets params for rformula	ml rformula set params formula featurescol labelcol forceindexlabel	0.500000
of a batch has started	started outputoperationstarted	0.125000
terms or words in the	ldamodel	0.034483
set k decayfactor timeunit	streaming	0.005025
to persist its values across operations after	persist storagelevel	0.166667
point in rdd 'x' to all mixture	gaussian mixture model predict	0.100000
as a :class	data frame	0.005000
train a naive bayes model given an	naive bayes train cls data lambda_	0.500000
set a java system property such as spark	spark context set system property	1.000000
get or compute the number of cols	mllib linalg coordinate matrix num cols	0.333333
tree (e g	tree	0.020833
of the :class dataframe in json format	sql data frame	0.005348
test the partition id	core task context tests test partition id	1.000000
in rdds in a sliding window over	value and window windowduration slideduration numpartitions	0.076923
from an rdd ordered in ascending	rdd take ordered	0.050000
number of nonzero	vector num	0.181818
lda keeplastcheckpoint is	distributed ldamodel get	0.066667
rdd of key-value pairs (of form	rdd save as	0.038462
of conditions and returns one	sql column otherwise value	0.050000
:param rdd an rdd	power iteration clustering train cls rdd	0.250000
matrix whose columns are the left singular vectors	mllib linalg singular	0.017544
to that user	model user	0.250000
returns true positive rate for a given	metrics true positive rate	0.250000
fractions	fractions	1.000000
optional key function	key	0.017857
sparse vector using either	vectors sparse	0.166667
so that :func	query manager	0.011905
first	head	0.166667
for every feature	linear model	0.066667
test that the model params	kmeans test test model params	0.250000
brackets pairs	brackets	0.058824
specifies the underlying output	data stream writer format	0.333333
the computation	mllib binary classification metrics unpersist	1.000000
batches of data from	linear algorithm	0.076923
for each key using an associative	by key	0.026316
wait a	timeout catch_assertions	0.125000
already partitioned data	by spill	0.047619
f-measure or f-measure for a given label category	mllib multiclass metrics f measure label beta	1.000000
list of active queries associated with	sql streaming query manager active	0.066667
applies transformation on a vector or	vector transformer transform vector	0.500000
multiple parameters passed as a list of	core spark conf set	0.100000
all the jobs started by this thread	job	0.023810
:class dataframe	data frame	0.145000
runs and profiles the method	core basic	0.066667
__init__(self	ml aftsurvival regression init	1.000000
column	col	0.114754
a local property	local property key	0.035714
sequences	sequences	1.000000
carry over its keys	linear algorithm	0.076923
value of	get	0.021739
to get or create global taskcontext	core task context get or create	0.250000
term	accumulator add term	0.066667
should not be called directly by users --	init sc value pickle_registry path	1.000000
setparams(self statement=none) sets params	set params statement	0.333333
rdd for dataframe from	from	0.045455
given string	ml param	0.009524
featurescol="features", labelcol="label", forceindexlabel=false)	formula featurescol labelcol forceindexlabel	0.400000
infer schema from an rdd of	infer schema rdd	0.250000
generates an rdd comprised of	mllib random rdds poisson vector rdd sc mean	0.200000
awaitanytermination()	query manager	0.011905
year of a	sql year col	0.050000
sets	evaluator set	1.000000
extract the minutes of	minute	0.040000
file	path	0.051020
makes a class inherit documentation from	mllib inherit doc	0.045455
distname	distname	0.833333
memory for	merger	0.025641
dataframe representing the result of the given	sqlquery	0.054054
a paired rdd where the first	factorization	0.038462
so that :func awaitanytermination() can be used	sql streaming query	0.011765
number of nonzero elements this	ml linalg dense vector num	0.250000
:func awaitanytermination() can	streaming query	0.010526
cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	tree regressor	0.058824
rdd of labeledpoint	mlutils load lib svmfile sc	0.125000
each dstreams in this context to	streaming context	0.055556
terminations	sql streaming	0.010204
dispatch to handle all function	cloud pickler save function	0.142857
rdd of int containing elements from	core spark context range	0.142857
a value to list of ints if possible	ml param type converters to list int value	0.333333
regression model derived from a least-squares fit	regression model	0.031250
convert this matrix to	linalg matrix	0.333333
in this model	model	0.016760
all the	core external merger	0.032258
this distributed model to a	ml distributed ldamodel to	0.166667
setparams(self featurescol="features", predictioncol="prediction", k=2	set params featurescol predictioncol k	1.000000
that has to be inherited by any streaminglinearalgorithm	linear algorithm	0.076923
the values for each key using an	by key	0.026316
the full class name	loader class cls clazz	0.333333
broadcast a read-only variable to the	spark context broadcast	0.125000
to all mixture	mllib gaussian mixture	0.045455
partial objects	core cloud pickler save partial	0.125000
be used	sql	0.002525
train the model on the incoming dstream	linear regression with sgd train on dstream	1.000000
separators inside brackets pairs e g	brackets split	0.083333
compare 2	test compare	0.166667
all globals names read or written to	code globals	0.125000
this matrix	matrix	0.030303
c{self} that is not contained in	subtract other numpartitions	0.111111
model	mllib logistic regression model	0.083333
the selector	mllib chi sq selector set selector	0.333333
converts matrix columns in an input dataframe from	mlutils convert matrix columns to	0.166667
combop	combop	1.000000
matrix stored in csc	matrix	0.015152
ranking	mllib ranking	1.000000
applies transformation on	transformer transform	0.333333
evaluates a list of conditions and	column otherwise value	0.200000
stream returning the result as a	stream reader	0.076923
__init__(self formula=none featurescol="features", labelcol="label",	rformula init formula featurescol labelcol	1.000000
l{statcounter} object that captures the mean variance	core rdd stats	0.083333
the specified string value that match regexp	regexp	0.076923
contains all the elements in seen in a	windowduration slideduration	0.083333
ordered in ascending order or as specified	ordered	0.076923
load a linearregressionmodel	load cls sc path	0.200000
return the column	mllib standard scaler	0.100000
soundex encoding	soundex col	0.055556
a string in libsvm format	libsvm	0.090909
for saving	mlwriter	0.062500
iterator of deserialized batches lists of objects from	serializer load stream without unbatching	0.200000
of document to rdd of	document	0.040000
labelcol label column name	label col	0.200000
versionadded : 0 9 0	logistic regression with sgd	0.500000
of the test	test	0.015152
onevsrest create and return a python wrapper of	ml one vs rest	0.052632
user	user	0.444444
saves	mode partitionby	0.200000
observed tokens in the training set	training	0.029412
kafka	kafka	0.666667
can	reset	0.011236
so that :func awaitanytermination() can	query manager reset	0.011905
recommends	model recommend	0.250000
the threshold if any	threshold	0.018182
approximately find at most	ml lshmodel approx	0.125000
for this	core external	0.016129
:py attr lda keeplastcheckpoint	distributed ldamodel get	0.066667
the model improves on toy data with no	logistic regression with sgdtests	0.200000
:py attr threshold	threshold value	0.333333
createcombiner	createcombiner	1.000000
test that the model params are	mllib streaming kmeans test test model params	0.250000
elements in one operation	core rdd	0.003460
adds a term to this accumulator's value	accumulator add term	0.066667
rdd 'x' to all mixture components	gaussian mixture model predict soft	0.142857
the list of column names skipping	sql	0.005051
a confidence column name	col	0.016393
function to each partition	f	0.010526
of rows	row	0.062500
number of possible outcomes	model num	0.083333
number	mllib linalg block matrix num	0.062500
value	value	0.085470
returns an active	sql	0.002525
rdd of	rdd	0.012232
signed shift the given value numbits right	sql shift right col numbits	1.000000
waits for the termination of this query	termination timeout	0.041667
gaussians in mixture	gaussian mixture	0.038462
rdd contains the count	count	0.016949
create a column scipy matrix from a	sci py tests scipy matrix	0.090909
that is reduced into numpartitions partitions	coalesce numpartitions shuffle	1.000000
multi-dimensional rollup for	frame rollup	0.055556
or list in each	oneatatime	0.111111
start	months start	1.000000
of partitions to use during reduce tasks	default reduce partitions	0.166667
index of	index	0.041667
a configuration property if not already set	core spark conf set if missing	0.500000
a single	single	0.222222
g accuracy/precision/recall objective history total iterations)	logistic regression	0.040000
into the returned	py spark streaming test case	0.333333
spark fair scheduler	spark	0.013158
which each rdd is generated by applying mappartitionswithindex()	map partitions with index f preservespartitioning	0.055556
serializes	serializer	0.062500
the model on toy	on model	0.166667
wait for new	sql streaming query manager	0.011905
matrix columns in an input dataframe	matrix columns	0.142857
from this thread such as the spark	core spark context	0.011628
week number of a	sql weekofyear	0.055556
creates a	sparkcontext sparksession jsqlcontext	0.500000
returns the soundex encoding	sql soundex col	0.055556
much of	external merger object size	0.032258
min number of partitions for hadoop rdds when	min partitions	0.200000
queries so	streaming query manager	0.011236
computes average values for each numeric	grouped data	0.035714
linkpower	link power	1.000000
the length of a string or binary	length	0.040000
a python topicandpartition	and partition init topic partition	0.055556
the underlying output data source	data frame writer format source	0.333333
the left singular vectors of the	singular	0.015625
converts matrix columns in an input dataframe	mlutils convert matrix columns to	0.166667
or	mllib hashing tf	0.125000
window function by name	window function name doc	1.000000
a csv file	csv path schema sep	0.333333
levenshtein distance of the two	levenshtein left	0.058824
length of a string or	length	0.040000
already partitioned data	by	0.014286
a param with a given string name	ml param params has param	0.019231
number of rows	num row	0.500000
of active queries associated with this	manager active	0.066667
streamingcontext from checkpoint data or	streaming streaming context get or	0.200000
params to the wrapped java object	ml java params to	0.045455
an associative and commutative reduce function but	reduce	0.041667
sets	ml elementwise product set	1.000000
featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01	featurescol predictioncol k probabilitycol	0.333333
dump already partitioned data into	group	0.025641
accumulator's value only	core accumulator value value	0.050000
return an iterator	core	0.006042
matrix columns in an input dataframe from	matrix columns	0.071429
dataframe outputted by the model's transform method	logistic regression summary predictions	0.200000
of sub-matrix blocks blockrowindex blockcolindex sub-matrix) that	linalg block matrix blocks	0.166667
maxiter=100 tol=1e-6	maxiter	0.083333
using an associative and commutative reduce function but	reduce by	0.200000
recommends the top "num"	mllib matrix factorization model recommend	0.250000
the rdd of document to rdd	document	0.040000
memory for this obj assume that all the	external merger object size obj	0.040000
an rdd comprised	random rdds poisson vector rdd	0.166667
setparams(self estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none): sets params	set params estimator estimatorparammaps evaluator numfolds	0.333333
a resulting rdd that contains a	core rdd cogroup	0.066667
globals names read or written	globals	0.076923
represents a range of offsets from a single	offset range	0.047619
fits a java model	ml java estimator fit java	0.333333
<http //en wikipedia	regressor	0.086957
model scale paramter	ml aftsurvival regression model scale	1.000000
jvm scala map from a dict	data frame jmap jm	0.111111
until any of	any termination	0.142857
value with another value	value subset	0.200000
kolmogorov-smirnov ks test for data sampled from	mllib stat statistics kolmogorov smirnov test data distname	0.111111
queries so that :func awaitanytermination() can	streaming query manager	0.011236
labeledpoint	lib svmfile sc path	0.125000
zips this rdd with	core rdd zip with	1.000000
95 0 99], quantilescol=none aggregationdepth=2):	fitintercept	0.058824
get depth of tree (e g	tree	0.020833
the square root of the mean squared	root mean squared	0.333333
the model trained	logistic regression model	0.083333
on a model with weights already	streaming linear regression with	0.111111
two function together	f g	1.000000
of the current [[dataframe]] and perform	pivot pivot_col values	0.050000
thread such as the spark fair scheduler pool	core spark	0.010309
shuffle	shuffle	0.833333
from flume	streaming flume	0.111111
splits str around pattern pattern is a	sql split str pattern	0.333333
line in libsvm format into label indices values	mlutils parse libsvm line line multiclass	0.111111
given a large dataset and an item	nearest neighbors dataset	0.333333
comprised of vectors	mllib random rdds	0.083333
left outer join	left outer join other numpartitions	0.111111
awaitanytermination()	sql streaming query manager reset	0.011905
test	core task context tests test	0.500000
from start inclusive to end inclusive	start end	0.181818
sort the list based on first value	streaming test case sort result based on	0.333333
after which the centroids of that	timeunit	0.025641
create a new	context create	0.083333
'x' has maximum membership in this model	mllib gaussian mixture model	0.062500
all the	object	0.027778
compare 2 ml params instances for the	compare params	0.200000
multi-dimensional rollup for the current :class dataframe	frame rollup	0.055556
stream query if	data stream writer	0.041667
sql user-defined type udt for	udt	0.500000
awaitanytermination() can be used	manager reset	0.011905
decisiontreeclassifier	decision tree classification	1.000000
accumulator's value only usable in	accumulator	0.012987
vectors or transform	tf transform	0.045455
parammap into a java parammap	map to java pyparammap	0.250000
this udf with a function and	defined function	0.066667
sets	normalizer set	1.000000
matrix columns in an input	matrix columns to ml dataset	0.142857
column containing a json string into a	from json col	0.083333
the given parameters in this grid to	param grid builder add grid	0.100000
prefix	prefix f	0.142857
mixin for param threshold threshold in binary	has threshold	0.250000
access fields by	struct type getitem key	0.200000
setparams(self scalingvec=none inputcol=none outputcol=none) sets params for	elementwise product set params scalingvec inputcol outputcol	0.333333
singularvaluedecomposition if computeu was set to	value decomposition u	0.100000
__init__(self	ml rformula init formula	0.500000
csv file and	csv path schema sep encoding	0.166667
specifies the input	stream reader	0.076923
again to wait for new terminations	streaming query manager	0.011236
to an int if possible	param type converters to int value	0.250000
the model to make predictions on	predict on	0.117647
returns	sql spark session	0.333333
already	external group by spill	0.047619
used again to wait	query	0.010753
the same time of day in utc	utc	0.050000
value of the given column	col	0.016393
train a naive bayes model given an rdd	mllib naive bayes train cls data lambda_	0.500000
k classes classification problem	classes	0.034483
threshold if	threshold	0.018182
sparkcontext is	spark context ensure	1.000000
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6	init featurescol labelcol predictioncol	0.083333
week number of a given date	sql weekofyear col	0.055556
awaitanytermination()	sql	0.002525
matrix columns in an	matrix columns from	0.142857
the jobs started by this thread until	job	0.023810
partitioned data	external	0.013889
distinct	distinct col rsd	1.000000
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	decision tree regressor	0.058824
value	col	0.032787
matrix other from this block	linalg block	0.076923
sparkcontext is	core spark context	0.011628
the cluster centers represented as a list	bisecting kmeans model cluster centers	0.095238
left outer join of c{self} and c{other}	core rdd left outer join other numpartitions	0.200000
save this model to the given	mllib saveable save sc	1.000000
model improves on toy data with no of	streaming logistic regression with sgdtests	0.200000
groups the :class dataframe using	data frame group by	0.200000
again	query	0.010753
rdd partitioned using	rdd partition by	0.062500
get or compute the number of	linalg indexed row matrix num	0.100000
extract the year of a given date	sql year col	0.050000
param with a given	ml param params has	0.019231
the right singular vectors	mllib linalg singular	0.017544
return a new dstream by applying 'left	left	0.066667
a matrix from the new mllib-local	matrices from	0.333333
the training set given the current parameter	training	0.029412
the spark session to use	session sparksession	0.083333
wait	timeout timeout	0.125000
number of possible outcomes for k classes classification	model num classes	0.500000
for each key using an	key func numpartitions partitionfunc	0.066667
converts vector columns in an	mlutils convert vector columns from ml	0.166667
'any' or 'all'	how	0.166667
compute	linalg row matrix	0.200000
array	array	0.900000
:py attr smoothing	smoothing value	1.000000
given string	ml param params	0.013699
loads a	reader	0.080000
inputformat	inputformatclass keyclass	0.125000
returns a :class dataframe representing the result	sql sqlquery	0.250000
behavior when data or table already	sql data frame writer mode savemode	0.071429
outer join	outer join other numpartitions	0.333333
adds an input option for	frame reader option key value	1.000000
returns a dummy params instance used as	ml param params dummy	0.111111
assumed to consist	ascending numpartitions keyfunc	0.100000
gaussian distributions as	gaussian mixture model gaussians df	0.166667
create an rdd for dataframe	spark session create	0.117647
of write() save	ml mlwritable save	0.166667
called when a receiver has	listener on receiver	0.500000
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	ml decision tree regressor	0.066667
with a randomly generated uid and	cross validator model	0.050000
of weights is close to	parameter accuracy	0.029412
path the	path	0.010204
a single script file calling	script with local functions	0.125000
a new rdd of int containing elements	core spark	0.010309
which predictions are known	regression model	0.031250
of columns for the	columns	0.019608
on the log	ldamodel log	0.125000
create an input stream that pulls events from	utils create stream ssc	0.200000
set the trigger for the stream query	sql data stream writer trigger	0.083333
for given unary	sql unary	0.200000
basic operation test for dstream count	streaming basic operation tests test count	0.500000
of all active stages	tracker get active	0.333333
create a python topicandpartition to map to the	and partition init topic partition	0.055556
return an rdd with the keys	core rdd keys	0.333333
that :func	sql streaming query	0.011765
awaitanytermination() can be used again to wait for	sql	0.002525
for feature selection by	mllib chi sq selector	0.200000
return a javardd of object by unpickling it	core	0.003021
as a :class dataframe	data frame reader	0.166667
in "predictions" which gives the probability of each	ml logistic regression summary probability	0.166667
into a jvm seq of	seq sc	0.055556
convert matrix attributes which	linalg matrix convert	0.166667
hadoop configuration which is	context hadoop	0.090909
underlying sql storage type	type sql type cls	0.250000
merges them with extra	map extra	0.040000
associative and commutative reduce	reduce	0.041667
jvm seq of column	seq sc cols	0.055556
numtrees	num trees	0.500000
:class dataframe to a data	sql data frame writer save	0.083333
spark configuration	spark conf init loaddefaults	0.250000
for each key using an	by key	0.026316
goodness	stat statistics	0.125000
onevsrestmodel create and	one vs rest model	0.058824
compute the	core rdd	0.003460
inputformat	inputformatclass keyclass valueclass	0.125000
a python rdd of key-value pairs (of	rdd save	0.038462
all	core external merger object size	0.032258
returns accuracy equals to the total number of	mllib multiclass metrics accuracy	0.166667
train a gradient-boosted trees model for regression	mllib gradient boosted trees train regressor	0.333333
a map of words to their vector representations	mllib word2vec model get vectors	0.166667
new spark configuration	spark conf init loaddefaults _jvm	0.250000
binomial logistic regression	logistic regression	0.080000
optional default value and user-supplied	param params	0.014925
>>> dm = densematrix(2 2 range 4	mllib linalg dense matrix	0.083333
use	spark context	0.023256
multiply	multiply	0.500000
table accessible via jdbc url url	jdbc url table column	0.166667
queries so that :func awaitanytermination() can be	query manager reset	0.011905
database table	table	0.031250
parse	parse	0.571429
that all the	size	0.009174
the rdd as non-persistent	core rdd unpersist	0.066667
matrix columns in an	matrix columns to ml	0.142857
featurescol	featurescol	0.156250
called when processing of	listener on output operation	0.166667
reducing each rdd	reduce func	1.000000
path a shortcut of write() save	ml one vs rest save	0.166667
:func awaitanytermination() can be used again to wait	sql streaming query manager	0.011905
two vectors	linalg dense vector	0.400000
globals names read or written to by	code globals	0.125000
calculates the length of a string or	sql length	0.050000
specifies the underlying output data source	writer format source	0.333333
test the python direct kafka rdd api with	streaming kafka stream tests test kafka rdd with	1.000000
squared error which is defined as	mllib regression	0.022727
gaussians in mixture	mllib gaussian mixture model	0.062500
:py attr max	max value	1.000000
the :class dataframe in	sql data frame writer	0.034884
number of months between date1	months between date1	0.333333
:py attr statement	statement value	1.000000
mlwritable	mlwritable	0.833333
to any hadoop file system using the l{org	sequence file path compressioncodecclass	0.500000
for	manager	0.011236
user	model user	0.250000
be placed	core modules	1.000000
test that the model	test test model	1.000000
set the selector	set selector	0.333333
unified dstream from multiple dstreams	streaming streaming context union	0.111111
of predicted ratings for input user and	matrix factorization model predict all user_product	0.050000
the residuals	summary residuals residualstype	0.333333
for every feature	mllib	0.010526
columns that make up each	matrix cols per	0.333333
:class datatype the data type string	datatype string	0.111111
an object is of	obj identifier	0.333333
the uid of this instance this updates both	ml param params reset uid newuid	0.058824
buckets the output by the given	writer bucket by numbuckets col	0.200000
an input stream that is to be	stream ssc	0.090909
generate	generate logistic input offset	1.000000
multi-dimensional rollup for the current :class dataframe	data frame rollup	0.055556
is generated by applying mappartitionswithindex() to each	map partitions with index f preservespartitioning	0.055556
a local	get local	0.333333
the accumulator's data type returning	accumulator param	0.038462
or compute the number	linalg row matrix num	0.100000
the soundex encoding for a string >>> df	soundex col	0.055556
test the python direct kafka rdd api	tests test kafka rdd	0.500000
terms to term frequency vectors or	hashing tf	0.125000
comprised of vectors containing i i d	random rdds poisson vector	0.125000
create a multi-dimensional rollup for the current	data frame rollup	0.055556
params are	params	0.006623
code for a shared param class	ml param gen param code name doc	0.333333
the stream query if this is	data stream	0.028571
columns	columns from ml	0.125000
fits a	ml estimator fit	0.083333
a list that contains all	collect	0.125000
parameters in this grid to fixed	ml param grid builder base	0.076923
the stream query if this	data stream	0.028571
external list	external	0.013889
fp-growth model that contains frequent itemsets	fpgrowth train cls	0.200000
line in libsvm format into label indices	parse libsvm line line multiclass	0.111111
python direct kafka rdd api with leaders	kafka rdd with leaders	0.500000
queries so that :func awaitanytermination() can be used	streaming query manager reset	0.011905
model fitted by gaussianmixture	gaussian mixture model	0.052632
evaluates the output with optional parameters	ml evaluator evaluate dataset params	1.000000
total log-likelihood for this model on the given	ml gaussian mixture summary log likelihood	0.142857
status	status	0.666667
computes the area under	metrics area under	0.166667
values for each key using	key func numpartitions	0.066667
applies transformation on	vector transformer transform	0.500000
decision tree-based ensemble algorithms	tree ensemble	0.055556
matrix to the	dense matrix	0.076923
in rdd 'x' to all mixture components	mllib gaussian mixture model predict soft	0.142857
the :class dataframe	frame writer save path	0.066667
[[arraytype]] of [[structtype]]s with the specified schema	schema	0.033333
setparams(self inputcol=none outputcol=none labels=none) sets params for this	index to string set params inputcol outputcol labels	0.333333
a model from the input java	java estimator	0.200000
in the rdd into a	core rdd	0.003460
convert this matrix to the new mllib-local representation	mllib linalg dense matrix as ml	0.333333
schema abstract >>> _parse_field_abstract("a")	abstract s	1.000000
window size	window size	1.000000
kafkardd	kafka rdd	0.142857
already partitioned	by	0.014286
flume	streaming flume utils	0.200000
the test this should	test	0.015152
training set given the current	ldamodel training	0.034483
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	ml decision tree regressor	0.066667
specifies the input schema	data frame reader schema schema	0.333333
add a py or zip dependency	context add py file path	0.166667
__init__(self formula=none	ml rformula init	0.500000
a dictionary a list of index value pairs	init size	0.066667
as a string column	string	0.041667
dict	dict sample	1.000000
bound on the log likelihood	ldamodel log likelihood dataset	0.142857
of the current [[dataframe]] and perform the specified	data pivot pivot_col values	0.050000
n elements from an rdd ordered in	rdd take ordered	0.050000
sets mincount the minimum number	set min count mincount	0.250000
is of	ml ldamodel is distributed	0.066667
"predictions" which gives the	logistic regression summary	0.090909
mixin for param seed random seed	has seed	1.000000
return a resulting rdd that contains a	rdd cogroup	0.066667
test that the model	streaming kmeans test test model	1.000000
the week number of a given	sql weekofyear col	0.055556
extract the year of a given date	sql year	0.050000
add a py or zip dependency for	context add py	0.166667
format or newline-delimited json	json path mode	0.125000
can be used again to wait for new	query manager reset	0.011905
of the :class dataframe in json	sql data frame writer	0.011628
at the specified	compression	0.071429
is set to	context set	0.125000
a python topicandpartition	partition init topic partition	0.055556
basic operation test for dstream countbyvalue	basic operation tests test	0.111111
values	mllib standard	0.125000
given parameters in this grid to fixed values	param grid builder add grid param values	0.333333
can be	reset	0.011236
dct	dct	0.750000
or transform the rdd	mllib hashing tf transform	0.045455
comprised of vectors containing	mllib random rdds	0.083333
of a batch of jobs	batch	0.068966
rdd of key-value pairs	rdd save as	0.038462
used again to wait for new terminations	sql streaming query manager	0.011905
summary of model	mllib decision tree model repr	1.000000
:func awaitanytermination() can be used again	reset	0.011236
content of the :class dataframe as the	sql data frame writer save as	0.071429
until any of the	manager await any termination	0.142857
the dispatch to handle all function types	core cloud pickler save function	0.142857
wait a given amount of time for	timeout	0.071429
broker's port	port	0.142857
null model	regression summary null	0.250000
:param rdd an rdd of	power iteration clustering train cls rdd	0.250000
for this :class dataframe a	sql data frame	0.005348
for prediction tasks regression	prediction	0.041667
item off the heap maintaining the heap invariant	core heappop heap	0.142857
merges them with extra values from input into	map extra	0.040000
basic operation test for dstream mappartitions	basic operation tests test	0.111111
hadoop-supported	path	0.020408
dstream in which each rdd contains the	by	0.014286
from the	load	0.111111
two columns of a dataframe as	sql data frame corr col1 col2	0.166667
used again to wait for	streaming query	0.010526
distributions as	mixture model gaussians df	0.333333
an input	stream reader	0.076923
fitting and predicting on incoming dstreams	streaming	0.005025
return	streaming py spark streaming	0.333333
the minutes	minute	0.040000
get the root directory that contains	root directory	0.333333
globals	globals	0.384615
mean squared error which is defined as the	mllib regression	0.022727
first	frame head	1.000000
standardization whether to standardize	standardization	0.076923
comprised of vectors containing	mllib random rdds exponential	0.125000
the levenshtein distance	sql levenshtein left right	0.058824
underlying sql storage type for this udt	user defined type sql type	0.500000
line in	line line	0.166667
get a local property set in this thread	spark context get local property key	0.066667
left outer join of c{self}	rdd left outer join other	0.111111
property for the given key	key	0.017857
as	sql	0.002525
value of	ml train validation	1.000000
model intercept	intercept	0.090909
a local property set in	spark context get local property key	0.066667
convert the vector into	vector to	1.000000
value of	ml aftsurvival regression	0.666667
:func awaitanytermination() can be used	sql streaming query	0.011765
bucketizer	ml bucketizer	0.250000
as the spark fair	core spark	0.010309
an rdd containing all pairs of elements	rdd	0.003058
>>> dm = densematrix(2 2 range 4	mllib linalg dense matrix repr	0.142857
functions	functions	0.357143
a cluster	cluster	0.142857
get all values as	spark conf get all	0.166667
that	external merger object size	0.032258
:class column for approximate distinct count	approx count distinct col	0.071429
associative function "func" and a	core	0.003021
multi-dimensional cube for the current :class dataframe using	frame cube	0.055556
receiverstarted	receiverstarted	0.714286
as non-persistent and remove all blocks for	unpersist	0.083333
the behavior when data or	data frame writer mode savemode	0.071429
for column of predicted clusters	ml clustering summary prediction col	0.111111
vector conduct pearson's chi-squared goodness	mllib stat statistics chi sq	0.066667
test that the final value of weights is	streaming logistic regression with sgdtests test	0.111111
sets	als set	1.000000
day of the month of a given date	dayofmonth col	0.031250
be used again	query	0.010753
day in the given timezone returns	sql	0.002525
converts vector columns	mllib mlutils convert vector columns from ml dataset	0.166667
centroids of that particular	timeunit	0.025641
comprised of	mllib random rdds gamma vector	0.125000
labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor", quantileprobabilities=[0 01	labelcol predictioncol	0.083333
shortcut of write() save path	ml pipeline save path	0.200000
the month of a given date as	sql dayofmonth col	0.031250
parquet files returning the result as	parquet	0.066667
label indexer	indexer	0.055556
computes column-wise summary statistics	stat statistics col stats	0.200000
from an rdd ordered in	rdd take ordered	0.050000
format or newline-delimited json	json	0.043478
create a new profiler using class	profiler collector new profiler	0.333333
number of rows	linalg block matrix num rows	0.200000
wait for new terminations	sql	0.002525
value of	ml param has standardization	1.000000
95 0 99], quantilescol=none aggregationdepth=2)	fitintercept	0.058824
new accumulator	accumulator	0.012987
flag indicating whether values containing quotes should always	escapequotes	1.000000
<http //jsonlines	path mode compression dateformat	1.000000
or	streaming context get or	0.200000
python rdd of	rdd	0.012232
functionality for statistic functions with	stat functions	0.333333
again	streaming query	0.010526
data in :class dataframe	data frame	0.005000
transfer this instance's params to the wrapped java	java params to	0.045455
a given	ml	0.001835
maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false	maxdepth	0.100000
numerical	quantile col probabilities relativeerror	0.166667
comprised of vectors containing i i	mllib random rdds log	0.125000
data	external group by spill	0.047619
set a java system property such as spark	core spark context set system property	1.000000
transforms term frequency tf vectors to tf-idf vectors	mllib idfmodel transform	0.142857
set initial centers should be	set initial centers centers weights	0.200000
rdd 'x' has maximum membership	gaussian mixture	0.038462
the python direct kafka stream api with	kafka direct stream	0.055556
group id	group groupid	0.142857
parameters in	grid builder	0.055556
the initial value of	initial	0.071429
comprised of vectors containing i i	random rdds log normal	0.125000
java array of	ml java wrapper new java array	0.333333
the right singular vectors of the singularvaluedecomposition	singular value decomposition v	0.250000
the given path a shortcut of write()	ml pipeline	0.095238
variance and count of the rdd's elements	core	0.003021
:py attr losstype	loss type value	1.000000
converts matrix columns in an input dataframe to	mlutils convert matrix columns from ml dataset	0.166667
stop	stop	0.473684
fast version of a heappush followed	core heappushpop heap item	0.142857
get or compute the	mllib linalg distributed matrix	0.333333
this docstring is not shown publicly	mllib linalg coordinate matrix init entries numrows numcols	0.333333
multi-dimensional cube for	sql data frame cube	0.055556
use	core spark context	0.011628
infer schema from an rdd	sql spark session infer schema rdd samplingratio	0.250000
to array	to array	1.000000
given parameters in this grid to fixed values	grid builder	0.055556
vector conduct pearson's chi-squared goodness	stat statistics chi sq	0.066667
converts vector columns	convert vector columns from ml dataset	0.166667
which predictions	regression	0.010000
pearson's chi-squared	chi sq	0.111111
of the :class dataframe to	frame writer save	0.066667
to_replace	to_replace	0.833333
vectors or transform the rdd of document	mllib hashing tf transform document	0.166667
sql context to use for saving	ml java mlwriter context sqlcontext	0.333333
can be used	frame	0.034483
set the initial value of weights	sgd set initial weights initialweights	0.333333
the dot product of two vectors we	linalg dense vector dot other	0.058824
corresponding string values	string	0.041667
lda keeplastcheckpoint is set	distributed ldamodel get	0.066667
of key value	key	0.017857
returns a java storagelevel	get java	0.111111
list of labels corresponding to	ml string indexer model labels	0.066667
the uid of this instance this updates	ml param params reset uid newuid	0.058824
the user and the second is an array	mllib matrix	0.047619
wait for	streaming streaming context await termination or timeout timeout	0.125000
a multi-dimensional rollup for the current	data frame rollup	0.055556
can	sql streaming query manager	0.011905
text file at the specified path	text path compression	0.333333
computes average values for each numeric	sql grouped data	0.041667
matrix columns	matrix columns to ml	0.142857
value to an int if possible	ml param type converters to int	0.250000
again to wait for	streaming	0.005025
spark	spark	0.157895
:	regression model	0.031250
:func awaitanytermination() can	query manager	0.011905
hot	hot	1.000000
:class dataframe as the specified table	sql data frame writer save as table name	0.333333
format (json lines text format or newline-delimited json	json path mode	0.125000
python topicandpartition to map to the	partition init topic partition	0.055556
accumulator's	accumulator param	0.038462
recommends the top	mllib matrix factorization model recommend	0.250000
python topicandpartition to	streaming topic and partition init topic partition	0.055556
given a java object	java params from java	0.333333
paired rdd	factorization model	0.043478
pipeline create and return a	pipeline	0.052632
norm of	norm	0.041667
computes column-wise summary statistics for	statistics col	0.200000
with two fields threshold recall curve	ml binary logistic regression summary recall by threshold	0.166667
for this obj assume that	obj	0.023810
matrix to the	linalg dense matrix	0.083333
value of	ml naive bayes	0.250000
query stop() or by an	query await	1.000000
a streaming dataframe/dataset is written	stream writer output mode outputmode	0.083333
a java object	from java	0.111111
weights computed	mllib linear model weights	0.250000
python code	code name	0.111111
locate the position of the	locate	0.076923
of expressions and returns	sql	0.002525
use only create a new hivecontext for testing	create for testing	0.333333
multilayer	multilayer	1.000000
returns the least value of the	sql least	0.055556
this rdd which is	core rdd	0.003460
objective function scaled loss + regularization at	linear regression training summary objective history	0.500000
file using string representations	file path compressioncodecclass	0.333333
parses a column containing a json	from json col	0.083333
index of the original partition	map partitions with index	0.100000
given a	cls	0.190476
the probability of obtaining a test	test	0.015152
sets params for	set params featurescol	0.100000
data or table already exists	sql data frame	0.005348
for	core external merger object size	0.032258
c{other}	join other	0.071429
examplepoint	example point	0.500000
the column	scaler model	0.153846
stream	stream	0.263158
:param rdd an rdd of	clustering train cls rdd	0.250000
a dictionary a list	size	0.036697
awaitanytermination() can	streaming query manager reset	0.011905
sets	support set	1.000000
underlying data source	sql data frame	0.010695
a new dstream in which each	by	0.014286
the explained variance	explained variance	0.333333
finding frequent items for	sql data frame freq items cols support	0.166667
class inherit documentation from its parents	mllib inherit doc cls	0.045455
this	has param	0.019231
the observed data against the expected	observed expected	0.166667
iterator	iterator key	0.200000
a param with	ml param params has param	0.019231
forked external process	pipe command env checkcode	0.166667
obj assume that	core external merger object size obj	0.040000
for every feature	mllib linear model	0.125000
regression	mllib regression	0.022727
of active queries associated with this sqlcontext >>>	query manager active	0.066667
the given path	sc path	0.333333
comprised of vectors	mllib random rdds gamma vector	0.125000
memory for	external merger	0.031250
returns micro-averaged label-based f1-measure	multilabel metrics micro f1measure	1.000000
much of memory for this	external merger object size	0.032258
labelcol	labelcol	0.185185
for this	core external merger object size	0.032258
each	clustering summary	0.500000
vectors which this	vector indexer model	0.200000
blocks in	blocks	0.076923
already partitioned	external group by	0.045455
companion	java params transfer	0.125000
total number of clusters	kmeans model k	0.250000
json <http //jsonlines	json path mode compression dateformat	0.166667
converters	converters	1.000000
file to which this	file	0.028571
the deviance for the	generalized linear regression summary deviance	0.125000
risk function corresponding to the expected value of	ml linear	0.066667
list of active queries associated with this sqlcontext	query manager active	0.066667
so that	streaming query manager reset	0.011905
return a new dstream by applying reducebykey	streaming dstream reduce by key	0.076923
returns a paired rdd where	factorization	0.038462
model params	model params	0.125000
for the stream query	stream	0.017544
number of nonzero elements	ml linalg dense vector num	0.250000
of this instance with	ml one vs rest	0.052632
basic operation test for dstream groupbykey	streaming basic operation tests test group	1.000000
be	reset	0.011236
rdd which is	rdd	0.003058
rdd's elements in	core	0.003021
contains a param with	params has	0.019231
queries	streaming query	0.010526
obj assume that	obj	0.023810
an rdd ordered in ascending	rdd take ordered	0.050000
predictioncol="prediction", maxiter=100 regparam=0 0 tol=1e-6 rawpredictioncol="rawprediction", fitintercept=true standardization=true	predictioncol maxiter	0.400000
defined on the class	ml param	0.009524
the deviance for the	regression summary deviance	0.125000
r2	r2	1.000000
a line	line line multiclass	0.166667
converter to drop the names	converter datatype	0.071429
the :class dataframe to the	sql data frame	0.005348
2 range 4	dense matrix repr	0.142857
memory for this obj assume that all the	size obj	0.040000
least value of the	sql least	0.055556
add a py or	add py	0.166667
to any hadoop file system using the l{org	as sequence file path compressioncodecclass	0.500000
term	term	0.240000
of the month of a given date as	sql dayofmonth	0.031250
pylist	pylist	1.000000
the initial value of weights	with sgd set initial weights initialweights	0.333333
data or table already exists	data	0.011628
input dataset which is an instance	dataset	0.020408
generates an rdd comprised	mllib random rdds uniform vector rdd sc	0.200000
so that :func awaitanytermination()	query manager reset	0.011905
computes column-wise summary statistics	statistics col stats rdd	0.200000
set initial centers should be	streaming kmeans set initial centers centers	0.200000
setparams(self scalingvec=none inputcol=none outputcol=none) sets params	ml elementwise product set params scalingvec inputcol outputcol	0.333333
a new sparkcontext at least the master and	core spark context init master	0.500000
libsvm format into label indices	mlutils parse libsvm	0.125000
creates a copy	model copy extra	0.333333
for the stream query if this is	sql data stream writer	0.041667
for each key using a custom	by key	0.026316
into	core external group by	0.045455
module in modlist	modlist	0.100000
to make predictions on batches	linear algorithm predict on	0.066667
a java	java cls	0.111111
converts matrix columns in an input dataframe	mllib mlutils convert matrix columns to ml	0.166667
a receiver has been started	receiver started receiverstarted	0.500000
versionadded : 0 9 0	kmeans	0.025641
broadcast a read-only variable to the	core spark context broadcast value	0.125000
of model	mllib decision tree model	0.076923
c{other}, return a resulting rdd that contains a	core rdd cogroup	0.066667
day of the month of a	dayofmonth col	0.031250
model on toy data	on model	0.166667
the length of a string or	length	0.040000
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	tree regressor	0.058824
merge	merge	1.000000
returns an mlwriter instance for this ml	mlwritable write	0.200000
generate	sgdtests generate logistic	1.000000
vector columns	vector columns to ml dataset	0.142857
evaluates the output with optional parameters	evaluator evaluate dataset params	1.000000
0] for feature selection by	chi sq selector set	0.125000
multi-dimensional rollup for the current	data frame rollup	0.055556
the old hadoop outputformat api mapred	hadoop dataset conf keyconverter valueconverter	0.083333
impurity="variance", seed=none variancecol=none)	tree regressor	0.058824
predict the label of one	predict x	0.016949
of this	ml param params reset	0.166667
profiling stats pstats stats	profiler stats	1.000000
:py attr lda keeplastcheckpoint is set	distributed ldamodel	0.052632
return a copy	core	0.003021
from	load stream	0.500000
each original	original	0.047619
root	get root	0.333333
returns true positive	metrics true positive	1.000000
sets	expansion set	1.000000
current [[dataframe]] and perform the specified	sql grouped data pivot pivot_col values	0.050000
a matrix	matrix	0.015152
of terms to term frequency vectors or transform	hashing tf transform	0.045455
:func	streaming	0.010050
that :func awaitanytermination() can be used again to	manager reset	0.011905
data of a	data stream	0.028571
numtrees=20 featuresubsetstrategy="auto", seed=none	ml random forest	0.071429
an rdd previously saved using l{rdd saveaspicklefile} method	spark context pickle file name minpartitions	0.250000
returns a paired	factorization	0.038462
a python topicandpartition to map to	topic and partition init topic partition	0.055556
singular values in descending order	mllib linalg singular value decomposition s	0.250000
a temporary table in the catalog	table	0.031250
resulting rdd that contains a	core rdd cogroup other numpartitions	0.066667
generates an rdd comprised	random rdds exponential vector rdd sc mean	0.200000
named table accessible via jdbc url url and	reader jdbc url table column lowerbound	0.166667
model	kmeans model	0.181818
mark the rdd as non-persistent and remove	rdd unpersist	0.066667
trigger	writer trigger	0.111111
correlation	method	0.041667
the number of clusters	mllib power iteration clustering model k	0.200000
dictionary of value count pairs	count by value	0.333333
given parameters in	grid builder	0.055556
with the spark sink deployed on	storagelevel maxbatchsize	0.045455
buckets the	numbuckets col	0.500000
the elements in iterator	iterator key reverse	0.200000
points from the population should be	mllib stat kernel density	0.066667
list of tables/views in the specified database	list tables dbname	1.000000
much of memory for this obj assume	object size obj	0.040000
start value	start	0.045455
set each dstreams in this context	streaming context	0.055556
the md5 digest and returns	sql md5 col	0.333333
partitioned data	core external	0.016129
__init__(self	classifier init	1.000000
to configure the kmeans algorithm for	streaming kmeans	0.035714
already partitioned data	external	0.013889
lines text format or newline-delimited json	writer json	0.125000
subsamplingrate	subsampling rate	1.000000
that :func awaitanytermination() can be used again to	streaming query manager reset	0.011905
model to be used for later scaling	standard scaler fit dataset	0.250000
dstream	streaming dstream map	1.000000
original column during fitting	scaler model original	0.062500
mixin for param inputcol input column name	has input col	1.000000
and count of the rdd's	core	0.003021
the trigger for the	trigger	0.071429
fwe	fwe fwe	1.000000
for data	data	0.011628
new dstream in which each	streaming streaming context	0.032258
java parammap	java pyparammap	0.500000
a param with a given string name	param params	0.014925
value	value subset	0.200000
elements from an rdd ordered in ascending order	core rdd take ordered	0.050000
can be	sql streaming query manager reset	0.011905
the dataframe in a	data frame	0.005000
from an :class rdd, a list	schema samplingratio verifyschema	0.029412
the observed data against the	observed	0.058824
can be used again to wait for	streaming query manager	0.011236
python object into java	ml py2java sc obj	0.333333
the column mean values	scaler model mean	0.125000
output a python rdd of	rdd save as	0.038462
an event time	eventtime	0.125000
column scipy matrix from a dictionary	sci py tests scipy matrix size	0.090909
this instance with a randomly generated uid	one vs rest	0.034483
sample covariance of col1 and col2	covar samp col1 col2	0.333333
an input stream that pulls events	stream	0.017544
this instance contains a param	param params has	0.019231
the :class dataframe to a data	sql data frame writer save path	0.142857
or replaces	or replace	0.500000
from an :class rdd,	schema samplingratio verifyschema	0.029412
the mean variance and count	core rdd	0.003460
that	core external merger object	0.032258
names of	sqlcontext table names	0.066667
udf with a function and	function	0.027778
awaitanytermination() can be	query manager reset	0.011905
script	script	0.857143
to all mixture components	mllib gaussian mixture model predict soft x	0.142857
this distributed model to a local representation this	ml distributed ldamodel to local	0.111111
a python topicandpartition	init topic partition	0.055556
tracker	tracker	1.000000
version of a heappush followed by a	heap	0.047619
of nodes in tree including	tree model	0.026316
group by key	group by	0.041667
to any hadoop file system using the l{org	save as sequence file path compressioncodecclass	0.500000
fp-growth model that	fpgrowth train cls data minsupport numpartitions	0.200000
index of the	index	0.041667
polling	polling	1.000000
of the date	date	0.037037
contains a param with a given string	param params has param	0.019231
of another	other	0.033333
specific group matched by a java regex	str pattern idx	0.111111
for the sample covariance of col1 and	covar samp col1	0.250000
unifying data of another dstream	union other	0.333333
of numpy arrays	ml bisecting	0.066667
in json	sql	0.002525
of the :class dataframe to a data source	data frame writer save	0.083333
the stream query if	data stream writer	0.041667
the year of a given	sql year col	0.050000
list of columns for the given	list columns	0.166667
an 'old' hadoop	spark context hadoop	0.090909
that all the	core	0.003021
setparams(self inputcol=none outputcol=none labels=none) sets params for	set params inputcol outputcol labels	0.333333
vector columns in an input dataframe	vector columns to ml	0.142857
a param with a	param params has	0.019231
:class dataframe to a data source	data frame	0.005000
threshold if any	threshold	0.018182
so that :func	streaming query manager	0.011236
the model	regression	0.010000
of parameters specified by the	ml	0.001835
mean	model mean	0.125000
obj assume that all the objects	merger object size obj	0.040000
to be used with the spark sink deployed	addresses storagelevel maxbatchsize	0.045455
setparams(self scalingvec=none inputcol=none outputcol=none) sets params for this	ml elementwise product set params scalingvec inputcol outputcol	0.333333
can be used again	streaming query manager reset	0.011905
for this ml instance	ml pipeline model	0.066667
load a model from the	mllib kmeans model load cls sc	0.333333
transfer	transfer	0.714286
broadcast a read-only variable	context broadcast	0.125000
__init__(self	validator init	1.000000
broadcast on	broadcast	0.052632
underlying sql storage	sql	0.002525
train the model on the incoming	streaming logistic regression with sgd train on	0.333333
test that the model params are set correctly	streaming kmeans test test model params	0.250000
items	items cols	0.125000
the dot product of two	mllib linalg dense vector dot other	0.058824
params shared by them	params copy	0.083333
a list of conditions and returns one of	sql column otherwise value	0.050000
return the column	mllib	0.010526
sets	items col set	1.000000
featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction",	featurescol labelcol predictioncol probabilitycol	0.333333
the file to which	file	0.028571
started	started	0.444444
a range of offsets from	range	0.030303
this instance with a randomly generated uid	cross validator model	0.050000
or names into a jvm seq of column	seq	0.043478
with new	df	0.111111
greatest	greatest	0.217391
field in "predictions" which gives the true label	logistic regression summary label	0.333333
return the column mean values	standard scaler model mean	0.125000
test a single script on a cluster	tests test single script on cluster	1.000000
setting the spark context call site	sccall site sync	0.200000
format into an rdd of labeledpoint	lib svmfile sc path	0.125000
converts matrix columns in an input	mlutils convert matrix columns to ml	0.166667
degrees of	regression summary degrees of	1.000000
labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0	labelcol predictioncol probabilitycol	0.500000
create a sparse vector using either a dictionary	vectors sparse size	0.166667
get or compute the number	num	0.025210
a py or zip	py file path	0.066667
:py attr family	family value	1.000000
the libsvm format into an rdd of labeledpoint	load lib svmfile sc path	0.125000
and incrementing	core task context	0.100000
computes column-wise summary statistics for the input	stat statistics	0.125000
saves the content of the dataframe in	sql data frame writer	0.011628
of columns that make up each block	cols per block	0.333333
'x' has maximum membership in	mllib gaussian mixture	0.045455
the initial value of weights	set initial weights	0.333333
adds an output option	frame writer option	1.000000
set initial centers should be set before	kmeans set initial centers centers weights	0.200000
saves the contents of the	writer save path format mode partitionby	0.200000
inherit documentation from	mllib inherit	0.045455
given path	sc path	0.333333
"num"	model	0.005587
compute the standard deviation of	rdd stdev	0.066667
add two	param add	0.250000
comprised of vectors containing i i	random rdds log	0.125000
model to make predictions on batches of data	streaming linear algorithm predict on	0.066667
awaitanytermination() can be	sql streaming query	0.011765
large dataset and an item approximately find at	lshmodel approx nearest neighbors dataset	0.166667
return a resulting rdd that contains	rdd cogroup other	0.066667
coldstartstrategy	cold start strategy	1.000000
generate	regression with sgdtests generate logistic input	1.000000
the first occurrence of substr	substr str	0.125000
load labeled points	load labeled points	1.000000
limits the result	limit	0.076923
predicted ratings for input	matrix factorization model predict all user_product	0.050000
cluster	mllib kmeans model cluster	0.333333
of document to	document	0.040000
wait for new	streaming query manager reset	0.011905
converter to drop the names	converter	0.052632
the termination of this query either	termination	0.035714
value to an int if possible	ml param type converters to int value	0.250000
path of a file added through c{sparkcontext addfile()}	core spark files get cls filename	0.200000
the kolmogorov-smirnov ks test for data sampled from	stat statistics kolmogorov smirnov test data distname	0.111111
list of names	table names	0.066667
return an numpy ndarray	ml linalg sparse matrix to array	1.000000
that has no partitions	core spark context empty	0.333333
predicted	prediction col	0.285714
train the model on the	streaming logistic regression with sgd train on	0.333333
seq of columns that describes the sort order	frame sort cols cols kwargs	0.142857
this instance contains a param with a given	params has param	0.019231
get the root	get root	0.333333
which is defined as the square root of	ml linear regression summary root	1.000000
string	ml param params	0.013699
set number of batches after	mllib streaming kmeans set	0.142857
finding frequent items	frame freq items	0.166667
parameters passed as a list of key-value pairs	spark conf set all pairs	0.500000
trigger for	trigger	0.071429
the dot product of two vectors we	dot other	0.050000
string	ml	0.001835
is checkpointed	is checkpointed	0.142857
after which the centroids of that particular	timeunit	0.025641
testing	testing	1.000000
names skipping	sql	0.005051
all the objects	core external	0.016129
or compute the number of	matrix num	0.088235
pipelinemodel create and return a python wrapper	pipeline model	0.071429
private abstract class representing a multiclass classification model	classification model	0.166667
vector columns in an input dataframe to	vector columns from	0.142857
of ensuring all received data	stopsparkcontext stopgracefully	0.050000
all globals names read or written to by	pickler extract code globals	0.125000
'x' to all mixture	mixture model	0.066667
this	external merger object	0.032258
the deviance for the null model	summary null deviance	0.250000
of	external merger	0.031250
the week number	sql weekofyear col	0.055556
of offsets from a	offset	0.021739
this test	ml other test	0.500000
the minutes of a given date	minute col	0.050000
fast version of a heappush followed by	core heappushpop heap item	0.142857
in multinomial logistic regression	mllib logistic regression	0.250000
the unicode as utf-8	streaming utf8	0.500000
model	discretizer create model	1.000000
a local	context get local	0.333333
tree model for classification or regression	tree model	0.026316
none by default	conf	0.050000
on a model with weights already set	mllib streaming linear regression with	0.111111
words closest in similarity	word2vec	0.052632
to wait for new terminations	sql streaming query manager reset	0.011905
with a given string name	param	0.012500
two-sided p-value of estimated coefficients	ml generalized linear regression training summary p values	0.333333
the += operator adds a term to	accumulator iadd term	0.142857
into an rdd of labeledpoint	mllib mlutils load lib svmfile sc path	0.125000
column of the current [[dataframe]] and perform the	sql grouped data pivot pivot_col values	0.050000
offsetrange	ranges	0.090909
the levenshtein distance of the	levenshtein left	0.058824
sort the elements in iterator	iterator	0.100000
for this obj assume that all the objects	object size obj	0.040000
string	split s	1.000000
to wait for	sql streaming query manager reset	0.011905
a column scipy matrix from a dictionary	sci py tests scipy matrix size	0.090909
save path	model save path	1.000000
number of classes values which the label	ml java classification model num classes	0.250000
values for each key	key func numpartitions	0.066667
for data sampled from a	data distname	0.083333
a line in libsvm format into label	parse libsvm line line	0.111111
class inherit documentation from its	inherit doc cls	0.045455
of a batch of jobs has completed	batch completed batchcompleted	0.333333
set a java system property such	context set system property cls key	1.000000
calculates the length of a	length	0.040000
:py attr lda keeplastcheckpoint is	ml distributed ldamodel get	0.066667
model fitted by :class decisiontreeregressor	decision tree regression model	1.000000
for which	isotonic regression model	0.100000
will convert each python object into java	to java	0.136364
function to the value of	map values f	0.125000
queries so that :func awaitanytermination() can be	query	0.010753
separates positive predictions from negative predictions	linear classification model	0.076923
wait a given amount of	timeout catch_assertions	0.125000
called when processing of a	streaming streaming listener on	0.200000
be used again	sql streaming query	0.011765
this instance contains a param with a given	param params has param	0.019231
returns a paired rdd where the first element	matrix factorization	0.040000
objects from	serializer load	0.083333
a model with weights already set	mllib streaming linear regression with	0.111111
observed tokens in the training	training	0.029412
given parameters in this grid	param grid builder base	0.076923
persistence	persistence	1.000000
k-means algorithm return the model	kmeans train rdd	0.333333
this instance	param params has param	0.019231
as the specified table	as table	0.200000
and c{other}	core rdd	0.006920
used again to	sql streaming	0.010204
a new name	name	0.043478
replacing a value	replace to_replace	0.200000
lda keeplastcheckpoint	distributed ldamodel get	0.066667
in orc	orc	0.083333
is later than	dayofweek	0.037037
train the model	logistic regression with sgd train	1.000000
accumulator's value only usable in driver	accumulator	0.012987
combiners	combiners	1.000000
a l{statcounter} object that captures the	core rdd stats	0.083333
list of active queries associated with this sqlcontext	sql streaming query manager active	0.066667
matrix columns in	matrix columns to	0.142857
order or as specified by the optional key	key	0.017857
a single script on	single script on	0.250000
correlation of	col1 col2 method	0.055556
update the	model update	0.500000
by the model's transform method	logistic regression summary predictions	0.200000
sets the context to	streaming streaming context	0.032258
of this instance with a	ml one vs rest model	0.111111
add a py or zip dependency for	spark context add py file path	0.166667
the area under the receiver operating characteristic roc	classification metrics area under roc	0.500000
output by the given columns if specified	sql data frame writer bucket by	0.500000
:func awaitanytermination()	sql streaming	0.010204
save this model to the	mllib kmeans model save	0.500000
globals names read or written to by codeblock	core cloud pickler extract code globals cls	1.000000
rdd of key-value pairs (of form	rdd	0.009174
to	streaming	0.015075
of labels corresponding to indices to be assigned	ml string indexer model labels	0.066667
of columns that make up each block	mllib linalg block matrix cols per block	0.333333
returns	sql data	0.073171
load a java model	loader load java cls sc	0.200000
add a py or zip	spark context add py	0.166667
save a	model save	0.200000
inputcols	inputcols	1.000000
particular job group if	status tracker get job ids for group jobgroup	0.500000
matrix other from this	linalg	0.022222
returns an mlwriter instance	java mlwritable write	0.200000
value of	ml stop	1.000000
previously saved using l{rdd saveaspicklefile} method	spark context pickle file name minpartitions	0.250000
loads a csv	reader csv	1.000000
adds two	linalg	0.022222
column containing a json	sql from json col	0.083333
awaitanytermination() can be used again to	streaming query	0.010526
dataframe in	sql data frame writer	0.011628
a paired	factorization model	0.043478
the executors if	unpersist blocking	0.166667
date which is later than	dayofweek	0.037037
returns the threshold	threshold	0.018182
values of the accumulator's data type	accumulator param	0.038462
test prediction on	tests test prediction	0.333333
dot product of two vectors we support	dense vector dot	0.050000
number of nonzero elements	linalg dense vector num	0.166667
labelcol="label", featurescol="features",	labelcol featurescol	1.000000
the month of a	dayofmonth col	0.031250
of	ml linear	0.066667
count of the rdd's elements in one operation	core rdd	0.003460
boundaries in increasing order for	boundaries	0.142857
model intercept	model intercept	0.500000
much	core	0.003021
saves the content of the :class dataframe	sql data frame writer	0.011628
the embedded params to the companion	transfer params to	0.333333
return a resulting rdd that contains a	rdd cogroup other	0.066667
specified path	path	0.030612
of jobs has completed	completed batchcompleted	0.250000
basic operation test for dstream map	basic operation tests test map	0.333333
with	ml param	0.009524
statistic functions with	stat functions	0.333333
index of the	map partitions with index	0.100000
residuals mse r-squared of model	ml linear regression model	0.166667
the left singular vectors	singular	0.015625
value of	ml param has step	1.000000
in tree including	mllib decision tree model	0.076923
comprised of vectors	mllib random rdds gamma	0.125000
a data source	source	0.105263
the stream	data stream writer	0.041667
of the rdd partitioned using the	rdd partition by	0.062500
values for each numeric columns for each group	grouped	0.035714
:func awaitanytermination() can be used	sql	0.002525
pipeline create and	pipeline from	0.142857
frame boundaries from start	start	0.045455
:func awaitanytermination() can be used again to	sql streaming query manager	0.011905
densematrix >>> dm = densematrix(2 2 range 4	dense matrix repr	0.142857
return an rdd created by piping elements	core rdd	0.003460
specifies the input data	data stream reader	0.200000
to wait for	sql streaming	0.010204
this	has	0.011628
train a gradient-boosted trees model for classification	mllib gradient boosted trees train classifier	0.333333
subset accuracy	subset accuracy	1.000000
terms or	ldamodel	0.034483
to receive accumulator updates in a daemon	update	0.055556
calculates the norm	linalg sparse vector norm	0.133333
the least value of the list	sql least	0.055556
spark sink deployed	addresses storagelevel maxbatchsize	0.045455
threshold threshold in binary	threshold	0.018182
returns the threshold if any	threshold	0.018182
this grid	add grid	0.500000
set sample points	set sample sample	0.333333
count	rdd	0.003058
for a given product and	product	0.029412
sparkcontext which is associated with this streamingcontext	spark context	0.023256
sort the list based on first value	test case sort result based on key	0.333333
specifies the underlying output data source	data frame writer format source	0.333333
of columns that describes the sort order	sql data frame sort cols cols kwargs	0.142857
rdd	rdd partition	0.062500
corresponding to that user	user	0.055556
for user who is running sparkcontext	core spark context spark user	0.250000
submit and test a single script file calling	spark submit tests test script with local functions	1.000000
sets params	set params featurescol labelcol	0.125000
spark configuration	spark conf	0.058824
returns	sql	0.202020
elements from an rdd	core rdd take	0.200000
setparams(self featurescol="features",	ml logistic regression set params featurescol	1.000000
l{statcounter} object that captures the mean variance	rdd stats	0.083333
to wait for	streaming query manager reset	0.011905
sample without replacement	frame sample	0.066667
table from catalog	table tablename	0.083333
create an input stream that	create stream ssc hostname port storagelevel	0.200000
summary of a data	summary	0.024390
values for each key using an associative	key func	0.066667
return a default	core spark	0.010309
of the non-streaming	write	0.071429
again to wait for new	sql streaming query manager	0.011905
sets	product set	1.000000
called when processing	listener on output operation	0.166667
contains a param with a given	params has param	0.019231
the centroids	timeunit	0.051282
infer schema from an rdd of row	sql sqlcontext infer schema rdd samplingratio	0.250000
creates a :class dataframe	session create	0.058824
seed=none): sets params for cross validator	cross validator set params	0.500000
test the partition	core task context tests test partition	1.000000
perform a left outer join of c{self}	left outer join other numpartitions	0.111111
left singular vectors	linalg singular	0.017544
that user	user	0.055556
the input dataset with optional parameters	dataset params	0.333333
the selector type	selector set selector type	0.111111
this	add	0.071429
contents of the :class dataframe to a data	sql data frame writer	0.011628
save	cloud pickler save	0.166667
set the initial value	set initial	0.111111
using the specified partitioner	numpartitions partitionfunc	0.250000
driver as	to local	0.125000
original column	max scaler model original	0.062500
so that :func awaitanytermination() can be used	streaming query manager reset	0.011905
pivot	pivot	1.000000
python broker to map to the	streaming broker	0.200000
of each rdd generated in this dstream	streaming dstream	0.027778
which is	mllib regression metrics	0.090909
name	ml	0.001835
the greatest value of the list	greatest	0.043478
to use for loading	ml java mlreader	0.200000
decode	decoder s	0.500000
string	params has param	0.019231
comprised of vectors containing	mllib random rdds log normal	0.125000
a test	test	0.015152
sub-matrix blocks blockrowindex blockcolindex sub-matrix) that form this	linalg block matrix blocks	0.166667
of write() save	ml pipeline model save	0.166667
that make up each	per	0.166667
rdd of key-value pairs	rdd	0.009174
signed shift	shift	0.142857
obj assume	object size obj	0.040000
year of a given date as integer	sql year	0.050000
dot product of two vectors we support	vector dot	0.050000
by the model's transform method	ml linear regression summary predictions	0.200000
test that the final value of weights	test	0.015152
of	external merger object	0.032258
stores user factors in two columns id and	alsmodel user factors	1.000000
fast version of a heappush	core heappushpop heap	0.142857
how much	core external merger object size	0.032258
terms or words	ldamodel	0.034483
the gaussianmixturemodel	mllib gaussian mixture model	0.062500
resolves a param and validates the	param param	0.100000
which is a	regression metrics	0.083333
list of predicted ratings for input user and	mllib matrix factorization model predict all user_product	0.050000
with extra values from	map extra	0.040000
singular vectors of the singularvaluedecomposition	mllib linalg singular value decomposition v	0.250000
numtopfeature	numtopfeatures	0.111111
or compute the number	mllib linalg indexed row matrix num	0.125000
for new terminations	sql streaming query manager	0.011905
cls	cls	0.238095
value for each original	ml min max scaler model original	0.062500
cost sum of squared distances of points	cost	0.105263
vector to the new mllib-local representation	sparse vector as	0.333333
comprised of	random rdds normal	0.125000
elements in rdds in a sliding window over	value and window windowduration slideduration	0.076923
to tf-idf vectors	mllib idfmodel transform x	0.142857
this instance is of type	ml ldamodel is distributed	0.066667
a param and validates the	param param	0.100000
worker process after the fork()	worker sock	1.000000
norm of	vector norm	0.055556
params for this	params	0.006623
compute the dot product of two vectors	dense vector dot other	0.050000
thread such as the spark	spark context	0.023256
estimated coefficients and intercept	generalized linear regression training summary	0.142857
the correlation of	col1 col2 method	0.055556
a python parammap into a	to	0.007692
java array of given java_class type useful for	ml java wrapper new java array pylist java_class	0.333333
again to	query	0.010753
the left singular	singular	0.015625
called when processing of a job of a	listener on output operation	0.166667
inside brackets	brackets	0.058824
again to wait for new	sql	0.002525
the square root	root	0.071429
load a java model	mllib java loader load java cls sc	0.200000
list of column	sql	0.005051
which is a risk function corresponding to	regression metrics	0.083333
train the model on the	mllib streaming linear regression with sgd train on	0.333333
the model	model	0.027933
an rdd of row or	rdd samplingratio	0.200000
dot product of two vectors	dense vector dot other	0.050000
copy of the rdd partitioned using the	rdd	0.003058
libsvm format into label indices	mllib mlutils parse libsvm	0.125000
comprised of vectors containing i	random rdds poisson vector	0.125000
this obj assume	external merger object size obj	0.040000
based on a	core spark context get	0.333333
to a boolean if possible	param type converters to boolean	0.250000
:py attr handleinvalid	handle invalid value	1.000000
queries so that :func awaitanytermination() can	sql streaming	0.010204
rdd is checkpointed and	rdd is checkpointed	0.166667
attr lda keeplastcheckpoint is set	ldamodel get	0.066667
based on	context	0.022727
single script file calling a global	script with local functions	0.125000
list of indices to	ml chi sq selector model	0.166667
of deserialized batches lists of	without unbatching	0.125000
sparkcontext which is associated with this	context spark	0.083333
that :func awaitanytermination() can	sql streaming	0.010204
or multiclass classification	data numclasses categoricalfeaturesinfo	0.250000
spark sink	maxbatchsize	0.037037
multiplies this blockmatrix by other,	multiply	0.100000
pop	heapreplace heap	1.000000
the objects	core external	0.016129
a class generated by namedtuple	core load namedtuple name fields	0.333333
compute the dot product of	linalg dense vector dot other	0.058824
how data	data stream	0.028571
large dataset and an item approximately	lshmodel approx nearest neighbors dataset	0.166667
infer schema from an rdd of	infer schema rdd samplingratio	0.250000
next memory limit if the	next limit	0.200000
count	core rdd	0.003460
so that :func awaitanytermination()	sql streaming query	0.011765
for the spark	spark	0.013158
be placed	modules	0.166667
iterator	iterator key reverse	0.200000
soundex encoding	soundex	0.043478
directory that contains	directory cls	1.000000
an rdd comprised of vectors	mllib random rdds log normal vector rdd	0.166667
param type	param type	0.333333
create a new hivecontext for testing	hive context create for testing cls	0.333333
of the :class dataframe to a	frame	0.034483
frame but not in another frame	frame subtract other	0.333333
model trained	model	0.005587
adds a term to this accumulator's	accumulator add term	0.066667
sort the list based	sort result based	1.000000
and incrementing as	core task context	0.100000
correlation of two columns of a dataframe as	sql data frame corr col1 col2 method	1.000000
a single	offset	0.021739
operation test for dstream groupbykey	operation tests test	0.111111
contents of the :class dataframe	frame writer save	0.066667
that	size	0.009174
comprised of vectors containing i i	random rdds exponential	0.125000
gets	params get estimator	1.000000
the given parameters in this grid to	grid builder add grid	0.100000
heappushpop	heappushpop	0.833333
rdd which is assumed to	rdd	0.003058
can be used again to	manager reset	0.011905
of this instance with a	ml one vs rest	0.052632
at	compression	0.071429
file system using the old hadoop outputformat api	hadoop dataset conf keyconverter valueconverter	0.083333
into a java	map to java	0.250000
the correlation	corr col1 col2 method	0.055556
inputformat with arbitrary key	inputformatclass keyclass valueclass	0.125000
parameter	parameter	1.000000
and vector	vectors	0.083333
which is a risk function corresponding to the	regression	0.020000
a function to	f	0.031579
minute	minute	0.200000
context to use for saving	ml mlwriter context	1.000000
this blockmatrix by other, another blockmatrix	mllib linalg block matrix multiply other	0.200000
python rdd of key-value pairs (of	rdd save as	0.038462
of the :class dataframe to the	sql data frame	0.005348
returns a dummy params instance used as a	param params dummy	0.111111
of the month which the given date	date	0.037037
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	regressor	0.043478
weights for each tree	ml tree ensemble model tree weights	0.333333
set the initial value of weights	initial weights	0.250000
an input option for	stream reader option key value	0.333333
column of the current [[dataframe]] and	sql grouped data pivot pivot_col values	0.050000
:py attr solver	solver value	1.000000
value to an int if possible	param type converters to int value	0.250000
defined as the square root of	root	0.035714
comprised of vectors containing i i d samples	mllib random rdds log	0.125000
an rdd containing all pairs	core rdd	0.003460
wait for the execution to	termination or timeout timeout	0.125000
boundaries defined from start inclusive to end inclusive	start end	0.090909
sparkcontext is initialized or	core spark context ensure initialized	0.333333
temporary table in the	table	0.031250
java pipelinemodel create and return	pipeline model from java	0.142857
or transform the rdd of document to rdd	mllib hashing tf transform document	0.166667
get or create global taskcontext	core task context get or create	0.250000
0 means 1 leaf	decision	0.052632
convert this distributed model to a	ml distributed ldamodel to	0.166667
a given string	ml param params	0.013699
wait	streaming query	0.010526
of	ml string	0.333333
stream foreachrdd get offsetranges	stream foreach get offset ranges	1.000000
defines the ordering columns in a :class windowspec	sql window spec order by	1.000000
an int	int	0.142857
metadata information	metadata	0.500000
dot product of two vectors we	vector dot other	0.050000
table in the catalog	table	0.031250
instance contains a param with	param params has	0.019231
the minutes of a given date	sql minute	0.050000
the ids of all active	active stage ids	0.200000
internal use only create a	context create	0.083333
as the	as	0.037037
perform a right outer join of c{self}	full outer join	0.111111
py or zip dependency for all tasks	py file path	0.066667
to by codeblock	cls	0.047619
private java model produced by a classifier	java classification model	0.333333
or two	linalg	0.044444
basic operation test for dstream groupbykey	streaming basic operation tests test group by key	1.000000
adds an input	stream	0.017544
saves the content of the :class dataframe	data frame writer	0.028169
the given parameters in this grid to	ml param grid builder add grid	0.100000
that :func awaitanytermination() can be used again to	query manager reset	0.011905
or transform the	hashing tf transform	0.045455
much of memory for this obj assume	core external merger object size obj	0.040000
prior	prior	0.833333
this instance with a randomly generated	validation split model	0.200000
fp-growth model	fpgrowth train cls data minsupport numpartitions	0.200000
index keys are categorical feature indices column indices	vector indexer model category maps	1.000000
of users for	users	0.066667
checkpointed version of this dataset checkpointing	checkpoint eager	0.250000
of numerical columns	quantile col probabilities relativeerror	0.166667
this instance	ml param params has param	0.019231
importance of each feature	ml random forest regression model feature importances	0.250000
all	merger object size	0.032258
list of terms to term frequency vectors or	hashing tf	0.125000
total	model total	1.000000
for the stream query if this	sql data stream	0.031250
the spark sink deployed	maxbatchsize	0.037037
matrix from the new mllib-local	matrices from ml	0.333333
function to each element	map f preservespartitioning	0.200000
then merges them with extra	map extra	0.040000
a left outer join	rdd left outer join	0.111111
and c{other}	join other numpartitions	0.071429
a list of conditions and returns one of	sql column otherwise	0.050000
loads a csv file stream and	stream reader csv path	0.500000
even if users construct taskcontext instead of using	task context new cls	0.333333
of int containing elements from start to	core spark context range start	0.090909
comprised of vectors containing i	mllib random rdds poisson	0.125000
item by key out of a dict	item key	0.250000
vector columns in an input dataframe to the	vector columns from	0.142857
sets	cols set	1.000000
to_profile passed in a profile object is returned	profiler profile func	0.200000
pearson's chi-squared goodness of fit test of the	mllib stat statistics chi sq test	0.166667
set the selector type of the	selector type	0.100000
socket	socket	1.000000
sliding window	value and window windowduration slideduration	0.076923
from one base to	frombase tobase	0.333333
:class pyspark sql types longtype column named	sql sqlcontext range start end step numpartitions	0.083333
return	standard scaler model	0.090909
forget about past terminated queries so	sql streaming query manager reset terminated	0.200000
saves the content of the dataframe	data frame writer	0.014085
week number of a given date as integer	weekofyear col	0.055556
return	py spark streaming	0.333333
double data type representing double precision floats	double type	1.000000
cold	cold	1.000000
can be used again to wait	streaming query	0.010526
for the given table/view in the specified	tablename	0.043478
computes an fp-growth model that contains frequent itemsets	mllib fpgrowth train cls data	0.100000
use for saving	ml java mlwriter	0.200000
nodes summed over all trees in	nodes	0.074074
merges them with extra values	map extra	0.040000
clazz	clazz	1.000000
the libsvm format into an rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures minpartitions	0.125000
onevsrest	one vs rest	0.034483
converts matrix columns in an input dataframe from	mlutils convert matrix columns	0.083333
depth	depth	0.555556
:py attr thresholds	thresholds value	1.000000
return sparkcontext which	streaming streaming context spark	0.083333
setparams(self featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01	mixture set params featurescol predictioncol k probabilitycol	0.500000
bandwidth of each sample	bandwidth bandwidth	0.125000
test that the	mllib streaming kmeans test test	0.500000
rdd an rdd	mllib power iteration clustering train cls rdd	0.250000
instance	ml param params	0.013699
are the left singular	mllib linalg singular	0.017544
a :class dataframe from an :class rdd,	schema samplingratio verifyschema	0.029412
to drop the names of fields in	sql	0.002525
columns in an input dataframe	columns from	0.125000
python topicandpartition to map to the	and partition init topic partition	0.055556
much of memory for this	object	0.027778
so that	query manager	0.011905
:class decisiontreeregressor	decision tree regression	1.000000
this model	kmeans model	0.272727
to term frequency vectors or transform the rdd	hashing tf transform	0.045455
sets	vector indexer set	1.000000
given a java pipelinemodel create and	pipeline model from java cls	0.200000
attach docstring from func	wrapped	0.142857
:class statcounter members as	stat counter as	0.333333
squared distance from a sparsevector or 1-dimensional numpy	squared distance other	0.153846
the model	logistic regression	0.040000
sort the list based on first value	case sort result based on key	0.333333
access fields by name or slice	sql struct type getitem key	0.200000
sparkcontext at least	spark context	0.023256
converts vector columns in an input dataframe from	mllib mlutils convert vector columns to	0.166667
multiclass classification model	classification model	0.166667
timezone returns	sql	0.002525
of indices and	ml linalg	0.030303
the content of the dataframe in	data frame writer	0.014085
instance with a randomly generated	cross validator	0.045455
datasets	dataseta	0.142857
a configuration	spark conf	0.058824
property set in this	property key	0.066667
a batch of jobs has started	batch started	0.333333
computes column-wise summary statistics for the input	statistics col	0.200000
matrix from the new mllib-local representation	mllib linalg matrices from	0.333333
in one	core rdd	0.003460
awaitanytermination() can be used again to wait for	streaming query	0.010526
addresses	addresses	1.000000
stepsize step size to	step size	0.125000
compute the number of rows	matrix num rows	0.200000
accumulator's	core	0.003021
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online",	init featurescol maxiter seed	0.250000
the spark fair	spark context	0.023256
ignore separators inside	sql ignore	0.333333
to term frequency vectors or transform the rdd	mllib hashing tf transform	0.045455
of the :class dataframe to	frame writer save path	0.066667
should have been	loader	0.125000
add two values	param add	0.250000
of the :class dataframe	frame	0.034483
lda keeplastcheckpoint	ldamodel	0.034483
kmeans	kmeans	0.179487
function	f	0.105263
comprised of i i d samples from	mllib random rdds	0.041667
day of the month of	dayofmonth col	0.031250
matrix columns in an input	matrix columns to	0.142857
columns in	columns from	0.125000
partitions to use during reduce tasks (e g	default reduce partitions	0.166667
identifier for the spark application	core spark context application	1.000000
runs the bisecting k-means algorithm return the	mllib bisecting kmeans train rdd k maxiterations mindivisibleclustersize	0.333333
to use for loading	mlreader	0.111111
prints the first n rows to the console	show n	0.333333
a new accumulator with	accumulator init aid	0.083333
add a py or zip	add py file path	0.166667
hadoop file system using the old hadoop	as hadoop	0.142857
weights computed	weights	0.066667
of the singularvaluedecomposition	value decomposition	0.200000
find all globals names read or written to	extract code globals	0.125000
each rdd is generated by applying mappartitionswithindex() to	map partitions with index f preservespartitioning	0.055556
sets the spark session to use for	session sparksession	0.083333
with the dispatch to handle all function	cloud pickler save function obj	0.142857
parses a column containing a json	json col	0.083333
the number of	row matrix num	0.100000
sets the accumulator's value only usable in driver	accumulator	0.012987
create a dense vector of 64-bit floats from	ml linalg vectors dense	0.166667
accumulator's	add	0.035714
setparams(self featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01	set params featurescol predictioncol k probabilitycol	0.500000
resulting rdd that contains a tuple with	core rdd cogroup	0.066667
outcomes for k classes classification problem in	classes	0.034483
ml params instances for the given param and	params m1	0.047619
of this instance	ml pipeline	0.047619
batches of data from a	streaming linear algorithm	0.076923
path a shortcut of write() save path	ml pipeline model save path	0.200000
model scale	model scale	1.000000
append	append	1.000000
pipelinemodel create and return a python wrapper of	ml pipeline model	0.066667
converts matrix columns in	mlutils convert matrix columns to ml dataset	0.166667
which each rdd contains the count of	count by	0.100000
of this :class dataframe as pandas pandas	to pandas	0.166667
can be used	sql streaming query manager reset	0.011905
dump the profile stats into directory path	spark context dump profiles path	1.000000
the column	scaler	0.105263
sets	classification evaluator set	1.000000
can be used again to wait	query	0.010753
squared distance from a sparsevector or	linalg sparse vector squared distance	0.166667
featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10	featurescol maxiter seed	0.200000
a single script file	single script	0.250000
sets	standard scaler set	1.000000
to	sql streaming query manager reset	0.011905
set initial centers should be	initial centers centers	0.200000
deployed on a flume	flume	0.071429
to be placed into	modules to	1.000000
mixtures using the expectation-maximization algorithm	mixture	0.052632
as a text file	as text file path	1.000000
return sparkcontext	spark	0.013158
convert a list of column or	to	0.007692
this obj assume that all the objects	obj	0.023810
a streaming dataframe/dataset is written to a	writer output mode outputmode	0.083333
test a single script on	tests test single script on	0.500000
defines the	spec	0.076923
:py attr p	p value	1.000000
dstream by applying reducebykey to each	streaming dstream reduce by key func numpartitions	0.076923
set numtopfeature for	numtopfeatures	0.111111
which is a risk	mllib regression metrics	0.090909
rdd is generated by applying mappartitionswithindex()	map partitions with index f preservespartitioning	0.055556
find synonyms of	find synonyms	0.333333
passed as a list	core spark conf set all	0.125000
return sparkcontext which	spark	0.013158
this rdd and its recursive dependencies	core rdd	0.003460
app name should be set	appname sparkhome pyfiles	0.500000
name for column of predicted clusters	ml clustering summary prediction col	0.111111
grouping	grouping	1.000000
for approximate distinct count of col	approx count distinct	0.071429
queries so that :func awaitanytermination()	streaming query manager	0.011236
the ids of all active stages	get active stage ids	0.250000
the driver returns none	driver	0.090909
sets the given parameters in this grid	builder add grid	0.100000
load a model from	svmmodel load cls sc	0.200000
convert a value to an int	to int	0.250000
python topicandpartition	and partition init topic partition	0.055556
uid	uid	0.750000
all trees in the ensemble	tree ensemble	0.111111
and count of the rdd's elements	core	0.003021
support vector machine on the	svmwith sgd	0.333333
value for each original column during	scaler model original	0.062500
a python topicandpartition to map to the	init topic partition	0.055556
of columns that make up each	cols per	0.333333
compute the dot product of two	linalg dense vector dot	0.058824
of memory	merger object	0.032258
create an input from tcp source	streaming context socket text stream	0.500000
set the trigger	writer trigger	0.111111
return an rdd containing	rdd	0.003058
compute the dot product of	dense vector dot	0.050000
for ml instances that provide :py class javamlwriter	java mlwritable	0.500000
area	area	0.636364
sci	sci	0.714286
test that the	sgdtests test	0.142857
class from an	rdd	0.006116
the number of fields	sql struct type len	0.200000
withmean	withmean	1.000000
return sparkcontext which is associated with this	streaming context spark	0.083333
class dataframe that with new specified	data frame to df	0.090909
jobs started by this thread until the group	job group	1.000000
values from this instance to another instance	values to extra	0.333333
the centroids of that	timeunit	0.025641
batch has completed	completed	0.058824
python code	code name doc	0.111111
a jvm seq of	seq sc cols converter	0.055556
values for each key using an	by key	0.026316
of :class pyspark sql	sql to	0.083333
dataset and an	dataset key numnearestneighbors distcol	1.000000
model	quantile discretizer create model	1.000000
returns a paired rdd where the first	factorization model	0.043478
memory for this	merger object	0.032258
rdd of key-value	core rdd	0.010381
mixin for param rawpredictioncol raw prediction	has raw prediction	0.333333
or compute the number of rows	mllib linalg block matrix num rows	0.200000
sort the list based on first value	case sort result based on	0.333333
again to wait for new	streaming query manager reset	0.011905
string	has param	0.019231
comprised of i i d samples	random rdds	0.038462
params	params featurescol	0.500000
to in this model	bisecting kmeans model	0.250000
selector	chi sq selector set selector	0.333333
local property set in	context get local property key	0.066667
basic operation test for dstream countbyvalue	streaming basic operation tests test count by value	1.000000
creates a :class dataframe from an :class	spark session create	0.058824
returns a new java	wrapper new java	0.166667
using the old hadoop outputformat api mapred package	as hadoop dataset conf keyconverter valueconverter	0.083333
a py or zip dependency for all tasks	py file	0.066667
model derived from a least-squares fit	model	0.005587
another dstream with	other	0.033333
week number of a given	sql weekofyear col	0.055556
the values for each key using an	key func	0.066667
given columns on	sql data frame	0.010695
test that the model predicts correctly on	mllib streaming kmeans test test predict on model	0.500000
the number	linalg row matrix num	0.100000
termination of	termination	0.035714
a param with a given string	ml	0.001835
fp-growth model that contains frequent itemsets	fpgrowth train cls data	0.200000
with a given	has param	0.019231
recover	recover	0.714286
create an input stream that pulls events	utils create stream	0.200000
returns the receiver operating characteristic roc	summary roc	0.333333
elements from an rdd ordered in ascending	rdd take ordered	0.050000
all values as a list of	all	0.083333
multiple parameters passed as	conf set all	0.250000
that :func awaitanytermination() can	reset	0.011236
java pipeline	pipeline from java	0.142857
replaces a	replace	0.142857
raw prediction scores into 0/1 predictions	mllib linear classification model	0.142857
__init__(self	init	0.765957
train a decision tree model	mllib decision tree train	0.333333
this imputer	ml imputer	0.250000
of column or names into a jvm seq	seq	0.043478
sparkcontext is initialized	spark context ensure initialized	0.333333
vector columns in	vector columns from ml	0.142857
calculates the norm of a	linalg sparse vector norm p	0.066667
iterations	iterations	0.391304
the objects	external merger object size	0.032258
parses the	sql expr str	0.125000
bisecting k-means algorithm	bisecting kmeans train	0.500000
instance contains a param with a given string	ml	0.001835
of a dataframe	data frame	0.005000
calculates the length of a string or	sql length col	0.050000
extra param values	extra	0.023810
norm of a sparsevector	norm p	0.055556
for new terminations	sql	0.002525
for binary or multiclass classification	classifier cls data numclasses categoricalfeaturesinfo	0.250000
value	accumulator	0.012987
day in utc returns another timestamp that corresponds	sql from utc timestamp timestamp tz	0.166667
ml params instances for the	params	0.006623
category	category	1.000000
context to	streaming context	0.055556
table	table mode	0.200000
@param input dataset for the test this should	case test func input func expected sort	0.333333
inside brackets pairs	brackets	0.058824
params instances	params	0.006623
parse a field in schema abstract >>> _parse_field_abstract("a")	parse field abstract s	1.000000
this tests a	tests	0.100000
which is a risk function	regression summary	0.035714
field in :py attr predictions which gives the	linear regression summary	0.013889
rdd[vector]	java vector	1.000000
batch has started	started	0.055556
the stream query if this	stream	0.017544
stream query if this is not set	stream	0.017544
function on rdds of the dstreams	context transform dstreams transformfunc	0.125000
streaming dataframe/dataset is written	writer output mode outputmode	0.083333
context to use for loading	mlreader context	1.000000
or transform the rdd of document to	hashing tf transform document	0.166667
for every	mllib linear	0.166667
this instance is	ldamodel is	0.200000
calculates the approximate quantiles	approx	0.047619
get a local property set in	context get local property key	0.066667
comprised of vectors containing	mllib random rdds poisson	0.125000
m1	m1	1.000000
trainratio=0 75 seed=none):	trainratio	0.142857
get depth of tree (e g depth	tree	0.020833
threshold that	threshold	0.018182
set bandwidth of each sample defaults to	stat kernel density set bandwidth bandwidth	0.142857
2 ml params	params m1	0.047619
with a given string name	params	0.006623
to set k decayfactor timeunit to configure the	streaming	0.005025
centroids according to	decayfactor timeunit	0.250000
a sparse vector using either a dictionary	linalg vectors sparse size	0.166667
an input stream that pulls events	stream ssc hostname port storagelevel	0.200000
of memory for this	merger object size	0.032258
set a local property	context set local property key value	0.200000
returns the soundex encoding for a	soundex col	0.055556
by	by key func	0.062500
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1	init featurescol labelcol predictioncol probabilitycol	0.333333
heappush	heappush	1.000000
model to make predictions on batches of data	algorithm predict on	0.066667
a	has param	0.038462
field in "predictions" which gives the features	linear regression summary features	0.333333
probabilitycol="probability", tol=0 01 maxiter=100	probabilitycol	0.050000
create a multi-dimensional rollup for the	frame rollup	0.055556
converts matrix columns in an input dataframe to	mllib mlutils convert matrix columns from ml dataset	0.166667
get or compute	linalg indexed row matrix	0.250000
two separate arrays of indices	ml linalg	0.030303
already partitioned	group	0.025641
a java parammap into a	java	0.012195
new dstream in which each rdd is generated	streaming streaming	0.047619
of offsets from	offset	0.021739
a gradient-boosted trees	mllib gradient boosted trees	1.000000
ranking algorithms	ranking	0.125000
used again to wait for new terminations	sql streaming	0.010204
values of the accumulator's data	accumulator param	0.038462
setparams(self	ml fpgrowth set params	1.000000
whose columns are the right singular vectors of	mllib linalg singular	0.017544
new dstream in which each rdd	by	0.014286
of	ml bisecting	0.133333
collect the distributed matrix on	matrix	0.015152
unifying data of another dstream with	union other	0.333333
__init__(self labelcol="label",	regression init labelcol	1.000000
be used	query	0.010753
used	manager reset	0.011905
inherit documentation	mllib inherit doc	0.045455
random forest <http //en wikipedia org/wiki/random_forest>_	random forest regressor	1.000000
the given parameters in this grid	builder add grid	0.100000
add a py or zip	add py	0.166667
used again to wait for new	streaming	0.005025
be used again to wait for	sql streaming query manager	0.011905
two-sided p-value of estimated coefficients and intercept	ml generalized linear regression training summary p values	0.333333
for loading	java mlreader	0.250000
local representation this discards info about the	local	0.038462
compute the number of	block matrix num	0.100000
create a new spark configuration	core spark conf init loaddefaults _jvm _jconf	0.250000
batches of data	mllib streaming linear algorithm	0.166667
the maximum	max	0.071429
string name	params has param	0.019231
num	num	0.050420
by	by key func numpartitions	0.062500
submit and test a single script file	spark submit tests test single script	0.500000
with a given	ml param params	0.013699
the model to make predictions on batches of	algorithm predict on	0.066667
convert this vector	linalg sparse vector	0.111111
users the	users	0.066667
saves the	format mode partitionby	0.200000
compute aggregates and	agg	0.166667
used again	query	0.010753
:class dataframe to a	frame writer save	0.066667
onevsrest create and return	one vs rest	0.034483
to a local representation this	to local	0.125000
items for columns possibly with false	items cols support	0.125000
so that :func awaitanytermination() can be	streaming query manager reset	0.011905
pipeline create and return a python	pipeline from	0.142857
param with	params has param	0.019231
how much of memory for this	external merger object size	0.032258
stop the	streaming context stop	0.125000
path	path path	0.333333
wait for new	reset	0.011236
possible outcomes for k classes classification problem in	classes	0.034483
for the given table/view in the specified database	tablename dbname	0.142857
of a job of a batch has started	started	0.055556
save this model to the	mllib naive bayes model save	0.500000
for which	ml isotonic regression model	0.125000
rdd containing the distinct	distinct numpartitions	0.142857
weightcol=none aggregationdepth=2):	linear svc	0.285714
comprised of	random rdds log	0.125000
added through c{sparkcontext addfile()}	core spark files get cls filename	0.200000
used in sql statements	name f returntype	0.125000
value of	ml kmeans	0.500000
datasets to	dataseta	0.142857
convert this vector to the new mllib-local representation	sparse vector as ml	0.333333
corr	corr	1.000000
underlying output	data stream writer format	0.333333
objects	external merger object size	0.032258
vector columns in an input	vector columns from ml dataset	0.142857
with a given	params	0.006623
__init__(self mintokenlength=1 gaps=true pattern="\s+", inputcol=none outputcol=none tolowercase=true)	tokenizer init mintokenlength gaps pattern inputcol	1.000000
note : experimental	multiclass classification evaluator	0.500000
single script on a cluster	single script on cluster	0.500000
elements in iterator	iterator key	0.200000
external	external	0.111111
calculates the md5 digest and	md5 col	0.333333
function on each rdd of this	transform func	0.058824
mixin for param featurescol features	has features	1.000000
kmeans algorithm for fitting and predicting on	kmeans	0.025641
which predictions are	regression	0.010000
inherit documentation from its parents	inherit	0.037037
how	merger object	0.032258
string column from one base to	conv col frombase tobase	0.166667
level to persist its values across operations after	persist storagelevel	0.166667
densevector with singular values in descending order	mllib linalg singular value decomposition s	0.250000
the given parameters in this grid	grid builder base on	0.076923
incrementing as	task context	0.142857
given string name	has param	0.019231
given string	params	0.006623
objective function scaled loss + regularization at	training summary objective history	0.500000
loading	java mlreader	0.250000
test predicted values on a toy	logistic regression with sgdtests test	0.111111
creates or replaces	create or replace	0.500000
pyspark sql types longtype column named	sql sqlcontext range start end step numpartitions	0.083333
a local property that affects	local property key value	0.076923
__init__(self mintokenlength=1 gaps=true pattern="\s+", inputcol=none outputcol=none tolowercase=true)	init mintokenlength gaps pattern inputcol	1.000000
sets	ml vector indexer set	1.000000
instance contains a param with a given	ml param params has param	0.019231
number of nonzero elements this	mllib linalg dense vector num	0.200000
using the specified partitioner	by numpartitions partitionfunc	0.250000
statistic functions	stat	0.076923
a column containing a json	from json col	0.083333
returns the precision-recall curve which is	ml binary logistic regression summary pr	0.083333
:class datatype the data type string format equals	datatype string s	0.111111
whose columns are the left singular vectors	linalg singular	0.017544
param with a given string	ml	0.001835
generates python code	code name doc	0.111111
this tokenizer	tokenizer	0.125000
from an :class rdd, a list or	schema samplingratio verifyschema	0.029412
be used with the spark sink	addresses storagelevel maxbatchsize	0.045455
the returned	test case	0.333333
string format equals	string	0.041667
converts matrix columns in an input dataframe to	mllib mlutils convert matrix columns from	0.166667
optional default value and user-supplied value in	param params	0.014925
the index of the original partition	with index	0.100000
compare 2 ml types asserting	persistence test compare	0.166667
so that :func awaitanytermination() can be used again	sql streaming query manager reset	0.011905
another value	value subset	0.200000
bisecting k-means algorithm return the	bisecting kmeans train rdd	0.333333
create an rdd for	spark session create	0.117647
create an input stream that pulls	create stream ssc hostname port	0.200000
partitioned data	group	0.025641
python broker to map to	streaming broker	0.200000
converts matrix columns	convert matrix columns to ml dataset	0.166667
find synonyms of a word	mllib word2vec model find synonyms word num	1.000000
dump already partitioned data	external	0.013889
by applying a function to each partition	f	0.010526
in rdd 'x' to all mixture	mixture	0.052632
the output with optional parameters	params	0.006623
for this obj assume	external merger object size obj	0.040000
schema in the tree	schema	0.033333
compute the dot product of two	vector dot	0.050000
kolmogorov-smirnov ks test for	mllib stat statistics kolmogorov smirnov test	0.166667
given a large dataset and an item approximately	lshmodel approx nearest neighbors dataset key numnearestneighbors distcol	0.166667
the accumulator's data type returning	accumulator	0.012987
mean variance and count	rdd	0.003058
ui	ui	1.000000
by the model's transform method	linear regression summary predictions	0.200000
training set given the current parameter estimates	training	0.029412
java object by pyrolite whenever	java object rdd rdd	0.500000
a range of	offset range	0.047619
multiplies this blockmatrix by other, another blockmatrix	linalg block matrix multiply other	0.200000
of partitions default 1 use a small	partitions numpartitions	0.500000
return sparkcontext which is	context spark	0.083333
checkpointinterval=10 impurity="variance",	decision tree regressor	0.058824
locally	locally	0.416667
the minutes of a given date as	minute col	0.050000
set bandwidth of each sample defaults to 1	set bandwidth bandwidth	0.142857
indicates whether this instance is of type distributedldamodel	ml ldamodel is distributed	0.066667
to consist of	ascending numpartitions keyfunc	0.100000
calculates the norm of	ml linalg dense vector norm p	0.333333
of rows	rows	0.272727
of nodes summed over	nodes	0.037037
get the cluster centers represented as	model cluster centers	0.090909
at	at	1.000000
dot product of two vectors we support	linalg dense vector dot	0.058824
the dot product of two vectors	linalg dense vector dot other	0.058824
contains the count of distinct elements in	count	0.016949
function without changing the keys this also	values f	0.062500
this obj assume that all the objects	object size obj	0.040000
sparse vector using either	mllib linalg vectors sparse	0.166667
of sub-matrix blocks blockrowindex blockcolindex sub-matrix) that form	linalg block matrix blocks	0.166667
values	standard scaler	0.076923
or buffer	array_like dtype	0.166667
get or create global	get or create cls	0.200000
which is a risk function corresponding to the	regression metrics	0.083333
two vectors	linalg	0.022222
that :func awaitanytermination() can be used	manager reset	0.011905
java	model from java	1.000000
convert	to	0.015385
a parquet file stream returning the	stream reader parquet path	0.083333
comprised	random rdds exponential vector	0.125000
cluster centers represented as a list of	mllib bisecting kmeans model cluster centers	0.083333
train the model	regression with sgd train	1.000000
back to	index to	0.040000
given string name	param params has param	0.019231
convert this vector to the new mllib-local representation	linalg vector as ml	1.000000
deviance for the null model	linear regression summary null deviance	0.250000
on a model with weights already	linear regression with	0.111111
of this instance with a randomly	ml one vs rest	0.052632
from	from ml mat	0.500000
train the model on the	streaming kmeans train on	0.333333
partial objects do	core cloud pickler save partial obj	0.125000
the column standard	mllib standard scaler model	0.100000
probability of obtaining a test statistic	stat test	0.166667
to	accumulator add	0.076923
predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0	predictioncol maxdepth	0.250000
underlying output	stream writer	0.041667
min number of partitions for hadoop rdds	min partitions	0.200000
set the selector type	chi sq selector set selector type	0.111111
in the rdd into	rdd	0.003058
an optional param map that overrides embedded	params	0.006623
make predictions on batches of data from	algorithm predict on	0.066667
initialweights	initialweights	1.000000
the probability	probability col	0.500000
test that	streaming logistic regression with sgdtests test	0.111111
of columns that make up each	linalg block matrix cols per	0.333333
2 ml params instances for the given param	params	0.006623
or multiclass	numclasses	0.111111
reducebykey	reduce	0.041667
broadcast a read-only variable to the	context broadcast value	0.125000
the levenshtein distance of the two given	levenshtein left	0.058824
lda keeplastcheckpoint is set to	distributed ldamodel	0.052632
separators inside brackets pairs	brackets	0.058824
computes average values for each numeric columns	sql grouped	0.043478
an input stream that pulls	stream ssc hostname port	0.200000
dir	dir	0.714286
the sort	frame sort	0.250000
setparams(self labelcol="label",	generalized linear regression set params labelcol	1.000000
return a new dstream by applying reducebykey	streaming dstream reduce by key func numpartitions	0.076923
rdd of	core rdd save	0.037975
awaitanytermination()	query manager reset	0.011905
loads a text file stream and	stream reader text path	0.333333
returns the threshold if any used	threshold	0.018182
generates an rdd	rdd sc mean	1.000000
jvm scala map from	frame jmap jm	0.111111
loads vectors saved using	load vectors	0.333333
new	manager reset	0.011905
wait	timeout catch_assertions	0.125000
finding frequent items for columns possibly with false	frame freq items cols	0.166667
rdd previously saved using l{rdd saveaspicklefile} method	spark context pickle file name minpartitions	0.250000
awaitanytermination() can be used again to	query manager	0.011905
of	ml ldamodel	0.222222
observed is vector conduct pearson's chi-squared goodness	stat statistics chi sq	0.066667
fits a java model to	ml java estimator fit java	0.333333
values for each numeric columns	grouped data	0.035714
the	size	0.009174
threshold=0 0 weightcol=none aggregationdepth=2):	linear svc	0.285714
for every	model	0.005587
applies standardization transformation on	mllib standard scaler model transform	0.500000
in c{self} that	other numpartitions	0.083333
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online", learningoffset=1024	ml lda init featurescol maxiter seed	0.250000
of the accumulator's data type returning	accumulator param	0.038462
onevsrestmodel create and	one vs rest	0.034483
field in "predictions" which gives the predicted	linear regression summary prediction col	0.142857
to a boolean if possible	param type converters to boolean value	0.250000
column as a :class column	data frame getitem item	0.250000
value pairs or two separate arrays of	ml	0.001835
should be maximized true default or minimized false	ml evaluator is larger better	0.166667
__init__(self	decision tree classifier init	1.000000
minimum number of	min count	0.076923
batches of data	algorithm	0.090909
their nearest center for this model	model compute	0.133333
for all users the	for users	0.500000
user and product	user product	0.250000
set bandwidth of each sample defaults to	mllib stat kernel density set bandwidth bandwidth	0.142857
return sparkcontext which is associated with	spark	0.013158
and count of the	core rdd	0.003460
containing the ids of all active stages	get active stage ids	0.250000
representing the result of the given query	sqlquery	0.027027
a list or gets an	sql column get	0.142857
wait	manager	0.011236
:class dataframe using the specified	sql data frame	0.005348
mergevalue	mergevalue	1.000000
to a string in	to	0.007692
output a python rdd of	core rdd save	0.037975
count of the rdd's elements in one operation	rdd	0.003058
computes column-wise summary statistics	mllib stat statistics col stats rdd	0.200000
boundaries from start inclusive to end inclusive	between start end	0.125000
again to	query manager reset	0.011905
extracts the embedded default param values	param params extract param	0.333333
of c{self} and c{other}	core rdd	0.006920
how much	core external	0.016129
an rdd comprised of vectors containing i	mllib random rdds exponential vector rdd	0.166667
dstreams in this context to	streaming context	0.055556
this bucketizer	bucketizer	0.125000
recovers all the partitions	recover partitions	1.000000
comprised of vectors containing i i d samples	mllib random rdds normal	0.125000
transpose this blockmatrix returns a new blockmatrix	mllib linalg block matrix transpose	1.000000
of labeledpoint	mlutils load lib svmfile	0.125000
the standard deviation	stdev	0.047619
whether this instance is	ldamodel is distributed	0.200000
degrees	ml generalized linear regression summary degrees	1.000000
least value	least	0.043478
local property set in this	core spark context get local property key	0.066667
a pearson's independence test using	chi square test test	0.333333
:py attr lda keeplastcheckpoint is	ldamodel	0.034483
a shortcut of write() save path	ml pipeline save path	0.200000
behavior when data	sql data frame writer mode savemode	0.071429
batch has started	started outputoperationstarted	0.125000
an javardd of object by unpickling it will	ml	0.001835
transform the rdd of document to rdd	transform document	0.250000
the given parameters in this grid to fixed	ml param grid builder base on	0.076923
much of memory for this obj assume that	size obj	0.040000
sparse matrix stored	sparse matrix	0.250000
to convert the java_model to a python model	discretizer create model java_model	0.250000
of	ml java params from	0.250000
sets	has elastic net param set	1.000000
vectors or	tf	0.076923
to a java pipelinemodel used for ml persistence	pipeline model to java	0.100000
"zerovalue"	fold	0.076923
degrees	generalized linear regression summary degrees	1.000000
produced by	clustering	0.066667
predicted ratings for	mllib matrix factorization model predict all user_product	0.050000
get all values as a	spark conf get all	0.166667
new :class column for approximate distinct count of	approx count distinct col	0.071429
setparams(self estimator=none estimatorparammaps=none evaluator=none trainratio=0 75 seed=none):	split set params estimator estimatorparammaps evaluator trainratio	0.500000
outputformat	outputformatclass keyclass	0.250000
correlation of two columns of a	col1 col2 method	0.055556
day of the month	dayofmonth	0.027027
standard deviation of this	core rdd stdev	0.066667
weighted averaged recall	weighted recall	1.000000
into label	mlutils parse	0.250000
the value of spark	get	0.021739
python rdd of key-value pairs (of	rdd	0.009174
onevsrest create	one vs rest	0.034483
forceindexlabel	forceindexlabel	0.714286
vector class for passing	vector	0.019231
of the dataframe	data frame writer	0.014085
have the same param	param	0.006250
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction",	init featurescol labelcol predictioncol probabilitycol	0.333333
param name	name	0.043478
jvm view associated with sparkcontext must be called	ml jvm	1.000000
setparams(self featurescol="features", labelcol="label", predictioncol="prediction",	regression set params featurescol labelcol predictioncol	0.500000
awaitanytermination() can be used	sql streaming query manager reset	0.011905
parses the expression	expr str	0.125000
weights	weights initialweights	0.333333
:class dataframe to the	sql data frame	0.005348
params to the	params to	0.035714
threshold that	threshold value	0.166667
is close	parameter accuracy	0.029412
an associative and commutative reduce function	rdd reduce	0.071429
:py attr mintf	min tf value	1.000000
of the mean squared	mean squared	0.250000
train the model on the	sgd train on	0.333333
returntype	returntype	0.357143
merger	merger	0.153846
window of time over this dstream	streaming dstream window	0.333333
with the frame boundaries defined from	range	0.030303
a right outer join of	full outer join other	0.111111
unicode	unicode	1.000000
the model on	on model	0.166667
after which the centroids of that particular batch	timeunit	0.025641
jobs started by	job	0.023810
iterator of deserialized objects from the input stream	serializer load stream stream	0.333333
an	mllib mllib streaming	0.500000
which is a risk	regression metrics	0.083333
returns a :class dataframe	sql	0.002525
a single kafka topicandpartition	offset	0.021739
load a model from the given path	load cls sc path	0.600000
converter to drop the names of fields	converter	0.052632
tablename	tablename	0.217391
a function on rdds of the dstreams	context transform dstreams transformfunc	0.125000
the model to make predictions on batches of	linear algorithm predict on	0.066667
rdd of document	document	0.040000
converts vector columns in an	mllib mlutils convert vector columns from ml	0.166667
converts vector columns in an input	mllib mlutils convert vector columns to	0.166667
an javardd of object by	ml	0.001835
set initial centers should be set before calling	kmeans set initial centers centers weights	0.200000
queries so that	manager	0.011236
attr lda keeplastcheckpoint is set	ml distributed ldamodel get	0.066667
test	tests test map	1.000000
a python topicandpartition to map	topic and partition init topic partition	0.055556
checkpointed and materialized either	checkpointed	0.083333
note this docstring is not shown publicly	mllib linalg coordinate matrix init entries numrows numcols	0.333333
converts vector columns	convert vector columns to ml dataset	0.166667
test that	logistic regression with sgdtests test	0.111111
a new rdd containing the distinct	distinct numpartitions	0.142857
an fp-growth model that contains frequent	mllib fpgrowth train cls	0.100000
month of	dayofmonth	0.027027
data against the expected	expected	0.076923
saves the content of	format mode partitionby	0.100000
value of	ml	0.341284
saves the contents of	mode partitionby	0.066667
restore an object	restore	0.125000
for binary or multiclass	data numclasses	0.250000
shortcut of write() save	ml mlwritable save	0.166667
comprised of vectors containing i	mllib random rdds normal vector	0.125000
qr decomposition of	tall skinny qr computeq	0.500000
broadcast on the	broadcast	0.052632
a batch has started	started	0.055556
roc	roc	0.500000
in one	rdd	0.003058
new dstream in which each rdd contains the	by	0.014286
perform a right outer join	rdd full outer join other numpartitions	0.111111
this	rdd	0.003058
"zerovalue" which may be added to the result	fold	0.076923
convert python list to java type array	to jarray gateway jtype arr	0.500000
the soundex encoding	sql soundex	0.055556
that all the objects	external merger object	0.032258
creates an external table based on the	sql sqlcontext create external table tablename path	0.250000
make predictions on a keyed	predict on values	1.000000
each original column during	model original	0.062500
input param	param	0.006250
of the singularvaluedecomposition if computeu was set to	value decomposition u	0.100000
length	length	0.240000
mark this	core	0.003021
ridge	ridge	1.000000
create a sparse vector using either	linalg vectors sparse	0.166667
convert to sparsematrix	matrix to sparse	1.000000
a function	user defined function	0.066667
returning the result as	reader	0.040000
a shortcut of write()	ml one vs rest	0.052632
the values for each key	by key	0.026316
of nonzero	ml	0.001835
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10	ml lda init featurescol maxiter seed	0.250000
points belongs to in	predict x	0.033898
or compute the number of	linalg row matrix num	0.100000
a large dataset and an item	nearest neighbors dataset key	0.333333
0 1 0] for feature selection by fdr	mllib chi sq selector set fdr fdr	0.200000
returns an mlwriter instance for this	ml mlwritable write	0.200000
instance's params to the	java params to	0.045455
and return its	core	0.003021
outputted by the model's transform method	logistic regression summary predictions	0.200000
dump already partitioned data	core external	0.016129
list of values	values	0.050000
test the python direct kafka stream	stream tests test kafka direct stream	0.625000
java onevsrest create and return a python wrapper	one vs rest from java	0.142857
of the importance of each feature	ml random forest regression model feature importances	0.250000
format into an rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures	0.125000
a batch of	batch	0.068966
value to list of ints if possible	ml param type converters to list int	0.333333
cast	cast	0.833333
:func awaitanytermination() can be used again to	streaming query manager	0.011236
matching keys in c{self} and c{other}	join other numpartitions	0.071429
an input stream	stream ssc addresses	0.166667
events from flume	streaming flume utils	0.200000
an fp-growth model that	mllib fpgrowth train cls data minsupport	0.100000
class for indexing categorical feature columns in	indexer	0.055556
the non-streaming :class dataframe out into	sql data frame write	0.071429
other from this block matrix this	mllib linalg block matrix	0.052632
a :class dataframe	data	0.046512
an fp-growth model that contains	mllib fpgrowth train cls data minsupport numpartitions	0.100000
sample without	frame sample	0.066667
write a :class dataframe	data frame	0.005000
dump the profile stats into directory	core spark context dump profiles	1.000000
streaming	streaming	0.040201
distributed model to a	ml distributed ldamodel to	0.166667
that all	core external	0.016129
get	conf get	1.000000
param with a given string	has	0.011628
get the rdd's current storage level	get storage level	1.000000
corresponding to that user	model user	0.250000
save this model to	mllib kmeans model save	0.500000
instance's params to the wrapped	ml java params to	0.045455
creating rdds comprised	random rdds	0.012821
certain time of day in utc returns	sql from utc	0.142857
to this accumulator's	accumulator	0.012987
a column scipy matrix from a	mllib sci py tests scipy matrix size	0.090909
simple sparse	sparse	0.125000
given columns on the file	sql data frame	0.005348
partial objects	save partial	0.125000
contains a param with	param params has param	0.019231
comprised of vectors containing i i d samples	random rdds exponential	0.125000
local property that	local property key value	0.076923
key using an associative	by key	0.026316
the approximate quantiles of	approx	0.047619
rdd 'x' to all mixture	mixture	0.052632
which is later than the value	dayofweek	0.037037
collect each rdds into the returned list	streaming test case collect dstream n	1.000000
comprised of vectors containing	mllib random rdds normal	0.125000
of columns for the given table/view in	columns	0.019608
dot product of	vector dot other	0.050000
elements	core spark	0.010309
discretizer	discretizer	1.000000
again to wait for new terminations	manager reset	0.011905
distributed model to a local representation this discards	ml distributed ldamodel to local	0.111111
matching	matching replace	0.250000
this	ml	0.003670
until any of	await any	0.142857
minutes of a	sql minute col	0.050000
resulting rdd that contains a tuple	core rdd cogroup other	0.066667
correlation of two columns	method	0.041667
all params with their optionally default values	param params explain params	0.250000
absolute	absolute	1.000000
for feature selection by fdr	chi sq selector set fdr fdr	0.200000
vectors which this transforms	vector	0.019231
featurescol=none labelcol=none predictioncol=none classifier=none): sets params for onevsrest	ml one vs rest set params	1.000000
statistic functions	data frame stat	0.250000
calculates the norm of a	linalg sparse vector norm	0.066667
the content of the :class dataframe in json	sql data frame writer	0.011628
defined as the square root	root	0.071429
return the column standard deviation	standard scaler model std	0.166667
trigger for the stream query	data stream writer trigger	0.083333
a function to each element	f preservespartitioning	0.200000
objective function scaled loss + regularization at	ml logistic regression training summary objective history	0.500000
the accumulator's value only usable	accumulator	0.012987
the kmeans algorithm for	streaming kmeans	0.035714
of write()	ml one vs rest	0.052632
dot product of two vectors	vector dot other	0.050000
extract the minutes of a	sql minute col	0.050000
and refresh all the cached	refresh	0.100000
cost sum	compute cost	0.142857
already partitioned data	group	0.025641
create an	create	0.034483
get all values as a list of	get all	0.166667
find all globals names read or written to	cloud pickler extract code globals	0.125000
quantileprobabilities	quantile probabilities	1.000000
a forked external process	pipe command env checkcode	0.166667
with this spark job on every node	core spark	0.010309
predict values for a single data point	predict x	0.033898
private abstract class representing a multiclass classification model	linear classification model	0.076923
numtopfeature for	numtopfeatures	0.111111
from start inclusive to end inclusive	between start end	0.250000
estimators that	estimator	0.083333
examplepoint	python only	0.500000
create a multi-dimensional rollup for the current	frame rollup	0.055556
single	offset	0.021739
squared distance from a sparsevector or	squared distance	0.142857
product and returns a list	product	0.029412
extract the week number	sql weekofyear col	0.055556
is called by the default implementation of	ml	0.001835
modlist	modlist	0.600000
transforms a java parammap into a python parammap	params transfer param map from java	1.000000
return whether this rdd is checkpointed and materialized	core rdd is checkpointed	0.333333
approximately find at most	lshmodel approx	0.100000
(json lines text format or newline-delimited json	json path	0.100000
for this obj assume that	merger object size obj	0.040000
obj assume	size obj	0.040000
this instance's params to the wrapped java object	ml java params to	0.045455
can be used again to wait for	sql	0.002525
word in	word2vec	0.052632
a dataframe with two fields threshold precision curve	ml binary logistic regression summary precision by threshold	0.166667
memory for this obj assume	obj	0.023810
used again to wait for new	streaming query	0.010526
the area under the precision-recall curve	binary classification metrics area under pr	0.333333
a byte	core framed serializer	1.000000
average values for each numeric	grouped data avg	0.058824
item	key	0.035714
right outer join of c{self} and	core rdd full outer join other	0.200000
java_model to a python model	ml quantile discretizer create model java_model	0.250000
create a new hivecontext for	hive context create for	0.250000
stores	ml alsmodel	0.222222
the embedded params to the	params to	0.035714
rdd of document to rdd of	document	0.040000
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params for	indexer set params maxcategories inputcol outputcol	0.333333
the kolmogorov-smirnov ks test for data	mllib stat statistics kolmogorov smirnov test data distname	0.111111
each point in rdd 'x' to all mixture	mixture model predict	0.125000
computes average values for each numeric columns for	sql grouped data avg	0.058824
sparse vector using either a dictionary a list	vectors sparse size	0.166667
the objects	external merger	0.031250
dataframe from an :class rdd,	schema samplingratio verifyschema	0.029412
partitionfunc	partitionfunc	0.833333
set initial centers should be set before calling	kmeans set initial centers centers	0.200000
be used again to wait for	streaming query manager	0.011236
rpad	rpad	1.000000
generates an rdd comprised of vectors containing i	random rdds uniform vector rdd sc	0.200000
wait for new	streaming query	0.010526
given	param	0.012500
checkpointed version of this dataset checkpointing can be	frame checkpoint eager	0.071429
:class column for approximate distinct count	approx count distinct	0.071429
a new dstream in which each rdd contains	by	0.014286
each rdds into the	dstream n	0.333333
approximate quantiles of	approx	0.047619
temporary table	table df tablename	0.083333
extract the week number of a	weekofyear col	0.055556
can be used	streaming query manager reset	0.011905
generates an rdd comprised of i	random rdds uniform rdd sc size numpartitions seed	0.200000
instance to a java onevsrest used for ml	ml one vs rest to java	0.166667
a local	local	0.115385
find all globals names read or written	code globals	0.125000
matrix columns in an input dataframe from the	matrix columns	0.071429
model derived from a least-squares	model	0.005587
so that :func awaitanytermination() can be	query manager reset	0.011905
sets the given parameters in this grid to	grid builder	0.055556
parse string representation back into	mllib linalg dense vector parse s	0.333333
instance is	ldamodel is distributed	0.200000
returns the explained variance regression score	ml linear regression summary explained variance	0.333333
string representation	mllib linalg	0.026316
kmeans	ml kmeans	0.250000
dataframe as pandas pandas	pandas	0.090909
parquet files returning the result as a	parquet	0.066667
ldatest	ldatest	1.000000
that :func awaitanytermination() can be used	query manager	0.011905
path of a file added through	core spark files get	0.125000
how much of	size	0.009174
java loader the default	java loader java loader	0.333333
labelcol="label", metricname="f1")	labelcol metricname	1.000000
sets	sq selector set	1.000000
column for distinct count of col	count distinct col	0.040000
it will convert each python object into java	to java	0.136364
queries so that	query manager reset	0.011905
tree (e g depth 0 means 1	tree model	0.026316
note : experimental	rdd count approx distinct	1.000000
the dot product of two	vector dot	0.050000
a randomly	cross validator model	0.050000
called when processing	streaming listener on	0.200000
return the column mean	scaler model mean	0.125000
converts vector columns in an input	mlutils convert vector columns from ml	0.166667
find the n	n	0.055556
which this rdd was checkpointed not defined if	checkpoint	0.062500
partitioned data into	core external group by spill	0.047619
gaussians in mixture	mixture	0.052632
of a dataframe as	sql data frame corr	0.166667
number	coordinate matrix num	0.500000
mixin for param stepsize step	has step	1.000000
week number of a given date as integer	sql weekofyear	0.055556
columns are the left singular vectors of	singular	0.015625
params for	params	0.026490
to an int if possible	ml param type converters to int value	0.250000
of weights is close	parameter accuracy	0.029412
sparse matrix stored in csc format	sparse matrix	0.250000
recommends the	model recommend	0.250000
comprised of vectors containing i i d	random rdds log	0.125000
that :func	reset	0.011236
multi-dimensional rollup for the current	sql data frame rollup	0.055556
rdd by applying a function to each	f	0.010526
new accumulator with	accumulator init	0.083333
gets	ml param params get	0.500000
class inherit documentation from its	inherit	0.037037
:py attr predictions which gives the predicted	generalized linear regression summary prediction col	0.250000
computes an fp-growth model that contains	mllib fpgrowth train cls data minsupport	0.100000
decayfactor timeunit to configure the	streaming	0.005025
array of features corresponding to that	features	0.043478
the model improves on toy data with no	regression with sgdtests	0.200000
elements in seen in a	windowduration slideduration	0.083333
a value to a boolean if possible	param type converters to boolean value	0.250000
that all the	external merger object size	0.032258
__init__(self strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none):	imputer init strategy missingvalue inputcols outputcols	1.000000
set initial centers should	mllib streaming kmeans set initial centers centers weights	0.200000
selector type of the chisqselector	set selector type selectortype	0.333333
new terminations	streaming query manager reset	0.011905
or compute the number	linalg indexed row matrix num	0.100000
compute the dot product of two vectors we	ml linalg dense vector dot other	0.090909
stratified sample without replacement based on the	frame sample	0.066667
mixin for param inputcols input column names	has input cols	1.000000
a specific group matched by a java	str pattern idx	0.111111
trace	trace	1.000000
performs the kolmogorov-smirnov ks test for data	stat statistics kolmogorov smirnov test data	0.111111
new dstream by applying a function to	f	0.010526
a query that is executing continuously in	streaming query	0.010526
a java storagelevel	get java	0.111111
split	split	0.312500
to track supported random forest	random forest	0.041667
training set	ldamodel training	0.034483
of conditions and returns one of multiple	sql column otherwise	0.050000
cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	regressor	0.043478
stepsize step size to be used for	step size	0.125000
computes column-wise summary statistics for	stat statistics	0.125000
with the spark sink deployed on a	maxbatchsize	0.037037
option of ensuring all received data has	stopsparkcontext stopgracefully	0.050000
converts vector columns in	mllib mlutils convert vector columns from ml	0.166667
truncate	truncate	1.000000
for each tree	model tree	1.000000
minutes of a	sql minute	0.050000
model that has	model	0.005587
system available on all nodes or any hadoop-supported	path	0.020408
the initial value	regression with sgd set initial	0.111111
return the latest	mllib streaming kmeans latest	0.500000
of :class pyspark sql	sql	0.005051
columns are the left singular vectors of the	singular	0.015625
for this	size	0.009174
content of the :class dataframe as	sql data frame writer save as	0.071429
text file using string representations	text file path compressioncodecclass	0.166667
option for	option key	1.000000
pca	pca	0.750000
rating	mllib matrix factorization	0.250000
perform a left outer join	rdd left outer join other numpartitions	0.111111
the spark	sparksession	0.125000
parses the expression string into the	sql expr	0.125000
for	object size	0.032258
recommends the top "num"	matrix factorization model recommend	0.250000
:class dataframenafunctions for handling missing values	data frame na	1.000000
partitioned	group	0.025641
set number	set	0.005917
to make predictions on batches of	algorithm predict on	0.066667
the trigger for the stream	stream writer trigger	0.083333
format into an rdd of labeledpoint	lib svmfile	0.125000
a column scipy matrix from a	sci py tests scipy matrix	0.090909
which is	regression metrics	0.083333
another value	value	0.008547
elements from an rdd	rdd	0.003058
how much of memory	core external merger object	0.032258
new dstream with an increased or decreased level	streaming dstream repartition numpartitions	1.000000
param	m1 m2 param	0.125000
mllib vector	vector	0.019231
the driver	driver	0.090909
if the rdd contains no elements	core rdd is empty	0.083333
attempt numbers are correctly reported	tests test attempt number	0.333333
prediction on a	prediction	0.041667
python rdd of	core rdd	0.013841
mlwriter for :py class javaparams	mlwriter	0.062500
calculates the length of a	sql length col	0.050000
column containing a json string into a [[structtype]]	json col	0.083333
table accessible via jdbc url url and connection	jdbc url table column lowerbound	0.166667
squared error which	regression	0.010000
return the column mean values	scaler model mean	0.125000
the area under the receiver operating characteristic roc	area under roc	0.500000
test a single	tests test single	1.000000
value of	ml dct	0.250000
the length of a	sql length	0.050000
param	ml	0.001835
strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none) sets params for this	set params strategy missingvalue inputcols outputcols	0.200000
rdd is checkpointed locally	rdd	0.003058
trait for multivariate statistical summary of a data	multivariate statistical summary	0.250000
an rdd comprised	random rdds normal vector rdd	0.166667
int containing elements from start to	core spark context range start	0.090909
instance with a randomly generated	train validation split model	0.166667
test the python direct kafka stream	tests test kafka direct stream from	0.333333
such as the spark fair	core spark context	0.011628
soundex encoding for a	sql soundex col	0.055556
fitting and predicting on incoming	streaming	0.005025
with a given string	ml param	0.009524
output a python rdd of	core rdd	0.013841
param with a given string name	param	0.012500
block matrices together the matrices	block	0.090909
of columns that describes	cols cols kwargs	0.090909
rdds of the dstreams	transform dstreams transformfunc	0.125000
awaitanytermination() can be	query	0.010753
again to wait for	sql streaming query manager	0.011905
model that can transform vectors	model	0.005587
maxbatchsize	maxbatchsize	0.185185
value of	ml param has prediction col	1.000000
model which can perform an online update	model	0.005587
adds a term to this	add term	0.066667
comprised of vectors containing i i	random rdds log normal vector	0.125000
the stream	stream writer	0.041667
+= operator adds a term	accumulator iadd term	0.142857
user who is running sparkcontext	context spark user	0.250000
in rdd 'x' has maximum membership in	gaussian mixture	0.038462
deviance for the fitted model	ml generalized linear regression summary deviance	0.125000
true positive rate for a	true positive rate	0.200000
the rdd of document to rdd of	document	0.040000
removes the specified table from the in-memory	sqlcontext uncache table tablename	0.250000
this instance contains a param	has	0.011628
wait	await termination timeout	0.166667
decision tree <http //en wikipedia	decision tree classifier	0.500000
given parameters in this grid to fixed values	ml param grid builder	0.055556
labeledpoint	mllib mlutils load lib svmfile	0.125000
layers	multilayer perceptron classification	0.333333
a python function including lambda function	function name	0.166667
jvm seq of column	seq sc	0.055556
levenshtein distance of the	sql levenshtein	0.058824
dstreams in this context	streaming context	0.055556
test that coefs are predicted accurately by	regression with tests test parameter accuracy	0.333333
ranking	mllib ranking metrics	0.500000
compute the number	mllib linalg coordinate matrix num	0.166667
that :func awaitanytermination()	sql streaming query manager reset	0.011905
rdd's elements in one	core	0.003021
to in this model	model	0.011173
maps a column of indices back to	index to	0.040000
the jobs started by this thread until the	job	0.023810
:param rdd an rdd	clustering train cls rdd	0.250000
abstract class representing a multiclass classification model	linear classification model	0.076923
the stream query if this	data stream writer	0.041667
content of the dataframe in	data frame	0.005000
onevsrest create and return a python wrapper	one vs rest	0.034483
returns the value of	get	0.021739
creates an external table	sql sqlcontext create external table tablename	1.000000
model for classification	model	0.005587
named options filter out those the value	opts schema	0.250000
the kmeans algorithm for fitting and predicting on	streaming kmeans	0.035714
to a mllib vector if possible	ml param type converters to vector	0.333333
file to be downloaded	file path recursive	0.500000
instance contains a param with	params has param	0.019231
nodes summed over	nodes	0.074074
comprised of	mllib random rdds log normal	0.125000
dictionary a list of index	init size	0.066667
javardd of object by unpickling	ml	0.001835
the vector representation of each word in vocabulary	mllib word2vec fit	0.200000
the mean	core rdd	0.003460
kolmogorov-smirnov ks test for data sampled from	stat statistics kolmogorov smirnov test data	0.111111
python topicandpartition to map	and partition init topic partition	0.055556
file at the specified path	path compression	1.000000
left outer join of c{self}	rdd left outer join other numpartitions	0.111111
columns on the file system	data frame	0.005000
a value with another value	value subset	0.200000
u	u	0.555556
distance from a sparsevector	distance	0.095238
replace null values alias for na fill()	data frame fillna value subset	0.166667
of points using the model trained	logistic regression model	0.083333
a right outer join of c{self} and	core rdd full outer join	0.200000
get a local property set	get local property key	0.066667
pulls events from flume	flume	0.071429
dummy params instance used as a	params dummy	0.111111
table named table accessible via jdbc url url	reader jdbc url table column	0.166667
rdd[vector]	java	0.012195
min value for each original	model original min	0.250000
all values as a list of key-value	all	0.083333
new hadoop outputformat api mapreduce	save as new apihadoop dataset conf keyconverter valueconverter	0.142857
produced by the model's transform method	clustering summary predictions	0.500000
until the group	group	0.025641
parameters in this	ml param grid builder add	0.200000
columns in an input dataframe to the	columns from	0.125000
from a list or pandas dataframe	from local data schema	0.333333
in a sliding window over	value and window windowduration slideduration	0.076923
jobs has completed	completed	0.058824
rdd	rdd save	0.038462
of this :class dataframe as pandas pandas dataframe	data frame to pandas	0.200000
year of a	year col	0.050000
onevsrestmodel used for ml persistence	ml one vs rest model	0.111111
wait until any of the queries on	manager await any termination timeout	0.166667
partial objects do not serialize correctly in python2	partial	0.076923
compare 2 ml types asserting	ml persistence test compare	0.166667
rdd	rdd to	0.200000
values for each key in the	by key numpartitions	0.111111
stream query if this	sql data stream writer	0.041667
awaitanytermination() can	sql	0.002525
ignore separators inside brackets pairs e	ignore brackets split	0.250000
or two separate arrays of indices	ml linalg	0.030303
that	sql streaming query	0.011765
calculates the length of a string or binary	length	0.040000
'x' has maximum membership in this model	gaussian mixture model	0.052632
prediction tasks regression and classification	prediction	0.041667
defines an event time	eventtime	0.125000
a l{statcounter} object that captures the	rdd stats	0.083333
return an rdd containing all	core rdd	0.003460
with this spark job on every	core spark context	0.011628
changes the uid of this instance	ml param params reset uid newuid	0.058824
a local representation	local	0.038462
numfolds=3 seed=none):	numfolds	0.125000
products for all users the	products for users	0.250000
computes column-wise summary statistics for	mllib stat statistics	0.125000
obj assume	core external merger object size obj	0.040000
value of :py attr	ml index	1.000000
content of the non-streaming	write	0.071429
feature selection by number of top features	chi sq selector set num top features	0.500000
initial value	mllib streaming logistic regression with sgd set initial	0.111111
a densevector with singular values in descending order	linalg singular value decomposition s	0.250000
of features corresponding to that user	user features	0.333333
underlying data	sql data frame	0.010695
weights computed	linear model weights	0.250000
setparams(self seed=none) sets params	params set params seed	0.500000
0] for feature selection by percentile	chi sq selector set percentile percentile	0.200000
coalesce	coalesce	0.714286
of the dataframe in a text	data frame writer text	0.200000
spark sink deployed on a	ssc addresses storagelevel maxbatchsize	0.045455
of obtaining a test	test	0.015152
single script file calling a	script with local functions	0.125000
which	isotonic regression model	0.100000
runs and profiles the method to_profile passed	core basic	0.066667
return a l{statcounter} object that captures the	rdd stats	0.083333
squared distance from a	vector squared distance	0.166667
each word in vocabulary	mllib word2vec fit data	0.200000
from start to	spark context range start	0.500000
on the dataset in a data source	source	0.052632
memory	external merger object	0.032258
the norm	linalg dense vector norm	0.333333
words to their vector representations	mllib word2vec model get vectors	0.166667
named table accessible via jdbc url url	jdbc url table	0.090909
@param input dataset for the test this	case test func input func expected sort	0.333333
create an input stream that	create stream ssc hostname port	0.200000
token	token	1.000000
that	external merger	0.031250
params shared by	params copy	0.083333
c{other}, return a resulting rdd that contains	rdd cogroup other numpartitions	0.066667
onevsrestmodel create and return a python	one vs rest model	0.058824
content of the dataframe	sql data frame	0.005348
the minutes of a given date as integer	minute col	0.050000
restore an object	restore name fields value	0.333333
return the	standard scaler	0.076923
train the model on the	linear regression with sgd train on	0.333333
current idf vector	idfmodel idf	0.166667
this	external merger	0.031250
return an numpy ndarray	linalg sparse matrix to array	1.000000
the dispatch to handle all function	core cloud pickler save function obj name	0.142857
of the month	dayofmonth col	0.031250
the specified partitioner	numpartitions partitionfunc	0.250000
the rdd's elements in one	core	0.003021
a line in	line line	0.166667
comprised of vectors containing i i d	mllib random rdds log normal vector	0.125000
comprised of vectors	random rdds	0.076923
distributed matrix on the driver as	block matrix to local matrix	0.250000
a class inherit documentation from its parents	mllib inherit	0.045455
libsvm format into an rdd of labeledpoint	lib svmfile sc	0.125000
compute the standard deviation of this rdd's elements	core rdd stdev	0.066667
convert this matrix	dense matrix	0.076923
:py attr featureindex	feature index value	1.000000
values for each key using an	key func	0.066667
dump already partitioned data	external group by	0.045455
jvm	jvm	0.833333
specification that defines the partitioning ordering and	spec	0.076923
all	core external merger object	0.032258
vector class	vector	0.019231
value of	ml param params	0.013699
a param with a	ml	0.001835
the mean variance and count of the rdd's	rdd	0.003058
a python rdd	rdd save as	0.038462
save this model to	mllib logistic regression model save	0.500000
operation test for dstream mappartitions	operation tests test	0.111111
frequency vectors or transform the rdd	hashing tf transform	0.045455
perform a right outer join	full outer join	0.111111
accuracy equals to the	accuracy	0.076923
offsetrange of specific kafkardd	streaming kafka rdd offset ranges	0.333333
type conversion happens	type conversion tests	1.000000
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none	random forest classifier	0.022727
pass else fail with an error	mllib mllib streaming test case eventually	0.250000
an rdd created	rdd	0.003058
the current [[dataframe]] and perform	grouped data pivot pivot_col values	0.050000
wait for the	timeout timeout	0.125000
globals names read or written	core cloud pickler extract code globals	0.125000
much of memory	external merger	0.031250
fields threshold recall curve	binary logistic regression summary recall by threshold	0.166667
specified table or view	sqlcontext table tablename	0.142857
"zerovalue" which may be added to	fold	0.076923
train a gradient-boosted trees model for	mllib gradient boosted trees train regressor cls data	0.333333
sets	remover set	0.600000
all nodes or any hadoop-supported file	file path	0.035714
comprised of	mllib random rdds normal	0.125000
the behavior when data	data frame writer mode savemode	0.071429
with types inferred from obj	type obj	0.333333
evaluates a list of conditions and returns	sql column otherwise	0.050000
maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256	maxdepth	0.100000
so that :func awaitanytermination()	query	0.010753
training	ldamodel training	0.034483
return	model	0.005587
stop words	stop words	0.500000
test for data	test data distname	0.166667
transforms a python	ml java params transfer param	0.250000
from an rdd ordered in	core rdd take ordered	0.050000
levenshtein distance	sql levenshtein	0.058824
test a script	tests test	0.055556
python topicandpartition to map	partition init topic partition	0.055556
values for each numeric columns for	sql grouped data	0.041667
means 1 leaf	decision	0.052632
of memory	object	0.027778
changes the uid	uid newuid	0.333333
calculates the length of a string or binary	length col	0.050000
local property set in this thread or	local property key	0.035714
__init__(self labelcol="label", featurescol="features", predictioncol="prediction",	generalized linear regression init labelcol featurescol predictioncol	1.000000
converts vector columns in an	mlutils convert vector columns to ml dataset	0.166667
pearson's independence test using dataset	ml chi square test test dataset	0.333333
converts matrix columns in an	convert matrix columns to	0.166667
content of the dataframe in	data frame writer	0.014085
python topicandpartition to map to the java related	partition init topic partition	0.055556
or create	streaming context get or create	0.200000
the dot product	vector dot other	0.100000
number of	linalg sparse vector num	0.200000
week number of a given date as integer	weekofyear	0.043478
checks whether a sparkcontext is initialized	spark context ensure initialized cls instance gateway conf	0.333333
a local property set in this thread	context get local property key	0.066667
:class column denoted by name	data frame getattr name	1.000000
to wait for new	streaming query manager	0.011236
model	ml linear regression model	0.166667
contains a param with a given string	ml param params has param	0.019231
load a java model from the given	load java cls sc	0.200000
the given parameters in this grid	param grid builder base on	0.076923
as	save as	0.333333
to be used with the spark sink	maxbatchsize	0.037037
instance	extra	0.023810
new name	name	0.043478
true positive	true positive	1.000000
items for	items cols	0.125000
:class dataframe as a temporary	data frame as	0.333333
specified partitioner	numpartitions partitionfunc	0.250000
compute the dot product of two vectors	dot other	0.050000
a large dataset and an item approximately find	lshmodel approx nearest neighbors dataset key numnearestneighbors distcol	0.166667
this instance	ml param	0.009524
basic operation test for dstream	streaming basic operation tests test	0.555556
rdd containing the distinct elements in this rdd	core rdd distinct numpartitions	0.250000
mixin for param checkpointinterval set checkpoint interval	has	0.011628
initmode="k-means||", initsteps=2 tol=1e-4 maxiter=20 seed=none)	initmode	0.166667
a py or zip dependency for all tasks	py file path	0.066667
finding frequent	frame freq	1.000000
new dstream in	streaming streaming context	0.032258
sparkconf optional	conf	0.050000
splits str around pattern pattern is a regular	sql split str pattern	0.333333
this instance with a randomly	one vs	0.125000
of labeledpoint	mlutils load lib svmfile sc path numfeatures minpartitions	0.125000
the given path a shortcut of write()	ml pipeline model	0.066667
set k decayfactor timeunit to configure the	streaming	0.005025
the companion	transfer	0.142857
define a windowing column	column over window	0.333333
attr predictions which gives the predicted value of	ml generalized linear regression summary prediction col	0.333333
retrieve gaussian distributions as	gaussian mixture model gaussians df	0.166667
accumulator's value only usable in driver	core accumulator value value	0.050000
extract the week number of a given date	weekofyear	0.043478
train	train	0.555556
aggregation	aggregation	1.000000
local property	local property key	0.035714
levenshtein distance of the	levenshtein left right	0.058824
a param	ml param	0.009524
returns	metrics	0.250000
produced by the	clustering	0.066667
saved	minpartitions	0.142857
locate the position	locate	0.076923
number of partitions for hadoop rdds when	partitions	0.066667
ordinal out of a list or gets an	column get	0.200000
return the column standard deviation values	standard scaler model std	0.166667
sets	count vectorizer set	1.000000
spark configuration	core spark conf init loaddefaults	0.250000
sort the	spark streaming test case sort result	0.333333
test python direct kafka rdd	kafka stream tests test kafka rdd	0.500000
vectors which this	vector	0.019231
list of labels corresponding to indices	ml string indexer model labels	0.066667
context	streaming streaming context	0.032258
thread such as the spark fair scheduler	spark	0.013158
underlying data	sql data stream	0.031250
squared distance from a sparsevector or 1-dimensional	sparse vector squared distance other	0.166667
columns that describes the	cols cols kwargs	0.090909
create a multi-dimensional cube	frame cube	0.055556
field in :py attr predictions	linear	0.025641
converts matrix columns in an input dataframe	convert matrix columns to ml dataset	0.166667
wait for	streaming streaming context await termination timeout	0.166667
much of memory for this	core external	0.016129
residual degrees of freedom	regression summary residual degree of freedom	0.250000
the year of	sql year col	0.050000
the stream query if	data stream	0.028571
:class dataframe to an external	sql data frame	0.005348
data or table already	sql data	0.024390
of the :class dataframe in	data frame	0.010000
decorator that makes a class inherit documentation from	inherit doc cls	0.045455
rdd is checkpointed locally	core rdd	0.003460
returns the greatest value	greatest	0.043478
predict the label of one or more examples	decision tree model predict	1.000000
waits for the termination of	termination	0.035714
of words to their vector representations	get vectors	0.142857
min value for each original column during	max scaler model original min	0.250000
parameters in this grid	param grid builder add grid	0.100000
value to a mllib vector if possible	ml param type converters to vector value	0.333333
compute the dot product of two vectors	dot	0.040000
file and	path schema	0.333333
names of tables in the database dbname	sqlcontext table names dbname	0.500000
the	core external merger	0.032258
the threshold	datasetb threshold	0.500000
loads vectors	load vectors sc path	0.333333
standardization	standardization	0.461538
the value of the date column	date	0.037037
prediction on	prediction	0.041667
vector class for passing data	vector	0.019231
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100	init featurescol labelcol predictioncol maxiter	0.333333
rdd of int containing elements from start	core spark context range start	0.090909
termination of this query	termination timeout	0.041667
instance contains a param	ml param params	0.013699
create and return a python wrapper of	ml	0.007339
columns	data frame	0.010000
weights is close	parameter accuracy	0.029412
drop	drop	0.555556
sql storage	type sql	0.250000
so that :func awaitanytermination() can be used	query manager reset	0.011905
and value	core spark	0.020619
a local representation this discards info about	local	0.038462
given parameters in this grid	param grid builder add grid	0.100000
awaitanytermination()	manager	0.011236
by	by	0.128571
of	merger object size	0.032258
min value for each original column during fitting	scaler model original min	0.250000
the date	next day date	0.100000
list of	ml bisecting kmeans model	0.076923
new :class dataframe replacing a value	data frame replace to_replace	0.100000
for indexing categorical feature columns in a	indexer	0.055556
a param with a given	param	0.012500
rank	rank	1.000000
reduce	default reduce	1.000000
the rdd as non-persistent and remove	rdd unpersist	0.066667
rdd of labeledpoint	load lib svmfile sc path	0.125000
of tree (e g	tree	0.020833
vector to the new mllib-local representation	linalg sparse vector as	0.333333
of the current [[dataframe]] and perform	grouped data pivot pivot_col values	0.050000
copy of the rdd	rdd	0.003058
returns	sql grouped	0.043478
multiple parameters passed as	conf	0.050000
test python direct kafka rdd	streaming kafka stream tests test kafka rdd	0.500000
of write() save	ml pipeline save	0.166667
adds a term to this accumulator's value	core accumulator add term	0.066667
jvm seq of columns that describes the sort	sql data frame sort cols cols kwargs	0.142857
results immediately to the master as a	locally	0.083333
creates a	create	0.068966
performs the kolmogorov-smirnov ks test	mllib stat statistics kolmogorov smirnov test	0.166667
the python direct kafka stream api	kafka direct stream from	0.125000
that :func	query manager	0.011905
use only create a new hivecontext for testing	sql hive context create for testing	0.333333
of the test method	mllib stat chi sq test result method	0.250000
iterator that contains all	to local iterator	0.333333
:class dataframe with an	data frame	0.005000
with another value	value subset	0.200000
options	options	0.888889
dataframenafunctions for handling missing values	na	0.166667
a new dstream by applying reducebykey to each	streaming dstream reduce by key func	0.076923
so that :func	manager	0.011236
a python rdd of key-value pairs (of	core rdd save	0.037975
list based on first	based on key	0.111111
labeledpoint	lib svmfile sc	0.125000
sets	depth set	1.000000
computes column-wise summary statistics for the input rdd[vector]	stat statistics col stats rdd	0.200000
sets params for this imputer	imputer set params	1.000000
for each numeric column for each group	sql grouped data	0.041667
called when a receiver has reported an error	streaming listener on receiver error receivererror	1.000000
for data sampled from a continuous	data	0.011628
queries so that :func awaitanytermination() can be used	query manager reset	0.011905
rating	matrix factorization	0.040000
more examples	decision tree model	0.050000
sets the threshold	set threshold value	0.500000
python direct kafka stream foreachrdd get	kafka direct stream foreach get	0.500000
predicted ratings for input user and product	matrix factorization model predict all user_product	0.050000
a given label	label	0.071429
an rdd comprised of vectors containing i i	mllib random rdds exponential vector rdd	0.166667
shut down	stop	0.052632
point in rdd 'x' to all mixture	mixture model	0.066667
set number	kmeans set	0.090909
labels corresponding	indexer model labels	0.166667
paramter	ml aftsurvival regression	0.333333
accessible via jdbc url url	jdbc url	0.200000
vectors	vectors sc	0.333333
external sort	external	0.013889
for k classes classification problem	classes	0.034483
stream query if	sql data stream	0.031250
explode	explode	1.000000
standard	mllib standard	0.125000
of offsets from a single kafka topicandpartition	offset	0.021739
value of	ml naive	1.000000
calculates the norm of a sparsevector	sparse vector norm p	0.066667
docstring is not shown publicly	mllib linalg row matrix init rows numrows numcols	0.333333
conditions and returns	sql column otherwise	0.050000
instance for	one vs rest	0.034483
mixin for param threshold threshold	has threshold	0.250000
als	als	0.714286
given binary operator	sql bin op name doc	0.200000
extract the day of the month of a	sql dayofmonth col	0.031250
computes average values for each numeric	sql grouped data avg	0.058824
push item onto heap maintaining the heap invariant	core heappush heap item	0.500000
rdd of key-value	rdd save	0.038462
close	parameter accuracy	0.029412
comprised of vectors containing i i d	mllib random rdds poisson vector	0.125000
generates an rdd comprised	mllib random rdds gamma vector rdd sc	0.200000
to term frequency vectors or transform the rdd	tf transform	0.045455
comprised of vectors containing i	mllib random rdds log	0.125000
into	cast	0.166667
the spark fair	core spark context	0.011628
java pipelinemodel create and return a	pipeline model from java	0.142857
list of columns for the given table/view	catalog list columns	0.166667
this :class dataframe a	sql data frame	0.005348
elements in	rdd	0.003058
initial	logistic regression with sgd set initial	0.111111
awaitanytermination() can be used again	manager	0.011236
selector type of the chisqselector	chi sq selector set selector type selectortype	0.333333
comprised	mllib random rdds log normal vector	0.125000
computes column-wise summary statistics for the input rdd[vector]	statistics col	0.200000
test prediction	tests test prediction	0.333333
datatype the data type	datatype	0.045455
of memory for this	object size	0.032258
new :class dataframe that has exactly	data frame coalesce	0.500000
read	read	0.555556
scale	offset scale	1.000000
by codeblock	cls	0.047619
instance's params to	java params to	0.045455
:class dataframe, using the given join expression	data frame join	0.500000
an exception	mllib linalg	0.026316
setparams(self labelcol="label",	regression set params labelcol	1.000000
:py class vectorindexer	vector indexer	0.200000
which the centroids of that particular	timeunit	0.025641
used again to wait	reset	0.011236
resulting rdd that contains a tuple with	rdd cogroup	0.066667
warning these have null parent estimators	gbtregression	0.142857
libsvm format into an rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures	0.125000
wait for	await termination timeout	0.166667
the ensemble	tree ensemble	0.111111
into label	mllib mlutils parse	0.250000
a coordinatematrix	matrix	0.015152
get the cluster centers represented as	mllib kmeans model cluster centers	0.083333
test	tests test group by key	1.000000
the initial	streaming logistic regression with sgd set initial	0.111111
of the :class dataframe as the specified table	data frame writer save as table name	0.333333
the rdd as non-persistent and remove all	core rdd unpersist	0.066667
test that the model params	test test model params	0.250000
queries so that :func awaitanytermination() can be used	streaming query manager	0.011236
list of	ml chi sq selector model	0.166667
offsets from	offset	0.021739
failed to execute	execution exception	1.000000
new terminations	query manager reset	0.011905
queries so	manager reset	0.011905
vector columns in an input dataframe	vector columns from ml	0.142857
dictionary	init size	0.066667
the python direct kafka stream foreachrdd	kafka direct stream foreach	0.500000
contains a param with a	ml param params has	0.019231
right outer join of c{self} and	core rdd full outer join	0.200000
called when processing of	streaming streaming listener on	0.200000
compute the	mllib linalg	0.052632
its recursive dependencies for debugging	to debug string	0.200000
function without changing the keys	f	0.010526
node depth 1	depth	0.111111
computes an fp-growth model	mllib fpgrowth train cls	0.100000
values for each key using an associative	by key func numpartitions partitionfunc	0.066667
contains a param with a	params	0.006623
the trigger for	writer trigger	0.111111
a shortcut of write() save path	ml pipeline model save path	0.200000
dump already partitioned data into disks	group by spill	0.047619
comprised of vectors containing i i d	mllib random rdds log	0.125000
:py attr degree	degree value	1.000000
loads a csv file and	reader csv path	0.333333
right outer join of c{self}	rdd full outer join other	0.111111
prints out	sql data frame print	1.000000
tree (e g depth 0 means	tree	0.020833
pass each value in the key-value	map values	0.166667
two columns of a dataframe as	sql data frame corr	0.166667
temp table from	temp table tablename	0.500000
converts matrix columns in an input dataframe to	mlutils convert matrix columns from	0.166667
inputformat with	inputformatclass	0.095238
converts matrix columns in an	mlutils convert matrix columns from ml	0.166667
convert the java_model to a python model	quantile discretizer create model java_model	0.250000
name of the file to	file	0.028571
java parammap into a python parammap	param map from java javaparammap	0.500000
the given path a shortcut of write()	ml one vs rest	0.052632
converts matrix columns in an input	mllib mlutils convert matrix columns from ml dataset	0.166667
creates an external table based on the dataset	sql sqlcontext create external table tablename path	0.250000
use only create a new	context create	0.083333
be used again to wait for new	manager reset	0.011905
in "predictions" which gives the probability of	ml logistic regression summary probability col	0.166667
norm of	linalg sparse vector norm p	0.066667
this instance	one vs	0.125000
a :class windowspec	sql window	0.200000
unpickling it will convert each python object into	rdd to	0.200000
mixin for param stepsize step size to	has step size	0.333333
the norm of a	vector norm	0.055556
returns subset	mllib multilabel metrics subset	1.000000
using the model	model	0.005587
which is a dataframe having	regression	0.010000
contains a	param params has param	0.019231
2 ml params instances for	params	0.006623
the input	frame reader	0.200000
mindocfreq=0 inputcol=none outputcol=none)	mindocfreq inputcol outputcol	1.000000
predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01 maxiter=100 seed=none)	predictioncol k probabilitycol	0.333333
or compute the number of	linalg indexed row matrix num	0.100000
field in "predictions"	summary	0.073171
and count of the rdd's	rdd	0.003058
group the	group	0.025641
all params with their optionally	params explain params	0.250000
test the python direct kafka stream	stream tests test kafka direct stream from	0.333333
accumulator's value	core accumulator	0.060606
get a local property set	local property key	0.035714
range of	offset range	0.047619
columns in an input dataframe to	columns from ml	0.125000
at pos	pos	0.022222
given	ml param params has param	0.019231
soundex encoding for a string >>> df =	sql soundex	0.055556
:class dataframe using	data frame	0.005000
return the column standard	standard scaler	0.076923
with a	params has param	0.019231
to select filter	model selected features	0.333333
setparams(self	ml als set params	1.000000
convert a	to	0.015385
as a :class dataframe	spark session	0.100000
on a model with weights already set	streaming linear regression with	0.111111
access fields by name or	sql struct type getitem key	0.200000
perform a right outer join	rdd full outer join	0.111111
the dot product of	dense vector dot other	0.050000
function without changing the keys this also retains	values f	0.062500
content of the dataframe	data frame writer	0.014085
parameters passed as a list	core spark conf set all	0.125000
column scipy matrix from	mllib sci py tests scipy matrix	0.090909
the training set given the current parameter	ldamodel training	0.034483
valuetype	valuetype	1.000000
removes the specified table from the	sqlcontext uncache table tablename	0.250000
recommend	recommend	0.384615
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params for	set params maxcategories inputcol outputcol	0.333333
partitioned data	group by spill	0.047619
elementwise	elementwise	1.000000
uid of this instance	ml param params reset uid	0.058824
create a python topicandpartition to map	and partition init topic partition	0.055556
called when processing of a job	streaming listener on output operation	0.166667
decision tree model for classification	decision tree model	0.050000
value of	ml isotonic	1.000000
to wait for new	streaming	0.005025
into	group by	0.041667
the standard deviation of this rdd's elements	rdd stdev	0.066667
disks	spill	0.038462
the dataframe in	sql data frame	0.005348
specifies how data of	sql data stream	0.031250
restore	restore	0.625000
compare 2 ml params	compare params	0.200000
of the rdd's elements in one	core rdd	0.003460
convert this distributed	distributed	0.111111
comprised of vectors containing i	random rdds poisson	0.125000
model which can perform	model	0.005587
of freedom for	of freedom	0.166667
stratified sample without replacement	frame sample	0.066667
rdd as non-persistent and remove all blocks	core rdd unpersist	0.066667
convert this vector to the	linalg vector	0.200000
all active	active	0.125000
sparkcontext is initialized or not	spark context ensure initialized	0.333333
given parameters in this grid to fixed values	grid builder add grid param values	0.333333
under	under	0.777778
cluster centers represented as a list of numpy	kmeans model cluster centers	0.060606
this udf with a function and	sql user defined function	0.083333
of this query either by :func	streaming	0.005025
groups the :class dataframe using the	data frame group by	0.200000
the accumulator's value	core accumulator value	0.045455
other	other	0.300000
an rdd comprised of vectors containing i i	mllib random rdds gamma vector rdd	0.166667
start to end exclusive	start end	0.090909
used	sql	0.002525
base-2 logarithm	log2 col	0.250000
two	mllib linalg	0.052632
of a batch has started	started	0.055556
python parammap into	map to	0.125000
an fp-growth model	mllib fpgrowth train cls data minsupport	0.100000
setparams(self estimator=none estimatorparammaps=none evaluator=none	train validation split set params estimator estimatorparammaps evaluator	1.000000
trees in the ensemble	mllib tree ensemble model	0.058824
partial	partial obj	0.125000
computes average values for each numeric	grouped	0.035714
a single kafka	offset	0.021739
dstreams in this context	streaming streaming context	0.032258
for distinct count	count distinct	0.040000
labelcol="label", predictioncol="prediction",	labelcol predictioncol	0.666667
versionadded : 0 9 0	linear regression with sgd	0.500000
dump already partitioned data into disks	core	0.003021
squared distance	squared distance other	0.076923
number of possible outcomes for k classes classification	num classes	0.500000
vector	vector value	0.333333
class inherit documentation from	inherit	0.037037
the residual degrees	generalized linear regression summary residual degree	0.500000
makes a class inherit documentation	inherit	0.037037
generates python code for a shared param class	param gen param code	0.333333
on the incoming dstream	on dstream	1.000000
merges them with extra	extra	0.023810
underlying sql	sql	0.002525
the given data type json	sql parse datatype json	0.333333
on a cluster	on cluster	1.000000
the :class dataframe to a data source	sql data frame writer save path	0.142857
of weights	weights	0.066667
an existing	samplingratio	0.100000
fits a java model to	java estimator fit java	0.333333
broker's hostname	host	0.166667
to the given path a shortcut of write()	ml pipeline	0.095238
returns an array containing the ids of all	stage ids	0.055556
given string name	params	0.006623
generates python code for a shared param class	ml param gen param code name	0.333333
output	writer	0.200000
save this model to	model save	0.200000
name	name value	1.000000
python direct kafka stream transform	kafka direct stream transform	0.500000
this matrix to a coordinatematrix	block matrix to coordinate matrix	0.333333
get depth of tree	tree model	0.026316
set multiple parameters passed as a list of	spark conf set all	0.125000
compute the dot product of two	linalg dense vector dot other	0.058824
the first n elements	n	0.027778
a function	map f	0.074074
mean	scaler model mean	0.125000
returns an array of	sql	0.002525
restore an object of namedtuple	restore name fields	0.333333
distributed model	ml distributed ldamodel	0.050000
mininstancespernode	min instances per node	1.000000
1 0] for feature selection by	chi sq selector set	0.125000
java onevsrestmodel create and return a python wrapper	one vs rest model from java	0.142857
has completed	completed batchcompleted	0.250000
impurity="variance")	gbtregressor	0.125000
return a jvm scala map from a	frame jmap jm	0.111111
rdd of int containing elements	core spark	0.010309
get or compute the number	mllib linalg block matrix num	0.125000
by weighted terms	mllib ldamodel	0.500000
number of nonzero elements	num	0.016807
single script on	single script on	0.250000
norm of a	sparse vector norm p	0.066667
to a coordinatematrix	to coordinate	1.000000
obj assume that	external merger object size obj	0.040000
the contents of the :class dataframe to	frame writer	0.050000
mixin for param elasticnetparam	has elastic net param	0.200000
left outer	left outer	0.333333
find "num"	model find	0.500000
this instance contains a param	param params has param	0.019231
a randomly generated uid and	cross validator	0.045455
already partitioned data into	spill	0.038462
event time	eventtime delaythreshold	0.333333
and group	core external group	0.045455
list of columns for the	list columns	0.166667
computes the levenshtein distance of the	levenshtein	0.045455
broadcast a read-only variable to	spark context broadcast value	0.125000
indices back to a new	index to	0.040000
be maximized true default or minimized false	evaluator is larger better	0.166667
dataset and an	dataset key	1.000000
all the objects	external	0.013889
params instances	params m1	0.047619
bisecting k-means algorithm	bisecting kmeans	0.083333
column names skipping null values	sql	0.005051
set the initial value	streaming logistic regression with sgd set initial	0.111111
the list of values	values	0.050000
set named options filter out those the value	sql option utils set opts schema	0.333333
the input stream	stream stream	0.500000
number of fields	struct type len	0.200000
this obj assume that all the objects	size obj	0.040000
sets the accumulator's value only	accumulator value value	0.050000
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic", maxiter=20	ml gbtclassifier	0.095238
in c{self} that is not contained in	subtract other numpartitions	0.111111
a specific group matched by a	str pattern idx	0.111111
datatype the data type string format equals	datatype string	0.111111
value for each original	scaler model original	0.062500
:py attr lda keeplastcheckpoint is set	ldamodel	0.034483
csv file	csv path	0.333333
new rdd by applying a function to	map f	0.037037
a py or	py file path	0.066667
indicates whether this instance is of type distributedldamodel	ml ldamodel is	0.066667
return the	spark streaming	0.333333
newline-delimited json <http //jsonlines	json path mode compression dateformat	0.166667
already	core external group	0.045455
test predicted values on a toy model	mllib streaming logistic regression with sgdtests test predictions	0.500000
transfer this instance's params to	java params to	0.045455
field in "predictions" which gives the probability of	ml logistic regression summary probability col	0.166667
table named table accessible via jdbc url url	reader jdbc url table column lowerbound	0.166667
unit length normalization	mllib normalizer	1.000000
to wait	manager	0.011236
items by creator and	core merger merge values iterator	0.166667
residuals deviance pvalues of	ml generalized linear regression	0.250000
lda keeplastcheckpoint is	ml distributed ldamodel	0.050000
test python direct kafka rdd	tests test kafka rdd	0.500000
stopgracefully	stopgracefully	1.000000
opts	opts	1.000000
rdds in a sliding window over	value and window windowduration slideduration numpartitions	0.076923
of the month of a given date as	dayofmonth col	0.031250
a new dstream by applying reducebykey	streaming dstream reduce by key func numpartitions	0.076923
be used again to wait for	streaming	0.005025
curve which is a dataframe having two	ml binary logistic regression summary	0.125000
n largest elements in a dataset	core nlargest n iterable key	0.333333
the correlation	col1 col2 method	0.055556
value	core accumulator add	0.076923
into disks	external	0.013889
call	call	0.857143
test python direct kafka rdd get	kafka stream tests test kafka rdd get	1.000000
initial value	streaming logistic regression with sgd set initial	0.111111
infer schema from an rdd of	sql spark session infer schema rdd samplingratio	0.250000
the given data type json string	sql parse datatype json string	0.333333
converts vector columns in an	mlutils convert vector columns to ml	0.166667
__init__(self inputcol=none outputcol=none)	scaler init inputcol outputcol	1.000000
awaitanytermination() can be used again	query manager	0.011905
generates python code for a	code name doc	0.111111
sha2	sha2	1.000000
cluster centers represented as	model cluster centers	0.090909
creates a :class dataframe	spark session create	0.058824
an rdd comprised	random rdds exponential vector rdd	0.166667
create a converter	create converter	0.166667
param and assert both have the same param	m1 m2 param	0.125000
grid to	grid	0.125000
which gives the predicted	regression summary prediction	1.000000
only create	context create	0.083333
merge the values for each key using an	key func	0.066667
compare 2 ml params instances	compare params m1 m2	0.200000
a pearson's independence test using dataset	chi square test test dataset	0.333333
from an rdd	core rdd take	0.200000
convert a value to a boolean if possible	param type converters to boolean value	0.250000
used again to wait for	sql streaming	0.010204
underlying output	stream writer format	0.333333
to term frequency vectors or transform	hashing tf transform	0.045455
predicted values on a toy	regression with sgdtests	0.200000
the initial	sgd set initial	0.111111
pos in	pos	0.022222
each value in the key-value	map	0.058824
of key value pairs	key	0.017857
get or compute the number of	mllib linalg block matrix num	0.125000
impurity="variance")	ml gbtregressor	0.333333
an rdd created	core rdd	0.003460
the month	sql dayofmonth col	0.031250
underlying output data source	stream writer format source	0.333333
for aggregator by name	name doc	0.333333
to d decimal places	d	0.125000
rate for a given	rate	0.090909
comprised of vectors	random rdds normal vector	0.125000
attr lda keeplastcheckpoint is set to	ml distributed ldamodel get	0.066667
contents of the :class dataframe to a data	data frame	0.005000
based on the dataset in	path	0.010204
called when	listener on	0.200000
the database table named table	table column	0.166667
this instance is of	ml ldamodel is	0.066667
given table/view in the specified database	tablename dbname	0.142857
based on the dataset in a data source	path source	1.000000
a java parammap	java	0.012195
a python rdd of key-value	core rdd save as	0.037500
this rdd's elements	rdd	0.003058
the :class dataframe in json	sql data frame	0.005348
with a	ml param params	0.013699
n rows to	n	0.027778
returns the threshold if any used for converting	threshold	0.018182
set master url to connect to	core spark conf set master value	1.000000
into java	py2java sc	1.000000
the given parameters in this grid to fixed	grid builder base on	0.076923
computes average values for each numeric columns for	sql grouped	0.043478
can	sql streaming	0.010204
:class column for approximate distinct count	approx count distinct col rsd	0.066667
for fitting and predicting on	streaming	0.005025
calculates the length of a	length col	0.050000
in a text file at the specified path	text path compression	0.333333
trigger for the stream query	sql data stream writer trigger	0.083333
the sum for each numeric	grouped data sum	0.083333
that particular batch has half the weightage	half life halflife	0.166667
docstring is not shown publicly	linalg indexed row matrix init rows numrows numcols	0.333333
parse a	parse	0.071429
elasticnetparam the elasticnet mixing parameter in range [0	elastic net	0.125000
in "predictions" which gives the predicted value of	ml linear regression summary prediction	0.500000
the centroids of that particular	timeunit	0.025641
classification	categoricalfeaturesinfo	0.166667
select filter	chi sq selector model selected features	0.333333
the	standard scaler model	0.090909
summary	summary	0.195122
compare 2	ml persistence test compare	0.166667
memory for	merger object	0.032258
first n rows to the console	frame show n truncate	0.333333
python topicandpartition to map to	topic and partition init topic partition	0.055556
a	param params has param	0.038462
stats pstats stats	stats	0.055556
param with	param params has	0.019231
how much of memory for this	core	0.003021
so	query manager reset	0.011905
a python rdd of key-value pairs (of form	rdd	0.009174
setparams(self labelcol="label", featurescol="features", predictioncol="prediction",	generalized linear regression set params labelcol featurescol predictioncol	1.000000
comprised of vectors containing i	random rdds exponential	0.125000
how data of a	sql data	0.024390
for input user and	model	0.005587
number of	mllib linalg dense vector num	0.200000
of conditions and	column otherwise value	0.200000
returns the :class statcounter members as a dict	core stat counter as dict	0.333333
matrix other from this	mllib linalg	0.026316
an	rdd	0.006116
distance from a sparsevector or	distance	0.095238
awaitanytermination() can be	streaming query manager reset	0.011905
the right singular vectors of the	mllib linalg singular	0.017544
is set to a	spark context set	0.166667
into a	to	0.007692
parammap into a java parammap	param map to java pyparammap	0.250000
how	core external	0.016129
setparams(self inputcol=none outputcol=none labels=none) sets params for	to string set params inputcol outputcol labels	0.333333
value in c{self} that	other numpartitions	0.083333
contains a param with a given	ml param params has param	0.019231
partitions to use during reduce tasks	core rdd default reduce partitions	0.166667
finding frequent items for columns possibly with	sql data frame freq items cols	0.166667
population	stat kernel density	0.200000
a class inherit documentation from its	inherit	0.037037
next memory limit if the memory is not	next limit	0.200000
tests whether this instance contains a param	ml param params has param paramname	0.142857
again to wait	query manager reset	0.011905
find synonyms	model find synonyms	0.333333
returns the number	num	0.008403
all params with their optionally	param params explain params	0.250000
on rdds of the dstreams	dstreams transformfunc	0.125000
row-oriented distributed matrix with no meaningful row indices	row matrix	0.200000
compute the dot product of	dot	0.040000
function scaled loss + regularization at each	history	0.181818
elements from an rdd	rdd take	0.200000
the user-supplied param	param	0.006250
a new sparkcontext at least the master	spark context init master	0.333333
or	status tracker get	0.500000
as non-persistent and remove all blocks for it	unpersist	0.083333
from the given path the	path	0.010204
model that can transform	model	0.005587
a	ml	0.003670
of labeledpoint	mlutils load lib svmfile sc path	0.125000
creates an external table based on the dataset	create external table tablename path	0.250000
the values for each key using an associative	by key func	0.062500
a new dstream by applying a function	map f	0.037037
all the jobs started by this thread until	job	0.023810
later than the value of the date column	day date dayofweek	0.333333
already partitioned	core external group by	0.045455
be	sql	0.002525
a class inherit documentation from	inherit	0.037037
in "predictions" which gives the	linear regression summary	0.041667
converts matrix columns in an input	mlutils convert matrix columns from ml dataset	0.166667
n rows to the	n	0.027778
be inherited by any streaminglinearalgorithm	linear algorithm	0.076923
on all nodes or any hadoop-supported file system	file path	0.035714
parses a line in libsvm format into label	parse libsvm line line	0.111111
the norm	mllib linalg sparse vector norm p	0.083333
a list or gets an	column get	0.200000
extract the minutes of a given date	minute col	0.050000
libsvm format	libsvm p	0.250000
0] for feature selection by fdr	mllib chi sq selector set fdr fdr	0.200000
next memory limit if the memory	sorter next limit	0.200000
python function including lambda function	function	0.055556
len with pad	len pad	1.000000
partitioned data into	group	0.025641
underlying output data source	data frame writer format source	0.333333
close to the desired	parameter accuracy	0.029412
a new dstream by applying reducebykey to each	streaming dstream reduce by key func numpartitions	0.076923
of the	sql	0.005051
the selector type	set selector type	0.111111
option for	reader option key value	0.500000
instance with a randomly	cross validator model	0.050000
given data type json	parse datatype json	0.333333
"zerovalue" which may be added to	rdd fold	0.125000
or compute	linalg block matrix	0.052632
the model	ml linear regression model	0.166667
input	stream reader	0.153846
groups the	group by	0.041667
ml params instances for the given	params	0.006623
text	text	0.615385
0 1 0] for feature selection by percentile	mllib chi sq selector set percentile percentile	0.200000
extract the minutes of a given	minute	0.040000
converts vector columns in an input dataframe	mllib mlutils convert vector columns to	0.166667
all params with their optionally default	param params explain params	0.250000
parameters passed as a list	spark conf set	0.111111
attr lda keeplastcheckpoint	distributed ldamodel get	0.066667
can be used again	query manager reset	0.011905
ignore separators inside brackets pairs	sql ignore brackets	0.250000
in tree	mllib decision tree	0.166667
the cluster centers represented as	mllib kmeans model cluster centers	0.083333
only if the rdd contains no elements at	core rdd is empty	0.083333
given a java	model from java cls	1.000000
minutes of a given	sql minute	0.050000
forget about past terminated	terminated	0.100000
awaitanytermination() can	sql streaming query manager reset	0.011905
the test this should be	test	0.015152
of vectors which this	ml vector	0.200000
configure the kmeans algorithm for	kmeans	0.025641
multi-dimensional rollup for the current	frame rollup	0.055556
the singularvaluedecomposition if computeu was set	value decomposition u	0.100000
the population should be	mllib stat kernel density	0.066667
value for each original column	scaler model original	0.062500
a hexadecimal number	unhex col	0.142857
codeobject	codeobject	1.000000
passed as a list	core spark conf	0.055556
a resulting rdd that contains a tuple	core rdd cogroup other numpartitions	0.066667
labeledpoint	mllib mlutils load lib svmfile sc path	0.125000
maxcategories	maxcategories	0.833333
users for a given product and	users product	0.142857
directory for	dir	0.142857
pipelines	pipelines	1.000000
norm	linalg dense vector norm	0.333333
computes column-wise summary statistics for the	mllib stat statistics col stats	0.200000
:func query stop() or by an	streaming query await	0.333333
a param with a	ml param params has	0.019231
dependent variable given	linear regression model base	0.142857
back to a new column of corresponding string	index to string	1.000000
comprised of vectors containing	random rdds gamma	0.125000
set to a different value	set	0.005917
:py attr elasticnetparam	elastic net param value	1.000000
the rdd's elements	core	0.003021
for	external	0.013889
spark_user for user who is running sparkcontext	context spark user	0.250000
data or create	get or create	0.111111
the embedded params to the companion	ml java params transfer params to	0.333333
an rdd created by piping elements	core rdd	0.003460
number of top features	set num top features	1.000000
saveable	saveable	1.000000
param with	has param	0.019231
an external list for list	external list of list	0.500000
inputformat with arbitrary key and	inputformatclass keyclass	0.125000
__init__(self featurescol="features", predictioncol="prediction", maxiter=20 seed=none k=4 mindivisibleclustersize=1 0)	ml bisecting kmeans init featurescol predictioncol maxiter seed	1.000000
mixin for param maxiter max number of iterations	has max iter	0.333333
maxiter	max iter	0.166667
an rdd comprised of vectors containing i i	mllib random rdds log normal vector rdd	0.166667
norm	norm p	0.166667
computes the levenshtein distance of the two given	levenshtein	0.045455
cost sum of	cost	0.105263
which is a risk function corresponding	mllib regression	0.045455
the deviance for the fitted model	generalized linear regression summary deviance	0.125000
the length of a string	length col	0.050000
sample	frame sample	0.066667
train the model on	with sgd train on	0.333333
the deviance for the fitted	summary deviance	0.125000
number of nonzero elements this	num	0.016807
the date	sql date	0.333333
until any	query manager await any termination	0.142857
underlying output	writer format	0.333333
dependent variable given a	linear regression model base	0.142857
inherit documentation from	mllib inherit doc cls	0.045455
train a gradient-boosted trees model for	mllib gradient boosted trees train classifier cls data	0.333333
number of	matrix num	0.147059
return a javardd of object	core	0.003021
residuals label - predicted value	ml linear regression summary residuals	0.500000
a new feature vector with a subarray	vector slicer	0.166667
setparams(self	als set params rank	1.000000
used again	sql	0.002525
with a given string name	has	0.011628
the :class dataframe using	sql data frame	0.005348
calculates the norm	linalg dense vector norm	0.333333
transforms the embedded params to the	params to	0.035714
param with a	param params has param	0.019231
called when processing	streaming listener on output operation	0.166667
the jobs	job	0.023810
term frequency vectors or transform the	mllib hashing tf transform	0.045455
column that generates monotonically increasing 64-bit integers	sql monotonically increasing id	0.333333
depth of tree (e	tree model	0.026316
be used again to wait for new	streaming query manager reset	0.011905
list for list	list of list	0.333333
libsvm format into label	parse libsvm	0.125000
which each rdd contains	by	0.014286
setparams(self	aftsurvival regression set params	1.000000
with arbitrary key and value class	core	0.006042
given parameters in this grid to fixed values	param grid builder	0.055556
add a file to	add file path	0.333333
this rdd which	core rdd	0.003460
merge the values for each key using	key func numpartitions partitionfunc	0.066667
hadoop configuration which	spark context hadoop	0.090909
squared selector model	sq selector model	1.000000
svcmodel	svcmodel	0.833333
setparams(self statement=none) sets params for this sqltransformer	sqltransformer set params statement	1.000000
the kolmogorov-smirnov ks test for data	mllib stat statistics kolmogorov smirnov test data	0.111111
set the initial value	with sgd set initial	0.111111
the residual degrees of freedom	summary residual degree of freedom	0.250000
submit and test a single script	spark submit tests test single script	0.500000
given parameters in this grid	ml param grid builder	0.055556
can	manager reset	0.011905
of that particular batch has half the weightage	half life halflife	0.166667
the day of the month of	sql dayofmonth	0.031250
sort the list based on	case sort result based on key outputs	0.333333
__init__(self scalingvec=none	elementwise product init scalingvec	1.000000
passed as a list of	core spark conf set all	0.125000
sorts this rdd which	core rdd sort by	0.200000
minutes of	sql minute col	0.050000
adds a term to this accumulator's	core accumulator add term	0.066667
resulting rdd that contains	core rdd cogroup other numpartitions	0.066667
converts matrix columns	mllib mlutils convert matrix columns to	0.166667
the norm of a sparsevector	norm p	0.055556
be used with the spark sink deployed	maxbatchsize	0.037037
vectors containing i i d samples drawn	std numrows	0.125000
set each dstreams in this	streaming streaming	0.047619
converts matrix columns in an input dataframe to	mllib mlutils convert matrix columns	0.083333
partial objects do	pickler save partial	0.125000
javardd of object by	ml	0.001835
only create a new hivecontext for testing	context create for testing cls	0.333333
:py attr casesensitive	case sensitive value	1.000000
sub-matrix blocks blockrowindex blockcolindex sub-matrix) that form this	mllib linalg block matrix blocks	0.166667
java	java javaparammap	0.125000
table	table column lowerbound	0.166667
add a py or zip dependency	add py	0.166667
save this model to	logistic regression model save	0.500000
conduct pearson's chi-squared	chi sq	0.111111
the model to make predictions on the	predict on	0.058824
of memory for this	external merger object	0.032258
py or zip dependency for all tasks to	py	0.050000
pairs or two	mllib linalg	0.026316
of labels corresponding to indices to be	ml string indexer model labels	0.066667
test of the observed data against the expected	test observed expected	1.000000
instance	params has param	0.019231
input param belongs	param	0.006250
create a multi-dimensional cube for	frame cube	0.055556
wait until any of	query manager await any termination timeout	0.166667
describe	describe	1.000000
set the initial value of weights	set initial weights initialweights	0.333333
as the specified	as	0.037037
the root mean squared error which is	regression	0.010000
returns true	sql	0.002525
the stream query if this is	stream writer	0.041667
be used again to	streaming	0.005025
with a randomly generated uid	cross validator model	0.050000
attr lda keeplastcheckpoint	ml distributed ldamodel	0.050000
by the model's transform method	summary predictions	0.230769
least value	sql least	0.055556
column	conv col	0.500000
standardization whether to standardize the	standardization	0.076923
:class dataframe, using	sql data frame	0.005348
preservespartitioning	preservespartitioning	0.454545
"zerovalue" which may be added	rdd fold by	0.125000
extract the year of	year col	0.050000
rdds in a sliding window	value and window windowduration slideduration	0.076923
mixed with hasmaxiter hasinputcol and hasseed	test	0.015152
python broker	streaming broker	0.200000
a py or zip	py	0.050000
stream	stream writer	0.041667
squared distance from	mllib linalg sparse vector squared distance	0.166667
probability of obtaining a test statistic result	mllib stat test result	0.166667
return a new rdd containing the distinct	distinct	0.055556
first occurrence of substr	substr	0.071429
restore an object of namedtuple	restore	0.125000
feature selection by	chi sq selector set	0.166667
generate	streaming logistic regression with sgdtests generate logistic	1.000000
or compute the number	num	0.025210
to	sql streaming query manager	0.011905
named options filter out those the value is	opts schema	0.250000
function to the value	map values f	0.125000
that maps a column of indices back	index	0.041667
mean squared error which is	regression	0.010000
add a py or zip dependency	context add py file	0.166667
vector columns in an input dataframe from the	vector columns to	0.142857
get or compute	linalg block matrix	0.052632
__init__(self	string init	1.000000
efficiency can also update c{value1}	value1 value2	0.250000
null type	null type	1.000000
levenshtein distance of	sql levenshtein left	0.058824
approximately find	lshmodel approx	0.100000
gaussianmixturemodel	gaussian mixture model	0.052632
broadcast a read-only	spark context broadcast value	0.125000
the month	sql dayofmonth	0.031250
splits=none inputcol=none	splits inputcol outputcol	1.000000
for this obj assume that all the objects	obj	0.023810
again to wait	manager	0.011236
the year of a given date as integer	sql year	0.050000
generate	logistic regression with sgdtests generate logistic input	1.000000
combine the items by creator and	core merger merge values iterator	0.166667
a function with	f	0.010526
as a hexadecimal number	unhex col	0.142857
file system using the old hadoop	save as hadoop	0.142857
rdd of key-value pairs (of form	core rdd save	0.037975
n elements in the	n	0.027778
sample without replacement based on the	sample	0.050000
in a sliding window	value and window windowduration slideduration	0.076923
test a script with a	tests test	0.055556
again to wait for new	sql streaming query	0.011765
the selector type of the	selector set selector type	0.111111
or any hadoop-supported	path	0.020408
associative and commutative reduce	core rdd reduce by	0.125000
wait for the execution to stop return	termination or timeout timeout	0.125000
utc returns	sql from utc	0.142857
create a new hivecontext for testing	sql hive context create for testing cls sparkcontext	0.333333
load a model from	mllib matrix factorization model load cls sc	0.333333
the contents of the :class dataframe	frame writer save path format	0.066667
of words to their vector representations	model get vectors	0.142857
memory	core	0.003021
with a	param params has param	0.019231
the libsvm format into an rdd of labeledpoint	lib svmfile sc path	0.125000
computes an fp-growth model that	mllib fpgrowth train cls data minsupport numpartitions	0.100000
the behavior when data or table	data frame writer mode savemode	0.071429
instance contains a param with	param params	0.014925
as a :class dataframe	data	0.034884
the training	training	0.029412
data or	get or	0.200000
find synonyms of a word	mllib word2vec model find synonyms word	1.000000
to the specified table	tablename overwrite	0.333333
saves the	mode partitionby	0.200000
value with another value	value	0.008547
the dot product of two	vector dot other	0.050000
string	ml param params has	0.019231
so that :func awaitanytermination() can	query	0.010753
accumulator	accumulator init aid	0.083333
k-means algorithm	kmeans	0.025641
of this	core rdd	0.003460
each original column during fitting	model original	0.062500
for which predictions	isotonic regression	0.090909
densevector with singular	linalg singular	0.017544
by applying a function to each	f	0.010526
to end exclusive increased by	end	0.066667
creates a new sqlcontext	sqlcontext init sparkcontext	1.000000
or two separate arrays of indices	ml	0.001835
can	manager	0.011236
a dependency on another module on a	module dependency on	0.142857
__init__(self	ml index to string init	1.000000
nearest	nearest	1.000000
drawn	numrows numcols numpartitions	0.125000
get or compute the	mllib linalg	0.052632
a python rdd of key-value pairs	rdd	0.009174
user and the second is an array	mllib	0.010526
awaitanytermination() can be used again to wait	reset	0.011236
count of distinct elements in rdds in	count	0.016949
a python rdd of key-value pairs (of form	core rdd save	0.037975
number of features i e length of vectors	ml vector indexer model num features	1.000000
in json format (json lines	sql	0.002525
make predictions on batches of data	streaming linear algorithm predict on	0.066667
list of labels corresponding to indices to	ml string indexer model labels	0.066667
wrapper of	ml java params from	0.250000
for this	core	0.003021
by its name	paramname	0.076923
the given parameters in this grid to	grid builder	0.055556
test predicted values on a toy model	regression with sgdtests test predictions	0.500000
length	length col	0.050000
test that the model predicts correctly on toy	kmeans test test predict on model	0.500000
gaussian mixtures using the expectation-maximization algorithm	gaussian mixture	0.038462
python parammap into a java parammap	map to java pyparammap	0.250000
into disks	core external group by	0.045455
single :class pyspark sql types longtype column named	sql sqlcontext range start end step numpartitions	0.083333
reduces	reduce f	1.000000
fast version of	core heappushpop heap	0.142857
a paired rdd where the first	matrix factorization	0.040000
this	object	0.027778
predicts rating for the given user	matrix factorization model predict user	0.500000
load a model from	model load cls sc	0.250000
given spark runtime configuration property	sql runtime config	0.500000
of clusters	mllib kmeans model k	0.250000
infer schema from an rdd of	session infer schema rdd	0.250000
force every module in modlist	modlist	0.100000
:class dataframe,	data frame	0.005000
this instance	one vs rest model	0.058824
__init__(self	slicer init	1.000000
each point in rdd 'x' to all mixture	gaussian mixture model predict	0.100000
numtrees=20	ml random forest	0.214286
pythontransformfunction java	transform function	0.166667
calculates the norm of a sparsevector	vector norm	0.055556
model	regression	0.010000
given a java pipeline create and return	pipeline from java cls	0.200000
given a java onevsrestmodel create	one vs rest model from java cls	0.200000
converts vector columns in an input dataframe to	mllib mlutils convert vector columns from ml	0.166667
>= 0 or at integral part when scale	bround col scale	1.000000
string	substr	0.071429
paired rdd where the first element is	matrix factorization	0.040000
data	streaming	0.005025
new	save as new	0.125000
instance contains a param with a	param params	0.014925
newuid	newuid	1.000000
restore an object	core restore name	0.333333
compare 2 ml types asserting that	compare	0.071429
a column of indices back to	index to	0.040000
set the selector type of	sq selector set selector type	0.111111
generate	sgdtests generate	1.000000
update the centroids according to data	kmeans model update data decayfactor timeunit	1.000000
contains a param with a given string name	has param	0.019231
outputs	outputs	1.000000
to a new column of corresponding string values	to string	0.500000
creates a :class dataframe	create	0.017241
of model	model	0.011173
all test results	test result	0.200000
compare 2 ml types	ml persistence test compare	0.166667
losstype="logistic",	ml gbtclassifier	0.095238
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance",	ml decision tree regressor	0.066667
contains a param with a given	ml param	0.009524
the week number of a	weekofyear col	0.055556
dump already partitioned data into disks	core external	0.016129
feature selection by number of top	chi sq selector set num top	0.500000
only create	sql hive context create	0.083333
associative and commutative reduce function	reduce	0.041667
columns in an input dataframe from	columns to	0.125000
update	model update	0.500000
queries	sql	0.002525
of the sparkui instance started by	ui web	0.333333
content of the :class dataframe in orc	data frame writer orc	0.200000
a line in libsvm format into label indices	parse libsvm line line	0.111111
extract the year of a	sql year	0.050000
items for columns possibly	items cols support	0.125000
norm of a	linalg sparse vector norm p	0.066667
param type conversion happens	param type conversion tests	0.333333
training set given the	distributed ldamodel training	0.034483
to configure the kmeans algorithm	kmeans	0.025641
partitioned using the specified partitioner	by numpartitions partitionfunc	0.250000
returns the schema of this :class dataframe	data frame schema	0.333333
labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6	labelcol predictioncol	0.083333
kolmogorov-smirnov ks	stat statistics kolmogorov smirnov	0.333333
precision-recall curve which is a dataframe	binary logistic regression summary pr	0.083333
pop and return	core heapreplace heap	1.000000
which is a dataframe	logistic regression	0.040000
of deserialized batches lists	without unbatching	0.125000
"predictions" which gives the features of	ml linear regression summary features	0.166667
__init__(self	ml string indexer init	1.000000
labeledpoint to a	labeled point to	0.125000
construct a taskcontext use get instead	core task context init	1.000000
consist of	ascending numpartitions keyfunc	0.100000
for this obj assume	object size obj	0.040000
inherit documentation from its parents	inherit doc cls	0.045455
cluster	bisecting kmeans model cluster	0.333333
to end exclusive increased	end	0.066667
data of a streaming dataframe/dataset is written to	sql data stream writer output mode outputmode	0.333333
called when a receiver	streaming streaming listener on receiver	0.500000
sets the given parameters in this grid	grid builder base	0.076923
adds a term to	core accumulator add term	0.066667
external list for	external	0.013889
a list of	ml bisecting	0.066667
weights computed for	linear model weights	0.250000
keeplastcheckpoint	keep last checkpoint	1.000000
registers this rdd as a temporary table using	frame register temp table	1.000000
an rdd ordered in ascending order	rdd take ordered	0.050000
get or compute the number of cols	mllib linalg indexed row matrix num cols	0.333333
each word in vocabulary	word2vec fit data	0.200000
model to	ldamodel to	0.500000
contains a param	ml param params	0.013699
inside brackets	brackets split	0.083333
create a multi-dimensional cube for the	frame cube	0.055556
wait for new	manager reset	0.011905
resolves a param and	param param	0.100000
loads a csv file stream	stream reader csv path schema	0.500000
creates a model from the	create	0.017241
the rdd as non-persistent and remove all	rdd unpersist	0.066667
name	has param	0.019231
named table accessible via jdbc url url and	jdbc url table column	0.166667
from disk then sort and group by key	core external group by merge sorted items index	1.000000
given parameters in this grid to fixed	param grid builder base	0.076923
the rdd of document to rdd of term	document	0.040000
to manage all the :class streamingquery streamingqueries active	streaming query manager	0.011236
the content of the dataframe in	sql data frame	0.005348
perform a right outer join of c{self}	rdd full outer join other	0.111111
for fitting and	streaming	0.005025
python object into an internal sql object	to internal obj	0.500000
value of	ml gbtclassifier	0.047619
__init__(self inputcol=none outputcol=none	ml vector slicer init inputcol outputcol	1.000000
the selector type	sq selector set selector type	0.111111
into a java parammap	to java pyparammap	0.250000
in json format (json lines text format	sql	0.002525
to this accumulator's	accumulator add	0.076923
distance from	distance other	0.133333
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	ml decision tree regressor	0.066667
two vectors	mllib linalg vectors	0.333333
to wait	manager reset	0.011905
dot product	dense vector dot	0.100000
stratified sample without replacement based	data frame sample	0.066667
parameters	params	0.026490
train a decision tree model for regression	mllib decision tree train regressor	0.333333
"predictions" which gives the probability of each class	ml logistic regression summary probability col	0.166667
of functions registered in the specified	functions	0.071429
be used again	sql	0.002525
broadcast a read-only variable to the cluster returning	spark context broadcast value	0.125000
so that :func awaitanytermination() can be used again	streaming query manager reset	0.011905
to write a :class dataframe	data frame	0.005000
count of the rdd's elements in	rdd	0.003058
add a py	core spark context add py file	0.166667
as the spark	spark	0.013158
contains a param with a	param	0.012500
data	group by	0.041667
gets the name	get	0.021739
converts matrix columns in an input dataframe from	mllib mlutils convert matrix columns to	0.166667
comprised of	mllib random rdds gamma	0.125000
contains	ml param	0.009524
submit and test a script	core spark submit tests test	0.090909
stop the execution of the streams with option	streaming streaming context stop	0.125000
user defined function udf	udf f returntype	0.200000
distinct count	count distinct col rsd	0.333333
wrap this udf with a function and	defined function	0.066667
tokens in the training set given the current	training	0.029412
commutative reduce	reduce	0.083333
can be used again	streaming query manager	0.011236
which is defined as the	mllib regression	0.022727
matrix to a coordinatematrix	row matrix to coordinate matrix	0.333333
ensemble	tree ensemble model	0.115385
partial objects do	cloud pickler save partial	0.125000
a param and validates the ownership	param params resolve param param	0.333333
param with a given	param params has param	0.019231
setparams(self	min max scaler set params	1.000000
the libsvm format into an rdd of labeledpoint	mllib mlutils load lib svmfile sc path numfeatures	0.125000
the values of each	by	0.014286
awaitanytermination() can be used again to wait for	streaming query manager	0.011236
create a java array	new java array	0.333333
__init__(self splits=none inputcol=none outputcol=none handleinvalid="error")	ml bucketizer init splits inputcol outputcol handleinvalid	1.000000
for each original	original	0.047619
in this grid to	param grid builder add grid	0.100000
a sliding window of time	window	0.037037
sql storage type	type sql type cls	0.250000
the dispatch to handle all function types	cloud pickler save function obj	0.142857
an exception	exception	0.125000
isotonic	isotonic	0.875000
rdd[vector] saveastextfile	mlutils	0.125000
count of	rdd	0.003058
the norm of a sparsevector	norm	0.041667
contains a param with	has param	0.019231
test prediction on a model with weights	mllib streaming linear regression with tests test prediction	0.500000
the current [[dataframe]] and perform the specified	grouped data pivot pivot_col values	0.050000
is set to a different value	spark context set	0.166667
curve which is a	ml binary logistic regression	0.142857
of time over this dstream	streaming dstream	0.027778
of the	core	0.003021
the correlation of two columns of a	method	0.041667
represents a range of offsets from	range	0.030303
train a naive bayes model given an rdd	naive bayes train cls data lambda_	0.500000
queries so	sql streaming query manager reset	0.011905
right outer join of c{self} and c{other}	core rdd full outer join other	0.200000
:class dataframe representing the database table named	sql data frame reader	0.111111
a class inherit documentation	inherit doc	0.045455
frame boundaries from start inclusive to end inclusive	start end	0.090909
files added through c{sparkcontext	core spark files	0.125000
sort	streaming py spark streaming test case sort result	0.333333
java loader the	java loader java loader	0.333333
number of	mllib linalg indexed row matrix num	0.062500
saves the content of the :class dataframe	data frame	0.010000
used with the spark sink deployed on a	storagelevel maxbatchsize	0.045455
a right outer join of	full outer join other numpartitions	0.111111
py or zip dependency for all	py file path	0.066667
modlist to be placed into main	core modules to main modlist	0.333333
load a model from the given	factorization model load cls sc	0.333333
new sqlcontext	sql sqlcontext init	0.500000
get the cluster centers represented as a list	cluster centers	0.090909
vector conduct pearson's chi-squared goodness of	stat statistics chi sq	0.066667
statements	f returntype	0.125000
the :class dataframe in json format (json	sql data frame	0.005348
the :class dataframe to a data source	sql data frame	0.005348
one or more examples	decision tree model	0.050000
the given parameters in this grid	param grid builder	0.055556
orc files returning the result as a	orc	0.083333
train the model on the incoming dstream	regression with sgd train on dstream	1.000000
returns	sql sqlcontext table	1.000000
used with the spark sink deployed on	addresses storagelevel maxbatchsize	0.045455
for which	regression model	0.031250
stream query	stream	0.017544
line in libsvm format	libsvm line line	0.333333
impurity="variance", seed=none	decision tree regressor	0.058824
in json format (json lines text	sql	0.002525
pearson's chi-squared goodness of fit test	mllib stat statistics chi sq test	0.166667
returns accuracy equals to the total	multiclass metrics accuracy	0.166667
of the points belongs to in	predict x	0.033898
total log-likelihood for this model on	ml gaussian mixture summary log likelihood	0.142857
file system uri	file	0.028571
column mean	scaler model mean	0.125000
compute the dot product of two vectors we	vector dot	0.050000
of numpy	ml bisecting kmeans	0.062500
param	ml param params	0.013699
which should be smaller than or equal	numiterations	0.050000
of columns for the given table/view	columns	0.019608
so that :func awaitanytermination() can be used again	streaming query manager	0.011236
memory string in	memory s	0.142857
be used	streaming	0.005025
be downloaded	recursive	0.125000
month of a given date	dayofmonth col	0.031250
contains a param with a	params has	0.019231
libsvm format into label	mllib mlutils parse libsvm	0.125000
that :func awaitanytermination() can	sql streaming query manager	0.011905
this	one vs rest model	0.058824
adds a term to this accumulator's value	add term	0.066667
:py attr initialweights	initial weights value	1.000000
the threshold if any used	threshold	0.018182
find synonyms	find synonyms	0.333333
this	one	0.117647
the content of the dataframe	data frame writer	0.014085
file and	path	0.010204
used again to wait for	sql streaming query	0.011765
number of products for all users	products for users num	0.200000
mixture	gaussian mixture	0.076923
name	params	0.006623
predicted values on a toy model	with sgdtests	0.200000
first argument-based logarithm of the	log arg1 arg2	0.200000
be used again to wait for	query manager	0.011905
partition from disk then sort	merge sorted items index	0.250000
make sure user configuration is respected spark-19307	submit tests test user configuration	1.000000
context to	streaming streaming context	0.032258
curve which is a	binary logistic regression	0.142857
python rdd of key-value pairs	core rdd save	0.037975
frequency vectors or transform the rdd of	mllib hashing tf transform	0.045455
the month of a given date	dayofmonth col	0.031250
approximately find at most k	ml lshmodel approx	0.125000
for	query manager reset	0.011905
min value for each original column during	original min	0.250000
rdd's elements	core	0.006042
distributed model to a local representation this	ml distributed ldamodel to local	0.111111
multiple parameters passed as a list	spark conf	0.058824
offset specified	offset	0.021739
minutes	minute	0.040000
the :class dataframe in json format (json lines	sql data frame	0.005348
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params for	vector indexer set params maxcategories inputcol outputcol	0.333333
for the termination of	termination	0.035714
sample without replacement based on the fraction given	sample	0.050000
a :class	data frame	0.005000
correlation of two	col1 col2 method	0.055556
the first n rows to the console	sql data frame show n	0.333333
of predicted clusters in predictions	ml clustering summary prediction	0.333333
how much of memory for	external merger	0.031250
impurity="gini",	classifier	0.100000
attr predictions	linear	0.025641
get number of nodes in tree including	mllib decision tree model num	1.000000
the heap invariant	core heappop heap	0.142857
function	function name f	0.166667
to list	to list	0.250000
columns in an	columns to ml dataset	0.125000
:class dataframe to a	frame writer save path format	0.066667
sql datum into a user-type object	user defined type deserialize datum	0.333333
given date	date	0.037037
list or a :class pandas dataframe	data frame data	0.142857
a python rdd of key-value pairs (of	core rdd save as	0.037500
system using the old hadoop outputformat api	as hadoop dataset conf keyconverter valueconverter	0.083333
awaitanytermination()	sql streaming	0.010204
pipeline create and return a python wrapper of	ml pipeline from	1.000000
on a	streaming logistic regression with	0.500000
the norm of	ml linalg dense vector norm	0.333333
dataset for the test this should	test	0.015152
accumulator's	core accumulator	0.060606
queries	sql streaming	0.010204
scipy sparse matrices if scipy is available	sci py tests	0.250000
versionadded : 1 2 0	logistic regression with lbfgs	1.000000
test prediction on a	tests test prediction	0.333333
to_profile passed in a profile object is returned	profiler profile	0.200000
trigger for the stream query if	sql data stream writer trigger	0.083333
outer join of	outer join other numpartitions	0.333333
the :class dataframe using the specified columns	sql data frame	0.005348
obj assume that all the	size obj	0.040000
such as the spark fair scheduler	spark	0.013158
mergecombiners	mergecombiners	1.000000
__init__(self	ml vector slicer init	1.000000
columns in an input	columns from ml	0.125000
the accumulator's value	accumulator	0.012987
the old hadoop outputformat api	as hadoop dataset conf keyconverter valueconverter	0.083333
the levenshtein distance of the two given	sql levenshtein left	0.058824
an rdd comprised of vectors	random rdds poisson vector rdd	0.166667
create a new spark configuration	core spark conf	0.055556
greatest value of the list of column names	sql greatest	0.055556
a labeledpoint to a	labeled point to	0.125000
that makes a class inherit documentation from its	mllib inherit doc	0.045455
receive accumulator updates in a daemon	update	0.055556
a given string	ml param params has	0.019231
the singularvaluedecomposition if computeu was set to	value decomposition u	0.100000
partitioned	spill	0.038462
find synonyms	mllib word2vec model find synonyms	0.333333
converts matrix columns in an input dataframe to	mllib mlutils convert matrix columns from ml	0.166667
the length of a	length	0.040000
vectors or transform the rdd of document to	tf transform document	0.166667
:py attr strategy	strategy value	1.000000
set number of batches after which	kmeans set	0.090909
residual degrees of freedom for the null	regression summary residual degree of freedom null	0.333333
converts vector columns in an input dataframe	mlutils convert vector columns from	0.166667
of the :class dataframe to a data source	data frame	0.005000
file using string representations of	file path compressioncodecclass	0.333333
:py attr maxsentencelength	max sentence length value	1.000000
the deviance for the null	generalized linear regression summary null deviance	0.250000
of this rdd's elements	core rdd	0.003460
copy all params defined on the class	ml param params copy params	0.200000
attr lda keeplastcheckpoint is	distributed ldamodel	0.052632
returns the current idf vector	mllib idfmodel idf	0.333333
"num" number of products for all users	products for users num	0.200000
under the	under	0.222222
active queries associated with this sqlcontext >>>	sql streaming query manager active	0.066667
sets	seed set	1.000000
sets the accumulator's value	core accumulator value value	0.050000
into java	mllib py2java sc	1.000000
a single script file calling a	script with local functions	0.125000
feature selection by fwe	mllib chi sq selector set fwe fwe	0.200000
which gives	regression summary	0.035714
convert this matrix to the new mllib-local representation	matrix as	0.250000
saves the	writer save path format mode partitionby	0.200000
of points using the model	model	0.005587
by other, another	multiply other	0.200000
the rowmatrix	mllib linalg row matrix	0.250000
feature	feature	1.000000
loads a csv file stream	stream reader csv path schema sep encoding	0.500000
create a new hivecontext for testing	context create for testing cls	0.333333
contains a	ml param params has param	0.019231
a multi-dimensional rollup for the	sql data frame rollup	0.055556
instance for params shared by	params copy	0.083333
comprised of vectors containing i i d samples	random rdds normal	0.125000
a class inherit documentation	mllib inherit	0.045455
of the current [[dataframe]] and perform the specified	grouped data pivot pivot_col values	0.050000
cost sum of squared distances of points to	compute cost x	0.142857
how much of memory for	external	0.013889
:class dataframe in	sql data frame	0.016043
represents an entry of	matrix entry	0.250000
user-supplied param	param	0.006250
count of the rdd's elements in one	core	0.003021
train a gradient-boosted trees model	mllib gradient boosted trees train classifier	0.333333
list of names	sqlcontext table names	0.066667
sets the sql context to use for saving	ml java mlwriter context sqlcontext	0.333333
in iterator	iterator key	0.200000
d samples drawn	shape scale numrows	0.125000
transforms the embedded params to	params to	0.035714
the :class dataframe in json	sql data frame writer	0.011628
comprised of vectors containing i i d samples	random rdds log	0.125000
values for each key using an	key func numpartitions partitionfunc	0.066667
the levenshtein distance of the two given	levenshtein	0.045455
names into a jvm seq of	seq sc	0.055556
python rdd of key-value pairs (of form	rdd	0.009174
the week number	weekofyear	0.043478
create an rdd for dataframe	create	0.034483
the content of the :class dataframe	data frame writer	0.084507
parses the expression	sql expr	0.125000
much of	merger object	0.032258
a batch has started	started outputoperationstarted	0.125000
contains	param params	0.014925
load a model from	factorization model load cls sc	0.333333
a new java object	wrapper new java obj java_class	0.333333
using the new	new	0.062500
calculates the norm of a sparsevector	norm	0.041667
:py attr mindivisibleclustersize	min divisible cluster size value	1.000000
this frame but not in another frame	sql data frame subtract other	0.333333
'x' to all mixture components	mixture model predict soft	0.142857
an	iteration clustering train cls	1.000000
each key using an associative and commutative reduce	rdd reduce by key	0.333333
each point in rdd 'x' to all mixture	mllib gaussian mixture model predict	0.100000
__init__(self threshold=0 0	ml binarizer init threshold	1.000000
are the right singular vectors of the singularvaluedecomposition	mllib linalg singular value decomposition v	0.250000
each original	min max scaler model original	0.062500
pair of characters as a hexadecimal number	sql unhex col	0.142857
save this model to the given path	naive bayes model save sc path	1.000000
generates an rdd comprised of	random rdds uniform vector rdd sc	0.200000
of the rdd partitioned	rdd	0.003058
add	spark context add	0.500000
field in "predictions" which gives the probability	logistic regression summary probability	0.333333
seq of columns that describes the sort	sort cols cols kwargs	0.142857
how much of memory for this obj assume	core external merger object size obj	0.040000
matrix stored in	matrix	0.015152
partitions to use during reduce tasks (e	reduce partitions	0.166667
resulting rdd that contains a tuple with the	core rdd cogroup	0.066667
compute the number	mllib linalg block matrix num	0.125000
an :class rdd, a list or a :class	schema samplingratio verifyschema	0.029412
given parameters in this grid to	param grid builder base	0.076923
adds output options	frame writer options	1.000000
given parameters in	param grid builder	0.055556
if possible	param type converters	1.000000
from an rdd ordered in ascending order	rdd take ordered	0.050000
exit	exit	1.000000
maxheap version	max heap	0.400000
multiple parameters passed as a list	spark conf set all	0.125000
:class dataframe whose schema	data	0.011628
for the stream query if	data stream	0.028571
accuracy/precision/recall objective history total iterations)	logistic regression	0.040000
log probability	ldamodel log prior	1.000000
the content of the dataframe	sql data frame	0.005348
sets	one hot encoder set	1.000000
back to a new column of	index to	0.040000
the final value of weights is close to	parameter accuracy	0.029412
test that the final value	mllib streaming logistic regression with sgdtests test	0.111111
so that :func awaitanytermination() can be used again	sql streaming	0.010204
behavior when	frame writer mode savemode	0.333333
micro	micro	1.000000
or c{other}, return a resulting rdd that contains	rdd cogroup other numpartitions	0.066667
reported an error	error receivererror	0.500000
returns the precision-recall curve which is a dataframe	ml binary logistic regression summary pr	0.083333
create a converter to	create converter datatype	0.166667
add a py or zip	spark context add py file	0.166667
__init__(self	min hash lsh init	1.000000
a param with a	has param	0.019231
set each dstreams in this context to	streaming streaming context	0.032258
"predictions" which gives the probability of	ml logistic regression summary probability	0.166667
that :func awaitanytermination() can be used again	streaming query manager	0.011236
a l{statcounter} object that captures the mean	core rdd stats	0.083333
degrees of freedom	ml generalized linear regression summary degrees of freedom	1.000000
value of	ml param has input col	1.000000
converts vector columns in an input dataframe from	mlutils convert vector columns	0.083333
of the accumulator's data type returning a new	accumulator param	0.038462
to a mllib vector	to vector value	0.250000
be used in sql statements	f returntype	0.125000
string	param	0.012500
to map to the java related object	init host port	0.200000
occurrence of substr	substr	0.071429
basic operation test for dstream count	basic operation tests test count	0.500000
the threshold if any used for converting	threshold	0.018182
a left outer join of	left outer join other	0.111111
set initial centers should be	kmeans set initial centers centers weights	0.200000
much of memory for this obj assume that	object size obj	0.040000
dataframe that stores item	ml alsmodel item	0.250000
equal	numiterations	0.050000
size default 100	size vectorsize	1.000000
"zerovalue" which may	rdd fold	0.125000
of active queries associated with this sqlcontext	query manager active	0.066667
the soundex encoding for a string >>>	soundex col	0.055556
already partitioned data into	core external group	0.045455
get or compute the number of	linalg block matrix num	0.100000
submit and test a single script on a	core spark submit tests test single script on	0.500000
returns a list of values	linalg dense vector values	0.200000
wait for the execution to	streaming context await termination or timeout timeout	0.125000
assert that an object is of type str	sql runtime config check type obj	1.000000
make predictions on a keyed dstream	predict on values dstream	1.000000
returns a java storagelevel	java	0.012195
__init__(self inputcol=none outputcol=none)	ml tokenizer init inputcol outputcol	1.000000
null	linear regression summary null	0.250000
chain	core chain	1.000000
the sql	sqlcontext	0.153846
batchstarted	batchstarted	1.000000
mincount the minimum number of	min count mincount	0.200000
onevsrestmodel create and return a python	one vs rest	0.034483
tf vectors to tf-idf vectors	mllib idfmodel transform	0.142857
clusters in predictions	clustering	0.066667
of the :class dataframe to a data	data frame	0.005000
a java array	java array	0.333333
numfolds	num folds	1.000000
pivots a column of the current [[dataframe]] and	sql grouped data pivot pivot_col values	0.050000
creates a new sqlcontext	sqlcontext init sparkcontext sparksession jsqlcontext	1.000000
can	frame	0.034483
and profiles the method to_profile passed in	core	0.003021
generates python code for a	code	0.071429
a java storagelevel based on a	core spark context get java	0.333333
predict the label	predict	0.034483
substring	substring	1.000000
of one or more examples	mllib decision tree model	0.076923
dataframe containing names of tables in the given	sqlcontext tables	0.333333
must have the	matrix add other	0.500000
this matrix to the new mllib-local representation	linalg dense matrix as ml	0.333333
parameters passed as a list of key-value pairs	core spark conf set all pairs	0.500000
load a model from the	mllib matrix factorization model load cls sc	0.333333
value of	ml param has handle	1.000000
key and value	core spark	0.020619
until termination	total	0.222222
to wait for new	query manager reset	0.011905
of memory for this obj assume that all	object size obj	0.040000
array of the most recent [[streamingqueryprogress]] updates for	recent progress	0.111111
predict the label of one or	predict	0.034483
calculates the norm of	ml linalg sparse vector norm	0.333333
return the column	scaler	0.105263
:py attr usercol	user col value	1.000000
as a text	as text	1.000000
returns a :class dataframe representing the	sql sqlcontext sql sqlquery	0.250000
invalidate	hive context	1.000000
a given	has	0.011628
nearest center for this model	model compute	0.133333
of conditions and returns one of	sql column otherwise	0.050000
dataframe containing names of tables in	sqlcontext tables	0.333333
register a java udf so it can	register java	0.166667
function to the	map values f	0.125000
extract the year of a given	sql year	0.050000
in one operation	core rdd	0.003460
a method for given unary	sql unary	0.200000
each pair of characters as a hexadecimal number	sql unhex col	0.142857
matrix columns in an input dataframe to the	matrix columns from	0.142857
memory	merger	0.025641
be used for later scaling	mllib standard scaler fit dataset	0.250000
representing the database table named table	table	0.031250
l{statcounter} object that captures the	rdd stats	0.083333
the non-streaming :class dataframe out into external	sql data frame write	0.071429
of this instance with the	ml java	0.076923
create an input stream that pulls events from	utils create stream ssc hostname port	0.200000
of iterations	iterations	0.043478
dataframe	data frame corr col1	0.166667
set the selector type of the	chi sq selector set selector type	0.111111
model to make predictions on batches of	mllib streaming linear algorithm predict on	0.066667
date which is later than the value	dayofweek	0.037037
of the :class dataframe	frame writer	0.050000
with this dataframe	data frame	0.005000
are the left singular	linalg singular	0.017544
for	reset	0.011236
data or table already exists	sql data	0.024390
given a java onevsrestmodel create and return a	one vs rest model from java cls	0.200000
kafkardd	streaming kafka rdd	1.000000
a right outer join	rdd full outer join other numpartitions	0.111111
matrix to the new mllib-local representation	mllib linalg dense matrix as	0.333333
index of	map partitions with index	0.100000
a param with a	ml param params	0.013699
arrays of indices	ml	0.001835
users for a given	users	0.066667
converts matrix columns in an input	mllib mlutils convert matrix columns to ml	0.166667
returns a :class dataframe	session sql	0.250000
feature selection by fdr	mllib chi sq selector set fdr fdr	0.200000
partition for key	external merger partition key	1.000000
how	external merger object size	0.032258
pairs split the list of values	values	0.050000
repartition	repartition	1.000000
converts matrix columns in an input	convert matrix columns from ml	0.166667
vectors which this transforms	vector indexer model	0.200000
generated in this dstream	streaming dstream	0.027778
ids of all active stages	get active stage ids	0.250000
are the left singular vectors of the	singular	0.015625
in mixture	mllib gaussian mixture model k	0.200000
cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	tree regressor	0.058824
every	mllib linear model	0.125000
much of memory for	merger object	0.032258
the stream	sql data stream writer	0.041667
find the	core find	0.500000
setparams(self estimator=none estimatorparammaps=none evaluator=none	split set params estimator estimatorparammaps evaluator	1.000000
as at text file using string	as text files prefix suffix	0.250000
ordered in	take ordered	0.125000
so that :func awaitanytermination() can be	query manager	0.011905
seq	seq	0.217391
set the initial value of weights	with sgd set initial weights	0.333333
which is assumed to consist of key value	key ascending numpartitions keyfunc	0.071429
the minutes	sql minute	0.050000
wait	streaming query manager	0.011236
convert matrix attributes which	mllib linalg matrix convert	0.166667
checkpointinterval=10 losstype="logistic", maxiter=20	gbtclassifier	0.076923
model	model	0.256983
instances	instances	1.000000
data types	data type	0.500000
dispatch to handle all function types	core cloud pickler save function obj name	0.142857
this udf with a function	function	0.027778
model	streaming logistic regression	0.500000
vector columns in an input dataframe from	vector columns to ml	0.142857
singularvaluedecomposition	value decomposition	0.200000
an rdd	exponential vector rdd	1.000000
returns a :class dataframe representing	sql sqlcontext sql sqlquery	0.250000
be used again to wait	sql streaming	0.010204
kmeans algorithm for	kmeans	0.025641
converts vector columns	convert vector columns	0.166667
new terminations	sql streaming	0.010204
this instance contains a param with	ml	0.001835
tests whether this	params has param paramname	0.142857
a py	py	0.050000
returns r^2^, the coefficient of determination	regression metrics r2	0.166667
a given product and returns a list of	product	0.029412
an array containing the ids of	stage ids	0.055556
wait for the execution to	timeout timeout	0.125000
with a	has	0.011628
the rdd contains no elements at all	core rdd is empty	0.083333
note : experimental	spark context binary records path recordlength	1.000000
for each key using an	by key func numpartitions partitionfunc	0.066667
degrees of freedom	linear regression summary degrees of freedom	1.000000
columns in an input dataframe to	columns from	0.125000
the values for each key using	key func numpartitions	0.066667
wait for the execution to stop return true	streaming context await termination or timeout timeout	0.125000
dispatch to handle all	core cloud pickler save	0.166667
the driver as	block matrix to local	0.500000
converts vector columns in an	convert vector columns	0.166667
will convert each python object into	mllib to	0.250000
of nonzero elements	ml linalg	0.030303
already partitioned data into disks	spill	0.038462
estimate of the importance of each feature	ml decision tree regression model feature importances	0.250000
the given block matrix other from this	mllib linalg	0.026316
update the centroids according to data	streaming kmeans model update data decayfactor timeunit	1.000000
dot product of two vectors	ml linalg dense vector dot	0.090909
wait until any of the queries	streaming query manager await any termination timeout	0.166667
signed shift	sql shift	0.500000
levenshtein distance of the	sql levenshtein left	0.058824
whether this instance is of type distributedldamodel	ml ldamodel is distributed	0.066667
percentile	percentile percentile	1.000000
computes column-wise summary statistics for	stat statistics col stats rdd	0.200000
output by	writer bucket by	0.100000
__init__(self formula=none featurescol="features", labelcol="label", forceindexlabel=false)	init formula featurescol labelcol forceindexlabel	1.000000
a copy of this instance	ml pipeline model copy extra	0.333333
mixin for param itemscol items	has items	1.000000
partitioned using the specified partitioner	numpartitions partitionfunc	0.250000
0 modeltype="multinomial", thresholds=none weightcol=none)	naive bayes	0.142857
active queries associated with this sqlcontext	query manager active	0.066667
how much	size	0.009174
because it's heavy used in tointernal	sql user defined type cached sql type	1.000000
a java parammap into a	java javaparammap	0.125000
to this params	params	0.006623
files added through l{sparkcontext	spark files	0.250000
rdd 'x' to all mixture	mllib gaussian mixture model	0.062500
given a large dataset and an item approximately	ml lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
in tree	decision tree	0.076923
a value to	to	0.030769
in rdd 'x' to all mixture	mllib gaussian mixture model	0.062500
cluster centers represented as a list of numpy	mllib bisecting kmeans model cluster centers	0.083333
data or table already	sql data frame	0.005348
startpos	startpos	0.833333
results immediately to the	locally	0.083333
trees in	trees	0.066667
to a rowmatrix	to row	1.000000
python direct kafka stream api	kafka direct stream from	0.125000
an rdd comprised of vectors containing	mllib random rdds exponential vector rdd	0.166667
text file	text file path	0.500000
dataframe in a	data frame	0.005000
property that affects jobs submitted	property key	0.066667
key and value class	core spark context	0.023256
calculates the length of a string or	length col	0.050000
parse a field in	sql parse field	1.000000
or newline-delimited json	writer json	0.125000
to end exclusive increased by step every element	end step numslices	0.500000
generate	mllib streaming logistic regression with sgdtests generate	1.000000
reduce	rdd default reduce	1.000000
variance and count of the rdd's elements in	rdd	0.003058
values for each key using	by key func numpartitions	0.062500
sets	ensemble params set	1.000000
total number	model total num	0.333333
parameters in this grid to fixed values	param grid builder base on	0.076923
ignore separators inside brackets pairs e	sql ignore brackets split	0.250000
impurity="variance", seed=none	regressor	0.043478
:class dataframe to a data	data frame writer save	0.083333
which gives the	linear regression	0.120000
a converter	converter datatype	0.071429
used again	sql streaming query manager reset	0.011905
the singularvaluedecomposition	value decomposition v	0.250000
a line in libsvm format into	mllib mlutils parse libsvm line line multiclass	0.111111
vector representation of each word in vocabulary	word2vec fit data	0.200000
the underlying output data source	frame writer format source	0.333333
each original column	min max scaler model original	0.062500
as the spark fair scheduler pool	spark context	0.023256
until any	any	0.083333
make predictions on batches	algorithm predict on	0.066667
test that	mllib streaming linear regression with tests test	1.000000
in orc	writer orc	0.500000
of freedom	of freedom	0.500000
make predictions on a keyed dstream	kmeans predict on values dstream	1.000000
any	any	0.416667
rdd of labeledpoint	lib svmfile sc	0.125000
term frequency vectors or transform	hashing tf transform	0.045455
vectors saved	vectors sc	0.333333
rows in this :class dataframe	data frame count	0.333333
splits the given string by given separator but	split s separator	0.333333
or	tracker get	0.500000
a sliding window over	value and window windowduration slideduration	0.076923
gbtclassifier	gbtclassifier	0.192308
of the accumulator's	accumulator	0.012987
given parameters in this grid to fixed values	builder add grid param values	0.333333
set initial centers should be	streaming kmeans set initial centers centers weights	0.200000
globals names read or written	cloud pickler extract code globals	0.125000
large dataset and an item approximately	lshmodel approx nearest neighbors dataset key	0.166667
setparams(self featurescol="features", predictioncol="prediction", k=2	gaussian mixture set params featurescol predictioncol k	1.000000
load a model from the	mllib svmmodel load cls sc	0.200000
the day of the month	sql dayofmonth col	0.031250
inverse document frequency idf	idf	0.111111
specified path	path mode	0.333333
of sub-matrix blocks blockrowindex blockcolindex sub-matrix) that form	mllib linalg block matrix blocks	0.166667
a java object	params from java	0.333333
on first value	on key outputs	0.333333
__init__(self featurescol="features",	init featurescol	1.000000
until any of	query manager await any	0.142857
c{self} that	other numpartitions	0.083333
of class conditional probabilities	ml naive bayes model theta	0.500000
labels=none)	labels	0.285714
extract a	extract	0.142857
the :class dataframe to	frame	0.034483
mlwriter	mlwriter	0.375000
lda keeplastcheckpoint is	ldamodel get	0.066667
returns the root mean squared error which is	regression metrics	0.083333
setparams(self featurescol="features", labelcol="label", predictioncol="prediction",	ml linear regression set params featurescol labelcol predictioncol	1.000000
private java model for prediction tasks regression and	java prediction model	0.500000
accumulator's	accumulator	0.038961
range of offsets	range	0.030303
return an rdd created by piping elements to	rdd	0.003058
the n elements from an rdd	rdd take	0.200000
model to make predictions on batches	algorithm predict on	0.066667
user who is running sparkcontext	spark context spark user	0.250000
comprised	mllib random rdds gamma vector	0.125000
sort the list based on	test case sort result based on	0.333333
classes values which the label	classes	0.034483
value of	ml param has features	1.000000
a large dataset and an item approximately find	lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
validate	validate	1.000000
returns an active query from	sql streaming	0.010204
column of the current [[dataframe]] and perform the	pivot pivot_col values	0.050000
note : experimental	count approx distinct relativesd	1.000000
specifies the underlying output	sql data frame writer format	0.333333
:func awaitanytermination() can be used	sql streaming	0.010204
sets params for this	set params	0.034483
wait for the execution to stop	streaming streaming context await termination or timeout timeout	0.125000
dump already partitioned data	core	0.003021
representation of each word in vocabulary	mllib word2vec fit data	0.200000
external database table via	url table mode	0.200000
awaitanytermination() can be used again to wait	query manager	0.011905
sparkcontext at least the	spark context	0.023256
= densematrix(2 2 range 4	dense matrix repr	0.142857
the minimum number of	min count	0.076923
__init__(self	standard scaler init	1.000000
tests	tests	0.600000
the month of a given date as integer	sql dayofmonth	0.031250
map data type	map type	1.000000
set pipeline	pipeline set	1.000000
train a gradient-boosted trees model	mllib gradient boosted trees train regressor cls data	0.333333
nodes or any hadoop-supported file system uri	file path	0.035714
later than the value of the date	day date dayofweek	0.333333
a multi-dimensional rollup for the	frame rollup	0.055556
name	ml param params has	0.019231
new	query	0.010753
depth of tree (e g depth 0 means	tree	0.020833
with weights already	with	0.055556
add two values of the	param add	0.250000
the levenshtein distance of the two given strings	levenshtein left	0.058824
right outer join of	rdd full outer join other numpartitions	0.111111
dataframe with single :class pyspark sql types longtype	sql sqlcontext range start end step numpartitions	0.083333
note : experimental	aftsurvival regression	0.666667
receiver	receiver	0.538462
"zerovalue"	rdd fold by	0.125000
jsparksession	jsparksession	1.000000
__init__(self inputcol=none	string indexer init inputcol outputcol	1.000000
by name	name	0.043478
current [[dataframe]] and perform the specified	grouped data pivot pivot_col values	0.050000
in this grid to fixed values	builder add grid param values	0.333333
returns weighted	multiclass metrics weighted	0.666667
for each key using an associative	by key func	0.062500
algorithm to mine frequent sequential patterns	prefix span	0.166667
defaultvaluestr	defaultvaluestr	1.000000
as non-persistent and remove all blocks	unpersist	0.083333
the log	ldamodel log	0.125000
transforms term frequency tf vectors to tf-idf vectors	mllib idfmodel transform x	0.142857
feature selection by percentile	mllib chi sq selector set percentile percentile	0.200000
the sample covariance of col1 and col2	covar samp col1 col2	0.333333
call	mllib call	1.000000
methods to set k decayfactor timeunit to	streaming	0.005025
regression model with l2-regularization	ridge regression with sgd	0.500000
optional default value and user-supplied value	ml param params	0.013699
term to this	accumulator add term	0.066667
the first n rows to the console	data frame show n truncate	0.333333
calculates the correlation of	corr col1 col2 method	0.055556
access fields by name	struct type getitem key	0.200000
dump already partitioned data	by	0.014286
table and	tablename	0.043478
data	data	0.267442
jobs started by this thread until	job	0.023810
:class dataframe whose schema starts with a string	data	0.011628
"zerovalue" which may be added to the result	rdd fold	0.125000
the given string by given separator but	split s separator	0.333333
histogram using	core rdd histogram	0.333333
sparkcontext which	spark	0.013158
set named options filter out those the	option utils set opts schema	0.333333
labels corresponding to indices to be assigned	string indexer model labels	0.166667
script on	script on	0.500000
instance's params to the wrapped	java params to	0.045455
multiple parameters passed as a list of	core spark conf set all	0.125000
validates the ownership	ml param params resolve	0.333333
the count of distinct elements in rdds in	count	0.016949
comprised	mllib random rdds gamma	0.125000
broadcast a read-only variable	broadcast	0.052632
for this pca	ml pca	0.333333
utils for generating	generator	1.000000
prefix of	prefix	0.083333
dump already	external group by spill	0.047619
dispatch to handle all function	core cloud pickler save function obj	0.142857
the rdd contains no elements	core rdd is empty	0.083333
onevsrestmodel create and return a	one vs rest model from	0.142857
for this obj assume that	core external merger object size obj	0.040000
generate	sgdtests generate logistic input offset	1.000000
trigger	trigger	0.428571
loads vectors saved using rdd[vector] saveastextfile	mllib mlutils load vectors sc path	1.000000
of this rdd's	core	0.003021
saving the content of the non-streaming :class dataframe	data frame write	0.166667
computes the levenshtein distance of the two	sql levenshtein left right	0.058824
left outer join of	rdd left outer join other	0.111111
termination of this query either by	termination timeout	0.041667
tests a	tests test	0.018519
file and	path schema sep encoding	0.333333
all mixture	gaussian mixture model predict	0.100000
comprised	mllib random rdds normal vector	0.125000
maxiter max number of	max iter	0.166667
variancecol column name for the biased sample variance	variance col	0.500000
applies standardization transformation on a vector	standard scaler model transform vector	1.000000
labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor",	labelcol predictioncol	0.083333
much of memory for this	object size	0.032258
the value of the date	date	0.037037
of the month	dayofmonth	0.027027
gaussians	gaussians	1.000000
computes an fp-growth model	mllib fpgrowth train cls data minsupport numpartitions	0.100000
wait	sql streaming query	0.011765
which is a dataframe having two fields fpr	logistic regression summary	0.045455
that :func awaitanytermination() can	sql	0.002525
columns of a dataframe	data frame corr col1 col2	0.166667
return the column mean	model mean	0.125000
the soundex encoding for a string	soundex	0.043478
key and value	core spark context	0.023256
in libsvm format into label indices values	mlutils parse libsvm	0.125000
arbitrary key and value class	core	0.006042
linear model that has a vector	linear model	0.066667
vectors or transform the rdd of document	hashing tf transform document	0.166667
length of a	length col	0.050000
py2java	py2java	1.000000
'x' has maximum membership	gaussian mixture	0.038462
load a model from the given	loader load cls sc	0.250000
users for a given product and returns	users product	0.142857
create an input stream	utils create stream ssc hostname port storagelevel	0.200000
a multi-dimensional rollup for the current :class	frame rollup	0.055556
the elements in iterator	iterator	0.100000
value pairs or two separate arrays of indices	ml	0.001835
d samples drawn	std numrows	0.125000
comprised of vectors containing i i	mllib random rdds	0.083333
the levenshtein distance of the	levenshtein left right	0.058824
return a resulting rdd that contains a tuple	core rdd cogroup other numpartitions	0.066667
of training iterations until termination	ml linear regression training summary total iterations	0.500000
queue	context queue	0.500000
of partitions to use during reduce tasks (e	reduce partitions	0.166667
the year of a given	year col	0.050000
of sub-matrix blocks blockrowindex blockcolindex sub-matrix) that	mllib linalg block matrix blocks	0.166667
hypothesis test	test result	0.200000
carry over its keys	mllib streaming linear algorithm	0.166667
set number of batches	mllib streaming kmeans set	0.142857
of a batch of jobs has completed	batch completed	0.333333
computes the levenshtein distance of the two	levenshtein	0.045455
that stores	alsmodel	0.142857
applies unit length normalization on	mllib normalizer transform	0.500000
wait until any of the queries on	sql streaming query manager await any termination timeout	0.166667
a new spark	spark	0.013158
of the :class dataframe in json format	sql data frame writer	0.011628
with a given	ml	0.001835
cluster centers represented as a list of	bisecting kmeans model cluster centers	0.047619
event time	eventtime	0.125000
class dataframe that with new	data frame to df	0.090909
on the executors if	unpersist blocking	0.166667
partitioned data into disks	by	0.014286
stream query if this is	sql data stream	0.031250
of tree (e g	tree model	0.026316
dataframe produced by	ml clustering	0.100000
the norm of a sparsevector	linalg sparse vector norm	0.066667
item by key out	item key	0.250000
data or table	sql data	0.024390
value of	ml param has thresholds	1.000000
dump already partitioned data	core external group by	0.045455
of objects from the	serializer load stream	0.250000
large dataset and an item approximately find	lshmodel approx nearest neighbors dataset key	0.166667
profile stats to stdout	core profiler show	0.166667
to wait for new terminations	streaming query manager reset	0.011905
a randomly generated uid	cross validator	0.045455
removes the specified table from the in-memory	catalog uncache table tablename	0.250000
a column	col	0.032787
number of rows	num rows	0.200000
modified to support __transient__ on new	cloud pickler save reduce func args state listitems	0.111111
multiclass classification	classification	0.071429
norm of	ml linalg dense vector norm	0.333333
saving	ml mlwriter	0.200000
so that :func	streaming query	0.010526
right outer join of c{self}	rdd full outer join	0.111111
a jvm scala map from a dict	data frame jmap jm	0.111111
of the :class dataframe to a data	data frame writer save path format	0.142857
contains a param with	has	0.011628
multi-dimensional cube	sql data frame cube	0.055556
an rdd of labeledpoint	mlutils load lib svmfile sc path	0.125000
of nodes in tree	decision tree	0.076923
day of the month of a	dayofmonth	0.027027
the kolmogorov-smirnov ks test for data sampled from	mllib stat statistics kolmogorov smirnov test data	0.111111
converts matrix columns in an input	convert matrix columns to	0.166667
based on first	based on key	0.111111
makes a class inherit documentation from	inherit	0.037037
a dictionary a list	init size	0.066667
perform a right outer join of c{self} and	core rdd full outer join other numpartitions	0.200000
number	matrix num	0.147059
content of the :class dataframe	data frame writer save	0.083333
sets the given parameters in this grid to	grid builder base on	0.076923
function to the value of each key-value	f	0.021053
instance	ml param	0.009524
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto",	random forest classifier	0.022727
save this model to the given path	mllib kmeans model save sc path	1.000000
separates positive predictions from negative predictions	mllib linear classification model	0.142857
:func awaitanytermination()	sql streaming query manager reset	0.011905
train a gradient-boosted trees model for classification	mllib gradient boosted trees train classifier cls data	0.333333
of memory for	object	0.027778
distributed matrix on the driver as	to local matrix	0.250000
rdd an rdd of	cls rdd	0.250000
a range	offset range	0.047619
save	save sc path	0.500000
calculates the correlation	corr col1 col2 method	0.055556
null model	summary null	0.250000
this instance contains a	has param	0.019231
:func awaitanytermination()	sql	0.002525
selector type of	selector type	0.100000
converts matrix columns	mlutils convert matrix columns from	0.166667
with the dispatch to handle all function types	core cloud pickler save function	0.142857
observed tokens in the training set given	distributed ldamodel training	0.034483
the accumulator's	accumulator value value	0.050000
sort the list based	case sort result based	1.000000
a param	has	0.011628
all mixture	mixture model	0.066667
returns the number of partitions	num partitions	0.250000
dump already partitioned data into disks	group	0.025641
or provide a new name	name	0.043478
in only	only point	1.000000
that :func awaitanytermination() can be used again to	sql streaming query	0.011765
this model	ml linear regression model	0.166667
the dot product of	mllib linalg dense vector dot	0.058824
gradient-boosted trees gbts <http //en wikipedia org/wiki/gradient_boosting>_	gbtclassifier	0.038462
fp-growth model	fpgrowth train cls	0.200000
:class dataframe replacing a value	data frame replace to_replace	0.100000
an intercept term	intercept	0.090909
first	data frame head	1.000000
numfolds=3 seed=none)	numfolds	0.125000
for distinct count of col or cols	count distinct col	0.040000
objects	external	0.013889
squared distance from a sparsevector	ml linalg sparse vector squared distance other	0.166667
an rdd of labeledpoint	load lib svmfile sc	0.125000
the	mllib	0.294737
default 100	vectorsize	0.166667
the given data type	datatype	0.045455
number of clusters	power iteration clustering model k	0.200000
featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01 maxiter=100 seed=none)	featurescol predictioncol k probabilitycol	0.333333
an rdd[vector]	mllib java vector	1.000000
matrix from the new mllib-local	matrices from	0.333333
represents a range of offsets	offset range	0.047619
string name	param params	0.014925
returns a list of values	mllib linalg dense vector values	0.200000
converts matrix columns	mllib mlutils convert matrix columns from	0.166667
compute a histogram using	core rdd histogram	0.333333
in c{self} and c{other}	join other	0.071429
adds a term to	term	0.080000
:class dataframe as pandas pandas dataframe	data frame to pandas	0.200000
value of	ml imputer	0.250000
transfer this instance to a java pipelinemodel used	pipeline model to java	0.100000
param and validates the ownership	param params resolve param param	0.333333
setparams(self	kmeans set params	1.000000
sort the	streaming test case sort result	0.333333
the norm of a	linalg sparse vector norm p	0.066667
stream query if this is not set it	sql data stream	0.031250
queries so that :func awaitanytermination() can	sql	0.002525
the levenshtein distance of	sql levenshtein left	0.058824
given path a shortcut of write()	ml pipeline model	0.066667
returns one of multiple possible result expressions	sql	0.002525
:py attr k	k value	1.000000
which is a risk function corresponding	regression	0.020000
this dataset checkpointing can be	frame checkpoint eager	0.071429
a given product and returns a	product	0.029412
or replaces a local temporary view	or replace temp view name	0.333333
setparams(self featurescol="features",	regression set params featurescol	1.000000
matrix columns in an input dataframe to the	matrix columns from ml dataset	0.142857
create a new spark	core spark	0.010309
to wait for new terminations	streaming query manager	0.011236
checkpointed and materialized either reliably or	checkpointed	0.083333
log	ldamodel log	0.250000
new spark configuration	core spark conf init loaddefaults _jvm	0.250000
offsets from a single	offset	0.021739
specified database	tablename dbname	0.142857
saves the	writer	0.040000
globals names read or written to	core cloud pickler extract code globals	0.125000
be used again to wait	query manager reset	0.011905
that :func awaitanytermination() can	streaming query manager reset	0.011905
keyconverter	keyconverter	0.833333
sets the given parameters in this	param grid builder add	0.200000
__init__(self	vectorizer init	1.000000
resets the configuration property for the given	sql runtime config unset	0.142857
internal function to get or create global	get or create	0.111111
document to rdd of term	document	0.040000
function to get	get	0.021739
key using an associative and commutative reduce	rdd reduce by key	0.333333
partitioned	group by	0.041667
sets window size	set window size	1.000000
for each original column during fitting	scaler model original	0.062500
number of classes values which	ml java classification model num classes	0.250000
approximate distinct count	approx count distinct col	0.071429
vector columns in an input dataframe from the	vector columns	0.071429
dummy params instance used as a	param params dummy	0.111111
rdd of int containing elements	core	0.003021
the least value of	least	0.043478
queries so	sql	0.002525
current [[dataframe]] and perform the	data pivot pivot_col values	0.050000
maxcategories	max categories	1.000000
returns a paired rdd where the first element	factorization model	0.043478
a new dstream by applying reducebykey to	streaming dstream reduce by key func	0.076923
returns an empty java parammap reference	ml java params empty java param map	1.000000
the sql context to use for loading	mlreader context sqlcontext	0.333333
sets	selector set	1.000000
and profiles the method to_profile passed in a	core	0.003021
offset	offset	0.130435
value of	ml tree ensemble	1.000000
maxheap version	max heap item	0.250000
a keyed dstream	values dstream	0.250000
or multiclass	data numclasses	0.250000
framed	framed	1.000000
a function and attach docstring from func	defined function wrapped	0.333333
sets	iter set	1.000000
squared distance from a	sparse vector squared distance other	0.166667
or list in each batch	oneatatime default	0.250000
returns a :class dataframe representing	sql sqlquery	0.250000
sliding window of time	window	0.037037
compute the dot product	vector dot	0.100000
property set in this thread or null	property key	0.066667
column for distinct	distinct	0.055556
large dataset and an item approximately	lshmodel approx nearest neighbors dataset key numnearestneighbors distcol	0.166667
given block matrix other from this block	linalg block	0.076923
the importance of each feature	ml gbtclassification model feature importances	0.250000
test that the final value of	logistic regression with sgdtests test	0.111111
sparse	sparse	0.750000
how much of memory for this obj assume	merger object size obj	0.040000
evaluator for multilabel classification	multilabel metrics	0.500000
:func awaitanytermination() can	sql streaming query manager reset	0.011905
index of the original	partitions with index	0.100000
a parquet file	parquet path	0.333333
training iterations until termination	linear regression training summary total iterations	1.000000
i i d samples drawn	numrows numcols	0.125000
used again to	manager	0.011236
for the stream query if this is not	sql data stream	0.031250
a model with weights already	linear regression with	0.111111
awaitanytermination()	sql streaming query	0.011765
of ensuring all received data has	stopsparkcontext stopgracefully	0.050000
add a	add	0.071429
registers a python function including lambda function as	sql catalog register function name f	1.000000
test that the final value	logistic regression with sgdtests test	0.111111
partitions to use during reduce tasks	reduce partitions	0.166667
java udf so it can be	java	0.012195
dataframe representing the result	sqlquery	0.054054
generates python code	code name doc defaultvaluestr	0.111111
term frequency vectors or transform the	hashing tf transform	0.045455
an input from tcp source	socket text stream	0.500000
this model	bisecting kmeans model	0.250000
returns the mean average precision map	mean average precision	0.166667
optional default value	ml param params	0.013699
utf8	utf8	1.000000
mean variance	rdd	0.003058
underlying sql	type sql	0.250000
:class dataframe	frame writer save path	0.066667
udf with a function	user defined function	0.066667
an rdd of row	rdd samplingratio	0.200000
the given parameters in this grid to fixed	ml param grid builder	0.055556
print the profile stats to stdout	core profiler show	0.166667
the minutes of a	minute col	0.050000
makes a class inherit documentation from its	inherit	0.037037
checks whether a	cls instance gateway conf	1.000000
a decision	mllib decision	0.125000
underlying data source	sql data	0.146341
vectors or transform the rdd of	tf transform	0.045455
of the current [[dataframe]] and perform	data pivot pivot_col values	0.050000
set a local property that	set local property key value	0.200000
given unary operator	sql unary op name doc	0.200000
streamingcontext object	ssc	0.166667
into a	param map to	0.125000
the distributed matrix on the	matrix	0.015152
number of times a token must appear	count	0.016949
the number	linalg indexed row matrix num	0.100000
this matrix to	linalg dense matrix	0.083333
columns in an	columns from	0.125000
minmaxscaler	min max	0.500000
from start	spark context range start	0.500000
class generated by namedtuple	namedtuple name fields	0.333333
:py attr stopwords	stop words value	1.000000
replaces	replace	0.142857
splits	sql split	1.000000
brackets pairs e	brackets split	0.083333
return as an dict	sql row as dict recursive	1.000000
datatype the data type string format	datatype string s	0.111111
of the :class dataframe to a data source	data frame writer save path format	0.142857
a right outer join	rdd full outer join other	0.111111
the test this should be list	test	0.015152
that :func awaitanytermination() can be	query	0.010753
the week number of	sql weekofyear col	0.055556
file	path schema	0.666667
by the model's transform method	regression summary predictions	0.200000
local	local	0.307692
predictioncol="prediction", labelcol="label", metricname="f1")	predictioncol labelcol metricname	1.000000
is of type	ml ldamodel is	0.066667
a list of numpy	ml bisecting	0.066667
the rdd's elements in	core rdd	0.003460
dot product of two vectors we	linalg dense vector dot other	0.058824
duplicate rows removed optionally only considering certain	duplicates subset	0.500000
computes the levenshtein distance of the	sql levenshtein	0.058824
write	write	0.357143
all the objects	object size	0.032258
should be smaller than or equal to	numiterations	0.050000
the year of	year	0.040000
server	server	0.714286
extract the day of the month	dayofmonth	0.027027
convert to	dense matrix to	1.000000
class representing a multiclass classification model	linear classification model	0.076923
invalidates and refreshes all the	catalog refresh by	0.200000
false positive	false positive	1.000000
0 1 0] for feature selection by	chi sq selector set	0.125000
mlreader for :py class	mlreader	0.037037
joins	on how	1.000000
based on the dataset in a	path	0.010204
dependent variable given a vector or an rdd	linear regression model base	0.142857
this matrix to a rowmatrix	linalg coordinate matrix to row matrix	0.333333
this rdd for	core rdd	0.003460
:class rdd, a list or	schema samplingratio verifyschema	0.029412
columns are the left singular	singular	0.015625
iterable this is used because	iterable	0.125000
and is of length len	len	0.071429
threshold	datasetb threshold	0.500000
applies transformation on a vector	vector transformer transform vector	0.500000
until any of the	query manager await any termination	0.142857
python code for a shared param class	param gen param code	0.333333
a streaming dataframe/dataset is written to a streaming	writer output mode outputmode	0.083333
:py attr droplast	drop last value	1.000000
elements in the stream will start and stop	test case take dstream	0.250000
an input from tcp source hostname port data	socket text stream hostname port	1.000000
the uid of	ml param params reset uid newuid	0.058824
sets	idf set	1.000000
much of memory	object	0.027778
of rdds	rdds	0.142857
multi-dimensional cube for the current	frame cube	0.055556
cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto",	random forest classifier	0.022727
create a converter to drop the names	create converter datatype	0.166667
conditions and	column otherwise value	0.200000
local representation this	local	0.038462
a	params has	0.038462
available	available	1.000000
consist of key	key ascending numpartitions keyfunc	0.071429
all trees in the ensemble	mllib tree ensemble	0.111111
calculates the length of a string or	length	0.040000
by other, another blockmatrix	linalg block matrix multiply other	0.200000
of this instance	ml	0.012844
each original column	max scaler model original	0.062500
libsvm format into an rdd of labeledpoint	load lib svmfile sc path numfeatures	0.125000
names of	table names	0.066667
point in rdd 'x' has maximum membership in	mllib gaussian mixture	0.045455
saves the	path format mode partitionby	0.200000
given string	params has param	0.019231
sets params for rformula	rformula set params formula	1.000000
term frequency vectors or transform the	tf transform	0.045455
new terminations	query manager	0.011905
count of	count	0.050847
find synonyms of	mllib word2vec model find synonyms	0.333333
linearregressionmodel	sc path	0.222222
data source	data	0.093023
params instances for the given param	params	0.006623
globals names read or written to by codeblock	extract code globals cls	1.000000
of the observed data against the expected	observed expected	0.166667
projection	projection	1.000000
:py attr seed	seed value	1.000000
trees in this ensemble	model trees	1.000000
to a local representation this discards	to local	0.125000
the standard deviation of this	rdd stdev	0.066667
terminations	manager reset	0.011905
property that affects	property key value	0.125000
a csv file and	csv path schema	0.166667
vector columns in an input dataframe	vector columns to ml dataset	0.142857
get the cluster centers represented as a list	kmeans model cluster centers	0.090909
start	range between start	0.200000
the given data type	parse datatype	0.333333
create an rdd for dataframe	session create	0.117647
the	core external merger object size	0.032258
value of	ml param has aggregation depth	1.000000
type	sql data type	1.000000
iterator of deserialized batches lists	stream without unbatching	0.125000
the deviance for the null	summary null deviance	0.250000
events from flume	streaming flume	0.111111
or c{other}, return a resulting rdd that contains	core rdd cogroup other numpartitions	0.066667
'x' has maximum membership	mllib gaussian mixture	0.045455
used with the spark sink deployed	ssc addresses storagelevel maxbatchsize	0.045455
perform a pearson's independence test using dataset	ml chi square test test dataset featurescol	0.333333
n elements	n	0.027778
by applying a function to each element	map f preservespartitioning	0.200000
the values for each key using	by key func	0.062500
train the model on the incoming	sgd train on	0.333333
the initial value of weights	regression with sgd set initial weights	0.333333
the behavior when data or table already	sql data frame writer mode savemode	0.071429
represents qr factors	qrdecomposition	0.166667
data sampled	data distname	0.083333
n rows	n truncate	0.250000
a given product and returns	product	0.029412
of columns	cols	0.052632
converts vector columns	mlutils convert vector columns to	0.166667
data or create	streaming context get or create	0.200000
to any hadoop file system using the new	new	0.062500
for feature selection by number of top	chi sq selector set num top	0.500000
of rows whose	ml	0.001835
a dataframe with two fields threshold recall curve	ml binary logistic regression summary recall by threshold	0.166667
of deserialized objects from the input stream	serializer load stream stream	0.333333
training set given the current parameter estimates	distributed ldamodel training	0.034483
so that :func awaitanytermination() can be used	sql streaming	0.010204
of gaussians in mixture	mixture model k	0.200000
predict	predict x	0.033898
java pipeline create and	pipeline from java	0.142857
named table	table	0.031250
default min number of partitions for hadoop	default min partitions	0.250000
formula	formula	0.833333
this frame but not in another frame	frame subtract other	0.333333
extract the week number of a	weekofyear	0.043478
test the python direct kafka stream api with	stream tests test kafka direct stream	0.125000
set initial centers should be	initial centers centers weights	0.200000
set number of batches after	kmeans set	0.090909
of active queries associated with this sqlcontext >>>	sql streaming query manager active	0.066667
queries	manager reset	0.011905
'x' to all mixture	gaussian mixture model predict	0.100000
calculates the norm	linalg dense vector norm p	0.333333
tf-idf vectors	mllib idfmodel transform x	0.142857
squared distance from a sparsevector or 1-dimensional numpy	ml linalg sparse vector squared distance	0.166667
be	sql streaming query manager reset	0.011905
calculates the norm of a	norm p	0.055556
model trained	mllib logistic regression model	0.083333
sparkcontext as sc app' syntax	spark context exit type value trace	0.333333
convert python list to java type array	sql to jarray gateway jtype arr	0.500000
'x' to all mixture components	gaussian mixture model predict soft	0.142857
objective function scaled loss + regularization at each	summary objective history	0.500000
contents of the :class dataframe to	frame writer	0.050000
set multiple parameters passed as a list of	core spark conf set all	0.125000
this instance with a randomly	one	0.058824
computes an fp-growth model that	mllib fpgrowth train cls data	0.100000
rdd as a dictionary of value count pairs	rdd count by value	1.000000
spark fair scheduler pool	spark context	0.023256
test python direct kafka	stream tests test kafka	1.000000
to a java pipelinemodel used for	ml pipeline model to java	0.100000
comprised of vectors	random rdds log	0.125000
dump already partitioned data into disks	core external group by	0.045455
pyparammap	pyparammap	0.833333
for a condition to pass else fail with	streaming test case eventually condition	0.333333
iterations default 1 which should be smaller than	iterations	0.043478
or compute the	mllib linalg coordinate	0.333333
:class dataframe	data frame writer save	0.083333
__init__(self inputcol=none outputcol=none labels=none)	index to string init inputcol outputcol labels	1.000000
generates an rdd comprised of vectors containing	mllib random rdds normal vector rdd sc	0.200000
the spark fair scheduler	core spark	0.010309
compare 2 ml params instances for the given	compare params m1 m2	0.200000
__init__(self inputcol=none outputcol=none labels=none)	ml index to string init inputcol outputcol labels	1.000000
elements from start to	core spark context range start	0.090909
rdd of points using the model trained	mllib logistic regression model	0.083333
numerical columns	quantile col probabilities relativeerror	0.166667
comprised of i i	random rdds	0.038462
python topicandpartition to map to the	init topic partition	0.055556
orc files returning the result	orc	0.083333
prefix of string in doc tests	prefix f	0.142857
matrix to	mllib linalg dense matrix	0.083333
contains a param with a given string name	params	0.006623
right outer join	rdd full outer join	0.111111
n rows to	n truncate	0.250000
generates an rdd comprised of vectors containing	random rdds normal vector rdd sc	0.200000
sets	ml als set	1.000000
until any of the queries	manager await any	0.142857
test that the final	logistic regression with sgdtests test	0.111111
is of type distributedldamodel	ml ldamodel is distributed	0.066667
a class inherit documentation from	inherit doc	0.045455
using string representations of	compressioncodecclass	0.111111
c{other}, return a resulting rdd that contains a	rdd cogroup other	0.066667
into label indices	parse	0.071429
return a copy of the rdd partitioned using	core rdd partition by	0.333333
predicting on incoming	streaming	0.005025
to use for saving	mlwriter	0.062500
compute	mllib linalg row	1.000000
for this obj assume that all the	object size obj	0.040000
the deviance for the null	linear regression summary null deviance	0.250000
infer schema from an rdd of row or	session infer schema rdd samplingratio	0.250000
probabilities	probabilities	1.000000
tcp server	server	0.142857
count of the rdd's elements	core	0.003021
the kmeans algorithm for fitting and	kmeans	0.025641
tree (e g depth 0 means 1 leaf	decision tree	0.076923
forget about past terminated	reset terminated	0.200000
this distributed model to a local representation	distributed ldamodel to local	0.111111
broadcast a read-only variable to	broadcast	0.052632
python rdd of key-value pairs	core rdd save as	0.037500
a new spark	core spark	0.010309
square	square	1.000000
setparams(self labelcol="label", featurescol="features", predictioncol="prediction",	regression set params labelcol featurescol predictioncol	1.000000
repr	repr	0.555556
param with a	params	0.006623
dataframe that stores	alsmodel	0.142857
defined from start inclusive to end inclusive	start end	0.090909
perform a pearson's independence test using dataset	ml chi square test test dataset featurescol labelcol	0.333333
sentence	sentence	1.000000
of tables/views in the specified	tables	0.071429
two	linalg	0.088889
stream transform get offsetranges	stream transform get offset ranges	1.000000
terms to term frequency vectors or transform the	hashing tf transform	0.045455
this instance is of	ml ldamodel is distributed	0.066667
the input data source format	sql data stream reader format source	0.333333
rate for	rate	0.090909
the norm of	norm	0.041667
again to wait for	streaming query manager reset	0.011905
a streaming dataframe/dataset is written	writer output mode outputmode	0.083333
find all globals names read or written to	globals	0.076923
be smaller than or equal	numiterations	0.050000
like '#,--#,--# --', rounded to d decimal places	number col d	1.000000
of terms or words in the	ml ldamodel	0.111111
size of number of	ml	0.001835
labelcol="label", forceindexlabel=false)	labelcol forceindexlabel	1.000000
queries so that :func awaitanytermination() can be	manager	0.011236
seed=none layers=none blocksize=128 stepsize=0 03 solver="l-bfgs", initialweights=none)	ml multilayer perceptron classifier	0.142857
binary mathfunction	binary mathfunction	1.000000
partitioned data	core external group	0.045455
perform a pearson's independence test using dataset	chi square test test dataset featurescol	0.333333
so that :func	reset	0.011236
return a jvm scala map from	frame jmap jm	0.111111
rdd's	rdd	0.006116
value in c{self} that is not contained	core rdd subtract other numpartitions	0.111111
returns the soundex encoding for a string	soundex	0.043478
return the weights for each tree	tree ensemble model tree weights	0.333333
setparams(self inputcol=none outputcol=none labels=none) sets params for this	set params inputcol outputcol labels	0.333333
of possible outcomes for k classes	classes	0.034483
of the first occurrence of substr	substr str	0.125000
spark fair scheduler pool	core spark context	0.011628
dataframe that with new specified column names	data frame to df	0.090909
matrix on the driver as	matrix to local matrix	0.250000
data or create	context get or create	0.200000
return	scaler model	0.076923
class for indexing categorical feature columns	indexer	0.055556
for feature selection by fdr	mllib chi sq selector set fdr fdr	0.200000
attr lda keeplastcheckpoint is	ldamodel get	0.066667
another timestamp that corresponds to the	timestamp timestamp tz	0.166667
env	env	1.000000
correlation method. currently only supports "pearson"	method	0.041667
on rdds of the dstreams	context transform dstreams transformfunc	0.125000
how much	external merger object	0.032258
based on	based on key outputs	0.111111
jvm seq of columns that describes the sort	data frame sort cols cols kwargs	0.142857
transfer this instance to a java onevsrest used	one vs rest to java	0.166667
parses the given data type json string	parse datatype json string json_string	1.000000
__init__(self inputcol=none outputcol=none indices=none names=none)	slicer init inputcol outputcol indices names	1.000000
windowing	over window	0.333333
dot product	vector dot other	0.100000
the new	as new	0.125000
shortcut of write() save	ml pipeline model save	0.166667
return a javardd of object by unpickling it	core rdd	0.003460
used again to wait for new terminations	reset	0.011236
file system	file	0.028571
this instance contains a	param params has	0.019231
by piping elements to a forked external process	pipe command env checkcode	0.166667
mean	core rdd	0.003460
matrix to a rowmatrix	linalg indexed row matrix to row matrix	0.333333
boolean :class column expression	condition	0.045455
until any	await any termination	0.142857
set initial centers should	set initial centers centers	0.200000
parameters in this grid	param grid builder base	0.076923
aggregate the values of	rdd aggregate	0.250000
nulltype	nulltype	1.000000
squared error which is	mllib regression	0.022727
methods to	streaming	0.005025
how much of	merger object size	0.032258
for each numeric columns	grouped	0.071429
model for prediction tasks regression and classification	prediction model	0.333333
mean variance and count of the	core	0.003021
thread such as the spark fair	core spark context	0.011628
the dispatch to handle all function types	cloud pickler save function	0.142857
return an rdd	core rdd	0.010381
area under	classification metrics area under	0.166667
using the specified partitioner	partition by numpartitions partitionfunc	0.250000
set bandwidth of each	kernel density set bandwidth bandwidth	0.142857
value	accumulator add	0.076923
:py attr predictions which gives	linear regression summary	0.013889
define a windowing	over window	0.333333
degrees	regression summary degrees	1.000000
property for the given key	key defaultvalue	1.000000
term to this accumulator's value	term	0.040000
for all users	for users	0.500000
paired rdd where	factorization	0.038462
calculates the correlation of two columns of	col2 method	0.055556
for saving the content of the non-streaming	write	0.071429
convert this matrix to the	dense matrix	0.076923
which is a dataframe having	regression summary	0.035714
this instance contains a	params	0.006623
a param with a	params	0.006623
to track supported impurity measures	tree classifier params	0.250000
so that :func awaitanytermination() can be used again	sql	0.002525
the uid	reset uid newuid	0.333333
broadcast a	spark context broadcast value	0.125000
rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures minpartitions	0.125000
operation test for dstream mappartitions	operation tests test map partitions	1.000000
input data	data stream reader	0.200000
train a	train cls	0.500000
a py	py file	0.066667
convert a number in	sql	0.002525
after which the centroids	timeunit	0.025641
curve which is a dataframe	ml binary logistic regression summary	0.125000
end exclusive increased by step	end step	0.500000
rdd is	core rdd	0.003460
two-sided p-value of estimated coefficients and intercept	ml linear regression summary p values	0.333333
associative and commutative reduce function	rdd reduce	0.071429
onevsrest create and return a	one vs rest from	0.142857
user and the second is an	mllib	0.010526
does this configuration	conf	0.050000
that	merger object	0.032258
error which is defined as the square root	linear regression summary root	0.500000
of nonzero	ml linalg dense vector	0.100000
and value class	core	0.006042
note : experimental	quantile discretizer	1.000000
returns weighted averaged f-measure	metrics weighted fmeasure beta	1.000000
an rdd of	rdd samplingratio	0.200000
residual degrees of freedom for the	summary residual degree of freedom	0.125000
param with self-contained documentation	param	0.006250
model should have been	mllib loader	0.333333
old hadoop outputformat api mapred package	as hadoop dataset conf keyconverter valueconverter	0.083333
compute the number of	num	0.025210
is	ldamodel is	0.200000
dump	external group by	0.045455
columns in an	columns from ml dataset	0.125000
and group by key	core external group by	0.045455
test that the	kmeans test test	0.500000
labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0	labelcol predictioncol maxdepth	0.333333
'with sparkcontext as sc app' syntax	spark context exit type value trace	0.333333
of memory	core external merger	0.032258
already partitioned data into disks	external group by spill	0.047619
sets params for rformula	rformula set params	1.000000
lda keeplastcheckpoint	distributed ldamodel	0.052632
test that the final value of weights is	regression with sgdtests test	0.111111
of number of	ml	0.001835
convert this	linalg	0.044444
stream query	data stream	0.028571
saves the content of the	format mode partitionby	0.100000
labeledpoint	load lib svmfile sc path	0.125000
given a java pipelinemodel create and return	pipeline model from java cls	0.200000
query	streaming query	0.010526
already partitioned data into	by	0.014286
create an input from tcp source	streaming streaming context socket text stream	0.500000
for this model	mllib linear model	0.125000
that	streaming query manager	0.011236
persist its values across operations after the first	core rdd persist storagelevel	0.166667
hack	hack	1.000000
java loader	java loader java loader	0.333333
column expression representing a user defined function udf	sql udf f returntype	0.200000
creates an external table based	sqlcontext create external table tablename path	0.250000
be used again to wait	streaming query manager	0.011236
of each	by	0.014286
of memory for	core external	0.016129
train a random forest model	mllib random forest train classifier cls	0.250000
can be used again to wait	streaming query manager	0.011236
the dot product of two vectors	dot	0.040000
:func awaitanytermination() can	manager reset	0.011905
new dstream in which each rdd is generated	streaming streaming context transform	0.066667
a paired rdd where the first element is	matrix factorization model	0.043478
commutative reduce function	reduce by	0.200000
can be used again to wait for new	sql streaming query manager	0.011905
model improves on toy data with no	streaming logistic regression with sgdtests	0.200000
trees model for	trees	0.066667
to make predictions on batches of	mllib streaming linear algorithm predict on	0.066667
save this model to the given	naive bayes model save sc	1.000000
function	map f	0.074074
param with a given string	params has param	0.019231
versionadded : 0 9 0	with sgd	1.000000
add a py or zip dependency for	add py	0.166667
:class datatype the data type string format	datatype string	0.111111
a left	left	0.066667
code for	code name doc defaultvaluestr	0.111111
new :class dataframe with an	data frame	0.005000
submit and	spark submit	0.800000
linear svm classifier	linear svcmodel	0.500000
generate	regression with sgdtests generate logistic	1.000000
computes column-wise summary statistics for the	stat statistics col stats	0.200000
creates a model from	create	0.017241
find synonyms of a word	model find synonyms word num	1.000000
mixin for param inputcols input	has input	0.500000
as a :class dataframe	grouped data	0.035714
wait until any of	any termination timeout	0.166667
attr lda keeplastcheckpoint is	distributed ldamodel get	0.066667
month of a given date as integer	dayofmonth	0.027027
saves the contents of the	format mode partitionby	0.100000
selector type of the	selector set selector type	0.111111
rdd of sub-matrix blocks blockrowindex blockcolindex sub-matrix) that	linalg block matrix blocks	0.166667
a value to an int if possible	ml param type converters to int	0.250000
file	file path	0.071429
broadcast a read-only variable to the cluster returning	context broadcast	0.125000
fits a model to the input dataset	dataset	0.020408
the given user and	user	0.055556
this imputer	imputer	0.100000
with a given string	param params	0.014925
cache the sqltype() into class	cls	0.047619
:py attr minsupport	min support value	1.000000
transfer this instance's params to the wrapped	params to	0.035714
test that the final value of	sgdtests test	0.142857
the kolmogorov-smirnov ks test for data sampled	stat statistics kolmogorov smirnov test data distname	0.111111
table	table mode properties	0.200000
same time of day in utc	utc	0.050000
extract the week number of a given	weekofyear col	0.055556
out	sql	0.002525
resulting rdd that contains a tuple with	rdd cogroup other	0.066667
similarities between columns of this matrix	matrix column similarities threshold	1.000000
of fields in obj	sql	0.002525
ml params instances	params m1 m2	0.047619
temporary table in the catalog	table	0.031250
elements in this rdd by applying c{f}	core rdd key by f	1.000000
destroy all data and metadata	destroy	0.111111
set named options filter out those the	sql option utils set opts schema	0.333333
extract	extract	0.857143
process after the fork()	sock	0.166667
squared	squared	1.000000
instance contains a param with a given string	param params has param	0.019231
in the specified	tablename	0.043478
comparing instances	ldatest compare m1 m2	0.250000
deserialize	deserialize	1.000000
foreachrdd get	foreach get	1.000000
used again to wait	manager reset	0.011905
positive	positive	1.000000
of two vectors	ml linalg dense	0.100000
a paired rdd where the first element is	matrix factorization	0.040000
setparams(self featurescol="features", predictioncol="prediction",	gaussian mixture set params featurescol predictioncol	1.000000
resulting rdd that contains a tuple with the	rdd cogroup other	0.066667
groups the :class dataframe using the	sql data frame group by	0.200000
the year	year	0.040000
sets the given parameters in this grid	ml param grid builder	0.055556
or transform the rdd of	mllib hashing tf transform	0.045455
method to convert the java_model to a python	java_model	0.090909
positive rate	positive rate	0.500000
external database table	table	0.031250
instance contains a param with a given string	has	0.011628
or as specified by the optional key function	key	0.017857
infer schema from an rdd of row	sql spark session infer schema rdd samplingratio	0.250000
a resulting rdd that contains	rdd cogroup	0.066667
input param belongs to	param	0.006250
saves the content	format mode partitionby	0.100000
compute the dot product of two vectors we	mllib linalg dense vector dot other	0.058824
instance	param params	0.014925
degrees of	summary degrees of	1.000000
a given product and returns a list	product	0.029412
month of	dayofmonth col	0.031250
flume	flume	0.500000
list of active queries associated with	query manager active	0.066667
new sqlcontext	sqlcontext init	0.500000
columns in an input	columns	0.078431
java model from the	java	0.012195
given data type	datatype	0.045455
sets	tree regressor params set	1.000000
of column or names into a jvm seq	seq sc cols converter	0.055556
with weights	with	0.055556
:class windowspec with the frame	sql window range between	0.166667
points using the model trained	logistic regression model	0.083333
setparams(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor",	aftsurvival regression set params featurescol labelcol predictioncol	0.500000
java	java	0.329268
the values for each key	key func numpartitions	0.066667
a string representation	mllib linalg	0.026316
until any of the queries on the	query manager await any termination	0.142857
on the log likelihood of	ml ldamodel log likelihood	0.166667
which is defined	mllib regression	0.022727
a sliding window of time over this dstream	streaming dstream window	0.333333
of the :class dataframe	sql data frame	0.026738
count	core	0.003021
contains a param	params has	0.019231
squared distance from a sparsevector or	mllib linalg sparse vector squared distance	0.166667
the minutes of a given date	minute	0.040000
property that	property key value	0.125000
month of a given	sql dayofmonth col	0.031250
new :class dataframe that drops	data frame drop	0.250000
getattr	getattr	0.833333
levenshtein distance of the two given strings	sql levenshtein left right	0.058824
the top "num" number of	num	0.016807
adds	core	0.003021
sparkcontext is initialized or not	core spark context ensure initialized	0.333333
deviance for	generalized linear regression summary deviance	0.125000
add a py or zip dependency	spark context add py	0.166667
right singular vectors of the singularvaluedecomposition	singular value decomposition v	0.250000
converts matrix columns in an input dataframe to	convert matrix columns	0.083333
featurescol=none labelcol=none predictioncol=none classifier=none): sets params for onevsrest	one vs rest set params featurescol labelcol	1.000000
estimated coefficients and	generalized linear regression training summary	0.142857
changes the uid	params reset uid newuid	0.333333
2 ml params	params	0.006623
second is an	mllib matrix	0.047619
"predictions" which gives the true label of each	ml linear regression summary label col	0.333333
can be used to	frame	0.034483
this rdd was	core rdd	0.003460
return an rdd containing all pairs of	core rdd	0.003460
decision tree model for classification or	decision tree model	0.050000
applying a function to each partition	f	0.010526
much of memory for	object	0.027778
:class dataframe using the	data frame	0.005000
setparams(self featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0	set params featurescol labelcol predictioncol maxdepth	1.000000
transpose this blockmatrix returns a new	transpose	0.142857
sliding window	window	0.037037
returns an mlreader instance	mlreadable read cls	0.400000
defined on the	ml param	0.009524
of indices back to a new	index to	0.040000
content of the dataframe in	sql data frame writer	0.011628
:py attr lda keeplastcheckpoint is set to	ldamodel	0.034483
a multi-dimensional rollup for	data frame rollup	0.055556
the old hadoop outputformat api mapred	as hadoop dataset conf keyconverter valueconverter	0.083333
specifies the behavior when	frame writer mode savemode	0.333333
compute similarities between columns of this matrix	linalg row matrix column similarities threshold	1.000000
which is associated with this streamingcontext	context	0.022727
the null	ml generalized linear regression summary null	0.250000
a list or pandas dataframe	local data schema	0.500000
the sqltype() into class	cls	0.047619
a column of the current [[dataframe]] and perform	grouped data pivot pivot_col values	0.050000
awaitanytermination() can be used	sql streaming query manager	0.011905
the trigger	trigger	0.071429
saves the contents of	save path format mode partitionby	0.200000
per	per	0.833333
output by the given columns	writer bucket by	0.100000
code for a shared param class	param gen param code name	0.333333
number of clusters	mllib kmeans model k	0.250000
queries so that :func awaitanytermination()	manager reset	0.011905
which is a risk	regression summary	0.035714
creates	sql sqlcontext create	0.500000
matrix on	matrix	0.015152
a param by its name	param paramname	0.111111
ordered list of	ml string indexer	0.166667
field	field	0.555556
for k classes classification problem in	classes	0.034483
the initial value of weights	initial weights initialweights	0.333333
local property set in this thread or null	context get local property key	0.066667
sparse vector using either a dictionary a	vectors sparse size	0.166667
returns a list of predicted ratings for input	mllib matrix factorization model predict all user_product	0.050000
by applying a function to each element	f preservespartitioning	0.200000
note : experimental	fpgrowth	0.285714
maximized true default or minimized false	ml evaluator is larger better	0.166667
columns for the given table/view in the specified	columns tablename	1.000000
convert this matrix to the	matrix	0.015152
start inclusive to end inclusive	start end	0.181818
the coefficient of determination	regression metrics r2	0.166667
an associative and	core rdd	0.003460
in mixture	gaussian mixture model	0.052632
a function on each rdd of this dstream	streaming kafka dstream transform func	0.500000
creates a :class dataframe from	spark session create	0.058824
dataframe with two fields threshold precision curve	ml binary logistic regression summary precision by threshold	0.166667
queries so that :func	sql streaming	0.010204
used again	sql streaming query	0.011765
create a sparse vector using either a	mllib linalg vectors sparse	0.166667
converts vector columns	mlutils convert vector columns to ml	0.166667
get total number	total num	0.333333
standard deviation of this rdd's elements	rdd stdev	0.066667
converts matrix columns in an input dataframe	convert matrix columns from ml	0.166667
column	standard scaler model	0.090909
:class dataframe to a data source	data frame writer	0.014085
model from the input java	java estimator	0.200000
compute the dot product of	vector dot	0.050000
this model	ml kmeans model	0.500000
an array containing the ids of all	stage ids	0.055556
value of weights	weights	0.066667
can be	sql	0.002525
computes column-wise summary statistics for	statistics	0.090909
the sum	sum	0.125000
__init__(self estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none)	cross validator init estimator estimatorparammaps evaluator numfolds	1.000000
comprised of i	random rdds	0.038462
number	vector num	0.272727
matrix	dense matrix	0.076923
lower bound on the log likelihood	ldamodel log likelihood	0.142857
create an rdd for dataframe from	session create from	0.500000
based on the dataset	path	0.010204
an input stream that is	stream ssc	0.090909
an iterator of deserialized batches lists of	stream without unbatching	0.125000
wrapped java object and return the java object	java	0.012195
returns a :class dataframe representing the result	sqlcontext sql sqlquery	0.250000
wait a given amount of time	timeout catch_assertions	0.125000
frame boundaries from start	between start	0.100000
average values for each numeric columns for each	sql grouped	0.043478
using an	by	0.014286
column for approximate distinct count	approx count distinct	0.071429
of this instance with a randomly generated uid	ml one vs rest	0.052632
add a py	context add py file path	0.166667
a term to this accumulator's value	accumulator add term	0.066667
the least value of the	least	0.043478
initial value of	sgd set initial	0.111111
the embedded params to the companion	params transfer params to	0.333333
computes the levenshtein distance of the two given	levenshtein left	0.058824
serializes a stream of list	serializer	0.062500
sum for each numeric columns for each group	sql grouped data sum	0.083333
approximate distinct count of	approx count distinct	0.071429
of memory for this	external merger object size	0.032258
in libsvm format into label	parse libsvm	0.125000
map of words to their vector representations	mllib word2vec model get vectors	0.166667
given amount of time for a condition to	condition	0.045455
to this accumulator's	add	0.035714
compute the number	mllib linalg distributed matrix num	0.166667
until any of the queries on the	await any termination	0.142857
singular value	singular value	0.500000
:py attr finalstoragelevel	final storage level value	1.000000
list of functions registered in the	catalog list functions	0.250000
computes the levenshtein distance of the two given	sql levenshtein	0.058824
the given path a shortcut of write() save	ml mlwritable save	0.166667
of two vectors we	ml linalg dense vector	0.100000
comprised of vectors	random rdds gamma vector	0.125000
bandwidth of each sample defaults	bandwidth bandwidth	0.125000
left multiplies this blockmatrix by other, another blockmatrix	linalg block matrix multiply other	0.200000
java object by pyrolite whenever the	java object rdd	0.500000
followed by a heappush	core heapreplace	0.250000
this task belong	core task context	0.100000
number of	ml logistic	0.166667
parameters in this grid	grid builder	0.055556
stored in the column	col	0.016393
checkpoint data or	streaming streaming context get or	0.200000
parse	sql parse	0.500000
basic operation test for dstream mappartitions	streaming basic operation tests test map	0.333333
squared distance from a sparsevector	linalg sparse vector squared distance other	0.166667
multi-dimensional rollup for the current :class	sql data frame rollup	0.055556
quantiles of numerical	quantile col probabilities relativeerror	0.166667
this instance's params	ml java params	0.125000
model fitted by	model	0.094972
for na fill()	frame fillna value subset	0.166667
:class	sql data frame	0.005348
impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)	ml random forest classifier	0.023256
class because it's heavy used in tointernal	sql user defined type cached sql type cls	0.500000
min value for each original column	original min	0.250000
returns a :class dataframe representing	sqlcontext sql sqlquery	0.250000
this vector to the new mllib-local representation	vector as ml	0.250000
find	core find	0.500000
new dstream by applying reducebykey to each	streaming dstream reduce by	0.076923
the objects	merger object size	0.032258
the :class dataframe to	sql data frame	0.005348
ignore separators inside brackets pairs	ignore brackets	0.250000
list of names of tables	names	0.050000
curve	ml binary logistic	1.000000
invalidate and refresh all the cached	hive context refresh	0.200000
instance contains a param with a given	params has param	0.019231
columns in an input dataframe	columns from ml dataset	0.125000
add a py or	spark context add py	0.166667
least value of the list of column	sql least	0.055556
queries	reset	0.011236
this udf with a function and	function	0.027778
the vector representation of each word in vocabulary	mllib word2vec fit data	0.200000
given parameters in this grid	grid builder	0.055556
of this instance with a randomly generated	ml cross validator model	0.333333
accumulator's value	core accumulator value	0.045455
the null	null	0.125000
rcond	rcond	0.833333
svmmodel	svmmodel	1.000000
or compute	linalg indexed row matrix	0.250000
sampled subset	sample withreplacement fraction seed	0.333333
make sure user configuration is respected spark-19307	test user configuration	1.000000
length of	sql length col	0.050000
persists the underlying rdd with the specified	mllib linalg block matrix persist	1.000000
python rdd of key-value pairs (of form	rdd save	0.038462
how much of memory for this	merger	0.025641
of	core external merger	0.032258
term to this accumulator's value	accumulator add term	0.066667
stringindexer	string indexer	0.250000
create an input stream that pulls	utils create stream ssc hostname	0.200000
implicit	implicit	1.000000
converts vector columns in an	convert vector columns from ml	0.166667
matrix on the driver as	to local matrix	0.250000
method for given unary operator	sql unary op name doc	0.200000
write() save path	pipeline save path	1.000000
the python direct kafka rdd api	kafka rdd	0.285714
has completed	completed outputoperationcompleted	0.125000
sets	tree params set	1.000000
from this thread such as the spark fair	spark context	0.023256
which	mllib regression	0.068182
list of numpy arrays	ml bisecting	0.066667
:func awaitanytermination() can be	sql streaming query manager	0.011905
type	data type	0.500000
:py attr lda keeplastcheckpoint is set to	distributed ldamodel	0.052632
dot product	dot other	0.100000
elements with matching keys in c{self} and c{other}	join other numpartitions	0.071429
get all values as a list	spark conf get all	0.166667
an rdd comprised of	mllib random rdds gamma vector rdd	0.166667
obj assume that	merger object size obj	0.040000
norm	norm	0.333333
classes values which	classes	0.034483
a left outer join of	left outer join	0.111111
returns the root mean squared error which is	linear regression summary	0.013889
makes a class inherit documentation from its parents	inherit	0.037037
distributed model to a local	ml distributed ldamodel to local	0.111111
classes values which the label can	classes	0.034483
again to wait for new terminations	streaming	0.005025
converts a labeledpoint to	mlutils convert labeled point to	0.250000
given parameters in this grid to fixed	param grid builder base on	0.076923
objects	core external merger	0.032258
large dataset and an item	nearest neighbors dataset	0.333333
for fitting and predicting	streaming	0.005025
of	ml tree	0.250000
that pulls events from flume	streaming flume utils	0.200000
wait for the execution to stop return true	context await termination or timeout timeout	0.125000
compare 2 ml types asserting that	persistence test compare	0.166667
is of	ml ldamodel is	0.066667
sets the sql context to use for loading	ml mlreader context sqlcontext	0.333333
ml types asserting that they are equivalent	pipelines m1 m2	0.166667
return a l{statcounter} object that captures	core rdd stats	0.083333
this	mllib linalg	0.026316
instance contains a param with a	ml param	0.009524
in a sliding window	value and window windowduration slideduration numpartitions	0.076923
tests whether this instance contains	ml param params has param paramname	0.142857
return sparkcontext	context spark	0.083333
drops	drop	0.222222
squared distance from a sparsevector or 1-dimensional numpy	mllib linalg sparse vector squared distance	0.166667
transforms a	params transfer param	0.250000
convert python object into java	py2java sc obj	0.333333
comprised of vectors	mllib random rdds log	0.125000
an exception if any error is found	mllib	0.010526
partitioned data	external group	0.045455
dataframe in a text	data frame writer text	0.200000
of day in the given timezone returns	sql	0.002525
value to a mllib vector if possible	ml param type converters to vector	0.333333
values of the accumulator's data type returning	accumulator param	0.038462
set the initial value of weights	streaming logistic regression with sgd set initial weights	0.333333
object	obj java_class	1.000000
spark sink deployed on a	storagelevel maxbatchsize	0.045455
instance with a randomly	split	0.125000
waits for the termination of	termination timeout	0.041667
is later than the value of the date	next day date dayofweek	0.333333
collect the distributed matrix on the driver as	block matrix to local matrix	0.250000
makes a class inherit documentation from	mllib inherit doc cls	0.045455
singular vectors of the singularvaluedecomposition	singular value decomposition v	0.250000
setparams(self	discretizer set params	1.000000
so that	sql streaming query manager	0.011905
documentation of all params with their optionally default	ml param params explain params	0.166667
a dstream	dstream	0.093750
default min number of partitions for hadoop rdds	spark context default min partitions	0.250000
datatype with types inferred from obj	type obj datatype	0.333333
or gets	get	0.021739
set named options filter out those the	set opts schema	0.333333
test that the model params are	kmeans test test model params	0.250000
set named options filter out those	set opts schema	0.333333
to a mllib vector if possible	ml param type converters to vector value	0.333333
a new dstream in which each rdd is	streaming streaming context transform	0.066667
of the :class dataframe to a data	data frame writer	0.014085
active stages	tracker get active	0.333333
cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none	random forest classifier	0.022727
extract the day of the month of	dayofmonth col	0.031250
thresholds thresholds in	thresholds	0.071429
year of a given date as	year col	0.050000
the deviance for the	summary deviance	0.125000
correlation	col2 method	0.055556
checkpointing using	checkpoint	0.062500
weights computed for every	mllib linear model weights	0.250000
+= operator	accumulator iadd	0.500000
test	tests test map values	1.000000
sinceversion	sinceversion	1.000000
ownership	ml param params resolve	0.333333
with a	ml param	0.009524
at text file	text files prefix suffix	1.000000
the test this	test	0.015152
of names of tables in	names	0.050000
precision of all the	precision	0.076923
use for saving	java mlwriter	0.250000
again to wait for	sql streaming query	0.011765
results immediately	locally	0.083333
tokens in the training set given	ldamodel training	0.034483
configuration	conf	0.300000
stop the execution of the streams with	streaming context stop	0.125000
array of layer	ml multilayer perceptron classification	0.333333
test	tests test count by	1.000000
objects	core external	0.016129
model should have been	loader	0.125000
names into a jvm seq of column	seq	0.043478
data of a	sql data	0.024390
the null model	ml generalized linear regression summary null	0.250000
number of nonzero elements	mllib linalg dense vector num	0.200000
comprised of vectors	random rdds poisson	0.125000
log	log	0.571429
given value	sql	0.002525
add a py or zip dependency for all	spark context add py	0.166667
vectors or transform the rdd	hashing tf transform	0.045455
changes the uid of this instance this updates	ml param params reset uid	0.058824
dataframe representing the result of the	sqlquery	0.054054
udf with a function and	defined function	0.066667
list of columns for the given	catalog list columns	0.166667
companion	ml java params transfer	0.125000
called when processing of	streaming listener on output operation	0.166667
with extra values from input into	map extra	0.040000
return an rdd containing all pairs of	rdd	0.003058
matrix columns in an	matrix columns to	0.142857
c{self} and c{other}	core rdd	0.006920
set bandwidth of each sample defaults	mllib stat kernel density set bandwidth bandwidth	0.142857
"predictions" which gives the true label	logistic regression summary label	0.333333
column for approximate distinct count of col	approx count distinct	0.071429
inside brackets pairs e	brackets split	0.083333
of the points belongs to in this model	kmeans model predict x	0.333333
until any of the queries	any	0.083333
basic operation test for dstream groupbykey	streaming basic operation tests test	0.111111
return a l{statcounter} object that captures the mean	core rdd stats	0.083333
return a new dstream by applying reducebykey to	streaming dstream reduce by key func	0.076923
add a py or zip	context add py	0.166667
return an rdd created by piping elements	rdd	0.003058
the model on toy data	on model	0.166667
partial	cloud pickler save partial obj	0.125000
be used again to	sql	0.002525
schema of	schema	0.033333
a param and validates the ownership	params resolve param param	0.333333
as min-max normalization or rescaling	min	0.041667
a map of words to their vector representations	get vectors	0.142857
to their vector representations	model get vectors	0.142857
a new dstream in which each rdd	by	0.014286
trait for multivariate statistical summary	multivariate statistical summary	0.250000
users for a given product	users product	0.142857
number	distributed matrix num	0.500000
a local property set in this thread or	get local property key	0.066667
from this block matrix	mllib linalg block matrix	0.052632
a function	defined function	0.066667
converts vector columns in an input dataframe	mllib mlutils convert vector columns to ml dataset	0.166667
the population should be a	mllib stat kernel density	0.066667
fast version of a heappush followed by a	core heappushpop heap	0.142857
of the :class dataframe as	sql data frame writer save as	0.071429
compute the dot product of two	vector dot other	0.050000
of the :class dataframe to a	frame writer save path	0.066667
removes the specified table from	catalog uncache table tablename	0.250000
as the spark fair scheduler pool	spark	0.013158
layer	multilayer perceptron classification	0.333333
which is a risk function corresponding	regression metrics	0.083333
version of a	heap item	0.125000
of months between	months between	0.333333
"predictions" which gives the true label of each	ml logistic regression summary label	0.333333
return sparkcontext	streaming context spark	0.083333
get a local property set in this	core spark context get local property key	0.066667
convert a list of column or names into	sql to	0.041667
contains a param with	param	0.012500
returns subset accuracy	metrics subset accuracy	1.000000
a streamingcontext from checkpoint data or create a	streaming streaming context get or create cls	1.000000
column scipy matrix from	mllib sci py tests scipy matrix size	0.090909
"predictions" which gives the predicted value of	ml linear regression summary prediction	0.500000
this instance contains a	ml param params has	0.019231
sort the list based on first	sort result based on key	0.333333
vs	vs	1.000000
daemon and workers terminate when stdin is closed	core daemon tests test termination stdin	1.000000
value of	ml regex tokenizer	1.000000
paired rdd where the first element is	matrix factorization model	0.043478
basic operation test for dstream	basic operation tests test	0.555556
of jobs has completed	completed	0.058824
global temporary	global temp	0.500000
all active stages	get active	0.333333
akaike's "an information criterion" aic for the fitted	generalized linear regression summary aic	0.250000
dummy params instance used as a	ml param params dummy	0.111111
new sparkcontext at	spark context init	0.083333
ints	int	0.142857
returns the soundex encoding	sql soundex	0.055556
attr lda keeplastcheckpoint is	ml distributed ldamodel	0.050000
list	list	0.666667
set the trigger for the stream	stream writer trigger	0.083333
converts matrix columns in an	convert matrix columns from	0.166667
tz	tz	1.000000
new hadoop outputformat api mapreduce package	new apihadoop dataset conf keyconverter valueconverter	0.142857
data type json	sql parse datatype json	0.333333
compare 2 ml params instances for	compare params m1	0.200000
the underlying output	data frame writer format	0.333333
this matrix to a rowmatrix	indexed row matrix to row matrix	0.333333
randomly generated	train validation split model	0.166667
in	builder base on	1.000000
of another dstream	other	0.033333
replacing a value with another value	replace to_replace value	1.000000
matrix columns in	matrix columns	0.142857
mixin for param fitintercept	has	0.011628
the model params are set	model params	0.125000
them with extra values from input into	extra	0.023810
l1-norm	metrics	0.041667
get all values as a list of key-value	spark conf get all	0.166667
i d samples drawn	numrows numcols numpartitions	0.125000
parses the expression string into the column	sql expr str	0.125000
the index of the	index	0.041667
parses the given data type json string	sql parse datatype json string json_string	1.000000
approximate distinct count of	approx count distinct col	0.071429
specifies the underlying output data source	stream writer format source	0.333333
convert python object into java	mllib py2java sc obj	0.333333
the precision-recall curve which is	binary logistic regression summary pr	0.083333
comprised of vectors containing i i	random rdds gamma	0.125000
parameters in this grid	ml param grid builder base	0.076923
the cluster centers represented as	kmeans model cluster centers	0.090909
null	regression summary null	0.250000
libsvm format into an rdd of labeledpoint	lib svmfile sc path numfeatures minpartitions	0.125000
instance with a randomly generated	train validation split	0.166667
labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction",	labelcol predictioncol probabilitycol	0.166667
or create global	or create	1.000000
model coefficients	model coefficients	1.000000
wait until any	any termination timeout	0.166667
a jvm seq	seq	0.043478
column for approximate distinct count	approx count distinct col	0.071429
for this obj assume that all the	external merger object size obj	0.040000
intermediate	intermediate	1.000000
regparam	reg param	1.000000
finding frequent items for columns possibly	sql data frame freq items	0.166667
curve which is a dataframe	binary logistic regression	0.142857
so	streaming query manager reset	0.011905
note : experimental	imputer model	1.000000
using an associative and commutative reduce function	core rdd reduce	0.083333
the number of rows	linalg indexed row matrix num rows	0.200000
string name	ml param params has param	0.019231
error which is defined as	mllib regression	0.022727
infer schema from an rdd of row	session infer schema rdd	0.250000
to make predictions on batches of data	streaming linear algorithm predict on	0.066667
an input stream that is	stream	0.017544
if	if	1.000000
sets	bayes set	1.000000
bucketed	bucketed	1.000000
if computeu was set	u	0.111111
setparams(self inputcol=none outputcol=none labels=none) sets params for	string set params inputcol outputcol labels	0.333333
:func awaitanytermination() can be used again to wait	query manager	0.011905
memory string in the format supported by	memory s	0.142857
word2vec model's vocabulary default 5	word2vec	0.052632
awaitanytermination() can be	streaming	0.005025
used again to wait for	streaming query manager	0.011236
the first n elements in the	n	0.027778
perform a left outer join of	rdd left outer join other numpartitions	0.111111
instance contains a param with	ml param params has	0.019231
for the test this should be	test	0.015152
set	stat kernel density set	1.000000
return sparkcontext which	context spark	0.083333
sets	raw prediction col set	1.000000
create a converter to drop the names of	create converter	0.166667
or transform	mllib hashing tf transform	0.045455
the given columns if specified	sql data frame	0.005348
return the first	py spark streaming	0.333333
returns weighted averaged recall	metrics weighted recall	1.000000
the root	get root	0.333333
main entry point for spark streaming functionality a	streaming context	0.055556
of this instance this	ml	0.001835
block matrix other from this block	mllib linalg block	0.111111
mark the rdd as non-persistent and remove all	rdd unpersist	0.066667
min value for each original	scaler model original min	0.250000
deserialized batches lists of objects from	serializer load stream without unbatching	0.200000
sort the list based on first	spark streaming test case sort result based on	0.333333
the uid of this instance this	ml param params reset uid newuid	0.058824
week number of a given	sql weekofyear	0.055556
parammap into a java parammap	to java pyparammap	0.250000
cost sum	cost	0.105263
layers	layers	1.000000
or names into a jvm seq of column	seq sc cols converter	0.055556
in utc	from utc	0.125000
representing the	sqlquery	0.027027
column of the current [[dataframe]] and	grouped data pivot pivot_col values	0.050000
function types	function	0.027778
for each key using an associative	by key func numpartitions partitionfunc	0.066667
column for distinct count of col	count distinct	0.040000
:func awaitanytermination() can be used again	streaming query manager	0.011236
given parameters in this grid to fixed	ml param grid builder	0.055556
a new	init	0.042553
the content of the dataframe in a	data frame writer	0.014085
isotonicregressionmodel	isotonic regression model	0.100000
values for each numeric columns for each group	grouped data avg	0.058824
terminations	streaming query	0.010526
instance contains a param	params has param	0.019231
string in the format supported by java e	s	0.071429
:func	reset	0.011236
set initial centers should be set before calling	set initial centers centers	0.200000
extract a specific group matched by a java	regexp extract str pattern idx	0.333333
use in broadcast	sql broadcast	0.500000
a given string name	has param	0.019231
value of	ml train validation split	0.333333
:class dataframe from external storage systems (e	data frame reader	0.166667
the deviance for the null model	linear regression summary null deviance	0.250000
the driver returns	driver	0.090909
an input option for	stream reader option key	0.333333
rdd for	rdd	0.003058
new	as new	0.125000
get total number of nodes summed over all	total num nodes	0.250000
the specified schema	schema options	0.125000
marks	blocking	0.142857
for list	list of list	0.333333
withmean	with mean	1.000000
parameters in this grid to fixed	param grid builder base	0.076923
stdout	core profiler show	0.166667
the trigger for the stream query if this	data stream writer trigger	0.083333
specific topic and partition for kafka	topic and partition	0.111111
be used again to wait for new terminations	streaming query manager	0.011236
1 0] for feature selection by fdr	chi sq selector set fdr fdr	0.200000
for the given columns specified	sql data frame	0.005348
much of memory for	object size	0.032258
gradient-boosted trees gbts <http //en wikipedia org/wiki/gradient_boosting>_	gbtregressor	0.125000
given data	sql	0.005051
a jvm seq of columns that describes	cols cols kwargs	0.090909
to persist its values across operations after the	persist storagelevel	0.166667
generates python code for a shared param class	param gen param code name doc defaultvaluestr	0.333333
can be used again to wait for new	manager reset	0.011905
prefix of string	prefix	0.083333
contains a param	params has param	0.019231
or as specified by the optional key	key	0.017857
this obj assume that	merger object size obj	0.040000
the sum for each numeric columns	sql grouped data sum	0.083333
__init__(self featurescol="features", labelcol="label", predictioncol="prediction",	ml linear regression init featurescol labelcol predictioncol	1.000000
cluster centers represented as	cluster centers	0.090909
the deviance for the null model	null deviance	0.250000
standardization whether to standardize the training	standardization	0.076923
mixin for param	has	0.244186
are the left singular vectors	mllib linalg singular	0.017544
:func awaitanytermination()	manager reset	0.011905
are the right singular vectors of	singular	0.015625
represents a matrix	matrix	0.015152
evaluates	java evaluator evaluate dataset	0.333333
then merges them with extra values	extra	0.023810
get or compute the number of rows	linalg row matrix num rows	0.200000
converts a labeledpoint to a string in	mlutils convert labeled point to	0.250000
shortcut of write() save	ml pipeline save	0.166667
vector to	linalg sparse vector	0.111111
has	has	0.058140
a range of offsets	range	0.030303
an rdd	core rdd take	0.200000
model trained using multinomial/binary logistic regression	logistic regression model	0.083333
a	spark submit	0.200000
returns the least value of	sql least	0.055556
calculates the length of a	sql length	0.050000
accumulator's value only usable	accumulator value	0.050000
libsvm format into label indices values	mlutils parse libsvm	0.125000
convert this matrix	matrix	0.015152
a labeledpoint to	labeled point to	0.125000
accumulator's value only	core accumulator value	0.045455
convert this matrix to the new mllib-local representation	mllib linalg matrix as ml	1.000000
centroids of that	timeunit	0.025641
ids of all active	active stage ids	0.200000
the month of a given date as integer	dayofmonth	0.027027
setparams(self min=0 0 max=1 0 inputcol=none outputcol=none)	scaler set params min max inputcol outputcol	1.000000
the idf vector	idfmodel idf	0.166667
returns an numpy ndarray	mllib linalg dense vector to array	1.000000
exception that stopped	streaming query exception	0.500000
+= operator adds a term	iadd term	0.142857
ignore separators inside brackets pairs e	sql ignore brackets	0.250000
a right outer join	full outer join	0.111111
test that the final	streaming logistic regression with sgdtests test	0.111111
lda keeplastcheckpoint is	distributed ldamodel	0.052632
__init__(self	hash lsh init	1.000000
cost sum of squared	cost	0.105263
train the model	linear regression with sgd train	1.000000
functions registered in the	functions	0.071429
wait for	manager reset	0.011905
table/view in the specified database	tablename dbname	0.142857
stream and	stream reader	0.076923
finding frequent	sql data frame freq	1.000000
:py attr lda keeplastcheckpoint	ldamodel	0.034483
for each original column	model original	0.062500
the month of a	dayofmonth	0.027027
until any of the	any termination	0.142857
training set given	ldamodel training	0.034483
the new hadoop outputformat api mapreduce	new apihadoop dataset conf keyconverter valueconverter	0.142857
the topics described	describe topics maxtermspertopic	0.333333
:class dataframe representing the database table	sql data frame reader	0.111111
cluster centers represented as a	kmeans model cluster centers	0.090909
c{self} that is not contained in c{other}	subtract other numpartitions	0.111111
parameters in this grid to fixed	param grid builder add grid param	0.250000
left outer join of	left outer join other numpartitions	0.111111
queries so that :func awaitanytermination()	sql streaming query manager	0.011905
partitioned data into disks	core external group	0.045455
right outer	rdd full outer	0.333333
returns the soundex encoding	soundex col	0.055556
values of the accumulator's	accumulator param	0.038462
dictionary a list of index value	init size	0.066667
represents a range of offsets from	offset range	0.047619
the uid	params reset uid	0.333333
:class datatype the data type	datatype	0.045455
json	writer json path mode	0.125000
local representation this discards info	local	0.038462
impurity	impurity	1.000000
hostname port data is received	hostname port storagelevel	0.500000
awaitanytermination() can	manager	0.011236
threshold=0 0 weightcol=none aggregationdepth=2):	ml linear svc	0.333333
kmeans algorithm for	streaming kmeans	0.035714
precision	metrics precision	0.200000
adds a term to	accumulator add term	0.066667
of gaussians in mixture	mllib gaussian mixture	0.045455
to the input dataset with optional parameters	dataset params	0.166667
started by this thread until the group	group	0.025641
context to	context	0.136364
once	once	1.000000
checkpointinterval=10 impurity="variance", seed=none	ml decision tree regressor	0.066667
values for each numeric columns for	sql grouped	0.043478
test that	test test	0.333333
file system using the old hadoop	hadoop	0.050000
that all the	external	0.013889
given block matrix other from this block matrix	mllib linalg block matrix	0.052632
ids	stage ids	0.055556
for new terminations	sql streaming query	0.011765
broadcast a	broadcast value	0.125000
accuracy equals to the total number	accuracy	0.076923
predict the value	predict x	0.016949
gets frequent sequences	prefix span model freq sequences	1.000000
pivot_col	pivot_col	1.000000
configure the kmeans algorithm for	streaming kmeans	0.035714
setparams(self	max scaler set params	1.000000
left outer join of c{self}	left outer join other	0.111111
registered with the dispatch to handle all	core cloud pickler save	0.166667
deviance for the	ml generalized linear regression summary deviance	0.125000
return sparkcontext which is associated	context spark	0.083333
squared distance between	squared distance v1 v2	1.000000
for feature selection by fwe	chi sq selector set fwe fwe	0.200000
predicted ratings for input user	matrix factorization model predict all user_product	0.050000
streams with option of ensuring all received	stopsparkcontext stopgracefully	0.050000
set a local property	context set local property key	0.200000
number of	ml	0.005505
field in "predictions" which gives	logistic regression summary	0.090909
default min number of partitions	spark context default min partitions	0.250000
lda keeplastcheckpoint is set to	ml distributed ldamodel get	0.066667
timeout	timeout	0.357143
python direct kafka stream	kafka direct stream from	0.125000
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20	random forest classifier	0.022727
set decay factor	mllib streaming kmeans set decay factor decayfactor	1.000000
behavior when data or table already	data frame writer mode savemode	0.071429
of values	values	0.100000
temp method for comparing instances	ldatest compare m1 m2	0.250000
returns the least value	least	0.043478
squared distance from a sparsevector or	vector squared distance	0.166667
a new :class dataframe with	sql data frame drop	0.333333
a unique	id	0.083333
comprised of vectors	random rdds exponential	0.125000
__init__(self	abs scaler init	1.000000
operation test for dstream count	operation tests test count	0.500000
value of each instance	ml	0.001835
in "predictions" which gives	linear regression summary	0.041667
:class dataframe, using the	data frame	0.005000
null	summary null	0.250000
given product	product	0.029412
items as iterator	items	0.066667
for each key	by key	0.026316
multi-dimensional cube for the	frame cube	0.055556
of products	products	0.142857
minutes of a given date	sql minute	0.050000
:class dataframe representing the	sql data frame reader	0.111111
number of gaussians in mixture	gaussian mixture	0.038462
be	sql streaming query	0.011765
the underlying rdd	mllib	0.010526
python direct kafka rdd api with	kafka rdd with	0.500000
centers	centers	0.250000
this	core	0.036254
wait for	sql	0.002525
lda	lda	0.333333
nodes in tree	decision tree	0.076923
int default none	thresh	1.000000
convert this matrix to the	linalg matrix	0.333333
the python direct kafka stream foreachrdd get offsetranges	kafka direct stream foreach get offset ranges	0.500000
with arbitrary key and value	core spark context	0.023256
remover	remover	1.000000
term frequency vectors or transform	mllib hashing tf transform	0.045455
test the python direct kafka	kafka stream tests test kafka direct	1.000000
the dot product	linalg dense vector dot other	0.058824
columns in an input dataframe from the :py	columns to ml dataset	0.125000
that :func awaitanytermination() can be used again	query manager reset	0.011905
:param rdd an rdd of	iteration clustering train cls rdd	0.250000
returns the greatest value	sql greatest	0.055556
an associative function "func" and a	core	0.003021
the user and the second is an	mllib matrix	0.047619
root mean squared error which is	mllib regression	0.022727
of each key	by key	0.026316
create a new dstream in	streaming streaming context transform	0.066667
the non-streaming :class dataframe	data frame write	0.166667
value to an int	to int value	0.250000
c{other}, return a resulting rdd that contains	rdd cogroup other	0.066667
the true label	label	0.142857
cached	cached	0.833333
for	size	0.009174
a csv	csv	0.222222
get a local	get local	0.333333
in the column	col	0.016393
to use for loading	ml mlreader	0.222222
can be	frame	0.034483
greatest value of the list	greatest	0.043478
of gaussians in mixture	mllib gaussian mixture model k	0.200000
a jvm seq	seq sc cols converter	0.055556
note : experimental	bucketed random projection lsh	1.000000
resets the configuration property for	sql runtime config unset	0.142857
clusters	mllib kmeans model k	0.250000
column after position pos	str pos	0.250000
sets the given parameters in this grid to	param grid builder base on	0.076923
return the column	mllib standard	0.125000
property if not already set	set if missing	1.000000
a coordinatematrix	coordinate	0.200000
python topicandpartition to map to	init topic partition	0.055556
which is a dataframe having two fields	regression summary	0.035714
libsvm format into an rdd of labeledpoint	mllib mlutils load lib svmfile sc	0.125000
left outer join of	left outer join	0.111111
broker to map to the java related object	broker init host port	1.000000
numiterations	numiterations	0.250000
generates an rdd comprised of vectors	mllib random rdds normal vector rdd sc	0.200000
session to use for loading	mlreader session	1.000000
file to	file path	0.035714
instance for this	one vs rest	0.034483
contents of the :class dataframe to a data	sql data frame writer save path	0.142857
get a local property set in	local property key	0.035714
observed tokens in the training set given the	training	0.029412
a python topicandpartition to	and partition init topic partition	0.055556
to set k decayfactor	streaming	0.005025
to wait for new terminations	query manager	0.011905
parses	json_string	0.125000
regression model	regression model	0.062500
restore an object of	core restore	0.333333
a java model from	java cls	0.111111
:func awaitanytermination() can be	manager reset	0.011905
create an rdd that has no partitions	spark context empty rdd	0.200000
input java	java estimator	0.200000
comprised of vectors containing i	mllib random rdds log normal	0.125000
or compute the number of	mllib linalg distributed matrix num	0.166667
create a python topicandpartition to	topic and partition init topic partition	0.055556
get a local property set in this thread	local property key	0.035714
of index value pairs or two	linalg	0.044444
partitioned data into	by spill	0.047619
with a	params	0.006623
the correlation of two columns of a	corr col1 col2 method	0.055556
queries so	sql streaming	0.010204
parameters specified by the param	param	0.006250
column of the current [[dataframe]] and perform	grouped data pivot pivot_col values	0.050000
sets the spark session to use	session sparksession	0.083333
a dummy params instance used as	param params dummy	0.111111
indexing categorical feature columns in	indexer	0.055556
for each numeric columns for each group	grouped data avg	0.058824
until any of	any	0.083333
or compute the number	linalg block matrix num	0.100000
convert this matrix to the new mllib-local representation	dense matrix as	0.333333
wait	context await termination timeout	0.166667
cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20	random forest classifier	0.022727
the residual degrees of freedom for	regression summary residual degree of freedom	0.125000
memory for this obj assume that all	merger object size obj	0.040000
:func awaitanytermination()	sql streaming query manager	0.011905
partial objects do not serialize correctly in python2	core cloud pickler save partial	0.125000
labeledpoint to a string in libsvm format	labeled point to libsvm	0.500000
input java	ml java estimator	0.200000
setparams(self stages=none) sets params for	set params stages	0.500000
setparams(self scalingvec=none inputcol=none outputcol=none) sets params for	set params scalingvec inputcol outputcol	0.333333
test that the model params are set correctly	mllib streaming kmeans test test model params	0.250000
list of names of tables in the	sqlcontext table names	0.066667
given parameters in	builder	0.090909
mixin for param threshold threshold in	has threshold	0.250000
stdout id is the rdd id	profiler show id	0.333333
exposes information about spark jobs	spark job info	1.000000
ignore separators inside brackets pairs e g	sql ignore brackets	0.250000
file	file obj	1.000000
a value to a mllib vector if possible	ml param type converters to vector	0.333333
be	sql streaming query manager	0.011905
dump already	core	0.003021
newline-delimited json	writer json path mode	0.125000
summed over all trees in the ensemble	tree ensemble model	0.076923
brackets pairs e g	brackets	0.058824
returns the value of spark sql configuration	sql sqlcontext get conf	0.333333
of gaussians in mixture	mixture	0.052632
computes average values for each numeric columns for	grouped	0.035714
return a new dstream in which each rdd	by	0.014286
"predictions"	summary	0.073171
perform a left outer join of c{self}	rdd left outer join other numpartitions	0.111111
returns the current idf vector	idfmodel idf	0.166667
decision tree model for classification or regression	decision tree model	0.050000
rdd of key-value pairs (of	rdd save	0.038462
group the values for each key	group by key numpartitions	0.333333
_jconf	_jconf	0.833333
large dataset and an item approximately find at	lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
whether this instance is of type	ml ldamodel is	0.066667
compute the dot product of two vectors we	vector dot other	0.050000
a param with a given	ml param params has param	0.019231
of this instance with a randomly generated	ml train validation split	0.666667
computes column-wise summary statistics	mllib stat statistics	0.125000
used again to	query	0.010753
compare 2 ml params instances for	compare params	0.200000
kernel	kernel	1.000000
path a shortcut of write()	ml mlwritable	0.142857
points belongs	predict x	0.033898
return a copy of the rdd partitioned	core rdd	0.003460
to	streaming query manager reset	0.011905
new	streaming	0.005025
impurity="variance",	ml decision tree regressor	0.066667
slideduration	slideduration	1.000000
how	merger object size	0.032258
be used	sql streaming query	0.011765
distributed model to a local representation this	distributed ldamodel to local	0.111111
numclasses	numclasses	0.555556
a :class dataframe	sql data frame	0.010695
convert matrix attributes	linalg matrix convert	0.166667
wait for the execution to	context await termination timeout	0.166667
:func awaitanytermination() can be used again	sql streaming query manager	0.011905
a text file at the specified path	writer text path compression	0.333333
topics described by weighted terms	mllib ldamodel describe topics maxtermspertopic	0.333333
again to wait for new	sql streaming	0.010204
underlying sql storage type	type sql type	0.250000
converts vector columns in an input dataframe	convert vector columns to ml dataset	0.166667
used again to wait	streaming	0.005025
generates an rdd comprised of vectors containing i	mllib random rdds gamma vector rdd sc	0.200000
:py attr maxiter	max iter value	1.000000
awaitanytermination() can	query manager reset	0.011905
resets the configuration property	sql runtime config unset	0.142857
converts matrix columns in an input dataframe	mlutils convert matrix columns from ml	0.166667
which gives the predicted	regression summary prediction col	1.000000
sort the list based on first	sort result based on key outputs	0.333333
model in jvm	vector transformer	0.250000
with a given string	ml	0.001835
output a python rdd	rdd	0.012232
__init__(self inputcol=none outputcol=none labels=none)	init inputcol outputcol labels	1.000000
parquet file	parquet path	0.333333
should	should	1.000000
setparams(self	ml linear regression set params	1.000000
given a java onevsrest create and return	one vs rest from java cls	0.200000
for params shared by them	params	0.006623
evaluates the model on a test dataset	ml generalized linear regression model evaluate dataset	1.000000
levenshtein distance	levenshtein left right	0.058824
:class dataframe as a	data frame as	0.333333
the soundex encoding for	sql soundex	0.055556
given data type json string	parse datatype json string	0.333333
values	mllib standard scaler model	0.100000
containing names of tables	tables	0.071429
can be used again	manager	0.011236
n elements from an rdd	rdd	0.003058
by applying a function	map f	0.074074
finding frequent items for columns	frame freq items cols	0.166667
belongs to this params	params	0.006623
collection function	size	0.009174
on all nodes or any hadoop-supported	path	0.020408
the length of	sql length col	0.050000
how data of	data	0.011628
:class dataframe	data	0.081395
bound on the log likelihood	log likelihood dataset	0.142857
add two	add	0.035714
a param	params	0.006623
of the rdd's elements in one operation	rdd	0.003058
a method for given unary operator	sql unary op name doc	0.200000
:class pyspark sql	sql to	0.083333
the given parameters in this grid to fixed	grid builder	0.055556
finding frequent items for columns possibly with	data frame freq items cols	0.166667
min value for each original column	model original min	0.250000
train the model on the	with sgd train on	0.333333
in rdd 'x' to all mixture	gaussian mixture model	0.052632
fraction	fraction	1.000000
an indexedrowmatrix	indexed row	1.000000
__init__(self	regression init	1.000000
the kmeans algorithm for fitting and predicting on	kmeans	0.025641
copy all params defined on the class to	param params copy params	0.200000
generated by applying mappartitionswithindex() to each rdds	map partitions with index f preservespartitioning	0.055556
of nonzero elements this	ml linalg dense vector	0.100000
returns weighted false positive rate	mllib multiclass metrics weighted false positive rate	1.000000
the null model	linear regression summary null	0.250000
datatype the data type string format	datatype string	0.111111
submit and test a single script file	core spark submit tests test single script	0.500000
weekofyear	weekofyear	0.217391
on the	on	0.148148
specific group matched by a java regex from	str pattern idx	0.111111
dstream	streaming dstream	0.138889
dm = densematrix(2 2 range 4	mllib linalg dense matrix repr	0.142857
instance	ml one vs rest	0.052632
hadoop configuration which is passed in as a	spark context hadoop	0.090909
rdd an rdd of	train cls rdd	0.250000
into the returned	test case	0.333333
to an external database table via	url table mode	0.200000
into a java parammap	map to java pyparammap	0.250000
:class dataframe to a data	data frame writer	0.014085
c{other}, return a resulting rdd that contains	core rdd cogroup other	0.066667
create a java	java wrapper new java	0.166667
outputted by the model's transform method	summary predictions	0.153846
:class dataframe in json format (json lines	sql data frame writer	0.011628
unique	unique	1.000000
return the column standard	mllib standard scaler model	0.100000
min	min	0.333333
a new dstream by applying reducebykey to	streaming dstream reduce by	0.076923
sql storage type for this udt	user defined type sql type cls	0.500000
to convert the java_model to a python model	quantile discretizer create model java_model	0.250000
this	ml other	0.500000
provide save() through their scala implementation	java saveable	0.333333
of object by unpickling it	ml	0.001835
the dot product of	vector dot	0.050000
dump already partitioned data	external group	0.045455
this instance contains a param	params	0.006623
this instance contains a param with a	ml param	0.009524
given a java object	from java	0.111111
regressor	regressor	0.217391
data into disks	external group by	0.045455
with arbitrary key and value	core spark	0.020619
list based on first	based on key outputs	0.111111
returns an mlreader instance for this class	ml mlreadable read cls	0.250000
load a streaming :class dataframe from	data stream	0.028571
mean average precision map of	mean average precision	0.166667
the deviance for the null model	regression summary null deviance	0.250000
instance with a randomly generated uid and	cross validator model	0.050000
groups the :class dataframe using the specified columns	data frame group by	0.200000
to all mixture components	gaussian mixture model predict soft	0.142857
much of memory for this obj assume that	obj	0.023810
a new profiler using class	collector new profiler	0.333333
comprised	mllib random rdds exponential vector	0.125000
operation	core	0.003021
simple sparse vector class for passing data	sparse vector	0.062500
parses the expression string into the column	expr str	0.125000
of layer	ml multilayer perceptron classification	0.333333
jobs has completed	completed batchcompleted	0.250000
table accessible via jdbc url url and connection	reader jdbc url table column lowerbound	0.166667
converts vector columns in an input	convert vector columns from	0.166667
computes the area under the precision-recall curve	binary classification metrics area under pr	0.333333
test statistic	mllib stat test	0.166667
called when processing of a batch of jobs	streaming listener on batch	0.333333
comprised of vectors containing i i	random rdds poisson	0.125000
hivecontext for	for	0.111111
set bandwidth of each sample defaults to	density set bandwidth bandwidth	0.142857
perform a left outer join	left outer join	0.111111
registers	data frame register	1.000000
new terminations	streaming query manager	0.011236
that all the objects	core external merger object size	0.032258
the null model	null	0.125000
such as the spark fair	core spark	0.010309
again to wait	sql streaming	0.010204
converts matrix columns in an input	mllib mlutils convert matrix columns from	0.166667
of the accumulator's data type	accumulator	0.012987
replaces a local temporary view with this	replace temp view name	0.333333
partial objects do not serialize	cloud pickler save partial	0.125000
c{self} that is not contained	core rdd subtract other numpartitions	0.111111
libsvm format into an rdd of labeledpoint	lib svmfile sc path numfeatures	0.125000
an input from tcp source	streaming streaming context socket text stream	0.500000
a l{statcounter} object that captures	stats	0.055556
a decorator that makes a class inherit documentation	inherit	0.037037
that with new	to df	0.250000
get or compute the number of	linalg row matrix num	0.100000
for statistic functions with :class dataframe	data frame stat functions	0.333333
a column scipy matrix from	sci py tests scipy matrix size	0.090909
saves the content	name format mode partitionby	0.200000
year	sql year col	0.050000
the dot product	dense vector dot other	0.100000
the specified path	path	0.030612
in obj	sql	0.002525
load a java	load java cls sc	0.200000
compute	linalg distributed matrix	0.333333
memory for this	external merger	0.031250
a temporary table	table	0.031250
block matrices together the matrices must have the	block matrix add other	0.500000
into a java	to java	0.045455
setparams(self labelcol="label", featurescol="features",	set params labelcol featurescol	1.000000
rdd partitioned using the	rdd	0.003058
cost sum of squared distances	compute cost	0.142857
the specified path	path mode	0.333333
stream query if this	data stream	0.028571
read an 'old' hadoop	spark context hadoop	0.090909
top "num" number of	num	0.016807
train a gradient-boosted trees model for classification	mllib gradient boosted trees train	0.166667
note : experimental	aftsurvival regression model	1.000000
number of gaussians in mixture	gaussian mixture model k	0.200000
of obtaining a test statistic result	mllib stat test result	0.166667
that :func awaitanytermination() can be used again to	sql streaming query manager	0.011905
__init__(self labelcol="label",	linear regression init labelcol	1.000000
with this spark job on	core spark	0.010309
the day of the month of a given	sql dayofmonth col	0.031250
new terminations	sql streaming query	0.011765
queries so that	sql	0.002525
applies standardization transformation on	transform	0.062500
combine functions and a neutral	zerovalue	0.076923
the dot product of two vectors we support	vector dot	0.050000
can be used again to	sql streaming query manager reset	0.011905
all the elements in seen in a sliding	windowduration slideduration	0.083333
representation of	ml linalg	0.060606
formula=none featurescol="features",	formula featurescol	0.400000
instance contains a param with a given string	params has param	0.019231
:class dataframe as the specified	sql data frame writer save as	0.071429
be used again to	query	0.010753
an rdd of labeledpoint	mllib mlutils load lib svmfile sc	0.125000
stop the execution	streaming context stop	0.125000
matrix from the new mllib-local	matrices from ml mat	0.333333
compute the	core	0.003021
the column	col	0.049180
of type	ml	0.001835
dataframe that with new specified	data frame to df	0.090909
of fit	ml estimator fit	0.083333
basic operation test for dstream countbyvalue	basic operation tests test count	0.500000
instance to	to	0.015385
the dispatch to handle all	cloud pickler save	0.166667
each original	ml min max scaler model original	0.062500
items for columns possibly with false	items	0.066667
of batches after which the centroids of that	timeunit	0.025641
the dispatch to handle all function types	core cloud pickler save function obj	0.142857
a param with a	params has param	0.019231
python parammap into a java	param map to java	0.250000
components	soft	0.166667
frequent	mllib prefix span model freq	1.000000
invalidates and refreshes all the cached	catalog refresh by	0.200000
disks	external group	0.045455
create a java array	java array	0.333333
in this grid to fixed values	param grid builder add grid param values	0.333333
find synonyms of a word	find synonyms word num	1.000000
dump already partitioned data into	external group	0.045455
convert this matrix to a coordinatematrix	indexed row matrix to coordinate matrix	0.333333
again to	streaming query manager	0.011236
a model with weights already set	streaming linear regression with	0.111111
the	rdd	0.009174
the index of the original partition	partitions with index	0.100000
converts matrix columns in	mlutils convert matrix columns	0.166667
by applying a function on each rdd	transform func	0.117647
each value in c{self} that is not contained	rdd subtract other numpartitions	0.111111
the norm	sparse vector norm	0.133333
return an javardd of	ml	0.001835
minutes of a given	sql minute col	0.050000
multi-dimensional cube for the current :class dataframe	sql data frame cube	0.055556
set initial centers should be set before calling	mllib streaming kmeans set initial centers centers weights	0.200000
class inherit documentation from its	inherit doc	0.045455
rdd as non-persistent	core rdd unpersist	0.066667
l{statcounter} object that captures the mean	rdd stats	0.083333
already partitioned data into disks	by	0.014286
containing a json string into	sql from json	0.166667
a function to each	f	0.010526
dm = densematrix(2 2 range 4	dense matrix repr	0.142857
computes the levenshtein distance of	levenshtein	0.045455
how much of memory for this	core external merger object	0.032258
a dictionary of value count pairs	count by value	0.333333
iterations default 1 which should	iterations	0.043478
already partitioned data into	external group by spill	0.047619
as a	reader	0.040000
model	linear	0.025641
transpose	transpose	0.857143
dump already	group	0.025641
adds	core accumulator	0.030303
a new spark configuration	spark conf init loaddefaults _jvm	0.250000
much of memory for	external merger	0.031250
value for each original column during	model original	0.062500
table via jdbc	jdbc url table	0.090909
parameters in this grid to	ml param grid builder add grid	0.100000
compare 2 ml types	persistence test compare	0.166667
params are set correctly	params	0.006623
sets	polynomial expansion set	1.000000
:class dataframe to a data source	sql data frame	0.005348
index of the original	with index	0.100000
curve	mllib binary classification metrics	1.000000
the termination of this query either by	termination timeout	0.041667
a resulting rdd that contains	core rdd cogroup	0.066667
new java	new java	0.166667
train a decision tree model for regression	mllib decision tree train	0.333333
own	own	1.000000
data into	external group by spill	0.047619
precision-recall curve which is	ml binary logistic regression summary pr	0.083333
an rdd ordered in ascending order or	rdd take ordered	0.050000
data or create	streaming streaming context get or create	0.200000
in	rdd	0.003058
test the	core task context tests test	0.500000
note : experimental	chi square test	1.000000
a default	spark	0.013158
and profiles the method	core	0.003021
of	ml linalg sparse vector	1.000000
binary byte array data type	binary type	1.000000
year of a given	year	0.040000
that	manager reset	0.011905
of column or names into a jvm seq	seq sc cols	0.055556
matrix columns	matrix columns	0.142857
whose columns are the right singular	linalg singular	0.017544
java_class	java_class	1.000000
a	add	0.035714
wait for the	streaming context await termination or timeout timeout	0.125000
into	into	1.000000
uid	uid newuid	0.333333
a python topicandpartition to	streaming topic and partition init topic partition	0.055556
with the frame boundaries	range between	0.166667
the number of rows	block matrix num rows	0.200000
the output by the	writer bucket by	0.100000
value of	ml min max scaler	1.000000
the input schema	frame reader schema schema	0.333333
a model with weights already	mllib streaming linear regression with	0.111111
the length	length col	0.050000
:func awaitanytermination() can be	streaming	0.005025
sets window size	mllib word2vec set window size	1.000000
wait	sql	0.002525
col1	col1	0.777778
in a numpy ndarray	ml linalg matrix to array	0.166667
a job of a batch has started	started	0.055556
the model trained	mllib tree ensemble model	0.058824
srccol	srccol	0.857143
a given string	params has param	0.019231
precision	precision	0.692308
be used again to wait for new terminations	sql	0.002525
the union	context union	0.333333
memory	core external merger object	0.032258
minsupport	minsupport	1.000000
convert this matrix to	dense matrix	0.076923
containing a json	sql from json	0.166667
partitioned	by	0.014286
an rdd containing all pairs of elements with	core rdd	0.003460
containing a json string into a [[structtype]] or	from json	0.166667
label of one or more examples	decision tree model	0.050000
integral data types	integral type	1.000000
the correlation of two columns of a	col1 col2 method	0.055556
columns in an input dataframe to the :py	columns from ml	0.125000
fast version	core heappushpop heap	0.142857
a partition from disk then sort	merge sorted items index	0.250000
of the accumulator's data type returning a new	accumulator	0.012987
copy all params defined	ml param params copy params	0.200000
log likelihood of	ml ldamodel log likelihood	0.166667
log probability	distributed ldamodel log prior	1.000000
command	command	1.000000
is	core spark context	0.011628
:class dataframestatfunctions for statistic functions	data frame stat	0.250000
sets the given parameters in this grid	param grid builder	0.055556
tests whether this instance contains	params has param paramname	0.142857
libsvm format into an rdd of labeledpoint	mllib mlutils load lib svmfile sc path numfeatures	0.125000
memory for	size	0.009174
partial objects do not serialize correctly	pickler save partial obj	0.125000
value of	ml has items col	1.000000
the accumulator's value only usable in driver	accumulator	0.012987
of fields	sql struct type len	0.200000
day of the month of a given	sql dayofmonth	0.031250
sum	sum	0.750000
transforms a	ml java params transfer	0.250000
the contents of the :class dataframe to	frame writer save path format	0.066667
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params	indexer set params maxcategories inputcol outputcol	0.333333
so	streaming	0.005025
__init__(self featurescol="features", labelcol="label", predictioncol="prediction",	tree classifier init featurescol labelcol predictioncol	1.000000
rating for	matrix factorization	0.040000
generates an rdd comprised of vectors containing i	mllib random rdds poisson vector rdd sc mean	0.200000
optional default	ml param params	0.013699
stop the execution of the streams	streaming context stop	0.125000
classification problem in multinomial	mllib	0.010526
setparams(self labelcol="label", featurescol="features",	linear regression set params labelcol featurescol	1.000000
that makes a class inherit documentation	inherit doc	0.045455
path a shortcut of write() save path	ml mlwritable save path	0.200000
the given parameters in this grid to	param grid builder	0.055556
isotonic regression	isotonic regression	0.090909
for this	object	0.027778
submit and test	spark submit tests test	0.363636
g	g	1.000000
use only create a new hivecontext for	create for	0.250000
function including lambda function	function name	0.166667
dump already partitioned data into disks	core external group by spill	0.047619
featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100	featurescol labelcol predictioncol maxiter	0.333333
seed=none numhashtables=1)	seed numhashtables	1.000000
abstract	abstract	1.000000
libsvm format into label indices values	parse libsvm	0.125000
which predictions	isotonic regression	0.090909
the least value of the	sql least	0.055556
string column after position pos	str pos	0.250000
add a file	context add file path	0.333333
much of memory for this obj assume	size obj	0.040000
converts vector columns	mlutils convert vector columns from ml dataset	0.166667
rdd partitioned using the specified partitioner	rdd partition by numpartitions partitionfunc	0.333333
converts matrix columns in an input	mllib mlutils convert matrix columns to ml dataset	0.166667
object and return the java object	java	0.012195
used	dataset	0.020408
create an input from tcp source	socket text stream	0.500000
compare 2	compare	0.142857
an input stream that pulls events from	stream ssc hostname	0.200000
by applying mappartitionswithindex() to	map partitions with index f preservespartitioning	0.055556
add a file	add file path	0.333333
dot product of two vectors	mllib linalg dense vector dot	0.058824
active queries associated with this	streaming query manager active	0.066667
for each key using an	by key func	0.062500
each word in vocabulary	mllib word2vec fit	0.200000
tables/views in the	tables	0.071429
the old hadoop outputformat api mapred package	hadoop dataset conf keyconverter valueconverter	0.083333
a decorator that makes a class inherit documentation	inherit doc cls	0.045455
onevsrestmodel	one vs rest	0.034483
selector type of	sq selector set selector type	0.111111
wait for	context await termination timeout	0.166667
from the	mllib	0.010526
add a py or zip dependency for all	add py file	0.166667
the dot product of two vectors we support	ml linalg dense vector dot	0.090909
summary of model	decision tree model repr	1.000000
sparkcontext which	streaming streaming context spark	0.083333
an fp-growth model that	mllib fpgrowth train cls data	0.100000
the dot product of two vectors	mllib linalg dense vector dot other	0.058824
separators inside brackets	brackets	0.058824
an array of the most recent [[streamingqueryprogress]] updates	recent progress	0.111111
sparkcontext	sql spark	0.125000
function to the value of	values f	0.062500
predictions which gives	linear regression summary	0.013889
line in libsvm format into	mlutils parse libsvm line line	0.111111
return the first	streaming py spark streaming	0.333333
the result as a :class dataframe	sql data frame	0.010695
defines the ordering columns in a :class windowspec	window spec order by	1.000000
this instance contains a param with a	ml param params	0.013699
of	ml param params reset	0.166667
for	mllib	0.010526
of the accumulator's data type	accumulator param	0.038462
prefersdecimal	prefersdecimal	1.000000
extract a	regexp extract	0.500000
disks	by spill	0.047619
until any	sql streaming query manager await any termination	0.142857
partial objects	core cloud pickler save partial obj	0.125000
partitioned data	core external group by	0.045455
a resulting rdd that contains a tuple	rdd cogroup	0.066667
:py attr lda keeplastcheckpoint is	ml distributed ldamodel	0.050000
matrix	matrix	0.333333
dm = densematrix(2 2 range 4	linalg dense matrix repr	0.142857
model to make predictions on batches of	algorithm predict on	0.066667
with a given string name	ml param params has param	0.019231
onevsrestmodel create and	one vs rest model from	0.142857
either by :func query stop() or by an	streaming query await	0.333333
as spark executor memory this must	spark	0.013158
an expression that gets	sql column get	0.142857
each pair of characters as a hexadecimal number	unhex col	0.142857
this instance with a randomly	split	0.125000
create a converter	create converter datatype	0.166667
create a multi-dimensional cube for the	data frame cube	0.055556
using a given combine functions and a neutral	zerovalue	0.076923
and count of the rdd's elements	core rdd	0.003460
convert this matrix to a coordinatematrix	linalg block matrix to coordinate matrix	0.333333
densematrix(2 2 range 4	mllib linalg dense matrix repr	0.142857
create an rdd for dataframe from	spark session create from	0.500000
accumulator's value only usable in driver	core accumulator	0.030303
a random forest	mllib random forest	1.000000
a jvm seq of columns that describes the	cols cols kwargs	0.090909
returns an mlwriter instance for this ml	java mlwritable write	0.200000
get a local property set in this thread	core spark context get local property key	0.066667
a resulting rdd that contains a tuple	core rdd cogroup	0.066667
predicts rating for the	matrix factorization model predict	0.250000
this instance contains a param with a	params has	0.019231
:py attr cachenodeids	cache node ids value	1.000000
train a gradient-boosted trees model for	mllib gradient boosted trees train classifier	0.333333
shortcut of write() save path	ml one vs rest save path	0.200000
returns the :class statcounter members	stat counter	0.333333
term to this	add term	0.066667
:class column denoted by	data frame getattr	1.000000
this	core accumulator	0.030303
returns an mlreader instance for this class	ml pipeline model read cls	1.000000
the number of	mllib linalg indexed row matrix num	0.062500
observed data against the expected distribution	observed expected	0.166667
to the wrapped	to	0.007692
assert both have the same param	param	0.006250
value of spark	get	0.021739
computes hex value of the	sql hex col	0.166667
content of the dataframe in a	sql data frame writer	0.011628
content of the dataframe in a	sql data frame	0.005348
the list based on first value	based on key	0.111111
class generated by namedtuple	core load namedtuple name fields	0.333333
featurescol=none labelcol=none predictioncol=none classifier=none): sets params for onevsrest	one vs rest set params featurescol	1.000000
max	max	0.571429
progress	progress	1.000000
document to rdd of	document	0.040000
again to wait	sql streaming query manager	0.011905
create a new hivecontext for	context create for	0.250000
test that coefs are predicted accurately	with tests test parameter accuracy	0.333333
column standard	standard scaler	0.076923
spark with	spark	0.013158
note : experimental	rdd count approx timeout confidence	1.000000
recursive dependencies for debugging	to debug string	0.200000
this obj assume that all the	size obj	0.040000
the spark sink	maxbatchsize	0.037037
the word2vec model's vocabulary default	mllib word2vec	0.125000
a python rdd of key-value pairs	core rdd	0.010381
names of tables in the	names	0.050000
set bandwidth of each sample defaults	kernel density set bandwidth bandwidth	0.142857
set a local property that affects	context set local property key value	0.200000
test that the model predicts correctly on	test test predict on model	0.500000
a java parammap	java pyparammap	0.500000
setparams(self inputcol=none outputcol=none handleinvalid="error") sets params for	set params inputcol outputcol handleinvalid	0.333333
dummy	dummy	1.000000
the month	dayofmonth col	0.031250
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10	set params featurescol maxiter seed	0.250000
0 99], quantilescol=none aggregationdepth=2)	fitintercept	0.058824
sample without replacement based	sample	0.050000
gbtclassification	gbtclassification	0.714286
min value for each original column	min max scaler model original min	0.250000
test that	sgdtests test	0.142857
key-value pairs in this dstream	streaming dstream	0.055556
extract the year of a given	year col	0.050000
comprised of vectors containing i i d samples	mllib random rdds poisson vector	0.125000
line	line line	0.166667
returns a :class dataframe representing	sql spark session sql sqlquery	0.250000
coefs are predicted accurately by fitting	parameter accuracy	0.029412
database table via jdbc	jdbc url table mode	0.200000
set a configuration	spark conf	0.058824
create a new accumulator	accumulator init	0.083333
attr lda	distributed ldamodel	0.052632
finding frequent items for columns possibly	freq items cols support	0.166667
rescale each feature individually to	max scaler	0.200000
an external table based on the dataset	external table tablename path	0.090909
format or newline-delimited json	writer json	0.125000
containing a json string	json	0.043478
parses a line	line line	0.166667
of the :class dataframe as the specified	data frame writer save as	0.071429
the mean average precision map	mean average precision	0.166667
a	ml param params has	0.038462
wait for the execution	termination or timeout timeout	0.125000
them with extra values from	map extra	0.040000
external table based on the dataset in a	external table tablename path	0.090909
create an input stream that pulls	utils create stream	0.200000
set	conf set	0.200000
already partitioned data	external group	0.045455
train a random forest model for	mllib random forest train classifier	0.250000
this instance with	one	0.117647
return a jvm scala map from	sql data frame jmap jm	0.111111
return each value in c{self} that	other numpartitions	0.083333
schema of this :class dataframe	data frame schema	0.333333
next memory limit if the memory is not	external sorter next limit	0.200000
day of the month	sql dayofmonth	0.031250
the index of the	map partitions with index	0.100000
by the given columns if	by	0.014286
model fitted by :py class lda	ldamodel	0.034483
point in rdd 'x' to all mixture	mixture model predict	0.125000
specifies the underlying output	writer format	0.333333
vector	linalg vector	0.200000
for the termination of this	termination	0.035714
list of indices	ml chi sq selector	0.100000
threshold if any used	threshold	0.018182
disks	external group by spill	0.047619
a param	ml param params has param	0.019231
stores item	ml alsmodel item	0.250000
dump already partitioned	external group	0.045455
left outer join of	rdd left outer join other numpartitions	0.111111
java onevsrest create	one vs rest from java	0.142857
calculates the norm of a sparsevector	mllib linalg sparse vector norm p	0.083333
the documentation of all params with their optionally	ml param params explain params	0.166667
verify the attempt numbers are correctly reported	task context tests test attempt number	0.333333
to	query manager reset	0.011905
regression model derived from a	regression model	0.031250
a jvm scala map from	frame jmap jm	0.111111
an rdd	rdd samplingratio	0.200000
the specified schema	schema	0.033333
calculates the norm of	norm	0.041667
losstype="logistic", maxiter=20	ml gbtclassifier	0.095238
squared distance from a sparsevector or	ml linalg sparse vector squared distance other	0.166667
an rdd of labeledpoint	mlutils load lib svmfile sc	0.125000
create a new spark configuration	core spark conf init loaddefaults _jvm	0.250000
in a :class windowspec	window	0.037037
windowing column	sql column over window	0.333333
use l{sparkcontext broadcast()}	core broadcast	0.200000
sets	ml hashing tf set	1.000000
instance contains a param with a	params has param	0.019231
operation test for dstream countbyvalue	operation tests test count	0.500000
defined from start	start	0.045455
the bisecting k-means	mllib bisecting kmeans	0.500000
log of class conditional probabilities	ml naive bayes model theta	0.500000
used again to wait	query manager	0.011905
for the stream query if this is not	data stream	0.028571
params to	java params to	0.045455
lda keeplastcheckpoint is set to	ldamodel	0.034483
to wait for	query manager	0.011905
creates a new sqlcontext	sql sqlcontext init sparkcontext	1.000000
parameters in this grid to fixed	grid builder base	0.076923
underlying output	sql data frame writer format	0.333333
list of predicted ratings for input	matrix factorization model predict all user_product	0.050000
gets a	ml param params get	0.500000
rdd of points using the model trained	logistic regression model	0.083333
this dataframe	data frame	0.005000
output a python rdd of key-value	rdd save as	0.038462
contains a param with a given string name	ml param params has	0.019231
create a new spark configuration	core spark conf init loaddefaults	0.250000
test the python direct kafka stream	kafka stream tests test kafka direct stream from	0.333333
pickle	pickle	1.000000
array containing the ids of all active stages	core status tracker get active stage ids	0.250000
the left singular vectors of the singularvaluedecomposition	singular value decomposition	0.166667
row object	row call	1.000000
extract the minutes of a	minute	0.040000
udfregistration for	sqlcontext	0.038462
key and value class	core spark	0.020619
get a local property set in	spark context get local property key	0.066667
any hadoop file system using the l{org	as sequence file path compressioncodecclass	0.500000
fit an intercept term	fit intercept	0.250000
a jvm seq of column	seq	0.043478
new sparkcontext at least the master and	core spark context init master	0.500000
of vectors which this	ml vector indexer	0.200000
minutes of a given	minute	0.040000
levenshtein distance of	levenshtein	0.045455
squared distance of two vectors	ml linalg dense vector squared distance other	1.000000
from a	from	0.045455
named table accessible via jdbc url url	jdbc url table column	0.166667
of two columns of a dataframe	data frame	0.005000
stores item factors in two columns id and	alsmodel item factors	1.000000
using the model trained	mllib tree ensemble model	0.058824
matrix columns in an input	matrix columns from ml dataset	0.142857
all	external merger object size	0.032258
extract the week number of a given	weekofyear	0.043478
given string	param params has	0.019231
underlying output	sql data frame writer	0.011628
note : experimental	core rdd sum approx timeout confidence	1.000000
particular batch has half	half	0.058824
__init__(self	ml min hash lsh init	1.000000
in libsvm format into label indices	mllib mlutils parse libsvm	0.125000
old hadoop outputformat api mapred	as hadoop dataset conf keyconverter valueconverter	0.083333
"predictions" which gives the true label of	ml logistic regression summary label col	0.333333
densematrix whose columns are the right singular	singular	0.015625
set the initial value of	mllib streaming logistic regression with sgd set initial	0.111111
unifying data of	union	0.090909
classifier	classifier	0.300000
how data	sql data	0.024390
test python direct kafka rdd messagehandler	tests test kafka rdd message handler	1.000000
possible outcomes for k classes classification	classes	0.034483
basic operation test for dstream mappartitions	streaming basic operation tests test	0.111111
parameters in	builder	0.090909
in a text	writer text	0.333333
set a local	set local	1.000000
java model from	java	0.012195
compute the number of cols	mllib linalg block matrix num cols	0.333333
specifies the behavior when data or table	sql data frame writer mode savemode	0.071429
outputformat api mapred package	outputformatclass keyclass valueclass	0.250000
how much of memory for	core external	0.016129
:class dataframe to an external database	sql data frame	0.005348
with a	ml	0.001835
forget about past terminated queries so that	terminated	0.100000
an rdd comprised of vectors containing i	mllib random rdds log normal vector rdd	0.166667
the column standard deviation	mllib standard scaler model std	0.166667
content of the :class dataframe as the specified	sql data frame writer save as	0.071429
basic operation test for dstream countbyvalue	streaming basic operation tests test	0.111111
of vectors which this transforms	ml vector indexer model	0.250000
frequency tf vectors to tf-idf vectors	mllib idfmodel transform x	0.142857
the receiver operating characteristic roc	summary roc	0.333333
resets the configuration property for the	runtime config unset	0.142857
with a dependency on	dependency	0.040000
memory for	core external merger object size	0.032258
the index of the	partitions with index	0.100000
params shared by	params	0.006623
:class dataframe as a temporary table	data frame as table	0.333333
create a multi-dimensional cube for	data frame cube	0.055556
the deviance for the fitted	ml generalized linear regression summary deviance	0.125000
a set of expressions and returns	sql	0.002525
a python rdd	core rdd	0.013841
true default or minimized false	evaluator is larger better	0.166667
trees in the ensemble	tree ensemble	0.111111
a field in :class structtype	struct field	0.500000
for dataframe from a list or pandas dataframe	from local data schema	0.333333
all the	merger object	0.032258
to use	context	0.022727
offsetranges	offset ranges	0.500000
names into a jvm seq	seq sc	0.055556
sql context to use for	context sqlcontext	0.083333
__init__(self inputcol=none outputcol=none)	max abs scaler init inputcol outputcol	1.000000
list of values	mllib linalg dense vector values	0.200000
for the test this should	test	0.015152
as a	as	0.074074
which is a risk function corresponding to	mllib regression	0.045455
creating rdds comprised of i i	random rdds	0.012821
sort the list	py spark streaming test case sort result	0.333333
onto heap maintaining the heap invariant	core heappush heap	1.000000
of terms or words in	ml ldamodel	0.111111
featurescol="features", predictioncol="prediction", maxiter=20 seed=none k=4 mindivisibleclustersize=1 0)	featurescol predictioncol maxiter seed	1.000000
returns the root mean squared error which is	mllib regression metrics	0.090909
length of a string or binary	sql length col	0.050000
old hadoop outputformat api	hadoop dataset conf keyconverter valueconverter	0.083333
be used again to	sql streaming	0.010204
of the month of a	dayofmonth	0.027027
ensemble	ensemble	0.400000
the kolmogorov-smirnov ks test for data sampled	mllib stat statistics kolmogorov smirnov test data	0.111111
that :func awaitanytermination() can be used again to	sql streaming	0.010204
containing the ids of all	stage ids	0.055556
featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0	featurescol labelcol predictioncol maxdepth	0.500000
again to	query manager	0.011905
test predicted values on a toy model	streaming logistic regression with sgdtests test	0.111111
train a decision tree model	mllib decision tree train regressor cls data	0.333333
be used again to wait for new	streaming query manager	0.011236
(i j s\ :sub ij\) tuples representing the	k maxiterations	0.333333
model with weights already set	streaming linear regression with	0.111111
evaluates the	java evaluator evaluate dataset	0.333333
model fitted by :py class idf	idfmodel	0.166667
into the returned	streaming py spark streaming test case	0.333333
stream api	stream	0.035088
jvm seq of columns that describes the	cols cols kwargs	0.090909
instance contains a param with a given	has	0.011628
columns in an	columns from ml	0.125000
a param with	has	0.011628
trigger for the stream query if	data stream writer trigger	0.083333
the model trained	mllib logistic regression model	0.083333
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20	ml random forest classifier	0.023256
of indices	ml chi sq selector model	0.166667
to configure the kmeans algorithm for fitting and	kmeans	0.025641
sets	quantile discretizer set	1.000000
model params are set	model params	0.125000
columns in	columns to ml dataset	0.125000
a densevector with singular values in descending order	singular value decomposition s	0.250000
params	params	0.139073
the soundex encoding for	sql soundex col	0.055556
content	content	0.857143
returns an array	sql	0.002525
k	k	0.785714
boundaries in increasing order for which	isotonic regression model boundaries	0.333333
:class windowspec with the ordering defined	sql window order by	1.000000
dstream by applying a function	map f	0.037037
this instance contains	param	0.012500
convert this matrix to the new mllib-local representation	matrix as ml	0.250000
the :class dataframe in json format	sql data frame writer	0.011628
with the frame boundaries	range	0.030303
a given string name	ml param params	0.013699
a different value or cleared	description interruptoncancel	0.166667
sorts	sort by	1.000000
fitted model by	generalized linear regression	0.090909
given parameters in this grid to fixed values	grid builder base on	0.076923
for the termination of this query either	termination	0.035714
numbuckets	numbuckets	0.714286
rdd partitioned using the	rdd partition	0.062500
or compute the	mllib linalg row matrix	0.250000
returns a :class dataframe representing the result of	sql spark session sql sqlquery	0.250000
value and add the new item	item	0.062500
this rdd as a temporary table using	temp table	0.500000
class	collector	0.166667
of two columns of a dataframe	data frame corr col1 col2	0.166667
featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01 maxiter=100	featurescol predictioncol k probabilitycol	0.333333
pr	pr	1.000000
a left outer join of c{self} and c{other}	core rdd left outer join other	0.200000
rdd of labeledpoint	load lib svmfile sc	0.125000
this accumulator's value	accumulator	0.012987
add a py or zip	spark context add py file path	0.166667
distinct count of col	count distinct col	0.080000
docstring is not shown publicly	linalg coordinate matrix init entries numrows numcols	0.333333
column mean values	scaler model mean	0.125000
with start offset	offset	0.021739
of	ml chi sq selector	0.100000
a column scipy matrix from a	mllib sci py tests scipy matrix	0.090909
conversion	conversion	0.857143
norm	vector norm	0.166667
underlying sql storage type for this udt	sql user defined type sql type cls	0.500000
this instance	param params has	0.019231
length of a	sql length	0.050000
receiver operating characteristic roc curve which is	ml binary logistic regression summary roc	0.166667
underlying sql storage type	sql type cls	0.250000
load a model from the	loader load cls sc	0.250000
output a python rdd of key-value pairs	core rdd	0.010381
whether this instance is of	ml ldamodel is	0.066667
ensemble	mllib tree ensemble	0.222222
instance contains a param with a	param params has	0.019231
singular vectors of the	singular	0.031250
aggregate the values of each	aggregate by	1.000000
the :class dataframe to a	frame writer save path format	0.066667
the dot product of two vectors we	vector dot other	0.050000
test that coefs are predicted accurately by fitting	tests test parameter accuracy	0.333333
can be	query	0.010753
new spark configuration	spark conf init loaddefaults _jvm _jconf	0.250000
an input stream that is to be	stream ssc addresses storagelevel	0.166667
partition	external merger partition	1.000000
jvm seq of columns that describes the sort	sort cols cols kwargs	0.142857
instance contains a param with	ml param params has param	0.019231
jvm seq of column	seq	0.043478
dstream	streaming kafka dstream	0.500000
new dstream in which	streaming streaming context	0.032258
contains a param	ml param params has	0.019231
__init__(self inputcol=none outputcol=none handleinvalid="error")	indexer init inputcol outputcol handleinvalid	1.000000
streaming dataframe/dataset is written to a	stream writer output mode outputmode	0.083333
inserts	insert into	0.500000
batch of jobs	batch	0.068966
as the square root of the	root	0.035714
be	streaming query manager reset	0.011905
transfer this instance to a java pipeline used	ml pipeline to java	1.000000
creates a table based on the dataset in	catalog create external table tablename path	0.250000
used again to wait for	sql streaming query manager	0.011905
a given string name	param	0.012500
computes the area under the precision-recall curve	mllib binary classification metrics area under pr	0.333333
colname	colname	1.000000
kmeans algorithm	kmeans	0.025641
a csv file and	csv path schema sep	0.166667
this instance contains a param with a	param params has	0.019231
test that the final value of	mllib streaming logistic regression with sgdtests test	0.111111
the += operator	iadd	0.142857
sort	streaming test case sort result	0.333333
test that the final value of	regression with sgdtests test	0.111111
original column	ml min max scaler model original	0.062500
content of the dataframe	data frame	0.005000
loads vectors saved using rdd[vector] saveastextfile	mlutils load vectors	1.000000
the content of the :class dataframe as the	data frame writer save as	0.071429
splits str around pattern pattern is a regular	split str pattern	0.333333
comprised of i i	mllib random rdds	0.041667
sorts the output in each bucket by	writer sort by col	1.000000
compute the number of	matrix num	0.088235
jvm seq of	seq sc cols	0.055556
columns in an input dataframe	columns to ml	0.125000
instance's params to the wrapped java object and	ml java params to	0.045455
"zerovalue" which	fold	0.076923
given parameters in this grid to fixed values	param grid builder base	0.076923
norm	dense vector norm	0.333333
summary of model	ensemble model repr	1.000000
be	streaming query	0.010526
a python topicandpartition to map to the	topic and partition init topic partition	0.055556
the rdd partitioned	rdd	0.003058
combine	combine	0.857143
the values for each key	by key numpartitions	0.111111
an fp-growth model	mllib fpgrowth train cls	0.100000
on toy	on	0.074074
this instance to a java pipelinemodel used for	pipeline model to java	0.100000
for distinct count of	count distinct col	0.040000
for the stream query	data stream writer	0.041667
:py	scaler	0.157895
the dot product of	linalg dense vector dot	0.058824
each key using	by key	0.026316
minutes of	minute col	0.050000
the values of each key	by key	0.026316
distributed model to a	distributed ldamodel to	0.166667
adds an input option for	frame reader option key	1.000000
a text	writer text	0.333333
value of	ml ngram	1.000000
columns that make up each block	mllib linalg block matrix cols per block	0.333333
dataframe from an :class rdd, a	schema samplingratio verifyschema	0.029412
md5	md5	0.625000
of column	sql	0.005051
set k decayfactor	streaming	0.005025
columns on the file	data frame	0.005000
pipelinemodel create and return a python	pipeline model from	0.142857
with the	range	0.030303
month of a given date as integer	sql dayofmonth col	0.031250
compute the dot product of two	dense vector dot	0.050000
memory for this obj assume that all the	object size obj	0.040000
or	or	0.857143
sets the given parameters in this grid	ml param grid builder base	0.076923
the group	group	0.025641
the objects	core external merger object size	0.032258
vector columns in an input	vector columns from	0.142857
set the selector	chi sq selector set selector	0.333333
the year	sql year col	0.050000
end exclusive increased by step every element	end step numslices	0.500000
trained	mllib logistic	0.200000
python code for a shared param class	ml param gen param code name doc	0.333333
predict the label	predict x	0.016949
the cluster centers represented as a list	cluster centers	0.090909
in a numpy ndarray	linalg matrix to array	0.142857
get	core rdd get	0.250000
square root of the	root	0.035714
particular job group	status tracker get job ids for group jobgroup	0.500000
tree (e g depth	tree model	0.026316
model fitted by :class gbtregressor	gbtregression model	1.000000
each	func numpartitions partitionfunc	1.000000
get or compute the number	mllib linalg distributed matrix num	0.166667
return sparkcontext which is associated with this	context spark	0.083333
of estimated coefficients and intercept	ml linear regression summary	0.142857
a multi-dimensional cube	frame cube	0.055556
values from this instance to another instance for	values to extra	0.333333
train a decision tree model for regression	mllib decision tree train regressor cls data categoricalfeaturesinfo	0.333333
content of the :class dataframe to the	data frame writer	0.014085
with the frame boundaries defined	range	0.030303
this query either by :func	sql streaming	0.010204
the sum for each numeric columns	grouped data sum	0.083333
matrix attributes which are	matrix	0.030303
used	query manager reset	0.011905
name or provide a new name	name	0.043478
of this instance with a randomly	ml one	0.083333
create a new dstream in which each rdd	streaming streaming	0.047619
function including lambda function	function	0.055556
sets	regex tokenizer set	1.000000
str around pattern pattern is	str pattern	0.250000
the rdd as non-persistent and remove all blocks	core rdd unpersist	0.066667
new vector with 1 0 bias appended	mllib mlutils append bias data	0.333333
line in libsvm format into label	mlutils parse libsvm line line multiclass	0.111111
the given parameters in this	param grid builder add	0.200000
be maximized true default or minimized false	ml evaluator is larger better	0.166667
the number of rows	mllib linalg block matrix num rows	0.200000
much of memory for	merger	0.025641
number	mllib linalg indexed row matrix num	0.062500
is vector conduct pearson's chi-squared goodness of	stat statistics chi sq	0.066667
the year of a given date	year	0.040000
or compute the	mllib linalg distributed matrix	0.333333
for this standardscaler	standard scaler	0.076923
param with a given string	params has	0.019231
the residual degrees of freedom for the	linear regression summary residual degree of freedom	0.125000
__init__(self featurescol="features",	tree classifier init featurescol	1.000000
mixin for param fitintercept whether	has	0.011628
to all mixture	mllib gaussian mixture model predict	0.100000
the given parameters in this grid to	param grid builder base on	0.076923
contents of the :class dataframe to	frame writer save	0.066667
recall curve	ml binary logistic regression summary recall by	1.000000
embedded params to the companion java object	ml java params transfer params to java	0.500000
from checkpoint data or	streaming streaming context get or	0.200000
key and	core spark	0.020619
a column	col options	0.500000
calculates a lower bound on the log likelihood	ldamodel log likelihood dataset	0.142857
field in :py attr predictions which	generalized linear regression	0.090909
linear	linear	0.307692
a line in libsvm format into label	mllib mlutils parse libsvm line line	0.111111
param with a given	params has	0.019231
computes column-wise summary statistics for the input	mllib stat statistics col stats	0.200000
of the chisqselector	selectortype	0.125000
an rdd for dataframe from	from	0.045455
this	accumulator	0.012987
count of the rdd's elements in one	rdd	0.003058
infer schema from	sql sqlcontext infer schema	1.000000
and the second is an array	mllib	0.010526
computes average values for each numeric columns	sql grouped data	0.041667
null values alias for na fill()	data frame fillna value subset	0.166667
index of	with index	0.100000
that :func awaitanytermination() can be used	sql streaming query	0.011765
multi-dimensional rollup for the current :class	data frame rollup	0.055556
produced by	ml clustering	0.100000
input stream	stream	0.017544
print the first num elements of	pprint num	0.250000
curve which is a dataframe having two fields	ml binary logistic regression summary	0.125000
:func awaitanytermination() can be used again to wait	sql streaming query	0.011765
an input stream that	stream ssc addresses	0.166667
dot product	mllib linalg dense vector dot	0.058824
from the	from ml mat	0.500000
called when a receiver has reported an error	listener on receiver error receivererror	1.000000
containing the ids of all active stages	tracker get active stage ids	0.250000
uid of	ml param params reset uid	0.058824
the accumulator's value only	core accumulator value value	0.050000
items for	items	0.066667
such as the spark	spark context	0.023256
represents a range of offsets from a	range	0.030303
values of the accumulator's data type returning	accumulator	0.012987
train a random forest model for binary or	mllib random forest train	0.250000
invalidate and refresh all	hive context refresh	0.200000
convert this matrix to a rowmatrix	linalg coordinate matrix to row matrix	0.333333
the given parameters in this grid to fixed	grid builder add grid param	0.250000
of specific kafkardd	kafka rdd offset	0.500000
of a batch has completed	completed outputoperationcompleted	0.125000
an iterator that contains all	to local iterator	0.333333
again to wait	query manager	0.011905
arg1	arg1	1.000000
predictions which gives the predicted value of	ml generalized linear regression summary prediction col	0.333333
multi-dimensional rollup for the	frame rollup	0.055556
a python rdd of key-value	core rdd save	0.037975
the length	sql length col	0.050000
load a model from	matrix factorization model load cls sc	0.333333
squared distance from	squared distance	0.142857
make predictions on a keyed dstream	mllib streaming kmeans predict on values dstream	1.000000
a heappop followed by a heappush	core heapreplace	0.250000
py or zip	py	0.050000
given label	label	0.071429
labelcol="label",	labelcol	0.814815
distributed model to a local representation this discards	distributed ldamodel to local	0.111111
values for each numeric	sql grouped data avg	0.058824
test this	test	0.015152
rdd of points using the model trained	tree ensemble model	0.038462
the file to	file	0.028571
(e g accuracy/precision/recall objective history total iterations) of	ml logistic regression	0.111111
queries so that :func awaitanytermination() can be used	query	0.010753
this instance contains a param with a given	params has	0.019231
squared distance from a sparsevector or 1-dimensional	squared distance	0.142857
wait for the execution to	context await termination or timeout timeout	0.125000
the given join	join	0.034483
__init__(self featurescol="features",	ml decision tree classifier init featurescol	1.000000
for binary or multiclass	classifier cls data numclasses	0.250000
ml params instances for the given param and	params m1 m2	0.047619
a paired rdd where the	factorization model	0.043478
point in rdd 'x' has maximum membership	mllib gaussian mixture	0.045455
test the python direct kafka stream transform	tests test kafka direct stream transform	1.000000
in	core rdd	0.003460
an	mllib mllib	0.500000
return the column	standard	0.071429
this instance contains a param with a given	ml	0.001835
the executors if the	unpersist blocking	0.166667
that :func	streaming query manager reset	0.011905
accumulator's value only usable in driver program	accumulator	0.012987
model intercept of linear svm classifier	ml linear svcmodel intercept	0.500000
this test	other test	0.333333
the dstreams	transform dstreams transformfunc	0.125000
singularvaluedecomposition if computeu was set to be	value decomposition u	0.100000
in json format	sql	0.002525
the catalog	sql catalog	0.285714
a new sqlcontext	sql sqlcontext init	0.500000
converts matrix columns in an input dataframe	mllib mlutils convert matrix columns	0.166667
number of cols	matrix num cols	1.000000
a param	ml	0.001835
increasing	increasing	1.000000
positive rate for a given	positive rate	0.166667
setparams(self featurescol="features",	ml gaussian mixture set params featurescol	1.000000
average values for each numeric	sql grouped data avg	0.058824
for statistic functions	stat	0.076923
format into an rdd of labeledpoint	load lib svmfile sc path numfeatures	0.125000
summary of a data matrix	summary	0.024390
in libsvm format into label indices	mlutils parse libsvm	0.125000
of labels corresponding to indices	ml string indexer model labels	0.066667
value to a boolean if possible	param type converters to boolean value	0.250000
date datetime date	date	0.037037
month of a given date	sql dayofmonth	0.031250
the training set given the current	ldamodel training	0.034483
area under	metrics area under	0.166667
wait until any of the queries on	streaming query manager await any termination timeout	0.166667
perform a pearson's independence test using dataset	chi square test test dataset featurescol labelcol	0.333333
set number of	kmeans set	0.090909
local property set in this	local property key	0.035714
save this model to the given path	saveable save sc path	1.000000
compute the dot product	dot	0.080000
the dot product of two vectors we	ml linalg dense vector dot other	0.090909
returns micro-averaged label-based precision	mllib multilabel metrics micro precision	1.000000
compute the number of cols	mllib linalg indexed row matrix num cols	0.333333
matrix attributes which	matrix	0.030303
until any of the queries	await any	0.142857
log2	log2	1.000000
the soundex encoding for	soundex	0.043478
the :class dataframe to	frame writer	0.050000
parses the expression string into the column	expr	0.076923
indices back to	index to	0.040000
a paired rdd where the	factorization	0.038462
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance",	regressor	0.043478
model	ml quantile discretizer create model	1.000000
class inherit documentation from its parents	inherit	0.037037
load a model from the given	java loader load cls sc	0.250000
wait for	manager	0.011236
a list of active queries associated with	streaming query manager active	0.066667
stream messagehandler	stream message handler	1.000000
has half the	half	0.058824
large dataset and an item approximately find	lshmodel approx nearest neighbors dataset	0.166667
minutes of a given date as	sql minute col	0.050000
numfolds=3	numfolds	0.125000
trait for multivariate statistical	multivariate statistical	1.000000
to fit	fit	0.100000
of tree (e	tree	0.020833
a list of numpy arrays	ml bisecting kmeans	0.062500
for which predictions	ml isotonic regression	0.111111
initial value	set initial	0.111111
a py or zip dependency for	py file path	0.066667
extract the minutes of a given date as	minute col	0.050000
calculates the norm of a sparsevector	mllib linalg sparse vector norm	0.083333
sql user-defined type udt for matrix	matrix udt	1.000000
number of columns that make up each	linalg block matrix cols per	0.333333
whose columns are the left singular vectors	singular	0.015625
even if users construct taskcontext instead	task context new	0.333333
but return	core rdd	0.003460
of spark sql	sql sqlcontext	0.047619
sets params for	set params formula	0.500000
pearson's chi-squared goodness of	stat statistics chi sq	0.066667
extract the year of a given	sql year col	0.050000
can be used	sql streaming query manager	0.011905
a receiver	receiver	0.153846
timeunit to configure the kmeans algorithm for fitting	streaming kmeans	0.035714
later than the value of the date	next day date dayofweek	0.333333
to a boolean	to boolean	0.250000
this instance contains a param	ml param params	0.013699
model	mllib decision tree model	0.076923
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic",	ml gbtclassifier	0.095238
dataframe representing the	sqlquery	0.054054
the root mean squared error which	linear regression summary	0.013889
column scipy matrix from a dictionary	sci py tests scipy matrix	0.090909
of the dataframe	sql data frame writer	0.011628
uid of this instance this updates	ml param params reset uid	0.058824
that :func awaitanytermination() can be used again	query	0.010753
vector columns	vector columns from	0.142857
function to the value of each	values f	0.062500
local property set	context get local property key	0.066667
create an input stream	utils create stream ssc hostname	0.200000
are the right singular vectors	linalg singular	0.017544
predict values for a	predict	0.068966
tests whether this instance contains a param with	paramname	0.076923
partitioned data into disks	spill	0.038462
python topicandpartition to map to	topic partition	0.055556
class inherit documentation	inherit doc cls	0.045455
sets	bisecting kmeans set	1.000000
test that	regression with tests test	1.000000
the selector type	mllib chi sq selector set selector type	0.111111
nodes or any hadoop-supported file	file path	0.035714
non-streaming :class dataframe	data frame write	0.166667
the :class dataframe to a data	data frame writer	0.014085
columns in an input dataframe to	columns	0.039216
kafka topic name	topic	0.142857
used again	streaming query manager	0.011236
wrapper for the model in jvm	java model wrapper	1.000000
external storage systems (e	writer	0.040000
the dispatch to handle all function types	core cloud pickler save function obj name	0.142857
strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none) sets params for	set params strategy missingvalue inputcols outputcols	0.200000
create a multi-dimensional cube for the current :class	sql data frame cube	0.055556
converts matrix columns in an input	convert matrix columns from ml dataset	0.166667
mindf	min df	1.000000
underlying output	sql data stream writer	0.041667
after which the centroids of	timeunit	0.025641
names	names	0.300000
for	merger object size	0.032258
min value for each original column	scaler model original min	0.250000
this	one vs rest	0.034483
local property set	core spark context get local property key	0.066667
new dstream by applying reducebykey	streaming dstream reduce by key	0.076923
a value to a mllib vector if possible	ml param type converters to vector value	0.333333
all globals names read or written	globals	0.076923
queries so that :func	query manager	0.011905
setparams(self numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")	set params numbuckets inputcol outputcol	1.000000
a list of values	linalg dense vector values	0.200000
add two values of the accumulator's	accumulator param add	0.333333
train the model on the incoming dstream	train on dstream	1.000000
stop	context stop	0.125000
the input schema	sql data frame reader schema schema	0.333333
set initial centers should be set before	initial centers centers	0.200000
objective function scaled loss + regularization at	objective history	0.500000
an rdd	normal vector rdd	1.000000
which is a dataframe having two fields	logistic regression summary	0.045455
or newline-delimited json	json path	0.100000
copy	model copy extra	0.333333
shortcut of write() save	ml one vs rest save	0.166667
of the :class dataframe to a data source	data frame writer save path	0.142857
submit and test a single script on	spark submit tests test single script on	0.500000
value of	ml param has	1.000000
term to this	core accumulator add term	0.066667
an rdd with the keys of each	rdd keys	0.250000
property	key	0.017857
instance contains	ml param params has	0.019231
starts at pos in byte and	pos	0.022222
matrix columns in an	matrix columns from ml	0.142857
random forest model	random forest	0.041667
the cluster centers represented as	mllib bisecting kmeans model cluster centers	0.083333
containing elements	core	0.003021
predicted ratings for input user and product pairs	mllib matrix factorization model predict all user_product	0.050000
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10	init featurescol maxiter seed	0.250000
net	net	1.000000
__init__(self rawpredictioncol="rawprediction", labelcol="label", metricname="areaunderroc")	init rawpredictioncol labelcol metricname	1.000000
java storagelevel	get java	0.111111
test the python direct kafka	stream tests test kafka direct	1.000000
this matrix to the	matrix	0.015152
list of indices to select filter	ml chi sq selector model selected features	0.500000
infer schema from an rdd of	sql sqlcontext infer schema rdd samplingratio	0.250000
estimator=none estimatorparammaps=none evaluator=none numfolds=3	estimator estimatorparammaps evaluator numfolds	0.200000
make predictions on batches of data from	linear algorithm predict on	0.066667
so it can be used in sql statements	returntype	0.071429
instance's params to the wrapped java object and	java params to	0.045455
memory for this	external merger object	0.032258
sparkcontext which is associated with this	streaming context spark	0.083333
convert this vector to the	mllib linalg vector	0.333333
the soundex encoding for a string >>> df	sql soundex col	0.055556
of names of tables	names	0.050000
value of	ml regression	1.000000
:func awaitanytermination() can be	reset	0.011236
value of	ml param has num	1.000000
vectors	vector indexer model	0.200000
sparkcontext which is associated	streaming streaming context spark	0.083333
from the	from	0.045455
of the stage	stage	0.062500
table	tablename	0.086957
get the cluster	cluster	0.142857
on a	on	0.037037
vector to	vector	0.038462
topicdistributioncol	topic distribution	1.000000
the residual degrees of freedom for	linear regression summary residual degree of freedom	0.125000
the spark fair scheduler pool	spark context	0.023256
instance contains	ml param	0.009524
predicted values on a toy	mllib streaming logistic regression with sgdtests	0.200000
in jvm	vector transformer	0.250000
of memory for this obj assume that all	obj	0.023810
generates an rdd	vector rdd sc	1.000000
such as the spark	spark	0.013158
of	ml java	0.307692
map stored in the column	col	0.016393
the left singular vectors of the	mllib linalg singular	0.017544
converts a labeledpoint	mllib mlutils convert labeled point	1.000000
can be used again	query manager	0.011905
comprised of vectors containing i	mllib random rdds gamma	0.125000
arrays of	ml	0.001835
return the	mllib standard	0.125000
converts matrix columns in an input dataframe	mlutils convert matrix columns to ml dataset	0.166667
selector	sq selector set selector	0.333333
py or zip dependency for all tasks	py file	0.066667
simple sparse vector class for	sparse vector	0.062500
:class dataframe	sql data	0.024390
already partitioned data into	external group	0.045455
test the python direct kafka stream	kafka stream tests test kafka direct stream	0.625000
python code for a	code name doc defaultvaluestr	0.111111
window specification that defines the	window spec	0.166667
'left	left	0.066667
query either by :func	sql streaming	0.010204
awaitanytermination() can be used again to	sql streaming query manager	0.011905
in rdd 'x' to all mixture components	mixture model predict soft	0.142857
this model	mllib bisecting kmeans model	0.333333
a class inherit documentation from its	mllib inherit doc	0.045455
passed in a profile object is returned	basic profiler profile func	0.200000
value of	ml multiclass	0.500000
stratified sample without replacement	sample	0.050000
binary or multiclass	data numclasses	0.250000
in mixture	mixture	0.052632
so it can be used in sql	sql sqlcontext	0.095238
this dstream	dstream	0.031250
used in sql	sql sqlcontext	0.095238
predicted clusters	clustering summary prediction	0.333333
awaitanytermination() can be used again to wait for	manager reset	0.011905
stepsize	step size	0.250000
with the spark sink	storagelevel maxbatchsize	0.045455
returns an mlwriter instance for	mlwritable write	0.200000
test python direct kafka rdd get offsetranges	stream tests test kafka rdd get offset ranges	1.000000
until any of the queries on	query manager await any termination	0.142857
an	mllib	0.042105
with a given string	ml param params has param	0.019231
with the spark sink deployed on	ssc addresses storagelevel maxbatchsize	0.045455
used again to wait	streaming query	0.010526
or newline-delimited json	writer json path	0.125000
get the configured value for some key or	conf get key defaultvalue	0.333333
assert that an object is of type str	sql runtime config check type obj identifier	1.000000
<http //en wikipedia org/wiki/decision_tree_learning>_	regressor	0.043478
in the user-supplied param	param	0.006250
be used again to	query manager reset	0.011905
rdd's elements in	rdd	0.003058
creates a new	init sparkcontext	1.000000
blocks in	col blocks	0.250000
with option of ensuring all received data has	stopsparkcontext stopgracefully	0.050000
rdd of key-value pairs (of	core rdd save	0.037975
java	wrapper new java	0.166667
an rdd comprised of vectors containing i	random rdds log normal vector rdd	0.166667
in a data source	source schema	0.181818
until any of	manager await any	0.142857
are the left singular vectors of the	linalg singular	0.017544
cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto",	ml random forest classifier	0.023256
gets a param	params get param	1.000000
tables/views in	tables	0.071429
spark session	session sparksession	0.083333
stop this streaming query	sql streaming query stop	1.000000
to wait for new	query manager	0.011905
for each training data point	gaussian mixture summary	0.200000
returns weighted true positive	mllib multiclass metrics weighted true positive	1.000000
on the log likelihood	log likelihood	0.125000
the sql context to use for saving	java mlwriter context sqlcontext	0.333333
returns weighted averaged recall	mllib multiclass metrics weighted recall	1.000000
:py attr lda	distributed ldamodel	0.052632
ml	ml mlwriter	0.200000
setparams(self seed=none) sets params for	params set params seed	0.500000
stream returning the result	stream reader	0.076923
comprised of vectors containing i	mllib random rdds log normal vector	0.125000
the dot product of	mllib linalg dense vector dot other	0.058824
"predictions" which gives the true label of	ml logistic regression summary label	0.333333
__init__(self inverse=false	ml dct init inverse	1.000000
selector type of the	chi sq selector set selector type	0.111111
returns subset accuracy	multilabel metrics subset accuracy	1.000000
set initial centers should be	kmeans set initial centers centers	0.200000
of	ml linalg	0.121212
this matrix to a coordinatematrix	linalg block matrix to coordinate matrix	0.333333
cost sum of squared	compute cost x	0.142857
of the :class dataframe	frame writer save path format	0.066667
for each original	max scaler model original	0.062500
spark fair	core spark	0.010309
mlwriter for :py	mlwriter	0.062500
scaler	scaler	0.263158
greatest value of the list of	greatest	0.043478
seq of columns that describes the	cols cols kwargs	0.090909
contents of the :class dataframe	frame writer save path format	0.066667
for saving	ml java mlwriter	0.200000
the column mean values	mean	0.034483
rdd of key-value pairs (of form c{rdd[	core rdd save as	0.037500
the accumulator's value only usable in driver program	accumulator value	0.050000
into a java parammap	param map to java pyparammap	0.250000
specific group matched by	str pattern idx	0.111111
error which is defined as	regression	0.010000
field in "predictions" which gives	linear regression summary	0.041667
or transform the rdd of document	tf transform document	0.166667
use only create a	context create	0.083333
containing a json	json	0.043478
registers a python function including lambda function as	register function	1.000000
converts matrix columns in an input dataframe	mllib mlutils convert matrix columns to	0.166667
problem in multinomial logistic regression	mllib logistic regression	0.250000
ml params instances for the given param	params	0.006623
finding frequent	freq	0.166667
data type json string	parse datatype json string	0.333333
content of the :class dataframe as the	data frame writer save as	0.071429
a list of index value pairs or two	linalg	0.044444
@param input dataset for the test this	test case test func input func expected sort	0.333333
a function to each rdd in	rdd func	0.250000
the :class statcounter members as	stat counter as	0.333333
weights is close to the desired	parameter accuracy	0.029412
with a given	ml param	0.009524
matrix whose columns are the left singular vectors	singular	0.015625
partial objects	save partial obj	0.125000
data	sql data frame	0.005348
accumulator's value only usable in driver program	core accumulator value	0.090909
sets	indexer set	1.000000
a text file using string representations of elements	text file path compressioncodecclass	0.166667
profile stats	profiles	0.250000
input dataset this	dataset	0.020408
categories	categories	1.000000
:class dataframe representing the result of the	sqlquery	0.027027
the length of a string	sql length	0.050000
creates an	create	0.017241
items	items cols support	0.125000
of	ml linalg dense vector	0.200000
dump the profile stats into directory path	core profiler collector dump profiles path	1.000000
tables in the database dbname	dbname	0.045455
ignore separators inside brackets pairs e	ignore brackets	0.250000
changes the uid	uid	0.125000
test	tests test count by value	1.000000
distance	distance	0.333333
contains	ml param params	0.013699
ordered	take ordered	0.125000
for the stream query if	sql data stream	0.031250
converts vector columns	mllib mlutils convert vector columns to ml dataset	0.166667
the number of rows	mllib linalg row matrix num rows	0.200000
dataset	dataset featurescol	1.000000
__init__(self featurescol="features", predictioncol="prediction", maxiter=20 seed=none k=4 mindivisibleclustersize=1 0)	kmeans init featurescol predictioncol maxiter seed	1.000000
the correlation of two columns	corr col1 col2 method	0.055556
the stream	sql data stream	0.031250
adds an input option	frame reader option	1.000000
write()	one vs rest	0.034483
spark call in the current call	spark call	1.000000
of two vectors we support	ml linalg dense vector	0.100000
of the rdd's elements in	core rdd	0.003460
set a java system property such as spark	spark context set system property cls key	1.000000
of terms to term frequency vectors or	hashing tf	0.125000
only create a new	hive context create	0.083333
the sql context to use for saving	ml java mlwriter context sqlcontext	0.333333
shortcut of	ml	0.007339
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	tree regressor	0.058824
the singularvaluedecomposition if computeu was set to be	value decomposition u	0.100000
params to the	ml java params to	0.045455
the points belongs to in this model	model predict x	0.250000
function in python	function	0.027778
the specified database	tablename dbname	0.142857
a range of offsets from a single kafka	range	0.030303
for	for	0.555556
pyspark sql	sql to	0.083333
vectors or transform the rdd	mllib hashing tf transform	0.045455
mixin for param thresholds thresholds in multi-class	has thresholds	0.250000
stream query if this is not	data stream writer	0.041667
wait for the execution to	streaming streaming context await termination or timeout timeout	0.125000
get or compute the number of	indexed row matrix num	0.100000
add a py or zip	context add py file	0.166667
convert this vector to the	vector	0.019231
levenshtein distance of the two given	levenshtein left	0.058824
test predicted values on a toy	regression with sgdtests test predictions	0.500000
object by pyrolite whenever the	object rdd rdd	0.500000
python topicandpartition to	partition init topic partition	0.055556
computes hex value of the	hex col	0.166667
setparams(self numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")	discretizer set params numbuckets inputcol outputcol relativeerror	1.000000
set k decayfactor timeunit to configure	streaming	0.005025
path a shortcut of write() save	ml pipeline model save	0.166667
converts matrix columns in an	mllib mlutils convert matrix columns from ml dataset	0.166667
called when	streaming listener on	0.200000
kolmogorov-smirnov ks test for data	stat statistics kolmogorov smirnov test data	0.111111
a python wrapper of	ml	0.009174
as	as	0.370370
a multi-dimensional rollup for the current	sql data frame rollup	0.055556
format into an rdd of labeledpoint	mllib mlutils load lib svmfile	0.125000
infer schema from an rdd	spark session infer schema rdd samplingratio	0.250000
__init__(self	rformula init formula	0.500000
add a file to	context add file path	0.333333
the first n rows to the console	sql data frame show n truncate	0.333333
makes a class inherit documentation from its parents	mllib inherit doc cls	0.045455
train the model on	mllib streaming logistic regression with sgd train on	0.333333
selector	selector set selector	0.333333
rowmatrix	row matrix	0.200000
str around pattern pattern is a regular expression	str pattern	0.250000
queries so	query manager	0.011905
so that :func awaitanytermination() can be used again	query manager	0.011905
vectors which this	vector indexer	0.200000
return a resulting rdd that contains a tuple	core rdd cogroup	0.066667
value for each original column during	ml min max scaler model original	0.062500
get number of trees in	model num trees	1.000000
by any streaminglinearalgorithm	streaming linear algorithm	0.076923
binary value of the given column	bin col	0.333333
nulltype	has nulltype	1.000000
of memory for this	core external	0.016129
this instance with a	one vs rest model	0.058824
convert this matrix to	linalg dense matrix	0.083333
for this	external merger object	0.032258
adds a term	term	0.080000
matrix to an indexedrowmatrix	coordinate matrix to indexed row matrix	0.333333
the libsvm format into an rdd of labeledpoint	load lib svmfile	0.125000
:class pyspark.sql.types.datatype object	returntype	0.071429
any hadoop file system using the new	new	0.062500
soundex encoding for a string	sql soundex col	0.055556
passed as a list of key-value pairs	spark conf set all pairs	0.500000
is an array of	mllib matrix	0.047619
converts vector columns in an input dataframe to	mlutils convert vector columns	0.083333
params to the wrapped java object and	java params to	0.045455
to wait for	streaming query	0.010526
containing elements from start to end exclusive	core spark context range start end	0.166667
train a decision tree model for regression	mllib decision tree train regressor cls data	0.333333
parses the expression string into the	expr	0.076923
dstream by applying reducebykey	streaming dstream reduce by key	0.076923
join of c{self}	join	0.068966
value of	ml param has input cols	1.000000
curve	binary	0.307692
return sparkcontext which is associated with	streaming context spark	0.083333
javardd of	ml	0.001835
this matrix to the	dense matrix	0.076923
instance's params to the wrapped	params to	0.035714
note : experimental	generalized linear regression	0.181818
queries so that :func awaitanytermination() can	query manager	0.011905
estimator=none estimatorparammaps=none evaluator=none trainratio=0 75	estimator estimatorparammaps evaluator trainratio	0.500000
set the initial value	mllib streaming logistic regression with sgd set initial	0.111111
computes hex value of the given column	sql hex col	0.166667
an input stream that pulls events from	stream ssc hostname port storagelevel	0.200000
calculates the correlation of	col2 method	0.055556
this model	model predict x	0.125000
be used again to wait for new	sql streaming	0.010204
sort the list based on first	case sort result based on key	0.333333
wait for	sql streaming query	0.011765
cluster	mllib bisecting kmeans model cluster	0.333333
runs and profiles the method to_profile	core	0.003021
threshold threshold in	threshold	0.018182
sparkcontext as	spark context	0.046512
a local property set	local property key	0.035714
home	home	1.000000
path a shortcut of write() save	ml mlwritable save	0.166667
linear model that has a vector of	linear model	0.066667
again to	sql streaming query manager	0.011905
vector conduct pearson's chi-squared goodness of fit test	mllib stat statistics chi sq test	0.166667
of the :class dataframe to	frame writer	0.050000
sets the accumulator's value only usable in	core accumulator value	0.045455
ordered in ascending order or as	take ordered	0.125000
the training	distributed ldamodel training	0.034483
of this instance with a	ml one	0.166667
n elements from an rdd	rdd take	0.200000
an rdd created by piping	rdd	0.003058
neutral	zerovalue	0.076923
partitioned	external group by spill	0.047619
dump	external group by spill	0.047619
been started	started receiverstarted	0.500000
to an external database table via	url table	0.200000
and profiles the method to_profile passed	core	0.003021
used again	streaming	0.005025
one	one	0.294118
forget about past terminated queries so that :func	query manager reset terminated	0.200000
a windowing column	sql column over window	0.333333
additional params overwriting embedded values	params	0.006623
and count of the	rdd	0.003058
a param with	param params has	0.019231
correlation	col1 col2 method	0.055556
contains a param with a given string	params	0.006623
dataframe with two fields threshold	threshold	0.036364
or compute the number of	mllib linalg block matrix num	0.125000
a dataframe	data frame corr col1 col2	0.166667
:func	streaming query manager reset	0.011905
layers=none blocksize=128 stepsize=0 03 solver="l-bfgs", initialweights=none)	ml multilayer perceptron classifier	0.142857
forget about past terminated queries so that	sql streaming query manager reset terminated	0.200000
the underlying :class	session	0.050000
queries so that	manager reset	0.011905
of the :class dataframe in json	sql data frame	0.005348
mean variance and count of the rdd's	rdd	0.003058
tests whether this instance contains a param with	params has param paramname	0.142857
this instance to a java onevsrest used for	one vs rest to java	0.166667
scores into 0/1 predictions	mllib linear classification model	0.142857
of type distributedldamodel	ml ldamodel	0.111111
in this grid to fixed	ml param grid builder add grid param	0.250000
java ml instance the	java mlreader java	0.333333
a private abstract class representing a multiclass classification	linear classification	0.142857
find	find	0.888889
names into a jvm seq of	seq	0.043478
inputformat with arbitrary	inputformatclass keyclass valueclass	0.125000
and refresh all the	refresh	0.100000
compute the dot product of two	mllib linalg dense vector dot other	0.058824
fp-growth model that contains frequent itemsets	fpgrowth train cls data minsupport	0.200000
the given parameters in this grid	ml param grid builder	0.055556
serializes objects using python's pickle serializer http //docs	pickle serializer	1.000000
has started	started	0.111111
queries	streaming	0.005025
"zerovalue" which may be added to the	rdd fold by	0.125000
algorithm return	train rdd	0.166667
used again to wait for new	streaming query manager reset	0.011905
input dataset for the	input	0.090909
parse string representation back into the densevector	mllib linalg dense vector parse s	0.333333
casesensitive	case sensitive	1.000000
udf so it can be used in sql	sql sqlcontext	0.095238
or newline-delimited json	json path mode	0.125000
given a large dataset and an item approximately	lshmodel approx nearest neighbors dataset	0.166667
matrix to the new mllib-local representation	dense matrix as ml	0.333333
can be used again to wait for new	streaming query manager	0.011236
infer schema from an rdd of row or	sql sqlcontext infer schema rdd samplingratio	0.250000
sort the	sort result	0.333333
for each key using an associative and	by key	0.026316
all the elements in seen in a	windowduration slideduration	0.083333
option for	option key value	1.000000
so	sql	0.002525
min value for each original column during fitting	min max scaler model original min	0.250000
train the model on the incoming	streaming linear regression with sgd train on	0.333333
numfeatures	numfeatures	1.000000
that pulls events from flume	flume utils	0.200000
mixin for param thresholds thresholds in multi-class classification	has thresholds	0.250000
a python object into an internal sql object	type to internal obj	0.500000
the weights	weights	0.066667
of type distributedldamodel	ml	0.001835
converts vector columns in an input dataframe from	mllib mlutils convert vector columns	0.083333
the :class dataframe	frame writer	0.050000
number of	linalg dense vector num	0.166667
computes the levenshtein distance of	sql levenshtein	0.058824
globals names read or written to by codeblock	pickler extract code globals cls	1.000000
a left outer join of	left outer join other numpartitions	0.111111
columns in an input dataframe	columns from ml	0.125000
columns in an input	columns to ml dataset	0.125000
value of	ml has support	1.000000
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online", learningoffset=1024	lda init featurescol maxiter seed	0.250000
resolves paths to files added through l{sparkcontext	spark files	0.250000
and commutative reduce function but return	core rdd reduce	0.083333
java object by pyrolite whenever the	java object rdd rdd	0.500000
is called by the default implementation of fit	ml estimator fit	0.083333
number of times a token	count	0.016949
a new profiler using class profiler_cls	profiler collector new profiler ctx	0.333333
sets the accumulator's value only usable in driver	core accumulator value	0.045455
that stores	ml alsmodel	0.222222
the model should have been	mllib loader	0.333333
objects	merger	0.025641
that :func	query manager reset	0.011905
an rdd of points using the model trained	tree ensemble model	0.038462
checkpointinterval=10 impurity="variance", seed=none variancecol=none)	ml decision tree regressor	0.066667
a python rdd of key-value	rdd save	0.038462
until any of the queries	any termination	0.142857
featurescol="features", labelcol="label", forceindexlabel=false) sets params for	set params formula featurescol labelcol forceindexlabel	0.166667
the ordering columns in	order by	0.142857
dstream by applying reducebykey	streaming dstream reduce by key func	0.076923
each original column during	max scaler model original	0.062500
calculates the norm	mllib linalg sparse vector norm p	0.083333
create	create	0.275862
left outer join of	rdd left outer join	0.111111
a resulting rdd that contains a	core rdd cogroup other numpartitions	0.066667
a pearson's independence test using dataset	ml chi square test test dataset featurescol labelcol	0.333333
or compute the number of rows	linalg block matrix num rows	0.200000
accessible via jdbc	reader jdbc	1.000000
memory	merger object	0.032258
handle	handle	1.000000
that :func awaitanytermination() can	query manager	0.011905
matrix	matrix size	1.000000
as the spark fair scheduler	spark	0.013158
much of memory	size	0.009174
to their nearest center for this model	model compute	0.133333
partitioned	core external group	0.045455
the threshold if any used for converting raw	threshold	0.018182
rdd contains the count of distinct elements in	count	0.016949
bool int long float string list or dict	to_replace	0.166667
save a linearregressionmodel	model save sc path	0.500000
set the trigger for the stream query	stream writer trigger	0.083333
an associative and commutative reduce	rdd reduce	0.071429
if the rdd contains no elements at	core rdd is empty	0.083333
new java object	ml java wrapper new java obj java_class	0.333333
initial value of weights	initial weights initialweights	0.333333
the correlation of two columns of a dataframe	data frame corr col1 col2 method	0.500000
much of memory	external	0.013889
set bandwidth of	density set bandwidth bandwidth	0.142857
same param	param	0.006250
the accumulator's value only usable	accumulator value value	0.050000
transforms a python	java params transfer param	0.250000
from the	load stream	0.500000
sum of	ml bisecting kmeans	0.062500
dot product of two vectors we	dense vector dot	0.050000
each original column	ml min max scaler model original	0.062500
removes the specified table from	sql sqlcontext uncache table tablename	0.250000
current smallest value and add the new item	item	0.062500
for k classes	classes	0.034483
the minutes of	minute	0.040000
the month of a	sql dayofmonth	0.031250
a right outer	full outer	0.333333
a paired	matrix factorization	0.040000
core	core	0.015106
topicdistributioncol or its default value	topic distribution col	0.250000
tree <http //en wikipedia org/wiki/decision_tree_learning>_	tree classifier	0.500000
gives the predicted	summary prediction	0.500000
uniform	uniform	1.000000
the model's transform method	ml logistic regression summary predictions	0.200000
of training iterations until termination	ml logistic regression training summary total iterations	0.500000
mark the rdd	rdd	0.003058
copy all params defined on the class to	ml param params copy params	0.200000
applies standardization transformation on a vector	mllib standard scaler model transform vector	1.000000
of the :class dataframe to a data source	data frame writer	0.014085
the input path	path	0.020408
create an input stream that pulls events from	create stream ssc	0.200000
input schema	frame reader schema schema	0.333333
fits a model to the	estimator fit	0.166667
any hadoop file system using the l{org	save as sequence file path compressioncodecclass	0.500000
next memory limit if the memory is not	sorter next limit	0.200000
given name	name	0.043478
gaussian distributions as	ml gaussian mixture model gaussians df	0.166667
the given path	path	0.061224
:func awaitanytermination() can be used again to wait	streaming query manager	0.011236
of	ml	0.174312
"zerovalue" which may be added to the	fold by	0.125000
predicts rating for the given	matrix factorization model predict	0.250000
a python rdd of	core rdd	0.013841
new sparkcontext at least the master	spark context init master	0.333333
evaluates a list of conditions and returns one	sql column otherwise value	0.050000
rescale each feature individually	max scaler	0.200000
has been	receiverstarted	0.142857
input	sql data frame reader	0.111111
featuresubsetstrategy	feature subset strategy	1.000000
transforms a python parammap	java params transfer param map	0.250000
the training set given the	distributed ldamodel training	0.034483
concatenates multiple input string columns together into a	concat ws sep	0.500000
setparams(self featurescol="features", labelcol="label", predictioncol="prediction",	logistic regression set params featurescol labelcol predictioncol	1.000000
contains a	ml param	0.009524
rdd of labeledpoint	lib svmfile sc path numfeatures	0.125000
setparams(self featurescol="features",	linear regression set params featurescol	1.000000
new class dataframe that	data frame to	0.250000
improves on toy data with no of	with sgdtests	0.200000
adds input	frame reader	0.200000
wait for the	streaming streaming context await termination or timeout timeout	0.125000
the :class dataframe in orc	data frame writer orc	0.200000
vectors which this transforms	vector indexer	0.200000
until any of the queries	await any termination	0.142857
the soundex encoding for a	sql soundex	0.055556
setparams(self formula=none featurescol="features", labelcol="label",	formula featurescol labelcol	0.200000
a new rdd containing the distinct	distinct	0.055556
path	path	0.306122
converts matrix columns in	convert matrix columns from ml	0.166667
as a text file using string representations of	as text file path compressioncodecclass	0.500000
the objects	external merger object	0.032258
training iterations until termination	regression training summary total iterations	1.000000
name of the file to which this rdd	file	0.028571
that :func awaitanytermination() can be used again	sql streaming query manager reset	0.011905
over all trees in the ensemble	ensemble model	0.117647
when stdin is closed	stdin	0.166667
in libsvm format into	parse libsvm	0.125000
stream returning the result as	stream reader	0.076923
elements in one	rdd	0.003058
based	based	0.750000
adds an output option	writer option	1.000000
then merges them with extra	extra	0.023810
queries so that :func	query	0.010753
for each numeric columns	sql grouped data	0.083333
represents a specific topic and partition	topic and partition	0.111111
value in c{self} that is not contained in	core rdd subtract other numpartitions	0.111111
stop the execution of the streams	stop	0.052632
parses	parse	0.071429
script with a dependency on another module on	module dependency on	0.142857
to use for saving	java mlwriter	0.250000
converts vector columns in	mlutils convert vector columns to ml	0.166667
objects from the	serializer load stream	0.250000
the dataframe in a	sql data frame	0.005348
refresh	refresh	0.500000
a list of values	mllib linalg dense vector values	0.200000
the dispatch to handle all function	cloud pickler save function obj	0.142857
the deviance for the fitted	regression summary deviance	0.125000
the dot product of two vectors we	dense vector dot	0.050000
extract the year	sql year col	0.050000
converts matrix columns in	mllib mlutils convert matrix columns from ml dataset	0.166667
with a given	ml param params has param	0.019231
it can be used in sql statements	returntype	0.071429
or two separate arrays of indices and	ml	0.001835
converts vector columns	mlutils convert vector columns from	0.166667
returns an mlwriter instance for this	ml java mlwritable write	0.200000
the spark fair scheduler pool	core spark	0.010309
invalidate and refresh all the cached the metadata	hive context refresh	0.200000
columns of a dataframe as	sql data frame corr col1 col2	0.166667
model which	model	0.005587
threshold recall curve	binary logistic regression summary recall by threshold	0.166667
return an iterator of deserialized objects from	core serializer load	0.500000
rawpredictioncol raw prediction a k	raw prediction	0.200000
forest model	forest model	1.000000
returns the value of spark	get	0.021739
factors in two columns id	factors	0.142857
value of	ml generalized linear regression	0.750000
the dataframe	data frame	0.005000
specifies the underlying output	data stream writer	0.041667
word2vec model's vocabulary	word2vec	0.052632
partial objects do not	core cloud pickler save partial obj	0.125000
attributes which are array-like or buffer	array_like dtype	0.166667
internal	internal	1.000000
droplast	drop last	1.000000
stop the execution	context stop	0.125000
awaitanytermination() can be used again to wait for	streaming query manager reset	0.011905
stream query	stream writer	0.041667
number of gaussians in mixture	gaussian mixture model	0.052632
:class dataframe in json format (json lines text	sql data frame writer	0.011628
obtaining a test statistic	mllib stat test	0.166667
fit test of the	test	0.015152
how	merger	0.025641
this instance with a randomly generated uid and	cross validator	0.045455
compute the number of rows	linalg indexed row matrix num rows	0.200000
gives	linear	0.076923
c{self} that is not contained in c{other}	rdd subtract other numpartitions	0.111111
much of memory	merger object	0.032258
length of	length col	0.050000
multiclass	data numclasses	0.250000
train or predict a logistic regression model on	logistic regression with sgd	0.500000
note : experimental	correlation	1.000000
is vector conduct pearson's chi-squared goodness of fit	mllib stat statistics chi sq	0.066667
separate arrays of indices and	ml	0.001835
get	num	0.008403
are the left singular vectors of the singularvaluedecomposition	mllib linalg singular value decomposition	0.250000
rdd, a list or a	schema samplingratio verifyschema	0.029412
create an input from tcp source	context socket text stream	0.500000
udt	user defined type	0.250000
into a jvm seq of	seq sc cols	0.055556
catalog	sql catalog	0.285714
from one base to another	frombase tobase	0.333333
parses the expression string into the column that	expr str	0.125000
for each key using an	key func	0.066667
the input dataset this is called by	dataset	0.020408
number of nonzero elements this	sparse vector num	0.200000
selector type of	set selector type	0.111111
converts matrix columns in an	mllib mlutils convert matrix columns from ml	0.166667
new java object	new java obj java_class	0.333333
be used in sql statements	name f returntype	0.125000
test that param type conversion happens	param type conversion tests	0.333333
pivots a column of the current [[dataframe]] and	pivot pivot_col values	0.050000
with the dispatch to handle all function types	core cloud pickler save function obj	0.142857
for this query	streaming query	0.010526
local property that affects jobs	local property key value	0.076923
unified dstream from multiple dstreams of the same	streaming streaming context union	0.111111
default min	core spark context default min	1.000000
multiple parameters passed as a list of	spark conf	0.058824
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic", maxiter=20	ml gbtclassifier	0.095238
soundex encoding for a string	soundex	0.043478
behavior when	writer mode savemode	0.333333
:func	query	0.010753
contains a param with a	param params has	0.019231
load a java model from the	load java cls sc	0.200000
of the current [[dataframe]] and	grouped data pivot pivot_col values	0.050000
find all globals names read or written to	code globals	0.125000
value decomposition	svd k computeu rcond	0.333333
or transform the rdd of document to rdd	tf transform document	0.166667
set a local property that affects jobs submitted	context set local property key	0.200000
creates a table based on the dataset	catalog create external table tablename path	0.250000
train the model	mllib streaming logistic regression with sgd train	1.000000
comprised	random rdds gamma vector	0.125000
levenshtein distance	sql levenshtein left right	0.058824
use only create a	sql hive context create	0.083333
compute the sum for each numeric columns for	grouped data sum	0.083333
a new dstream in which each	streaming streaming context transform	0.066667
train the model on	train on	0.333333
in modlist to be placed into main	modules to main modlist	0.333333
a function to	map f	0.074074
:class dataframe to a data	data frame	0.005000
__init__(self formula=none	rformula init	0.500000
sequence	sequence	1.000000
test the python direct kafka rdd api	kafka stream tests test kafka rdd	0.500000
index keys are categorical feature indices column indices	ml vector indexer model category maps	1.000000
to	query	0.010753
py	py	0.300000
can be used again to	query manager	0.011905
finding frequent items for columns possibly with false	data frame freq items	0.166667
that	merger	0.025641
99], quantilescol=none aggregationdepth=2):	fitintercept	0.058824
improves on toy data with no of batches	with sgdtests	0.200000
train the model	sgd train	1.000000
as the square root of	root	0.035714
stream query if this is not set	sql data stream writer	0.041667
adds an input option for	stream reader option key	0.333333
find the maximum item in this rdd	core rdd max key	0.333333
computes the levenshtein distance of	levenshtein left right	0.058824
add a py or zip dependency for all	spark context add py file	0.166667
probabilitycol="probability", tol=0 01 maxiter=100 seed=none)	probabilitycol	0.050000
a new dstream in which each rdd	streaming streaming context	0.032258
original column	scaler model original	0.062500
a value to a boolean if possible	ml param type converters to boolean value	0.250000
latest model	streaming linear algorithm latest model	1.000000
finding frequent items	sql data frame freq items	0.166667
@param input dataset for the test this should	test case test func input func expected sort	0.333333
property that affects jobs	property key value	0.125000
of the rdd partitioned using the specified partitioner	rdd partition by numpartitions partitionfunc	0.333333
called when a receiver has reported an error	streaming streaming listener on receiver error receivererror	1.000000
queries so	manager	0.011236
a python wrapper of	ml java params from	0.250000
the index of	partitions with index	0.100000
default min number of partitions for	default min partitions	0.250000
the kolmogorov-smirnov ks test for data sampled from	stat statistics kolmogorov smirnov test data	0.111111
value in the key-value	map	0.058824
this distributed model to a local representation	ml distributed ldamodel to local	0.111111
marks the	blocking	0.142857
schema	schema row	1.000000
sets	chi sq selector set	0.166667
by the given columns	by	0.014286
the minutes of a	minute	0.040000
featurescol="features", labelcol="label", forceindexlabel=false) sets params for rformula	rformula set params formula featurescol labelcol forceindexlabel	0.500000
rdd	rdd save as	0.038462
the kmeans algorithm	streaming kmeans	0.035714
converts vector columns in an	convert vector columns from ml dataset	0.166667
spec	spec	0.384615
fast version of a heappush	core heappushpop heap item	0.142857
a right outer join of	rdd full outer join	0.111111
java storagelevel based on	context get java	0.333333
external	external list	0.166667
model	mllib kmeans model	0.250000
list of names of	sqlcontext table names	0.066667
new terminations	streaming	0.005025
into disks	core external group by spill	0.047619
a paired rdd where	factorization model	0.043478
add a py	spark context add py file	0.166667
comprised of vectors containing i	mllib random rdds poisson vector	0.125000
creates a global temporary view with this dataframe	sql data frame create global temp view name	1.000000
comprised of vectors containing i i	mllib random rdds normal vector	0.125000
r^2^, the coefficient of determination	mllib regression metrics r2	0.166667
resolves a param and validates the ownership	param params resolve param param	0.333333
all globals names read or written	code globals	0.125000
in "predictions" which gives the	logistic regression summary	0.090909
mixin for param thresholds thresholds	has thresholds	0.250000
find all globals names read or written	globals	0.076923
the approximate quantiles	approx	0.047619
data	core external group by spill	0.047619
layers=none blocksize=128 stepsize=0 03 solver="l-bfgs", initialweights=none)	multilayer perceptron classifier	0.333333
that all the objects	external merger object size	0.032258
convert a matrix from the new mllib-local	matrices from	0.333333
indicates whether this instance is	ldamodel is distributed	0.200000
coordinate	coordinate	0.500000
class representing a multiclass classification	classification	0.071429
add a py	context add py file	0.166667
of	size	0.009174
skinny	skinny	1.000000
update the	kmeans model update	0.500000
variance	rdd	0.003058
combine functions and a	core rdd	0.003460
number of cols	num cols	1.000000
registers this rdd as a temporary table using	register temp table	1.000000
attempt	attempt	1.000000
specification that defines the partitioning	spec	0.076923
an input	stream	0.017544
separate arrays of indices	ml	0.001835
the standard deviation of this rdd's	core rdd stdev	0.066667
densematrix whose columns are the right singular vectors	linalg singular	0.017544
the behavior when data	sql data frame writer mode savemode	0.071429
matrix to	linalg dense matrix	0.083333
wait for the execution to	streaming context await termination timeout	0.166667
the new mllib-local representation	as	0.148148
map of words to their vector representations	get vectors	0.142857
regression model	regression	0.040000
for binary or multiclass	cls data numclasses	0.250000
squared distance from a	mllib linalg sparse vector squared distance other	0.166667
predicted values on a toy model	logistic regression with sgdtests	0.200000
converts vector columns in an	mllib mlutils convert vector columns from ml dataset	0.166667
original column during	scaler model original	0.062500
of tables/views in the specified database	tables dbname	0.500000
param and	param param	0.100000
multiclass	classifier cls data numclasses	0.250000
__init__(self	indexer init	1.000000
group id to all the jobs	job	0.023810
soundex encoding for a string	sql soundex	0.055556
the second is an array of	mllib matrix	0.047619
setparams(self min=0 0 max=1 0	max scaler set params min max	1.000000
the trigger for the stream query if	sql data stream writer trigger	0.083333
string prefix-time suffix	to file name prefix suffix timestamp	1.000000
awaitanytermination() can be used again to wait	streaming query manager reset	0.011905
index of the original	map partitions with index	0.100000
of partitions	partitions	0.066667
test that the	test training	0.500000
a function and attach docstring from func	sql user defined function wrapped	0.333333
rdd returns	sql	0.002525
this model	model compute	0.133333
this instance	params has param	0.019231
norm of a	vector norm p	0.055556
range of offsets from a single kafka topicandpartition	range	0.030303
to consist	ascending numpartitions keyfunc	0.100000
returns the root mean squared error	metrics	0.041667
:class statcounter members as a dict	core stat counter as dict	0.333333
an fp-growth model that contains frequent	mllib fpgrowth train cls data minsupport	0.100000
be used again	streaming query manager	0.011236
represents an entry of a coordinatematrix	entry	0.250000
converts vector columns in an input dataframe	convert vector columns to ml	0.166667
rdd an rdd	train cls rdd	0.250000
power	power	1.000000
densematrix >>> dm = densematrix(2 2 range 4	mllib linalg dense matrix	0.083333
can be used again to	manager	0.011236
memory for this	object size	0.032258
of active queries associated with this	query manager active	0.066667
linear regression	linear regression	0.160000
transfer this instance's params	ml java params	0.125000
index	with index	0.100000
this instance's params	java params	0.200000
are the right singular vectors	singular	0.015625
java object and return the java object	java	0.012195
byte array that starts at pos	pos	0.022222
:param rdd an rdd of	train cls rdd	0.250000
returns a new java object	new java obj java_class	0.333333
transforms the embedded params to the companion	params transfer params to	0.333333
is vector conduct pearson's chi-squared goodness of	mllib stat statistics chi sq	0.066667
java model from the given	java	0.012195
sets the sql context to use for saving	java mlwriter context sqlcontext	0.333333
this configuration contain a given key?	spark conf contains key	0.333333
perform a right outer join of c{self}	full outer join other	0.111111
string column	string	0.041667
right outer join of	full outer join	0.111111
onevsrest create and return a python	one vs rest	0.034483
parses the expression string	expr str	0.125000
words to their vector representations	model get vectors	0.142857
on	context	0.022727
value of	ml multilayer perceptron	1.000000
foreachrdd	foreach	0.166667
with the specified schema	schema options	0.125000
converts matrix columns in	mllib mlutils convert matrix columns to ml	0.166667
matrix to the new mllib-local representation	dense matrix as	0.333333
a left outer join of c{self}	left outer join	0.111111
this instance	pipeline	0.052632
large dataset and an item approximately	ml lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
the streams with option of ensuring all received	stopsparkcontext stopgracefully	0.050000
two vectors we	linalg dense vector	0.200000
setparams(self	lda set params	1.000000
average values for each numeric columns for	sql grouped data avg	0.058824
this	core external	0.016129
a column containing a json string into a	json col	0.083333
from an	rdd	0.006116
waits for the termination	termination	0.035714
nodes in tree	mllib decision tree	0.166667
hadoop file system using the new	save as new	0.125000
:class dataframe as the	data frame writer save as	0.071429
to a local representation this discards info about	to local	0.125000
sets the given spark runtime configuration property	sql runtime config set key value	1.000000
__init__(self withmean=false withstd=true	standard scaler init withmean withstd	1.000000
each element	preservespartitioning	0.181818
the distributed matrix on the driver as	local matrix	0.250000
param with a	params has param	0.019231
day of the month of a given date	sql dayofmonth col	0.031250
create	utils create	1.000000
used	streaming query	0.010526
used again to wait	sql streaming query	0.011765
week number of a given date	sql weekofyear	0.055556
test predicted values on a toy model	streaming logistic regression with sgdtests test predictions	0.500000
a jvm seq of column	seq sc	0.055556
new	init	0.042553
does this configuration contain a given key?	conf contains key	0.333333
matrix columns in an input dataframe	matrix columns to ml dataset	0.142857
test that the final	sgdtests test	0.142857
on a model	mllib streaming linear regression	0.333333
test the python direct kafka stream	streaming kafka stream tests test kafka direct stream	0.625000
a shortcut of write()	ml pipeline model	0.066667
returns its elements in a numpy ndarray	linalg matrix to array	0.142857
the non-streaming :class dataframe out into external storage	sql data frame write	0.071429
a left outer join	left outer join	0.111111
week number of a given	weekofyear	0.043478
calculates the correlation of two columns of a	col2 method	0.055556
data	data distname	0.083333
can	sql streaming query manager reset	0.011905
of the non-streaming :class dataframe	data frame write	0.166667
number of partitions for hadoop rdds	partitions	0.066667
lambda function	function	0.055556
count of the	core	0.003021
returns an mlreader instance for this class	ml one vs rest read cls	1.000000
the dot product of two	ml linalg dense vector dot	0.090909
spark fair scheduler	core spark	0.010309
model which can	model	0.005587
a csv file	csv path	0.333333
property that affects jobs submitted from	property key value	0.125000
create an input stream that pulls events	utils create stream ssc	0.200000
value or cleared	description interruptoncancel	0.166667
train the model on the incoming	with sgd train on	0.333333
results immediately to the master	locally func	0.142857
around pattern pattern	pattern	0.142857
how much of memory	size	0.009174
with the dispatch to handle all function types	core cloud pickler save function obj name	0.142857
root directory that contains	get root directory cls	0.333333
ignore separators inside brackets	sql ignore brackets	0.250000
from an rdd ordered	rdd take ordered	0.050000
randomly generated	cross validator	0.045455
gaussianmixture	gaussian mixture	0.038462
the month of a given date as	dayofmonth	0.027027
which is a risk	linear regression summary	0.013889
the rdd's	rdd	0.003058
hostname port data	hostname port	1.000000
of binomial logistic regression	ml logistic regression	0.222222
returns an mlwriter instance for	java mlwritable write	0.200000
algorithm return the model	train rdd	0.166667
of the rdd partitioned	rdd partition by	0.062500
topics	topics	0.750000
new rdd containing the distinct	distinct numpartitions	0.142857
a job of a batch has started	started outputoperationstarted	0.125000
the frame boundaries defined from start	between start	0.100000
frequent	prefix span model freq	1.000000
norm of	sparse vector norm p	0.066667
count of the rdd's	core rdd	0.003460
extracts the embedded default param values	params extract param map	0.333333
local property set in	core spark context get local property key	0.066667
base	base	1.000000
attr lda keeplastcheckpoint	ml distributed ldamodel get	0.066667
f	f	0.052632
the norm of a	sparse vector norm	0.066667
fits a java model	estimator fit java	0.333333
rdd of labeledpoint	load lib svmfile	0.125000
parameters specified by the param grid	param grid	0.200000
groups the :class dataframe	data frame group by	0.200000
await	await	0.833333
matrix other from this block matrix this	linalg block matrix	0.052632
max abs vector	abs scaler model max abs	1.000000
default min	context default min	1.000000
of column or names into a jvm seq	seq sc	0.055556
is not contained in c{other}	rdd subtract	0.333333
returns accuracy equals to the	metrics accuracy	0.166667
comprised	random rdds poisson vector	0.125000
cachenodeids=false checkpointinterval=10 impurity="variance",	decision tree regressor	0.058824
to this accumulator's value	add	0.035714
a dense vector of 64-bit floats from a	ml linalg vectors dense	0.166667
approximately find at most k items which	ml lshmodel approx	0.125000
__init__(self inputcol=none outputcol=none indices=none names=none)	ml vector slicer init inputcol outputcol indices names	1.000000
can be used again to wait for	streaming query	0.010526
note : deprecated in	sql	0.002525
a	spark submit tests	1.000000
polynomial expansion	polynomial expansion	1.000000
a java	get java	0.111111
generates an rdd comprised of vectors containing	random rdds gamma vector rdd sc	0.200000
parameters in this grid to	ml param grid builder base	0.076923
__init__(self labelcol="label", featurescol="features", predictioncol="prediction",	ml generalized linear regression init labelcol featurescol predictioncol	1.000000
mean squared error which is	linear regression summary	0.013889
neighbors	neighbors	1.000000
an expression that	sql column	0.333333
size to be used for	size	0.009174
linearregression	linear	0.025641
add the new item	item	0.062500
dstream by applying reducebykey to each	streaming dstream reduce by key func	0.076923
shortcut of write()	ml mlwritable	0.142857
attempt numbers are correctly reported	test attempt number	0.333333
abstract class representing a multiclass classification	classification	0.071429
the observed data against	observed	0.058824
value of	ml stop words remover	1.000000
this	core external merger object	0.032258
changes the uid of	ml param params reset uid newuid	0.058824
configuration contain a given key?	spark conf contains key	0.333333
resets the configuration property for	runtime config unset	0.142857
of a list or gets an	column get	0.200000
elements in seen in a sliding window	window windowduration slideduration	0.333333
external sort when the	external	0.013889
loads a csv file and	reader csv path schema	0.333333
much	core external	0.016129
utc returns another timestamp that corresponds to the	sql from utc timestamp timestamp tz	0.166667
comprised of vectors containing i i d	random rdds log normal vector	0.125000
that can be used to read data streams	read	0.111111
column from one base	col frombase tobase	0.166667
:py attr outputcol	output col value	1.000000
summary	repr	0.222222
list based on first value	based on key	0.111111
bisecting k-means algorithm return the model	bisecting kmeans train rdd	0.333333
ml params	params	0.006623
class wraps a function rdd[x] -> rdd[y]	transform function	0.166667
javardd of object by unpickling it will	ml	0.001835
used	sql streaming query manager reset	0.011905
week number of	weekofyear col	0.055556
checkpointinterval=10 impurity="variance", seed=none variancecol=none)	regressor	0.043478
compute the number of cols	mllib linalg row matrix num cols	0.333333
of names of tables in the	sqlcontext table names	0.066667
trees in the ensemble	mllib tree ensemble	0.111111
step	step	0.600000
list of	ml chi sq selector	0.100000
broadcast a read-only	context broadcast	0.125000
setparams(self estimator=none estimatorparammaps=none evaluator=none trainratio=0 75 seed=none):	set params estimator estimatorparammaps evaluator trainratio	0.500000
reverse	reverse	0.833333
pipeline create	pipeline	0.052632
returns a :class dataframe representing the result of	session sql sqlquery	0.250000
compute the number	linalg row matrix num	0.100000
a model with weights already	streaming linear regression with	0.111111
model improves on toy data with no	mllib streaming logistic regression with sgdtests	0.200000
in modlist	modlist	0.100000
absolute path of a file added through	core spark files get	0.125000
for new terminations	manager	0.011236
new hadoop outputformat api mapreduce	new apihadoop dataset conf keyconverter valueconverter	0.142857
instance's params to the	ml java params to	0.045455
minutes of a given	minute col	0.050000
first n rows to the console	data frame show n	0.333333
__init__(self labelcol="label", featurescol="features", predictioncol="prediction",	linear regression init labelcol featurescol predictioncol	1.000000
much of	core external merger object size	0.032258
the kolmogorov-smirnov ks test for data sampled	stat statistics kolmogorov smirnov test data	0.111111
columns	columns to ml dataset	0.125000
which is a	mllib regression metrics	0.090909
compute the standard deviation	rdd stdev	0.066667
a py or zip dependency for	py	0.050000
the stream query if this is not set	sql data stream writer	0.041667
any hadoop file system using the l{org	save as sequence file path	0.500000
returns a paired rdd where the	matrix factorization model	0.043478
documentation of	ml param	0.009524
to	add	0.035714
total log-likelihood for this	ml gaussian mixture summary log likelihood	0.142857
the accumulator's value only usable in driver program	core accumulator value value	0.050000
the user and the second is an array	mllib	0.010526
labeledpoint	mlutils load lib svmfile sc path numfeatures	0.125000
pivots a column of the current [[dataframe]] and	data pivot pivot_col values	0.050000
spark	core spark context	0.011628
file and	path schema sep	0.333333
pearson's chi-squared goodness of fit	mllib stat statistics chi sq	0.066667
with the spark sink deployed	addresses storagelevel maxbatchsize	0.045455
of nonzero elements this	ml linalg	0.030303
py	py file path	0.066667
inherit documentation	inherit doc	0.045455
for the termination of this query either by	termination	0.035714
for the sample covariance of col1 and col2	covar samp col1 col2	0.333333
to stdout id is the rdd id	core profiler show id	0.333333
param	param	0.100000
a randomly generated	validation split model	0.200000
python type list	arr	0.166667
converts vector columns in an input dataframe	convert vector columns	0.166667
densematrix >>> dm = densematrix(2 2 range 4	dense matrix	0.076923
create a new sparkcontext at least the	spark context init	0.083333
saves the contents of	writer save path format mode partitionby	0.200000
text format or newline-delimited json <http //jsonlines	json path mode compression dateformat	0.166667
the content of	writer	0.040000
which predictions are known	isotonic regression model	0.100000
:class column for distinct count of col	count distinct	0.040000
creates a global temporary view with this dataframe	data frame create global temp view name	1.000000
recommends	matrix factorization model recommend	0.250000
batches of	streaming linear algorithm	0.076923
set initial centers should be set before	streaming kmeans set initial centers centers weights	0.200000
pyspark sql types longtype column	sql sqlcontext range start end step numpartitions	0.083333
summary (e	summary	0.024390
api with start offset	from offset	0.125000
column or names into a jvm seq of	seq sc	0.055556
by any streaminglinearalgorithm	linear algorithm	0.076923
the accumulator's value only usable in driver	core accumulator value value	0.050000
dispatch to handle all	cloud pickler save	0.166667
returns an	sql streaming	0.010204
vector columns in an input dataframe from	vector columns to	0.142857
the given parameters in this grid	grid builder base	0.076923
ordered in ascending	take ordered	0.125000
the default implementation of fit	ml estimator fit	0.083333
this instance contains a param with a given	ml param	0.009524
of nodes in tree	mllib decision tree	0.166667
sets the accumulator's value only usable	accumulator	0.012987
merge the values for each key using an	key func numpartitions	0.066667
vector to the new mllib-local representation	vector as ml	0.500000
train the model on the incoming dstream	with sgd train on dstream	1.000000
to the wrapped java object and	to	0.007692
converts	convert	0.444444
labels corresponding	string indexer model labels	0.166667
pipeline create and return a python	pipeline	0.052632
of	merger object	0.032258
matrix columns in an input dataframe to the	matrix columns	0.071429
converts vector columns in an input dataframe	convert vector columns from ml dataset	0.166667
distributed model to	distributed ldamodel to	0.166667
value	aid value	1.000000
approximate	approx	0.095238
the residual degrees of freedom for the null	summary residual degree of freedom null	0.333333
is later than the	dayofweek	0.037037
params	params m1	0.047619
mean	mllib standard scaler model mean	0.125000
conduct pearson's chi-squared goodness of	stat statistics chi sq	0.066667
a	spark	0.065789
accumulator's value only usable in	core accumulator value	0.045455
approximate distinct count	approx count distinct col rsd	0.066667
given java_class type useful	pylist java_class	0.500000
deviance for the null	ml generalized linear regression summary null deviance	0.250000
underlying data source	sql data stream	0.031250
the greatest value of the list of	sql greatest	0.055556
the entry point to programming	session	0.050000
much	core external merger object	0.032258
converts matrix columns	mlutils convert matrix columns	0.166667
the number of	linalg block matrix num	0.100000
return a l{statcounter} object that captures the	core rdd stats	0.083333
locate the position of the first occurrence	locate	0.076923
cost sum of	cost x	0.142857
incoming	streaming	0.005025
get or compute the number	linalg block matrix num	0.100000
timestamp	timestamp	0.833333
the accumulator's data type returning a new value	accumulator param	0.038462
the points belongs to in this model	kmeans model predict x	0.333333
deserialized batches lists	stream without unbatching	0.125000
sort the list based	streaming test case sort result based	1.000000
indices back to a new column of	index to	0.040000
will convert each python object into	to	0.023077
return a copy of the rdd partitioned using	core rdd partition	0.333333
year of a given date	year col	0.050000
value of	ml param has prediction	1.000000
comprised of vectors containing i	random rdds normal vector	0.125000
dump already partitioned data into	core external	0.016129
translate	translate	1.000000
a right outer	rdd full outer	0.333333
outcomes for k classes classification	classes	0.034483
that each of the points belongs to in	predict x	0.033898
dump	core external group	0.045455
a function and	user defined function	0.066667
a heappop	core heappop	0.500000
queries so that :func awaitanytermination() can be used	streaming	0.005025
to be inherited by any streaminglinearalgorithm	streaming linear algorithm	0.076923
adds	accumulator	0.012987
underlying output	sql data stream writer format	0.333333
using an associative and	core rdd	0.003460
python rdd of	rdd save	0.038462
manage all the :class streamingquery streamingqueries active	streaming query manager	0.011236
the :class dataframe to	frame writer save path format	0.066667
of a heappop followed by a heappush	core heapreplace	0.250000
is of length len	len	0.071429
so that :func awaitanytermination() can be	sql	0.002525
terminations	query	0.010753
weightcol=none	linear	0.051282
column for approximate distinct count of	approx count distinct col rsd	0.066667
contents of the :class dataframe	frame writer save path	0.066667
only create a new hivecontext for testing	sql hive context create for testing	0.333333
expected value of	ml linear	0.066667
create	hive context create	0.083333
again to wait for	query	0.010753
the array or map stored in the column	col	0.016393
list of names of tables in the	table names	0.066667
point in rdd 'x' to all mixture	mllib gaussian mixture	0.045455
outputted by the model's transform method	ml logistic regression summary predictions	0.200000
stepsize step	step	0.100000
configuration property for the given key	key	0.017857
model with weights already	linear regression with	0.111111
infer schema from an rdd of row or	sql sqlcontext infer schema rdd	0.250000
function to the value	f	0.021053
divisible	divisible	1.000000
computes column-wise summary statistics for the input rdd[vector]	stat statistics	0.125000
c{self} that is not contained in c{other}	core rdd subtract other numpartitions	0.111111
rdd of key-value pairs (of	rdd	0.009174
get total number of nodes summed	model total num nodes	0.250000
an :class rdd, a	schema samplingratio verifyschema	0.029412
for this udt	sql user defined type	0.500000
associative and commutative reduce function	core rdd reduce	0.083333
spark fair scheduler	core spark context	0.011628
a jvm seq of column	seq sc cols	0.055556
path a shortcut of write()	ml pipeline	0.095238
a python parammap into a java parammap	param map to java pyparammap	0.250000
on kafka	kafka	0.111111
other from this block	linalg block	0.076923
:py attr intermediatestoragelevel	intermediate storage level value	1.000000
batches after which the centroids	timeunit	0.025641
attr lda	distributed ldamodel get	0.066667
as a	spark	0.013158
mixin for param checkpointinterval	has	0.011628
tuple so python knows how to pickle row	row reduce	1.000000
inserts the content of the :class dataframe	sql data frame writer insert into	0.500000
compute the sum for each numeric columns	grouped data sum	0.083333
transforms a java parammap into a python parammap	java params transfer param map from java	1.000000
and	core spark context	0.023256
input data	sql data stream reader	0.250000
for each key	by key func numpartitions	0.062500
wait	streaming	0.005025
cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	random forest classifier	0.022727
set	option utils set	1.000000
new class dataframe	data frame	0.005000
generates an rdd comprised of vectors containing	mllib random rdds uniform vector rdd sc	0.200000
load a model from the given	mllib power iteration clustering model load cls sc	0.500000
to a data source	data	0.011628
awaitanytermination() can be used again to	query	0.010753
for each original column during	model original	0.062500
converts vector columns in an input	mlutils convert vector columns	0.166667
convert this vector to the new mllib-local representation	mllib linalg vector as	1.000000
a l{statcounter} object that captures the	stats	0.055556
load a linearregressionmodel	model load cls sc path	0.500000
type	type cls	0.250000
the length of a string	sql length col	0.050000
of	core external merger object	0.032258
return the slideduration in seconds	slide duration	0.333333
format into an rdd of labeledpoint	load lib svmfile sc path numfeatures minpartitions	0.125000
into an rdd of labeledpoint	load lib svmfile sc path	0.125000
number in	sql	0.002525
set the initial	regression with sgd set initial	0.111111
attr lda keeplastcheckpoint	ldamodel	0.034483
so that	manager reset	0.011905
checkpointinterval=10 impurity="variance", seed=none	tree regressor	0.058824
param	params has	0.019231
test prediction on a model with weights	streaming linear regression with tests test prediction	0.500000
add a py or zip dependency	context add py	0.166667
:class dataframe in	data frame writer	0.028169
forget about past terminated queries so that :func	streaming query manager reset terminated	0.200000
inherit documentation from its	inherit doc cls	0.045455
that :func awaitanytermination() can be used again	streaming query manager reset	0.011905
already partitioned data into	group	0.025641
value of	ml param has elastic net	1.000000
the number of	block matrix num	0.100000
the rdd's elements in one	rdd	0.003058
distinct elements in rdds in a sliding window	value and window windowduration slideduration numpartitions	0.076923
already partitioned data into disks	group by spill	0.047619
an rdd comprised of vectors containing	random rdds gamma vector rdd	0.166667
left singular	linalg singular	0.017544
create a java	wrapper new java	0.166667
comprised of i i d samples	mllib random rdds	0.041667
jvm scala map from a dict	frame jmap jm	0.111111
for distinct count of col or cols	count distinct	0.040000
parse a string representation back into the vector	mllib linalg vectors parse s	0.333333
model to the input dataset this is	dataset	0.020408
rdd of labeledpoint	mllib mlutils load lib svmfile sc path	0.125000
extracts the embedded default param values and user-supplied	params extract param map	0.333333
can be used	streaming	0.005025
setparams(self seed=none) sets params for this test	other test params set params seed	1.000000
a local property set in this	get local property key	0.066667
columns in	columns from ml dataset	0.125000
a multi-dimensional cube for the	sql data frame cube	0.055556
which is a dataframe having two fields fpr	regression	0.010000
in json format (json	sql	0.002525
cachenodeids=false checkpointinterval=10 seed=none impurity="gini",	classifier	0.050000
much	core external merger	0.032258
instance with a randomly	cross validator	0.045455
columns in an input dataframe from	columns to ml	0.125000
by this thread until the group	group	0.025641
:class dataframe in orc	sql data frame writer orc	0.200000
containing a json string	sql from json	0.166667
the norm of a sparsevector	vector norm	0.055556
accumulator	accumulator init	0.083333
add two values	add	0.035714
python object into an internal sql object	data type to internal obj	0.500000
that provide save() through their scala implementation	java saveable	0.333333
ml params instances for the given	params m1	0.047619
for each original column	scaler model original	0.062500
param with	has	0.011628
python rdd of key-value pairs (of form	core rdd save as	0.037500
for new	sql	0.002525
wait for new	sql	0.002525
ndcg value	ndcg	0.100000
params to the wrapped java object and	ml java params to	0.045455
outputformat api mapred package	dataset conf keyconverter valueconverter	0.250000
data source	source	0.210526
setparams(self inputcol=none outputcol=none labels=none) sets params	set params inputcol outputcol labels	0.333333
columns are the left singular	linalg singular	0.017544
counts	grouped data	0.035714
create a new dstream in which	streaming streaming	0.047619
return the	train rdd	0.166667
arbitrary key and value class from	core spark context	0.023256
predicted ratings for input user and product pairs	matrix factorization model predict all user_product	0.050000
the content of the :class dataframe as	data frame writer save as	0.071429
impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)	random forest classifier	0.022727
generates python code for a shared param class	ml param gen param code name doc	0.333333
dataframe that stores item	alsmodel item	0.250000
the libsvm format into an rdd of labeledpoint	load lib svmfile sc path numfeatures	0.125000
the predicted	prediction	0.083333
internal function to get or create global taskcontext	core task context get or create cls	0.250000
initial value of weights	streaming logistic regression with sgd set initial weights	0.333333
given product and returns a	product	0.029412
the first occurrence of substr	substr	0.071429
stream	sql data stream	0.031250
computeq	computeq	1.000000
a local property that affects jobs submitted	local property key	0.035714
get or compute the number of	matrix num	0.088235
set the initial value of	with sgd set initial	0.111111
jvm seq of columns that describes the sort	frame sort cols cols kwargs	0.142857
support __transient__ on new objects	cloud pickler save reduce func args state listitems	0.111111
pipelinemodel create	pipeline model from	0.142857
sparkcontext which is associated with	streaming context spark	0.083333
a new spark configuration	spark conf	0.058824
number of gaussians in mixture	mllib gaussian mixture model	0.062500
finding frequent items for columns	sql data frame freq items cols support	0.166667
__init__(self featurescol="features", labelcol="label", predictioncol="prediction",	classifier init featurescol labelcol predictioncol	1.000000
items by creator and combiner	core merger merge values iterator	0.166667
parameters in this grid to fixed	param grid builder base on	0.076923
ids of all active stages	core status tracker get active stage ids	0.250000
spark sink deployed on	maxbatchsize	0.037037
get the rdd's current storage level	rdd get storage level	1.000000
for this	merger object	0.032258
converts a sql datum into a user-type object	user defined type deserialize datum	0.333333
python topicandpartition to map to the java	init topic partition	0.055556
create a new accumulator with	accumulator	0.012987
of features corresponding	features	0.043478
columns in an input dataframe	columns	0.078431
square root of the mean	root mean	0.250000
:py attr lda	ml distributed ldamodel	0.050000
extract a specific group matched by a java	sql regexp extract str pattern idx	0.333333
threshold if any used for converting	threshold	0.018182
dot product of two vectors we support	dense vector dot other	0.050000
a given amount of time for a condition	condition	0.045455
calculates the correlation of	method	0.041667
make predictions on batches of	linear algorithm predict on	0.066667
__init__(self n=2	ml ngram init n	1.000000
initial value of	regression with sgd set initial	0.111111
queries so that	streaming	0.005025
this instance's params	params	0.006623
this rdd and its recursive dependencies for	core rdd	0.003460
the objects	object size	0.032258
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params for	ml vector indexer set params maxcategories inputcol outputcol	0.333333
for distinct count of col or	count distinct	0.040000
table in the	table df tablename	0.083333
to a boolean if possible	ml param type converters to boolean value	0.250000
the spark sink	ssc addresses storagelevel maxbatchsize	0.045455
vectors	mllib linalg vectors	0.333333
object into java	ml py2java sc obj	0.333333
the values for each key	by key func numpartitions partitionfunc	0.066667
akaike's "an information criterion" aic for	ml generalized linear regression summary aic	0.250000
positive rate for a given label	positive rate label	1.000000
column mean values	mllib standard scaler model mean	0.125000
for each numeric columns for	grouped data	0.071429
and refresh all the cached the metadata	refresh	0.100000
jm	jm	1.000000
this instance	param	0.012500
converts vector columns in an input	mllib mlutils convert vector columns	0.166667
given string name	ml param	0.009524
mllib	mllib	0.052632
specified table	tablename overwrite	0.333333
finding frequent items for columns possibly	data frame freq items cols	0.166667
number of users for a given product	users product num	0.333333
containing the ids of all active	active stage ids	0.200000
using the new	save as new	0.125000
add a py or	core spark context add py file	0.166667
of a job of a batch has completed	completed	0.058824
test method	mllib stat chi sq test result method	0.250000
point in rdd 'x' has maximum membership	gaussian mixture	0.038462
:class streamingquery streamingqueries active	streaming query manager	0.011236
of conditions and	column otherwise	0.200000
cost sum	compute cost x	0.142857
for	sql streaming query manager	0.011905
or create	streaming streaming context get or create	0.200000
train a regression model with l2-regularization	ridge regression with sgd train cls	1.000000
can be used again to wait	sql streaming	0.010204
for each numeric columns for each	grouped data	0.071429
k decayfactor timeunit to	streaming	0.005025
datatype	datatype	0.500000
the user and the second is an	mllib	0.010526
limits the result count to the number	limit num	1.000000
of rows whose distance	ml	0.001835
partitions to use during reduce tasks (e g	core rdd default reduce partitions	0.166667
extract the week number of a given	sql weekofyear col	0.055556
a new sparkcontext	spark context init	0.083333
of labeledpoint	lib svmfile sc	0.125000
array containing the ids	stage ids	0.055556
sparse vector using either a dictionary a list	mllib linalg vectors sparse size	0.166667
this broadcast	core broadcast	0.200000
__init__(self mindocfreq=0	idf init mindocfreq	1.000000
of the specified string value that match regexp	regexp	0.076923
path a shortcut of write() save	ml pipeline save	0.166667
parameters in this grid to	builder add grid	0.100000
trigger for the	trigger	0.071429
category if	mllib multiclass	0.285714
a multi-dimensional cube for the current :class	sql data frame cube	0.055556
return all partitioned items as iterator	core external merger external items	1.000000
for kmeans	kmeans	0.025641
observed tokens in the training set given the	distributed ldamodel training	0.034483
inherit documentation from its	inherit	0.037037
for each numeric columns for each	sql grouped	0.086957
set application name	core spark conf set app name	1.000000
the objects	merger	0.025641
greatest value of the list of column	sql greatest	0.055556
ownership	param params resolve	0.333333
this instance with a randomly	split model	0.200000
be	query manager	0.011905
in one operation	core	0.003021
sets	cross validator set	1.000000
degrees of	ml generalized linear regression summary degrees of	1.000000
as spark executor	spark	0.013158
instance contains a param with a given	ml	0.001835
any hadoop file system using the old hadoop	hadoop	0.050000
to load a :class dataframe	data frame	0.005000
write()	mlwritable	0.166667
columns that make up each block	cols per block	0.333333
using the old hadoop outputformat api mapred	as hadoop dataset conf keyconverter valueconverter	0.083333
a term to this accumulator's	term	0.040000
spark sink deployed on	storagelevel maxbatchsize	0.045455
number of nonzero	dense vector num	0.166667
this instance contains	ml param params has	0.019231
for this obj assume that all	external merger object size obj	0.040000
broadcast a read-only variable	spark context broadcast	0.125000
partial objects do not serialize correctly in python2	save partial obj	0.125000
dot product of two vectors we	ml linalg dense vector dot other	0.090909
in a profile object is returned	basic profiler profile	0.200000
calculates the norm	vector norm p	0.166667
to this accumulator's value	core accumulator	0.030303
window specification that defines	window spec	0.166667
external database table via	url table	0.200000
view with the given view name in	view viewname	0.500000
partial objects do not serialize	pickler save partial obj	0.125000
the deviance for the fitted model	regression summary deviance	0.125000
of the observed data against the expected distribution	observed expected	0.166667
a python function including lambda function	function name f	0.166667
checkpoint the	checkpoint	0.062500
py or zip dependency for	py file path	0.066667
the dot product of two	dense vector dot other	0.050000
feature	regression model feature	1.000000
compare 2 ml params	compare params m1	0.200000
__init__(self inputcol=none	ml string indexer init inputcol outputcol	1.000000
objective function scaled loss + regularization at each	ml logistic regression training summary objective history	0.500000
global temporary view with this	global temp view name	0.500000
dataframe in a	sql data frame writer	0.011628
the python direct kafka stream transform get	kafka direct stream transform get	0.500000
values and then merges them with extra values	extra	0.023810
to this	accumulator	0.012987
by applying 'left	left	0.066667
a certain time of day in utc	from utc	0.125000
wrap this udf with a function and	user defined function	0.066667
load an rdd previously saved	minpartitions	0.071429
ordered in ascending order	ordered	0.076923
norm of a sparsevector	mllib linalg sparse vector norm	0.083333
a list of active queries associated with this	sql streaming query manager active	0.066667
be used again to wait	query manager	0.011905
randomly generated	validation split model	0.200000
next	next	1.000000
a given	param	0.012500
initial value of weights	with sgd set initial weights	0.333333
so that :func awaitanytermination() can be used	streaming query manager	0.011236
the fraction given on each stratum	by col fractions seed	0.142857
test the partition id	task context tests test partition id	1.000000
(e g accuracy/precision/recall objective history total iterations)	logistic regression	0.040000
local temporary view	temp view name	0.111111
parameters passed as a list of	spark conf set all	0.125000
to use for saving	ml mlwriter	0.200000
a local property set in this	spark context get local property key	0.066667
:class dataframe using the specified columns so	data frame	0.005000
on first value	on key	0.333333
get pipeline stages	pipeline get stages	1.000000
loads parquet files returning the	reader parquet	0.200000
uid	params reset uid newuid	0.333333
pearson's chi-squared goodness of fit test	stat statistics chi sq test	0.166667
mixin for param regparam regularization parameter (>= 0)	has reg param	1.000000
large dataset and an item approximately find at	lshmodel approx nearest neighbors dataset key	0.166667
paired rdd where the first element	factorization model	0.043478
mine frequent sequential patterns	prefix span	0.166667
the accumulator's value only usable in driver program	accumulator	0.012987
the length of a string or	sql length col	0.050000
for this obj assume that	size obj	0.040000
convert python object into java	ml py2java sc obj	0.333333
adds an output option for	writer option key	1.000000
of the :class dataframe to a data source	sql data frame writer save	0.083333
that :func awaitanytermination() can be used again	sql streaming query manager	0.011905
of vectors which this transforms	ml vector	0.200000
min value for each original column during	ml min max scaler model original min	0.250000
values for each key using an associative	by key func numpartitions	0.062500
performs the kolmogorov-smirnov ks test for data	mllib stat statistics kolmogorov smirnov test data distname	0.111111
converts matrix columns in an input dataframe	mlutils convert matrix columns	0.166667
thread such as the spark fair scheduler	core spark context	0.011628
memory for this obj assume that all the	merger object size obj	0.040000
already partitioned	group by spill	0.047619
:class column for distinct count of col or	count distinct	0.040000
l{sparkcontext} that this rdd was created	core rdd context	0.166667
rdd of key-value pairs (of form c{rdd[	rdd	0.009174
a term	term	0.040000
set the selector type of	set selector type	0.111111
converts vector columns in an	mllib mlutils convert vector columns	0.166667
that they are equivalent	pipelines m1 m2	0.166667
point in rdd 'x' to all mixture components	mllib gaussian mixture model predict soft	0.142857
size default 5	size windowsize	1.000000
a list or a :class pandas dataframe	data frame data	0.142857
demonstrate udt in scala java and python	example point	0.500000
value of	ml random	1.000000
parses a line in libsvm format into label	parse libsvm line line multiclass	0.111111
local property set in this thread or null	core spark context get local property key	0.066667
use	context init	1.000000
of all the queries	mllib ranking metrics	0.250000
using the old hadoop	as hadoop	0.142857
each training data point	gaussian mixture summary	0.200000
sets random seed	mllib word2vec set seed seed	1.000000
note : experimental	chi sq selector	0.083333
property set	property key	0.066667
right singular vectors	singular	0.015625
the first n rows to the console	frame show n	0.333333
makes a class inherit documentation from	mllib inherit	0.045455
python	streaming	0.005025
key using an associative and commutative reduce function	reduce by key	0.333333
the soundex encoding for a string	sql soundex	0.055556
sets the spark session	session sparksession	0.083333
position pos	pos	0.022222
column containing a json	json col	0.083333
of data from a dstream	dstream	0.031250
calculates the md5 digest and returns	sql md5 col	0.333333
result as a string column	string	0.041667
test that the model params are	test test model params	0.250000
generates an rdd comprised	random rdds uniform rdd sc size numpartitions seed	0.200000
parameters in this grid to fixed values	ml param grid builder add grid param values	0.333333
cost sum of squared distances of points to	compute cost	0.142857
the dot product of	dot	0.040000
point	point	1.000000
even if users construct taskcontext instead	task context new cls	0.333333
for indexing categorical feature columns in a dataset	indexer	0.055556
multiclass	multiclass	0.357143
of memory for	merger	0.025641
featurescol="features",	formula featurescol	0.400000
add a py or zip dependency for	spark context add py	0.166667
sets window size default 5	mllib word2vec set window size windowsize	1.000000
consist of key value	key ascending numpartitions keyfunc	0.071429
dot product of two vectors	dense vector dot	0.050000
from flume	flume utils	0.200000
number of rows in	count	0.016949
outputformat api	outputformatclass keyclass valueclass	0.250000
right outer join of c{self} and c{other}	core rdd full outer join other numpartitions	0.200000
for new	query manager	0.011905
:py attr nonnegative	nonnegative value	1.000000
dictionary a list of index value pairs	size	0.036697
wait for the execution to stop	or timeout timeout	0.125000
this matrix to a coordinatematrix	matrix to coordinate matrix	0.333333
false positive rate	false positive rate	1.000000
matrix to a rowmatrix	row matrix to row matrix	0.333333
converts vector columns in an input	mllib mlutils convert vector columns from ml dataset	0.166667
converts vector columns in an	mllib mlutils convert vector columns to ml dataset	0.166667
this rdd's elements	core	0.003021
as a temporary table in	as table df tablename	0.250000
akaike's "an information criterion" aic for the	ml generalized linear regression summary aic	0.250000
metricname="f1")	metricname	0.166667
onevsrestmodel	one vs rest model	0.058824
python object into an internal sql object	sql data type to internal obj	0.500000
already	external group by	0.045455
merge the values for each key	key	0.035714
python parammap into a java	map to java	0.250000
of indices	ml linalg	0.030303
copy of the rdd partitioned using	rdd partition by	0.062500
computes the levenshtein distance	sql levenshtein left right	0.058824
functions registered in	functions	0.071429
soundex encoding for a string >>> df	sql soundex col	0.055556
dispatch to handle all function	cloud pickler save function obj	0.142857
get total number of nodes summed over	model total num nodes	0.250000
the year of a given date as	sql year	0.050000
python topicandpartition to	and partition init topic partition	0.055556
the key-value	map	0.058824
sort the list based on first	case sort result based on key outputs	0.333333
has reported an	receivererror	0.142857
model for classification or regression	model	0.005587
locate the position of the first occurrence of	locate	0.076923
obj assume that all the	merger object size obj	0.040000
partitioned data	group by	0.041667
the features	features col	0.250000
make predictions on a keyed	kmeans predict on values	1.000000
of the :class dataframe to a data source	sql data frame writer save path format	0.142857
set bandwidth	density set bandwidth bandwidth	0.142857
selector type of	mllib chi sq selector set selector type	0.111111
the kolmogorov-smirnov ks test for	stat statistics kolmogorov smirnov test	0.166667
of memory for this obj assume	object size obj	0.040000
that :func awaitanytermination() can be used	sql streaming query manager reset	0.011905
stages	get	0.021739
already partitioned	core external	0.016129
of write()	ml pipeline model	0.066667
__init__(self degree=2 inputcol=none outputcol=none)	polynomial expansion init degree inputcol outputcol	1.000000
of all active stages	core status tracker get active	0.333333
a randomly generated	cross validator	0.045455
instance contains a param	params	0.006623
params to the wrapped java object and return	ml java params to	0.045455
returns a :class	sql spark session sql	0.250000
script on a	script on	0.500000
are the right singular vectors of the	mllib linalg singular	0.017544
embedded	ml param params	0.013699
option	option	1.000000
wait for	await termination or timeout timeout	0.125000
can be used again	sql streaming	0.010204
used with the spark sink deployed	maxbatchsize	0.037037
dump the profile stats into directory path	core spark context dump profiles path	1.000000
local property set in this thread or	context get local property key	0.066667
use only create a new hivecontext for	hive context create for	0.250000
select	select	1.000000
with this streamingcontext	context	0.022727
return a javardd	core	0.003021
unifying	union	0.090909
kolmogorov-smirnov ks test for data sampled from	mllib stat statistics kolmogorov smirnov test data	0.111111
conditions and returns one of multiple possible result	sql column otherwise	0.050000
resulting rdd that contains a	rdd cogroup other	0.066667
path	path mode	0.333333
fp-growth model that	fpgrowth train cls data minsupport	0.200000
:param rdd an rdd of	mllib power iteration clustering train cls rdd	0.250000
random forest model for classification or regression	random forest	0.041667
queries so that :func	manager	0.011236
utc returns another timestamp that corresponds	sql from utc timestamp timestamp tz	0.166667
a dependency on another module on a cluster	module dependency on cluster	0.500000
truncated to the unit specified by	trunc	0.142857
or transform the rdd of document to	mllib hashing tf transform document	0.166667
warning these have null parent estimators	ml gbtclassification	1.000000
perform a right outer join	rdd full outer join other	0.111111
all the objects	object	0.027778
matrix columns in an input	matrix columns	0.142857
sets	ngram set	1.000000
parses a line	line line multiclass	0.166667
>>> dm = densematrix(2 2 range 4	dense matrix	0.076923
c{self} and c{other}	join other numpartitions	0.071429
all mixture components	mllib gaussian mixture model predict soft	0.142857
:func awaitanytermination() can	reset	0.011236
of values	linalg dense vector values	0.200000
column	sql	0.007576
for column of predicted clusters in	ml clustering summary prediction col	0.111111
list of tables/views in the	catalog list tables	0.250000
test	kafka stream tests test	0.562500
wait until any of the	manager await any termination timeout	0.166667
the column	standard scaler	0.076923
associative and commutative reduce function	core rdd reduce by	0.125000
a densematrix whose columns are the right singular	linalg singular	0.017544
all	object size	0.032258
list of active queries associated with this	query manager active	0.066667
comprised	random rdds exponential	0.125000
parses the expression string	sql expr str	0.125000
user-defined type udt	user defined type	0.250000
saved using	sc path minpartitions	0.250000
computes column-wise summary statistics for the input rdd[vector]	mllib stat statistics col	0.200000
statements	name f returntype	0.125000
fits a java model to the	ml java estimator fit java	0.333333
or names into a jvm seq	seq	0.043478
the minutes of a given date as integer	sql minute	0.050000
value of	ml param has raw	1.000000
optional default value and user-supplied value	params	0.006623
model to a local representation this	ldamodel to local	0.200000
__init__(self inputcol=none	indexer init inputcol outputcol	1.000000
contains a	ml param params has	0.019231
of partitions to use during reduce tasks	reduce partitions	0.166667
singular vectors of the singularvaluedecomposition	linalg singular value decomposition v	0.250000
applying 'left	left	0.066667
create new	sql	0.002525
tree including	mllib decision tree model	0.076923
jvm seq of	seq sc	0.055556
libsvm format into an rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures minpartitions	0.125000
dump the profile stats into directory	profiler collector dump profiles	1.000000
value of	ml standard scaler	0.400000
each value in c{self} that is not contained	subtract other numpartitions	0.111111
the dot product	mllib linalg dense vector dot	0.058824
set number of batches after which the	kmeans set	0.090909
the same param	m1 m2 param	0.125000
of tree (e g depth 0	tree	0.020833
day in utc	utc	0.100000
an rdd	rdd take	0.200000
binary byte array	binary	0.076923
computes the levenshtein distance	sql levenshtein left	0.058824
:py attr maxmemoryinmb	max memory in mb value	1.000000
mixin for param predictioncol prediction	has prediction	1.000000
which is a dataframe having	logistic regression summary	0.045455
debugging	to debug string	0.200000
root directory that contains files added through c{sparkcontext	core spark files get root directory	1.000000
approximately find at	lshmodel approx	0.100000
rdd returns	sql spark	0.125000
density	density	1.000000
rdd of points using the model	regression model	0.031250
with this spark job on every node	core spark context	0.011628
items	items	0.400000
uid of this	ml param params reset uid newuid	0.058824
fields threshold precision curve	binary logistic regression summary precision by threshold	0.166667
function and attach docstring from func	sql user defined function wrapped	0.333333
unique	id	0.083333
cluster centers represented as a list of numpy	ml bisecting kmeans model cluster centers	0.333333
set initial centers should be set before	set initial centers centers weights	0.200000
return a	rdd	0.003058
weights computed for	model weights	0.166667
the specified partitioner	by numpartitions partitionfunc	0.250000
return an javardd of object	ml	0.001835
returns a	sql spark	0.250000
test	test	0.227273
finding frequent items for columns possibly with	freq items	0.166667
ids of all active stages	status tracker get active stage ids	0.250000
inherit documentation	inherit doc cls	0.045455
estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none): sets params	set params estimator estimatorparammaps evaluator numfolds	0.333333
parse a field in schema abstract >>> _parse_field_abstract("a")	sql parse field abstract s	1.000000
returns a densevector with singular	singular	0.015625
new class dataframe that with new	data frame to df	0.090909
basic operation test for dstream combinebykey	streaming basic operation tests test combine by key	1.000000
the vector	vector	0.019231
setparams(self labelcol="label", featurescol="features", predictioncol="prediction",	set params labelcol featurescol predictioncol	1.000000
wait until any of the	query manager await any termination timeout	0.166667
the number of	mllib linalg block matrix num	0.062500
memory for this obj assume	core external merger object size obj	0.040000
awaitanytermination() can be used again to wait for	streaming	0.005025
the python direct kafka stream foreachrdd get	kafka direct stream foreach get	0.500000
to support __transient__ on new	cloud pickler save reduce func args state listitems	0.111111
and only if the rdd contains no elements	core rdd is empty	0.083333
the	standard	0.071429
computes the sum of	ml bisecting kmeans	0.062500
of months between date1 and date2	months between date1 date2	0.333333
the stream query if this is not set	stream writer	0.041667
to support __transient__ on	cloud pickler save reduce func args state listitems	0.111111
ml	ml java mlwriter	0.200000
setparams(self numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")	quantile discretizer set params numbuckets inputcol outputcol	1.000000
a condition	condition	0.045455
much of memory for this	merger	0.025641
the cluster centers represented as a	kmeans model cluster centers	0.090909
offset	from offset	0.125000
point in rdd 'x' has maximum membership in	gaussian mixture	0.038462
rdd's elements	core rdd	0.006920
for the stream	sql data stream	0.031250
input dataset for	input	0.090909
javaparammap	javaparammap	1.000000
full class name	class cls	0.333333
until any of the	await any	0.142857
multiclass classification	numclasses categoricalfeaturesinfo	0.250000
performs the kolmogorov-smirnov ks test for data sampled	stat statistics kolmogorov smirnov test data	0.111111
of	ml distributed	0.400000
defined on the class to current object	ml param	0.009524
training set	training	0.029412
parammap into a python	from	0.045455
of the	rdd	0.003058
partial objects do not serialize	core cloud pickler save partial obj	0.125000
predicted ratings for input user and product	mllib matrix factorization model predict all user_product	0.050000
comprised of vectors containing	random rdds gamma vector	0.125000
multi-dimensional cube for the current :class	sql data frame cube	0.055556
to make predictions on batches of data	mllib streaming linear algorithm predict on	0.066667
setparams(self inputcol=none outputcol=none labels=none) sets params for	index to string set params inputcol outputcol labels	0.333333
jvm seq of columns that describes	cols cols kwargs	0.090909
for the stream query if	sql data stream writer	0.041667
__init__(self estimator=none estimatorparammaps=none evaluator=none	ml cross validator init estimator estimatorparammaps evaluator	1.000000
a local property set in this thread or	spark context get local property key	0.066667
akaike's "an information criterion" aic for the fitted	ml generalized linear regression summary aic	0.250000
of this instance with a randomly generated	ml cross validator	0.166667
:class dataframe to a data source	data frame writer save	0.083333
load a model from the	model load cls sc	0.250000
the population should be a rdd	mllib stat kernel density	0.066667
number of rows	count	0.016949
create a new profiler using class profiler_cls	profiler collector new profiler ctx	0.333333
that :func awaitanytermination() can	streaming	0.005025
new :class dataframe that drops the specified	data frame drop	0.250000
contains a param with a given	params has	0.019231
create a	hive context create	0.083333
outputoperationstarted	outputoperationstarted	1.000000
new spark configuration	core spark conf	0.055556
number	set num	0.500000
find the n smallest elements in a dataset	core nsmallest n iterable key	0.333333
with a function and	defined function	0.066667
a term to this accumulator's value	core accumulator add term	0.066667
of specific	offset	0.021739
train the model on the incoming	kmeans train on	0.333333
leaf	mllib decision	0.125000
int containing elements from start	core spark context range start	0.090909
instance contains a	params has	0.019231
parses the expression string into the column	sql expr	0.125000
to configure the kmeans algorithm	streaming kmeans	0.035714
scipy sparse matrices if scipy is available	sci	0.142857
standard deviation	stdev	0.047619
broadcast a read-only variable	broadcast value	0.125000
property	property key	0.066667
an rdd containing all	core rdd	0.003460
a param with a given	params has	0.019231
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", classifier=none)	one vs rest init featurescol labelcol predictioncol classifier	1.000000
a test statistic	stat test	0.166667
of a job of a batch has started	started outputoperationstarted	0.125000
get or compute the number	mllib linalg coordinate matrix num	0.166667
setparams(self formula=none featurescol="features", labelcol="label", forceindexlabel=false)	formula featurescol labelcol forceindexlabel	0.200000
a new accumulator with	accumulator init	0.083333
in c{self} that is not contained	rdd subtract other numpartitions	0.111111
squared error which is defined	regression	0.010000
term frequency vectors or	tf	0.076923
spark session to use for loading	mlreader session sparksession	0.333333
returns an active query from	sql	0.002525
set to a different	set	0.005917
given product and	product	0.029412
load the ldamodel from disk	mllib ldamodel load cls sc path	1.000000
levenshtein distance of	levenshtein left	0.058824
a given	ml param params has param	0.019231
of features i e length	features	0.043478
or	get	0.043478
forget about past terminated queries so that	manager reset terminated	0.200000
the given	sc	0.187500
content of the :class dataframe	data frame	0.025000
the trigger for	trigger	0.071429
a column containing a json string into	sql from json col	0.083333
an rdd containing all	rdd	0.003058
instance to a java pipelinemodel used for ml	pipeline model to java	0.100000
for statistic functions	data frame stat	0.250000
local property set in this thread	local property key	0.035714
compare 2 ml params instances for the	compare params m1	0.200000
external sort when the memory	core external	0.016129
the week number of a given date	weekofyear	0.043478
in a sliding window over	value and window windowduration slideduration numpartitions	0.076923
sets	tree ensemble params set	1.000000
a param with	ml param params has	0.019231
or an rdd of points using the model	regression model	0.031250
the length of a string or	length col	0.050000
oneatatime	oneatatime	0.555556
value of	ml aftsurvival	1.000000
compression	compression	0.357143
data into	core external group by	0.045455
to make predictions on batches	streaming linear algorithm predict on	0.066667
named table accessible via jdbc url url	reader jdbc url table column	0.166667
tokens in the training set given the current	ldamodel training	0.034483
weightage	life halflife	0.500000
:class gbtregressor	gbtregression	0.142857
labeled points saved using rdd	labeled points sc path minpartitions	0.250000
:class dataframe	sql data frame	0.085561
the current [[dataframe]] and perform	pivot pivot_col values	0.050000
maxiterations	maxiterations	1.000000
broadcast a read-only variable to the cluster	spark context broadcast	0.125000
perform a pearson's independence test using	ml chi square test test	0.333333
featurescol="features",	featurescol	0.750000
returns an empty java parammap reference	java params empty java param map	1.000000
thresholds thresholds in multi-class classification to adjust	thresholds	0.071429
in this context to	streaming streaming context	0.032258
for the stream query if this is	stream	0.017544
or compute the number of rows	block matrix num rows	0.200000
make predictions on a dstream	streaming kmeans predict on dstream	1.000000
of the importance of each feature	ml decision tree regression model feature importances	0.250000
runs and profiles the	core	0.003021
gradient-boosted trees	gradient boosted trees	1.000000
can be used	query manager	0.011905
key-value pairs	set all pairs	0.500000
passed as a list of key-value pairs	core spark conf set all pairs	0.500000
streaming :class dataframe from	data stream	0.028571
life	life	1.000000
returns a :class	sqlcontext sql	1.000000
a new accumulator with a	accumulator	0.012987
an input stream that pulls	stream	0.017544
return as an dict	row as dict recursive	1.000000
rawpredictioncol raw prediction a k a confidence	raw prediction	0.200000
a list of	ml bisecting kmeans	0.062500
parses a line in libsvm format into label	mllib mlutils parse libsvm line line	0.111111
even if users construct taskcontext instead of	core task context new	0.333333
for which predictions are	regression	0.010000
the expected	expected	0.076923
in	in	1.000000
the kolmogorov-smirnov ks test	mllib stat statistics kolmogorov smirnov test	0.166667
this thread such as the spark fair scheduler	core spark context	0.011628
the root mean squared error which is defined	linear regression summary	0.013889
unbatching	unbatching	1.000000
code	code name doc	0.111111
standardization whether to standardize the training features before	standardization	0.076923
create a column scipy matrix from a	mllib sci py tests scipy matrix size	0.090909
convert this matrix to the	linalg dense matrix	0.083333
new :class column for distinct count of col	count distinct col	0.040000
using the new hadoop outputformat api mapreduce package	save as new apihadoop dataset conf keyconverter valueconverter	0.142857
finding frequent items for columns possibly	sql data frame freq items cols	0.166667
pipelinemodel create and	pipeline model	0.071429
the given parameters in this grid to	ml param grid builder base on	0.076923
finding frequent items for columns possibly with false	freq items	0.166667
block matrix this	block matrix	0.500000
for each numeric	sql grouped	0.086957
on each stratum	by col fractions seed	0.142857
this	param params has param	0.019231
__init__(self featurescol="features", labelcol="label", predictioncol="prediction",	regression init featurescol labelcol predictioncol	0.333333
for each numeric columns for each group	sql grouped data avg	0.058824
initial	regression with sgd set initial	0.111111
document to	document	0.040000
length of a	length	0.040000
computes column-wise summary statistics for the	mllib stat statistics col stats rdd	0.200000
restore an object of namedtuple	core restore name fields value	0.333333
save this model to the given	java saveable save sc	1.000000
squared error which is defined as the	mllib regression	0.022727
inherit documentation from its	inherit doc	0.045455
this	streaming	0.020101
of the rdd's elements in one operation	core rdd	0.003460
classification problem in multinomial logistic	mllib logistic	0.200000
partitioned	by spill	0.047619
used again to	sql streaming query	0.011765
the levenshtein distance	levenshtein left right	0.058824
python direct kafka rdd	kafka rdd	0.285714
returns the least value of the list	sql least	0.055556
fields	sql struct type len	0.200000
with a given string	param params has param	0.019231
recommends	factorization model recommend	0.250000
again to wait for	manager	0.011236
disks	core external	0.016129
used again to	manager reset	0.011905
boundaries from start inclusive to end inclusive	start end	0.090909
:func awaitanytermination() can be used again	manager	0.011236
infer schema from an rdd	sql sqlcontext infer schema rdd	0.250000
mean	standard scaler model mean	0.125000
column for the sample covariance of col1 and	covar samp col1	0.250000
containing a json string into a [[structtype]] or	sql from json	0.166667
a multi-dimensional rollup for the current :class	data frame rollup	0.055556
observed	observed	0.352941
of columns	mllib linalg block matrix cols	0.333333
create a python topicandpartition to map to the	init topic partition	0.055556
do external sort when the memory goes above	external sorter sorted	1.000000
default	context default	1.000000
so that	sql streaming query manager reset	0.011905
computes the levenshtein distance of the	sql levenshtein left	0.058824
area	metrics area	0.333333
sets	stop words remover set	0.600000
output a python rdd of key-value	core rdd	0.010381
partial objects do not serialize correctly in	core cloud pickler save partial	0.125000
: deprecated in	sql	0.002525
the items by creator and	core merger merge values iterator	0.166667
squared distance from a	vector squared distance other	0.166667
the root mean squared error which is defined	mllib regression	0.022727
of this instance	ml one vs	0.142857
approximate quantiles of numerical	approx quantile col probabilities relativeerror	0.166667
predictions which gives the predicted value of each	ml generalized linear regression summary prediction col	0.333333
elasticnetparam the elasticnet mixing parameter	elastic net	0.125000
a multi-dimensional rollup for the	data frame rollup	0.055556
key using an associative and	by key	0.026316
registers a python function including lambda function as	catalog register function name f	1.000000
stream query if this is	data stream writer	0.041667
to files added through l{sparkcontext	spark files	0.250000
transforms a python parammap into a java parammap	java params transfer param map to java pyparammap	1.000000
values for each numeric	sql grouped data	0.041667
contains	ml	0.001835
frequency vectors or transform the rdd of	tf transform	0.045455
inherit documentation	mllib inherit doc cls	0.045455
value of	ml min max	1.000000
return	standard	0.071429
converts vector columns in an input dataframe to	mlutils convert vector columns from	0.166667
copy of the rdd partitioned	rdd partition by	0.062500
return a resulting rdd that contains a	core rdd cogroup	0.066667
this broadcast on	core broadcast	0.200000
chi-squared	chi sq	0.111111
wrapped	wrapped	0.714286
setparams(self featurescol="features", labelcol="label",	regression set params featurescol labelcol	1.000000
fitintercept whether to fit an intercept term	fit intercept	0.250000
of points using the model	regression model	0.031250
of the rdd's elements in one	rdd	0.003058
tree	decision tree	0.076923
the database dbname	dbname	0.045455
the index of	map partitions with index	0.100000
for each key using a custom set	by key	0.026316
type	type	0.341463
ndcg value	metrics ndcg	0.200000
setparams(self	generalized linear regression set params	1.000000
calculates the length of a string	sql length	0.050000
the levenshtein distance of the two given strings	sql levenshtein	0.058824
non-streaming :class dataframe out	sql data frame write	0.071429
globals names read or written to by	globals	0.076923
the correlation of two columns of	method	0.041667
"zerovalue" which	rdd fold by	0.125000
get total number of	total num	0.333333
wait for the execution to stop return true	termination or timeout timeout	0.125000
passed as a list of	spark conf set all	0.125000
inputcol=none outputcol=none indices=none names=none)	inputcol outputcol indices names	1.000000
the default implementation of	ml estimator	0.125000
can be used again to	sql streaming query	0.011765
extract the week number of a given date	weekofyear col	0.055556
vectors	vectors sc path	0.333333
column as a :class	data frame	0.005000
convert the java_model to a python model	create model java_model	0.250000
create a method for given unary	sql unary	0.200000
elementtype	elementtype	1.000000
the stream query if this is	stream	0.017544
which	linear regression summary	0.027778
:class dataframe to	frame writer save path format	0.066667
level	level	0.625000
create a column scipy matrix from a dictionary	mllib sci py tests scipy matrix	0.090909
a value to a boolean	to boolean value	0.250000
streamingcontext from checkpoint data or create	context get or create	0.200000
names into a jvm seq	seq	0.043478
time of day in utc	from utc	0.125000
set the selector type	mllib chi sq selector set selector type	0.111111
dump already partitioned data into	group by	0.041667
for the stream query	stream writer	0.041667
streaming dataframe/dataset is written to	writer output mode outputmode	0.083333
vector	vector	0.423077
forget about past terminated queries so that :func	reset terminated	0.200000
configuration property if not already set	conf set if missing	1.000000
of labeledpoint	lib svmfile sc path	0.125000
return a new rdd	rdd	0.003058
into label indices values	mllib mlutils parse	0.250000
load a model from the given path	svmmodel load cls sc path	1.000000
cluster that each of the points belongs to	predict x	0.033898
columns in an input dataframe from the	columns to ml dataset	0.125000
again to	streaming	0.005025
java pipeline create and return a	pipeline from java	0.142857
by	key by	1.000000
this udf with a function	user defined function	0.066667
extract the minutes of a given	minute col	0.050000
(json lines text format or newline-delimited json	json path mode	0.125000
partial objects do not	pickler save partial	0.125000
python direct kafka stream foreachrdd get offsetranges	kafka direct stream foreach get offset ranges	0.500000
of this	ml java	0.076923
param with a	param params has	0.019231
a paired rdd where the first	matrix factorization model	0.043478
instance contains	ml param params	0.013699
arrays of indices	ml linalg	0.030303
a jvm seq of	seq	0.043478
returns the idf vector	ml idfmodel idf	0.333333
forget about past terminated	query manager reset terminated	0.200000
pearson's chi-squared goodness of fit test of	stat statistics chi sq test	0.166667
first argument-based logarithm of the second	log arg1 arg2	0.200000
data into	core	0.003021
via	url	0.076923
term frequency vectors or transform the rdd	hashing tf transform	0.045455
test	tests test group	1.000000
code	code name doc defaultvaluestr	0.111111
column after position pos	pos	0.022222
chi	chi	1.000000
finding frequent items for	frame freq items cols	0.166667
simple sparse vector class	sparse vector	0.062500
transformeddstream to transform on kafka rdd	kafka transformed dstream	0.333333
so that :func awaitanytermination() can be used again	query manager reset	0.011905
udf registration	udf	0.142857
of memory for this	core external merger object size	0.032258
of blocks in	blocks	0.076923
mixture	mixture model predict	0.125000
matrix to an indexedrowmatrix	matrix to indexed row matrix	0.333333
model fitted by multilayerperceptronclassifier	multilayer perceptron classification model	0.500000
the ordering columns in a	order by	0.142857
content of the :class dataframe to	sql data frame writer	0.011628
which is a dataframe having two	regression summary	0.035714
a paired rdd where	matrix factorization model	0.043478
aggregate the	rdd aggregate	0.250000
be inherited by any streaminglinearalgorithm	streaming linear algorithm	0.076923
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10	set params featurescol maxiter seed	0.250000
the elements in seen in a sliding window	window windowduration slideduration	0.333333
compute the number of	mllib linalg indexed row matrix num	0.125000
single kafka	offset	0.021739
the threshold	datasetb threshold distcol	0.500000
area under	area under	0.166667
rows in	sql	0.005051
specific kafkardd	streaming kafka rdd offset	0.500000
the attempt numbers are correctly reported	task context tests test attempt number	0.333333
so that :func awaitanytermination() can	sql	0.002525
of write() save path	ml one vs rest save path	0.200000
train a gradient-boosted trees model for	mllib gradient boosted trees train regressor cls	0.333333
broadcast	context broadcast value	0.125000
utility class that can load ml instances	mlreader	0.037037
list of predicted ratings for	mllib matrix factorization model predict all user_product	0.050000
do profiling on	core profiler profile	0.500000
standard deviation	rdd stdev	0.066667
in	param grid builder	0.055556
parameters in this grid to	grid builder add grid	0.100000
sets	mllib word2vec set	0.800000
be used	streaming query	0.010526
sparkcontext which is associated with	spark	0.013158
a batch of jobs has completed	batch completed	0.333333
converts vector columns in an input	mllib mlutils convert vector columns from	0.166667
mincount	mincount	0.857143
partial	save partial	0.125000
multi-dimensional rollup for the current :class dataframe using	frame rollup	0.055556
fit test of the observed data against the	test observed	0.090909
summary of a	summary	0.024390
param with a given string	param params has param	0.019231
a densematrix whose columns are the right singular	mllib linalg singular	0.017544
time for a condition	condition	0.045455
this dataset checkpointing can be used	frame checkpoint eager	0.071429
a	matrix	0.015152
tree (e g depth 0 means	tree model	0.026316
py or zip dependency for	py	0.050000
old hadoop	save as hadoop	0.142857
this matrix	dense matrix	0.076923
aggregationdepth	aggregation depth	1.000000
elements in	core	0.003021
be	streaming	0.005025
the += operator adds a term to this	accumulator iadd term	0.142857
list of functions registered	catalog list functions	0.250000
stream foreachrdd	stream foreach	1.000000
instance with a randomly generated	validation split	0.200000
line in	line line multiclass	0.166667
choose one directory for spill by number n	core external merger get spill dir n	1.000000
numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)	ml random forest	0.071429
return a jvm scala map from a	sql data frame jmap jm	0.111111
a local property that affects jobs	local property key value	0.076923
for new	streaming query manager	0.011236
the :class statcounter members	stat counter	0.333333
:py attr lda	ldamodel get	0.066667
returns a paired rdd	matrix factorization	0.040000
rdd's elements in one operation	core	0.003021
other from this block	mllib linalg block	0.111111
for params	params copy	0.083333
converts vector columns in an input dataframe to	mlutils convert vector columns from ml dataset	0.166667
the rdd partitioned using	rdd partition by	0.062500
extract the minutes	sql minute	0.050000
an input from tcp source	streaming context socket text stream	0.500000
awaitanytermination() can be used again to wait	streaming query	0.010526
perform a left outer join of c{self} and	core rdd left outer join other	0.200000
can be used again to wait for new	streaming	0.005025
the :class dataframe to a data	sql data frame writer save	0.083333
the null	generalized linear regression summary null	0.250000
rdd by applying a function to	f	0.021053
the attempt numbers are correctly reported	context tests test attempt number	0.333333
dummy params instance used as a placeholder	ml param params dummy	0.111111
position of the first occurrence of substr	substr str	0.125000
the mean variance and count of	rdd	0.003058
such as the spark fair scheduler	spark context	0.023256
reducebykey to each rdd	reduce	0.041667
applying a function to	map f	0.074074
to be placed into main	modules to main	1.000000
the :class dataframe as	sql data frame writer save as	0.071429
replacement	replacement	1.000000
parses the expression string into	sql expr	0.125000
that can be used to read data in	read	0.111111
value for each original column	max scaler model original	0.062500
that makes a class inherit documentation from its	mllib inherit	0.045455
gaussians in mixture	mllib gaussian mixture	0.045455
as spark	core spark	0.010309
sparkcontext	spark	0.026316
as the spark fair scheduler	core spark	0.010309
of a	matrix	0.015152
already partitioned data	core external	0.016129
for new terminations	streaming	0.005025
"predictions" which gives the features of each instance	ml linear regression summary features	0.166667
of column names skipping	sql	0.005051
a :class dataframereader	spark session	0.100000
for a given product	product	0.029412
:class dataframe	sql data frame writer	0.046512
and count of the rdd's elements in	core	0.003021
the probability of obtaining a test statistic	stat test	0.166667
pearson's chi-squared goodness of fit test of the	stat statistics chi sq test	0.166667
until any of the	sql streaming query manager await any	0.142857
in "predictions" which gives the true label of	ml linear regression summary label	0.333333
seqfunc	seqfunc	1.000000
the spark sink deployed on	storagelevel maxbatchsize	0.045455
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor",	init featurescol labelcol predictioncol	0.083333
column of predicted clusters in	ml clustering summary prediction col	0.111111
batches	algorithm	0.090909
an rdd of points using the model	regression model	0.031250
hadoop configuration which is passed in as	context hadoop	0.090909
a param with a given string name	ml param params	0.013699
number of nonzero elements this	vector num	0.181818
python wrapper of	ml java	0.076923
with extra	map extra	0.040000
queries so that :func awaitanytermination() can be	sql streaming query	0.011765
use only create a new hivecontext for testing	sql hive context create for testing cls	0.333333
ml params	params m1	0.047619
for feature selection by	chi sq selector set	0.166667
the year of a given date	sql year	0.050000
converts matrix columns in an	mllib mlutils convert matrix columns to ml	0.166667
python parammap	param map from	0.250000
value of	ml bisecting	0.133333
to an external database table via jdbc	jdbc url table	0.090909
:class dataframe to	frame writer save path	0.066667
set	density set	1.000000
be used again to wait	manager	0.011236
vector columns in an	vector columns to ml dataset	0.142857
an object is of type	type obj	0.333333
an javardd of object by unpickling	ml	0.001835
dump already partitioned data into	core	0.003021
the :class streamingquery streamingqueries active	streaming query manager	0.011236
performs the kolmogorov-smirnov ks test	stat statistics kolmogorov smirnov test	0.166667
is later than the value of the date	date dayofweek	0.333333
add a py or zip dependency for all	spark context add py file path	0.166667
registers	sql data frame register	1.000000
of months	months	0.125000
the behavior when data or table already	data frame writer mode savemode	0.071429
groups the :class dataframe using the specified	sql data frame group by	0.200000
contains a param with	ml param	0.009524
given a large dataset and an item approximately	ml lshmodel approx nearest neighbors dataset	0.166667
items for columns possibly	items cols	0.125000
with the dispatch to handle all	cloud pickler save	0.166667
the dataframe in a text	data frame writer text	0.200000
used	streaming query manager reset	0.011905
will convert each python object into java	rdd to java	0.333333
comprised of vectors containing i i	random rdds gamma vector	0.125000
predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01 maxiter=100	predictioncol k probabilitycol	0.333333
model from	power iteration clustering model	0.500000
test that	test training	0.500000
squared distance between two vectors	mllib linalg vectors squared distance v1 v2	1.000000
predictions which gives	generalized linear regression summary	0.090909
n smallest elements in a dataset	core nsmallest n iterable key	0.333333
test	tests test	0.481481
the kolmogorov-smirnov ks test for data sampled	mllib stat statistics kolmogorov smirnov test data distname	0.111111
how much of	core	0.003021
an input stream from an queue of rdds	streaming streaming context queue stream rdds	1.000000
returns weighted averaged recall	multiclass metrics weighted recall	1.000000
model that has a vector of	model	0.005587
multiple parameters passed as a list	core spark conf	0.055556
for which predictions are	ml isotonic regression model	0.125000
dot product of	dot other	0.050000
adds an output option for	writer option key value	1.000000
of users for a	users	0.066667
or compute the number	matrix num	0.088235
by	by key	0.026316
spark sink deployed	maxbatchsize	0.037037
hostname	hostname	1.000000
default value	default	0.222222
spark session to use	session sparksession	0.083333
squared distance from a sparsevector or	mllib linalg sparse vector squared distance other	0.166667
finding frequent items for columns possibly with	sql data frame freq items	0.166667
converts matrix columns in an	mllib mlutils convert matrix columns	0.166667
dataframe produced by the model's transform method	ml clustering summary predictions	0.500000
list of labels corresponding to indices to be	ml string indexer model labels	0.066667
of this	ml one vs	0.142857
this thread such as the spark	core spark	0.010309
distinct count of col or cols	count distinct	0.040000
by the given columns if	bucket by	0.200000
into a jvm seq	seq sc	0.055556
table named table accessible via jdbc url url	reader jdbc url table	0.166667
the residual degrees of freedom for the	summary residual degree of freedom	0.125000
of tables/views in	tables	0.071429
this udt	sql user defined	0.500000
computes column-wise summary statistics for the	stat statistics col	0.200000
from the population should be a rdd	mllib stat kernel density	0.066667
hashing	hashing	1.000000
comprised of vectors containing i i d	mllib random rdds normal	0.125000
rdd is checkpointed and materialized either reliably or	rdd is checkpointed	0.166667
that all	object size	0.032258
points belongs to in this model	mllib kmeans model predict x	0.333333
a left outer join	left outer join other	0.111111
the spark sink deployed on a	ssc addresses storagelevel maxbatchsize	0.045455
squared distance from a sparsevector or 1-dimensional	vector squared distance other	0.166667
saves the content of	mode partitionby	0.133333
isotonicregression	isotonic	0.125000
the correlation of two columns of	corr col1 col2 method	0.055556
two vectors we support	linalg	0.022222
param is explicitly set by user or has	ml param params is defined param	1.000000
sub-matrix blocks blockrowindex blockcolindex sub-matrix) that	linalg block matrix blocks	0.166667
with scipy sparse matrices if scipy is available	sci py tests	0.250000
parameters in this grid to fixed	builder add grid param	0.250000
the word2vec model's	mllib word2vec	0.125000
between date1 and date2	between date1 date2	1.000000
squared	vector squared	1.000000
to wait for new	sql	0.002525
string in the	s	0.071429
later than the value of the	dayofweek	0.037037
with single :class pyspark sql types longtype column	sql sqlcontext range start end step numpartitions	0.083333
model	ldamodel	0.034483
with a randomly	cross validator model	0.050000
wait for the execution	context await termination timeout	0.166667
the day of the month of a given	sql dayofmonth	0.031250
of type	ml ldamodel	0.111111
partial objects do	partial obj	0.125000
with	range between	0.166667
the current [[dataframe]] and perform the specified aggregation	sql grouped data pivot pivot_col values	0.050000
distinct elements in this rdd	core rdd distinct numpartitions	0.250000
buckets the output by the	writer bucket by numbuckets	0.200000
wait for the execution to stop return true	or timeout timeout	0.125000
:func awaitanytermination() can be	query	0.010753
but not in	subtract	0.111111
given value numbits right	right col numbits	0.500000
get or compute the	mllib linalg indexed row matrix	0.250000
the content of the :class dataframe in orc	data frame writer orc	0.200000
sets	split set	1.000000
optional default	params	0.006623
the given parameters in this grid to	ml param grid builder base	0.076923
converts vector columns in an input	convert vector columns to ml dataset	0.166667
convert to sparsematrix	linalg dense matrix to sparse	1.000000
set bandwidth of	kernel density set bandwidth bandwidth	0.142857
for the given key	key	0.035714
persist its values across operations after the	persist storagelevel	0.166667
sparkcontext is	core spark context ensure	1.000000
the deviance for the fitted model	ml generalized linear regression summary deviance	0.125000
default min number of partitions	default min partitions	0.250000
for params shared	params	0.006623
dot product of	dot	0.040000
or names into a jvm seq of	seq sc cols	0.055556
reported an	receivererror	0.142857
save	model save	0.400000
as spark executor memory	spark	0.013158
this	params has	0.019231
model fitted by :py class maxabsscaler	max abs scaler model	1.000000
a new dstream in which	streaming streaming	0.047619
field in :py attr predictions which gives	generalized linear regression summary	0.090909
converts matrix columns in an input dataframe to	convert matrix columns from	0.166667
outputted by the model's transform method	ml linear regression summary predictions	0.200000
that :func awaitanytermination() can be used again	sql streaming query	0.011765
a converter to	converter datatype	0.071429
use the model to make predictions on batches	mllib streaming linear algorithm predict on	0.066667
a new column of corresponding string values	string	0.041667
is	is distributed	0.200000
one	add	0.035714
point in rdd 'x' to all mixture	gaussian mixture model	0.052632
convert this matrix	linalg dense matrix	0.083333
setparams(self featurescol="features", labelcol="label",	logistic regression set params featurescol labelcol	1.000000
table named table accessible via jdbc url url	jdbc url table	0.090909
population should be a rdd	stat kernel density	0.200000
deviance for the fitted model	summary deviance	0.125000
checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none	random forest classifier	0.022727
value of	ml param has handle invalid	1.000000
add a py or	context add py	0.166667
with a given string	params has	0.019231
for data sampled	data	0.011628
python module	module cls	0.333333
__init__(self mindocfreq=0	ml idf init mindocfreq	1.000000
java onevsrest create and	one vs rest from java	0.142857
:param rdd an rdd of	cls rdd	0.250000
returns the	ml	0.001835
soundex encoding for a string >>>	sql soundex col	0.055556
can be used again to wait for new	streaming query manager reset	0.011905
vector of 64-bit floats from	ml linalg vectors	0.250000
for column of predicted clusters in predictions	ml clustering summary prediction col	0.111111
the deviance for the null	ml generalized linear regression summary null deviance	0.250000
for each numeric columns for each	sql grouped data avg	0.058824
vector columns in an	vector columns from	0.142857
compute the number	block matrix num	0.100000
be	query	0.010753
memory limit	limit	0.076923
creates a :class dataframe from	session create	0.058824
to fixed values	param values	0.500000
a term to this accumulator's value	add term	0.066667
dump already partitioned	core external group by spill	0.047619
values of a dstream and	values dstream	0.250000
name of the file	file	0.028571
:py attr mindocfreq	min doc freq value	1.000000
convert matrix attributes which	ml linalg matrix convert	0.166667
__init__(self estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none)	validator init estimator estimatorparammaps evaluator numfolds	1.000000
coordinate format	coordinate	0.100000
comprised of i	mllib random rdds	0.041667
model to the input dataset this is called	dataset	0.020408
saved using rdd saveastextfile	sc path minpartitions	0.250000
if the rdd contains no elements at all	core rdd is empty	0.083333
computes column-wise summary statistics for the input	mllib stat statistics	0.125000
train the model	train	0.333333
or compute the	mllib linalg indexed row matrix	0.250000
instance contains a param with	param params has param	0.019231
average values for each numeric columns for	grouped	0.035714
instance is of	ml ldamodel is distributed	0.066667
computes the area under the precision-recall	classification metrics area under pr	0.333333
a multi-dimensional rollup for	frame rollup	0.055556
sets mincount the minimum number of times	set min count mincount	0.250000
sets	param has elastic net param set	1.000000
of	ml alsmodel	0.111111
__init__(self mindocfreq=0 inputcol=none outputcol=none)	ml idf init mindocfreq inputcol outputcol	1.000000
given combine functions and a	core	0.003021
database table via	url table mode properties	0.200000
the name of the file to	file	0.028571
pipeline create and return	pipeline	0.052632
on a model	linear regression	0.040000
set	kernel density set	1.000000
the termination of this	termination timeout	0.041667
save a code	core cloud pickler save codeobject	1.000000
computes hex value of the given	sql hex col	0.166667
of the month of a	dayofmonth col	0.031250
wait	streaming streaming context await termination timeout	0.166667
with singular values in descending order	mllib linalg singular value decomposition s	0.250000
get the cluster	mllib bisecting kmeans model cluster	0.333333
names	sqlcontext table names	0.066667
the python direct kafka rdd api with leaders	kafka rdd with leaders	0.500000
function corresponding to the expected value of	ml	0.001835
the profile stats	profiles	0.250000
sets	regression evaluator set	1.000000
test statistic result	stat test result	0.166667
an external list	external	0.013889
runs and	core basic	0.066667
dot product of two vectors we support	ml linalg dense vector dot	0.090909
of weights is close to the desired	parameter accuracy	0.029412
an rdd that has no partitions or	core spark context empty rdd	0.200000
this model instance	ml linear regression model	0.166667
chisquared feature	chi sq	0.111111
of rows in	sql	0.005051
ignore	core ignore	0.500000
string value that match regexp	regexp	0.076923
data or	context get or	0.200000
how data	sql data stream	0.031250
the underlying output data source	sql data stream writer format source	0.333333
how much of memory for this	external	0.013889
specifies how data of	data	0.011628
over this dstream	streaming dstream	0.055556
with extra values from	extra	0.023810
squared distance from	vector squared distance other	0.166667
of indices	ml chi	0.100000
extract the week number of a	sql weekofyear col	0.055556
replaces a local temporary view	replace temp view name	0.333333
comprised	mllib random rdds log	0.125000
return the weights for each tree	ensemble model tree weights	0.333333
dstream by applying reducebykey to each	streaming dstream reduce by	0.076923
returns weighted averaged	mllib multiclass metrics weighted fmeasure	1.000000
on a spark	spark	0.013158
to an int	to int	0.250000
the pearson correlation coefficient for col1 and col2	corr col1 col2	0.500000
so that :func awaitanytermination() can be used again	reset	0.011236
list of columns	catalog list columns	0.166667
for approximate distinct count of	approx count distinct	0.071429
number of	set num	0.500000
transformfunc	transformfunc	1.000000
to the input dataset	dataset	0.061224
this :class dataframe	sql data frame	0.005348
this instance contains a param with a	ml param params has	0.019231
the :class dataframe	data frame	0.040000
parameters in this grid to fixed values	grid builder base	0.076923
computes column-wise summary statistics for the input rdd[vector]	mllib stat statistics	0.125000
top "num" number	num	0.016807
much of memory for	external	0.013889
setparams(self labelcol="label",	ml generalized linear regression set params labelcol	1.000000
parameters in this grid to fixed	grid builder	0.055556
:func awaitanytermination()	manager	0.011236
dataset	dataset	0.163265
elements from an rdd ordered in ascending order	rdd take ordered	0.050000
an indexedrowmatrix	indexed	0.142857
instance for params shared by them	params	0.006623
data in	data	0.011628
setparams(self min=0 0 max=1 0	ml min max scaler set params min max	1.000000
true if the table is currently cached in-memory	catalog is cached tablename	0.250000
generates python code for	code name	0.111111
trigger for the stream	data stream writer trigger	0.083333
based on	based on key	0.111111
:class dataframe representing the database table named	sql data frame	0.005348
initial value	logistic regression with sgd set initial	0.111111
of	core rdd	0.006920
of the importance of each feature	ml random forest classification model feature importances	0.250000
column scipy matrix from a dictionary of	mllib sci py tests scipy matrix size	0.090909
the termination of this query either	termination timeout	0.041667
on the driver returns	on driver	0.333333
with extra values from input into	extra	0.023810
withstd	withstd	1.000000
values for each key using an associative	by key	0.026316
densematrix >>> dm = densematrix(2 2 range 4	mllib linalg dense matrix repr	0.142857
generates an rdd comprised of vectors	random rdds exponential vector rdd sc mean	0.200000
to any hadoop file system using the new	as new	0.125000
the file to which this	file	0.028571
given combine functions and a	core rdd	0.003460
accumulator with a	accumulator	0.012987
find all globals names read or written to	pickler extract code globals	0.125000
a python topicandpartition to map to the java	partition init topic partition	0.055556
params	param params	0.029851
the specified table	tablename overwrite	0.333333
statement	statement	0.857143
from start to end exclusive increased by step	spark context range start end step	1.000000
data into disks	group by	0.041667
extract the week number	weekofyear col	0.055556
evaluates the	ml java evaluator evaluate dataset	0.333333
an iterator of deserialized objects from the	serializer load	0.083333
convert each python object into java	mllib to java	0.333333
converts matrix columns in an input	mllib mlutils convert matrix columns from ml	0.166667
the max value	max	0.071429
wait until any	streaming query manager await any termination timeout	0.166667
impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1	ml random forest classifier	0.023256
:func awaitanytermination() can be used again to wait	sql streaming	0.010204
changes the uid of this	ml param params reset uid	0.058824
the key-value	map values	0.166667
converts matrix columns	convert matrix columns	0.166667
length of a string	length col	0.050000
of iterations default 1 which should be smaller	iterations	0.043478
note :	approx	0.095238
sort order	sql data frame sort	0.250000
number of rows of blocks in the blockmatrix	mllib linalg block matrix num row blocks	1.000000
use only create a new hivecontext for testing	context create for testing cls	0.333333
the selector type of the chisqselector	chi sq selector set selector type selectortype	0.333333
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance",	tree regressor	0.058824
of the file to which	file	0.028571
a :class pandas dataframe	data frame data	0.142857
:func awaitanytermination() can be used again to wait	manager	0.011236
or transform the rdd of document	mllib hashing tf transform document	0.166667
convert a	sql to	0.083333
find the maximum item	max key	0.333333
params instances for	params m1	0.047619
libsvm format into an rdd of labeledpoint	load lib svmfile sc	0.125000
logistic regression	logistic regression model	0.083333
squared distance from	squared distance other	0.153846
persist its values across operations after the	rdd persist storagelevel	0.166667
sets	imputer set	1.000000
by the param grid	param grid	0.200000
this instance contains a param with a	has	0.011628
add a py or zip dependency for all	core spark context add py file	0.166667
value for each original column	original	0.047619
params	ml param params	0.013699
add a py or zip dependency for	add py file path	0.166667
value of each key-value pairs in this dstream	streaming dstream	0.055556
wait for	query manager reset	0.011905
:func query stop() or by an	sql streaming query await	0.333333
again to wait for new terminations	reset	0.011236
__init__(self withmean=false withstd=true	init withmean withstd	1.000000
so that :func awaitanytermination()	query manager	0.011905
computes average values for each numeric columns	grouped data	0.035714
code for a shared param class	ml param gen param code name	0.333333
how	how	0.833333
embedded params to the companion	transfer params to	0.333333
stream	data stream writer	0.041667
a row-oriented distributed matrix with indexed rows	indexed row matrix	0.333333
indexedrowmatrix	linalg indexed row matrix	0.250000
values for each numeric columns for each	sql grouped	0.043478
stratified sample without	frame sample	0.066667
extract the minutes of a given date as	sql minute col	0.050000
can be used again to wait	manager	0.011236
javaclassname	javaclassname	1.000000
the bisecting k-means algorithm	mllib bisecting kmeans train	0.500000
column of indices back to a new column	index to	0.040000
given string name	ml param params has param	0.019231
rdds in a sliding window over	value and window windowduration slideduration	0.076923
the dependent variable given a vector or	mllib linear regression model base	0.200000
model that has a vector of coefficients and	model	0.005587
obj assume that all the objects	core external merger object size obj	0.040000
the sum for each numeric	sql grouped data sum	0.083333
recordlength	recordlength	1.000000
at pos in byte and is of length	pos	0.022222
wait until any of the queries on the	await any termination timeout	0.166667
set pipeline stages	pipeline set stages value	1.000000
casesensitive=false) sets	ml stop words remover set	0.333333
matrix to the new mllib-local representation	linalg dense matrix as ml	0.333333
broadcast a read-only variable to the cluster	broadcast value	0.125000
the week number of a given date as	weekofyear col	0.055556
saves	path format mode partitionby	0.200000
range of offsets from a	range	0.030303
the kolmogorov-smirnov ks test for data	stat statistics kolmogorov smirnov test data	0.111111
fit test of the observed data	test observed	0.090909
value that match regexp	sql regexp	0.125000
an external database table via	url table	0.200000
of vectors containing i i d samples drawn	numrows numcols numpartitions	0.125000
demonstrate udt in only python	python only point	1.000000
format into an rdd of labeledpoint	mlutils load lib svmfile sc	0.125000
bandwidth of each sample defaults to 1	bandwidth bandwidth	0.125000
which	ml isotonic regression	0.111111
set the trigger for the	trigger	0.071429
the accumulator's value only usable in	accumulator value	0.050000
a :class datatype the data type string format	datatype string s	0.111111
the length of a string or binary expression	length col	0.050000
local	get local	0.333333
of each class as	ml	0.001835
private abstract class representing a multiclass classification	classification	0.071429
globals names read or written to by	extract code globals	0.125000
:class dataframe representing the	sqlquery	0.027027
generates an rdd comprised of vectors containing i	random rdds exponential vector rdd sc mean	0.200000
receiver operating characteristic roc	roc	0.200000
so	reset	0.011236
converts vector columns in an	mllib mlutils convert vector columns to ml	0.166667
the :class dataframe using the specified columns	data frame	0.005000
wait	sql streaming query manager	0.011905
the partitions	partitions	0.066667
as the square root	root	0.071429
the rdd partitioned	rdd partition	0.062500
trigger for the stream query if this	stream writer trigger	0.083333
convert matrix attributes which are array-like	ml linalg matrix convert	0.166667
in sql statements	returntype	0.071429
buffer	array_like dtype	0.166667
norm of a	norm p	0.055556
be used with the spark sink deployed on	storagelevel maxbatchsize	0.045455
transpose this coordinatematrix	mllib linalg coordinate matrix transpose	1.000000
returns	session sql	0.250000
impurity="variance",	tree regressor	0.058824
the n elements from an rdd	rdd	0.003058
the libsvm format into an rdd of labeledpoint	load lib svmfile sc path numfeatures minpartitions	0.125000
broadcast a read-only variable	spark context broadcast value	0.125000
computes column-wise summary statistics for the	mllib stat statistics col	0.200000
in iterator	iterator key reverse	0.200000
create an input stream that	utils create stream ssc hostname	0.200000
probability	prior	0.166667
a new dstream by applying a function to	f	0.010526
returns the greatest value of the	greatest	0.043478
curve which is a dataframe having two	binary logistic regression	0.142857
content of the :class dataframe	sql data frame writer	0.058140
which is a risk function corresponding	regression summary	0.035714
that	query manager	0.011905
train a naive bayes model given an	mllib naive bayes train cls data lambda_	0.500000
make predictions on batches of data from	streaming linear algorithm predict on	0.066667
deserialized batches lists of objects from the	serializer load stream without unbatching	0.200000
dstream and other	other numpartitions	0.083333
keyed	values	0.050000
generates an rdd comprised of	mllib random rdds exponential vector rdd sc mean	0.200000
fitted model by type	generalized linear regression	0.090909
note : experimental	core rdd count approx timeout confidence	1.000000
dot product	vector dot	0.100000
until any of the queries on the	sql streaming query manager await any	0.142857
number of	num	0.168067
rescale each feature individually to a common range	max scaler	0.200000
gaussian distributions as a	gaussian mixture model gaussians df	0.166667
values for each numeric columns for each group	sql grouped	0.043478
dump already partitioned data	core external group by spill	0.047619
param with a given string	param params has	0.019231
the sum of	ml bisecting kmeans	0.062500
until any of the queries on the	await any	0.142857
the deviance for	linear regression summary deviance	0.125000
values alias for na fill()	frame fillna value subset	0.166667
a given string	params	0.006623
month of a	dayofmonth col	0.031250
a sliding window of time over	window	0.037037
performs the kolmogorov-smirnov ks test for data	mllib stat statistics kolmogorov smirnov test data	0.111111
deviation	std	0.142857
returns an mlwriter instance for	ml mlwritable write	0.200000
value and parent	ml persistence test	1.000000
a multi-dimensional cube for the	data frame cube	0.055556
tree	tree model	0.052632
specifies the input data source format	data stream reader format source	0.333333
a shortcut of write() save	ml pipeline model save	0.166667
sets	set key	0.500000
wait for the execution to stop	context await termination or timeout timeout	0.125000
test a	tests test	0.111111
create a multi-dimensional cube for	sql data frame cube	0.055556
that all the	external merger	0.031250
kmeans algorithm for fitting and	streaming kmeans	0.035714
of memory for this	size	0.009174
a new dstream in	streaming streaming context	0.032258
of columns that make up each block	block matrix cols per block	0.333333
class dataframe that with new specified column	data frame to df	0.090909
every feature	model	0.005587
left outer join of c{self}	left outer join	0.111111
this instance contains a param with	ml param	0.009524
create a java array of	ml java wrapper new java array	0.333333
terminations	manager	0.011236
with this spark job	core spark context	0.011628
objects	core	0.003021
loads orc files returning the result as	reader orc path	0.200000
can	sql streaming query	0.011765
how much of memory for this	core external merger object size	0.032258
return a jvm scala map from a dict	sql data frame jmap jm	0.111111
this instance's params to the wrapped	java params to	0.045455
tree ensemble model	tree ensemble model	0.038462
converts matrix columns in an	mlutils convert matrix columns to ml	0.166667
outer join of	outer join other	0.333333
this instance contains a	ml param	0.009524
dump already partitioned	external group by spill	0.047619
the param grid	param grid	0.200000
a group id to all the jobs started	job	0.023810
saving	ml java mlwriter	0.200000
parameters in this grid to	param grid builder	0.055556
deviance for the	generalized linear regression summary deviance	0.125000
note : experimental	bucketed random projection lshmodel	1.000000
find synonyms	word2vec model find synonyms	0.333333
dataframe representing the result of the given query	sqlquery	0.054054
of	merger	0.025641
thread such as the spark	spark	0.013158
with a randomly generated	validation split	0.200000
the column	model	0.005587
compute the dot product of two vectors we	linalg dense vector dot other	0.058824
multi-dimensional cube for	data frame cube	0.055556
calculates the correlation of two columns	method	0.041667
with arbitrary key and value class	core spark context	0.023256
optional	param params	0.014925
the dot product of two	linalg dense vector dot	0.058824
to the same time of day in	to	0.007692
featurescol features	features	0.043478
greatest value of the list of column	greatest	0.043478
until any	sql streaming query manager await any	0.142857
of number of	ml clustering	0.100000
commutative reduce function	reduce	0.083333
string format	string	0.041667
returns an active query	sql	0.002525
the given user	user	0.055556
function in	function	0.027778
stop the underlying :class sparkcontext	spark session stop	1.000000
awaitanytermination() can be	sql	0.002525
output a python rdd of key-value pairs	rdd save as	0.038462
calculates the length of a string or binary	sql length	0.050000
already	by	0.014286
a column of the current [[dataframe]] and	sql grouped data pivot pivot_col values	0.050000
internal function to get or create global taskcontext	core task context get or create	0.250000
latest model	streaming kmeans latest model	1.000000
this accumulator's value	core	0.003021
contents of the :class dataframe to a data	sql data frame writer save path format	0.142857
set bandwidth	set bandwidth bandwidth	0.142857
:py attr scalingvec	scaling vec value	1.000000
to persist its values across operations after	core rdd persist storagelevel	0.166667
can be	query manager reset	0.011905
to wait for new	sql streaming query manager	0.011905
collect the distributed matrix on the driver as	matrix to local matrix	0.250000
returns an mlwriter instance for	ml java mlwritable write	0.200000
drawn	std numrows	0.125000
return an javardd of object by	ml	0.001835
user defined function udf	sql udf f returntype	0.200000
for the stream query	sql data stream writer	0.041667
into the returned	spark streaming test case	0.333333
from the input java	java estimator	0.200000
dispatch to handle all function types	core cloud pickler save function	0.142857
to the wrapped java	to	0.007692
the content of the non-streaming	write	0.071429
decode the unicode as utf-8	streaming utf8 decoder s	1.000000
a	ml param	0.019048
comprised of vectors containing i	random rdds exponential vector	0.125000
dump already partitioned data into disks	external group	0.045455
or replaces a local temporary view with	or replace temp view name	0.333333
model for prediction tasks regression	prediction model	0.333333
to the number	num	0.008403
a randomly generated	train validation split model	0.166667
wait for	streaming context await termination timeout	0.166667
null values	sql	0.005051
the values for each key using an associative	by key func numpartitions	0.062500
query that is executing continuously in	streaming query	0.010526
ml params instances for the given	params m1 m2	0.047619
to	accumulator	0.012987
an rdd ordered	rdd take ordered	0.050000
labelcol="label", featurescol="features", predictioncol="prediction",	labelcol featurescol predictioncol	1.000000
default implementation of	ml estimator	0.125000
to save	pickler save	1.000000
latest	streaming kmeans latest	1.000000
minsupport	min support	1.000000
points using the model	model	0.005587
true positive rate for a given label	true positive rate label	1.000000
calculates the md5 digest and	md5	0.125000
every	model	0.005587
be used again to wait for new	query manager	0.011905
the dot product of two vectors we support	linalg dense vector dot other	0.058824
mean variance and count of the rdd's elements	rdd	0.003058
method for given binary operator	sql bin op name doc	0.200000
much	external merger object	0.032258
local property that affects jobs submitted from	local property key value	0.076923
pass else fail with	streaming test case eventually	0.500000
the number	mllib linalg row matrix num	0.062500
converts matrix columns in an input	convert matrix columns to ml	0.166667
new profiler using class profiler_cls	profiler collector new profiler ctx	0.333333
default min	default min	1.000000
until any	any termination	0.142857
right outer join of c{self}	full outer join other numpartitions	0.111111
join	join	0.275862
this model	ml gaussian mixture model	0.500000
is generated by applying mappartitionswithindex() to each rdds	map partitions with index f preservespartitioning	0.055556
right outer join of c{self} and	core rdd full outer join other numpartitions	0.200000
multi-dimensional rollup for the	sql data frame rollup	0.055556
predicting on incoming dstreams	streaming	0.005025
similarities between columns of	column similarities threshold	0.333333
:class dataframe with the default storage level (c{memory_and_disk})	sql data frame cache	1.000000
contents of the :class dataframe to a data	sql data frame	0.005348
of words to their vector representations	mllib word2vec model get vectors	0.166667
parses a column containing a json string	sql from json col	0.083333
to a mllib	to	0.007692
adds a term to this	core accumulator add term	0.066667
maxheap version of	max heap	0.200000
prefix of string in doc tests to	prefix f	0.142857
binary or multiclass classification	numclasses categoricalfeaturesinfo	0.250000
computes the levenshtein distance of the	sql levenshtein left right	0.058824
one or more examples	mllib decision tree model	0.076923
given parameters in this grid	grid builder add grid	0.100000
file with	file	0.028571
basic operation test for dstream map	streaming basic operation tests test map	0.333333
and c{other}	join other	0.071429
threshold	threshold distcol	0.500000
wait for	timeout timeout	0.125000
to all mixture components	mixture model predict soft	0.142857
param with a	has	0.011628
seed=none numtrees=20	random forest	0.041667
onevsrest create and	one vs rest from	0.142857
training set given the current parameter	distributed ldamodel training	0.034483
a dataframe that stores	ml alsmodel	0.222222
with two fields threshold precision curve	ml binary logistic regression summary precision by threshold	0.166667
the content of the :class dataframe in parquet	data frame writer parquet	0.200000
of col1	col1	0.111111
converts matrix columns in an input dataframe from	convert matrix columns	0.083333
this model instance	generalized linear regression model	0.200000
vector columns in	vector columns to	0.142857
read a 'new api' hadoop	spark context new apihadoop	0.333333
two columns of a dataframe	data frame corr	0.166667
returns the greatest value of the list of	greatest	0.043478
create a method for given unary operator	sql unary op name doc	0.200000
merge the values for each key using	by key func	0.062500
ws	ws	1.000000
a given string	param params has param	0.019231
norm of	mllib linalg sparse vector norm p	0.083333
based	path	0.010204
and count	core	0.003021
wait for the execution	streaming context await termination timeout	0.166667
arbitrary key and value class	core spark	0.020619
to a query that is executing continuously in	streaming query	0.010526
finding frequent items for columns possibly	frame freq items	0.166667
this obj assume that	core external merger object size obj	0.040000
instance contains	param params	0.014925
number of partitions for	partitions	0.066667
convert this vector to the new mllib-local representation	linalg sparse vector as	0.333333
point in rdd 'x' to all mixture	mllib gaussian mixture model predict	0.100000
java object	from java	0.111111
standard deviation of this rdd's	stdev	0.047619
dot product of two vectors we support	linalg dense vector dot other	0.058824
parammap into	map to	0.125000
boundary start inclusive	start	0.045455
setparams(self min=0 0 max=1 0 inputcol=none outputcol=none)	max scaler set params min max inputcol outputcol	1.000000
squared distance from a sparsevector	ml linalg sparse vector squared distance	0.166667
property	key value	0.200000
train a random forest model	mllib random forest train classifier cls data	0.250000
the left singular vectors of	singular	0.015625
"predictions" which gives the probability	logistic regression summary probability	0.333333
uid	params reset uid	0.333333
__init__(self labelcol="label", featurescol="features",	ml generalized linear regression init labelcol featurescol	1.000000
return the	mllib standard scaler model	0.100000
returns weighted false	mllib multiclass metrics weighted false	1.000000
the sort order	frame sort	0.250000
awaitanytermination() can	query manager	0.011905
mixture	mllib gaussian mixture model k	0.200000
the stream query	stream	0.017544
transfer this instance's params to the	java params to	0.045455
onevsrest create and return a python wrapper of	ml one vs rest from	1.000000
list of tables/views in the	list tables	0.250000
are the left singular	singular	0.015625
add	core spark context add	0.500000
either by :func query stop() or by an	sql streaming query await	0.333333
the dot product of two vectors	dense vector dot other	0.050000
the stream query if this is not	sql data stream	0.031250
for this	ml other	0.500000
new dstream by applying reducebykey to each rdd	streaming dstream reduce by key func numpartitions	0.076923
given string name	ml param params	0.013699
function	values f	0.125000
a dataframe that stores item	ml alsmodel item	0.250000
be used again	sql streaming query manager reset	0.011905
a py	py file path	0.066667
squared	mllib linalg sparse vector squared	1.000000
as the specified table	save as table	1.000000
dstreams	transform dstreams transformfunc	0.125000
so that	streaming	0.005025
ignore separators inside brackets pairs	sql ignore brackets split	0.250000
given a large dataset and an item approximately	lshmodel approx nearest neighbors dataset key	0.166667
labeledpoint	mllib mlutils load lib svmfile sc	0.125000
spark fair	core spark context	0.011628
tree <http //en wikipedia	tree classifier	0.500000
columns are the right singular	mllib linalg singular	0.017544
profiling on	profiler	0.090909
compare 2 ml params instances for the given	compare params m1	0.200000
generated by applying mappartitionswithindex() to each	map partitions with index f preservespartitioning	0.055556
training	logistic regression training summary	0.500000
to wait for new terminations	query manager reset	0.011905
dummy params instance used as a placeholder	params dummy	0.111111
model on toy	on model	0.166667
generate	with sgdtests generate logistic input	1.000000
rdd of key-value pairs (of form c{rdd[	core rdd	0.010381
perform a left	left	0.066667
back	index	0.041667
thresholds thresholds	thresholds	0.071429
calculates the norm	sparse vector norm	0.133333
the maximum item in this rdd	core rdd max key	0.333333
:func awaitanytermination() can be used	streaming query manager	0.011236
loads parquet files returning the result	reader parquet	0.200000
that :func awaitanytermination() can be used again	streaming query	0.010526
of labels corresponding to indices to	ml string indexer model labels	0.066667
an fp-growth model that contains	mllib fpgrowth train cls	0.100000
of columns that describes the sort order	sort cols cols kwargs	0.142857
can be used again to wait for new	sql streaming	0.010204
which is a	regression summary	0.071429
cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1 0)	ml random forest classifier	0.023256
comprised of vectors containing i i d	random rdds gamma	0.125000
of all active stages	status tracker get active	0.333333
the sort order	data frame sort	0.125000
uid of this instance this updates	ml param params reset uid newuid	0.058824
a java	java	0.121951
copy of this instance	ml pipeline model copy extra	0.333333
matrix columns in	matrix columns to ml dataset	0.142857
sparse vector using either a dictionary a	linalg vectors sparse size	0.166667
an rdd comprised of vectors containing i i	random rdds log normal vector rdd	0.166667
range 4	mllib linalg dense matrix repr	0.142857
new dstream in	streaming streaming context transform	0.066667
values for each numeric	sql grouped	0.043478
dump already	group by spill	0.047619
drop the names of fields in obj	sql	0.002525
the selector type	chi sq selector set selector type	0.111111
finding frequent items for	freq items	0.166667
which predictions	ml isotonic regression model	0.125000
:func awaitanytermination() can be	sql streaming query	0.011765
right outer join	rdd full outer join other	0.111111
to list of ints if possible	ml param type converters to list int value	0.333333
partial objects	partial obj	0.125000
wait for the execution to	await termination or timeout timeout	0.125000
calculates the norm	vector norm	0.166667
restore an object of namedtuple	core restore	0.333333
to the given path a shortcut of write()	ml pipeline model	0.066667
or transform the rdd of document to	tf transform document	0.166667
column or names into a jvm seq	seq sc cols	0.055556
then merges them with extra values from	extra	0.023810
residuals label - predicted value	linear regression summary residuals	0.500000
resulting rdd that contains a tuple	rdd cogroup other	0.066667
a matrix from the new mllib-local representation	mllib linalg matrices from ml mat	0.333333
content of the :class dataframe to	data frame writer	0.014085
creates a new sqlcontext	sql sqlcontext init sparkcontext sparksession	1.000000
:class dataframe to	frame writer	0.050000
of determination	mllib regression metrics r2	0.166667
point to programming spark with	spark session	0.100000
new hivecontext for testing	for testing	0.333333
a param with a given string name	params	0.006623
the	standard scaler	0.076923
convert the vector into an numpy ndarray	ml linalg vector to array	1.000000
hadoop file system using the new	as new	0.125000
a paired rdd where the	matrix factorization	0.040000
a pearson's independence test using dataset	chi square test test dataset featurescol	0.333333
note : experimental	generalized linear regression summary	0.090909
return the	scaler	0.052632
:class dataframe in orc	data frame writer orc	0.200000
labeledpoint	lib svmfile sc path numfeatures	0.125000
finding frequent items for columns possibly with false	freq items cols support	0.166667
optional default value	params	0.006623
load a model from	load cls sc	0.214286
comprised of	random rdds log normal	0.125000
forget about past terminated queries so that :func	terminated	0.100000
partial objects do not serialize	save partial obj	0.125000
create a new hivecontext for	sql hive context create for	0.250000
is set	spark context set	0.166667
condition to pass else fail with an	mllib mllib streaming test case eventually condition	0.333333
gaussian distributions as a dataframe	ml gaussian mixture model gaussians df	0.166667
setparams(self	ml aftsurvival regression set params	1.000000
vertical	vertical	1.000000
mixin for param confidence	has confidence	1.000000
that :func awaitanytermination() can be used again to	streaming query	0.010526
memory for	merger object size	0.032258
generates an rdd	normal vector rdd sc mean	1.000000
line in libsvm format into label indices	mlutils parse libsvm line line multiclass	0.111111
versionadded : 0 9 0	svmwith sgd	0.333333
of column names skipping null	sql	0.005051
confidence column name	col	0.016393
in libsvm format	libsvm	0.181818
gaussian	gaussian	0.900000
an rdd of labeledpoint	lib svmfile sc path	0.125000
fast version of a heappush followed	heappushpop heap	0.142857
to	manager	0.011236
use the model to make predictions on	predict on	0.117647
into	to	0.007692
attr lda	ml distributed ldamodel	0.050000
the specified	tablename	0.043478
matrix columns in an input dataframe	matrix columns to	0.142857
document	document	0.240000
a new java	java wrapper new java	0.166667
wait for the execution to stop return	or timeout timeout	0.125000
and count	core rdd	0.003460
java array	java array	0.333333
limits the result count	limit	0.076923
a python topicandpartition to map	and partition init topic partition	0.055556
length of a string or	sql length col	0.050000
root directory that contains	root directory cls	0.333333
randomly generated uid	cross validator	0.045455
model fitted by kmeans	kmeans model	0.090909
the norm	dense vector norm p	0.333333
comprised of vectors containing i i d	random rdds poisson	0.125000
value numbits	col numbits	1.000000
list of	sql	0.005051
:class dataframe as pandas pandas	to pandas	0.166667
sample without replacement based on the fraction	data frame sample	0.066667
a column containing a json string into	from json col	0.083333
naive	naive	1.000000
block matrix other from this block matrix	linalg block matrix	0.052632
that :func awaitanytermination() can be	streaming	0.005025
or buffer to array	to array array_like dtype	0.166667
string str	str	0.090909
function corresponding to the expected value of	ml linear	0.066667
start to	start	0.045455
return the column mean values	mllib standard scaler model mean	0.125000
accumulator's data type returning a new	accumulator	0.012987
function with	f	0.010526
save	save sc	1.000000
new dstream in which each rdd is	streaming streaming context transform	0.066667
batch	batch	0.241379
window	window	0.407407
in :py attr predictions which gives the	linear regression summary	0.013889
a densevector with singular values in descending order	mllib linalg singular value decomposition s	0.250000
daemon and workers terminate on	core daemon tests test termination	0.166667
transforms	transformer transform	0.166667
setparams(self	ml bisecting kmeans set params	1.000000
to any hadoop file system using the l{org	as sequence file path	0.500000
that daemon and workers terminate on sigterm	core daemon tests test termination sigterm	0.333333
number of rows	mllib linalg row matrix num rows	0.200000
:class randomforestregressor	random forest regression	1.000000
a flume	streaming flume	0.111111
weights of layers	ml multilayer perceptron classification model weights	0.500000
the input data source format	data stream reader format source	0.333333
create an input stream that	utils create stream ssc	0.200000
model to a local representation	ldamodel to local	0.200000
table in the	table	0.031250
the cluster	cluster	0.142857
vector columns in	vector columns from ml dataset	0.142857
this obj assume that all	core external merger object size obj	0.040000
range 4	linalg dense matrix repr	0.142857
sc	sc	0.312500
a column scipy matrix from a dictionary of	mllib sci py tests scipy matrix size	0.090909
compute the dot product of	mllib linalg dense vector dot other	0.058824
calculates the norm of a	mllib linalg sparse vector norm	0.083333
ranking	ranking	0.875000
as	writer save as	0.333333
of this instance with a randomly	ml one vs rest model	0.111111
a dataframe	data frame corr	0.166667
predicts rating	matrix factorization model predict	0.250000
dstream in which each rdd contains the count	count by	0.100000
named options filter out those	opts schema	0.250000
used	sql streaming query manager	0.011905
that has exactly	coalesce	0.142857
load an isotonicregressionmodel	mllib isotonic regression model load cls sc	1.000000
:py attr lda keeplastcheckpoint is set	ml distributed ldamodel	0.050000
the dot product of two vectors we support	dense vector dot other	0.050000
value of	ml elementwise	1.000000
or multiclass classification	numclasses categoricalfeaturesinfo	0.250000
and return a python wrapper of	ml	0.007339
convert this matrix to a rowmatrix	row matrix to row matrix	0.333333
a py or zip dependency	py file	0.066667
data	by spill	0.047619
each rdds	dstream n	0.333333
a given string name	params	0.006623
returns weighted averaged	metrics weighted	1.000000
setparams(self estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none):	estimator estimatorparammaps evaluator numfolds	0.200000
profiler	profiler	0.545455
applying a function	f	0.031579
columns in an input dataframe to	columns from ml dataset	0.125000
jdbc	jdbc	0.857143
starttime	starttime	1.000000
returns the :class statcounter members as	stat counter as	0.333333
a class inherit documentation from	inherit doc cls	0.045455
the input dataset this is	dataset	0.020408
fields in	sql	0.002525
awaitanytermination() can be used again	streaming query manager	0.011236
by the param	param	0.006250
replacing a	replace to_replace	0.200000
the content of the dataframe in	data frame	0.005000
function to	map f	0.074074
load a model from the given	matrix factorization model load cls sc	0.333333
instance contains	has param	0.019231
context to use for loading	ml java mlreader context	1.000000
values for each numeric columns	grouped data avg	0.058824
the	external	0.013889
param grid	param grid	0.200000
generates an rdd comprised of vectors containing	random rdds exponential vector rdd sc mean	0.200000
next memory limit if	next limit	0.200000
data	spill	0.038462
base-2 logarithm of the	log2 col	0.250000
awaitanytermination() can be used again to	sql	0.002525
which each	by	0.014286
computes column-wise summary statistics for	stat statistics col stats	0.200000
dumps	dumps	1.000000
for the stream	sql data stream writer	0.041667
calculates the norm of	vector norm p	0.055556
frequency vectors or transform	mllib hashing tf transform	0.045455
fits a java model	java estimator fit java	0.333333
functions registered in the specified database	functions dbname	0.500000
prints the first n rows to the console	sql data frame show n	0.333333
of this instance with a randomly generated	ml one vs rest model	0.111111
extract the year of a	year col	0.050000
least value of	sql least	0.055556
info	info	0.857143
end exclusive increased by	end	0.066667
setparams(self inverse=false inputcol=none outputcol=none) sets params for	set params inverse inputcol outputcol	0.333333
parameters in this grid	ml param grid builder add grid	0.100000
how much of memory for this obj assume	obj	0.023810
squared distance from a	squared distance other	0.153846
the spark sink deployed	ssc addresses storagelevel maxbatchsize	0.045455
seed=none): sets	set	0.005917
partial objects	partial	0.076923
represents a range of	range	0.030303
each of the points belongs	predict x	0.033898
a param with a given	param params has param	0.019231
in "predictions" which gives the predicted	linear regression summary prediction col	0.142857
dataframe produced	clustering	0.066667
which predictions	regression model	0.031250
partitioned data	external group by	0.045455
from	sql from	0.500000
this matrix to the new mllib-local representation	mllib linalg dense matrix as	0.333333
a param with a given	has	0.011628
a subclass of params	params	0.013245
passed as a list of	core spark conf set	0.100000
create a new sparkcontext at least the master	spark context init master	0.333333
[[arraytype]] of [[structtype]]s with the specified schema	schema options	0.125000
the spark_home	spark home	0.500000
of each key-value pairs in this dstream	streaming dstream	0.055556
to wait	query	0.010753
java onevsrestmodel create	one vs rest model from java	0.142857
vector columns in an input	vector columns to ml	0.142857
points using the model trained	mllib logistic regression model	0.083333
a	core rdd	0.003460
the	scaler	0.052632
objective function scaled loss + regularization at each	objective history	0.500000
ids of	stage ids	0.055556
tree including	tree model	0.026316
maxabsscaler	max abs	0.500000
word	word num	0.333333
model	regression model	0.031250
this instance is	is distributed	0.200000
samp	samp	1.000000
a model with weights	linear regression with	0.111111
2 ml params instances for the	params m1	0.047619
that :func awaitanytermination() can be used again	sql streaming	0.010204
can be used again to wait for	query manager	0.011905
contains a param with a given string name	has	0.011628
local property set in this thread	get local property key	0.066667
value of	ml multilayer perceptron classifier	0.571429
line in libsvm format into	mlutils parse libsvm line line multiclass	0.111111
featurescol	features col	0.250000
finding frequent items	sql data frame freq items cols support	0.166667
sql	sqlcontext	0.153846
a converter to drop the names of	converter datatype	0.071429
of names of tables in the	table names	0.066667
internal use only create a	create	0.017241
based on a	core spark context	0.011628
deprecated in 2 1 use approx_count_distinct instead	sql approx count distinct col rsd	0.333333
column of :class pyspark sql types stringtype or	sql to timestamp col format	1.000000
save this model to the given	mllib naive bayes model save sc	1.000000
param with a given string	has param	0.019231
note : experimental	clustering summary	0.500000
a test statistic result	mllib stat test result	0.166667
:class column for approximate distinct count of	approx count distinct col	0.071429
set a	set	0.011834
and	core rdd	0.013841
a lower bound on the log likelihood	log likelihood	0.125000
java object	java params from java	0.333333
data into	by spill	0.047619
broadcast a read-only variable to the	broadcast	0.052632
specifies some hint on the current dataframe	sql data frame hint name	1.000000
of the file to which this	file	0.028571
submit and test a	submit tests test	0.285714
:class dataframe in json	sql data frame writer	0.011628
rdd returns	sql spark session	0.166667
elements from an rdd ordered in	core rdd take ordered	0.050000
of conditions and returns one of multiple possible	sql column otherwise value	0.050000
of functions registered in the	functions	0.071429
combinebykey	combine by key	1.000000
limits the result count to the	limit	0.076923
a python topicandpartition to map to the	partition init topic partition	0.055556
the result as a :class dataframe	data	0.011628
partial objects do not serialize	pickler save partial	0.125000
terms to term frequency vectors or	mllib hashing tf	0.125000
returns a paired rdd where	matrix factorization model	0.043478
a column containing a json	json col	0.083333
the given parameters in this grid	ml param grid builder base on	0.076923
__init__(self featurescol="features",	ml linear regression init featurescol	1.000000
rest	rest	1.000000
nodes in tree including	tree model	0.026316
the topics	topics	0.125000
python direct kafka stream api with start	kafka direct stream from	0.125000
of parameters specified by the param grid	ml param grid	1.000000
least value of the list of column	least	0.043478
queries so that :func awaitanytermination() can be	sql streaming query manager reset	0.011905
to this accumulator's	core accumulator	0.030303
matrix stored	matrix	0.015152
return the	mllib	0.031579
densevector with singular values in descending order	linalg singular value decomposition s	0.250000
specified table or view as a :class	sqlcontext table tablename	0.142857
to all mixture	mixture model predict	0.125000
k a confidence column name	col	0.016393
dataframe	data frame corr col1 col2	0.166667
of vectors	ml vector indexer model	0.250000
aggregate the values of	aggregate	0.111111
an rdd comprised	mllib random rdds poisson vector rdd	0.166667
:class	regression	0.040000
2 1 use approx_count_distinct instead	approx count distinct col rsd	0.066667
the model	ml generalized linear regression model	0.166667
scalingvec	scaling vec	1.000000
of each	ml	0.009174
optional default value and	params	0.006623
new spark	spark	0.013158
of the rdd's	rdd	0.003058
return a copy of	core	0.003021
queries so that :func awaitanytermination() can	reset	0.011236
this model	mllib kmeans model	0.250000
or compute the number of	mllib linalg row matrix num	0.125000
load labeled points	mllib mlutils load labeled points sc path	1.000000
test that the model predicts correctly on toy	streaming kmeans test test predict on model	0.500000
for distinct	distinct	0.055556
a receiver has reported an error	receiver error receivererror	0.500000
parammap into a java	map to java	0.250000
field in "predictions" which gives the probability of	ml logistic regression summary probability	0.166667
key-value pairs	pairs	0.142857
comprised of vectors containing i i d	mllib random rdds log normal	0.125000
impurity="gini", numtrees=20	random forest classifier	0.045455
stop the underlying :class sparkcontext	sql spark session stop	1.000000
this instance with a randomly generated uid and	one	0.058824
convert this	mllib linalg	0.052632
average values for each numeric columns for each	sql grouped data avg	0.058824
an input stream that	stream ssc hostname port storagelevel	0.200000
the format	format	0.111111
train the model on the incoming	mllib streaming logistic regression with sgd train on	0.333333
saved	path minpartitions	0.250000
transfer this instance to	to	0.015385
with scipy sparse matrices if scipy is available	sci py	0.500000
can be used again to wait	streaming	0.005025
a given	params has param	0.019231
of predicted ratings for input user and product	mllib matrix factorization model predict all user_product	0.050000
contains a param with a given string name	ml	0.001835
jvm seq of	seq sc cols converter	0.055556
called by a worker process after the fork()	core worker sock	0.500000
data sampled from a continuous distribution	data distname	0.083333
that starts at pos in byte	pos	0.022222
awaitanytermination() can be used again to	streaming query manager reset	0.011905
query that	query	0.010753
beta	beta	0.833333
the centroids of that particular batch	timeunit	0.025641
vectors or transform the rdd of	mllib hashing tf transform	0.045455
model trained on the	model	0.005587
into a jvm seq	seq sc cols converter	0.055556
the rdd's elements in one operation	rdd	0.003058
setparams(self numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")	discretizer set params numbuckets inputcol outputcol	1.000000
return a resulting rdd that contains a tuple	rdd cogroup	0.066667
given timezone returns	sql	0.002525
content of the :class dataframe in parquet	sql data frame writer parquet	0.200000
converts	to	0.007692
get the offsetrange of specific kafkardd	kafka rdd offset ranges	0.333333
driver returns	driver	0.090909
mean variance and count	core rdd	0.003460
fits a model to	ml estimator fit	0.083333
how	object	0.027778
the rdd of document to	document	0.040000
onevsrestmodel create and return	one vs rest model from	0.142857
this broadcast on the	core broadcast	0.200000
__init__(self inputcol=none outputcol=none	vector slicer init inputcol outputcol	1.000000
a left outer join of c{self} and	core rdd left outer join other	0.200000
using an associative and commutative reduce function	rdd reduce by	0.200000
adds a	accumulator	0.012987
this rdd as a temporary table using the	temp table	0.500000
validates the block	linalg block	0.076923
cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	ml decision tree regressor	0.066667
into a jvm seq of column	seq sc cols converter	0.055556
awaitanytermination() can be	reset	0.011236
stream query if this is	stream writer	0.041667
generates python code for a shared param class	ml param gen param code	0.333333
this instance to	to	0.015385
column containing a json string into a [[structtype]]	sql from json col	0.083333
internal use only create a new hivecontext for	create for	0.250000
the month of	dayofmonth col	0.031250
accumulator's data type	accumulator param	0.038462
standardscaler	ml standard scaler	0.200000
return the first	streaming	0.005025
the initial value of weights	initial weights	0.250000
init	init	0.106383
return sparkcontext which is	streaming streaming context spark	0.083333
is vector conduct pearson's chi-squared goodness	stat statistics chi sq	0.066667
:class dataframe a	sql data frame	0.005348
converts matrix columns	convert matrix columns from ml dataset	0.166667
returns a paired rdd where the first	matrix factorization	0.040000
whether this instance is	ldamodel is	0.200000
for distinct count	count distinct col	0.040000
a param with a given	param params has	0.019231
regex	regex	1.000000
number of training	ml logistic regression training summary	0.500000
:class dataframe whose schema starts with	data	0.011628
internal function to get	get	0.021739
context	context	0.250000
new dstream in which each rdd	streaming streaming	0.047619
instance to a java onevsrest used for	one vs rest to java	0.166667
a given string name	params has	0.019231
original column during	max scaler model original	0.062500
test the python direct kafka stream foreachrdd	tests test kafka direct stream foreach	1.000000
port	port	0.857143
of this instance this updates	ml	0.001835
sets the	set	0.011834
a new rdd of int containing elements	core	0.003021
gets a	get	0.043478
extract the week number of a given	sql weekofyear	0.055556
returns a :class dataframe representing the result of	sql sqlquery	0.250000
wait until any of the queries on the	sql streaming query manager await any termination timeout	0.166667
contains a param with a given string	params has	0.019231
and c{other}	join	0.034483
boundaries in increasing order for which	regression model boundaries	0.333333
bisecting k-means algorithm based on the	bisecting kmeans	0.166667
rdd of sub-matrix blocks blockrowindex blockcolindex sub-matrix) that	mllib linalg block matrix blocks	0.166667
by the given	bucket by	0.200000
property that affects	property key	0.066667
spark configuration	spark conf init loaddefaults _jvm	0.250000
result as a :class dataframe	data frame	0.015000
can be used again to wait	reset	0.011236
densematrix whose columns are the right singular	mllib linalg singular	0.017544
set a java system property such	context set system property cls	1.000000
in rdd 'x' to all mixture	gaussian mixture model predict	0.100000
of cols	cols	0.263158
value	core accumulator	0.030303
block matrix other from this block matrix this	linalg block matrix	0.052632
asserting that they are equivalent	pipelines m1 m2	0.166667
the threshold	threshold	0.054545
to be used for later scaling	standard scaler fit dataset	0.250000
arbitrary key and value class from	core spark	0.020619
model fitted by randomforestclassifier	random forest classification model	1.000000
set number of batches after	set	0.005917
the null model	generalized linear regression summary null	0.250000
dataframe as	sql data frame corr col1	0.166667
computes column-wise summary statistics	statistics	0.090909
sample without replacement based on the fraction	sample	0.050000
this	one vs	0.125000
outputs a new feature vector with a subarray	vector slicer	0.166667
set the initial value of	initial	0.071429
a left outer join of c{self} and	core rdd left outer join	0.200000
they are equivalent	pipelines m1 m2	0.166667
converts vector columns	mlutils convert vector columns to ml dataset	0.166667
this params	ml param params	0.013699
sort the list based on	streaming test case sort result based on key	0.333333
runs and profiles the	core basic	0.066667
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic",	gbtclassifier	0.076923
stopped	streaming	0.005025
of each instance as	ml	0.001835
obtaining a test statistic	stat test	0.166667
a receiver has	receiver	0.153846
param	param params has param	0.019231
infer schema from an rdd of row	sql spark session infer schema rdd	0.250000
loads a csv file	reader csv path schema sep	0.666667
or two separate arrays of	ml	0.001835
params to the	java params to	0.045455
kolmogorov-smirnov ks test for	stat statistics kolmogorov smirnov test	0.166667
function and attach docstring from func	defined function wrapped	0.333333
sql storage type for this udt	sql user defined type sql type cls	0.500000
strategy	strategy	1.000000
this instance's params to the	params to	0.035714
range of offsets	offset range	0.047619
a windowing column	column over window	0.333333
error which is defined as the	linear regression summary	0.013889
load a model	mllib svmmodel load cls sc	0.200000
for udf registration	udf	0.142857
error which is	linear regression summary	0.013889
using an associative and commutative reduce	rdd reduce by	0.200000
using an associative	by	0.014286
standardization whether to	standardization	0.076923
each point in rdd 'x' to all mixture	mllib gaussian mixture	0.045455
datatype the data type string	datatype string s	0.111111
this thread such as the spark fair scheduler	spark	0.013158
obj assume that all the objects	object size obj	0.040000
rows in this	sql	0.002525
a multi-dimensional cube	data frame cube	0.055556
the spark	spark	0.026316
for feature selection by percentile	mllib chi sq selector set percentile percentile	0.200000
spark configuration	core spark conf init loaddefaults _jvm	0.250000
the values for each key using an	by key func numpartitions partitionfunc	0.066667
correlation	corr col1 col2 method	0.055556
rdd as non-persistent	rdd unpersist	0.066667
conduct pearson's chi-squared goodness	stat statistics chi sq	0.066667
an associative function "func" and a	core rdd	0.003460
this instance is of type distributedldamodel	ml ldamodel is	0.066667
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	ml random forest classifier	0.023256
an rdd of row	rdd	0.006116
this sqltransformer	ml sqltransformer	0.250000
document to rdd	document	0.040000
select filter	selected features	0.333333
centers represented as a list	centers	0.150000
string name	param params has param	0.019231
that each of the points belongs to	predict x	0.033898
return sparkcontext which is associated	spark	0.013158
awaitanytermination()	streaming query manager	0.011236
a column that generates monotonically increasing 64-bit integers	monotonically increasing id	0.333333
compute the number of	row matrix num	0.100000
for each original column during fitting	original	0.047619
a batch	batch	0.068966
already partitioned data into disks	by spill	0.047619
m2	m2	1.000000
current [[dataframe]] and perform the	pivot pivot_col values	0.050000
:py attr lda	distributed ldamodel get	0.066667
pos in byte and	pos	0.022222
matrix to an indexedrowmatrix	mllib linalg coordinate matrix to indexed row matrix	0.333333
create a new spark configuration	spark conf init loaddefaults _jvm _jconf	0.250000
be used again to	streaming query manager	0.011236
computes the levenshtein distance of the two given	levenshtein left right	0.058824
observed tokens in the training	ldamodel training	0.034483
wait for the execution to stop return	timeout timeout	0.125000
residuals mse r-squared of	ml linear regression	0.500000
save a code object	core cloud pickler save codeobject obj	1.000000
instance contains a	ml	0.001835
creates or	create or	1.000000
is the user and the second is an	mllib	0.010526
of the query	streaming query	0.010526
decayfactor timeunit to	streaming	0.005025
iterations default 1 which	iterations	0.043478
a java system	system	0.142857
instance contains a param with a given string	ml param params	0.013699
awaitanytermination() can be used again	query	0.010753
and c{other}	core	0.006042
dictionary a list of index value pairs or	size	0.036697
convert the java_model to a python model	model java_model	0.200000
of tree	tree	0.020833
train a gradient-boosted trees model for regression	mllib gradient boosted trees train regressor cls	0.333333
count to the number	num	0.008403
contains the count of	count	0.016949
stop the execution	streaming streaming context stop	0.125000
train the model on the incoming	regression with sgd train on	0.333333
initial	with sgd set initial	0.111111
api with start offset specified	offset	0.021739
so that :func awaitanytermination() can be used	streaming query	0.010526
decayfactor timeunit to configure the kmeans algorithm for	streaming kmeans	0.035714
the left singular	linalg singular	0.017544
each cluster for each training	summary	0.024390
can be used again to wait	sql	0.002525
true label	label	0.142857
singular value decomposition svd factors	singular value decomposition	0.166667
single script file	single script	0.250000
sort the list based on first	test case sort result based on key outputs	0.333333
instance is of type distributedldamodel	ml ldamodel is distributed	0.066667
the sum for each numeric columns for	sql grouped data sum	0.083333
values for each numeric columns for each group	sql grouped data	0.041667
the behavior when data or table already exists	data frame writer mode savemode	0.071429
key value	key	0.017857
rows of blocks in	row blocks	0.250000
a param with a given	has param	0.019231
test the python direct kafka stream transform	stream tests test kafka direct stream transform	1.000000
java parammap into a python	from java	0.111111
get total number of nodes summed over	total num nodes	0.250000
test the partition id	tests test partition id	1.000000
:class dataframe,	sql data frame	0.005348
retrieve gaussian distributions as	ml gaussian mixture model gaussians df	0.166667
load labeled points saved	mllib mlutils load labeled points sc path minpartitions	0.250000
dump already	by	0.014286
the month of a given date	sql dayofmonth col	0.031250
this streaming query	sql streaming query	0.011765
memory for this	size	0.009174
an input stream from an queue of rdds	streaming context queue stream rdds	1.000000
for new terminations	query	0.010753
the correlation of	corr col1 col2 method	0.055556
set initial centers should be set before calling	mllib streaming kmeans set initial centers centers	0.200000
maxheap variant of _siftdown	core siftdown max heap	1.000000
total log-likelihood for this model	gaussian mixture summary log likelihood	0.142857
two	linalg dense	0.400000
of numpy arrays	ml bisecting kmeans model	0.076923
new column of corresponding string	string	0.041667
window specification that defines the partitioning ordering and	window spec	0.166667
compute the standard deviation of	stdev	0.047619
set bandwidth of each sample defaults to	kernel density set bandwidth bandwidth	0.142857
until any of the queries on the associated	query manager await any termination	0.142857
:py attr binary	binary value	1.000000
by a worker	core worker	0.500000
of words to their vector representations	word2vec model get vectors	0.166667
java onevsrestmodel create and return a	one vs rest model from java	0.142857
perform a left outer join	rdd left outer join other	0.111111
with arbitrary key and value class from	core spark	0.020619
applies standardization transformation on a vector	scaler model transform vector	1.000000
conduct pearson's chi-squared goodness of fit	mllib stat statistics chi sq	0.066667
returns an mlwriter instance	mlwritable write	0.200000
saved using l{rdd saveaspicklefile} method	spark context pickle file name minpartitions	0.250000
dot product of two vectors we support	dot	0.040000
test the python direct kafka stream transform get	tests test kafka direct stream transform get	1.000000
this instance is of type	ml ldamodel is	0.066667
cost sum of squared distances	compute cost x	0.142857
the attempt numbers are correctly reported	tests test attempt number	0.333333
setparams(self scalingvec=none inputcol=none outputcol=none) sets params	elementwise product set params scalingvec inputcol outputcol	0.333333
names of tables in the database dbname	names dbname	0.500000
a file added through	core spark files	0.125000
the given user and product	user product	0.250000
function on each rdd of	transform func	0.058824
labeledpoint to a string	labeled point to	0.125000
obj assume that all the	object size obj	0.040000
that match regexp	regexp	0.076923
setparams(self p=2 0 inputcol=none outputcol=none)	normalizer set params p inputcol outputcol	1.000000
new feature vector with a subarray	vector slicer	0.166667
data against the expected distribution	expected	0.076923
thread such as the spark	core spark context	0.011628
on a per stage basis	profiler collector	0.142857
this instance's params to the wrapped java object	java params to	0.045455
two block matrices together the matrices must have	linalg block matrix add other	1.000000
extract the day of the month of a	dayofmonth col	0.031250
queries so that :func awaitanytermination()	streaming query manager reset	0.011905
to term frequency vectors or transform	tf transform	0.045455
first letter of each word to upper case	initcap col	1.000000
converts matrix columns in	mlutils convert matrix columns from ml dataset	0.166667
specified table	sqlcontext table tablename	0.142857
a parquet file stream returning the result as	stream reader parquet path	0.083333
pos in byte and is	pos	0.022222
the dependent variable given a vector or an	mllib linear regression model base	0.200000
params to the wrapped java	java params to	0.045455
:class dataframe	sql data frame writer save	0.083333
create a sparse vector using either a dictionary	linalg vectors sparse size	0.166667
that :func awaitanytermination()	streaming query manager reset	0.011905
:py attr formula	formula	0.166667
wrap this udf with a function and	function	0.027778
hadoop configuration which is passed in	context hadoop	0.090909
trained using multinomial/binary logistic regression	logistic regression	0.040000
trigger for the stream	sql data stream writer trigger	0.083333
returns a :class	sql sqlcontext sql	1.000000
curve which is a dataframe having two fields	ml binary logistic regression	0.142857
and	rdd	0.003058
into a jvm seq	seq	0.043478
an rdd comprised of	mllib random rdds normal vector rdd	0.166667
the offsetrange	ranges	0.090909
update the	mllib streaming kmeans model update	0.500000
columns in an input dataframe to the	columns from ml	0.125000
parameters in this grid to fixed values	grid builder add grid param values	0.333333
function func	func	0.125000
json string	json string	1.000000
instance contains	has	0.011628
an fp-growth model that contains frequent itemsets	mllib fpgrowth train cls data minsupport numpartitions	0.100000
a paired rdd where the	matrix factorization model	0.043478
of the observed data against the	observed	0.058824
versionadded : 1 2 0	with lbfgs	1.000000
number of possible outcomes	num	0.008403
active queries associated with this sqlcontext	sql streaming query manager active	0.066667
vectors of the singularvaluedecomposition if computeu was set	value decomposition u	0.100000
dense vector represented by	dense vector	1.000000
calculates the correlation of two	corr col1 col2 method	0.055556
contains the count of distinct elements in rdds	count	0.016949
its elements in a numpy ndarray	ml linalg matrix to array	0.166667
or its default	col	0.016393
hexadecimal number	unhex col	0.142857
used again to	reset	0.011236
already partitioned	external group	0.045455
a lower bound on the log likelihood	ldamodel log likelihood	0.142857
pairs or two separate arrays of indices	ml	0.001835
elements in this rdd by	core rdd key by	1.000000
every	linear model	0.066667
content and	content	0.142857
warning these have null parent estimators	ml gbtregression	1.000000
param belongs	param	0.006250
with arbitrary key and value class	core spark	0.020619
with this spark job on every	core spark	0.010309
given parameters in this grid to fixed values	param grid builder base on	0.076923
index of the original partition	index	0.041667
finding frequent items for columns possibly	freq items cols	0.166667
in mixture	mllib gaussian mixture	0.045455
pprint	pprint	1.000000
from an rdd ordered in ascending order or	core rdd take ordered	0.050000
:func awaitanytermination() can be used	manager	0.011236
__init__(self degree=2	polynomial expansion init degree	1.000000
contents of the :class dataframe to a data	sql data frame writer save	0.083333
into disks	external group by	0.045455
new hivecontext for testing	for testing cls sparkcontext	0.333333
in the training set	ldamodel training	0.034483
need	need	1.000000
including	model	0.005587
wait a given amount of	timeout	0.071429
create	submit tests create	0.500000
any streaminglinearalgorithm	linear algorithm	0.076923
given	param params has param	0.019231
in :class dataframe	frame	0.034483
module	module cls	0.333333
sort the list based on first value	spark streaming test case sort result based on	0.333333
the root	root	0.035714
this matrix to an indexedrowmatrix	coordinate matrix to indexed row matrix	0.333333
vector columns in an	vector columns to ml	0.142857
terminations	query manager reset	0.011905
this accumulator's	add	0.035714
formula=none featurescol="features", labelcol="label", forceindexlabel=false) sets params for	set params formula featurescol labelcol forceindexlabel	0.166667
output by the	writer bucket by	0.100000
setparams(self	ml count vectorizer set params	1.000000
such as the spark fair scheduler pool	core spark context	0.011628
a dictionary a	size	0.036697
a paired rdd where	matrix factorization	0.040000
saved	sc path minpartitions	0.250000
called when processing	streaming streaming listener on output operation	0.166667
the area	area	0.181818
an input stream	stream ssc	0.181818
view with the given view name in the	view viewname	0.500000
depth of tree (e g depth	tree	0.020833
:func awaitanytermination()	query	0.010753
new dstream in which each rdd is	streaming streaming context	0.032258
module in modlist to be placed into main	modules to main modlist	0.333333
the norm of	linalg sparse vector norm p	0.066667
an rdd that has no partitions or	spark context empty rdd	0.200000
function to the value of each key-value pairs	map values f	0.125000
zip	zip	0.750000
write() save	mlwritable save	1.000000
vectors to tf-idf vectors	mllib idfmodel transform	0.142857
so that :func awaitanytermination() can be used again	sql streaming query	0.011765
creates a	spark session create	0.058824
from	offset	0.021739
scale	scale	0.800000
variance and count of the rdd's	core rdd	0.003460
stats to stdout id is the rdd id	core profiler show id	0.333333
find norm of the given	norm	0.041667
lda keeplastcheckpoint	ldamodel get	0.066667
items for columns possibly with false positives	items	0.066667
columns are the right singular vectors of	linalg singular	0.017544
until any	streaming query manager await any	0.142857
default min number of partitions	core spark context default min partitions	0.250000
sql storage type	type sql type	0.250000
this obj assume	size obj	0.040000
month of a given date as	sql dayofmonth	0.031250
using the old hadoop outputformat api mapred package	hadoop dataset conf keyconverter valueconverter	0.083333
broadcast a read-only variable to the cluster returning	broadcast	0.052632
external	external list of	0.166667
has been started	started receiverstarted	0.500000
was checkpointed not defined if	checkpoint	0.062500
again to wait	sql streaming query	0.011765
wait for the	termination or timeout timeout	0.125000
setparams(self	ml gaussian mixture set params	1.000000
equal to	numiterations	0.050000
randomly	cross validator model	0.050000
configuration property if not already set	conf set if missing key value	1.000000
of the :class dataframe in	sql data frame	0.016043
item	item	0.500000
for this sqltransformer	sqltransformer	0.125000
field by name in	field name	0.166667
stop the execution of	streaming streaming context stop	0.125000
thresholds thresholds in multi-class classification to	thresholds	0.071429
a column scipy matrix from a dictionary of	sci py tests scipy matrix	0.090909
cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	decision tree regressor	0.058824
and count of the rdd's	core rdd	0.003460
python code for	code name	0.111111
contains a param with	params	0.006623
the accumulator's	core accumulator value	0.045455
in	col	0.032787
so that	sql streaming query	0.011765
make sure user configuration is respected spark-19307	tests test user configuration	1.000000
predicted ratings for input user and	matrix factorization model predict all user_product	0.050000
wait for	query manager	0.011905
saves the content of the dataframe in	data frame writer	0.014085
this block matrix	mllib linalg block matrix	0.052632
the ensemble	ensemble model	0.117647
that :func awaitanytermination() can be	streaming query manager reset	0.011905
label	label	0.428571
called when a receiver has	streaming listener on receiver	0.500000
into a jvm seq of column	seq sc	0.055556
comprised of	mllib random rdds poisson	0.125000
0 weightcol=none	linear	0.051282
or compute the number	mllib linalg block matrix num	0.125000
freedom	freedom	1.000000
receiver operating characteristic roc curve which	binary logistic regression summary roc	0.166667
the accumulator's value only usable in	core accumulator value value	0.050000
names of tables in	table names	0.066667
the table is currently cached in-memory	catalog is cached tablename	0.250000
tests whether this instance contains a param with	has param paramname	0.142857
an input stream that is	stream ssc addresses storagelevel	0.166667
the accumulator's value	core accumulator value value	0.050000
verify the attempt numbers are correctly reported	tests test attempt number	0.333333
boundary end inclusive	end	0.066667
job of a batch has started	started	0.055556
soundex encoding for	sql soundex col	0.055556
at pos in byte	pos	0.022222
a new dstream in	streaming streaming context transform	0.066667
a job of a batch has completed	completed	0.058824
termination of this query either	termination	0.035714
interruptoncancel	interruptoncancel	1.000000
squared error	squared error	1.000000
that all the objects	external	0.013889
used again to wait for new	streaming query manager	0.011236
how much	core external merger	0.032258
elements in rdds in a sliding window	value and window windowduration slideduration	0.076923
compute the number	mllib linalg row matrix num	0.125000
python parammap into	param map to	0.125000
format into an rdd of labeledpoint	load lib svmfile	0.125000
contains a param with a given	ml param params has	0.019231
a histogram using	core rdd histogram	0.333333
create an input stream that pulls events	utils create stream ssc hostname port	0.200000
creating rdds comprised of i i d samples	random rdds	0.012821
from an rdd	core rdd	0.003460
value of	ml validator params	1.000000
a class inherit documentation from its	mllib inherit doc cls	0.045455
update the	streaming kmeans model update	0.500000
the values for each key using	by key	0.026316
set a local property that affects jobs	set local property key	0.200000
attempt numbers are correctly reported	task context tests test attempt number	0.333333
perform a pearson's independence test using dataset	ml chi square test test dataset	0.333333
create an rdd for dataframe from a	spark session create from	0.500000
:	linear regression	0.080000
for params shared	params copy	0.083333
return sparkcontext which is	spark	0.013158
computes column-wise summary statistics for	statistics col stats rdd	0.200000
for the stream query if this is	data stream writer	0.041667
a function and	defined function	0.066667
sets the accumulator's value	accumulator	0.012987
__init__(self featurescol="features", labelcol="label",	classifier init featurescol labelcol	1.000000
multiple parameters passed as a list	core spark conf set	0.100000
a group id to all the jobs	job	0.023810
finding frequent items for columns	data frame freq items cols	0.166667
conditions and returns one of multiple	sql column otherwise	0.050000
year of a given date	sql year	0.050000
for the termination	termination timeout	0.041667
again to wait for new	query manager	0.011905
the n smallest elements in a dataset	core nsmallest n iterable key	0.333333
a multi-dimensional rollup	data frame rollup	0.055556
new rdd of int containing elements from	core spark context range	0.142857
value of the date	next day date	0.100000
the given string by given separator but	s separator	0.333333
a mllib vector	vector value	0.333333
gets the name of	get	0.021739
an input stream that is to be used	stream	0.017544
rdd is checkpointed	rdd	0.003058
registers a python function including lambda function as	register function name	1.000000
matrix columns in an input dataframe to	matrix columns from ml dataset	0.142857
more examples	mllib decision tree model	0.076923
indicates whether this instance is	is distributed	0.200000
string in the format supported by java	s	0.071429
labeledpoint to a string in	labeled point to	0.125000
class representing a multiclass classification model	classification model	0.166667
number of	coordinate matrix num	0.500000
be used again to wait for new	manager	0.011236
class dataframe that with new specified column names	data frame to df	0.090909
a dictionary	size	0.036697
operation test for dstream mappartitions	operation tests test map	0.333333
calculates the norm	norm p	0.166667
cost sum of squared	compute cost	0.142857
based on	path	0.010204
the current [[dataframe]] and perform the	data pivot pivot_col values	0.050000
format at the specified	compression	0.142857
a job of a batch has completed	completed outputoperationcompleted	0.125000
the minutes of a given	sql minute	0.050000
called by a worker	core worker	0.500000
queries	query manager	0.011905
a param	param params	0.014925
transforms	transform	0.062500
returns the soundex encoding for	sql soundex	0.055556
column containing a json string	from json col	0.083333
count of the rdd's	core	0.003021
original column during	original	0.047619
partitioned	external group	0.045455
0 means 1 leaf	mllib decision	0.125000
associative and	core	0.003021
the returned	streaming py spark streaming test case	0.333333
termination of this	termination	0.035714
a new java	ml java wrapper new java	0.333333
the spark sink deployed	storagelevel maxbatchsize	0.045455
a field in schema abstract >>> _parse_field_abstract("a")	field abstract s	0.500000
frequency vectors or	mllib hashing tf	0.125000
new dstream by applying reducebykey to	streaming dstream reduce by key	0.076923
wait until any	await any termination timeout	0.166667
returns a :class dataframe representing the	session sql sqlquery	0.250000
or compute the number of	mllib linalg coordinate matrix num	0.166667
the initial value	initial	0.071429
rdd to the	rdd	0.003058
field in "predictions" which gives the true label	linear regression summary label col	0.333333
names into a jvm seq of column	seq sc cols	0.055556
:py attr lda keeplastcheckpoint	ml distributed ldamodel get	0.066667
of active queries associated with	streaming query manager active	0.066667
__init__(self n=2	ngram init n	1.000000
in "predictions" which gives the true label of	ml logistic regression summary label	0.333333
this instance contains a param with a given	param params	0.014925
obj assume	external merger object size obj	0.040000
iterator of deserialized objects from the	serializer load	0.083333
returns the greatest value of	greatest	0.043478
paired rdd where the first element is the	matrix factorization	0.040000
__init__(self labelcol="label", featurescol="features",	generalized linear regression init labelcol featurescol	1.000000
load a model from the given	mllib svmmodel load cls sc	0.200000
spark sink	storagelevel maxbatchsize	0.045455
output a python rdd of key-value	core rdd save as	0.037500
the given spark runtime configuration property	sql runtime config	0.500000
is checkpointed and	is checkpointed	0.142857
again	sql streaming query manager	0.011905
cross validator	cross validator	0.045455
libsvm format into an rdd of labeledpoint	lib svmfile	0.125000
a model to the input dataset this is	dataset	0.020408
of	sql	0.010101
weights is close to the	parameter accuracy	0.029412
spark configuration	core spark conf	0.055556
parameters passed as a list	spark conf set all	0.125000
specifies the underlying output	sql data frame writer	0.011628
"predictions" which gives the probability of each class	ml logistic regression summary probability	0.166667
and count of the rdd's elements in	core rdd	0.003460
observed is vector conduct pearson's chi-squared	chi sq	0.111111
the levenshtein distance of the	sql levenshtein left right	0.058824
test the python direct kafka stream transform	kafka stream tests test kafka direct stream transform	1.000000
awaitanytermination() can be	sql streaming query manager	0.011905
tokens in the training set given the current	distributed ldamodel training	0.034483
convert this matrix to a coordinatematrix	linalg indexed row matrix to coordinate matrix	0.333333
removes the specified table from	uncache table tablename	0.250000
generates	sc mean	1.000000
specific group matched by a	str pattern idx	0.111111
the :class dataframe	frame	0.034483
inputformat with arbitrary key and value	inputformatclass keyclass valueclass	0.125000
residual degrees of freedom	linear regression summary residual degree of freedom	0.250000
stop the execution of the streams	streaming streaming context stop	0.125000
the list based	based	0.125000
parallelism	parallelism	1.000000
inputformat with arbitrary	inputformatclass	0.095238
the :class dataframe as the specified table	data frame writer save as table	0.333333
vector columns in an input	vector columns to ml dataset	0.142857
wait for new	sql streaming	0.010204
make predictions on batches of	streaming linear algorithm predict on	0.066667
date1	date1	1.000000
them with extra	map extra	0.040000
inputformat with arbitrary key and value class	inputformatclass	0.095238
0 95 0 99], quantilescol=none aggregationdepth=2)	fitintercept	0.058824
converts matrix columns	mllib mlutils convert matrix columns from ml	0.166667
the selector type of the	selector type	0.100000
passed in a profile object is returned	profiler profile	0.200000
broadcast	broadcast value	0.125000
interface used to write a :class dataframe	data frame	0.005000
forest	forest	0.583333
this instance contains a param with a	params has param	0.019231
until any of the queries on the associated	streaming query manager await any	0.142857
create a new dstream in which each rdd	streaming streaming context	0.032258
how much	external	0.013889
any hadoop-supported file system	file path	0.035714
is	spark context	0.023256
squared distance from a sparsevector or 1-dimensional	squared distance other	0.153846
for each numeric columns	sql grouped	0.086957
converts matrix columns in an input dataframe to	convert matrix columns from ml dataset	0.166667
train the model on the	kmeans train on	0.333333
from start to end exclusive increased	range start end	0.333333
on the	with	0.055556
mixin for param numfeatures number of features	has num features	1.000000
setparams(self featurescol="features",	mixture set params featurescol	1.000000
week number of a given date as	sql weekofyear	0.055556
:class dataframe	frame writer save	0.066667
cluster centers represented as	bisecting kmeans model cluster centers	0.095238
with the specified schema	schema	0.033333
the given data type json	datatype json	0.333333
how much	core external merger object	0.032258
mintokenlength	mintokenlength	1.000000
test the	task context tests test	0.500000
for a shared param class	param gen param	0.333333
sets	train validation split set	1.000000
a csv file and	csv path	0.166667
sorter	sorter	1.000000
get the cluster	mllib kmeans model cluster	0.333333
d decimal places	d	0.125000
get all values as	get all	0.166667
dataframe, using the given join	join	0.034483
train the model	with sgd train	1.000000
a model to be used for later scaling	mllib standard scaler fit dataset	0.250000
libsvm format into	parse libsvm	0.125000
for	key value	0.800000
returns	sql streaming	0.020408
dump already partitioned	external group by	0.045455
the date column	date	0.037037
right outer join	full outer join other numpartitions	0.111111
java array of given java_class type useful	ml java wrapper new java array pylist java_class	0.333333
returns the precision-recall curve which is a	binary logistic regression summary pr	0.083333
load a java model	load java cls sc	0.200000
the cluster centers represented as	model cluster centers	0.090909
to pass else fail with	test case eventually	0.500000
track supported impurity measures	tree classifier params	0.250000
sliding window of time over this dstream	streaming dstream window	0.333333
converts vector columns	mllib mlutils convert vector columns to ml	0.166667
average values for each numeric columns for	grouped data avg	0.058824
called when processing of a job	streaming streaming listener on output operation	0.166667
isnan	isnan	1.000000
that all the	core external merger object	0.032258
train the model	streaming logistic regression with sgd train	1.000000
find the spark_home	find spark home	1.000000
rdd into	rdd	0.003058
set multiple parameters passed as a list of	spark conf set	0.111111
note : experimental	linear svc	0.142857
load a java model from the given path	load java cls sc path	1.000000
for saving	java mlwriter	0.250000
parameters in this grid to	param grid builder base	0.076923
such as the spark fair scheduler	core spark context	0.011628
onevsrestmodel used for ml persistence	one vs rest model	0.058824
given value to scale decimal places	sql	0.002525
item by key out of	item key	0.250000
distance from	distance	0.095238
warning these have null parent estimators	gbtclassification	0.142857
get the n elements	num	0.008403
whether to fit an intercept term	fit intercept	0.250000
string	has	0.011628
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none	random forest classifier	0.022727
predict values for a	predict x	0.033898
value of	ml one hot	1.000000
reduced into numpartitions partitions	numpartitions shuffle	1.000000
timeunit	streaming	0.005025
set bandwidth of	mllib stat kernel density set bandwidth bandwidth	0.142857
mean variance and	core	0.003021
standardization whether	standardization	0.076923
as spark executor memory	core spark	0.010309
predict the label of one or	predict x	0.016949
a param from the param	param	0.006250
vectors or	mllib hashing tf	0.125000
the md5 digest and returns	sql md5	0.333333
a dataframe as	sql data frame corr col1	0.166667
onevsrest and onevsrestmodel	one vs rest	0.034483
of months between date1	months between date1	0.333333
model	java model	0.333333
that :func awaitanytermination() can	query	0.010753
names of tables in the	sqlcontext table names	0.066667
sparkcontext which is associated with this	streaming streaming context spark	0.083333
the root directory that contains	get root directory	0.333333
instance for	ml one vs rest	0.052632
instance contains a param with a given string	params	0.006623
minimum	min	0.083333
of	core external merger object size	0.032258
runs and profiles the method to_profile passed in	core	0.003021
labeledpoint to	labeled point to	0.125000
conditions and returns one of	sql column otherwise	0.050000
into an rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures	0.125000
rollup	rollup	1.000000
training iterations until termination	logistic regression training summary total iterations	1.000000
cached in-memory	cached	0.166667
squared distance from a sparsevector or	sparse vector squared distance	0.166667
predicts rating for the	mllib matrix factorization model predict	0.250000
the += operator	core accumulator iadd	0.500000
set a	spark	0.013158
awaitanytermination() can be used again to	streaming	0.005025
mixin for param elasticnetparam the elasticnet mixing	has elastic net param	0.200000
elements in this rdd	core rdd key	1.000000
simple tcp server that intercepts shutdown() in order	accumulator server	1.000000
trainratio=0 75	trainratio	0.142857
curve which	binary logistic regression	0.142857
stdin	stdin	0.833333
can	streaming query manager reset	0.011905
the input param	param	0.006250
thread such as the spark fair	core spark	0.010309
to wait for new	sql streaming query	0.011765
the accumulator's data	accumulator	0.012987
root directory that contains	get root directory	0.333333
memory for this	merger	0.025641
comprised of vectors	random rdds log normal	0.125000
as a temporary table in	as table	0.200000
elasticnetparam the	elastic net	0.125000
the new hadoop outputformat api mapreduce package	save as new apihadoop dataset conf keyconverter valueconverter	0.142857
convert this matrix to a rowmatrix	mllib linalg coordinate matrix to row matrix	0.333333
context to use for saving	java mlwriter context	1.000000
centroids according	decayfactor timeunit	0.250000
depth of tree (e g depth 0	tree	0.020833
much of	core	0.003021
that :func	manager	0.011236
line in libsvm format	libsvm line line multiclass	0.333333
map to the java related object	init host port	0.200000
given unary	sql unary	0.200000
calculates the length	sql length col	0.050000
that	reset	0.011236
on	context get	0.333333
splits	split	0.062500
of int containing elements from start	core spark context range start	0.090909
of predicted ratings for	matrix factorization model predict all user_product	0.050000
the sort order	sort	0.111111
average values for each numeric columns for each	grouped data avg	0.058824
spark sql	sql sqlcontext	0.095238
persist its values across operations after the	core rdd persist storagelevel	0.166667
in mixture	gaussian mixture model k	0.200000
minutes	sql minute col	0.050000
in c{self} that is not contained in c{other}	core rdd subtract other numpartitions	0.111111
clusters in	clustering	0.066667
is associated with this streamingcontext	context	0.022727
labeled points	labeled points	1.000000
this distributed model to	distributed ldamodel to	0.166667
fit	estimator fit	0.166667
vector columns in	vector columns	0.142857
variance and count	rdd	0.003058
stream query if	stream	0.017544
partitions to use during reduce tasks	rdd default reduce partitions	0.166667
dataframe	data frame data	0.142857
into	core external group by spill	0.047619
for new	query	0.010753
get	get	0.260870
return a callsite representing	core	0.003021
note : experimental	fpgrowth model	1.000000
an exception if any	mllib	0.010526
setparams(self	logistic regression set params	1.000000
an external	external list	0.166667
mixin for param checkpointinterval set checkpoint interval (>=	has	0.011628
quantile	quantile	0.833333
vector to the	vector	0.038462
logistic regression model on the	logistic regression with	0.250000
comprised of vectors containing i	mllib random rdds exponential	0.125000
partial objects do not serialize correctly	save partial	0.125000
the given parameters in this	grid builder add	0.200000
url url and connection	url	0.076923
nodes in tree including	mllib decision tree model	0.076923
which the centroids of that particular batch	timeunit	0.025641
even if users construct taskcontext instead	core task context new	0.333333
a list of	core spark context	0.011628
creates a :class	session create	0.058824
load an isotonicregressionmodel	mllib isotonic regression model load cls sc path	1.000000
return an iterator that contains all	local iterator	0.333333
dump already	external	0.013889
stat	stat	0.384615
the cluster centers represented as a list	model cluster centers	0.090909
instance for this	ml pipeline model	0.066667
that all the	core external	0.016129
model's transform method	linear regression summary predictions	0.200000
function for aggregator by name	function name doc	0.500000
extracts the embedded default param values	ml param params extract param map	0.333333
from the given path	path	0.040816
how much of memory for	object	0.027778
named table accessible via jdbc url url	jdbc url table column lowerbound	0.166667
of	ml string indexer	0.166667
points	points sc path	1.000000
generate	logistic regression with sgdtests generate	1.000000
rdd of labeledpoint	mlutils load lib svmfile	0.125000
sets the given parameters in this grid	grid builder base on	0.076923
this	ml param params has	0.019231
an rdd ordered in	rdd take ordered	0.050000
out of a list or gets an	sql column get	0.142857
for the stream query if	data stream writer	0.041667
string name	ml	0.001835
again	manager reset	0.011905
default min number of partitions for hadoop	spark context default min partitions	0.250000
__init__(self n=2 inputcol=none outputcol=none)	ngram init n inputcol outputcol	1.000000
min number of partitions	min partitions	0.200000
the content of the :class dataframe in parquet	sql data frame writer parquet	0.200000
which is a dataframe	regression	0.010000
sets	max iter set	1.000000
a column scipy matrix from a dictionary	mllib sci py tests scipy matrix	0.090909
saves the contents of	path format mode partitionby	0.200000
numnearestneighbors	numnearestneighbors	1.000000
the rdd contains no elements at	core rdd is empty	0.083333
be used again to wait for new terminations	query manager reset	0.011905
to select filter	selected features	0.333333
transforms a python parammap into	java params transfer param map to	0.500000
original	min max scaler model original	0.062500
used	reset	0.011236
value to a mllib vector if possible	param type converters to vector	0.333333
condition	condition	0.318182
the :class dataframe to the	data frame	0.005000
so that :func awaitanytermination() can be used	manager	0.011236
an rdd comprised of vectors	mllib random rdds poisson vector rdd	0.166667
selector type	chi sq selector set selector type	0.111111
storagelevel based on a	core spark context	0.011628
specifies the underlying output	stream writer format	0.333333
the week number of	weekofyear	0.043478
dstreams	dstreams	1.000000
create a multi-dimensional cube for the current	data frame cube	0.055556
globals names read or written to	globals	0.076923
converts matrix columns in an input dataframe from	convert matrix columns to ml dataset	0.166667
already partitioned data	group by	0.041667
:class dataframe to the	data frame	0.005000
with matching keys in c{self} and c{other}	join	0.034483
the underlying output	data frame writer	0.014085
that :func awaitanytermination() can be	sql streaming query manager reset	0.011905
right	rdd full	0.333333
queries so that :func awaitanytermination()	query manager reset	0.011905
the values for each key using	by key func numpartitions	0.062500
be used again	query manager reset	0.011905
'x' to all mixture components	gaussian mixture model predict soft x	0.142857
param with a given	param	0.012500
result as a :class dataframe	grouped data	0.035714
train the model on	kmeans train on	0.333333
or compute the number of cols	mllib linalg coordinate matrix num cols	0.333333
extract the week number of	sql weekofyear col	0.055556
impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1 0)	ml random forest classifier	0.023256
that	streaming	0.005025
much of memory	core external merger object size	0.032258
functions registered in the specified	functions	0.071429
for each original column	min max scaler model original	0.062500
get or compute the	mllib linalg coordinate	0.333333
gaussians in mixture	mixture model	0.066667
converts matrix columns in an input dataframe	convert matrix columns from	0.166667
get number	model num	0.083333
current [[dataframe]] and perform	data pivot pivot_col values	0.050000
with arbitrary key and value	core	0.006042
text file using string representations of	text file path compressioncodecclass	0.166667
number of columns	mllib linalg block matrix cols	0.333333
compare	ml persistence test compare	0.166667
basic operation test for dstream mappartitions	streaming basic operation tests test map partitions	1.000000
dataframe with two columns * items - itemset	fpgrowth model freq itemsets	1.000000
given data type json	sql parse datatype json	0.333333
sets the accumulator's value only	core accumulator value value	0.050000
generates an rdd comprised of	random rdds gamma vector rdd sc	0.200000
iterator of deserialized batches lists of	without unbatching	0.125000
a shortcut of write() save path	ml mlwritable save path	0.200000
whose columns are the right singular vectors of	singular	0.015625
save this model to the given	mllib logistic regression model save sc	1.000000
can	sql	0.002525
the old hadoop outputformat api mapred package	as hadoop dataset conf keyconverter valueconverter	0.083333
generates an rdd comprised of vectors	random rdds poisson vector rdd sc mean	0.200000
train the model on the incoming	mllib streaming linear regression with sgd train on	0.333333
in "predictions" which gives the probability	logistic regression summary probability col	0.333333
sparkcontext which is	streaming streaming context spark	0.083333
cluster assignments cluster sizes of	ml bisecting kmeans	0.062500
memory for this	core external merger	0.032258
function	sql user defined function	0.083333
mixture components	mllib gaussian mixture model predict soft	0.142857
called when processing of a	streaming listener on output operation	0.166667
transforms a python parammap	params transfer param map	0.250000
output a python rdd of key-value pairs	rdd save	0.038462
the weights of layers	ml multilayer perceptron classification model weights	0.500000
instance contains a param with a given string	ml param params has param	0.019231
java pipeline create and return a python	pipeline from java	0.142857
a large dataset and an item approximately	lshmodel approx nearest neighbors dataset key numnearestneighbors distcol	0.166667
awaitanytermination() can be used	reset	0.011236
udfregistration for udf registration	sqlcontext udf	1.000000
used with the spark sink deployed on a	maxbatchsize	0.037037
a paired rdd	factorization	0.038462
the training set given the current parameter estimates	ldamodel training	0.034483
a shared param class	param gen param	0.333333
the distributed matrix on	matrix	0.015152
collect each rdds into the returned list	streaming py spark streaming test case collect dstream	1.000000
finding frequent items for columns possibly with false	freq items cols	0.166667
number of partitions for hadoop	partitions	0.066667
in the specified database	tablename dbname	0.142857
elastic	elastic	1.000000
of	ml java wrapper new	0.250000
kolmogorov-smirnov ks test	mllib stat statistics kolmogorov smirnov test	0.166667
calculates the norm of	ml linalg sparse vector norm p	0.333333
on the log likelihood	log likelihood dataset	0.142857
the uid of this instance this updates	ml param params reset uid	0.058824
operator	op name doc	0.166667
drops the global temporary	drop global temp	1.000000
formula=none featurescol="features", labelcol="label",	featurescol labelcol	0.095238
applies transformation on	mllib vector transformer transform	0.500000
loads vectors saved	load vectors	0.333333
of names	names	0.050000
of the dataframe in	data frame writer	0.014085
convert this vector to the new mllib-local representation	linalg sparse vector as ml	0.333333
a randomly generated	train validation split	0.166667
file	path schema sep encoding	0.666667
rdd of key-value pairs (of form c{rdd[	rdd save	0.038462
this instance's params to the	ml java params to	0.045455
awaitanytermination() can be used again	query manager reset	0.011905
comprised of vectors containing i i d samples	random rdds exponential vector	0.125000
value of	ml tree	0.750000
py or zip dependency	py file path	0.066667
initialized or not	ensure initialized	0.333333
initmode	initmode	0.833333
with	ml	0.001835
rdd contains the count of distinct elements	count	0.016949
make predictions on batches of data from a	mllib streaming linear algorithm predict on	0.066667
for data sampled from a continuous distribution	data	0.011628
a labeledpoint to a string	labeled point to	0.125000
model fitted by gbtclassifier	gbtclassification model	1.000000
with a	ml param params has	0.019231
column or names into a jvm seq of	seq sc cols	0.055556
matrix on the driver as	local matrix	0.250000
how much of	core external merger object size	0.032258
converts vector columns in an input dataframe	mllib mlutils convert vector columns	0.166667
to wait for new	sql streaming	0.010204
or any hadoop-supported file	file path	0.035714
rdd of key-value pairs (of form c{rdd[ k	rdd	0.009174
general pyspark tests that depend on	sci py tests	0.250000
null model	linear regression summary null	0.250000
param with a given string name	param params	0.014925
or two separate arrays of	ml linalg	0.030303
java parammap into a python parammap	param map from java	0.500000
dct	ml dct	0.250000
new :class column for distinct count of col	count distinct	0.040000
so that :func	manager reset	0.011905
test that	test	0.030303
data sampled from a continuous	data	0.011628
the driver as	matrix to local	0.500000
set initial centers should	kmeans set initial centers centers	0.200000
each	func numpartitions	1.000000
their vector representations	word2vec model get vectors	0.166667
the context to	streaming streaming context	0.032258
this instance with a randomly generated	validation split	0.200000
this instance contains a param with	has param	0.019231
separators inside brackets pairs e g	brackets	0.058824
the approximate quantiles of numerical	approx quantile col probabilities relativeerror	0.166667
a configuration property if not already set	core spark conf set if missing key value	0.500000
accumulator's value only usable	core accumulator value value	0.050000
adds a term to this	term	0.080000
random forest <http //en wikipedia org/wiki/random_forest>_	random forest classifier	0.022727
limits the	limit	0.076923
types asserting that they are equivalent	pipelines m1 m2	0.166667
levenshtein distance of	sql levenshtein left right	0.058824
'old' hadoop	spark context hadoop	0.090909
(json lines text format or newline-delimited json	writer json path mode	0.125000
the contents of the :class dataframe	frame writer	0.050000
vector to the new mllib-local representation	sparse vector as ml	0.333333
queries so that :func awaitanytermination() can be	streaming query manager reset	0.011905
the accumulator's	core accumulator value value	0.050000
codeblock	cls	0.047619
rating for	mllib matrix factorization	0.250000
column standard	standard scaler model	0.090909
the norm of a sparsevector	sparse vector norm p	0.066667
object into java	mllib py2java sc obj	0.333333
table named table	table column lowerbound	0.166667
defined function in	defined function	0.066667
random seed	seed	0.111111
memory for this obj assume that	object size obj	0.040000
result as a :class dataframe	sql data	0.024390
disks	group by spill	0.047619
get total number of nodes summed	total num nodes	0.250000
other from this	linalg	0.022222
represents a row-oriented distributed matrix with indexed rows	indexed row matrix	0.333333
a flume	flume	0.071429
first n rows to the console	data frame show n truncate vertical	0.333333
transform get offsetranges	transform get offset ranges	1.000000
:param rdd an rdd	mllib power iteration clustering train cls rdd	0.250000
the :class dataframe in parquet	sql data frame writer parquet	0.200000
evaluator for binary classification	binary classification metrics	0.500000
of memory for this obj assume that all	merger object size obj	0.040000
to a java pipelinemodel used for	pipeline model to java	0.100000
create an rdd for dataframe from a	session create from	0.500000
are the left singular vectors	singular	0.015625
all the objects	external merger object	0.032258
spark sink deployed on	ssc addresses storagelevel maxbatchsize	0.045455
model by	linear	0.025641
current [[dataframe]] and	sql grouped data pivot pivot_col values	0.050000
load labeled points	mlutils load labeled points sc	1.000000
libsvm format into	mllib mlutils parse libsvm	0.125000
instance contains a param with a given string	params has	0.019231
set	mllib streaming kmeans set	0.142857
date truncated to the unit specified by the	trunc date	0.500000
checkpoint the dstream	checkpoint	0.062500
memory for this	core external merger object	0.032258
resulting rdd that contains a tuple with the	rdd cogroup other numpartitions	0.066667
:class dataframe, using the given join	data frame join	0.500000
code for	code name doc	0.111111
or compute the number	block matrix num	0.100000
columns	mllib linalg block matrix cols	0.333333
a vector	vector	0.057692
squared distance from a sparsevector or	sparse vector squared distance other	0.166667
and predicting on incoming dstreams	streaming	0.005025
converts vector columns in	mllib mlutils convert vector columns from	0.166667
use the model to make predictions on the	predict on	0.058824
probability	probability	0.600000
system using the old hadoop outputformat api	save as hadoop dataset conf keyconverter valueconverter	0.083333
param with	param params has param	0.019231
returns an mlreader instance for	mlreadable read cls	0.200000
of memory for	core external merger object size	0.032258
to a local representation this discards info	to local	0.125000
json <http //jsonlines	writer json path mode compression dateformat	0.166667
least	least	0.217391
as	spark	0.013158
table in	table	0.031250
note : developerapi	type converters	1.000000
decorator that makes a class inherit documentation from	mllib inherit doc cls	0.045455
sets window size	word2vec set window size	1.000000
seq of columns that describes the sort	data frame sort cols cols kwargs	0.142857
all the	core external	0.016129
convert this matrix to a coordinatematrix	block matrix to coordinate matrix	0.333333
applies unit length normalization on a vector	normalizer transform vector	1.000000
set pipeline stages	ml pipeline set stages value	1.000000
into	param map to	0.125000
with a function and attach docstring from func	user defined function wrapped	0.333333
number of nonzero elements this	dense vector num	0.166667
returns an	sql	0.005051
formula=none featurescol="features", labelcol="label", forceindexlabel=false)	featurescol labelcol forceindexlabel	0.400000
train the model	kmeans train	0.333333
set initial centers should be set	mllib streaming kmeans set initial centers centers weights	0.200000
estimated coefficients and	linear regression summary	0.013889
a python rdd of	rdd	0.012232
return the	mllib standard scaler	0.100000
idf vector	ml idfmodel idf	0.333333
dump already partitioned data into disks	by	0.014286
right singular vectors of the	mllib linalg singular	0.017544
as a temporary	as	0.037037
mean	core	0.003021
comprised of vectors containing i i d samples	random rdds	0.076923
generated unique long ids	unique id	1.000000
distcol	distcol	1.000000
__init__(self labelcol="label", featurescol="features",	regression init labelcol featurescol	1.000000
k classes classification problem in	classes	0.034483
in the ensemble	tree ensemble model	0.076923
while tracking the	preservespartitioning	0.090909
approximate quantiles of numerical columns	approx quantile col probabilities relativeerror	0.166667
convert	convert	0.555556
the right singular	singular	0.015625
be used again to wait for	streaming query	0.010526
group id to all the jobs started	job	0.023810
:func awaitanytermination() can	streaming	0.005025
cartesian product with another :class dataframe	data frame cross join other	1.000000
are the right singular	mllib linalg singular	0.017544
an external table based on the	external table tablename path	0.090909
__init__(self	imputer init	1.000000
the initial	regression with sgd set initial	0.111111
set the trigger for the stream	data stream writer trigger	0.083333
kolmogorov-smirnov ks test for data sampled from	stat statistics kolmogorov smirnov test data distname	0.111111
cross	cross	0.857143
transforms	ml java params transfer param	0.250000
persist its values across operations after	core rdd persist storagelevel	0.166667
the :class dataframe using	data frame	0.005000
computes an fp-growth model	mllib fpgrowth train cls data minsupport	0.100000
vector columns in an input dataframe to	vector columns from ml	0.142857
predicts rating for the given user	mllib matrix factorization model predict user	0.500000
skipping null	sql	0.005051
l{statcounter} object that captures the mean variance and	rdd stats	0.083333
a dataframe	data frame	0.005000
specifies the behavior when data	data frame writer mode savemode	0.071429
file system using	file path	0.035714
than or equal to	numiterations	0.050000
:class dataframe as a temporary table in	data frame as table	0.333333
mean squared error which is defined as the	linear regression summary	0.013889
dataframe as pandas pandas	to pandas	0.166667
internal use only create	context create	0.083333
indices back to a new column	index to	0.040000
spark configuration	spark conf init	0.250000
create an input stream that pulls	utils create stream ssc hostname port storagelevel	0.200000
key using an	by key	0.026316
d samples drawn	numrows numcols numpartitions	0.125000
lda keeplastcheckpoint is set	distributed ldamodel	0.052632
:class dataframe	frame writer	0.050000
squared distance from a sparsevector or 1-dimensional numpy	sparse vector squared distance	0.166667
a large dataset and an item approximately	lshmodel approx nearest neighbors dataset	0.166667
the list of column names skipping null	sql	0.005051
test for data sampled from a continuous	test data	0.166667
converts vector columns	mllib mlutils convert vector columns to	0.166667
sets vector size	set vector size	1.000000
methods to set k decayfactor timeunit	streaming	0.005025
for each numeric columns for	sql grouped data	0.083333
instance contains a param with a given string	param	0.012500
generates an rdd comprised of vectors	random rdds log normal vector rdd sc mean	0.200000
with the dispatch to handle all function	cloud pickler save function obj name	0.142857
squared distance from a sparsevector or	squared distance other	0.153846
that	sql streaming query manager reset	0.011905
train a gaussian mixture clustering model	mllib gaussian mixture train cls rdd k convergencetol	0.500000
partial objects do not serialize correctly	cloud pickler save partial	0.125000
creates a :class dataframe from an :class	session create	0.058824
format or newline-delimited json	writer json path mode	0.125000
the values for each key using	key func	0.066667
batch has half the weightage	half life halflife	0.166667
mean values	model mean	0.125000
k=2 probabilitycol="probability", tol=0 01	k probabilitycol	0.333333
used with the spark sink deployed on	maxbatchsize	0.037037
that starts at pos in	pos	0.022222
be used again to	query manager	0.011905
objective function scaled loss + regularization at each	logistic regression training summary objective history	0.500000
given string	ml	0.001835
awaitanytermination() can	query	0.010753
of numpy	ml	0.001835
save this rdd as a text	core rdd save as text	1.000000
train the model on the	regression with sgd train on	0.333333
bandwidth	bandwidth	1.000000
computes column-wise summary statistics for the	stat statistics col stats rdd	0.200000
attr lda keeplastcheckpoint is set	ldamodel	0.034483
in	streaming	0.005025
product and returns a list of rating objects	product	0.029412
use mappartitionswithindex instead	core rdd map partitions with split f preservespartitioning	0.500000
already	external	0.013889
pairs or two separate arrays of	ml linalg	0.030303
convert this vector to	vector	0.019231
in c{self} that is not contained	subtract other numpartitions	0.111111
featurescol="features", predictioncol="prediction",	featurescol predictioncol	1.000000
withreplacement	withreplacement	1.000000
objective function scaled loss + regularization at	summary objective history	0.500000
a multi-dimensional cube for the current :class	data frame cube	0.055556
collector	collector	0.833333
points from the population should be a rdd	mllib stat kernel density	0.066667
the length of a string or binary	length col	0.050000
the accumulator's value only usable in driver program	core accumulator value	0.090909
vector to	sparse vector	0.062500
awaitanytermination() can be used again to	sql streaming query	0.011765
the explained	explained	0.181818
matrix from the new mllib-local representation	mllib linalg matrices from ml mat	0.333333
loads vectors	load vectors	0.333333
private class to track supported random forest	random forest	0.041667
prefix of string in doc	prefix f	0.142857
applies standardization transformation on a	transform	0.062500
windowsize	windowsize	0.833333
eventtime	eventtime	0.625000
sets	param has reg param set	1.000000
item onto heap maintaining the heap invariant	core heappush heap item	0.500000
how much of memory for	core external merger object size	0.032258
this instance	params	0.006623
the year of a given date as integer	year col	0.050000
from start inclusive to end inclusive	range between start end	0.250000
so that :func	sql streaming query manager	0.011905
partial	cloud pickler save partial	0.125000
convert this vector to the new mllib-local representation	linalg vector as	1.000000
returns a dummy params instance used as	param params dummy	0.111111
text format or newline-delimited json	writer json	0.125000
window function	window function	1.000000
forest <http //en wikipedia org/wiki/random_forest>_	forest classifier	1.000000
a given product	product	0.029412
of document	document	0.040000
return an rdd containing all	rdd	0.003058
class dataframe	data frame	0.005000
__init__(self inputcol=none outputcol=none handleinvalid="error")	string indexer init inputcol outputcol handleinvalid	1.000000
the	core external merger object	0.032258
object is on right side	reverse	0.166667
of key-value pairs	set all pairs	0.500000
a list of	ml	0.001835
a :class datatype the data type string	datatype string	0.111111
to the input dataset this is called by	dataset	0.020408
'new api' hadoop	spark context new apihadoop	0.333333
saved using rdd[vector] saveastextfile	mlutils	0.125000
test python direct kafka rdd messagehandler	kafka stream tests test kafka rdd message handler	1.000000
transforms a python parammap into a	ml java params transfer param map to	0.500000
storagelevel based on	context get	0.333333
sets	mixture set	1.000000
return a new :class dataframe with	sql data frame drop	0.333333
in c{self} and c{other}	join other numpartitions	0.071429
set bandwidth of	stat kernel density set bandwidth bandwidth	0.142857
left singular vectors of the	singular	0.015625
computes the area under the	area under	0.166667
sq	sq	0.833333
weights for each tree	tree ensemble model tree weights	0.333333
given data type	sql	0.002525
larger	larger	1.000000
for the stream query if this is not	data stream writer	0.041667
mean values	mllib standard scaler model mean	0.125000
elements in this rdd	core rdd to	1.000000
wait for	streaming context await termination or timeout timeout	0.125000
of users for a given	users	0.066667
a param with a given	param params	0.014925
compute the dot product of two vectors we	ml linalg dense vector dot	0.090909
the mean variance and count	rdd	0.003058
a local property set in this thread	get local property key	0.066667
for this model	model	0.016760
prefs	prefs	1.000000
values for each key using	by key	0.026316
the sql context to use for	context sqlcontext	0.083333
:func	sql streaming	0.020408
queries so that :func awaitanytermination() can be	sql streaming query manager	0.011905
convert this distributed model	distributed ldamodel	0.052632
again to wait for new	streaming	0.005025
of objects from	serializer load stream	0.250000
can be used again to wait for	sql streaming	0.010204
that make up each block	per block	1.000000
fits a java model to	estimator fit java	0.333333
estimator=none estimatorparammaps=none evaluator=none trainratio=0 75 seed=none):	estimator estimatorparammaps evaluator trainratio	0.500000
an exception if any error is	mllib linalg	0.026316
parses	sql expr	0.125000
params to the wrapped java object and	params to	0.035714
of this	ml param	0.019048
year	year col	0.050000
column for approximate distinct count	approx count distinct col rsd	0.066667
densematrix whose columns are the right singular	linalg singular	0.017544
sets the given parameters in this grid to	param grid builder base	0.076923
mindivisibleclustersize	mindivisibleclustersize	1.000000
computes the area	classification metrics area	0.333333
partitioned data into disks	group by	0.041667
and parent	persistence test	1.000000
for	mllib linear	0.166667
compute the number of	indexed row matrix num	0.100000
utc	from utc	0.125000
perform a right outer join of c{self} and	core rdd full outer join	0.200000
recursive	recursive	0.625000
cost sum of	compute cost x	0.142857
decode the	decoder	0.142857
convert	sql to	0.083333
comprised of vectors containing i i d	mllib random rdds normal vector	0.125000
:class windowspec with the frame boundaries	sql window range	0.166667
can	query manager reset	0.011905
correlation of two columns of	corr col1 col2 method	0.055556
window size default 5	window size windowsize	1.000000
a python rdd of key-value pairs (of form	rdd save as	0.038462
log probability	log prior	1.000000
the centroids according to data	data decayfactor timeunit	0.500000
how much	merger	0.025641
sort order	data frame sort	0.125000
depth 0 means 1 leaf	decision	0.052632
pipelinemodel used for	ml pipeline model	0.066667
stream	data stream	0.028571
sets	sqltransformer set	1.000000
columns are the right singular vectors	singular	0.015625
evaluates the	ml evaluator evaluate dataset	0.285714
periodically checkpoint the dstream operations for master fault-tolerance	checkpoint directory	1.000000
for new	sql streaming query manager reset	0.011905
field in "predictions" which gives the features	linear regression summary features col	0.333333
computes an fp-growth model that contains frequent itemsets	mllib fpgrowth train cls data minsupport	0.100000
in _input_kwargs	only func	1.000000
batch of	batch	0.068966
new :class dataframe that has exactly numpartitions partitions	data frame coalesce numpartitions	0.500000
to configure the kmeans algorithm for fitting	kmeans	0.025641
expression representing a user defined function udf	sql udf f returntype	0.200000
in a text file at the specified path	writer text path compression	0.333333
dump the profile stats into directory path	collector dump profiles path	1.000000
transform get	transform get	1.000000
to an external database table	table	0.031250
the spark sink deployed	addresses storagelevel maxbatchsize	0.045455
the :class dataframe	frame writer save	0.066667
pipeline create and return a python wrapper of	ml pipeline	0.047619
intercept	intercept	0.636364
the year of a	year	0.040000
returns subset	metrics subset	1.000000
api with start offset specified	from offset	0.125000
a new dstream in which each rdd	streaming streaming	0.047619
set master url to connect to	spark conf set master value	1.000000
inputformat with arbitrary	inputformatclass keyclass	0.125000
__init__(self splits=none inputcol=none	bucketizer init splits inputcol outputcol	1.000000
mixin for param rawpredictioncol raw prediction a	has raw prediction	0.333333
with a	has param	0.019231
the sort	sql data frame sort	0.250000
py or zip dependency for	py file	0.066667
the month which the given date	date	0.037037
a python	from	0.045455
featureindex	feature index	1.000000
deserialized objects from the	serializer load	0.083333
number of classes values which the	ml java classification model num classes	0.250000
onevsrestmodel create	one vs rest model	0.058824
root mean squared error which is defined as	regression	0.010000
databases tables functions etc	catalog	0.062500
keyword	keyword	1.000000
number of months between	months between	0.333333
unifying data	union	0.090909
the rdd into a	core rdd	0.003460
this distributed model to a local representation this	distributed ldamodel to local	0.111111
block matrices together the matrices must have	block matrix add other	0.500000
wait	await termination or timeout timeout	0.125000
c{other}, return a resulting rdd that contains a	rdd cogroup other numpartitions	0.066667
model trained on	model	0.005587
new profiler using class	profiler collector new profiler	0.333333
can be used again to wait	query manager reset	0.011905
this instance contains a param with	ml param params has	0.019231
model from the given	mllib power iteration clustering model	0.500000
set named options filter out those the value	set opts schema	0.333333
number of nonzero	mllib linalg sparse vector num	0.200000
distribution	distribution	1.000000
awaitanytermination() can	streaming query	0.010526
set a	core spark	0.010309
python code for a shared param class	param gen param code name	0.333333
na fill()	data frame fillna value subset	0.166667
objects from the	serializer load	0.083333
creates a :class dataframe from an :class	sql spark session create	0.142857
l{statcounter} object that captures the mean	core rdd stats	0.083333
with the frame	range	0.030303
the	external merger object	0.032258
index value pairs or two	mllib linalg	0.026316
return the column mean	mllib standard scaler model mean	0.125000
each cluster	clustering summary cluster	1.000000
note : experimental	linear svcmodel	0.500000
loads parquet files returning the result as a	reader parquet	0.200000
system using the old hadoop	save as hadoop	0.142857
the dot product of two vectors we support	ml linalg dense vector dot other	0.090909
coordinatematrix	coordinate	0.200000
dataframe	data	0.011628
is set to a different	spark context set	0.166667
skipping null values	sql	0.005051
list of conditions and returns one of	sql column otherwise	0.050000
spark_home	spark home	0.500000
objective	ml logistic regression training summary objective	1.000000
can be	query manager	0.011905
with a dependency on another module on	module dependency on	0.142857
map of words to their vector representations	model get vectors	0.142857
until any	streaming query manager await any termination	0.142857
sort the	py spark streaming test case sort result	0.333333
to their vector representations	word2vec model get vectors	0.166667
whether to fit	fit	0.100000
if computeu was set to be	u	0.111111
a list of numpy arrays	ml bisecting kmeans model	0.076923
convert each python object into	to	0.023077
which is a dataframe having two	logistic regression	0.040000
create a new hivecontext for testing	sql hive context create for testing cls	0.333333
area under the precision-recall curve	mllib binary classification metrics area under pr	0.333333
__init__(self estimator=none estimatorparammaps=none evaluator=none	validator init estimator estimatorparammaps evaluator	1.000000
a dummy params instance used as a placeholder	params dummy	0.111111
convert the java_model to a python	java_model	0.090909
a pearson's independence test using dataset	ml chi square test test dataset	0.333333
accuracy	accuracy	0.461538
this thread such as the spark fair	core spark context	0.011628
which predictions are	isotonic regression	0.090909
data of another dstream with	other	0.033333
name	params has	0.019231
to set k decayfactor timeunit	streaming	0.005025
uid	param params reset uid	0.333333
update the	update	0.055556
sets the sql context	context sqlcontext	0.083333
gaussians in mixture	mllib gaussian mixture model k	0.200000
new hadoop outputformat api mapreduce package	save as new apihadoop dataset conf keyconverter valueconverter	0.142857
list of values	linalg dense vector values	0.200000
a boolean	boolean value	0.500000
compute the dot product of	dense vector dot other	0.050000
obj assume that all	obj	0.023810
waits for the termination of this	termination timeout	0.041667
if observed is vector conduct pearson's chi-squared goodness	mllib stat statistics chi sq	0.066667
find the minimum	min	0.041667
of write() save path	ml pipeline save path	0.200000
columns for	columns	0.019608
the accumulator's	accumulator	0.025974
setparams(self inputcol=none outputcol=none) sets params for this tokenizer	tokenizer set params inputcol outputcol	1.000000
returns the soundex encoding for a string >>>	soundex col	0.055556
train the model on the incoming dstream	streaming linear regression with sgd train on dstream	1.000000
lda keeplastcheckpoint is set to	ml distributed ldamodel	0.050000
make predictions on the	predict on	0.058824
again to wait	sql streaming query manager reset	0.011905
this dataset checkpointing can be used to	frame checkpoint eager	0.071429
be	manager reset	0.011905
evaluates the output with	ml evaluator evaluate dataset	0.142857
computeu	computeu	1.000000
representation	mllib linalg	0.105263
for each original column during	original	0.047619
the number of rows	indexed row matrix num rows	0.200000
converts vector columns in an input dataframe	convert vector columns from ml	0.166667
products for all	products for	0.500000
list or gets an	sql column get	0.142857
word in	mllib word2vec	0.125000
hivecontext for testing	for testing cls sparkcontext	0.333333
lda keeplastcheckpoint	ml distributed ldamodel	0.050000
how much	external merger object size	0.032258
[0 0 1 0] for feature selection by	mllib chi sq selector	0.150000
the accumulator's value only usable	core accumulator value value	0.050000
computes the sum of	ml bisecting kmeans model	0.076923
streaming dataframe/dataset is written to a	writer output mode outputmode	0.083333
accumulator's data	accumulator	0.012987
the l{sparkcontext} that this rdd was created on	core rdd context	0.166667
underlying output data source	sql data frame writer format source	0.333333
paired rdd where the first element is	factorization	0.038462
all the objects	merger	0.025641
the levenshtein distance of the two	sql levenshtein left right	0.058824
compare 2 ml types	test compare	0.166667
the dependent variable given	mllib linear regression model base	0.200000
this rdd	rdd	0.003058
column	getitem item	1.000000
prediction	prediction	0.291667
min value for each original column during fitting	max scaler model original min	0.250000
transforms the embedded params	params	0.006623
convert each python object into	rdd to	0.200000
:param rdd an rdd	train cls rdd	0.250000
so that	query	0.010753
train a gradient-boosted trees model for classification	mllib gradient boosted trees train classifier cls	0.333333
initsteps	init steps	1.000000
a class inherit documentation from	mllib inherit doc	0.045455
be used again	streaming	0.005025
this coordinatematrix	mllib linalg coordinate matrix	0.250000
mixin for param checkpointinterval set checkpoint	has	0.011628
columns in an input dataframe from the	columns to ml	0.125000
length of a string or binary expression	sql length col	0.050000
returns r^2^, the coefficient of determination	mllib regression metrics r2	0.166667
much of memory	core	0.003021
be used in sql statements	returntype	0.071429
an exception if any error	mllib linalg	0.026316
separate arrays of	ml linalg	0.030303
each cluster	summary cluster	1.000000
function "func" and a	core	0.003021
compute the standard deviation of this rdd's	stdev	0.047619
given parameters in this grid to fixed	grid builder base on	0.076923
specified partitioner	by numpartitions partitionfunc	0.250000
create	sql hive context create	0.083333
minutes of	sql minute	0.050000
get or compute the number of	mllib linalg row matrix num	0.125000
instance contains a	param params has param	0.019231
queries so that	sql streaming query manager reset	0.011905
this vector to the new mllib-local representation	linalg sparse vector as ml	0.333333
for each key using a custom set of	by key	0.026316
in libsvm format into label	mllib mlutils parse libsvm	0.125000
for each original column during fitting	min max scaler model original	0.062500
cache	cache	0.750000
__init__(self featurescol="features", labelcol="label", predictioncol="prediction",	decision tree classifier init featurescol labelcol predictioncol	1.000000
registered with the dispatch to handle all	cloud pickler save	0.166667
multi-dimensional rollup for the current :class dataframe	sql data frame rollup	0.055556
process	process	1.000000
boundaries in increasing order for which predictions	ml isotonic regression model boundaries	0.333333
extract the year of a given date	year col	0.050000
a dataframe as	sql data frame corr	0.166667
extract the day of the month of	sql dayofmonth col	0.031250
optional parameters	parameters	1.000000
perform a left outer join of	left outer join other numpartitions	0.111111
gets summary e g	summary	0.097561
a multi-dimensional rollup for the current :class dataframe	frame rollup	0.055556
data	external group	0.045455
return a sampled subset of this rdd	core rdd sample withreplacement fraction seed	1.000000
stratified sample	frame sample	0.066667
from the input java	ml java estimator	0.200000
setparams(self numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")	quantile discretizer set params numbuckets inputcol outputcol relativeerror	1.000000
2 ml params instances	params	0.006623
as non-persistent and	unpersist	0.083333
the content of the dataframe in a	data frame	0.005000
a	params	0.013245
how much of memory for this	merger object	0.032258
perform a left outer join of	rdd left outer join	0.111111
terminations	sql	0.002525
attr predictions which gives the predicted	generalized linear regression summary prediction	0.250000
the spark context call site	sccall site sync	0.200000
converts vector columns in	convert vector columns from	0.166667
input dataset this is called by the default	dataset	0.020408
applies transformation on a vector or an rdd[vector]	mllib java vector transformer transform vector	1.000000
converts a labeledpoint to	mllib mlutils convert labeled point to	0.250000
the rdd partitioned using the specified partitioner	rdd partition by numpartitions partitionfunc	0.333333
to make predictions on batches of	linear algorithm predict on	0.066667
of the :class dataframe	sql data frame writer	0.046512
the greatest value of the list of column	sql greatest	0.055556
goodness of	mllib stat statistics	0.125000
year of a given date as	year	0.040000
the :class dataframe as the specified	data frame writer save as	0.071429
in :py attr predictions which	linear regression	0.040000
compute the standard deviation of this rdd's	core rdd stdev	0.066667
an rdd that has no partitions or elements	core spark context empty rdd	0.200000
vector columns in an input dataframe	vector columns from	0.142857
optional	params	0.006623
data of	sql data	0.024390
ignore separators inside brackets pairs e g	ignore brackets	0.250000
attr lda keeplastcheckpoint	ldamodel get	0.066667
format into an rdd of labeledpoint	load lib svmfile sc	0.125000
into	external	0.013889
contains a param with a given string name	params has	0.019231
new	streaming query manager reset	0.011905
left singular vectors of the	linalg singular	0.017544
vectors or transform	hashing tf transform	0.045455
the termination of this query	termination	0.035714
minutes of a given date as	sql minute	0.050000
the first n rows to the console	frame show n truncate	0.333333
an integer	n	0.027778
are the right singular	singular	0.015625
returns a paired rdd where	matrix factorization	0.040000
of the dataframe	sql data frame	0.005348
from a single kafka	offset	0.021739
extract the year	sql year	0.050000
computes column-wise summary statistics for	statistics col stats	0.200000
convert matrix attributes which are	mllib linalg matrix convert	0.166667
schema	schema	0.433333
a python topicandpartition to map to the	topic partition	0.055556
disks	group	0.025641
save this rdd as a text file	core rdd save as text file path	1.000000
samples drawn	std numrows	0.125000
__init__(self formula=none featurescol="features",	ml rformula init formula featurescol	1.000000
conduct pearson's chi-squared goodness of fit	stat statistics chi sq	0.066667
already partitioned data into disks	group	0.025641
the content of the dataframe	data frame	0.005000
the :class dataframe in parquet	data frame writer parquet	0.200000
a given string name	params has param	0.019231
that all the objects	merger object	0.032258
a multi-dimensional cube for the current :class	frame cube	0.055556
levenshtein distance of the	levenshtein left	0.058824
a python parammap into a java parammap	to java pyparammap	0.250000
this thread such as the spark fair	spark	0.013158
rdd, a	schema samplingratio verifyschema	0.029412
bucketizer	bucketizer	0.750000
of predicted ratings for input user	mllib matrix factorization model predict all user_product	0.050000
model	mllib streaming logistic regression	0.500000
number of columns	matrix cols	0.333333
value in c{self} that is not contained in	subtract other numpartitions	0.111111
sample without replacement based on	sample	0.050000
compute the standard deviation of this rdd's	rdd stdev	0.066667
sets	classifier params set	1.000000
"zero value"	seqfunc combfunc numpartitions	0.500000
squared distance from a	mllib linalg sparse vector squared distance	0.166667
model to the input dataset this	dataset	0.020408
the initial value of	with sgd set initial	0.111111
in utc returns another timestamp that corresponds to	sql from utc timestamp timestamp tz	0.166667
rdd of labeledpoint	mlutils load lib svmfile sc path	0.125000
itemsets	itemsets	1.000000
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance",	decision tree regressor	0.058824
vector class for passing data to mllib	vector	0.019231
the java related object	init host port	0.200000
predictions which gives the predicted	linear regression summary prediction col	0.142857
java udf so it	java	0.012195
instance with a randomly generated uid and	cross validator	0.045455
wait until any of the queries on the	query manager await any termination timeout	0.166667
given parameters in this grid	param grid builder	0.055556
the bisecting k-means algorithm return	mllib bisecting kmeans train rdd	0.333333
use for saving	mlwriter	0.062500
sets random	mllib word2vec set	0.200000
generates an rdd comprised of vectors containing	mllib random rdds exponential vector rdd sc mean	0.200000
the given parameters in	param grid builder	0.055556
can be	manager	0.011236
item factors in two columns id and	item factors	1.000000
can be used again to wait for	sql streaming query manager reset	0.011905
:class dataframe representing the database	sql data frame	0.005348
of	ml chi sq selector model	0.166667
gets the name of the	get	0.021739
versionadded : 1 5 0	lda	0.066667
a new java object	ml java wrapper new java obj java_class	0.333333
groupid	groupid	0.833333
column-major dense	dense	0.111111
the list of column names	sql	0.005051
make predictions on	streaming kmeans predict on	0.500000
paired rdd where the first element is the	factorization model	0.043478
values	size values	0.250000
for input user and product	model	0.005587
count of the rdd's elements in one operation	core	0.003021
shut down the sparkcontext	core spark context stop	1.000000
kafka	offset	0.021739
checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	ml random forest classifier	0.023256
py or zip dependency for all tasks	py	0.050000
all globals names read or written to	extract code globals	0.125000
__init__(self rawpredictioncol="rawprediction", labelcol="label", metricname="areaunderroc")	ml binary classification evaluator init rawpredictioncol labelcol metricname	1.000000
+= operator adds a term to this	core accumulator iadd term	0.142857
sets the accumulator's value only usable in	core accumulator	0.030303
new dstream in which each rdd	streaming streaming context transform	0.066667
extract the day of the month	sql dayofmonth	0.031250
load a model from the given	mllib java loader load cls sc	0.250000
the jobs started by this thread	job	0.023810
queries so that :func awaitanytermination()	sql	0.002525
dataframe from an :class rdd, a list or	schema samplingratio verifyschema	0.029412
objective	logistic regression training summary objective	1.000000
simple sparse vector class for passing	sparse vector	0.062500
locate the position of the first	locate	0.076923
computes column-wise summary statistics for the input rdd[vector]	statistics col stats	0.200000
the libsvm format into an rdd of labeledpoint	lib svmfile sc path numfeatures	0.125000
mlwriter for :py class javaparams types	mlwriter	0.062500
libsvm format	libsvm	0.181818
the objects	merger object	0.032258
demonstrate udt in only python	python only	0.500000
labels corresponding	model labels	0.166667
compute the standard deviation of this	core rdd stdev	0.066667
the :class dataframe to	frame writer save path	0.066667
sets the accumulator's value	accumulator value value	0.050000
of iterations default 1	iterations	0.043478
local representation this discards	local	0.038462
set number of batches after which the	mllib streaming kmeans set	0.142857
python broker to	streaming broker	0.200000
column of indices back to a new	index to	0.040000
collect the distributed matrix on the driver as	to local matrix	0.250000
memory string in the format supported by java	memory s	0.142857
g accuracy/precision/recall objective history total iterations) of model	ml logistic regression model	0.500000
ignore separators inside brackets pairs	ignore brackets split	0.250000
queries so that :func awaitanytermination()	streaming query	0.010526
two block matrices together the matrices	mllib linalg block	0.111111
returns a new java	ml java wrapper new java	0.333333
test that	streaming kmeans test test	0.500000
value to	to	0.030769
awaitanytermination() can be used again to wait	sql streaming	0.010204
weightcol=none aggregationdepth=2):	ml linear svc	0.333333
can be used again to	streaming query	0.010526
estimate of the importance of each feature	ml random forest classification model feature importances	0.250000
the root directory that contains	root directory cls	0.333333
a left outer join	rdd left outer join other numpartitions	0.111111
population should be	stat kernel density	0.200000
input option for	stream reader option key	0.333333
set a local property that affects	context set local property key	0.200000
the embedded params	params	0.006623
buckets the output by	writer bucket by numbuckets col	0.200000
select filter	selector model selected features	0.333333
so	streaming query manager	0.011236
for debugging	to debug string	0.200000
value of	ml param decision	1.000000
setparams(self	gaussian mixture set params	1.000000
columns in an input	columns from	0.125000
specific group matched by a java	str pattern idx	0.111111
list of conditions and returns	sql column otherwise	0.050000
curve which	binary logistic regression summary	0.222222
of partitions to use during reduce tasks	core rdd default reduce partitions	0.166667
an rdd comprised	random rdds log normal vector rdd	0.166667
a term to this accumulator's	accumulator add term	0.066667
layer sizes including input and output layers	multilayer perceptron classification model layers	1.000000
approximately	lshmodel approx	0.200000
the minutes of a given date as	minute	0.040000
returns the soundex encoding for	soundex col	0.055556
find synonyms of a	model find synonyms	0.333333
string in the format	s	0.071429
converts vector columns in	convert vector columns from ml	0.166667
all mixture	mllib gaussian mixture model	0.062500
to an int	to int value	0.250000
labels corresponding to indices to be assigned	model labels	0.166667
stream and	stream	0.017544
c{self} and c{other}	join other	0.071429
the list of column names skipping null values	sql	0.005051
even if users construct taskcontext instead of using	core task context new	0.333333
instance contains	params	0.006623
again to wait for	streaming query manager	0.011236
of names of	table names	0.066667
rdd as non-persistent and	rdd unpersist	0.066667
an rdd comprised of	mllib random rdds exponential vector rdd	0.166667
representing a multiclass classification model	linear classification model	0.076923
:class dataframe to a data source	sql data frame writer save	0.083333
[[pcamodel]] that contains the principal components of	mllib pca fit data	0.166667
setparams(self statement=none) sets params for this	set params statement	0.333333
right outer	full outer	0.333333
for new terminations	sql streaming query manager reset	0.011905
json	json path mode	0.125000
the distributed matrix on the driver as	matrix to local matrix	0.250000
dataframe produced	ml clustering	0.100000
this model	regression model	0.062500
the database table named table	table	0.031250
the contents of the :class dataframe to a	frame writer save	0.066667
for rformula	ml rformula	0.250000
year of	sql year	0.050000
a left outer join of c{self}	left outer join other	0.111111
system using the old hadoop outputformat api mapred	save as hadoop dataset conf keyconverter valueconverter	0.083333
selector type of the chisqselector	mllib chi sq selector set selector type selectortype	0.333333
submit and test a single script	submit tests test single script	0.500000
of memory for	external merger object	0.032258
which is a risk function corresponding to	regression	0.020000
pairs or two	linalg	0.044444
of this query either by :func	sql streaming	0.010204
serializes a stream of list of pairs split	serializer	0.062500
transpose this coordinatematrix	linalg coordinate matrix transpose	1.000000
value of	ml vector indexer	0.200000
test that the model predicts correctly on	streaming kmeans test test predict on model	0.500000
evaluates the	evaluator evaluate dataset	0.285714
sets	elastic net param set	1.000000
:py class pyspark.ml.linalg.vector	vec	0.166667
total log-likelihood for this model on the given	gaussian mixture summary log likelihood	0.142857
until any of	sql streaming query manager await any	0.142857
of indices to	ml chi sq	0.100000
length	sql length col	0.050000
comprised of vectors containing i	mllib random rdds normal	0.125000
maxiter max number of iterations (>= 0)	max iter	0.166667
list	list of list	0.333333
returns the documentation of	ml param	0.009524
format or newline-delimited json <http //jsonlines	json path mode compression dateformat	0.166667
registered with the dispatch to handle all function	core cloud pickler save function obj	0.142857
save this model to the given path	model save sc path	0.500000
loads vectors saved using rdd[vector] saveastextfile	mllib mlutils load vectors sc	1.000000
the spark fair scheduler pool	core spark context	0.011628
table based on	table tablename path	0.166667
this instance contains a param with	params has param	0.019231
for	manager reset	0.011905
data of a streaming dataframe/dataset is written	data stream writer output mode outputmode	0.333333
verify the attempt numbers are correctly reported	context tests test attempt number	0.333333
rdd by applying c{f}	rdd key by f	1.000000
based on first value	based on	0.111111
to wait for new	sql streaming query manager reset	0.011905
sort the list based	test case sort result based	1.000000
convert a list of	to	0.007692
returns weighted true positive	multiclass metrics weighted true positive	1.000000
the :class dataframe to a	frame	0.034483
csv file	csv path schema sep	0.333333
regression model derived from	regression model	0.031250
gradient	gradient	1.000000
creates a model from the input java model	ml java estimator create model	1.000000
rdd that is reduced into numpartitions partitions	rdd coalesce numpartitions shuffle	0.500000
load a model from the given	load cls sc	0.214286
an rdd for dataframe from a	from	0.045455
partial objects do not	partial obj	0.125000
be used	query manager	0.011905
or compute the number	row matrix num	0.100000
version of a heappush followed by a heappop	heap item	0.125000
globals names read or written	pickler extract code globals	0.125000
list of conditions and	column otherwise value	0.200000
broadcast a	broadcast	0.052632
values for each key	key	0.035714
local property set	get local property key	0.066667
with a function and	sql user defined function	0.083333
:class dataframe representing the result	sqlquery	0.027027
number of top features	num top features	1.000000
batch has half the	half	0.058824
right outer join of	full outer join other numpartitions	0.111111
for dataframe from	from	0.045455
an external list	external list of	0.166667
a param and	param param	0.100000
:py attr n	n value	1.000000
the least value of the list	least	0.043478
day of the month of a	sql dayofmonth col	0.031250
be used in sql	sql sqlcontext	0.095238
mixture	gaussian mixture model predict	0.100000
in the rdd	rdd	0.003058
piping elements to a forked external process	pipe command env checkcode	0.166667
the spark sink deployed on a	storagelevel maxbatchsize	0.045455
a given combine functions and a neutral	zerovalue	0.076923
param by its name	param paramname	0.111111
how much of memory	merger object	0.032258
coefficient	coefficient	1.000000
cleanup	cleanup	1.000000
user-defined type udt for examplepoint	python only udt	1.000000
for a given product and returns a list	product	0.029412
return all merged items as iterator	core external merger items	1.000000
an external database table	table mode	0.200000
transfer this instance's params to the	params to	0.035714
of memory	merger	0.025641
version of this dataset checkpointing can	frame checkpoint eager	0.071429
fitted models	model	0.005587
applies transformation on a	vector transformer transform	0.500000
of this instance with a randomly generated	ml one	0.083333
merge the values for each key	key func numpartitions partitionfunc	0.066667
basic operation test for dstream combinebykey	basic operation tests test combine by key	1.000000
a python rdd of	core rdd save as	0.037500
awaitanytermination() can be used again to wait for	sql streaming query	0.011765
passed as a list of	spark conf	0.058824
total number	total num	0.333333
parameters in this grid to fixed values	ml param grid builder base	0.076923
sql context to use for saving	ml mlwriter context sqlcontext	0.333333
version of this dataset checkpointing	checkpoint eager	0.250000
specifies the underlying output	data frame writer	0.014085
replaces a local temporary view with	replace temp view name	0.333333
deviance for the fitted	linear regression summary deviance	0.125000
prefixspan algorithm to mine frequent sequential patterns	prefix span	0.166667
returns an active query from this	sql	0.002525
a randomly generated	cross validator model	0.050000
an input stream	stream ssc hostname port storagelevel	0.200000
is not contained in	core rdd subtract	0.333333
onevsrestmodel	one vs rest model from	0.142857
a param with	param params has param	0.019231
which is a dataframe having two fields	regression	0.010000
value"	seqfunc combfunc numpartitions	0.500000
generates an rdd comprised of	random rdds uniform rdd sc size numpartitions seed	0.200000
level to persist its values across operations after	rdd persist storagelevel	0.166667
0] for feature selection by	chi sq selector	0.125000
index of the	with index	0.100000
transforms a	java params transfer	0.250000
arrays of indices and	ml linalg	0.030303
to stdout	profiler show	0.166667
jvm view associated with sparkcontext must be called	jvm	0.166667
list of conditions and returns one of multiple	sql column otherwise value	0.050000
converts matrix columns in an input	mlutils convert matrix columns from	0.166667
general pyspark tests that depend on scipy	sci py tests	0.250000
sort the list based on first	case sort result based on	0.333333
computes the sum of	ml bisecting	0.066667
for input user	model	0.005587
list of tables/views in the specified database	catalog list tables dbname	1.000000
awaitanytermination() can be used again	sql streaming query manager reset	0.011905
load labeled points saved using	mlutils load labeled points sc path minpartitions	0.250000
produced by the model's transform method	ml clustering summary predictions	0.500000
the libsvm format into an rdd of labeledpoint	load lib svmfile sc	0.125000
comprised of vectors containing i i d	mllib random rdds gamma vector	0.125000
tests whether this	paramname	0.076923
pass else fail with	test case eventually	0.500000
brackets pairs	brackets split	0.083333
python rdd of key-value pairs (of	core rdd save	0.037975
that makes a class inherit documentation from	inherit doc cls	0.045455
the python direct kafka stream api with start	kafka direct stream	0.055556
create a new dstream in which each rdd	streaming streaming context transform	0.066667
feature	mllib	0.010526
input path	path	0.020408
using an associative and commutative reduce function but	rdd reduce by	0.200000
that	core	0.003021
test that the	test training and	0.500000
inclusive to end inclusive	end	0.133333
so that :func awaitanytermination() can	streaming	0.005025
lshmodel	lshmodel	1.000000
them with extra values	map extra	0.040000
into an rdd of labeledpoint	lib svmfile	0.125000
sparkcontext	spark context	0.069767
return a jvm scala map from a dict	data frame jmap jm	0.111111
prints the first n rows to the console	data frame show n	0.333333
returns a :class	sql	0.005051
construct a taskcontext use get instead	core task context	0.100000
a local property set in this thread or	core spark context get local property key	0.066667
columns in an input dataframe	columns to	0.125000
the dot product of two vectors we	dot	0.040000
for which predictions	isotonic regression model	0.100000
partitioned using the specified partitioner	partition by numpartitions partitionfunc	0.250000
compute the number of	mllib linalg coordinate matrix num	0.166667
again	sql streaming	0.010204
a mllib vector	vector	0.019231
instance contains a param	ml	0.001835
of linear	ml linear	0.066667
of numpy	ml bisecting kmeans model	0.076923
column from one base to	col frombase tobase	0.166667
"zerovalue"	fold by	0.125000
python topicandpartition to map to the	topic and partition init topic partition	0.055556
a label indexer	indexer	0.055556
synonyms of a word	synonyms word num	1.000000
a list of	spark	0.013158
sets	col set	1.000000
by other,	multiply	0.100000
setparams(self	bisecting kmeans set params	1.000000
test the python direct kafka stream api	stream tests test kafka direct stream	0.250000
for input user and product pairs	model	0.005587
optional default value and user-supplied value in a	param params	0.014925
this obj assume that all the	external merger object size obj	0.040000
python code for a	code name	0.111111
of this instance this updates both	ml param params	0.013699
for the stream query	sql data stream	0.031250
loads a csv file stream and	stream reader csv path schema sep encoding	0.500000
stop the	context stop	0.125000
the model's transform method	summary predictions	0.230769
a list of conditions and returns one	sql column otherwise	0.050000
pipeline	pipeline from	0.142857
nodes	nodes	0.259259
the dot product of two vectors we support	mllib linalg dense vector dot other	0.058824
vectors or transform the	tf transform	0.045455
the key-value pair rdd through a flatmap	rdd flat map values	0.333333
the list based on first	based on key outputs	0.111111
compute the	mllib linalg coordinate matrix	0.250000
codeblock co	cls co	0.333333
python code	code name doc defaultvaluestr	0.111111
relativeerror	relativeerror	1.000000
the minutes	minute col	0.050000
user-defined type udt for examplepoint	example point udt	1.000000
test for data sampled from	test data	0.166667
approx	approx	0.238095
sort the list based on first	test case sort result based on key	0.333333
infer schema from an rdd of row or	infer schema rdd samplingratio	0.250000
that all	external	0.013889
heapreplace	heapreplace	1.000000
data or table already exists	data frame	0.005000
converter to drop the names of fields	converter datatype	0.071429
a left outer join of	rdd left outer join other numpartitions	0.111111
the spark session to use for loading	mlreader session sparksession	0.333333
new terminations	sql	0.002525
awaitanytermination() can be used again to wait for	query	0.010753
a :class column	data frame getitem item	0.250000
this instance contains a param with	params has	0.019231
for this obj assume that	object size obj	0.040000
uid	param params reset uid newuid	0.333333
instance's params to	params to	0.035714
namedtuple	namedtuple	0.833333
an rdd	poisson vector rdd	1.000000
value pairs or two	mllib linalg	0.026316
test python direct kafka rdd messagehandler	stream tests test kafka rdd message handler	1.000000
python direct kafka stream api with	kafka direct stream from	0.125000
queries so	reset	0.011236
columns	columns from ml dataset	0.125000
returns a paired rdd where the first	factorization	0.038462
reader	reader	0.200000
return the	py spark streaming	0.333333
test that coefs are predicted accurately by	tests test parameter accuracy	0.333333
the termination of	termination	0.035714
data	core external group by	0.045455
property that	property key	0.066667
adds a term	core accumulator add term	0.066667
the given parameters in this grid	param grid builder base	0.076923
this vector to the new mllib-local representation	vector as	0.250000
reset	reset	0.056180
assert both have the same param	m2 param	0.125000
infer schema from an rdd of	sqlcontext infer schema rdd	0.250000
called when processing of a job of	streaming streaming listener on output operation	0.166667
evaluates a list of conditions and returns one	sql column otherwise	0.050000
compare 2 ml params instances	compare params m1	0.200000
test this should be list	test	0.015152
compute the	mllib linalg block matrix	0.052632
returns the soundex encoding for a string	sql soundex	0.055556
dot product of two vectors we	dot	0.040000
the index of	with index	0.100000
of clusters	kmeans model k	0.250000
which predictions are	ml isotonic regression model	0.125000
a column containing a json string	json col	0.083333
accumulator's value only usable in driver program	accumulator value	0.050000
this instance	has	0.011628
queue of	streaming context queue	0.500000
in multinomial logistic regression	mllib logistic regression model	0.083333
the accumulator's value only usable in driver program	core accumulator	0.030303
the python direct kafka stream api	kafka direct stream	0.111111
computes an fp-growth model that contains frequent itemsets	mllib fpgrowth train cls	0.100000
which is	logistic regression summary	0.045455
using rdd[vector] saveastextfile	mlutils	0.125000
that :func awaitanytermination() can be used again to	streaming query manager	0.011236
rdd 'x' has maximum membership in	mllib gaussian mixture	0.045455
of the :class dataframe to a data source	sql data frame writer save path	0.142857
:class	data frame	0.030000
randomly generated	train validation split	0.166667
representing a user defined function udf	sql udf f returntype	0.200000
a given string	ml param	0.009524
can be used again to wait for new	manager	0.011236
kmeans algorithm	streaming kmeans	0.035714
broadcast a read-only variable to the	context broadcast	0.125000
the standard deviation	core rdd stdev	0.066667
norm	dense vector norm p	0.333333
dataset for the test	test	0.015152
train a random forest model for binary	mllib random forest train classifier cls data	0.250000
sets	regressor params set	1.000000
buckets the output by	writer bucket by numbuckets	0.200000
:func awaitanytermination() can be	streaming query	0.010526
deviance for the	summary deviance	0.125000
in	sql conv	0.250000
the ensemble	mllib tree ensemble	0.111111
two fields threshold recall curve	ml binary logistic regression summary recall by threshold	0.166667
data into disks	spill	0.038462
returns weighted false positive rate	multiclass metrics weighted false positive rate	1.000000
na fill()	frame fillna value subset	0.166667
for which	regression	0.010000
code for a shared param class	ml param gen param code name doc defaultvaluestr	0.333333
the stream query if this	sql data stream	0.031250
stream query if	stream writer	0.041667
create an rdd that has no partitions	core spark context empty rdd	0.200000
compute the number of rows	mllib linalg block matrix num rows	0.200000
day of the month of a given	dayofmonth	0.027027
onevsrest create and return a python wrapper	one vs rest from	0.142857
use only create a new hivecontext for testing	hive context create for testing cls	0.333333
calculates the norm of a sparsevector	vector norm p	0.055556
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params for this	set params maxcategories inputcol outputcol	0.333333
tests whether this instance contains a param with	param params has param paramname	0.142857
to all mixture	gaussian mixture model	0.052632
an iterator of deserialized batches lists of	without unbatching	0.125000
directory for spill	path	0.010204
of the dataframe in a	data frame writer	0.014085
frequency vectors or transform	hashing tf transform	0.045455
1 0] for feature selection by fwe	mllib chi sq selector set fwe fwe	0.200000
get or compute	linalg row matrix	0.200000
or transform the	tf transform	0.045455
a column containing a json	sql from json col	0.083333
in mixture	gaussian mixture	0.038462
the :class statcounter members as a dict	core stat counter as dict	0.333333
to term frequency vectors or	hashing tf	0.125000
values for each numeric columns for	grouped data avg	0.058824
residual degrees of freedom for the null	linear regression summary residual degree of freedom null	0.333333
each	by	0.028571
dispatch to handle all function types	cloud pickler save function obj	0.142857
0 weightcol=none aggregationdepth=2):	ml linear svc	0.333333
the model to make predictions on batches of	streaming linear algorithm predict on	0.066667
mllib vector	vector value	0.333333
a certain time of day in utc returns	sql from utc	0.142857
defines	window spec range between	1.000000
a java udf so it	java	0.012195
formula=none featurescol="features", labelcol="label",	formula featurescol labelcol	0.400000
or	context get or	0.200000
for which predictions	ml isotonic regression model	0.125000
partitioned data into disks	core	0.003021
that has no partitions	spark context empty	0.333333
column of :class pyspark sql types stringtype or	sql to date col format	1.000000
the year of a	sql year col	0.050000
be used with the spark sink deployed on	maxbatchsize	0.037037
eventually	eventually	1.000000
set initial centers should be set	streaming kmeans set initial centers centers	0.200000
boosted trees model for	boosted trees	0.166667
subsampling	subsampling	1.000000
of data points in each cluster	ml clustering summary cluster sizes	0.333333
from the given path the model	path	0.010204
called when processing of a batch of jobs	listener on batch	0.333333
maxcategories=20 inputcol=none outputcol=none)	maxcategories inputcol outputcol	1.000000
contains a param	has	0.011628
the uid	uid	0.125000
given user	user	0.055556
number	linalg indexed row matrix num	0.100000
partitioned data into	external group by spill	0.047619
generates an rdd comprised of	mllib random rdds uniform vector rdd sc	0.200000
python rdd of key-value pairs	core rdd	0.010381
generates an rdd comprised	random rdds gamma vector rdd sc	0.200000
with this dataframe	sql data frame	0.005348
of the specified string value that match regexp	sql regexp	0.125000
convert this matrix to an indexedrowmatrix	mllib linalg block matrix to indexed row matrix	0.333333
which the centroids of that	timeunit	0.025641
by	core	0.003021
registered with the dispatch to handle all function	cloud pickler save function	0.142857
each original column	model original	0.062500
c{self} and c{other}	core	0.006042
the key-value pair rdd through a flatmap function	core rdd flat map values f	0.333333
for feature selection by number of top features	mllib chi sq selector set num top features	0.500000
0 weightcol=none	ml linear	0.133333
this dataset checkpointing	checkpoint eager	0.250000
given parameters in this grid to fixed	ml param grid builder base	0.076923
features corresponding to that	features	0.043478
mixture	mixture model	0.133333
recall curve	binary logistic regression summary recall by	1.000000
lambda_	lambda_	1.000000
rdd containing the distinct	distinct	0.055556
saving	java mlwriter	0.250000
month of a given date as integer	dayofmonth col	0.031250
:py attr lda keeplastcheckpoint	ldamodel get	0.066667
distributed model to	ml distributed ldamodel to	0.166667
training set given the	training	0.029412
precision curve	binary logistic regression summary precision by	1.000000
jobs started by this	job	0.023810
already partitioned data into disks	core external	0.016129
:py attr predictions which	linear regression	0.040000
this thread such as the spark fair scheduler	spark context	0.023256
a java array of given java_class type useful	ml java wrapper new java array pylist java_class	0.333333
gaussians in mixture	mixture model k	0.200000
set bandwidth of each sample defaults	stat kernel density set bandwidth bandwidth	0.142857
decayfactor timeunit to configure the kmeans algorithm for	kmeans	0.025641
a given string name	ml param params has	0.019231
an exception if any error	mllib	0.010526
can be used	sql streaming query	0.011765
return a l{statcounter} object that captures the mean	rdd stats	0.083333
an exception if	mllib linalg	0.026316
set initial centers should be set before calling	initial centers centers weights	0.200000
given product and returns	product	0.029412
of a batch of jobs has	batch	0.068966
given a java	from java cls	1.000000
an :class pyspark rdd of :class row	data frame rdd	0.500000
into an rdd of labeledpoint	lib svmfile sc path numfeatures	0.125000
underlying sql storage type for this udt	user defined type sql type cls	0.500000
of day in utc returns	sql from utc	0.142857
build	build	1.000000
computes hex value of the given column which	sql hex col	0.166667
parameters in this grid to	grid builder base on	0.076923
perform a right outer join of c{self}	rdd full outer join other numpartitions	0.111111
how much of memory for this	core external merger	0.032258
the content of the :class dataframe to the	sql data frame writer	0.011628
items for columns	items	0.066667
for data sampled from	data	0.011628
return the column mean	standard scaler model mean	0.125000
create a new profiler using class profiler_cls	collector new profiler ctx	0.333333
of numerical	quantile col probabilities relativeerror	0.166667
of a coordinatematrix	matrix	0.015152
splits str around pattern pattern is a	split str pattern	0.333333
values of the accumulator's data type	accumulator	0.012987
setparams(self numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")	ml quantile discretizer set params numbuckets inputcol outputcol	1.000000
scipy sparse matrices if scipy is available	sci py	0.500000
the levenshtein distance of the two given	sql levenshtein left right	0.058824
deviance for	linear regression summary deviance	0.125000
2 ml params instances for the given param	params m1 m2	0.047619
root mean squared error which is defined as	mllib regression	0.022727
add a py	add py file path	0.166667
chisquared feature selector	chi sq selector	0.041667
setparams(self min=0 0 max=1 0	set params min max	1.000000
computes column-wise summary statistics for the input	statistics	0.090909
a local property set	spark context get local property key	0.066667
param with a given string	ml param params has	0.019231
new :class column for distinct count	count distinct col	0.040000
the initial value	logistic regression with sgd set initial	0.111111
1 0] for feature selection by fdr	mllib chi sq selector set fdr fdr	0.200000
the most recent [[streamingqueryprogress]] updates	recent progress	0.111111
test of the	test	0.015152
returns a paired rdd where the	factorization	0.038462
relativeerror	relative error	1.000000
terms	ldamodel	0.034483
the accumulator's value	accumulator value value	0.050000
itemscol items column name	items col	0.500000
one operation	core	0.003021
according	decayfactor	0.142857
labels corresponding to indices	indexer model labels	0.166667
a java storagelevel based	get java	0.111111
new :class dataframe sorted by	data frame sort	0.125000
copy all params defined on the	ml param params copy params	0.200000
the second is an array	mllib	0.010526
the sort	data frame sort	0.125000
load a linearregressionmodel	regression model load cls sc path	1.000000
distinct count of col or cols	count distinct col	0.040000
sets	gaussian mixture set	1.000000
or list	oneatatime	0.111111
than or equal	numiterations	0.050000
sets params for rformula	ml rformula set params	1.000000
the kolmogorov-smirnov ks test for data sampled from	mllib stat statistics kolmogorov smirnov test data distname	0.111111
converts vector columns in	mlutils convert vector columns to	0.166667
this	param	0.012500
specifies the input	data frame reader	0.166667
new rdd by applying a function	f	0.021053
of this rdd's elements	core	0.003021
the given parameters in this grid to	grid builder base	0.076923
param	ml param	0.019048
rows	rows	0.727273
a large dataset and an item approximately	lshmodel approx nearest neighbors dataset key	0.166667
the deviance for the fitted model	linear regression summary deviance	0.125000
an iterator of deserialized batches lists	stream without unbatching	0.125000
represents an entry of a coordinatematrix	matrix entry	0.250000
that all	merger object size	0.032258
with a given string	ml param params	0.013699
parse a string representation back into the	mllib linalg vectors parse s	0.333333
step every element	step numslices	0.333333
params instances for the	params	0.006623
time of day in utc	utc	0.100000
generates an rdd comprised	random rdds log normal vector rdd sc mean	0.200000
sum for each numeric columns	sql grouped data sum	0.083333
value of	ml random forest	0.142857
param with a given	ml param	0.009524
use for loading	ml mlreader	0.222222
called when a	streaming listener on	0.200000
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic", maxiter=20	gbtclassifier	0.076923
dependency on another module on	module dependency on	0.142857
add a py or zip dependency for	core spark context add py	0.166667
the correlation	col2 method	0.055556
accumulator's value only	accumulator	0.012987
java object by pyrolite whenever	java object rdd	0.500000
for this model	linear model	0.066667
returns a :class dataframe	spark session sql	0.250000
find synonyms of a	mllib word2vec model find synonyms	0.333333
params instances for the	params m1	0.047619
transforms a	ml java params transfer param map	0.250000
rowindices	rowindices	1.000000
returns the soundex encoding for	soundex	0.043478
in the training set given the current	distributed ldamodel training	0.034483
the day of the month of a	sql dayofmonth col	0.031250
used again to wait for new terminations	streaming	0.005025
from	context range	1.000000
for use in broadcast	sql broadcast	0.500000
instance contains a param	ml param	0.009524
or more examples	decision tree model	0.050000
in a text	text	0.076923
>>> dm = densematrix(2 2 range 4	linalg dense matrix	0.083333
write() save	pipeline save	1.000000
predict values for a single data	predict	0.068966
finding frequent items for columns possibly	freq items	0.166667
the content of the dataframe in	sql data frame writer	0.011628
applies a function to all	foreach f	1.000000
java	java wrapper new java	0.166667
a csv file	csv path schema	0.333333
the behavior when data or table	sql data frame writer mode savemode	0.071429
day of the month of a given date	dayofmonth	0.027027
instance with a randomly generated uid	cross validator model	0.050000
the spark fair scheduler pool	spark	0.013158
a dictionary a list of index value pairs	size	0.036697
instance for params shared by them	params copy	0.083333
right outer join of c{self}	full outer join other	0.111111
returns a :class dataframe representing the result of	spark session sql sqlquery	0.250000
maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0	maxdepth	0.100000
[[pcamodel]] that contains the principal components	mllib pca fit data	0.166667
calculates the correlation	col1 col2 method	0.055556
wait for new terminations	streaming query manager	0.011236
so that :func	sql	0.002525
train a regression model with l2-regularization	mllib ridge regression with sgd train cls	1.000000
ml params	params m1 m2	0.047619
used again to wait	manager	0.011236
the list based on first	based on key	0.111111
transforms the	ml transformer transform	0.500000
the python direct kafka	kafka direct	0.500000
compute the dot product of two vectors we	mllib linalg dense vector dot	0.058824
a value with another value	value	0.008547
area under the precision-recall	classification metrics area under pr	0.333333
until any of the queries on	query manager await any	0.142857
of estimated coefficients and intercept	ml generalized linear regression training summary	0.166667
sparkcontext	sc	0.062500
underlying data	sql data	0.146341
an external database table via jdbc	jdbc url table	0.090909
this tests	tests	0.100000
inputformat with arbitrary key and value	inputformatclass keyclass	0.125000
extract the minutes of a given date as	sql minute	0.050000
a unified dstream from multiple dstreams of the	streaming streaming context union	0.111111
much of memory	merger object size	0.032258
like '#,--#,--# --', rounded to	number col	0.500000
partial objects do not	cloud pickler save partial obj	0.125000
params instances for the given param and	params m1	0.047619
infer schema from an rdd of	sqlcontext infer schema rdd samplingratio	0.250000
the initial value of weights	set initial weights initialweights	0.333333
computes column-wise summary statistics for the	statistics	0.090909
generates a :py attr countvectorizermodel	count vectorizer	0.166667
transfer this instance to a java pipelinemodel used	ml pipeline model to java	0.100000
list of	catalog list	0.500000
the trigger for the stream query if	data stream writer trigger	0.083333
cachenodeids=false checkpointinterval=10 impurity="variance",	tree regressor	0.058824
is of type	ml ldamodel is distributed	0.066667
get or compute the number of cols	mllib linalg distributed matrix num cols	0.333333
columns in an input dataframe from	columns to ml dataset	0.125000
or compute the number of cols	mllib linalg distributed matrix num cols	0.333333
calculates the approximate	approx	0.047619
for new terminations	streaming query manager	0.011236
components	predict soft	1.000000
in sql	sql sqlcontext	0.095238
dataframe in	sql data frame	0.005348
list of conditions and returns one	sql column otherwise value	0.050000
instance contains a	has param	0.019231
return a copy of the rdd	core rdd	0.003460
to tf-idf vectors	mllib idfmodel transform	0.142857
matrix to an indexedrowmatrix	linalg coordinate matrix to indexed row matrix	0.333333
be used again to wait	query	0.010753
gaps	gaps	1.000000
hadoop configuration which is passed in	hadoop	0.050000
dataframe in a text	sql data frame writer text	0.200000
mark the rdd as non-persistent and	core rdd unpersist	0.066667
the levenshtein distance of the two	levenshtein	0.045455
the correlation of two	col2 method	0.055556
multiple parameters passed as a list	core spark conf set all	0.125000
"predictions" which gives the features of each	ml linear regression summary features	0.166667
make class generated by namedtuple picklable	core hack namedtuple cls	1.000000
parses a line in libsvm format into	mlutils parse libsvm line line multiclass	0.111111
of key-value pairs	all pairs	0.500000
defined on the class to current object	param	0.006250
comprised of	random rdds gamma vector	0.125000
0 1 0] for feature selection by percentile	chi sq selector set percentile percentile	0.200000
private abstract class representing a multiclass classification	linear classification	0.142857
create a new profiler using class	core profiler collector new profiler	0.333333
inherit documentation	mllib inherit	0.045455
sets the	set key	0.500000
the current idf vector	idfmodel idf	0.166667
returns the number of clusters	power iteration clustering model k	0.200000
hadoop-supported file system uri	file path	0.035714
month of a given date as	dayofmonth	0.027027
a column containing a json string	from json col	0.083333
create an input stream	utils create stream ssc	0.200000
returns weighted true positive rate	multiclass metrics weighted true positive rate	1.000000
arbitrary key and value class	core spark context	0.023256
the kmeans algorithm for	kmeans	0.025641
attr lda	ldamodel	0.034483
dot product of two vectors	linalg dense vector dot other	0.058824
checkpoint the dstream operations	checkpoint	0.062500
timestamp that	timestamp timestamp	1.000000
for	sql streaming	0.010204
in the blockmatrix	mllib linalg block matrix	0.052632
batches of data	streaming linear algorithm	0.076923
in tree	tree	0.020833
list or gets an item by key out	column get item key	1.000000
__init__(self inputcol=none	init inputcol outputcol	0.166667
of nonzero elements this	ml linalg dense	0.100000
value that match regexp	regexp	0.076923
list of active queries associated with this	manager active	0.066667
compute the dot product of	mllib linalg dense vector dot	0.058824
function to the	values f	0.062500
until any	query manager await any	0.142857
the correlation of	col2 method	0.055556
until any of the queries on	any	0.083333
dstreams	streaming	0.005025
an rdd containing all pairs of elements	core rdd	0.003460
value of :py attr	ml quantile	0.250000
withmean=false withstd=true inputcol=none outputcol=none)	withmean withstd inputcol outputcol	1.000000
commutative reduce function but	rdd reduce	0.071429
attr predictions which gives the	generalized linear regression summary	0.090909
save a linearregressionmodel	linear regression model save sc path	1.000000
sets	tf set	1.000000
this instance contains a param	ml param params has param	0.019231
sets	multilayer perceptron classifier set	1.000000
mean squared error which is defined as	regression	0.010000
index of	partitions with index	0.100000
in modlist to be placed into main	core modules to main modlist	0.333333
both have the same param	param	0.006250
list of active queries associated with this	sql streaming query manager active	0.066667
a param with a given string name	ml param params has	0.019231
master url to connect to	master value	1.000000
extract the minutes	minute	0.040000
conditions and	column otherwise	0.200000
functionality for working with missing	na functions	1.000000
lda	ml distributed ldamodel	0.050000
this instance with a randomly	one vs rest model	0.058824
python rdd	rdd save as	0.038462
multiclass classification model	linear classification model	0.076923
all params with their optionally default values and	param params explain params	0.250000
locate the position of	locate	0.076923
use only create a new hivecontext for testing	hive context create for testing	0.333333
labeledpoint	labeled point	0.500000
stream query if this is not set	data stream	0.028571
makes a class inherit documentation from its	inherit doc cls	0.045455
the minimum item in this rdd	core rdd min key	0.333333
of partitions to use during reduce tasks	rdd default reduce partitions	0.166667
instance contains	param	0.012500
vector columns in an input dataframe to the	vector columns from ml dataset	0.142857
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6	aftsurvival regression init featurescol labelcol predictioncol	0.500000
create a sparse vector using either	mllib linalg vectors sparse	0.166667
the current [[dataframe]] and perform the specified aggregation	data pivot pivot_col values	0.050000
of the :class dataframe to a data	data frame writer save path	0.142857
clusters	mllib power iteration clustering model k	0.200000
conditions and returns one	sql column otherwise value	0.050000
field in	field	0.222222
compare 2 ml params instances	compare params	0.200000
__init__(self	ml cross validator init	1.000000
r^2^, the coefficient of determination	regression metrics r2	0.166667
python parammap into	to	0.007692
offsets	offset	0.021739
an :class rdd, a list	schema samplingratio verifyschema	0.029412
extract the week number	sql weekofyear	0.055556
pearson's independence test using dataset	ml chi square test test dataset featurescol labelcol	0.333333
file	file	0.228571
test that the final value of	with sgdtests test	0.111111
the dataframe in a	sql data frame writer	0.011628
lower bound on the log likelihood	log likelihood dataset	0.142857
:class dataframe out into external storage	sql data frame	0.005348
tests whether this instance contains a	has param paramname	0.142857
squared	linalg sparse vector squared	1.000000
cost sum of squared distances of	compute cost	0.142857
observed is vector conduct pearson's chi-squared goodness	mllib stat statistics chi sq	0.066667
for each original column	original	0.047619
containsnull	containsnull	1.000000
used again to wait for	reset	0.011236
norm	mllib linalg sparse vector norm p	0.083333
returns the threshold if	threshold	0.018182
test that the final value of	streaming logistic regression with sgdtests test	0.111111
minutes of	minute	0.040000
:py attr optimizedocconcentration	optimize doc concentration value	1.000000
:class rdd,	schema samplingratio verifyschema	0.029412
based on	based on	0.111111
new :class dataframe sorted by the specified column	data frame sort	0.125000
calculates the norm	sparse vector norm p	0.133333
use only create	create	0.017241
converts matrix columns in an input dataframe	convert matrix columns	0.166667
:py attr docconcentration	doc concentration value	1.000000
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online", learningoffset=1024	ml lda set params featurescol maxiter seed	0.250000
which	logistic regression	0.040000
convert a list of column or names	sql to	0.041667
stream foreachrdd get	stream foreach get	1.000000
an :class rdd, a list or	schema samplingratio verifyschema	0.029412
this vector to	mllib linalg sparse vector	0.111111
set the initial value of	set initial	0.111111
or compute the number of rows	num rows	0.200000
for each key using	by key func	0.062500
value of	ml chi sq	0.400000
aftsurvival	aftsurvival	1.000000
comprised of vectors containing i i	random rdds normal	0.125000
of ensuring all received	stopsparkcontext stopgracefully	0.050000
method for given unary	sql unary	0.200000
a shortcut of write() save path	ml one vs rest save path	0.200000
multiclass classification	cls data numclasses categoricalfeaturesinfo	0.250000
to wait	streaming	0.005025
returned	streaming test case	0.333333
a param with	param params	0.014925
to a data	data	0.011628
receive accumulator updates in	update	0.055556
specified table or view as	sqlcontext table tablename	0.142857
configure	streaming	0.005025
to set	streaming	0.005025
partitioned data	external group by spill	0.047619
of	ml java classification model	1.000000
it will convert each python object into java	rdd to java	0.333333
set initial centers should be set before calling	streaming kmeans set initial centers centers weights	0.200000
which is a risk function	linear regression summary	0.013889
been	receiverstarted	0.142857
numpy ndarray	ml linalg matrix to array	0.166667
tf vectors to tf-idf vectors	mllib idfmodel transform x	0.142857
number of	mllib linalg row matrix num	0.062500
data	group	0.025641
the week number	sql weekofyear	0.055556
to	core accumulator add	0.076923
returns one of multiple	sql	0.002525
so that :func awaitanytermination()	sql streaming	0.010204
a specific topic and	topic and	0.333333
or create	get or create	0.111111
:py attr withmean	with mean value	1.000000
of labeledpoint	load lib svmfile sc path	0.125000
:func awaitanytermination() can	streaming query manager reset	0.011905
predictions which gives the predicted value of each	ml generalized linear regression summary prediction	0.333333
nsmallest	nsmallest	1.000000
:py attr lda keeplastcheckpoint is set	ldamodel get	0.066667
of	core external	0.016129
labels corresponding to	model labels	0.166667
stringify	stringify	1.000000
of	ml chi	0.100000
array of features corresponding to	features	0.043478
algorithm	algorithm	0.454545
next memory limit if the	sorter next limit	0.200000
objective function scaled loss + regularization at each	regression training summary objective history	0.500000
infer schema from an rdd of row	sql sqlcontext infer schema rdd	0.250000
add	add	0.250000
binary or multiclass	cls data numclasses	0.250000
a line in	line line multiclass	0.166667
of indices to	ml chi sq selector	0.100000
how	external merger	0.031250
a file added through	core spark files get	0.125000
the word2vec model's vocabulary default 5	mllib word2vec	0.125000
finding frequent items	data frame freq items cols	0.166667
length of	length	0.040000
partial objects do not serialize correctly in python2	pickler save partial obj	0.125000
number of	linalg block matrix num	0.100000
returns a new java	java wrapper new java	0.166667
model	model compute	0.066667
orc files returning	orc	0.083333
tree including	decision tree model	0.050000
array of features corresponding to that user	user features	0.333333
trees in the ensemble	tree ensemble model	0.076923
how data of a streaming dataframe/dataset is written	data stream writer output mode outputmode	0.333333
query either by :func	streaming	0.005025
the underlying rdd with the	mllib	0.010526
already partitioned data into	core external group by	0.045455
app name should be	appname sparkhome pyfiles	0.500000
gets a param by its name	param params get param paramname	1.000000
get all values	conf get all	0.166667
vector columns in	vector columns from	0.142857
saves the content of the :class dataframe	sql data frame	0.005348
randomforestregressor	random forest	0.041667
tree (e g depth 0	tree model	0.026316
outputformat api	outputformatclass keyclass	0.250000
the behavior when	writer mode savemode	0.333333
create a converter to drop the	create converter	0.166667
for multiclass classification evaluator	ml multiclass classification evaluator	0.500000
returns true positive rate for	metrics true positive rate	0.250000
labeled points	labeled points sc	1.000000
sets	tokenizer set	1.000000
note :	count approx	1.000000
return the weights for each tree	ml tree ensemble model tree weights	0.333333
the residual degrees of freedom	linear regression summary residual degree of freedom	0.250000
the rdd's elements in	core	0.003021
stratified sample without replacement based on	sample	0.050000
or compute	linalg coordinate	0.333333
:class dataframe, using the given join	sql data frame join	0.500000
output by the given	writer bucket by	0.100000
:class dataframe as	data frame as	0.333333
string in the format supported by	s	0.071429
c	c	1.000000
user defined function in	user defined function	0.066667
with	param params has	0.019231
this dataset checkpointing can	frame checkpoint eager	0.071429
value of	ml generalized	0.666667
even if users construct taskcontext instead of	core task context	0.100000
parses a line in libsvm format into	mlutils parse libsvm line line	0.111111
the length of a string or	sql length	0.050000
compute the dot product of	vector dot other	0.050000
transforms a python	java params transfer param map	0.250000
key-value	map	0.117647
converts matrix columns in an	mlutils convert matrix columns	0.166667
specified table	table	0.031250
creates a :class dataframe from	sql spark session create	0.142857
correlation of	corr col1 col2 method	0.055556
code for a shared param class	ml param gen param code	0.333333
path a shortcut of	ml	0.007339
a	param	0.025000
function to get or create global	get or create	0.111111
sets params	set params	0.137931
removes the specified table from the in-memory cache	catalog uncache table tablename	0.250000
sets the given parameters in this grid to	ml param grid builder base	0.076923
methods to set	streaming	0.005025
a multi-dimensional cube	sql data frame cube	0.055556
computes the singular value decomposition	compute svd k computeu rcond	1.000000
started by this thread until the group id	group groupid	0.142857
at least the master	master	0.100000
column of the current [[dataframe]] and	pivot pivot_col values	0.050000
version of a heappush	heap	0.047619
a new vector with 1 0 bias appended	mlutils append bias data	0.333333
new rdd of int containing elements	core	0.003021
null	sql	0.005051
submit and test a single script	core spark submit tests test single script	0.500000
inherit documentation from its parents	mllib inherit doc	0.045455
the receiver operating characteristic roc curve which is	binary logistic regression summary roc	0.166667
that	core external	0.016129
rdd previously saved using l{rdd saveaspicklefile} method	core spark context pickle file name minpartitions	0.250000
ordered in ascending order or as	ordered	0.076923
initial value of	initial	0.071429
support vector machine on	svmwith sgd	0.333333
be used again to	sql streaming query	0.011765
tree model	tree model	0.026316
with the dispatch to handle all function	core cloud pickler save function	0.142857
for new	query manager reset	0.011905
elements from	core spark context range	0.142857
or multiclass	cls data numclasses	0.250000
returns the soundex encoding for a string >>>	soundex	0.043478
the length of a string or binary expression	length	0.040000
the values for each key using	key func numpartitions partitionfunc	0.066667
sets	decision tree params set	1.000000
using the old hadoop outputformat api	hadoop dataset conf keyconverter valueconverter	0.083333
creates a new sparksession	sql spark session init sparkcontext jsparksession	1.000000
itemscol	itemscol	1.000000
of all active stages	get active	0.333333
a java parammap into a python	from java javaparammap	0.500000
an rdd created by	rdd	0.003058
left multiplies this blockmatrix by other, another	multiply other	0.200000
which is	regression summary	0.071429
using the old hadoop outputformat api mapred package	save as hadoop dataset conf keyconverter valueconverter	0.083333
of the rdd	rdd	0.003058
the specified table	tablename	0.043478
update the centroids according to data	update data decayfactor timeunit	1.000000
a dictionary a list of index value	size	0.036697
transforms a python parammap	ml java params transfer param	0.250000
accessible via jdbc url url and	jdbc url	0.200000
a parquet	parquet	0.066667
new hadoop outputformat api mapreduce package	as new apihadoop dataset conf keyconverter valueconverter	0.142857
generate	generate logistic input	1.000000
for each numeric columns for each group	grouped data	0.107143
attr lda keeplastcheckpoint is set to	ldamodel	0.034483
creates an external table	create external table tablename	1.000000
printable representation of row used in python repl	sql row repr	0.250000
converts vector columns in an input dataframe	convert vector columns to	0.166667
a condition to pass else fail with an	mllib mllib streaming test case eventually condition	0.333333
the current [[dataframe]] and	data pivot pivot_col values	0.050000
returns an mlwriter instance for	model write	1.000000
:class dataframestatfunctions for statistic functions	frame stat	0.250000
param	ml param params has	0.019231
regexp	sql regexp	0.125000
train the model on	mllib streaming kmeans train on	0.333333
standard deviation of this rdd's	core rdd stdev	0.066667
a densematrix whose columns are the right singular	singular	0.015625
names into a jvm seq of column	seq sc cols converter	0.055556
data into disks	external group by spill	0.047619
number of columns that make up each	cols per	0.333333
pairs or two separate arrays of indices and	ml	0.001835
accumulator's value	accumulator value value	0.050000
min value for each original	original min	0.250000
std	std	0.714286
parses a line in libsvm format	libsvm line line	0.333333
__init__(self	binary classification evaluator init	1.000000
labeledpoint	lib svmfile	0.125000
set the initial	logistic regression with sgd set initial	0.111111
param with a given	params	0.006623
idf	idf	0.666667
results immediately to the master as a dictionary	locally func	0.142857
much of memory for this obj assume that	external merger object size obj	0.040000
degrees of	linear regression summary degrees of	1.000000
instance	param	0.012500
get or compute	linalg	0.044444
trained	logistic	0.062500
test for data sampled	test data distname	0.166667
instance contains a param with	ml param params	0.013699
setparams(self formula=none featurescol="features", labelcol="label", forceindexlabel=false) sets params for	set params formula featurescol labelcol forceindexlabel	0.166667
>>> dm = densematrix(2 2 range 4	dense matrix repr	0.142857
of the :class dataframe	frame writer save	0.066667
vectors saved using	vectors sc	0.333333
the model's transform method	linear regression summary predictions	0.200000
column for approximate distinct count of	approx count distinct col	0.071429
predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0	predictioncol maxdepth	0.250000
squared error which is defined	linear regression summary	0.013889
column	mllib standard	0.125000
blockmatrix	mllib linalg block matrix	0.105263
and name or provide a	core	0.003021
the stream query	data stream writer	0.041667
local representation	local	0.038462
value of	ml count	1.000000
or compute the number of	row matrix num	0.100000
perform a left outer join of	left outer join	0.111111
that :func awaitanytermination() can be	sql streaming query	0.011765
be used again	query manager	0.011905
destroy all data and metadata related	destroy	0.111111
makes a class inherit documentation	mllib inherit doc cls	0.045455
elements from an rdd ordered in	rdd take ordered	0.050000
this idf	ml idf	0.250000
setparams(self estimator=none estimatorparammaps=none evaluator=none trainratio=0 75	split set params estimator estimatorparammaps evaluator trainratio	0.500000
only create a new hivecontext for testing	hive context create for testing cls sparkcontext	0.333333
to the expected value of	ml linear	0.066667
items for columns possibly with false	items cols	0.125000
probabilitycol	probabilitycol	0.250000
of this instance with	ml one vs rest model	0.111111
sqltests	sqltests	1.000000
of this instance with a randomly	ml cross validator model	0.333333
which predictions	ml isotonic regression	0.111111
is set to a	core spark context set	0.166667
the value	get	0.021739
an upper triangular matrix r in	mllib linalg qrdecomposition r	0.333333
partial objects do	partial	0.076923
the day of the month of	sql dayofmonth col	0.031250
values for each numeric columns for each	grouped data	0.035714
given table/view in the specified	tablename	0.043478
is the user and the second is an	mllib matrix	0.047619
get the cluster centers represented as a	mllib bisecting kmeans model cluster centers	0.083333
rdd	rdd set	1.000000
the rdd into	rdd	0.003058
returns the idf vector	idfmodel idf	0.166667
with a function and	user defined function	0.066667
get spark_user for user who is running sparkcontext	core spark context spark user	0.250000
get or compute	linalg coordinate	0.333333
test	tests test group by	1.000000
the input java	ml java estimator	0.200000
sum for each numeric columns for	grouped data sum	0.083333
precision of all the queries	metrics precision	0.200000
the dot product of two vectors	mllib linalg dense vector dot	0.058824
attributes which are array-like or buffer to array	to array array_like dtype	0.166667
stream query	sql data stream writer	0.041667
flags for controlling	level	0.125000
a converter to drop the names of fields	converter datatype	0.071429
applying a function on rdds of the dstreams	dstreams transformfunc	0.125000
used again	sql streaming	0.010204
the number of rows	linalg block matrix num rows	0.200000
used again to wait for new	query	0.010753
model to a	ldamodel to	0.500000
the cluster centers represented as a list of	ml bisecting kmeans model cluster centers	0.333333
l{sparkcontext} that this rdd was created on	core rdd context	0.166667
for a condition to pass else fail with	test case eventually condition	0.333333
copy all params	params copy params	1.000000
so that :func awaitanytermination() can be used again	streaming query	0.010526
factorization	factorization	0.192308
generate	with sgdtests generate	1.000000
dataset in a data source	source schema	0.181818
original column during fitting	model original	0.062500
java	ml java	0.076923
rdd an rdd of	iteration clustering train cls rdd	0.250000
square root	root	0.071429
model fitted by :class	regression model	0.125000
to	reset	0.011236
the uid of this	ml param params reset uid	0.058824
a list of conditions and returns	sql column otherwise	0.050000
lda	ldamodel	0.034483
the stream query if this is not set	data stream	0.028571
in utc	utc	0.100000
for which	ml isotonic regression	0.111111
saves the content of the	name format mode partitionby	0.200000
line in libsvm format into label indices	mllib mlutils parse libsvm line line	0.111111
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none	ml random forest classifier	0.023256
featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online", learningoffset=1024 0	featurescol maxiter seed	0.200000
outputformat api mapred	dataset conf keyconverter valueconverter	0.250000
test that param	param	0.006250
ignore	sql ignore	0.333333
returns weighted	mllib multiclass metrics weighted	0.666667
vector columns in an input dataframe from the	vector columns to ml	0.142857
sparkcontext	streaming context spark	0.083333
the output by the given columns if	writer bucket by	0.100000
default min number of partitions for hadoop rdds	context default min partitions	0.250000
the values for each key	key func numpartitions partitionfunc	0.066667
new dstream by applying reducebykey to	streaming dstream reduce by key func numpartitions	0.076923
print the profile stats to stdout	profiler show	0.166667
methods	streaming	0.005025
globals names read or written	code globals	0.125000
for this idf	idf	0.111111
the mean variance and count of the	core	0.003021
new :class column for approximate distinct count	approx count distinct col rsd	0.066667
the stream query if this is not	sql data stream writer	0.041667
a local property that affects jobs submitted from	local property key value	0.076923
transforms	java params transfer param map	0.250000
cachenodeids	cache node ids	1.000000
sets	input cols set	1.000000
that stopped	streaming query	0.010526
set the selector	selector set selector	0.333333
to term frequency vectors or	mllib hashing tf	0.125000
__init__(self minsupport=0 3 minconfidence=0 8 itemscol="items", predictioncol="prediction", numpartitions=none)	init minsupport minconfidence itemscol predictioncol	1.000000
streaming :class dataframe from external storage systems (e	data stream reader	0.200000
randomly	split	0.125000
sparkui instance started by	ui web	0.333333
comprised of vectors containing i i d samples	mllib random rdds poisson	0.125000
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto",	ml random forest classifier	0.023256
aggregate	rdd aggregate	0.250000
a column of the current [[dataframe]] and	data pivot pivot_col values	0.050000
create a new dstream in	streaming streaming context	0.032258
get or compute the number of	block matrix num	0.100000
embedded params to the	params to	0.035714
contains a param	param params	0.014925
sets the accumulator's value only	core accumulator	0.030303
creates an external	sqlcontext create external	1.000000
stream query if this is not set	stream writer	0.041667
fast version of a heappush followed	core heappushpop heap	0.142857
each dstreams in this context	streaming context	0.055556
fits a java model to the	estimator fit java	0.333333
jobgroup	jobgroup	1.000000
soundex encoding for	soundex col	0.055556
the value of the date column	next day date	0.100000
lda	ldamodel get	0.066667
an rdd containing all pairs of	rdd	0.003058
number of columns	block matrix cols	0.333333
of columns for	columns	0.019608
rdd contains no elements at	core rdd is empty	0.083333
parses the given	json_string	0.125000
outputted by the model's transform method	regression summary predictions	0.200000
with	has param	0.019231
predicts rating for the given user and product	matrix factorization model predict user product	1.000000
this thread such as the spark	core spark context	0.011628
finding frequent items	freq items cols	0.166667
parquet file stream returning the result as	stream reader parquet path	0.083333
so that :func awaitanytermination()	manager	0.011236
the week number of a	sql weekofyear	0.055556
lines text format or newline-delimited json	json path mode	0.125000
factors	factors	0.714286
comprised of	random rdds exponential vector	0.125000
for feature selection by fwe	mllib chi sq selector set fwe fwe	0.200000
censor	censor	1.000000
__init__(self	tree classifier init	1.000000
list based on	based on key outputs	0.111111
this rdd's	core rdd	0.003460
boundaries in increasing order for	model boundaries	0.500000
the global temporary	global temp	0.500000
rdd is checkpointed	core rdd	0.003460
an rdd comprised	mllib random rdds log normal vector rdd	0.166667
two vectors we support	linalg dense	0.200000
vectors or transform the rdd of document	tf transform document	0.166667
result as a :class dataframe	sql data frame	0.010695
saves the contents	save path format mode partitionby	0.200000
calculates the length of a string	length	0.040000
observed tokens in the training set given the	ldamodel training	0.034483
linear model	linear model	0.066667
of another dstream with	other	0.033333
for each original column	max scaler model original	0.062500
with two fields threshold precision curve	binary logistic regression summary precision by threshold	0.166667
which is	logistic regression	0.040000
python code for	code	0.071429
receiver operating characteristic roc	summary roc	0.333333
multiple parameters passed as a list of	spark conf set	0.111111
converts vector columns in	mllib mlutils convert vector columns to ml	0.166667
prints the first n rows to the console	frame show n truncate	0.333333
set bandwidth	kernel density set bandwidth bandwidth	0.142857
converts vector columns in an input dataframe	mlutils convert vector columns	0.166667
name	has	0.011628
streamingcontext from checkpoint data or create	streaming context get or create	0.200000
function	map values f	0.125000
idf	ml idf	0.250000
onevsrestmodel create and return a python wrapper of	ml one vs rest model from	1.000000
soundex encoding for a string >>> df =	sql soundex col	0.055556
signed shift the	shift	0.142857
wait for the execution to	or timeout timeout	0.125000
this instance with a randomly generated	one	0.058824
__init__(self min=0 0 max=1 0	ml min max scaler init min max	1.000000
add a py or zip dependency for all	add py	0.166667
offset specified	from offset	0.125000
convert a list of column or names	to	0.007692
awaitanytermination() can be used again	sql streaming query	0.011765
string format	string s	0.333333
tests whether this instance contains a param with	ml param params has param paramname	0.142857
data or	data	0.011628
sets window	set window	1.000000
the year of	sql year	0.050000
ndcg value of all	ndcg	0.100000
rank	alsmodel rank	1.000000
the spark	core spark	0.020619
with	params has	0.019231
max	scaler model max	1.000000
the column	mllib standard scaler model	0.100000
the contents of the :class dataframe to a	frame writer save path	0.066667
this	core external merger	0.032258
the java object	java	0.012195
type of	type	0.024390
comprised of	random rdds	0.115385
transforms a java parammap into a python parammap	ml java params transfer param map from java	1.000000
:class dataframe in	data frame	0.010000
with a given string name	params has	0.019231
that all	external merger object size	0.032258
family	family	1.000000
this instance contains a param with a given	has	0.011628
partitioned data into disks	external	0.013889
infer schema from an rdd	sqlcontext infer schema rdd	0.250000
the number of rows	matrix num rows	0.200000
class for indexing categorical feature columns in a	indexer	0.055556
pipeline create and return a	pipeline from	0.142857
new	sql	0.005051
dictionary a	init size	0.066667
the date column	day date	0.100000
the column mean values	model mean	0.125000
covar	covar	1.000000
called when processing of a batch of	streaming streaming listener on batch	0.333333
large dataset and an item approximately	ml lshmodel approx nearest neighbors dataset key	0.166667
this instance contains	ml	0.001835
the left singular vectors of the singularvaluedecomposition	linalg singular value decomposition	0.250000
params instances for	params	0.006623
recommends the	matrix factorization model recommend	0.250000
term to this accumulator's	accumulator add term	0.066667
stages	tracker get	0.500000
creates	sparkcontext	0.142857
serializer	serializer	0.312500
the given name	name	0.043478
train a random forest model for binary	mllib random forest train classifier cls	0.250000
blockmatrix by other,	multiply	0.100000
inputcol input column name	input col	0.500000
rdd as non-persistent and remove	core rdd unpersist	0.066667
a python topicandpartition to map to the java	streaming topic and partition init topic partition	0.055556
and value class from	core	0.006042
mixin for param itemscol items column name	has items col	1.000000
in :py attr predictions which gives	linear regression summary	0.013889
number of cols	distributed matrix num cols	1.000000
contains a	params has	0.019231
instance contains a param with a	has	0.011628
new	streaming query	0.010526
saves	writer save path format mode partitionby	0.200000
for user who is running sparkcontext	context spark user	0.250000
sparse vector using either a dictionary	linalg vectors sparse size	0.166667
:py attr mininstancespernode	min instances per node value	1.000000
the objects	core external merger	0.032258
a class inherit documentation	mllib inherit doc cls	0.045455
a line in libsvm format into label indices	parse libsvm line line multiclass	0.111111
class inherit documentation from	mllib inherit doc	0.045455
an rdd containing	core rdd	0.003460
bayes classifiers	bayes	0.250000
a windowing	over window	0.333333
1 0] for feature selection by	mllib chi sq selector set	0.150000
content of the non-streaming :class dataframe	data frame write	0.166667
first spark call in the current call stack	first spark call	1.000000
make predictions on a	predict on	0.058824
predict the label of one or more examples	decision tree model predict x	1.000000
the :class dataframe	frame writer save path format	0.066667
of the importance of each feature	ml gbtregression model feature importances	0.250000
use for loading	mlreader	0.111111
test the python direct kafka stream api with	tests test kafka direct stream	0.125000
boundaries defined from start inclusive to end inclusive	between start end	0.125000
the dot product of two vectors	vector dot	0.050000
with the spark sink deployed on a	addresses storagelevel maxbatchsize	0.045455
gets a param	get param	1.000000
used again to wait for	query manager	0.011905
term to	add term	0.066667
wait for the execution	await termination or timeout timeout	0.125000
calculates the correlation of two columns	corr col1 col2 method	0.055556
returns a java	java	0.012195
bisecting k-means algorithm based on	bisecting kmeans	0.166667
this broadcast on the executors if	core broadcast unpersist blocking	0.500000
an 'old' hadoop	context hadoop	0.090909
ordered list of labels corresponding to indices to	ml string indexer model labels	0.066667
or create	context get or create	0.200000
:class windowspec	sql window	0.600000
creates a :class	create	0.017241
version of	heap	0.047619
squared distance from a sparsevector	vector squared distance other	0.166667
the accumulator's data type returning a new	accumulator	0.012987
dstream by applying a function	f	0.010526
linear data	linear data	1.000000
the number of rows	row matrix num rows	0.200000
a multi-dimensional rollup for	sql data frame rollup	0.055556
test the python direct kafka	streaming kafka stream tests test kafka direct	1.000000
awaitanytermination() can be used again to	sql streaming query manager reset	0.011905
weights computed for	mllib linear model weights	0.250000
table and update the	tablename	0.043478
setparams(self	ml min max scaler set params	1.000000
calculates the correlation	method	0.041667
values for each numeric columns for each	grouped	0.035714
value of the given column which	col	0.016393
underlying :class	session	0.100000
sql	type sql	0.250000
create an input stream that pulls	create stream ssc	0.200000
add a py or zip dependency for	context add py file path	0.166667
of memory for	merger object	0.032258
number of columns	num	0.008403
of a word	word num	0.333333
string	params	0.006623
each key using an	by key	0.026316
this accumulator's	accumulator add	0.076923
get all values as a	conf get all	0.166667
a param with a given string	params has	0.019231
sigterm	sigterm	1.000000
model derived from a	model	0.005587
columns in an input dataframe	columns to ml dataset	0.125000
given	param params	0.014925
a class inherit documentation from its	mllib inherit	0.045455
nodes summed over all trees in the	nodes	0.074074
a decision tree	mllib decision tree	0.166667
finding frequent items for	freq items cols	0.166667
the correlation of two	col1 col2 method	0.055556
formula=none featurescol="features", labelcol="label", forceindexlabel=false) sets params for rformula	rformula set params formula featurescol labelcol forceindexlabel	0.500000
computes hex value of	hex col	0.166667
convert this distributed model to a local representation	distributed ldamodel to local	0.111111
the left singular vectors of the	linalg singular	0.017544
regression	regression	0.220000
much of memory for this	core external merger object	0.032258
converts vector columns in an input	convert vector columns	0.166667
shared param class	ml param gen param	0.333333
return an rdd with the keys of each	core rdd keys	0.333333
the new mllib-local representation	as ml	0.333333
concat	concat	1.000000
converts vector columns in an input	mlutils convert vector columns from	0.166667
:func awaitanytermination() can	query manager reset	0.011905
returns a dataframe with two fields threshold	threshold	0.036364
that with new specified	to df	0.250000
k-means	kmeans	0.025641
__init__(self	classification evaluator init	1.000000
the model to make predictions on batches	streaming linear algorithm predict on	0.066667
test that	regression with sgdtests test	0.111111
python direct kafka stream foreachrdd	kafka direct stream foreach	0.500000
set the initial value of weights	sgd set initial weights	0.333333
contains a param	params	0.006623
tests whether this instance contains	param paramname	0.111111
such as the spark fair scheduler pool	spark	0.013158
load a java model from the given	loader load java cls sc	0.200000
train a random forest model for binary or	mllib random forest train classifier cls data	0.250000
system using the new	save as new	0.125000
of numpy	ml bisecting	0.066667
setparams(self labelcol="label", featurescol="features",	regression set params labelcol featurescol	1.000000
a column of the current [[dataframe]] and perform	pivot pivot_col values	0.050000
generates an rdd comprised of	random rdds poisson vector rdd sc mean	0.200000
of terms or words	ml ldamodel	0.111111
1 which should be smaller than or equal	numiterations	0.050000
a string column	string	0.041667
columns	block matrix cols	0.333333
on incoming dstreams	streaming	0.005025
params instances for the given	params m1 m2	0.047619
the dot product	vector dot	0.100000
a param with a given string name	ml param	0.009524
the deviance for the	deviance	0.111111
each rdd generated in this dstream	streaming dstream	0.027778
this class	ml java	0.076923
passed in a profile object is returned	profiler profile func	0.200000
the year of a given date	year col	0.050000
onevsrest create and	one vs rest	0.034483
queries so that	reset	0.011236
expected distribution	expected	0.076923
:class dataframe in json format (json	sql data frame writer	0.011628
curve	mllib binary	1.000000
points belongs to in this model	mllib bisecting kmeans model predict x	0.333333
min value for each original	max scaler model original min	0.250000
matrix columns	matrix columns from	0.142857
the mean variance	rdd	0.003058
:class column for distinct	distinct col	0.166667
the sql context to use for loading	java mlreader context sqlcontext	0.333333
params for	params featurescol labelcol	0.500000
values for each key	by key	0.026316
submit and test a	core spark submit tests test	0.363636
setparams(self inverse=false inputcol=none outputcol=none) sets params	set params inverse inputcol outputcol	0.333333
instance is of type	ml ldamodel is distributed	0.066667
sum	data sum	0.333333
memory for this obj assume that all the	obj	0.023810
with a given string	has	0.011628
squared distance from a sparsevector	vector squared distance	0.166667
precision-recall curve which is a dataframe containing two	binary logistic regression summary pr	0.083333
a configuration property if not already set	spark conf set if missing key value	0.500000
a local representation this discards info	local	0.038462
how much of	merger object	0.032258
input option	stream reader option	0.333333
a temporary table in the	table df tablename	0.083333
array or map stored in the column	col	0.016393
a lower bound on the log	ldamodel log	0.125000
parquet file stream returning the result as a	stream reader parquet path	0.083333
until any of the	streaming query manager await any termination	0.142857
from	power iteration clustering	1.000000
called by the default implementation of fit	ml estimator fit	0.083333
norm of	norm p	0.055556
__init__(self labelcol="label",	init labelcol	1.000000
sets the given parameters in	ml param grid builder	0.055556
the dot product of two vectors we support	mllib linalg dense vector dot	0.058824
find the maximum	max	0.071429
term to	term	0.040000
a	params has param	0.038462
new feature vector with a subarray of the	vector slicer	0.166667
programming spark with	spark session	0.100000
of layers	ml multilayer perceptron classification model	1.000000
predictions which	linear regression	0.040000
the minutes of a given	sql minute col	0.050000
this vector to the new mllib-local representation	mllib linalg sparse vector as ml	0.333333
the number	row matrix num	0.100000
pos in byte and is of length	pos	0.022222
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	decision tree regressor	0.058824
a new dstream by applying reducebykey to	streaming dstream reduce by key func numpartitions	0.076923
a new dstream by applying a function	f	0.010526
columns of a dataframe as	sql data frame corr col1	0.166667
access fields by name	sql struct type getitem key	0.200000
create a python topicandpartition to map to the	topic partition	0.055556
fields threshold recall curve	ml binary logistic regression summary recall by threshold	0.166667
for this dct	ml dct	0.250000
a field by name in a structfield	field name	0.166667
to a :class datatype the data type	datatype	0.045455
value of	ml param has elastic net param	1.000000
values for each key	by key func numpartitions	0.062500
a java udf so it can be	java	0.012195
instance's params to the	params to	0.035714
recommends the top	recommend	0.153846
predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction",	predictioncol probabilitycol	0.111111
values for each numeric columns for each group	sql grouped data avg	0.058824
rdd of key-value pairs (of form c{rdd[	core rdd save	0.037975
this accumulator's value	core accumulator	0.030303
sets the context	streaming streaming context	0.032258
destroy all data and	destroy	0.111111
in which each rdd	by	0.014286
returns a densevector with singular	mllib linalg singular	0.017544
the norm	norm	0.125000
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1	random forest classifier	0.022727
conditions and returns one of multiple	sql column otherwise value	0.050000
a java object	java	0.012195
contains a param with a given	params	0.006623
in	builder	0.181818
convert this matrix to an indexedrowmatrix	matrix to indexed row matrix	0.333333
the selector type of the	sq selector set selector type	0.111111
using the given join	join	0.034483
extract a specific group matched by a	regexp extract str pattern idx	0.333333
params to the wrapped	java params to	0.045455
to in this model	mllib bisecting kmeans model	0.333333
groups the :class dataframe using	sql data frame group by	0.200000
to make predictions on batches of	streaming linear algorithm predict on	0.066667
return	core external	0.016129
approximately find at most k	lshmodel approx	0.100000
are the right singular vectors of the singularvaluedecomposition	linalg singular value decomposition v	0.250000
of indices back to a new column of	index to	0.040000
daemon	daemon	1.000000
value for each original column	model original	0.062500
contains	params has	0.019231
libsvm format into an rdd of labeledpoint	mllib mlutils load lib svmfile	0.125000
disks	external	0.027778
a sparse vector using either a dictionary	mllib linalg vectors sparse size	0.166667
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online",	ml lda set params featurescol maxiter seed	0.250000
setparams(self numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")	set params numbuckets inputcol outputcol relativeerror	1.000000
cost sum of squared distances of	cost	0.105263
matrix columns in an input dataframe from	matrix columns to	0.142857
a java storagelevel	java	0.012195
number of possible	model num	0.083333
receivererror	receivererror	0.714286
an rdd of row or tuple	rdd samplingratio	0.200000
a paired rdd where	factorization	0.038462
from flume	flume	0.071429
and count of	core	0.003021
the dataframe in	data frame	0.005000
all the	external merger object size	0.032258
every feature	mllib linear model	0.125000
for multiclass classification	ml multiclass classification	0.500000
an rdd that has no partitions or elements	spark context empty rdd	0.200000
result count to the number	num	0.008403
__init__(self inputcol=none outputcol=none handleinvalid="error")	init inputcol outputcol handleinvalid	1.000000
into	group	0.025641
gets summary (e	summary	0.024390
with single :class pyspark sql types longtype	sql sqlcontext range start end step numpartitions	0.083333
scores into 0/1 predictions	linear classification model	0.076923
infer schema from an rdd	sqlcontext infer schema rdd samplingratio	0.250000
of object by unpickling it will	ml	0.001835
the values for each key	key func	0.066667
get the offsetrange of	ranges	0.090909
parameters in this grid to fixed	ml param grid builder add grid param	0.250000
java_model to a python model	model java_model	0.200000
concatenates multiple input string columns together into	sql concat ws sep	0.500000
also known as min-max normalization or rescaling	min	0.041667
elements	core	0.009063
matrix columns in an input dataframe from	matrix columns to ml dataset	0.142857
get all values	core spark conf get all	0.166667
save this model to the	naive bayes model save	0.500000
name	param	0.012500
as spark	spark	0.013158
incoming dstreams	streaming	0.005025
jvm	vector transformer	0.250000
mixin for param labelcol label column name	has label col	1.000000
and the second is an	mllib	0.010526
column-major dense matrix	dense matrix	0.076923
function to each rdd in	rdd func	0.250000
train a support vector machine on the given	mllib svmwith sgd train cls	1.000000
already	by spill	0.047619
a resulting rdd that contains a tuple with	core rdd cogroup other	0.066667
for multiclass	ml multiclass	0.500000
only create a	context create	0.083333
applies unit length normalization on a	normalizer transform	0.500000
py or zip dependency	py file	0.066667
the schema of this :class dataframe	data frame schema	0.333333
param and assert both have the same param	param	0.006250
converts matrix columns in an input dataframe	convert matrix columns from ml dataset	0.166667
mixture components	mixture model predict soft x	0.142857
awaitanytermination() can	reset	0.011236
setparams(self scalingvec=none inputcol=none outputcol=none) sets params	set params scalingvec inputcol outputcol	0.333333
sort	case sort result	0.333333
wait for the	context await termination timeout	0.166667
in :py attr predictions which gives the predicted	generalized linear regression summary prediction	0.250000
levenshtein distance of	sql levenshtein	0.058824
class inherit documentation	inherit doc	0.045455
java pipelinemodel create	pipeline model from java	0.142857
finding frequent items for columns	freq items cols	0.166667
accumulator's data type returning a new value	accumulator param	0.038462
:class dataframe to a	frame writer	0.050000
a list of conditions and	column otherwise value	0.200000
norm	vector norm p	0.166667
match regexp	sql regexp	0.125000
is defined as the square root	root	0.071429
a line in libsvm format into	mlutils parse libsvm line line	0.111111
external list	external list	0.166667
dataframe containing names of tables in the	sqlcontext tables	0.333333
get the n elements from	num	0.008403
perform a left outer join	left outer join other	0.111111
other from this	mllib linalg	0.026316
the label of one or more examples	decision tree model	0.050000
objective function scaled loss + regularization at each	linear regression training summary objective history	0.500000
left singular vectors of the singularvaluedecomposition	singular value decomposition	0.166667
python direct kafka	kafka direct	0.500000
list of numpy arrays	ml	0.001835
registers	sqlcontext register	1.000000
rdd containing the distinct elements in this rdd	core rdd distinct	0.250000
parses the	expr str	0.125000
:class dataframe to a data source	data frame writer save path	0.142857
compute the number of rows	row matrix num rows	0.200000
residuals deviance pvalues of model	ml generalized linear regression model	0.166667
of columns that make up each block	linalg block matrix cols per block	0.333333
serializes a stream of list of	serializer	0.062500
given columns on the	sql data frame	0.010695
return a new dstream by applying a function	map f	0.037037
an rdd comprised of vectors containing	random rdds poisson vector rdd	0.166667
converts matrix columns in an input	convert matrix columns	0.166667
this rdd and its recursive dependencies for debugging	core rdd to debug string	1.000000
columns are the left singular vectors	singular	0.015625
seqop	seqop	1.000000
create a multi-dimensional rollup for	sql data frame rollup	0.055556
function translate any character in the srccol by	translate srccol	1.000000
sets	set key value	0.500000
returns the least value of the list of	least	0.043478
private java	java	0.012195
only create a new hivecontext for testing	sql hive context create for testing cls sparkcontext	0.333333
current [[dataframe]] and perform the specified aggregation	data pivot pivot_col values	0.050000
csv file	csv path schema sep encoding	0.333333
create a new profiler using	new profiler	0.333333
rdd contains no elements at all	core rdd is empty	0.083333
jvm scala map from	data frame jmap jm	0.111111
that all	merger object	0.032258
returns a java	get java	0.111111
squared distance from	linalg sparse vector squared distance	0.166667
generic function	createcombiner mergevalue mergecombiners numpartitions	0.500000
system using the new	new	0.062500
convert this matrix to a rowmatrix	linalg indexed row matrix to row matrix	0.333333
the n largest elements in a dataset	core nlargest n iterable key	0.333333
param with a given	ml param params	0.013699
set the initial value of weights	regression with sgd set initial weights initialweights	0.333333
with a randomly generated	validation split model	0.200000
so that	query manager reset	0.011905
id to all the jobs started by this	job	0.023810
given a large dataset and an item	nearest neighbors dataset key numnearestneighbors distcol	0.333333
stdout	profiler collector show	1.000000
__init__(self threshold=0 0 inputcol=none outputcol=none)	ml binarizer init threshold inputcol outputcol	1.000000
term frequency vectors or transform the rdd	mllib hashing tf transform	0.045455
test the	streaming kafka stream tests test	0.437500
infer schema from an rdd of row	infer schema rdd	0.250000
stop the execution of the streams with option	streaming context stop	0.125000
basic operation test for dstream countbyvalue	streaming basic operation tests test count	0.500000
a class generated by namedtuple	load namedtuple name fields	0.333333
much of memory for this obj assume	obj	0.023810
contains a param with a given string	has	0.011628
convert this distributed model to	distributed ldamodel to	0.166667
right	full	0.066667
elements with matching keys in c{self} and c{other}	join other	0.071429
returns an mlwriter instance	ml java mlwritable write	0.200000
old hadoop outputformat api	as hadoop dataset conf keyconverter valueconverter	0.083333
string	s	0.142857
code	code	0.428571
partial objects do not serialize correctly	core cloud pickler save partial obj	0.125000
"predictions" which gives the predicted	linear regression summary prediction	0.142857
ndcg value of all the	metrics ndcg	0.200000
local property that affects	local property key value	0.076923
with the spark sink deployed on	addresses storagelevel maxbatchsize	0.045455
test a single script file	tests test single script	0.500000
transfer this instance to a java onevsrest used	ml one vs rest to java	0.166667
matrix to the	mllib linalg dense matrix	0.083333
the threshold that	threshold	0.018182
:class list of :class	data frame	0.005000
the value of the date	day date	0.100000
model to make predictions on batches of data	mllib streaming linear algorithm predict on	0.066667
specified string value that match regexp	sql regexp	0.125000
multi-dimensional rollup for	sql data frame rollup	0.055556
k decayfactor timeunit	streaming	0.005025
invariant	core heappop	0.500000
approximately find	ml lshmodel approx	0.125000
ignore the	ignore	0.100000
sets the threshold that	set threshold	0.500000
forest <http //en wikipedia org/wiki/random_forest>_	forest regressor	1.000000
i i d samples drawn	std numrows	0.125000
featurescol="features", labelcol="label", forceindexlabel=false)	featurescol labelcol forceindexlabel	0.400000
converts matrix columns in an	convert matrix columns to ml dataset	0.166667
the right singular	mllib linalg singular	0.017544
this	size	0.009174
a labeledpoint to a string in	labeled point to	0.125000
returns the explained	explained	0.181818
converts matrix columns in an	convert matrix columns	0.166667
the values for each key	by key func	0.062500
sets the given parameters in this	grid builder add	0.200000
cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1	random forest classifier	0.022727
for this obj assume that all the	core external merger object size obj	0.040000
a resulting rdd that contains a tuple with	rdd cogroup	0.066667
generates an rdd comprised of vectors	mllib random rdds gamma vector rdd sc	0.200000
of this :class dataframe as pandas pandas	pandas	0.090909
value to a boolean if possible	ml param type converters to boolean	0.250000
__init__(self inputcol=none outputcol=none labels=none)	to string init inputcol outputcol labels	1.000000
bisecting	bisecting	1.000000
termination of this query	termination	0.035714
an rdd	vector rdd	1.000000
sets	standardization set	1.000000
the norm of	mllib linalg sparse vector norm p	0.083333
makes a class inherit documentation	inherit doc cls	0.045455
contains a param with a given	ml	0.001835
started	started batchstarted	0.250000
:func awaitanytermination() can be	streaming query manager reset	0.011905
weights is close to the desired value	parameter accuracy	0.029412
python rdd of key-value pairs	rdd save	0.038462
creates an external table based on	sql sqlcontext create external table tablename path	0.250000
much	object	0.027778
feature selection by fwe	chi sq selector set fwe fwe	0.200000
columns that make up each block	linalg block matrix cols per block	0.333333
in the key-value pair rdd through a flatmap	rdd flat map	0.333333
that makes a class inherit documentation	inherit	0.037037
soundex encoding	sql soundex	0.055556
load a model from the given path	factorization model load cls sc path	1.000000
all the	core	0.003021
the rdd's elements in one operation	core rdd	0.003460
right singular	singular	0.015625
create an input stream that pulls events	create stream	0.200000
(e g depth 0 means 1 leaf	decision	0.052632
each original column during	original	0.047619
waits for the termination of this	termination	0.035714
be used again	streaming query manager reset	0.011905
feature selection by number of top features	mllib chi sq selector set num top features	0.500000
note : experimental	rformula	0.222222
this udf with a function	defined function	0.066667
jvm scala map from a	frame jmap jm	0.111111
accumulator's data type returning a new value for	accumulator param	0.038462
convert a value to a mllib vector	to vector	0.250000
single sequence	partitionfunc	0.166667
decayfactor	streaming	0.005025
return a javardd	core rdd	0.003460
the residual degrees	ml generalized linear regression summary residual degree	0.500000
:func awaitanytermination() can be used again to wait	streaming query manager reset	0.011905
and value	core	0.006042
model on	on model	0.166667
creates a global	create global	1.000000
location where spark is installed on cluster nodes	sparkhome	0.166667
data of	data	0.011628
dump already	core external group	0.045455
number of	indexed row matrix num	0.100000
and the second is an array of	mllib	0.010526
return sparkcontext which is	streaming context spark	0.083333
used again	query manager	0.011905
later than the value of the date column	date dayofweek	0.333333
specified table as a	reader table tablename	0.500000
test	streaming kafka stream tests test	0.562500
can be used again to wait for	sql streaming query	0.011765
the number of clusters	power iteration clustering model k	0.200000
to each rdd in	rdd func	0.250000
inside brackets pairs e g	brackets	0.058824
featurescol=none labelcol=none predictioncol=none classifier=none): sets params for onevsrest	one vs rest set params featurescol labelcol predictioncol	1.000000
containing the ids	stage ids	0.055556
create an input stream that pulls	create stream ssc hostname port storagelevel	0.200000
create a python topicandpartition to	streaming topic and partition init topic partition	0.055556
0 9 0 95 0 99], quantilescol=none aggregationdepth=2)	fitintercept	0.058824
the initial	mllib streaming logistic regression with sgd set initial	0.111111
the dot product of two vectors we support	linalg dense vector dot	0.058824
add a py or	spark context add py file path	0.166667
sort the list based on first value	test case sort result based on	0.333333
api with start offset	offset	0.021739
cross validator	ml cross validator	0.166667
model from the given path	path	0.020408
this instance contains a param	ml param	0.009524
setparams(self featurescol="features",	gaussian mixture set params featurescol	1.000000
and return its path	core	0.003021
merges them with extra values from input	map extra	0.040000
join of	join other	0.142857
dot product of two	vector dot other	0.050000
fractional	fractional	1.000000
returns the date	sql date	0.333333
test that the final value	with sgdtests test	0.111111
current [[dataframe]] and	data pivot pivot_col values	0.050000
key-value pairs	all pairs	0.500000
libsvm format into label indices	parse libsvm	0.125000
test for	test	0.015152
the :class dataframe as the specified table	sql data frame writer save as table name	0.333333
use only create a new hivecontext for testing	context create for testing cls sparkcontext	0.333333
resolves a param and validates the ownership	ml param params resolve param param	0.333333
checkpointed and	checkpointed	0.083333
or multiclass classification	cls data numclasses categoricalfeaturesinfo	0.250000
initial value of	with sgd set initial	0.111111
or	get or	0.200000
returns a paired rdd where the	matrix factorization	0.040000
comprised of vectors	mllib random rdds log normal	0.125000
instance contains a param with a	ml param params	0.013699
configuration property for the given key	conf key defaultvalue	1.000000
with arbitrary key and	core spark	0.020619
dump the profile stats into directory path	dump profiles path	1.000000
profilers on a per stage basis	profiler collector	0.142857
the trigger for the stream query	sql data stream writer trigger	0.083333
in the training	ldamodel training	0.034483
defined on the class to	param	0.006250
a word	word num	0.333333
wrapper of	ml	0.009174
off the heap maintaining the heap	heap	0.047619
point in rdd 'x' to all mixture	mixture	0.052632
return a resulting rdd that contains	core rdd cogroup	0.066667
converter to	converter	0.052632
initial value of weights	logistic regression with sgd set initial weights initialweights	0.333333
approximate distinct count of col	approx count distinct	0.071429
local property	local property key value	0.076923
size of number of	ml clustering	0.100000
a new dstream by applying 'full	full	0.066667
default 5	windowsize	0.166667
create a java	new java	0.166667
singular vectors of the	mllib linalg singular	0.035088
initial	initial	0.428571
pipelinemodel used for	pipeline model	0.071429
the input dataset this is called by the	dataset	0.020408
return an rdd containing all pairs of elements	core rdd	0.003460
better	better	1.000000
comprised of vectors	random rdds log normal vector	0.125000
value of	ml tree ensemble params	1.000000
items for columns	items cols	0.125000
new accumulator with a	accumulator	0.012987
or replaces a local temporary view with this	or replace temp view name	0.333333
comprised of	mllib random rdds log normal vector	0.125000
already partitioned data into	group by	0.041667
an rdd comprised of vectors containing i	random rdds exponential vector rdd	0.166667
all globals names read or written to	globals	0.076923
so	streaming query	0.010526
param and validates the	param param	0.100000
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto",	ml random forest classifier	0.023256
in libsvm format	libsvm p	0.250000
local property set in this thread or null	spark context get local property key	0.066667
it can be used in sql	sql sqlcontext	0.095238
between date1	between date1	1.000000
test statistic result	mllib stat test result	0.166667
set sample points from the population should be	mllib stat kernel density set sample sample	0.333333
indicates whether a training summary exists for	has summary	1.000000
setparams(self inputcol=none outputcol=none labels=none) sets params for this	string set params inputcol outputcol labels	0.333333
trees model	trees	0.066667
sets the accumulator's	core accumulator value value	0.050000
java parammap	java javaparammap	0.125000
new rdd of int containing elements	core spark	0.010309
test prediction on a model with weights already	linear regression with tests test prediction	0.500000
call java	mllib call java	1.000000
specifies the behavior when data or table already	data frame writer mode savemode	0.071429
limits the result count to the number specified	sql data frame limit num	1.000000
given parameters in this grid to	grid builder base on	0.076923
finding frequent items for columns possibly	frame freq items cols support	0.166667
sparkcontext which is associated	context spark	0.083333
previously saved	minpartitions	0.071429
value of	ml one	0.083333
:func awaitanytermination() can be used again	sql streaming query	0.011765
generated	train validation	1.000000
submit	submit	0.454545
frame boundaries from start	range between start	0.200000
instance contains a param	param	0.012500
for which predictions are known	regression model	0.031250
n elements from an rdd ordered in ascending	rdd take ordered	0.050000
the norm	linalg dense vector norm p	0.333333
values	standard	0.071429
which is a risk function corresponding to the	regression summary	0.035714
profile stats to stdout	profiler show	0.166667
spark	sparksession	0.125000
order	order	1.000000
wait	query	0.010753
class to track supported random forest parameters	random forest params	0.250000
behavior when data or	data frame writer mode savemode	0.071429
groups the :class dataframe	sql data frame group by	0.200000
set numtopfeature	numtopfeatures	0.111111
create a method for binary operator this	op name doc	0.166667
of model	ensemble model	0.058824
url url and connection properties	url	0.076923
number of rows of blocks in	num row blocks	0.500000
a data	sql data	0.024390
their vector representations	mllib word2vec model get vectors	0.166667
binary or multiclass classification	classifier cls data numclasses categoricalfeaturesinfo	0.250000
this instance with a randomly generated	one vs	0.125000
get or compute the number	linalg indexed row matrix num	0.100000
operation test for dstream mapvalues	operation tests test map values	1.000000
separators inside brackets	brackets split	0.083333
class inherit documentation from	inherit doc cls	0.045455
returns weighted averaged	mllib multiclass metrics weighted	0.333333
given path a shortcut of write() save path	ml one vs rest save path	0.200000
loads a parquet file stream returning	stream reader parquet path	0.083333
splits	splits	0.833333
stream query if this is	data stream	0.028571
a shortcut of write() save	ml pipeline save	0.166667
the dataframe in	data frame writer	0.014085
of this instance with	ml train	0.181818
calculates the length of a string	length col	0.050000
external sort when	core external	0.016129
given parameters in this grid to	ml param grid builder base	0.076923
min value for each original column during fitting	original min	0.250000
until any of	await any termination	0.142857
lower bound on the log likelihood	ldamodel log likelihood dataset	0.142857
:class dataframe as the specified table	sql data frame writer save as table	0.333333
frequency vectors or transform the	tf transform	0.045455
module in modlist to be placed into main	core modules to main modlist	0.333333
levenshtein distance of the two given	sql levenshtein left right	0.058824
returns the schema of	schema	0.033333
d samples drawn	numrows numcols	0.125000
much of memory for	core external merger object size	0.032258
create a new rdd of int containing elements	core spark context	0.011628
:py attr numfolds	num folds value	1.000000
= densematrix(2 2 range 4	mllib linalg dense matrix repr	0.142857
the correlation of two columns of	col2 method	0.055556
loads vectors saved using	load vectors sc	0.333333
inserts the content of the :class dataframe to	sql data frame writer insert into	0.500000
window of time	window	0.037037
for pipeline	pipeline	0.052632
:class dataframe that has exactly numpartitions partitions	data frame coalesce numpartitions	0.500000
convert this distributed model to a local	distributed ldamodel to local	0.111111
the month of a given date as integer	dayofmonth col	0.031250
in this model	model predict	0.333333
get or compute the number	mllib linalg indexed row matrix num	0.125000
log	distributed ldamodel log	1.000000
<http //en wikipedia	classifier	0.100000
columns of a dataframe	data frame corr	0.166667
sets	string set	1.000000
params	ml java params	0.125000
cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20	ml random forest classifier	0.023256
the cluster	mllib kmeans model cluster	0.333333
much of	object	0.027778
stopwords	stopwords	1.000000
heap maintaining the heap	heap	0.047619
computes an fp-growth model that contains frequent	mllib fpgrowth train cls data minsupport	0.100000
convert datetime or unix_timestamp into time	streaming dstream jtime timestamp	1.000000
be used again to wait for new	query	0.010753
generates an rdd comprised	mllib random rdds poisson vector rdd sc mean	0.200000
a line in libsvm format into	mlutils parse libsvm line line multiclass	0.111111
seed=none): sets params	set params	0.034483
value for each original column during fitting	min max scaler model original	0.062500
trained	tree ensemble	0.055556
inherited by any streaminglinearalgorithm	linear algorithm	0.076923
:py attr percentile	percentile value	1.000000
active queries associated with	manager active	0.066667
prefix of string in	prefix f	0.142857
threshold threshold in binary classification prediction in range	threshold	0.018182
temporary view with this	temp view name	0.111111
generalized	generalized	1.000000
characters as a hexadecimal number	unhex col	0.142857
key using	by key	0.026316
a dictionary of values	values	0.050000
a global	global	0.111111
extract the year of	sql year	0.050000
column standard	mllib standard scaler model	0.100000
rdd of key-value pairs (of form	rdd save	0.038462
new :class dataframe replacing a value with another	data frame replace to_replace	0.100000
the count of distinct elements in rdds	count	0.016949
note : experimental	generalized linear regression model	0.200000
an rdd of labeledpoint	mllib mlutils load lib svmfile sc path	0.125000
keys	keys	0.555556
returns accuracy equals	multiclass metrics accuracy	0.166667
query was terminated by an exception or none	sql streaming query exception	1.000000
get offsetranges	get offset ranges	1.000000
nulltype in	sql has nulltype	1.000000
__init__(self	regex tokenizer init	1.000000
on	streaming logistic regression with	0.500000
param with a given string	ml param params	0.013699
tree (e g depth 0	tree	0.020833
array containing the ids of all active stages	tracker get active stage ids	0.250000
each rdd contains the count	count by	0.100000
commutative reduce function	rdd reduce	0.142857
parse a field in	parse field	1.000000
for input	model	0.005587
or compute the number of	block matrix num	0.100000
this matrix	linalg dense matrix	0.083333
a class inherit documentation from its parents	inherit	0.037037
sets the accumulator's value only usable	accumulator value	0.050000
the norm of a	norm p	0.055556
infer	infer	1.000000
param with a	param params	0.014925
value of spark sql	sql sqlcontext get	0.333333
converts vector columns in	mllib mlutils convert vector columns to ml dataset	0.166667
column containing a json string into a	sql from json col	0.083333
tokens in the training set given the	ldamodel training	0.034483
tree (e	tree	0.020833
this	param params	0.014925
matrix to a rowmatrix	indexed row matrix to row matrix	0.333333
of int containing elements	core	0.003021
mincount the minimum number of times	min count mincount	0.200000
variance and count of the	rdd	0.003058
that all the	merger object size	0.032258
function on each rdd	transform func	0.117647
default min number of partitions for	context default min partitions	0.250000
converts vector columns in an input	mlutils convert vector columns to	0.166667
each dstreams in this context to	streaming streaming context	0.032258
a specific group matched by a java regex	str pattern idx	0.111111
densematrix	linalg block matrix	0.052632
vector columns	vector columns to	0.142857
singularvaluedecomposition if computeu was set	value decomposition u	0.100000
optional key	key	0.017857
external database table	table mode properties	0.200000
python rdd of	rdd save as	0.038462
the key-value pair rdd through a flatmap	rdd flat map	0.333333
sparsevector	sparse vector	0.062500
that	streaming query manager reset	0.011905
given parameters in this	grid builder add	0.200000
the dispatch to handle all function types	cloud pickler save function obj name	0.142857
extracts the embedded default param values and	ml param params extract param map	0.333333
builder	builder	0.454545
of document to rdd	document	0.040000
sets	perceptron classifier set	1.000000
the values for each key	key	0.035714
compute the standard deviation	stdev	0.047619
column standard deviation	mllib standard scaler model std	0.166667
the right singular vectors of the	linalg singular	0.017544
index of the original partition	partitions with index	0.100000
compare 2 ml types asserting that	ml persistence test compare	0.166667
mixture	mixture model k	0.200000
of the current [[dataframe]] and	data pivot pivot_col values	0.050000
of freedom for the null model	of freedom null	0.500000
value of	ml linear	0.066667
returns the precision-recall curve which is	binary logistic regression summary pr	0.083333
the :class dataframe to a data source	data frame	0.005000
the dot product of two vectors we	mllib linalg dense vector dot other	0.058824
for each key	by key numpartitions	0.111111
every element	numslices	0.166667
of a file added through c{sparkcontext addfile()}	core spark files get cls filename	0.200000
awaitanytermination() can be used again to wait	sql streaming query	0.011765
already partitioned	core	0.003021
create a new spark configuration	core spark conf init	0.250000
calculates the correlation of two columns	col1 col2 method	0.055556
number of features i e length	num features	0.333333
to get or create global	get or create cls	0.200000
table accessible via jdbc url url and connection	jdbc url table column	0.166667
table	table	0.312500
broadcast a read-only variable to the	broadcast value	0.125000
or two separate arrays of indices and	ml linalg	0.030303
setparams(self featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01 maxiter=100	gaussian mixture set params featurescol predictioncol k probabilitycol	0.500000
incrementing as expected	task context	0.142857
loads orc files returning the	reader orc path	0.200000
a local property set in this	local property key	0.035714
value for each original	model original	0.062500
distance between	distance v1 v2	1.000000
from the	mllib power iteration clustering	0.500000
squared	ml linalg sparse vector squared	1.000000
a left outer	left outer	0.333333
shortcut of write()	ml pipeline model	0.066667
sql context to	context sqlcontext	0.083333
a list of predicted ratings for input user	matrix factorization model predict all user_product	0.050000
months between date1	months between date1	0.333333
the given parameters in this grid to fixed	param grid builder add grid param	0.250000
accumulator's	core accumulator value value	0.050000
of obtaining a test statistic	mllib stat test	0.166667
in rdd 'x' to all mixture	mllib gaussian mixture	0.045455
squared distance from	sparse vector squared distance	0.166667
only create a new hivecontext for testing	sql hive context create for testing cls	0.333333
used again to	sql streaming query manager reset	0.011905
dependency on	dependency	0.040000
accumulator's value only usable in	core accumulator	0.030303
in this grid to fixed values	grid builder add grid param values	0.333333
behavior when data or table	data frame writer mode savemode	0.071429
precision of	metrics precision	0.200000
return a new rdd that is	core rdd coalesce	1.000000
each value in c{self} that is not contained	core rdd subtract other numpartitions	0.111111
a python object into an internal sql object	data type to internal obj	0.500000
sql storage type	sql type cls	0.250000
:class dataframe as a temporary table in	data frame as table df	0.333333
wait until any of	await any termination timeout	0.166667
rdd of	core rdd	0.013841
broadcast a	spark context broadcast	0.125000
param is explicitly set by user or has	params is defined param	1.000000
dataframe	data frame writer	0.014085
rdd 'x' to all mixture components	mixture model predict soft	0.142857
a streaming dataframe/dataset is written to a	stream writer output mode outputmode	0.083333
convert this vector	sparse vector	0.062500
the column mean	model mean	0.125000
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1 0	init featurescol labelcol predictioncol probabilitycol	0.333333
all mixture	mixture	0.052632
underlying output	data stream writer	0.041667
for each numeric	sql grouped data avg	0.058824
the area under	area under	0.166667
an object	obj	0.047619
creates a :class	spark session create	0.058824
:func awaitanytermination() can be used again to	streaming query manager reset	0.011905
note :	core rdd count approx	1.000000
from this thread such as the spark fair	spark	0.013158
casesensitive=false) sets	set	0.005917
single :class pyspark sql types longtype column	sql sqlcontext range start end step numpartitions	0.083333
on	streaming	0.005025
into a jvm seq of	seq sc cols converter	0.055556
returns a list of predicted ratings for	mllib matrix factorization model predict all user_product	0.050000
vectors to tf-idf vectors	mllib idfmodel transform x	0.142857
param with a given string name	ml param	0.009524
serializes a stream of list of pairs	serializer	0.062500
the key-value pair rdd through a flatmap function	rdd flat map values f	0.333333
selector type of	chi sq selector set selector type	0.111111
:func awaitanytermination() can	query	0.010753
base-2 logarithm of	log2 col	0.250000
convert this vector to the new mllib-local representation	vector as ml	0.250000
or compute the number of	num	0.025210
rdd 'x' to all mixture	mllib gaussian mixture model predict	0.100000
approximate distinct count of	approx count distinct col rsd	0.066667
comprised of vectors containing	random rdds log normal vector	0.125000
create an input stream that	utils create stream ssc hostname port	0.200000
return a resulting rdd that contains	rdd cogroup other numpartitions	0.066667
file to	file	0.028571
vector columns	vector columns	0.142857
each original column during fitting	max scaler model original	0.062500
sample without replacement based	data frame sample	0.066667
creates a	sparkcontext	0.142857
seed=none k=4 mindivisibleclustersize=1	maxiter seed	0.500000
of document to rdd of term	document	0.040000
csv	csv	0.777778
model	ml generalized linear regression model	0.166667
association	association	1.000000
fast version of a heappush followed by	heappushpop heap item	0.142857
cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1 0)	random forest classifier	0.022727
the input param belongs	param	0.006250
stop the execution of the	streaming streaming context stop	0.125000
predicted values on a toy model	regression with sgdtests	0.200000
true	true	1.000000
generates python code	code	0.071429
numfeatures=1 << 18 binary=false inputcol=none outputcol=none)	numfeatures binary inputcol outputcol	1.000000
of tree (e g depth 0 means	tree	0.020833
returns a :class dataframe representing the result	spark session sql sqlquery	0.250000
r	r	1.000000
train a gradient-boosted trees model for	mllib gradient boosted trees train	0.333333
boundaries defined from start	start	0.045455
number of times a	count	0.016949
make predictions on batches of data from	mllib streaming linear algorithm predict on	0.066667
which is a risk function	mllib regression metrics	0.090909
__init__(self	string indexer init	1.000000
named options filter out those the	opts schema	0.250000
set the trigger for	writer trigger	0.111111
with a function and attach docstring from func	defined function wrapped	0.333333
to a new column of	to	0.007692
value	core	0.003021
awaitanytermination() can be used again to wait for	manager	0.011236
merge the values for each key	key func numpartitions	0.066667
generates an rdd comprised	mllib random rdds normal vector rdd sc	0.200000
return a new	core	0.003021
again to wait for	reset	0.011236
for the stream	stream writer	0.041667
to pass else fail with an error	mllib mllib streaming test case eventually	0.250000
an rdd comprised of vectors containing i i	random rdds exponential vector rdd	0.166667
a resulting rdd that contains a	rdd cogroup	0.066667
an rdd	gamma vector rdd	1.000000
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10	ml lda set params featurescol maxiter seed	0.250000
create a new spark	spark	0.013158
certain time of day in utc	utc	0.050000
seq of columns that describes the sort order	data frame sort cols cols kwargs	0.142857
python code for a shared param class	ml param gen param code	0.333333
a	has	0.023256
this dstream	streaming dstream	0.277778
on	transform	0.125000
k=2 initmode="k-means||", initsteps=2 tol=1e-4 maxiter=20 seed=none)	k initmode	1.000000
instance is	is distributed	0.200000
a l{statcounter} object that captures the mean	rdd stats	0.083333
paramter	aftsurvival regression	0.333333
an rdd of row or tuple	rdd	0.006116
greatest value	sql greatest	0.055556
comprised of vectors	mllib random rdds log normal vector	0.125000
as the spark fair scheduler pool	core spark context	0.011628
converts matrix columns in an input dataframe to	mlutils convert matrix columns	0.083333
max abs vector	ml max abs scaler model max abs	1.000000
the log	log	0.071429
the content of the :class dataframe	sql data frame	0.021390
pyspark sql	sql	0.005051
non-streaming :class dataframe out into external	sql data frame write	0.071429
much of memory for this	external merger object	0.032258
single script file calling a global function	script with local functions	0.125000
__init__(self formula=none featurescol="features", labelcol="label",	ml rformula init formula featurescol labelcol	1.000000
boosted trees	boosted trees	0.166667
hadoop-supported file system	file path	0.035714
the training set	ldamodel training	0.034483
generate	with sgdtests generate logistic	1.000000
from start to	range start	0.500000
:class windowspec with	sql window range	0.166667
given parameters in this grid to fixed	param grid builder add grid param	0.250000
multiplies this blockmatrix by other, another blockmatrix	mllib linalg block matrix multiply other	0.200000
maxdepth	max depth	1.000000
property set in this thread	property key	0.066667
read a 'new api' hadoop	new apihadoop	0.333333
column scipy matrix from a	sci py tests scipy matrix size	0.090909
an input stream that	stream ssc addresses storagelevel	0.166667
next memory limit if the memory	external sorter next limit	0.200000
value of weights	weights initialweights	0.333333
tf	tf	0.384615
null values alias for na fill()	sql data frame fillna value subset	0.166667
mlutils	mlutils	0.625000
squared distance from a sparsevector or 1-dimensional numpy	ml linalg sparse vector squared distance other	0.166667
a python topicandpartition to	topic and partition init topic partition	0.055556
value of	ml hashing	1.000000
vectors of the singularvaluedecomposition	value decomposition	0.200000
of memory for	core external merger object	0.032258
multi-dimensional cube for the	sql data frame cube	0.055556
to all the jobs started	job	0.023810
the initial	logistic regression with sgd set initial	0.111111
of this	rdd	0.003058
compute the dot product	dense vector dot other	0.100000
obj assume that all	size obj	0.040000
__init__(self featurescol="features", predictioncol="prediction", maxiter=20 seed=none k=4 mindivisibleclustersize=1 0)	bisecting kmeans init featurescol predictioncol maxiter seed	1.000000
with the dispatch to handle all function	core cloud pickler save function obj name	0.142857
is not contained in	subtract	0.111111
the value of	get	0.021739
expected	expected	0.461538
rdd 'x' has maximum membership in this model	mllib gaussian mixture model predict	0.100000
configuration	conf init loaddefaults _jvm _jconf	1.000000
test predicted values on a toy	with sgdtests test	0.111111
transfer this instance to a java pipeline used	pipeline to java	1.000000
a paired rdd where the first element is	factorization	0.038462
converts vector columns in an	mlutils convert vector columns	0.166667
only create a new hivecontext for testing	create for testing cls sparkcontext	0.333333
vector conduct pearson's chi-squared goodness of fit test	stat statistics chi sq test	0.166667
builds and returns all combinations	builder build	1.000000
predicted ratings for input user	mllib matrix factorization model predict all user_product	0.050000
load labeled points saved using rdd	load labeled points sc path minpartitions	0.250000
all the	merger object size	0.032258
given parameters in this grid to	ml param grid builder add grid	0.100000
which each rdd contains the count of distinct	count by	0.100000
converts vector columns	convert vector columns to ml	0.166667
checkpointinterval=10 impurity="variance", seed=none variancecol=none)	decision tree regressor	0.058824
globals names read or written to by	pickler extract code globals	0.125000
returns weighted false	multiclass metrics weighted false	1.000000
cachenodeids=false checkpointinterval=10 impurity="variance",	ml decision tree regressor	0.066667
a list of active queries associated with this	query manager active	0.066667
to make predictions on batches	algorithm predict on	0.066667
sets the context to	streaming context	0.055556
svmwith	svmwith	1.000000
rdds of the dstreams	context transform dstreams transformfunc	0.125000
a left outer join of c{self} and c{other}	core rdd left outer join	0.200000
:class dataframe replacing	data frame replace to_replace	0.100000
this query	streaming query	0.010526
__init__(self featurescol="features",	linear regression init featurescol	1.000000
maps	maps	1.000000
the :class dataframe to a data source	data frame writer save path	0.142857
test that the model params are set	streaming kmeans test test model params	0.250000
makes a class inherit documentation from its	mllib inherit doc	0.045455
or compute the number of cols	mllib linalg block matrix num cols	0.333333
of a streaming dataframe/dataset is written to	stream writer output mode outputmode	0.083333
already partitioned data	core external group by	0.045455
for approximate distinct count	approx count distinct	0.071429
a pearson's independence test using dataset	ml chi square test test dataset featurescol	0.333333
error which	mllib regression	0.022727
length of a string or binary expression	sql length	0.050000
deviation values	std	0.142857
minutes of a	minute	0.040000
a right outer join	full outer join other numpartitions	0.111111
mixin for param maxiter max number of	has max iter	0.333333
parses the expression string	sql expr	0.125000
the trigger for the stream query	data stream writer trigger	0.083333
cost sum of squared distances of	compute cost x	0.142857
decayfactor timeunit to configure the kmeans algorithm	streaming kmeans	0.035714
using an associative and commutative reduce	reduce by	0.200000
mixin for param stepsize step size	has step size	0.333333
of	of	1.000000
sort the list based on first value	sort result based on key outputs	0.333333
can be used again	query	0.010753
group by key	external group by	0.045455
containing the distinct elements in this rdd	core rdd distinct	0.250000
paired rdd where the	matrix factorization model	0.043478
maxsentencelength	max sentence length	1.000000
the approximate quantiles of numerical columns	approx quantile col probabilities relativeerror	0.166667
columns specified	sql data frame	0.005348
filters rows using	data frame filter	1.000000
code for a	code name doc defaultvaluestr	0.111111
extract the week number of	weekofyear	0.043478
levenshtein distance of the two given	levenshtein	0.045455
thread such as the spark	core spark	0.010309
submitted from this thread such as the spark	spark	0.013158
rdds in a sliding window	value and window windowduration slideduration numpartitions	0.076923
fwe	fwe	1.000000
setparams(self labelcol="label", featurescol="features", predictioncol="prediction",	linear regression set params labelcol featurescol predictioncol	1.000000
hadoop configuration which is	hadoop	0.050000
a :class dataframe from external storage systems (e	data frame reader	0.166667
:func awaitanytermination() can be used again	streaming query manager reset	0.011905
awaitanytermination() can be used again to wait	sql	0.002525
representing a multiclass classification	classification	0.071429
__init__(self	ml standard scaler init	1.000000
inputformatclass	inputformatclass	0.238095
sql context to use for saving	java mlwriter context sqlcontext	0.333333
:class dataframe in parquet	data frame writer parquet	0.200000
java_stage	java_stage	0.555556
orc	orc	0.500000
function to get or create global taskcontext	core task context get or create cls	0.250000
setparams(self strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none) sets params	set params strategy missingvalue inputcols outputcols	0.200000
v	v	1.000000
saves the content	mode partitionby	0.133333
comprised of vectors containing	random rdds poisson	0.125000
comprised of vectors containing i i d samples	mllib random rdds	0.083333
approximately find at most k items	ml lshmodel approx	0.125000
this	linalg	0.022222
a converter to drop the names of	converter	0.052632
of parameters specified by the param	ml param	0.009524
:class dataframe whose	data	0.011628
and other	other numpartitions	0.083333
:func awaitanytermination() can be used again to	reset	0.011236
returns the soundex encoding for a string >>>	sql soundex	0.055556
stratified sample without replacement based on the fraction	sample	0.050000
this instance's params to the wrapped java	ml java params to	0.045455
cluster centers represented as a list of numpy	bisecting kmeans model cluster centers	0.047619
comprised of vectors containing i i d samples	random rdds log normal	0.125000
an input stream from an queue of rdds	context queue stream rdds	1.000000
wait a given amount	timeout	0.071429
months	months	0.750000
the objects	size	0.009174
this instance contains a param with a	param params has param	0.019231
each rdd contains the count of distinct elements	count by	0.100000
labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1	labelcol predictioncol probabilitycol	0.166667
converter to	converter datatype	0.071429
string column after position pos	pos	0.022222
new java	ml java wrapper new java	0.333333
replace null values alias for na fill()	sql data frame fillna value subset	0.166667
convert this matrix to a coordinatematrix	mllib linalg indexed row matrix to coordinate matrix	0.333333
fast version of a heappush	heappushpop heap	0.142857
this vector to	sparse vector	0.062500
versionadded : 0 9 0	regression with sgd	1.000000
tests whether this instance contains a param	param params has param paramname	0.142857
collect each rdds into the returned list	spark streaming test case collect dstream	1.000000
to the given path a shortcut of write()	ml one vs rest	0.052632
to all mixture	mixture model	0.066667
rdd of key-value pairs (of form	core rdd save as	0.037500
all the	core external merger object	0.032258
that :func awaitanytermination() can be used	query manager reset	0.011905
buckets the output by the given	writer bucket by numbuckets	0.200000
convert each python object into java	to java	0.136364
tables/views in the specified	tables	0.071429
load labeled points saved using rdd saveastextfile	mlutils load labeled points sc path minpartitions	0.250000
selector type of the chisqselector	selector set selector type selectortype	0.333333
on a model with weights	linear regression with	0.111111
sql context to use for loading	ml java mlreader context sqlcontext	0.333333
computes hex	hex	0.166667
the embedded params to the companion java object	ml java params transfer params to java	0.500000
spark jobs	spark job	1.000000
new terminations	query	0.010753
restore an object of	restore name fields value	0.333333
sets	min max scaler set	1.000000
qr decomposition	tall skinny qr computeq	0.500000
finding frequent items for columns	freq items	0.166667
a value to a mllib vector	to vector value	0.250000
accumulator's data type returning a new value for	accumulator	0.012987
queries so that :func awaitanytermination()	streaming	0.005025
obj assume	merger object size obj	0.040000
an fp-growth model	mllib fpgrowth train cls data minsupport numpartitions	0.100000
+= operator	core accumulator iadd	0.500000
iterator of deserialized objects from	serializer load	0.083333
default param	param map	0.500000
value for each original column	ml min max scaler model original	0.062500
test that the final value of weights	logistic regression with sgdtests test	0.111111
minutes of a given date	sql minute col	0.050000
__init__(self withmean=false withstd=true	ml standard scaler init withmean withstd	1.000000
the trigger for the stream query if this	stream writer trigger	0.083333
dispatch to handle all function types	core cloud pickler save function obj	0.142857
0] for feature selection by fdr	chi sq selector set fdr fdr	0.200000
converts vector columns in an	mlutils convert vector columns from ml dataset	0.166667
data into disks	by	0.014286
by the	bucket by	0.200000
sort the list based on	sort result based on key	0.333333
lda keeplastcheckpoint is set to	distributed ldamodel get	0.066667
convert matrix attributes which are array-like	linalg matrix convert	0.166667
sets vector size	mllib word2vec set vector size	1.000000
comprised of	mllib random rdds poisson vector	0.125000
a list of conditions and returns one	sql column otherwise value	0.050000
an rdd comprised of	random rdds exponential vector rdd	0.166667
in json format (json lines text format or	sql	0.002525
again to wait for new	query	0.010753
be used again to wait for new terminations	streaming	0.005025
which is	linear regression summary	0.027778
until any of	query manager await any termination	0.142857
specified by the param grid	param grid	0.200000
are the right singular	linalg singular	0.017544
all the elements in seen in	windowduration slideduration	0.083333
year of a given date	sql year col	0.050000
an fp-growth model that	mllib fpgrowth train cls data minsupport numpartitions	0.100000
a local temporary view with this	temp view name	0.222222
context to use for loading	java mlreader context	1.000000
to consist of key	key ascending numpartitions keyfunc	0.071429
persist its values across operations after the first	persist storagelevel	0.166667
hadoop outputformat api mapreduce	apihadoop dataset conf keyconverter valueconverter	0.500000
__init__(self degree=2	ml polynomial expansion init degree	1.000000
of the :class dataframe in orc	sql data frame writer orc	0.200000
dump already partitioned data into disks	spill	0.038462
find norm of the given vector	vectors norm vector p	1.000000
is set to a	context set	0.125000
:func awaitanytermination() can be	query manager	0.011905
list of predicted ratings for	matrix factorization model predict all user_product	0.050000
new terminations	manager	0.011236
category if	multiclass	0.142857
selectortype	selector type	0.100000
in this model	mllib kmeans model	0.250000
a multiclass classification	classification	0.071429
lda keeplastcheckpoint is set to	ldamodel get	0.066667
that stores item	ml alsmodel item	0.250000
is checkpointed and materialized either reliably	is checkpointed	0.142857
kolmogorov-smirnov ks test for data sampled from a	mllib stat statistics kolmogorov smirnov test data distname	0.111111
logisticregression	logistic regression	0.040000
key-value pair rdd through a flatmap	core rdd flat map	0.333333
start to end exclusive increased	start end	0.090909
each value in the key-value	map values	0.166667
function without changing the keys this also	f	0.010526
of conditions and returns one of	sql column otherwise value	0.050000
destroy all data	destroy	0.111111
output options	writer options	1.000000
specifies the behavior when	writer mode savemode	0.333333
convert a value to an int if possible	ml param type converters to int	0.250000
be used again to wait	streaming query manager reset	0.011905
the observed data	observed	0.058824
columns in an input dataframe from	columns	0.039216
objects	object	0.027778
:func	sql streaming query	0.011765
a given string name	param params has	0.019231
parameters in this grid	ml param grid builder base on	0.076923
numpartitions partitions	numpartitions	0.250000
the absolute path of a file added through	core spark files get	0.125000
table based on the	table tablename path	0.166667
computes column-wise summary statistics for the	statistics col stats rdd	0.200000
given data type json string	datatype json string	0.333333
cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto",	ml random forest classifier	0.023256
each rdd contains the count of distinct	count by	0.100000
cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)	random forest classifier	0.022727
__init__(self scalingvec=none inputcol=none outputcol=none)	ml elementwise product init scalingvec inputcol outputcol	1.000000
for each key using	key func numpartitions	0.066667
fit	fit	0.700000
explained variance	explained variance	0.333333
java parammap	java	0.012195
column of corresponding string values	string	0.041667
blockmatrix by other, another blockmatrix	linalg block matrix multiply other	0.200000
mean	rdd	0.003058
stdout	profiler show	0.166667
dump	group by spill	0.047619
true label	label col	0.400000
insert	insert	1.000000
an rdd ordered in	core rdd take ordered	0.050000
collect each rdds into the returned list	test case collect dstream	1.000000
already	external group	0.045455
queries so that :func	reset	0.011236
monotonically	sql monotonically	1.000000
class generated by namedtuple	namedtuple	0.166667
new dstream in	streaming streaming	0.047619
an input stream that pulls events	stream ssc hostname	0.200000
boundaries in increasing order for which predictions are	ml isotonic regression model boundaries	0.333333
from external storage systems (e	reader	0.040000
to wait	sql streaming	0.010204
of c{self} and	core	0.006042
converts vector columns in an input dataframe	mlutils convert vector columns to ml	0.166667
and then merges them with extra	map extra	0.040000
a range	range	0.030303
class inherit documentation	mllib inherit	0.045455
value of the given column which could	col	0.016393
of the first occurrence of substr	substr	0.071429
forget about past terminated	streaming query manager reset terminated	0.200000
labeledpoint	load lib svmfile sc path numfeatures minpartitions	0.125000
data or	streaming streaming context get or	0.200000
fmeasure	fmeasure	1.000000
for this pca	pca	0.125000
removes all cached tables from the in-memory cache	sql sqlcontext clear cache	1.000000
the sort order	sql data frame sort	0.250000
a python parammap	param map from	0.250000
in this context to	streaming context	0.055556
value of	ml standard	1.000000
:func awaitanytermination() can be used again	sql	0.002525
sample without	sample	0.050000
resulting rdd that contains a tuple	core rdd cogroup other numpartitions	0.066667
fp-growth model that	fpgrowth train cls	0.200000
broadcast a read-only variable to the cluster	spark context broadcast value	0.125000
used in sql statements	returntype	0.071429
saves the content of the	mode partitionby	0.133333
with another	other	0.033333
of this instance with	ml param	0.009524
a file added through c{sparkcontext addfile()}	core spark files get cls filename	0.200000
returns a :class dataframe representing the	sql spark session sql sqlquery	0.250000
day of the month of	sql dayofmonth col	0.031250
frame but not in another frame	data frame subtract other	0.333333
the files in	core	0.003021
stratified sample without replacement based	frame sample	0.066667
the levenshtein distance of the	sql levenshtein left	0.058824
model that has a	model	0.005587
dependent variable given a vector	linear regression model base	0.142857
a python parammap into a java	to java	0.045455
the sql context to use	context sqlcontext	0.083333
the :class dataframe to a data	data frame writer save	0.083333
returns a dummy params instance used as	params dummy	0.111111
model fitted by logisticregression	logistic regression model	0.083333
choose one directory for spill by number n	external merger get spill dir n	1.000000
so	sql streaming query manager	0.011905
whether this instance is	is distributed	0.200000
recommends the top "num"	factorization model recommend	0.250000
create an input stream that pulls events	create stream ssc hostname port storagelevel	0.200000
to a java pipelinemodel used	ml pipeline model to java	0.100000
user-facing catalog api accessible through sparksession catalog	catalog	0.062500
be used again	reset	0.011236
find norm of the given vector	mllib linalg vectors norm vector	1.000000
containing elements	core spark context	0.011628
in place	in place	1.000000
sets params	set params featurescol	0.100000
that makes a class inherit documentation	mllib inherit doc	0.045455
java parammap into	java	0.012195
test for data	test data	0.166667
positive rate for	positive rate	0.166667
used again to wait for new	query manager	0.011905
predictioncol prediction column name	prediction col	0.142857
outputformat api mapred	outputformatclass	0.111111
of a file added through	core spark files	0.125000
this instance contains a	has	0.011628
private class to track supported impurity measures	tree regressor params	0.250000
stop the execution of	streaming context stop	0.125000
predicted ratings for input	mllib matrix factorization model predict all user_product	0.050000
matrix attributes	matrix	0.030303
setparams(self	set params rank	1.000000
which is	regression	0.040000
day of the month of a given	dayofmonth col	0.031250
sets vector	set vector	1.000000
creates an external table based	sql sqlcontext create external table tablename path	0.250000
even if users construct taskcontext instead of	task context	0.142857
set k decayfactor timeunit to	streaming	0.005025
ml instance	mlreader	0.037037
transforms the input dataset with optional parameters	transformer transform dataset params	1.000000
rescale each feature individually to a	max scaler	0.200000
'x' to all mixture components	mixture model predict soft x	0.142857
create a converter to drop the	create converter datatype	0.166667
attr predictions which gives	generalized linear regression summary	0.090909
enter	enter	1.000000
the uid	params reset uid newuid	0.333333
including lambda function	function name f	0.166667
get a local property set	spark context get local property key	0.066667
this dataframe	sql data frame	0.005348
converts matrix columns in	mllib mlutils convert matrix columns to ml dataset	0.166667
all the objects	external merger	0.031250
libsvm format into an rdd of labeledpoint	load lib svmfile	0.125000
contains	params has param	0.019231
again to wait for	sql streaming query manager reset	0.011905
param with	ml param params	0.013699
of this instance with	ml one vs	0.142857
the null	regression summary null	0.250000
in "predictions" which gives the probability of	ml logistic regression summary probability	0.166667
"predictions" which gives the true label of each	ml logistic regression summary label col	0.333333
a jvm seq	seq sc cols	0.055556
show	show	1.000000
column scipy matrix from	sci py tests scipy matrix	0.090909
new :class dataframe with an alias set	data frame alias alias	0.500000
a converter to drop the	converter	0.052632
of names of	sqlcontext table names	0.066667
id	groupid	0.166667
__init__(self inputcol=none outputcol=none indices=none names=none)	vector slicer init inputcol outputcol indices names	1.000000
iterations default 1	iterations	0.043478
python topicandpartition to	init topic partition	0.055556
partial objects do not serialize correctly	save partial obj	0.125000
make sure user configuration is respected spark-19307	core spark submit tests test user configuration	1.000000
that	query manager reset	0.011905
make predictions on batches of data from a	streaming linear algorithm predict on	0.066667
kmeans algorithm for fitting and predicting	streaming kmeans	0.035714
intermediatestoragelevel	intermediate storage level	1.000000
test for data sampled from a continuous distribution	test data	0.166667
file added through c{sparkcontext addfile()}	core spark files get cls filename	0.200000
the n	n	0.055556
much of	object size	0.032258
arbitrary key and value	core spark	0.020619
null model	ml generalized linear regression summary null	0.250000
given parameters in this grid	ml param grid builder add grid	0.100000
c{self} and	core	0.006042
passed in a profile object is returned	basic profiler profile	0.200000
number of times a token must	count	0.016949
inserts the content of the :class dataframe	data frame writer insert into	0.500000
soundex encoding	sql soundex col	0.055556
+= operator adds a term to	accumulator iadd term	0.142857
minutes	minute col	0.050000
each rdds into the	dstream n block	0.333333
numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	random forest	0.041667
libsvm format into an rdd of labeledpoint	load lib svmfile sc path	0.125000
an input stream that	stream ssc hostname	0.200000
the cluster centers represented as	cluster centers	0.090909
the values for each key using an	by key func	0.062500
from a single	offset	0.021739
fast version of a heappush followed by a	core heappushpop heap item	0.142857
the residual degrees of freedom	generalized linear regression summary residual degree of freedom	0.250000
weights computed for every feature	linear model weights	0.250000
all	core	0.003021
numtrees=20 featuresubsetstrategy="auto",	ml random forest	0.142857
partitions to use during reduce tasks (e	rdd default reduce partitions	0.166667
the content of the :class dataframe in	data frame writer	0.028169
setparams(self p=2	normalizer set params p	1.000000
all the jobs	job	0.023810
of names	table names	0.066667
active queries associated with	streaming query manager active	0.066667
test that the final value of weights	sgdtests test	0.142857
param with a given string	ml param	0.009524
values of the accumulator's data type returning a	accumulator	0.012987
the accumulator's data type	accumulator	0.012987
generates an rdd comprised of vectors	mllib random rdds uniform vector rdd sc	0.200000
containing the distinct	distinct	0.055556
l{statcounter} object that captures the mean	stats	0.055556
the first n rows to the console	show n	0.333333
make predictions on batches of	mllib streaming linear algorithm predict on	0.066667
schema in	schema	0.033333
converts matrix columns	mlutils convert matrix columns from ml dataset	0.166667
:class column for distinct count of col	count distinct col	0.040000
dot product of two vectors we support	dot other	0.050000
return the column mean values	model mean	0.125000
specifies the underlying output	sql data stream writer format	0.333333
returns a list of predicted ratings for	matrix factorization model predict all user_product	0.050000
rowmatrix	linalg row	0.500000
components	predict soft x	1.000000
is assumed to consist of key value pairs	key ascending numpartitions keyfunc	0.071429
line in libsvm format into label indices values	mllib mlutils parse libsvm line line	0.111111
vector size	vector size	1.000000
transform the rdd	transform	0.062500
or transform	hashing tf transform	0.045455
load a model from the given path	java loader load cls sc path	1.000000
compute the standard deviation of	core rdd stdev	0.066667
makes a class inherit documentation from its parents	inherit doc	0.045455
numpartitions	num partitions	0.250000
transfer this instance's params to the wrapped java	params to	0.035714
collect each rdds into the returned list	streaming test case collect dstream n block	1.000000
to fixed	param	0.006250
columns in an input dataframe to the :py	columns from	0.125000
with a given string name	param params has param	0.019231
right singular	mllib linalg singular	0.017544
the frame boundaries defined from start	start	0.045455
but return	core	0.003021
ntile	ntile	1.000000
note :	rdd count approx	1.000000
for	merger	0.025641
perform a right outer join of c{self} and	core rdd full outer join other	0.200000
max abs vector	max abs scaler model max abs	1.000000
that all the objects	object size	0.032258
function including lambda function	function name f	0.166667
converts	sql to	0.041667
corresponding to the expected value of	ml	0.001835
the accumulator's data type	accumulator param	0.038462
link	link	1.000000
matrix columns in an	matrix columns	0.142857
computes the area under	classification metrics area under	0.166667
for each original column during	min max scaler model original	0.062500
conduct pearson's chi-squared goodness of fit test of	stat statistics chi sq test	0.166667
underlying sql storage type	sql type	0.250000
of columns for the given	columns	0.019608
for list	list	0.066667
added through	core spark files get	0.125000
computes average values for each numeric columns for	grouped data	0.035714
the cluster centers represented as	bisecting kmeans model cluster centers	0.095238
value of	ml param has input	1.000000
:class dataframe to an	sql data frame	0.005348
year of a given date as integer	year	0.040000
distributed matrix whose columns are the left singular	linalg singular	0.017544
take	take	1.000000
cluster centers represented as a list of numpy	mllib kmeans model cluster centers	0.083333
dot product	dense vector dot other	0.100000
0 1 0] for feature selection by fdr	chi sq selector set fdr fdr	0.200000
of objects from the	serializer load	0.083333
param with a given	has param	0.019231
this model	mllib linear model	0.125000
be used with the spark sink	maxbatchsize	0.037037
create a python topicandpartition to map	partition init topic partition	0.055556
with the spark sink	ssc addresses storagelevel maxbatchsize	0.045455
the ownership	ml param params resolve	0.333333
list of :class	data frame	0.010000
null	null	0.500000
labels corresponding to indices to	model labels	0.166667
a value to list	to list	0.250000
predictions which	generalized linear regression	0.090909
"predictions" which gives the probability of each	ml logistic regression summary probability	0.166667
pair rdd through a flatmap	core rdd flat	1.000000
number of top	set num top	1.000000
specifies the input	reader	0.040000
setparams(self inputcol=none outputcol=none labels=none) sets params	string set params inputcol outputcol labels	0.333333
a local property set in this	context get local property key	0.066667
choose marshal or pickle as serialization protocol automatically	auto serializer	1.000000
all mixture components	mllib gaussian mixture model predict soft x	0.142857
test the partition	context tests test partition	1.000000
comprised of	mllib random rdds exponential	0.125000
return whether this rdd	core rdd	0.003460
output a python rdd	rdd save as	0.038462
+= operator adds a term to	iadd term	0.142857
count of col or cols	count	0.016949
for new	sql streaming query	0.011765
summed over all trees in the ensemble	mllib tree ensemble model	0.058824
extracts the embedded default param values and user-supplied	param params extract param map	0.333333
to the same time of day in utc	to utc	1.000000
controlling	level	0.125000
words	words	1.000000
load labeled points saved using rdd	mlutils load labeled points sc path minpartitions	0.250000
called when processing of a	streaming listener on	0.200000
param with a given	param params has	0.019231
all	all	0.416667
contains a param with a given string	ml	0.001835
factors in two columns id and	factors	0.142857
create an input stream that pulls events	create stream ssc hostname port	0.200000
into	mllib mlutils parse	0.250000
term to this	term	0.040000
__init__(self inputcol=none outputcol=none	min hash lsh init inputcol outputcol	1.000000
set sample points from the population should	mllib stat kernel density set sample sample	0.333333
centroids of that particular batch has	timeunit	0.025641
mixin for param elasticnetparam the elasticnet mixing parameter	has elastic net param	0.200000
an rdd previously saved	minpartitions	0.071429
code for a shared param class	param gen param code name doc	0.333333
residual degrees	linear regression summary residual degree	0.500000
k classes	classes	0.034483
a paired rdd	factorization model	0.043478
a param with a given string name	param	0.012500
the	core rdd	0.010381
lda keeplastcheckpoint is set	ldamodel get	0.066667
bandwidth of each sample defaults to 1 0	bandwidth bandwidth	0.125000
gateway	gateway	0.833333
linear regression model derived from a least-squares	linear regression model	0.066667
parameters passed as a list of	core spark conf	0.055556
much	external merger object size	0.032258
convert a list of column	to	0.007692
paired	factorization	0.038462
python direct kafka rdd get offsetranges	kafka rdd get offset ranges	1.000000
parameters passed as a list of	spark conf set	0.111111
applying a function	map f	0.074074
dataframestatfunctions for statistic functions	data frame stat	0.250000
n rows to the	n truncate	0.250000
session to	session	0.050000
of two	ml linalg dense vector	0.100000
drawn	numrows numcols	0.125000
the spark sink deployed on a	addresses storagelevel maxbatchsize	0.045455
are array-like or buffer to array	to array array_like dtype	0.166667
a new spark configuration	spark conf init loaddefaults _jvm _jconf	0.250000
specifies how data of a	sql data stream	0.031250
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto",	ml random forest classifier	0.023256
sets	logistic regression set	1.000000
first argument-based logarithm of the second argument	log arg1 arg2	0.200000
wait	streaming context await termination or timeout timeout	0.125000
find norm of the given vector	linalg vectors norm vector	1.000000
on	with	0.111111
t-statistic of estimated coefficients and intercept	ml linear regression summary t values	1.000000
the sum	data sum	0.333333
submit and test a single script file calling	submit tests test script with local functions	1.000000
multi-dimensional cube for the current	data frame cube	0.055556
pickle_registry	pickle_registry	1.000000
number of months between date1 and date2	months between date1 date2	0.333333
create a new accumulator with a	accumulator	0.012987
the standard deviation of this	core rdd stdev	0.066667
accumulator	accumulator	0.077922
new	new	0.437500
given string	has	0.011628
a	param params	0.029851
number of	model num	0.250000
the rdd	rdd partition by	0.062500
create a column scipy matrix from a dictionary	sci py tests scipy matrix	0.090909
forceindexlabel=false)	forceindexlabel	0.285714
a multiclass classification	linear classification	0.142857
push item	item	0.062500
java onevsrest create and return a	one vs rest from java	0.142857
attr lda keeplastcheckpoint is set to	distributed ldamodel	0.052632
newline-delimited json	json path	0.100000
get the cluster centers represented as	mllib bisecting kmeans model cluster centers	0.083333
the kmeans algorithm for fitting	kmeans	0.025641
underlying sql storage type for	type sql type	0.250000
this model instance	mixture model	0.066667
or	col	0.049180
that	sql	0.005051
creates an external	create external	1.000000
computes the levenshtein distance of	sql levenshtein left right	0.058824
value of weights is close	parameter accuracy	0.029412
this model	ml generalized linear regression model	0.166667
return	spark streaming	0.333333
gaussian	ml gaussian	0.333333
as spark executor memory this	spark	0.013158
that :func	sql streaming	0.010204
normal	normal	1.000000
train the model on the incoming dstream	streaming kmeans train on dstream	1.000000
linear regression model	linear regression model base	0.142857
returns accuracy equals to the total number of	metrics accuracy	0.166667
numuserblocks	num user blocks	1.000000
underlying output data source	frame writer format source	0.333333
test that the final	with sgdtests test	0.111111
sets the accumulator's value only usable in	accumulator value value	0.050000
train a gradient-boosted trees model for	mllib gradient boosted trees train classifier cls	0.333333
every feature	mllib	0.010526
again to	sql streaming query manager reset	0.011905
type of the chisqselector	type selectortype	1.000000
set pipeline stages	pipeline set stages	1.000000
number of columns that make up each block	cols per block	0.333333
dataframe	data frame corr	0.166667
onevsrest	one vs rest from	0.142857
:class dataframe in json format (json lines text	sql data frame	0.005348
featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online",	featurescol maxiter seed	0.200000
least value of the list of	least	0.043478
comprised of vectors containing i i	mllib random rdds log normal vector	0.125000
comprised of vectors containing	mllib random rdds log	0.125000
:class dataframe	sql data frame reader	0.111111
signed shift the	sql shift	0.500000
convert this matrix to the new mllib-local representation	mllib linalg matrix as	1.000000
minconfidence	minconfidence	1.000000
class	class	1.000000
a jvm scala map from a	data frame jmap jm	0.111111
trees model for classification	trees	0.066667
for this udt	user defined	0.333333
all mixture components	mixture model predict soft x	0.142857
script file	script	0.142857
this rdd and its	core rdd	0.003460
in "predictions" which gives the features of each	ml linear regression summary features col	0.166667
number of nonzero elements	vector num	0.181818
of labeledpoint	mllib mlutils load lib svmfile sc	0.125000
a term to this accumulator's	core accumulator add term	0.066667
return whether this rdd is	core rdd is	1.000000
to select filter	selector model selected features	0.333333
numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")	numbuckets inputcol outputcol relativeerror	1.000000
copy all params defined	param params copy params	0.200000
names into a jvm seq of	seq sc cols converter	0.055556
1 0] for feature selection by	chi sq selector	0.125000
string name	has	0.011628
right outer join of	full outer join other	0.111111
this instance to a java onevsrest used	one vs rest to java	0.166667
for this model	model compute	0.133333
used again to wait for	sql streaming query manager reset	0.011905
mindocfreq=0	mindocfreq	0.166667
converts vector columns in an input	mllib mlutils convert vector columns to ml dataset	0.166667
this model	mixture model	0.066667
this dct	dct	0.125000
and	core	0.048338
table in	table df tablename	0.083333
instance contains a param with	params	0.006623
executors if	unpersist blocking	0.166667
setparams(self inputcol=none outputcol=none) sets params	set params inputcol outputcol	0.333333
outputformat	outputformatclass	0.111111
of weights is close to the desired value	parameter accuracy	0.029412
train a gradient-boosted trees model for regression	mllib gradient boosted trees train	0.166667
each original column during	scaler model original	0.062500
in libsvm format into	mllib mlutils parse libsvm	0.125000
:class windowspec with	sql window range between	0.166667
create a dense	dense	0.111111
columns for the given table/view in	columns	0.019608
point in rdd 'x' to all mixture components	gaussian mixture model predict soft x	0.142857
columns in an input dataframe to the :py	columns	0.039216
return a new dstream by applying 'full	full	0.066667
set the selector type of	chi sq selector set selector type	0.111111
which is a risk	regression	0.020000
return a copy of the rdd partitioned using	core rdd	0.003460
contain a given key?	contains key	1.000000
labels corresponding to indices to be	model labels	0.166667
is generated by applying mappartitionswithindex() to	map partitions with index f preservespartitioning	0.055556
transform the rdd of	transform	0.062500
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20	ml random forest classifier	0.023256
extracts the embedded default param values	ml param params extract param	0.333333
block	block	0.727273
submit and test a script	submit tests test	0.142857
much of memory for	merger object size	0.032258
pearson correlation coefficient for col1 and col2	corr col1 col2	0.500000
calculates the length of	sql length col	0.050000
mincount the minimum number of times a token	min count mincount	0.200000
to this	core	0.003021
computes column-wise summary statistics	stat statistics	0.125000
the norm of	ml linalg sparse vector norm	0.333333
seq of columns that describes the sort	frame sort cols cols kwargs	0.142857
an fp-growth model	mllib fpgrowth train cls data	0.100000
conditions and returns one of	sql column otherwise value	0.050000
set the initial	sgd set initial	0.111111
model with weights	mllib streaming linear regression with	0.111111
withmean=false withstd=true	withmean withstd	1.000000
that :func awaitanytermination() can be used	sql streaming	0.010204
the rdd partitioned using the	rdd partition	0.062500
wait for the execution	streaming context await termination or timeout timeout	0.125000
"word"	synonyms word	0.166667
ensure that daemon and workers terminate on sigterm	core daemon tests test termination sigterm	0.333333
get the cluster centers represented as a list	model cluster centers	0.090909
format or newline-delimited json	writer json path	0.125000
or list in	oneatatime default	0.250000
specifies how data of	data stream	0.028571
from	spark context range	1.000000
returns the receiver operating characteristic roc curve which	binary logistic regression summary roc	0.166667
returns the value	get	0.021739
make predictions on batches of	algorithm predict on	0.066667
gets summary (e g	summary	0.024390
a logistic	logistic	0.062500
a param and validates the ownership	ml param params resolve param param	0.333333
condition to pass else fail with	streaming test case eventually condition	0.333333
create a python topicandpartition to map to	init topic partition	0.055556
is set to a different value	context set	0.125000
again to wait	reset	0.011236
results immediately to the master as a	locally func	0.142857
this	streaming streaming	0.047619
to a certain time of day in utc	from utc	0.125000
the libsvm format into an rdd of labeledpoint	mlutils load lib svmfile sc path	0.125000
standard deviation of	rdd stdev	0.066667
__init__(self labelcol="label",	ml generalized linear regression init labelcol	1.000000
stopwords	stop words	0.500000
for every	mllib	0.010526
rdd of points using the model	model	0.005587
can be used again to	query manager reset	0.011905
stage	stage	0.437500
a densematrix	ml linalg matrices dense numrows numcols values	0.333333
null	ml generalized linear regression summary null	0.250000
partial objects do not serialize correctly	cloud pickler save partial obj	0.125000
awaitanytermination() can	streaming	0.005025
week number of a given date	weekofyear col	0.055556
ordered in	ordered	0.076923
add a py or zip dependency	core spark context add py	0.166667
removes the specified table from the in-memory	sql sqlcontext uncache table tablename	0.250000
obj assume that all	object size obj	0.040000
trained	mllib tree ensemble	0.111111
cluster centers represented as a list	bisecting kmeans model cluster centers	0.095238
create a new dstream in which each	streaming streaming context transform	0.066667
in the database dbname	dbname	0.045455
get or compute the number	block matrix num	0.100000
attr predictions which gives	linear regression summary	0.013889
parse string representation back into the sparsevector	mllib linalg sparse vector parse s	1.000000
the :class statcounter members as a dict	core stat counter as dict sample	0.333333
generates an rdd comprised	random rdds poisson vector rdd sc mean	0.200000
bisecting k-means	bisecting kmeans	0.083333
in	builder base	1.000000
so that :func awaitanytermination() can be	manager	0.011236
minutes of a given date as	minute col	0.050000
return a new rdd	core rdd	0.006920
of nodes summed	nodes	0.037037
resulting rdd that contains a tuple	rdd cogroup other numpartitions	0.066667
min value for	min	0.041667
the deviance for	regression summary deviance	0.125000
estimated coefficients	linear regression summary	0.013889
an javardd of object	ml	0.001835
the jobs started by	job	0.023810
containing a json string into	json	0.043478
a labeledpoint to a string in libsvm format	labeled point to libsvm	0.500000
partial objects do not	save partial obj	0.125000
shared param class	param gen param	0.333333
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10	lda set params featurescol maxiter seed	0.250000
each dstreams in this	streaming streaming	0.047619
note : experimental	imputer	0.200000
or map stored in the column	col	0.016393
an object is of	obj	0.023810
of the month of	sql dayofmonth col	0.031250
java udf	java	0.012195
output a python rdd of key-value	rdd	0.009174
by other, another blockmatrix	mllib linalg block matrix multiply other	0.200000
the root mean squared error which	mllib regression	0.022727
add a py	context add py	0.166667
returns the full class name	class cls	0.333333
of two columns of a dataframe as	sql data frame corr col1	0.166667
elements from start to end exclusive increased by	core spark context range start end	0.166667
components	params	0.006623
true default or minimized false	ml evaluator is larger better	0.166667
forget about past terminated queries so that	reset terminated	0.200000
setparams(self mindocfreq=0 inputcol=none outputcol=none) sets params for this	set params mindocfreq inputcol outputcol	0.333333
a python function including lambda function	function	0.055556
elements	rdd	0.006116
outer	outer	1.000000
sets the given parameters in this grid to	grid builder add grid	0.100000
model to make predictions on batches	streaming linear algorithm predict on	0.066667
multi-level	depth	0.222222
column containing a json string into	json col	0.083333
copy all params defined on	ml param params copy params	0.200000
specified	sql data frame	0.010695
this	ml param params	0.013699
submit and test a script with	submit tests test	0.142857
sets	validator set	1.000000
broadcast on the executors if the	broadcast unpersist blocking	0.500000
setparams(self splits=none inputcol=none outputcol=none handleinvalid="error") sets params for	set params splits inputcol outputcol handleinvalid	0.500000
versionadded : 1 2 0	regression with lbfgs	1.000000
cube	cube	1.000000
conf	conf	0.250000
:py attr variancecol	variance col value	1.000000
provides methods	streaming	0.005025
parses a line in libsvm format into	parse libsvm line line	0.111111
default min	spark context default min	1.000000
the rdd's	core rdd	0.003460
stopwords=none casesensitive=false) sets	ml stop words remover set	0.333333
and count of	rdd	0.003058
position of the first occurrence of substr	substr	0.071429
onevsrestmodel create and return	one vs rest model	0.058824
the coefficient of determination	mllib regression metrics r2	0.166667
for spill by	core	0.003021
in :py attr predictions	generalized linear	0.200000
or newline-delimited json <http //jsonlines	writer json path mode compression dateformat	0.166667
other from this block matrix	mllib linalg block matrix	0.052632
so that :func awaitanytermination()	sql streaming query manager reset	0.011905
distinct count of	count distinct col rsd	0.333333
a streaming dataframe/dataset is written to a streaming	stream writer output mode outputmode	0.083333
given parameters in this grid to	grid builder base	0.076923
sets the given parameters in this grid	param grid builder base on	0.076923
in matching	matching	0.111111
the libsvm format into an rdd of labeledpoint	mllib mlutils load lib svmfile sc path	0.125000
dot product of	mllib linalg dense vector dot	0.058824
params	java params	0.400000
removes the specified table from the	sql sqlcontext uncache table tablename	0.250000
java function	java func sc func	1.000000
compute the	mllib linalg distributed	0.333333
the :class dataframe to a data	sql data frame	0.005348
strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none)	strategy missingvalue inputcols outputcols	0.333333
:py attr predictions which gives the	linear regression summary	0.013889
converts matrix columns in	convert matrix columns from ml dataset	0.166667
to by codeblock co	cls co	0.333333
set named options filter out those	sql option utils set opts schema	0.333333
all	external merger	0.031250
weights	model weights	0.166667
fits a model to the input dataset	ml estimator fit dataset	1.000000
the minutes of	sql minute	0.050000
an rdd comprised of vectors containing i	random rdds gamma vector rdd	0.166667
setparams(self inverse=false inputcol=none outputcol=none) sets params for this	set params inverse inputcol outputcol	0.333333
correlation of	method	0.041667
converts vector columns in an input dataframe to	convert vector columns	0.083333
create an input stream that	utils create stream	0.200000
returning the	reader	0.040000
partitioned data into	core external group	0.045455
centroids of	timeunit	0.025641
a list of predicted ratings for input	matrix factorization model predict all user_product	0.050000
set the trigger for the stream query if	sql data stream writer trigger	0.083333
vector to the new mllib-local representation	vector as	0.500000
sqltransformer	ml sqltransformer	0.250000
list of indices	ml	0.001835
the new	save as new	0.125000
in the key-value	map values	0.166667
much of memory for	external merger object	0.032258
associative and commutative reduce	core rdd reduce	0.083333
pipelinemodel used for ml	ml pipeline model	0.066667
:py attr metricname	metric name value	1.000000
sparkcontext which	context spark	0.083333
whose columns are the left singular vectors of	mllib linalg singular	0.017544
be used	sql streaming query manager reset	0.011905
a numpy ndarray	ml linalg matrix to array	0.166667
driver as	block matrix to local	0.500000
can be used to truncate	frame	0.034483
adds a term	accumulator add term	0.066667
set bandwidth of each sample defaults to 1	kernel density set bandwidth bandwidth	0.142857
create a new dstream in which	streaming streaming context	0.032258
summary e g residuals deviance pvalues of model	ml generalized linear regression model summary	1.000000
awaitanytermination() can be used again to wait for	reset	0.011236
local property set in this thread	core spark context get local property key	0.066667
for binary or multiclass classification	cls data numclasses categoricalfeaturesinfo	0.250000
right singular vectors of the singularvaluedecomposition	linalg singular value decomposition v	0.250000
nodes summed	nodes	0.074074
converts vector columns in an input dataframe to	convert vector columns from ml dataset	0.166667
feature selection by number of	chi sq selector set num	0.250000
featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 tol=1e-6 seed=none	featurescol labelcol predictioncol maxiter	0.333333
until any of the queries on the	streaming query manager await any	0.142857
specification that defines the partitioning ordering and frame	spec	0.076923
class inherit documentation from its parents	mllib inherit doc	0.045455
model fitted by naivebayes	naive bayes model	0.500000
return an rdd created by piping	core rdd	0.003460
an input stream that is to be	stream	0.017544
node depth 1	model depth	1.000000
how much of	core external	0.016129
of indices	ml chi sq selector	0.100000
the dot product of two vectors	ml linalg dense vector dot	0.090909
load a model	loader load cls sc	0.250000
copy all params defined on	param params copy params	0.200000
boundaries in increasing order	boundaries	0.142857
old hadoop	hadoop	0.050000
compute the number of rows	block matrix num rows	0.200000
each point in rdd 'x' to all mixture	mixture model	0.066667
of the points belongs to	predict x	0.033898
users for a given product and returns a	users product	0.142857
densematrix	linalg block matrix to	1.000000
trees	trees	0.533333
the context	context	0.022727
items for columns possibly with false positives using	items cols	0.125000
sub-matrix blocks blockrowindex blockcolindex sub-matrix) that form	mllib linalg block matrix blocks	0.166667
objective function scaled loss + regularization at	logistic regression training summary objective history	0.500000
param with a given string	params	0.006623
elements in one	core rdd	0.003460
this standardscaler	standard scaler	0.076923
the number	mllib linalg indexed row matrix num	0.062500
stream query if this is not set it	stream writer	0.041667
of this instance with a randomly generated uid	ml one	0.083333
specifies the input	frame reader	0.200000
return a copy of the rdd partitioned	core rdd partition	0.333333
the values for each key in	by key numpartitions	0.111111
left outer join of c{self}	rdd left outer join	0.111111
loads a csv file stream and	stream reader csv path schema sep	0.500000
returns all	sql	0.002525
a local property set in	core spark context get local property key	0.066667
this udt	user defined	0.333333
value of	ml param has reg	1.000000
the norm of	sparse vector norm p	0.066667
again to wait for new terminations	manager	0.011236
a per stage basis	profiler collector	0.142857
compute the sum for each numeric	sql grouped data sum	0.083333
awaitanytermination() can be used again to wait	streaming query manager	0.011236
extracts the embedded default param values and user-supplied	params extract param	0.333333
the uid of this instance this updates both	ml param params reset uid	0.058824
items for columns possibly	items	0.066667
spark configuration	core spark conf init loaddefaults _jvm _jconf	0.250000
a field in	field	0.222222
the accumulator's value only	core accumulator value	0.045455
__init__(self	tokenizer init	1.000000
python rdd of	core rdd save	0.037975
on a model with weights already	mllib streaming linear regression with	0.111111
this model to	model	0.005587
to_profile passed in a profile object is returned	basic profiler profile	0.200000
of words closest in similarity to	ml word2vec	0.142857
deviance for the fitted model	generalized linear regression summary deviance	0.125000
of blocks in	col blocks	0.250000
dataframe representing the result of	sqlquery	0.054054
each original column	original	0.047619
in this	streaming streaming	0.047619
limits the result count to	limit	0.076923
for the stream query if	stream	0.017544
parameters in this grid to fixed values	param grid builder base	0.076923
shortcut of write() save path	ml pipeline model save path	0.200000
memory for this obj assume	size obj	0.040000
:class dataframe, using the	sql data frame	0.005348
which is a dataframe having two	logistic regression summary	0.045455
as a temporary table in the	as table df tablename	0.250000
list of numpy	ml bisecting kmeans	0.062500
number of columns that make up each	matrix cols per	0.333333
smirnov	smirnov	1.000000
arbitrary key and	core	0.006042
outer join of	outer join	0.250000
saved using rdd	path minpartitions	0.250000
svc	svc	0.714286
or compute	linalg coordinate matrix	0.250000
contains a param with a given string	has param	0.019231
this thread until the group	group	0.025641
set bandwidth of each sample	stat kernel density set bandwidth bandwidth	0.142857
the mean variance and	core rdd	0.003460
test that	test training and prediction	0.500000
returns accuracy equals to the total number	metrics accuracy	0.166667
add a py or zip	add py file	0.166667
versionadded : 0 9 0	ridge regression with sgd	0.500000
broadcast a read-only	broadcast	0.052632
again to	reset	0.011236
:class pandas dataframe	data frame data	0.142857
param with a given string name	ml param params	0.013699
receiver operating characteristic roc curve which is	binary logistic regression summary roc	0.166667
can be used again	streaming	0.005025
dump already partitioned data into disks	group by	0.041667
setparams(self	scaler set params	1.000000
next memory limit if	external sorter next limit	0.200000
values for each key	by key func numpartitions partitionfunc	0.066667
an external table based on	external table tablename path	0.090909
that all the objects	external merger	0.031250
a line in libsvm format into label indices	mllib mlutils parse libsvm line line multiclass	0.111111
state	state	1.000000
associative and commutative reduce function but return	core rdd reduce	0.083333
accum_param	accum_param	1.000000
rawpredictioncol raw prediction a k a	raw prediction	0.200000
sub-matrix blocks blockrowindex blockcolindex sub-matrix) that	mllib linalg block matrix blocks	0.166667
seed=none)	seed	0.111111
partitioned data	core	0.003021
converts vector columns in an input	mlutils convert vector columns to ml	0.166667
maxiter	maxiter	0.416667
for each key in the	by key numpartitions	0.111111
java related object	init host port	0.200000
of the list of	sql	0.005051
the kmeans algorithm	kmeans	0.025641
setparams(self scalingvec=none inputcol=none outputcol=none) sets params for this	set params scalingvec inputcol outputcol	0.333333
basic operation test for dstream mapvalues	basic operation tests test map	0.333333
inputformat with arbitrary key and value class from	inputformatclass keyclass	0.125000
as a :class dataframe	sql data frame	0.010695
number of partitions to use during reduce tasks	reduce partitions	0.166667
a java udf so it can be used	java	0.012195
a unified dstream from multiple dstreams of	streaming streaming context union	0.111111
merge the values for each key	by key func	0.062500
each rdd	by	0.014286
month	dayofmonth col	0.031250
the day of the month of a given	dayofmonth	0.027027
perform a right outer join of	rdd full outer join other	0.111111
copy of the rdd partitioned using the	rdd partition by	0.062500
of products for all	products for	0.500000
with the spark sink deployed	maxbatchsize	0.037037
an rdd ordered	core rdd take ordered	0.050000
to be used with the spark sink	ssc addresses storagelevel maxbatchsize	0.045455
number of training iterations until termination	ml logistic regression training summary total iterations	0.500000
partial objects do not	core cloud pickler save partial	0.125000
converts vector columns	mllib mlutils convert vector columns from	0.166667
the ids of	stage ids	0.055556
value of	ml quantile	0.750000
sparkcontext which is associated with this streamingcontext	streaming streaming context spark context	0.500000
even if users construct taskcontext instead	task context	0.142857
data into	spill	0.038462
return a default otherwise	core spark	0.010309
of active queries associated with this	streaming query manager active	0.066667
passed as a list	core spark conf set	0.100000
key value pairs	key	0.017857
inputcol=none outputcol=none	inputcol outputcol	0.181818
comprised of vectors containing i i d samples	mllib random rdds log normal vector	0.125000
fitting and predicting	streaming	0.005025
how much	core	0.003021
for	linear model	0.066667
class	java	0.012195
:func awaitanytermination() can be used again to	streaming	0.005025
the returned	spark streaming test case	0.333333
stream api with start	stream from	0.250000
get the cluster centers represented as a	cluster centers	0.090909
dataframe with two fields threshold precision curve	binary logistic regression summary precision by threshold	0.166667
for comparing instances	ldatest compare m1 m2	0.250000
basic operation test for dstream mappartitions	basic operation tests test map partitions	1.000000
the deviance for the fitted	linear regression summary deviance	0.125000
in the training set given the	ldamodel training	0.034483
:py attr numtrees	num trees value	1.000000
sparkcontext which is associated with this	spark	0.013158
can be used	query manager reset	0.011905
get depth of tree (e	tree	0.020833
squared error which is defined as	linear regression summary	0.013889
registers this rdd as a temporary table using	data frame register temp table	1.000000
rdd by applying a function	map f	0.037037
set the selector type	set selector type	0.111111
precision-recall curve which is a dataframe	ml binary logistic regression summary pr	0.083333
next memory limit if the memory is	next limit	0.200000
the rdd as non-persistent and	rdd unpersist	0.066667
inverse=false inputcol=none outputcol=none)	inverse inputcol outputcol	1.000000
a class inherit documentation	inherit	0.037037
update	streaming kmeans model update	0.500000
accumulator's value	accumulator	0.025974
the given table/view in the specified	tablename	0.043478
instance contains a param with a	ml param params has param	0.019231
random	random	1.000000
squared error which is	linear regression summary	0.013889
the cluster centers represented as a	cluster centers	0.090909
tree pattern	tree	0.041667
so that :func awaitanytermination() can be used	query manager	0.011905
test that	with sgdtests test	0.111111
to receive accumulator updates in a	update	0.055556
persist its values across operations after	persist storagelevel	0.166667
this distributed model	distributed ldamodel	0.052632
extra	extra	0.166667
all globals names read or written to by	code globals	0.125000
queries so that	streaming query manager reset	0.011905
compute the number of	mllib linalg block matrix num	0.125000
input	input	0.545455
distributed matrix on the driver as	matrix to local matrix	0.250000
sets	num features set	1.000000
that all	merger	0.025641
user-facing configuration api accessible through sparksession conf	runtime config	0.333333
a python parammap into	param map to	0.125000
wait for the execution to	streaming streaming context await termination timeout	0.166667
parammap into	param map to	0.125000
of memory for	external merger object size	0.032258
in libsvm format into label	mlutils parse libsvm	0.125000
memory for this obj assume that all	obj	0.023810
__init__(self inputcol=none outputcol=none	lsh init inputcol outputcol	1.000000
accumulator's data	accumulator param	0.038462
applies standardization transformation on	scaler model transform	0.500000
internal use only create a new hivecontext for	context create for	0.250000
compute the dot product of two vectors	dense vector dot	0.050000
returns the mean average precision map of	mean average precision	0.166667
that	query	0.021505
product	product	0.235294
a param	params has param	0.019231
the selector	chi sq selector set selector	0.333333
the right singular	linalg singular	0.017544
the given path the model	path	0.010204
most recent [[streamingqueryprogress]] updates for this query	streaming query recent progress	0.500000
how much of memory	external	0.013889
a new spark configuration	core spark conf	0.055556
get all values as a	get all	0.166667
underlying sql storage	type sql	0.250000
sets params for this imputer	ml imputer set params	1.000000
a line in libsvm format into label	mlutils parse libsvm line line multiclass	0.111111
the dot product of two vectors we	mllib linalg dense vector dot	0.058824
elements from an rdd ordered	core rdd take ordered	0.050000
as spark executor memory this	core spark	0.010309
class to track supported impurity measures	tree regressor params	0.250000
content of the dataframe	sql data frame writer	0.011628
create a python topicandpartition	init topic partition	0.055556
curve which is	ml binary logistic regression summary	0.125000
for feature selection by number	chi sq selector set num	0.250000
as	by key	0.026316
underlying output data source	writer format source	0.333333
ensuring all received data has been	stopsparkcontext stopgracefully	0.050000
train a random forest model for binary	mllib random forest train classifier	0.250000
maxdepth	maxdepth	0.500000
fits a java model to the input dataset	ml java estimator fit java dataset	1.000000
active queries associated with this sqlcontext >>> sq	query manager active	0.066667
list of names of	names	0.050000
until any	manager await any termination	0.142857
of the :class dataframe in json format (json	sql data frame	0.005348
test the python direct kafka stream api	kafka stream tests test kafka direct stream from	0.333333
for each original	min max scaler model original	0.062500
function without changing the keys this	values f	0.062500
invalidates and refreshes all	catalog refresh by	0.200000
add a py or zip dependency for all	context add py	0.166667
with	param params	0.014925
that :func awaitanytermination()	streaming query manager	0.011236
over	over	1.000000
day in utc	from utc	0.125000
a param with a	params has	0.019231
to a java onevsrestmodel used for ml persistence	one vs rest model to java	1.000000
an external database table	table mode properties	0.200000
tests whether this instance contains a	params has param paramname	0.142857
column or names into a jvm seq of	seq	0.043478
this matrix to a rowmatrix	linalg indexed row matrix to row matrix	0.333333
a resulting rdd that contains a	rdd cogroup other numpartitions	0.066667
load a :class dataframe	data frame	0.005000
to this accumulator's value	core accumulator add	0.076923
mean squared error which is defined	linear regression summary	0.013889
be used again to wait for	query manager reset	0.011905
the contents of the :class dataframe	frame writer save path	0.066667
residual degrees of freedom for the null model	regression summary residual degree of freedom null	0.333333
accumulator's value only usable in driver	accumulator value	0.050000
computes the	compute	0.181818
until any of the queries on the	manager await any	0.142857
data points in each cluster	clustering summary cluster sizes	1.000000
returns an array containing the ids	stage ids	0.055556
get or compute the number of rows	linalg indexed row matrix num rows	0.200000
bisectingkmeans	bisecting kmeans	0.083333
params for	params featurescol labelcol predictioncol	0.500000
saves the contents	format mode partitionby	0.100000
map	map sc	1.000000
a py or zip dependency for all	py file path	0.066667
idx	idx	1.000000
wait for new terminations	sql streaming query	0.011765
file to which this rdd	file	0.028571
into	by spill	0.047619
infer schema from an rdd of row	sqlcontext infer schema rdd samplingratio	0.250000
if the stage info	stage info	0.142857
rdd 'x' has maximum membership in this model	gaussian mixture model predict x	0.500000
dataframe from an :class rdd, a list	schema samplingratio verifyschema	0.029412
load a	load cls	1.000000
runs and profiles	core basic	0.066667
given value to scale decimal places using half_even	sql	0.002525
of a batch of jobs has started	batch started batchstarted	0.333333
main	main	1.000000
counts	grouped data count	1.000000
for each key using	by key func numpartitions partitionfunc	0.066667
:func awaitanytermination() can be	sql streaming	0.010204
setparams(self featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01 maxiter=100	set params featurescol predictioncol k probabilitycol	0.500000
returns a list of predicted ratings for input	matrix factorization model predict all user_product	0.050000
restore an object	restore name fields	0.333333
__init__(self	evaluator init	1.000000
evaluates the output	ml java evaluator evaluate dataset	0.333333
of the file	file	0.028571
singular values in descending order	singular value decomposition s	0.250000
tests whether this instance contains a param	paramname	0.076923
comprised of vectors containing	random rdds log	0.125000
minutes	sql minute	0.050000
with a given string	param	0.012500
instance contains a param with	params has	0.019231
provide a new name	name	0.043478
a java parammap into a python parammap	param map from java	0.500000
string	params has	0.019231
cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)	ml random forest classifier	0.023256
context to use for loading	ml mlreader context	1.000000
tobase	tobase	1.000000
computes column-wise summary statistics	statistics col	0.200000
memoryview	memoryview	1.000000
paired rdd where the first	factorization	0.038462
existing rdd	samplingratio	0.100000
code for a	code name	0.111111
cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	ml decision tree regressor	0.066667
day in utc returns	sql from utc	0.142857
recommends the	mllib matrix factorization model recommend	0.250000
get the cluster	kmeans model cluster	0.333333
of linear svm classifier	ml linear svcmodel	1.000000
a new java	wrapper new java	0.166667
new accumulator with a	accumulator init aid	0.083333
__init__(self inputcol=none outputcol=none	index to string init inputcol outputcol	1.000000
a multi-dimensional rollup for the current	frame rollup	0.055556
of	ml linear regression	0.500000
partitions to use during reduce tasks (e	default reduce partitions	0.166667
model	ensemble model	0.058824
of the date	day date	0.100000
the termination of this query either by	termination	0.035714
mlreader for :py class javaparams types	mlreader	0.037037
for	streaming query	0.010526
matrix to an indexedrowmatrix	linalg block matrix to indexed row matrix	0.333333
or names into a jvm seq of	seq sc cols converter	0.055556
minutes of a given date as integer	sql minute	0.050000
a py or	py file	0.066667
value of	ml param has output	1.000000
none by default	compressioncodecclass	0.111111
specifies the underlying output	frame writer	0.050000
a param with a	param params	0.014925
input dataset with optional parameters	dataset params	0.333333
compare	compare	0.500000
get the cluster centers represented as a	mllib kmeans model cluster centers	0.083333
squared distance from a sparsevector or 1-dimensional numpy	mllib linalg sparse vector squared distance other	0.166667
dense vector of 64-bit floats from a	ml linalg vectors dense	0.166667
generates python code for	code	0.071429
train a gradient-boosted trees model	mllib gradient boosted trees train regressor	0.333333
converts matrix columns in an	mllib mlutils convert matrix columns from	0.166667
a sparse vector using either a dictionary a	vectors sparse size	0.166667
test python direct kafka rdd get	streaming kafka stream tests test kafka rdd get	1.000000
on a	regression with	0.200000
ids	ids	1.000000
this accumulator's value	accumulator add	0.076923
average values for each numeric columns	grouped data avg	0.058824
the value of the date column	day date	0.100000
converts vector columns	convert vector columns to	0.166667
labels corresponding to	string indexer model labels	0.166667
levenshtein distance of	levenshtein left right	0.058824
this matrix	mllib linalg dense matrix	0.083333
coefs are predicted accurately	parameter accuracy	0.029412
converts matrix columns in an input	mlutils convert matrix columns from ml	0.166667
is set	core spark context set	0.166667
of a batch has completed	completed	0.058824
class to track supported impurity measures	tree classifier params	0.250000
the root directory that contains	root directory	0.333333
external sort when the memory	external	0.013889
for converting raw prediction scores into 0/1 predictions	mllib linear classification model	0.142857
setparams(self inputcol=none outputcol=none labels=none) sets params	index to string set params inputcol outputcol labels	0.333333
pos in byte and is of	pos	0.022222
default or minimized false	evaluator is larger better	0.166667
lower bound on the log	ldamodel log	0.125000
train the model on the incoming dstream	logistic regression with sgd train on dstream	1.000000
into label indices values	parse	0.071429
that all the objects	size	0.009174
of layers	ml multilayer perceptron classification	0.333333
in "predictions" which gives the features of each	ml linear regression summary features	0.166667
the explained variance regression score	ml linear regression summary explained variance	0.333333
how much of memory	object size	0.032258
returns weighted averaged	multiclass metrics weighted	0.333333
average values for each numeric columns	sql grouped	0.043478
with	with	0.444444
the weights	model weights	0.166667
a model from the input	estimator	0.083333
active queries associated with this sqlcontext	streaming query manager active	0.066667
given key	key	0.017857
predict values for a single	predict	0.068966
partial objects do not serialize correctly in python2	core cloud pickler save partial obj	0.125000
stratified sample	data frame sample	0.066667
return an rdd created by piping	rdd	0.003058
create an input stream	create stream ssc hostname port storagelevel	0.200000
for	core external merger object	0.032258
column for distinct count of col or cols	count distinct	0.040000
java	from java	0.444444
with the dispatch to handle all function	cloud pickler save function	0.142857
java	new java	0.166667
dm = densematrix(2 2 range 4	dense matrix	0.076923
called by a	core	0.003021
test the python direct kafka rdd api with	kafka stream tests test kafka rdd with	1.000000
in this grid	grid builder add grid	0.100000
python code for	code name doc defaultvaluestr	0.111111
__init__(self	count vectorizer init	1.000000
called when processing of a job of	streaming listener on output operation	0.166667
get a local property set in this	spark context get local property key	0.066667
mode	mode	1.000000
using the model trained	mllib svmmodel	1.000000
__init__(self	aftsurvival regression init	1.000000
returns a :class dataframe	sql spark session sql	0.250000
a new column of corresponding string	string	0.041667
dstream by applying reducebykey to	streaming dstream reduce by key	0.076923
model	mllib streaming linear regression	0.333333
returns a :class dataframe representing the	sqlcontext sql sqlquery	0.250000
an input stream that pulls	stream ssc hostname port storagelevel	0.200000
for new terminations	query manager	0.011905
memory for this obj assume that	external merger object size obj	0.040000
of a file added through	core spark files get	0.125000
generates an rdd comprised of vectors	random rdds uniform vector rdd sc	0.200000
list of functions registered	list functions	0.250000
so that :func awaitanytermination() can be	reset	0.011236
this blockmatrix by other, another blockmatrix	linalg block matrix multiply other	0.200000
standardscaler	standard	0.071429
create a new profiler using class profiler_cls	core profiler collector new profiler ctx	0.333333
given user and	user	0.055556
as the specified table	save as table name	1.000000
an array	mllib matrix	0.047619
an rdd comprised of vectors containing i i	random rdds normal vector rdd	0.166667
to make predictions on batches of data from	algorithm predict on	0.066667
specifies how data	data stream	0.028571
soundex	soundex	0.217391
the catalog	catalog	0.187500
accumulator's value only usable	accumulator	0.012987
test for data sampled from a continuous distribution	test data distname	0.166667
number of	ml logistic regression	0.111111
this pca	ml pca	0.333333
or transform the rdd of	hashing tf transform	0.045455
numrows	numrows	1.000000
wait for the execution to stop return	context await termination or timeout timeout	0.125000
and predicting on incoming	streaming	0.005025
returns a paired rdd where the first element	factorization	0.038462
the distinct elements in this rdd	core rdd distinct	0.250000
a left outer join of	rdd left outer join	0.111111
a list of	core spark	0.020619
storage type	type	0.024390
key and value class from	core	0.006042
parameters passed as a list	core spark conf set	0.100000
cost sum of squared distances of points	compute cost	0.142857
converts matrix columns in an input dataframe	mllib mlutils convert matrix columns from	0.166667
that stores item	alsmodel item	0.250000
an exception if	mllib	0.010526
a local property that affects jobs submitted from	local property key	0.035714
brackets pairs e	brackets	0.058824
list of	ml string	0.333333
the levenshtein distance	sql levenshtein left	0.058824
in rdd 'x' has maximum membership	mllib gaussian mixture	0.045455
a labeledpoint	labeled point	0.500000
can be	sql streaming query manager	0.011905
case	case	1.000000
model for prediction tasks regression and	prediction model	0.333333
as non-persistent and remove	unpersist	0.083333
__init__(self formula=none featurescol="features", labelcol="label",	init formula featurescol labelcol	1.000000
corresponding to the expected value of	ml linear	0.066667
setparams(self featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01 maxiter=100	mixture set params featurescol predictioncol k probabilitycol	0.500000
ml params instances	params m1	0.047619
multi-dimensional rollup for the current :class dataframe using	sql data frame rollup	0.055556
unicode as utf-8	streaming utf8	0.500000
contains a param	param params has param	0.019231
local temporary view with this	temp view name	0.222222
__init__(self	max abs scaler init	1.000000
columns are the right singular vectors of	mllib linalg singular	0.017544
'with sparkcontext as	core spark context	0.011628
new :class dataframe that drops the	data frame drop	0.250000
tests whether this	param paramname	0.111111
numeric data types	numeric type	1.000000
"predictions" which gives the true label of	ml linear regression summary label col	0.333333
the vector representation of each word in vocabulary	word2vec fit	0.200000
add two values of the accumulator's data	accumulator param add	0.333333
base-2 logarithm of the argument	log2 col	0.250000
column from one base to another	conv col frombase tobase	0.166667
this instance contains a param with	param params has	0.019231
summed over all trees in the ensemble	mllib tree ensemble	0.111111
contains a param with a given string	param	0.012500
points to their nearest center for this model	model compute	0.133333
test that coefs are predicted accurately by fitting	streaming linear regression with tests test parameter accuracy	0.333333
return a resulting rdd that contains a tuple	rdd cogroup other	0.066667
of this instance with a	ml train	0.181818
otherwise	otherwise	1.000000
the length of	length col	0.050000
that :func awaitanytermination() can be used	reset	0.011236
infer schema from an rdd	infer schema rdd	0.250000
the underlying output	data stream writer format	0.333333
queries so that :func awaitanytermination()	reset	0.011236
:class windowspec with the frame boundaries defined	sql window range between	0.166667
wait for the execution to	await termination timeout	0.166667
minimum number of times a token	min count	0.076923
checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto",	random forest classifier	0.022727
deviance for the null model	summary null deviance	0.250000
setparams(self	validation split set params	1.000000
either by :func	streaming	0.005025
an input from tcp source hostname port data	context socket text stream hostname port	1.000000
table via jdbc	jdbc url table mode properties	0.200000
0 1 0] for feature selection by fwe	chi sq selector set fwe fwe	0.200000
associative and commutative reduce	rdd reduce	0.071429
of each cluster for each training data point	ml gaussian mixture summary	1.000000
observed data against	observed	0.058824
until the group id	group groupid	0.142857
rdd is	rdd	0.003058
a py or zip dependency for all	py file	0.066667
pearson's chi-squared goodness of	mllib stat statistics chi sq	0.066667
a new profiler using class profiler_cls	core profiler collector new profiler ctx	0.333333
version of a heappush followed by	heap	0.047619
of the :class dataframe as the specified table	sql data frame writer save as table name	0.333333
data source path	path path	0.333333
load a java model from	load java cls sc	0.200000
transformer	transformer	0.833333
predictioncol	predictioncol	0.178571
l{statcounter} object that captures the mean variance	stats	0.055556
__init__(self labelcol="label", featurescol="features",	linear regression init labelcol featurescol	1.000000
seq of columns that describes the sort	sql data frame sort cols cols kwargs	0.142857
class inherit documentation	mllib inherit doc cls	0.045455
to get	get	0.021739
evaluates the model on a test dataset	generalized linear regression model evaluate dataset	1.000000
which	regression summary	0.107143
test the python direct kafka stream	tests test kafka direct stream	0.625000
of the :class dataframe	data frame writer	0.070423
an input stream	stream ssc addresses storagelevel	0.166667
returns a new	ml java wrapper new	0.250000
observed data against the	observed	0.058824
set application	core spark conf set app	1.000000
problem in multinomial logistic	mllib logistic	0.200000
a param with a	ml param params has param	0.019231
until any of the queries	query manager await any	0.142857
stratified sample without replacement based on the fraction	frame sample	0.066667
list for list	of list	0.333333
adds an input	stream reader	0.076923
fitting	streaming	0.005025
compute the dot product of two vectors we	dot	0.040000
the vector representation of each word in vocabulary	word2vec fit data	0.200000
return an rdd containing all pairs	core rdd	0.003460
an rdd comprised of	random rdds gamma vector rdd	0.166667
setparams(self inputcol=none outputcol=none handleinvalid="error") sets params for this	set params inputcol outputcol handleinvalid	0.333333
computes hex value	hex col	0.166667
of the mean squared error	mean squared error	0.250000
field in "predictions" which gives the true label	linear regression summary label	0.333333
infer schema from an rdd of row	sqlcontext infer schema rdd	0.250000
columns of a dataframe	data frame	0.005000
mixture components	gaussian mixture model predict soft	0.142857
of this instance with a randomly generated uid	ml one vs rest model	0.111111
java pipelinemodel create and return a python wrapper	pipeline model from java	0.142857
registers a python function including lambda function as	sql catalog register function	1.000000
awaitanytermination() can be used again to wait for	sql streaming	0.010204
a :class dataframe representing	sqlquery	0.027027
whose columns are the right singular vectors of	linalg singular	0.017544
an rdd comprised of vectors containing i	mllib random rdds gamma vector rdd	0.166667
arrays of	ml linalg	0.030303
local property set in this thread	context get local property key	0.066667
a column scipy matrix from a dictionary	mllib sci py tests scipy matrix size	0.090909
with another :class dataframe, using the given join	data frame join other	1.000000
function to the value of each	map values f	0.125000
lines text format or newline-delimited json	writer json path mode	0.125000
into an rdd of labeledpoint	load lib svmfile	0.125000
seed=none impurity="variance")	gbtregressor	0.125000
evaluates the output	java evaluator evaluate dataset	0.333333
least value of	least	0.043478
bisecting k-means algorithm based on the paper	bisecting kmeans	0.166667
the old hadoop outputformat api	hadoop dataset conf keyconverter valueconverter	0.083333
wait until any of the queries	sql streaming query manager await any termination timeout	0.166667
value of each	ml	0.001835
line in libsvm format into label	mlutils parse libsvm line line	0.111111
a param with a	ml param	0.009524
the precision-recall curve which is	ml binary logistic regression summary pr	0.083333
prefix of	prefix f	0.142857
test that the final value of weights	regression with sgdtests test	0.111111
version of a	heap	0.047619
set initial centers should be set before	mllib streaming kmeans set initial centers centers	0.200000
t	t	1.000000
which is a	logistic regression summary	0.045455
cluster centers represented as	mllib kmeans model cluster centers	0.083333
broadcast a read-only variable	core spark context broadcast	0.125000
mixin for param standardization whether to standardize the	has standardization	0.250000
lda	ml distributed ldamodel get	0.066667
the model	regression model	0.031250
an rdd comprised of vectors containing	mllib random rdds poisson vector rdd	0.166667
parameters in this grid to	param grid builder add grid	0.100000
of memory for this obj assume that	external merger object size obj	0.040000
vector	sparse vector	0.062500
session to use for	session	0.050000
of a list or gets an	sql column get	0.142857
kwargs	kwargs	1.000000
params for	params featurescol labelcol predictioncol classifier	0.500000
under the receiver operating characteristic roc	under roc	1.000000
an external table based	external table tablename path	0.090909
the dot product of two	ml linalg dense vector dot other	0.090909
and commutative reduce	core rdd reduce by	0.125000
the stream query if this is not	data stream writer	0.041667
value of	ml param	0.266667
of the binary value of the given column	bin col	0.333333
used again to wait for new terminations	sql streaming query	0.011765
for each original column during fitting	max scaler model original	0.062500
a list of numpy	ml bisecting kmeans	0.062500
persist its values across operations after	rdd persist storagelevel	0.166667
rdd by applying a function to each partition	f	0.010526
:class statcounter members	stat counter	0.333333
__init__(self inputcol=none outputcol=none	ml min hash lsh init inputcol outputcol	1.000000
start offset specified	offset	0.021739
computes column-wise summary statistics for	mllib stat statistics col stats rdd	0.200000
named table accessible via jdbc url url and	jdbc url table	0.090909
find synonyms of a word	word2vec model find synonyms word num	1.000000
in c{self} that is not contained in	core rdd subtract other numpartitions	0.111111
that separates positive predictions from negative predictions	linear classification model	0.076923
of a streaming dataframe/dataset is written to a	stream writer output mode outputmode	0.083333
given a java pipeline	pipeline from java cls	0.200000
set the initial value of weights	logistic regression with sgd set initial weights initialweights	0.333333
the uid	reset uid	0.333333
vector columns in an	vector columns from ml dataset	0.142857
saves the content of	name format mode partitionby	0.200000
converts matrix columns in an input dataframe	mllib mlutils convert matrix columns from ml	0.166667
parses the expression string into the column that	sql expr	0.125000
stopwords=none casesensitive=false) sets	remover set	0.200000
broadcast a read-only variable to the cluster	core spark context broadcast	0.125000
that pulls events from flume	flume	0.071429
contains a param with a	has param	0.019231
schema in the	schema	0.033333
jvm scala map from a dict	sql data frame jmap jm	0.111111
as the spark	core spark	0.010309
how much of memory	core external merger	0.032258
stream query if this is	sql data stream writer	0.041667
parallel prefixspan algorithm to mine frequent sequential patterns	prefix span	0.166667
python collection to form an rdd using xrange	core spark context parallelize c numslices	0.250000
the given parameters in this grid to	param grid builder base	0.076923
an input stream that pulls events	stream ssc	0.090909
dummy params instance used as a placeholder to	ml param params dummy	0.111111
accumulator's value only	accumulator value	0.050000
for statistic functions with	stat functions	0.333333
with a randomly	split model	0.200000
compute the dot product of two vectors	linalg dense vector dot	0.058824
values and then merges them with extra	extra	0.023810
of columns of blocks in	col blocks	0.250000
original	original	0.285714
sets the accumulator's	core accumulator value	0.045455
"zerovalue" which	fold by	0.125000
type of the	type	0.024390
of predicted ratings for input	matrix factorization model predict all user_product	0.050000
to get or create global	get or create	0.111111
local property set in this thread or null	local property key	0.035714
:class dataframe to a data	data frame writer save path	0.142857
the null model	regression summary null	0.250000
of	ml train	0.181818
embedded params	params	0.006623
ensuring all received data has	stopsparkcontext stopgracefully	0.050000
get total number of nodes	total num nodes	0.250000
left multiplies this blockmatrix by other,	multiply	0.100000
model to make predictions on batches of	streaming linear algorithm predict on	0.066667
estimate of the importance of each feature	ml gbtregression model feature importances	0.250000
awaitanytermination() can be used	streaming query	0.010526
this obj assume that	external merger object size obj	0.040000
given parameters in this grid	ml param grid builder base on	0.076923
much	core external merger object size	0.032258
create a	submit tests create	0.500000
i d samples drawn	std numrows	0.125000
monotonically	monotonically	1.000000
specifies how data of a	sql data	0.024390
:func awaitanytermination()	streaming query	0.010526
how much	object size	0.032258
awaitanytermination() can be used again to wait	sql streaming query manager reset	0.011905
function scaled loss + regularization at	history	0.181818
the sql context to	context sqlcontext	0.083333
zip()	pair	1.000000
given parameters in this grid to fixed values	ml param grid builder base on	0.076923
list based	based	0.125000
new rdd by applying a function	map f	0.037037
of the :class dataframe	data frame	0.030000
partial objects do not serialize correctly in	pickler save partial obj	0.125000
of a batch	batch	0.068966
test python direct kafka	streaming kafka stream tests test kafka	1.000000
create a python topicandpartition to map to	streaming topic and partition init topic partition	0.055556
attr lda keeplastcheckpoint is set	distributed ldamodel	0.052632
can be	manager reset	0.011905
test that coefs are predicted accurately by fitting	linear regression with tests test parameter accuracy	0.333333
functions and a neutral	zerovalue	0.076923
the distinct	distinct numpartitions	0.142857
calculates the correlation of two columns of a	method	0.041667
mincount the	mincount	0.142857
which is defined as	regression	0.010000
linear model that	linear model	0.066667
save this model to the given path	mllib naive bayes model save sc path	1.000000
average values for each numeric columns	grouped	0.035714
colptrs	colptrs	1.000000
the buckets	buckets	0.111111
inserts the content of the :class dataframe to	data frame writer insert into	0.500000
dot product of	dense vector dot	0.050000
load a model from the given path	mllib kmeans model load cls sc path	1.000000
the accumulator's data type returning a	accumulator param	0.038462
parameters in this	builder add	0.200000
old hadoop outputformat api	save as hadoop dataset conf keyconverter valueconverter	0.083333
return	core rdd	0.006920
outer join' between rdds of this dstream and	outer join	0.250000
to form an rdd using xrange	core spark context parallelize c numslices	0.250000
java object	java obj java_class	1.000000
used again to wait for	sql	0.002525
values	scaler model	0.076923
cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	regressor	0.043478
create a python topicandpartition to map to the	streaming topic and partition init topic partition	0.055556
date	day date	0.100000
norm of a	norm	0.041667
data into	external	0.013889
rdd of key-value pairs (of form c{rdd[ k	core rdd save	0.037975
adds	add	0.035714
representation	mllib linalg dense	1.000000
defined as the square root of the	root	0.035714
test python direct kafka rdd get offsetranges	tests test kafka rdd get offset ranges	1.000000
mean variance	core rdd	0.003460
densematrix(2 2 range 4	linalg dense matrix repr	0.142857
depth of tree (e g	tree	0.020833
only create a	hive context create	0.083333
new :class column for distinct	distinct col	0.166667
to any hadoop file system using the new	save as new	0.125000
already partitioned data into disks	core	0.003021
items for columns possibly with	items cols support	0.125000
or names into a jvm seq	seq sc cols	0.055556
the cluster centers represented as a list of	cluster centers	0.060606
a file to be	file path	0.035714
for this obj assume that all	core external merger object size obj	0.040000
registers	frame register	1.000000
the spark session to use for	session sparksession	0.083333
column for approximate distinct count of	approx count distinct	0.071429
of write() save path	ml mlwritable save path	0.200000
:class dataframe to	frame	0.034483
original column during	ml min max scaler model original	0.062500
functions and a	core	0.003021
create an input stream that	create stream	0.200000
data type json string	sql parse datatype json string	0.333333
an rdd of labeledpoint	lib svmfile sc path numfeatures minpartitions	0.125000
chi-squared hypothesis test	chi sq test result	1.000000
java onevsrestmodel create and	one vs rest model from java	0.142857
the norm	linalg sparse vector norm p	0.133333
return	mllib standard scaler	0.100000
metricname	metricname	0.833333
chisqselector	selectortype	0.125000
commutative reduce function	rdd reduce by	0.200000
converts vector columns	convert vector columns from ml	0.166667
a python parammap into a java	map to java	0.250000
a	core accumulator	0.030303
memory for this	object	0.027778
pearson's independence test using	ml chi square test test	0.333333
:class dataframe replacing a value with	data frame replace to_replace	0.100000
a resulting rdd that contains	core rdd cogroup other	0.066667
functions with	functions	0.071429
and then merges them with extra	extra	0.023810
a python rdd of	core rdd save	0.037975
be used again to	reset	0.011236
in rdd 'x' has maximum membership	gaussian mixture	0.038462
another dstream	other	0.033333
transfer this instance's params to	ml java params to	0.045455
an rdd comprised of vectors containing i	random rdds normal vector rdd	0.166667
given parameters in this	ml param grid builder add	0.200000
a param with a given string	has	0.011628
to the wrapped java object and return	to	0.007692
mean variance and count of the rdd's elements	core rdd	0.003460
:py attr inputcol	input col value	1.000000
a java parammap into	java	0.012195
set named options filter out those	utils set opts schema	0.333333
i i d samples drawn	numrows numcols numpartitions	0.125000
both :py attr numuserblocks and :py attr numitemblocks	num blocks value	1.000000
log likelihood	ldamodel log likelihood dataset	0.142857
load labeled	mllib mlutils load labeled	1.000000
=	get	0.021739
create a sparse vector using either	vectors sparse	0.166667
this instance with a randomly generated	cross validator	0.045455
compute	compute	0.454545
left outer join of c{self} and	core rdd left outer join other numpartitions	0.200000
an rdd comprised of vectors	mllib random rdds gamma vector rdd	0.166667
precision of all the	metrics precision	0.200000
the number	linalg block matrix num	0.100000
spill by	core	0.003021
loads orc files returning the result	reader orc path	0.200000
py or zip	py file path	0.066667
__init__(self numfeatures=1 << 18 binary=false inputcol=none outputcol=none)	hashing tf init numfeatures binary inputcol outputcol	1.000000
functions registered	functions	0.071429
loading	ml mlreader	0.222222
the distinct elements in this rdd	core rdd distinct numpartitions	0.250000
converts a	sql to	0.041667
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	regressor	0.043478
join	join other numpartitions	0.142857
number of	ml java classification model num	1.000000
predicted ratings for	matrix factorization model predict all user_product	0.050000
wait for new	streaming query manager	0.011236
this obj assume that all	merger object size obj	0.040000
gets a param by its name	params get param paramname	1.000000
function but return	core	0.003021
the	mllib standard scaler	0.100000
utc returns another timestamp that corresponds to	sql from utc timestamp timestamp tz	0.166667
streaming dataframe/dataset is written to a streaming sink	stream writer output mode outputmode	0.083333
converts vector columns in an input dataframe to	mlutils convert vector columns from ml	0.166667
this matrix to an indexedrowmatrix	mllib linalg block matrix to indexed row matrix	0.333333
create a sparse vector using either a	linalg vectors sparse	0.166667
function without changing the keys this	f	0.010526
wait	termination or timeout timeout	0.125000
2 ml params instances for the given	params	0.006623
maps a column of indices back to a	index to	0.040000
initial value of weights	regression with sgd set initial weights	0.333333
a jvm seq	seq sc	0.055556
the underlying output	frame writer format	0.333333
or equal to	numiterations	0.050000
sets	forest params set	1.000000
add a py or zip dependency for	add py file	0.166667
the ensemble	tree ensemble model	0.076923
list of predicted ratings for input user	matrix factorization model predict all user_product	0.050000
"predictions" which gives the features of each	ml linear regression summary features col	0.166667
setparams(self minsupport=0 3 minconfidence=0 8 itemscol="items", predictioncol="prediction", numpartitions=none)	ml fpgrowth set params minsupport minconfidence itemscol predictioncol	1.000000
:class dataframe that has exactly	data frame coalesce	0.500000
an rdd comprised of vectors containing i i	mllib random rdds normal vector rdd	0.166667
this distributed	ml distributed	0.200000
of obtaining a test statistic	stat test	0.166667
slide	slide	1.000000
calculates a lower bound on the log likelihood	log likelihood	0.125000
:py attr impurity	impurity value	1.000000
selector	mllib chi sq selector set selector	0.333333
the content of the :class dataframe in	data frame	0.010000
jvm scala map from	sql data frame jmap jm	0.111111
merge the values for each key using	by key func numpartitions	0.062500
saved using rdd	minpartitions	0.071429
with the dispatch to handle all function	core cloud pickler save function obj	0.142857
can be used in sql statements	f returntype	0.125000
summary e g	summary	0.097561
instance contains a param	ml param params has param	0.019231
test	test predictions	1.000000
stop the	stop	0.105263
returns a :class dataframe representing	session sql sqlquery	0.250000
the norm of a sparsevector	linalg sparse vector norm p	0.066667
of labeledpoint	load lib svmfile sc path numfeatures	0.125000
first occurrence of substr	substr str	0.125000
local property set in	spark context get local property key	0.066667
string column from one base	col frombase tobase	0.166667
the weights for each tree	tree ensemble model tree weights	0.333333
set the selector type of the chisqselector	selector type selectortype	0.333333
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic",	ml gbtclassifier	0.095238
convert a dict into	to	0.007692
is not contained in c{other}	subtract	0.111111
applying a function to each element	f preservespartitioning	0.200000
resulting rdd that contains a	rdd cogroup	0.066667
rdd that is	rdd coalesce	0.500000
for this test	other test	0.333333
a param with a given string	ml param params has	0.019231
deprecated in	sql	0.002525
sub-matrix blocks blockrowindex blockcolindex sub-matrix) that form	linalg block matrix blocks	0.166667
and incrementing as expected	core task context	0.100000
setparams(self k=none inputcol=none outputcol=none) set params for this	set params k inputcol outputcol	0.333333
python rdd of key-value	rdd	0.009174
a list of predicted ratings for input user	mllib matrix factorization model predict all user_product	0.050000
get depth of tree (e g	tree model	0.026316
and a	core	0.006042
of each cluster for	ml	0.001835
returns a :class	session sql	0.250000
uid of this	ml param params reset uid	0.058824
all the objects	merger object size	0.032258
the driver as	local	0.038462
points using the model	regression model	0.031250
the key-value pair rdd through a flatmap	core rdd flat map	0.333333
programming	session	0.050000
old hadoop outputformat api mapred package	save as hadoop dataset conf keyconverter valueconverter	0.083333
singular vectors	singular	0.031250
indices to select filter	sq selector model selected features	0.333333
list of	ml string indexer	0.166667
counts	grouped	0.035714
deserialized batches lists	without unbatching	0.125000
wait for new terminations	query	0.010753
the precision-recall curve which is a dataframe containing	ml binary logistic regression summary pr	0.083333
the dataframe	sql data frame writer	0.011628
of columns that describes the sort	sort cols cols kwargs	0.142857
:class dataframe as the specified table	data frame writer save as table	0.333333
already partitioned data into	group by spill	0.047619
a sparse vector using either a	mllib linalg vectors sparse	0.166667
checks whether a sparkcontext is initialized or not	spark context ensure initialized cls instance gateway conf	0.333333
given parameters in this grid to fixed	grid builder base	0.076923
set the selector type of the chisqselector	sq selector set selector type selectortype	0.333333
accumulator's value only usable in	core accumulator value value	0.050000
of the accumulator's	accumulator param	0.038462
a python topicandpartition to map to the java	topic partition	0.055556
squared distance from a sparsevector or 1-dimensional numpy	vector squared distance other	0.166667
accuracy/precision/recall objective history total iterations) of	ml logistic regression	0.111111
from start	start	0.090909
bound on the log likelihood	ldamodel log likelihood	0.142857
instance for params	params copy	0.083333
number of cols	coordinate matrix num cols	1.000000
sets the sql context to use for loading	ml java mlreader context sqlcontext	0.333333
__init__(self inputcol=none outputcol=none seed=none numhashtables=1)	hash lsh init inputcol outputcol seed numhashtables	1.000000
fractional data types	fractional type	1.000000
current status of the query	streaming query status	1.000000
items for columns possibly with false positives	items cols	0.125000
extract the week number of	weekofyear col	0.055556
awaitanytermination()	streaming query manager reset	0.011905
int containing elements	core spark	0.010309
or any hadoop-supported file system	file path	0.035714
is set to a different	core spark context set	0.166667
the training	ldamodel training	0.034483
__init__(self	ml fpgrowth init	1.000000
perform a right outer join of c{self}	full outer join other numpartitions	0.111111
underlying sql storage type for this udt	sql user defined type sql type	0.500000
creates an external table based on	create external table tablename path	0.250000
the given parameters in	builder	0.090909
conduct pearson's chi-squared goodness of fit test	mllib stat statistics chi sq test	0.166667
comprised of vectors containing i i	mllib random rdds gamma vector	0.125000
dot product of two vectors	vector dot	0.050000
:py attr lda keeplastcheckpoint is	distributed ldamodel get	0.066667
of (i j s\ :sub ij\) tuples representing	k maxiterations	0.333333
the minutes of a	sql minute	0.050000
given path a shortcut of	ml	0.007339
memory for this obj assume that	merger object size obj	0.040000
stratified sample without replacement based on the	data frame sample	0.066667
test that the final value of weights is	mllib streaming logistic regression with sgdtests test	0.111111
return the java object	java	0.012195
awaitanytermination() can be used	sql streaming	0.010204
param with	ml param params has	0.019231
that all the	object	0.027778
ext	ext	1.000000
with pad	pad	0.142857
temporary table	table df	0.083333
withstd	with std	1.000000
python rdd of key-value pairs	rdd save as	0.038462
squared distance from a sparsevector or	ml linalg sparse vector squared distance	0.166667
distinct count of col or	count distinct col	0.040000
minimum item in this rdd	core rdd min key	0.333333
memory	size	0.009174
external list for list	external list of list	0.500000
instance contains	params has	0.019231
that :func awaitanytermination() can	sql streaming query	0.011765
cartesian product with another	cross join other	1.000000
checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto",	random forest classifier	0.022727
how much of memory for	core external merger object	0.032258
wait until any of the queries	manager await any termination timeout	0.166667
given path the model	path	0.010204
observed data	observed	0.058824
:py attr lda keeplastcheckpoint is set to	ml distributed ldamodel get	0.066667
test the python direct kafka stream foreachrdd get	tests test kafka direct stream foreach get	1.000000
with a	param	0.012500
sql statements	name f returntype	0.125000
a value to a boolean	to boolean	0.250000
a java udf so it can	java	0.012195
so that :func awaitanytermination() can be	sql streaming query	0.011765
vectors	vectors	0.583333
model that	model	0.011173
a k a confidence column name	col	0.016393
selector type of the chisqselector	selector type selectortype	0.333333
lda keeplastcheckpoint is set	ml distributed ldamodel	0.050000
json	writer json	0.125000
selector type of the	set selector type	0.111111
extract the week number	weekofyear	0.043478
creates a table based on the	catalog create external table tablename path	0.250000
queries so that	query	0.010753
already partitioned data	core external group by spill	0.047619
residual degrees of freedom	generalized linear regression summary residual degree of freedom	0.250000
load	mlutils load	1.000000
[[structtype]]s with the specified schema	schema	0.033333
a right outer join	full outer join other	0.111111
transforms a	java params transfer param	0.250000
of columns of blocks	blocks	0.076923
the deviance for the null	null deviance	0.250000
to wait for	manager reset	0.011905
using the new hadoop outputformat api mapreduce	save as new apihadoop dataset conf keyconverter valueconverter	0.142857
batches of	algorithm	0.090909
comprised of	random rdds poisson vector	0.125000
as a hexadecimal number	sql unhex col	0.142857
precision-recall curve which is a	ml binary logistic regression summary pr	0.083333
the column mean	standard scaler model mean	0.125000
to	to	0.215385
range of offsets from a single	offset range	0.047619
types	type	0.024390
sets the sql context to use for saving	mlwriter context sqlcontext	0.333333
jtype	jtype	1.000000
instances that provide javamlreader	java mlreadable	0.250000
memory for	object size	0.032258
:py attr predictions which gives the predicted	linear regression summary prediction col	0.142857
load a model	svmmodel load cls sc	0.200000
the first n rows to the console	show n truncate	0.333333
squared selector	sq selector	1.000000
local property set in	get local property key	0.066667
feature	mllib linear model	0.125000
load a java model from the given path	java loader load java cls sc path	1.000000
dot product of two vectors	mllib linalg dense vector dot other	0.058824
length of	sql length	0.050000
to be used with the spark sink	addresses storagelevel maxbatchsize	0.045455
has started	started batchstarted	0.250000
cachenodeids=false checkpointinterval=10 losstype="logistic", maxiter=20	ml gbtclassifier	0.095238
"predictions" which gives the predicted value of	ml linear regression summary prediction col	0.500000
test that the model params	mllib streaming kmeans test test model params	0.250000
with a randomly generated	cross validator	0.045455
model with weights already	streaming linear regression with	0.111111
data into disks	group	0.025641
outcomes for k classes	classes	0.034483
configuration	conf init loaddefaults _jvm	1.000000
rdd	rdd partition by	0.062500
json	writer json path	0.125000
polynomial	polynomial	1.000000
and attach docstring from func	wrapped	0.142857
multi-dimensional cube for the current	sql data frame cube	0.055556
start offset	offset	0.021739
string	param params	0.014925
temporary table in the	table df	0.083333
that :func awaitanytermination() can be used again	manager reset	0.011905
used with the spark sink deployed	addresses storagelevel maxbatchsize	0.045455
copies param	param	0.006250
distinct count of col	count distinct col rsd	0.333333
__init__(self	ml linear regression init	1.000000
str	str	0.454545
the right singular vectors of	mllib linalg singular	0.017544
abstract class representing a multiclass classification	linear classification	0.142857
the norm of a	mllib linalg sparse vector norm p	0.083333
timecolumn	timecolumn	1.000000
create a multi-dimensional rollup for	frame rollup	0.055556
the week number of a given date as	sql weekofyear col	0.055556
:func awaitanytermination() can be used again to wait	sql	0.002525
a new rdd	core rdd	0.003460
to a new column of corresponding string	to string	0.500000
labels corresponding to indices to be assigned	indexer model labels	0.166667
be used again to wait for new	sql streaming query manager reset	0.011905
stop the execution of the streams with	context stop	0.125000
an rdd comprised of vectors containing	random rdds normal vector rdd	0.166667
returns the root mean squared error which	regression metrics	0.083333
byte	framed serializer	1.000000
outer join of c{self}	outer join other	0.333333
size to be used for each iteration of	size	0.009174
deserialized batches lists of	stream without unbatching	0.125000
this configuration	conf	0.050000
queue of	streaming streaming context queue	0.500000
the deviance for the null model	generalized linear regression summary null deviance	0.250000
in this grid to	grid builder add grid	0.100000
return sparkcontext which is associated	streaming context spark	0.083333
returns a :class dataframe representing the result	sql sqlcontext sql sqlquery	0.250000
checkpointinterval=10 impurity="variance",	tree regressor	0.058824
is of type distributedldamodel	ml ldamodel is	0.066667
contains a param with a given	has	0.011628
func	func	0.625000
by :func	streaming	0.005025
a number in	sql	0.002525
for prediction tasks regression and classification	prediction	0.041667
content of the :class dataframe	sql data frame writer save	0.083333
tests	tests test	0.018519
current [[dataframe]] and perform the specified aggregation	sql grouped data pivot pivot_col values	0.050000
output a python rdd	core rdd save as	0.037500
infer schema from an rdd of row or	infer schema rdd	0.250000
rdd 'x' has maximum membership	mllib gaussian mixture	0.045455
__init__(self	ml binary classification evaluator init	1.000000
elements in this rdd	core rdd	0.013841
accessible via jdbc url url	reader jdbc url	0.250000
comprised of vectors containing i i d samples	random rdds poisson	0.125000
loads orc files returning the result as a	reader orc path	0.200000
libsvm format into an rdd of labeledpoint	lib svmfile sc path	0.125000
pipeline create and	pipeline	0.052632
:func awaitanytermination() can be	query manager reset	0.011905
an associative and commutative reduce function but	rdd reduce	0.071429
class	core profiler collector	1.000000
the accumulator's value only usable in	core accumulator	0.030303
instance contains a param with a given	params has	0.019231
as specified by the optional key	key	0.017857
or compute the number of	indexed row matrix num	0.100000
a left outer join	left outer join other numpartitions	0.111111
return a list that contains all	collect	0.125000
used	streaming	0.005025
column of the current [[dataframe]] and perform	pivot pivot_col values	0.050000
sort the elements in iterator	iterator key reverse	0.200000
:class column for distinct	distinct	0.055556
of vectors	ml vector indexer	0.200000
shift	shift	0.714286
make predictions on batches of data from a	algorithm predict on	0.066667
bins	bins	1.000000
a data source	source schema	0.181818
levenshtein distance of the two	sql levenshtein left	0.058824
function to get or create global	get or create cls	0.200000
already	group	0.025641
correlation of two columns of a dataframe	data frame corr col1 col2 method	0.500000
set application name	core spark conf set app name value	1.000000
:func	sql streaming query manager reset	0.011905
predicts rating for the given user and	matrix factorization model predict user	0.500000
the dot product of two vectors	vector dot other	0.050000
a py or zip dependency for all tasks	py	0.050000
direct	direct	1.000000
the content of the dataframe in a	sql data frame	0.005348
pipelinemodel used for ml persistence	pipeline model	0.071429
a value to a mllib	to	0.007692
with the default storage level (c{memory_and_disk})	cache	0.125000
parammap into a	map to	0.125000
comprised of	mllib random rdds exponential vector	0.125000
spark configuration	core spark conf init	0.250000
a param with a given string name	has	0.011628
ranking	ranking metrics	0.666667
cols	cols	0.526316
retrieve gaussian distributions as a dataframe	ml gaussian mixture model gaussians df	0.166667
comprised	random rdds	0.115385
that :func awaitanytermination()	sql streaming query	0.011765
matrix columns in an	matrix columns to ml dataset	0.142857
year of	sql year col	0.050000
variance and count of	core	0.003021
memory string in the format	memory s	0.142857
of estimated coefficients	ml linear regression summary	0.142857
infer schema from	infer schema	1.000000
with arbitrary key and	core	0.006042
infer schema from	sqlcontext infer schema	1.000000
local checkpointing using spark's existing caching layer	local checkpoint	0.500000
do external sort when the memory goes above	core external sorter sorted	1.000000
the model	streaming logistic regression	0.500000
each rdds	dstream	0.031250
extracts the embedded default param values and user-supplied	ml param params extract param	0.333333
by a worker process after the fork()	core worker sock	0.500000
setparams(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor",	set params featurescol labelcol predictioncol	0.100000
dot product of	mllib linalg dense vector dot other	0.058824
in one	core	0.003021
queries so that :func awaitanytermination()	sql streaming query	0.011765
as a dictionary of value count pairs	count by value	0.333333
restore an object of	restore	0.125000
:class dataframe from an :class rdd,	schema samplingratio verifyschema	0.029412
mark the rdd as non-persistent and	rdd unpersist	0.066667
this block matrix this	linalg block matrix	0.052632
a param with a given	params has param	0.019231
md5 digest and	md5	0.125000
external sort	core external	0.016129
memory for this obj assume that	core external merger object size obj	0.040000
dump already partitioned data into	by spill	0.047619
this instance contains	param params has	0.019231
default	core spark context default	1.000000
right singular	linalg singular	0.017544
with the dispatch to handle all function types	cloud pickler save function	0.142857
partitioned data into disks	group by spill	0.047619
substr	substr str	0.125000
each dstreams in this	streaming	0.005025
embedded params to the companion	java params transfer params to	0.333333
given data type string	sql	0.002525
the model	linear regression model	0.133333
params	params m1 m2	0.047619
note : experimental	gaussian mixture summary	0.200000
a column of the current [[dataframe]] and	grouped data pivot pivot_col values	0.050000
a function	sql user defined function	0.083333
an rdd created by	core rdd	0.003460
returns one of	sql	0.002525
levenshtein distance	levenshtein	0.045455
into a jvm seq of	seq	0.043478
root mean squared error which is defined	mllib regression	0.022727
a resulting rdd that contains a tuple with	core rdd cogroup	0.066667
contains a param with a given string name	param params	0.014925
param is explicitly set by user or has	param params is defined param	1.000000
convert a dict into	sql to	0.041667
one operation	core rdd	0.003460
the least value	sql least	0.055556
dot product of two vectors	ml linalg dense vector dot other	0.090909
position pos	str pos	0.250000
each point in rdd 'x' to all mixture	gaussian mixture	0.038462
rdd's elements in one operation	rdd	0.003058
the spark	spark context	0.023256
term frequency tf vectors to tf-idf vectors	mllib idfmodel transform x	0.142857
a new spark configuration	core spark conf init loaddefaults _jvm _jconf	0.250000
of nodes in tree including	decision tree model	0.050000
fwe	set fwe fwe	1.000000
kolmogorov-smirnov ks test for data sampled from a	stat statistics kolmogorov smirnov test data	0.111111
kmeans algorithm for fitting and	kmeans	0.025641
matrix columns in an input dataframe	matrix columns from ml dataset	0.142857
the weights for each tree	ml tree ensemble model tree weights	0.333333
matrix columns	matrix columns to	0.142857
feature	linear	0.025641
a randomly generated	validation split	0.200000
processingtime	processingtime	1.000000
set initial centers should	initial centers centers	0.200000
splits str around pattern pattern is	sql split str pattern	0.333333
value of the date column	date	0.037037
output a python rdd	core rdd	0.013841
given java_class type useful for	pylist java_class	0.500000
compute the number	matrix num	0.088235
if the stage	stage	0.062500
calculates the length of a string or binary	sql length col	0.050000
if specified	sql data frame	0.005348
model with weights	linear regression with	0.111111
n elements from an rdd ordered	rdd take ordered	0.050000
an fp-growth model that contains frequent itemsets	mllib fpgrowth train cls	0.100000
the behavior when data or table already exists	sql data frame writer mode savemode	0.071429
the initial value of	logistic regression with sgd set initial	0.111111
the += operator adds a term	core accumulator iadd term	0.142857
impurity="gini", numtrees=20 featuresubsetstrategy="auto",	random forest classifier	0.045455
unpickling it will convert each python object into	mllib to	0.250000
of this	ml one	0.166667
f1measure	f1measure	0.833333
for cross	ml cross	0.333333
wait for	streaming query	0.010526
deviance for	deviance	0.111111
to configure the kmeans algorithm for fitting and	streaming kmeans	0.035714
goodness of	stat statistics	0.125000
truncated to the unit specified by the	trunc	0.142857
column for distinct count of	count distinct	0.040000
applies transformation on a vector	transformer transform vector	0.500000
month of a given date	dayofmonth	0.027027
for the null model	null	0.062500
values for each numeric	grouped data avg	0.058824
sets the given parameters in this grid	ml param grid builder add grid	0.100000
commutative reduce function but return	core rdd reduce	0.083333
the uid of this instance	ml param params reset uid	0.058824
used to write a :class dataframe	data frame	0.005000
instance's params to the wrapped java object	java params to	0.045455
checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)	ml random forest classifier	0.023256
of memory	external	0.013889
a [[pcamodel]] that contains the principal components	mllib pca fit data	0.166667
a randomly generated uid and	cross validator model	0.050000
storage type	type cls	0.250000
rdd >>> rdd	rdd	0.003058
is	is	0.666667
entries	entries	1.000000
are the left singular vectors of the singularvaluedecomposition	singular value decomposition	0.166667
distributions as a	mixture model gaussians df	0.333333
training iterations until termination	training summary total iterations	1.000000
the standard deviation of this rdd's elements	core rdd stdev	0.066667
sets the accumulator's value only	accumulator	0.012987
string column in json format	col	0.016393
a column scipy matrix from	sci py tests scipy matrix	0.090909
calculates the norm of a	sparse vector norm p	0.066667
terminations	streaming	0.005025
cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20	random forest classifier	0.022727
given path a shortcut of write()	ml mlwritable	0.142857
operation	core rdd	0.003460
this configuration	core spark conf	0.055556
weights computed	model weights	0.166667
train a gradient-boosted trees model for	mllib gradient boosted trees train regressor	0.333333
the model's transform method	logistic regression summary predictions	0.200000
stages	status tracker get	0.500000
compute the dot product	dot other	0.100000
database dbname	dbname	0.045455
:class dataframe using the specified columns so we	sql data frame	0.005348
converts vector columns in an	convert vector columns to ml	0.166667
much of	external merger	0.031250
keyclass	keyclass	1.000000
the values for each key in the	by key numpartitions	0.111111
set a local property that affects jobs	context set local property key	0.200000
converts vector columns	mlutils convert vector columns	0.166667
used again to wait for new terminations	query	0.010753
with weights already set	with	0.055556
columns on	data frame	0.010000
cloud	cloud	1.000000
from one base	frombase tobase	0.333333
stream	stream from	0.250000
representing a user defined function udf	udf f returntype	0.200000
coefs are predicted accurately by fitting on	parameter accuracy	0.029412
registers	register	0.125000
for new terminations	streaming query	0.010526
an error	mllib mllib	0.500000
ml params instances for	params m1 m2	0.047619
the bisecting k-means algorithm return the model	mllib bisecting kmeans train rdd	0.333333
an alias set	alias alias	0.333333
of	ml estimator	0.125000
note : experimental	logistic regression summary	0.045455
to wait	streaming query manager reset	0.011905
converts matrix columns	convert matrix columns to ml	0.166667
saving the content of the non-streaming	write	0.071429
each rdd contains the count of	count by	0.100000
batches of data from	mllib streaming linear algorithm	0.166667
initialized or	ensure initialized	0.333333
this matrix to the new mllib-local representation	dense matrix as ml	0.333333
containing a json string into a	sql from json	0.166667
used with the spark sink deployed	storagelevel maxbatchsize	0.045455
matrix columns in an input	matrix columns from ml	0.142857
the selector	sq selector set selector	0.333333
given parameters in this grid to fixed values	ml param grid builder add grid param values	0.333333
of a batch of jobs has started	batch started	0.333333
return an numpy ndarray	mllib linalg sparse matrix to array	1.000000
the :class dataframe	data frame writer save	0.083333
of the current [[dataframe]] and perform the specified	pivot pivot_col values	0.050000
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20	ml random forest classifier	0.023256
returns the precision-recall curve which is a dataframe	binary logistic regression summary pr	0.083333
:py class pyspark.ml.linalg.matrix	mat	0.166667
the greatest value	greatest	0.043478
__init__(self strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none):	ml imputer init strategy missingvalue inputcols outputcols	1.000000
the :class dataframe using the	data frame	0.005000
how much of memory for this	merger object size	0.032258
stratified sample without replacement	data frame sample	0.066667
this model to transform input	model	0.005587
an rdd comprised of vectors containing i	random rdds poisson vector rdd	0.166667
sort the elements in iterator	iterator key	0.200000
which is a dataframe having two fields fpr	regression summary	0.035714
create a multi-dimensional cube for the current	frame cube	0.055556
used again to wait for new	sql streaming query manager	0.011905
by the given	by	0.014286
sort the list based	spark streaming test case sort result based	1.000000
l{statcounter} object that captures	stats	0.055556
local property set in this	spark context get local property key	0.066667
given parameters in this grid to fixed	grid builder add grid param	0.250000
heap	heap	0.285714
with a randomly generated uid and	cross validator	0.045455
new row object	sql row call	0.500000
columns in an input dataframe to the :py	columns from ml dataset	0.125000
2 ml params	params m1 m2	0.047619
blockmatrix by other, another	multiply other	0.200000
extract the minutes of a	minute col	0.050000
this obj assume that all	size obj	0.040000
new	streaming query manager	0.011236
a 'new api' hadoop	new apihadoop	0.333333
model fitted by prefixspan >>> data = [	prefix span model	1.000000
into label indices	mlutils parse	0.250000
pipelinemodel create and return	pipeline model	0.071429
value of	ml cross	0.333333
on incoming	streaming	0.005025
only create	hive context create	0.083333
the norm	linalg sparse vector norm	0.133333
for the test this	test	0.015152
which is assumed to consist	ascending numpartitions keyfunc	0.100000
the cluster centers represented as a list	mllib kmeans model cluster centers	0.083333
specifies the underlying output	data frame writer format	0.333333
given parameters in this grid to fixed	param grid builder	0.055556
extract the year of a given date as	sql year col	0.050000
value of	ml gbtregressor	0.333333
names into a jvm seq of column	seq sc	0.055556
default param	param	0.006250
tcp server to receive accumulator updates in a	update server	0.333333
returning the result as a	reader	0.040000
is of type	type	0.024390
this instance contains	param params has param	0.019231
terminations	sql streaming query manager	0.011905
features corresponding to that user	model user features	0.333333
a converter to drop the names	converter	0.052632
makes a class inherit documentation from	inherit doc cls	0.045455
save this model to the given	bayes model save sc	1.000000
a param with a given string	param params	0.014925
converts vector columns in an input dataframe from	mlutils convert vector columns to ml	0.166667
with a given string	params	0.006623
make predictions on	predict on	0.176471
for comparing instances	ml ldatest compare m1 m2	0.250000
cluster centers represented as a	cluster centers	0.090909
files in disks	core external	0.016129
labelcol="label", predictioncol="prediction", maxiter=100	labelcol predictioncol maxiter	0.333333
from an :class rdd, a list or a	schema samplingratio verifyschema	0.029412
curve which is	binary logistic regression summary	0.111111
an array containing the ids of all active	active stage ids	0.200000
dstream by applying reducebykey to each rdd	streaming dstream reduce by key func	0.076923
vocabsize	vocabsize	1.000000
that :func awaitanytermination() can be used again to	reset	0.011236
by the optional key	key	0.017857
a local temporary view with	temp view name	0.111111
named table	table column	0.166667
on a	core spark context get	0.333333
true positive rate for a given	true positive rate	0.200000
sets vector size default 100	set vector size vectorsize	1.000000
a list of active queries associated with	sql streaming query manager active	0.066667
a new accumulator with	accumulator	0.012987
two separate arrays of indices and	ml linalg	0.030303
of features	features	0.043478
given join expression	join	0.034483
number of data points in each cluster	ml clustering summary cluster sizes	0.333333
__init__(self formula=none	init	0.021277
get number of trees in ensemble	tree ensemble model num trees	1.000000
residual degrees	ml generalized linear regression summary residual degree	0.500000
specified table	table name	0.333333
as the spark fair	spark	0.013158
frequency vectors or transform the	mllib hashing tf transform	0.045455
a l{statcounter} object that captures the mean variance	stats	0.055556
belongs to this params	param params	0.014925
already partitioned data into	core	0.003021
shortcut of write() save path	ml mlwritable save path	0.200000
dummy params instance used as a placeholder to	param params dummy	0.111111
posexplode	posexplode	1.000000
model on	model	0.011173
this rdd's	core	0.003021
get all values as a list	get all	0.166667
pandas	pandas	0.454545
boundaries in increasing order for which predictions are	regression model boundaries	0.333333
the java_model to a python model	discretizer create model java_model	0.250000
then merges them with extra values from input	map extra	0.040000
external table	external table tablename	0.500000
of write() save	ml one vs rest save	0.166667
wait for	sql streaming query manager reset	0.011905
mixin for param elasticnetparam the elasticnet	has elastic net param	0.200000
this instance contains a param	param params	0.014925
batches of data from	streaming linear algorithm	0.076923
parses a line in libsvm format into label	mllib mlutils parse libsvm line line multiclass	0.111111
to a boolean if possible	ml param type converters to boolean	0.250000
labels corresponding to indices to	string indexer model labels	0.166667
a pearson's independence test using dataset	chi square test test dataset featurescol labelcol	0.333333
merges them with extra values from input into	extra	0.023810
set the trigger for the	writer trigger	0.111111
application	application	1.000000
of the dataframe in a	sql data frame writer	0.011628
precision of all	metrics precision	0.200000
threshold precision curve	binary logistic regression summary precision by threshold	0.166667
for this standardscaler	ml standard scaler	0.200000
for each numeric columns	grouped data avg	0.058824
by the given columns	bucket by	0.200000
off the heap maintaining the heap invariant	core heappop heap	0.142857
invalidates and refreshes all the cached data	catalog refresh by	0.200000
queries so that :func awaitanytermination() can be	reset	0.011236
list of columns for the	catalog list columns	0.166667
values of the accumulator's data type returning a	accumulator param	0.038462
again to wait for new terminations	sql streaming query manager reset	0.011905
parammap into a java	to java	0.045455
inherit documentation from its	mllib inherit doc	0.045455
of time for a condition to	condition	0.045455
used to load a streaming :class dataframe from	data stream	0.028571
make predictions on	kmeans predict on	0.500000
right singular vectors of	mllib linalg singular	0.017544
parameters in this grid to fixed	param grid builder	0.055556
the input	reader	0.040000
schema in the tree format	schema	0.033333
checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none	ml random forest classifier	0.023256
stratified sample without	sample	0.050000
save this model to the	mllib logistic regression model save	0.500000
for this bucketizer	ml bucketizer	0.250000
java udf so it can be used	java	0.012195
return the column	mllib standard scaler model	0.100000
model that can	model	0.005587
new mllib-local representation	as ml	0.333333
operation test for dstream flatmapvalues	operation tests test flat map values	1.000000
parameters in this grid to fixed	ml param grid builder base on	0.076923
perform a left outer	rdd left outer	0.333333
start	sql add months start	1.000000
read an 'old' hadoop	core spark context hadoop	0.333333
data or	streaming context get or	0.200000
generates an rdd comprised	random rdds uniform vector rdd sc	0.200000
which is defined as the square root	linear regression summary root	0.500000
squared distance from a sparsevector	sparse vector squared distance other	0.166667
merges them with extra values from input	extra	0.023810
>>> dm = densematrix(2 2 range 4	linalg dense matrix repr	0.142857
an rdd of points using the model	model	0.005587
the stage info	stage info	0.142857
adds a	core accumulator add	0.076923
rdd 'x' has maximum membership in	gaussian mixture	0.038462
a sparse vector using either a dictionary a	mllib linalg vectors sparse size	0.166667
converts matrix columns in an	mlutils convert matrix columns from	0.166667
the bisecting k-means algorithm	mllib bisecting kmeans	0.500000
list of predicted ratings for input user and	matrix factorization model predict all user_product	0.050000
the initial	initial	0.071429
if using checkpointing	get checkpoint	1.000000
rating for the	matrix factorization	0.040000
values for each key	key func	0.066667
that makes a class inherit documentation	inherit doc cls	0.045455
column mean values	model mean	0.125000
comprised of vectors containing i i	mllib random rdds exponential	0.125000
convert the vector into	ml linalg vector to	1.000000
udf with a function and	user defined function	0.066667
of gaussians in mixture	gaussian mixture model k	0.200000
contains a param with a given	ml param params	0.013699
conduct pearson's chi-squared goodness of fit test	stat statistics chi sq test	0.166667
directory for	path	0.010204
wait for the execution to stop	context await termination timeout	0.166667
columns in an input	columns to	0.125000
table in the catalog	table df tablename	0.083333
create a column scipy matrix from	sci py tests scipy matrix size	0.090909
starts at pos in byte	pos	0.022222
so that :func awaitanytermination() can be	sql streaming	0.010204
collect each rdds into the returned list	test case collect dstream n	1.000000
:func awaitanytermination()	reset	0.011236
a randomly	split model	0.200000
in this grid to	ml param grid builder add grid	0.100000
convert matrix attributes which are	ml linalg matrix convert	0.166667
to in this model	mllib kmeans model	0.250000
error which is	mllib regression	0.022727
akaike's "an information criterion" aic for	generalized linear regression summary aic	0.250000
rdd of key-value pairs	core rdd save as	0.037500
generate	sgdtests generate logistic input	1.000000
params instances for the given	params m1	0.047619
from an rdd ordered in ascending order	core rdd take ordered	0.050000
calculates the norm of a sparsevector	linalg sparse vector norm p	0.066667
the latest model	mllib streaming kmeans latest model	0.500000
directory that contains	directory	0.166667
a	offset	0.043478
test of	test	0.015152
in	core	0.012085
the levenshtein distance of	levenshtein left right	0.058824
function without changing the	f	0.010526
unpersist	unpersist	0.416667
param with a given string name	param params has	0.019231
create a densematrix	ml linalg matrices dense numrows numcols values	0.333333
memory string in the format supported	memory s	0.142857
memory for	external merger object	0.032258
sets	aftsurvival regression set	1.000000
year of a given	sql year	0.050000
obj assume that all	merger object size obj	0.040000
string name	ml param	0.009524
date column	day date	0.100000
:class windowspec with the	sql window range	0.166667
__init__(self featurescol="features", labelcol="label",	init featurescol labelcol	1.000000
pearson's chi-squared goodness of fit test of	mllib stat statistics chi sq test	0.166667
set a configuration property if not already set	core spark conf set if missing key value	0.500000
the current [[dataframe]] and	grouped data pivot pivot_col values	0.050000
of the current [[dataframe]] and	pivot pivot_col values	0.050000
heappop followed by a heappush	core heapreplace	0.250000
note : experimental	linear regression training summary	0.500000
test this should be list of lists	test	0.015152
queries so that	streaming query	0.010526
value of	ml normalizer	1.000000
in rdds in a sliding window over	value and window windowduration slideduration	0.076923
converts vector columns in an input	mllib mlutils convert vector columns to ml	0.166667
fast version	heappushpop heap	0.142857
elasticnetparam the elasticnet mixing parameter in range	elastic net	0.125000
specifies the input schema	sql data frame reader schema schema	0.333333
indicates whether this instance is of	ml ldamodel is	0.066667
linear regression model derived from a	linear regression model	0.066667
setparams(self	ml logistic regression set params	1.000000
featurescol=none labelcol=none predictioncol=none classifier=none): sets params for onevsrest	one vs rest set params	1.000000
of	external merger object size	0.032258
a given string name	has	0.011628
number of nonzero elements	mllib linalg sparse vector num	0.200000
of vectors which	ml vector indexer	0.200000
wait for the execution to stop return true	streaming streaming context await termination or timeout timeout	0.125000
maxheap version of a	max heap item	0.250000
path of a file added through	core spark files	0.125000
of deserialized batches lists	stream without unbatching	0.125000
squared distance from a sparsevector or 1-dimensional numpy	vector squared distance	0.166667
makes a class inherit documentation from its parents	mllib inherit doc	0.045455
vector class for	vector	0.019231
value of	ml regression evaluator	1.000000
fit test of the observed data against	test observed	0.090909
a large dataset and an item approximately	lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
randomly generated uid and	cross validator model	0.050000
checkpointinterval=10 impurity="gini",	classifier	0.050000
message	message	1.000000
this streaming	streaming	0.005025
returns the soundex encoding for a string >>>	sql soundex col	0.055556
number of nonzero	num	0.016807
the	mllib standard scaler model	0.100000
or transform the rdd	hashing tf transform	0.045455
an iterator of deserialized objects from	serializer load	0.083333
merges them with extra values from	extra	0.023810
fp-growth model that contains	fpgrowth train cls data	0.200000
the given timezone returns	sql	0.002525
a python rdd of key-value pairs (of form	core rdd	0.010381
passed as a list	spark conf	0.058824
ignore separators inside brackets	ignore brackets	0.250000
shape	shape	1.000000
range of offsets from a single kafka topicandpartition	offset range	0.047619
runs the bisecting k-means algorithm return	mllib bisecting kmeans train rdd k maxiterations mindivisibleclustersize	0.333333
add a	context add	0.500000
numtrees=20	random forest	0.125000
predicts rating	mllib matrix factorization model predict	0.250000
dbname	dbname	0.227273
the mean	core	0.003021
matrix columns in an input dataframe from	matrix columns to ml	0.142857
cluster centers represented as a list of numpy	model cluster centers	0.060606
data sampled from a	data distname	0.083333
hadoop configuration which is passed in	spark context hadoop	0.090909
arrays of indices and	ml	0.001835
table in the	table df	0.083333
returns the greatest value of the list of	sql greatest	0.055556
the minimum item	min key	0.333333
restore an object of namedtuple	restore name fields value	0.333333
of this :class dataframe	data frame	0.010000
to wait for	manager	0.011236
total log-likelihood	gaussian mixture summary log likelihood	0.142857
comparing instances	ml ldatest compare m1 m2	0.250000
this instance with a	one	0.117647
number of partitions to use during reduce tasks	rdd default reduce partitions	0.166667
converts matrix columns in an input dataframe	mlutils convert matrix columns from	0.166667
a profile object is returned	profiler profile func	0.200000
2 ml params instances for the	params	0.006623
add a py or	context add py file	0.166667
imputer	ml imputer	0.250000
with extra values from input	extra	0.023810
called by the default implementation of	ml estimator	0.125000
model's transform method	logistic regression summary predictions	0.200000
sets the given parameters in this grid	ml param grid builder base on	0.076923
converts matrix columns in	mllib mlutils convert matrix columns	0.166667
memory for this	merger object size	0.032258
termination	termination	0.214286
test that the	with sgdtests test	0.111111
rawpredictioncol raw prediction a	raw prediction	0.200000
that param type conversion happens	param type conversion tests	0.333333
from start to end exclusive increased by step	context range start end step	1.000000
vector columns in an	vector columns	0.142857
convert a matrix from the new mllib-local representation	mllib linalg matrices from	0.333333
a dataframe that stores	alsmodel	0.142857
values for each numeric columns for each	grouped data avg	0.058824
partial objects do	save partial	0.125000
svm classifier	svcmodel	0.166667
string name	params	0.006623
window of	window	0.037037
list of labels corresponding	ml string indexer model labels	0.066667
is set to	spark context set	0.166667
create a python topicandpartition	and partition init topic partition	0.055556
ordered list of	ml	0.001835
of tree (e g depth 0 means 1	tree model	0.026316
str list	col	0.016393
a list of predicted ratings for	matrix factorization model predict all user_product	0.050000
computes an fp-growth model that contains	mllib fpgrowth train cls	0.100000
restore an object of	restore name fields	0.333333
test for data sampled from	test data distname	0.166667
as	reader	0.040000
the profile stats to stdout	core profiler show	0.166667
find synonyms of	model find synonyms	0.333333
censorcol	censor col	1.000000
the dot product of	linalg dense vector dot other	0.058824
a multi-dimensional cube for the	frame cube	0.055556
dstream and other	other	0.033333
sparkcontext which is	streaming context spark	0.083333
multi-dimensional rollup	data frame rollup	0.055556
dump already partitioned data	group by	0.041667
setparams(self inputcol=none outputcol=none labels=none) sets params	to string set params inputcol outputcol labels	0.333333
wait until any of the queries on	any termination timeout	0.166667
makes a class inherit documentation from its parents	mllib inherit	0.045455
with a function	user defined function	0.066667
into	core external group	0.045455
profile object is returned	profiler profile func	0.200000
stream transform get	stream transform get	1.000000
value of	ml stop words	1.000000
the values of a dstream and	values dstream	0.250000
in this	core	0.006042
create a new	create	0.017241
convert to	matrix to	1.000000
saves the content	writer	0.040000
create an input stream that pulls events	create stream ssc	0.200000
predict the value	predict	0.034483
queries so that :func awaitanytermination() can be used	sql	0.002525
returns the root mean squared error which	mllib regression metrics	0.090909
tree <http //en wikipedia	tree regressor	0.058824
a dictionary a list of index value	init size	0.066667
a :class list of :class	data frame	0.005000
rdd of document to	document	0.040000
create a python topicandpartition to map	init topic partition	0.055556
with start offset specified	offset	0.021739
returns the root mean squared	metrics	0.041667
in "predictions" which gives the predicted	linear regression summary prediction	0.142857
probabilitycol="probability", rawpredictioncol="rawprediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0	probabilitycol	0.150000
get total number of nodes	model total num nodes	0.250000
values	sql	0.005051
libsvm format into label indices values	mllib mlutils parse libsvm	0.125000
list of names of tables	sqlcontext table names	0.066667
new :class column for distinct	distinct	0.055556
vector conduct pearson's chi-squared goodness of	mllib stat statistics chi sq	0.066667
the left singular vectors	mllib linalg singular	0.017544
the number	mllib linalg block matrix num	0.062500
comprised of vectors	random rdds normal	0.125000
called when processing of a	streaming streaming listener on output operation	0.166667
transforms a python parammap into a java	params transfer param map to java	1.000000
to "word"	synonyms word	0.166667
an input stream that is to	stream ssc	0.090909
how much of	merger	0.025641
finding frequent items	freq items cols support	0.166667
create a python topicandpartition to	partition init topic partition	0.055556
broadcast a read-only	core spark context broadcast value	0.125000
return sparkcontext which is associated with this	streaming streaming context spark	0.083333
return whether this rdd is checkpointed	core rdd is checkpointed	0.333333
data of another	other	0.033333
downloaded	recursive	0.125000
that :func awaitanytermination() can be used	streaming query manager reset	0.011905
test that the final value	streaming logistic regression with sgdtests test	0.111111
vectors or transform the rdd of	hashing tf transform	0.045455
2 ml params instances	params m1	0.047619
rdd as non-persistent and remove all blocks for	core rdd unpersist	0.066667
train the model on the incoming	linear regression with sgd train on	0.333333
linear regression model derived from	linear regression model	0.066667
provides methods to set k	streaming	0.005025
original	max scaler model original	0.062500
topicandpartition	offset	0.021739
set the initial value	initial	0.071429
registers a python function including lambda function as	sql catalog register function name	1.000000
contains a param with a given string	param params	0.014925
of tree (e g depth	tree	0.020833
number of possible outcomes for	model num	0.083333
python rdd of key-value pairs (of form c{rdd[	core rdd	0.010381
test that the final	regression with sgdtests test	0.111111
this	object size	0.032258
this stringindexer	string indexer	0.250000
broadcast a read-only variable to	context broadcast	0.125000
basic operation test for dstream filter	streaming basic operation tests test filter	1.000000
to use for saving	ml java mlwriter	0.200000
get or compute	linalg distributed	0.333333
inputformat with arbitrary key	inputformatclass	0.095238
unique identifier for the spark application	core spark context application id	0.500000
k=2	k	0.142857
saves the content of the dataframe	sql data frame	0.005348
sets	has reg param set	1.000000
poisson	poisson	1.000000
already	core external	0.016129
which gives	linear regression	0.120000
sort the list based on first	sort result based on	0.333333
wait for new	sql streaming query manager reset	0.011905
n elements in	n	0.027778
mixture components	gaussian mixture model predict soft x	0.142857
convert a value to a mllib vector	to vector value	0.250000
start offset specified	from offset	0.125000
embedded	param params	0.014925
corresponding string	string	0.041667
fraction given on each stratum	by col fractions seed	0.142857
returns a new java object	java wrapper new java obj java_class	0.333333
resets	sql runtime config unset	0.142857
temporary table in	table	0.031250
finding frequent items for columns possibly with false	sql data frame freq items	0.166667
file to which	file	0.028571
mlreader	mlreader	0.222222
sets the sql context to	context sqlcontext	0.083333
vector of 64-bit floats from a	ml linalg vectors	0.250000
the new hadoop outputformat api mapreduce	as new apihadoop dataset conf keyconverter valueconverter	0.142857
much of memory	object size	0.032258
count of distinct	count	0.016949
:func awaitanytermination() can be used again	sql streaming query manager reset	0.011905
of this instance with a randomly	ml cross validator	0.166667
py or	py file path	0.066667
this obj assume	obj	0.023810
a given string	has param	0.019231
rdd 'x' to all mixture	mllib gaussian mixture	0.045455
perform a left outer join of c{self}	rdd left outer join other	0.111111
all globals names read or written to	core cloud pickler extract code globals	0.125000
converts vector columns in an input dataframe	mlutils convert vector columns to ml dataset	0.166667
accuracy equals to the total number of	accuracy	0.076923
the vector	ml linalg vector	0.500000
inherit documentation from its	mllib inherit	0.045455
prints the first n rows to the console	show n truncate	0.333333
load a java model from the given path	mllib java loader load java cls sc path	1.000000
points	points sc	1.000000
id to all the jobs started by	job	0.023810
list of indices to	ml	0.001835
representing a multiclass classification model	classification model	0.166667
makes a class inherit documentation	mllib inherit	0.045455
of names of tables in the database dbname	table names dbname	0.500000
top "num" number of products for all users	products for users num	0.200000
a new sqlcontext	sqlcontext init	0.500000
setparams(self scalingvec=none inputcol=none outputcol=none) sets params	product set params scalingvec inputcol outputcol	0.333333
again to	manager	0.011236
and then merges them with extra values	extra	0.023810
sets random seed	word2vec set seed seed	1.000000
saves the	name format mode partitionby	0.200000
so that :func awaitanytermination() can be used	sql	0.002525
that has no partitions or	spark context empty	0.333333
computes an fp-growth model that contains frequent	mllib fpgrowth train cls data	0.100000
:class linearregression	linear regression	0.040000
resets the configuration property	runtime config unset	0.142857
finding frequent items for columns possibly with	frame freq items cols support	0.166667
a test statistic result	stat test result	0.166667
only create a new hivecontext for	sql hive context create for	0.250000
final value of weights is close	parameter accuracy	0.029412
__init__(self splits=none	ml bucketizer init splits	1.000000
data	data iterations step	0.666667
use only create a new hivecontext for	sql hive context create for	0.250000
broadcast a read-only variable to the cluster returning	context broadcast value	0.125000
rdd 'x' to all mixture	gaussian mixture model	0.052632
value of	ml param has reg param	1.000000
optional default value and user-supplied value	param params	0.014925
number of nonzero elements	sparse vector num	0.200000
a paired rdd	matrix factorization model	0.043478
or names into a jvm seq	seq sc	0.055556
(json lines text format or newline-delimited json	json	0.043478
generates python code for a	code name doc defaultvaluestr	0.111111
function on each rdd of this dstream	streaming kafka dstream transform func	0.500000
wait for new	query manager reset	0.011905
job of a batch has completed	completed	0.058824
test the python direct kafka stream transform get	stream tests test kafka direct stream transform get	1.000000
be used again to wait for new	reset	0.011236
length of a string or binary	length col	0.050000
convert this matrix to	matrix	0.015152
year of a given date as integer	year col	0.050000
numfolds	numfolds	0.625000
a param with	ml	0.001835
using l{rdd saveaspicklefile} method	core spark context pickle file name	1.000000
a dense	dense	0.111111
the number of	linalg row matrix num	0.100000
of memory	core external	0.016129
add a py or zip dependency	add py file path	0.166667
depth of tree (e g depth	tree model	0.026316
of the :class dataframe in	data frame writer	0.028169
batches after which the centroids of that particular	timeunit	0.025641
add a py or zip dependency for	core spark context add py file	0.166667
that :func awaitanytermination() can be	query manager	0.011905
again to wait for new	manager reset	0.011905
with start offset specified	from offset	0.125000
convert a list of	sql to	0.041667
contents of the :class dataframe to a data	data frame writer save path format	0.142857
in this frame but not in another frame	sql data frame subtract other	0.333333
onevsrestmodel create and return a	one vs rest	0.034483
creates a :class	sql spark session create	0.142857
elements from start to end exclusive	core spark context range start end	0.166667
of conditions and returns one	sql column otherwise	0.050000
:py attr lda keeplastcheckpoint	distributed ldamodel	0.052632
multi-dimensional cube for	frame cube	0.055556
active stages	status tracker get active	0.333333
using the old hadoop outputformat api	save as hadoop dataset conf keyconverter valueconverter	0.083333
creates or replaces a local temporary view with	create or replace temp view name	0.500000
of a dataframe as	sql data frame corr col1	0.166667
c{self} that is not contained	subtract other numpartitions	0.111111
converter to drop the	converter datatype	0.071429
pearson's independence test using dataset	ml chi square test test dataset featurescol	0.333333
instance for this	pipeline model	0.071429
stream api with start	stream	0.017544
stratified sample without replacement based	sample	0.050000
the root mean squared error which	regression	0.010000
setparams(self	ml generalized linear regression set params	1.000000
n rows to	n truncate vertical	0.250000
left singular vectors	singular	0.015625
the deviance for the null	regression summary null deviance	0.250000
defined	ml param	0.009524
restore an object of	core restore name fields	0.333333
given product and returns a list	product	0.029412
extract the year of	sql year col	0.050000
of a job of a batch has completed	completed outputoperationcompleted	0.125000
create a new dstream in which each	streaming streaming context	0.032258
mean variance and count of	core rdd	0.003460
given data type string to	sql	0.002525
infer schema from an rdd of row or	spark session infer schema rdd samplingratio	0.250000
product and returns a	product	0.029412
in this	grid builder add	0.200000
get all values as a	core spark conf get all	0.166667
wait for the execution to stop	streaming context await termination timeout	0.166667
sets the accumulator's value only	core accumulator value	0.045455
load a	model load cls	1.000000
rdd as non-persistent and remove all blocks	rdd unpersist	0.066667
labeledpoint	mlutils load lib svmfile sc	0.125000
defined on the	param	0.006250
forest model for classification or regression	forest	0.083333
convert this matrix to the new mllib-local representation	linalg dense matrix as	0.333333
a word	word	0.111111
creates an	sqlcontext create	0.500000
instance contains a param	param params has	0.019231
with a function and attach docstring from func	sql user defined function wrapped	0.333333
sparkcontext	sparkcontext	0.714286
the mean variance and count of	core rdd	0.003460
transfer this instance's params	params	0.006623
passed an illegal or inappropriate argument	illegal argument exception	1.000000
of indices to select filter	ml chi sq selector model selected features	0.500000
perform a right outer join	full outer join other	0.111111
a local property set in	local property key	0.035714
deviance for	ml generalized linear regression summary deviance	0.125000
create new row object	sql row call	0.500000
the minimum number of times a token must	min count	0.076923
returning	reader	0.040000
setparams(self featurescol="features",	ml linear regression set params featurescol	1.000000
weight	weight	1.000000
queries so that :func	sql streaming query manager	0.011905
[0 0 1 0] for feature selection by	chi sq selector	0.125000
choose one directory	external sorter get path	0.333333
sparkhome	sparkhome	0.833333
gets a	param params get	0.500000
returns a list of active queries associated with	streaming query manager active	0.066667
by the default implementation of	ml	0.001835
so that :func awaitanytermination() can be used again	query	0.010753
stepsize step size	step size	0.125000
get or compute the number of	num	0.025210
converts matrix columns	mlutils convert matrix columns from ml	0.166667
the underlying	mllib	0.010526
url url	url	0.076923
list of tables/views in	catalog list tables	0.250000
weighted averaged	weighted	0.166667
model	streaming linear regression	0.333333
instance for params shared	params	0.006623
maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10	maxdepth	0.100000
the :class dataframe as the	data frame writer save as	0.071429
or compute	linalg distributed	0.333333
returns a list of active queries associated with	query manager active	0.066667
until any of	streaming query manager await any	0.142857
wait until any of the queries on the	streaming query manager await any termination timeout	0.166667
checkpointed and materialized	checkpointed	0.083333
memory for this	core external merger object size	0.032258
estimated coefficients	generalized linear regression training summary	0.142857
all params with their optionally default values and	params explain params	0.250000
batches	mllib streaming linear algorithm	0.166667
trees in the ensemble	ensemble model	0.117647
instance contains a param with	param	0.012500
computes the levenshtein distance	levenshtein left	0.058824
choose one directory for spill by number n	core external sorter get path n	1.000000
return a javardd of object by	core	0.003021
pretty printing of a densematrix	str	0.090909
as the	save as	0.333333
compute the dot product of two vectors we	dense vector dot	0.050000
user-supplied values and then merges them with extra	extra	0.023810
decision tree <http //en wikipedia	decision tree regressor	0.058824
extract the minutes of a given	sql minute col	0.050000
broadcast a read-only variable to the cluster returning	core spark context broadcast value	0.125000
to all the jobs	job	0.023810
this instance contains	params has param	0.019231
compute the number of	linalg block matrix num	0.100000
load a	mllib linear regression model load cls	1.000000
value of	ml param has raw prediction	1.000000
scala	scala	1.000000
:class dataframe as a temporary table in	data frame as table df tablename	0.333333
rdd of labeledpoint	load lib svmfile sc path numfeatures	0.125000
expected value of	ml	0.001835
of params	params	0.013245
to end exclusive	end	0.066667
terminations	streaming query manager	0.011236
return the topics	topics	0.125000
any hadoop-supported	path	0.020408
from disk then sort	merge sorted items index	0.250000
name	param params has param	0.019231
dictionary a list of index value pairs or	init size	0.066667
note : experimental	min hash lshmodel	1.000000
cluster centers represented as	kmeans model cluster centers	0.090909
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto",	random forest classifier	0.022727
loads a parquet file stream returning the result	stream reader parquet path	0.083333
algorithm return the model	rdd	0.003058
the stream query if this is not	stream writer	0.041667
active queries associated with this	query manager active	0.066667
data into disks	core external group by	0.045455
:py attr predictions which gives the	generalized linear regression summary	0.090909
converts matrix columns in an	mlutils convert matrix columns to	0.166667
submit and test a single script file	submit tests test single script	0.500000
queries so that :func awaitanytermination() can	manager	0.011236
called when a	streaming streaming listener on	0.200000
get the offsetrange	ranges	0.090909
newline-delimited json	json	0.043478
values in descending order	value decomposition s	1.000000
intercept of linear svm classifier	ml linear svcmodel intercept	0.500000
tokens in the training set	training	0.029412
sets	index to string set	1.000000
instance's params	ml java params	0.125000
to an external database table	table mode properties	0.200000
for dataframe from a	from	0.045455
a new dstream by applying reducebykey to each	streaming dstream reduce by key	0.076923
wait for the execution to stop	streaming context await termination or timeout timeout	0.125000
create an input stream that pulls events	create stream ssc hostname	0.200000
attr predictions which	linear regression	0.040000
a list of index value pairs or two	mllib linalg	0.026316
local property set	spark context get local property key	0.066667
of the accumulator's data	accumulator	0.012987
content of the :class dataframe	sql data frame	0.021390
extract the day of the month of a	dayofmonth	0.027027
compute the dot product	linalg dense vector dot	0.058824
a python rdd	core rdd save	0.037975
model to a local	ldamodel to local	0.200000
representing the result of	sqlquery	0.027027
find the spark_home	core find spark home	1.000000
each original	max scaler model original	0.062500
to wait for new terminations	streaming	0.005025
new dstream in which each	streaming streaming context transform	0.066667
and profiles the	core basic	0.066667
left singular vectors of the singularvaluedecomposition	linalg singular value decomposition	0.250000
soundex encoding for a string >>> df =	soundex	0.043478
matrix attributes which are array-like or	matrix	0.030303
__init__(self inputcol=none outputcol=none)	init inputcol outputcol	0.333333
convert this distributed model to a local representation	ml distributed ldamodel to local	0.111111
squared error which is defined as	regression	0.010000
matrix other from this block	mllib linalg block	0.111111
represents a range of offsets	range	0.030303
bandwidth of	bandwidth bandwidth	0.125000
column for distinct count	count distinct	0.040000
returns a paired rdd where the first	matrix factorization model	0.043478
setparams(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor",	regression set params featurescol labelcol predictioncol	0.250000
list of column names skipping null values	sql	0.005051
add a py or zip dependency for	spark context add py file	0.166667
a python rdd of key-value pairs (of	rdd save as	0.038462
again	sql	0.002525
partial objects do not serialize correctly in	save partial obj	0.125000
comprised of vectors containing i i	mllib random rdds gamma	0.125000
get a local property set in this	context get local property key	0.066667
read an 'old' hadoop	context hadoop	0.090909
the accumulator's value only	accumulator value	0.050000
of substr	substr	0.071429
the :class dataframe to	data frame	0.005000
all the	size	0.009174
of [[structtype]]s with the specified schema	schema	0.033333
both have the same param	m1 m2 param	0.125000
setparams(self formula=none featurescol="features", labelcol="label", forceindexlabel=false) sets params	set params formula featurescol labelcol forceindexlabel	0.166667
returns the soundex encoding for a	soundex	0.043478
value for each original column during fitting	scaler model original	0.062500
of	ml bisecting kmeans model	0.153846
basic	basic	1.000000
again to wait for new	streaming query manager	0.011236
broadcast a read-only	context broadcast value	0.125000
the column	mllib	0.010526
parses the	sql expr	0.125000
of	ml logistic	0.166667
chi squared	chi sq	0.111111
importance of each feature	ml decision tree classification model feature importances	0.250000
a param with	params has	0.019231
this instance is	ldamodel is distributed	0.200000
of nodes	nodes	0.037037
the model to make predictions on batches	algorithm predict on	0.066667
an exception handler	exception handler	1.000000
distributed model to a local representation	distributed ldamodel to local	0.111111
given	ml param	0.009524
to this accumulator's	core accumulator add	0.076923
average values for each numeric	sql grouped data	0.041667
a dictionary a list of index	size	0.036697
with a given	params has param	0.019231
private class to track supported random forest parameters	random forest params	0.250000
a class inherit documentation from	mllib inherit doc cls	0.045455
in :py attr predictions which	generalized linear regression	0.090909
wait for new terminations	sql streaming query manager reset	0.011905
names of fields in obj	sql	0.002525
external sort when	external	0.013889
test python direct kafka	tests test kafka	1.000000
of columns that describes the	cols cols kwargs	0.090909
instance	ml param params has	0.019231
a multi-dimensional cube for the current :class dataframe	sql data frame cube	0.055556
iterations until termination	total iterations	1.000000
specified table as a :class dataframe	spark session table tablename	1.000000
an iterator that contains all	local iterator	0.333333
minimum number of times a	min count	0.076923
in "predictions" which gives the true label of	ml linear regression summary label col	0.333333
vector	vector init	1.000000
list of column names	sql	0.005051
save	save	0.875000
randomly	split model	0.200000
condition to	condition	0.045455
currently implemented using parallelized pool adjacent violators algorithm	isotonic regression	0.090909
return a jvm scala map from a dict	frame jmap jm	0.111111
model's transform method	regression summary predictions	0.200000
all mixture	mllib gaussian mixture	0.045455
columns that describes the sort	sql data frame sort cols cols kwargs	0.142857
converts matrix	convert matrix	1.000000
all globals names read or written	core cloud pickler extract code globals	0.125000
the singular value decomposition	svd k computeu rcond	0.333333
jvm seq	seq sc cols	0.055556
parses	expr str	0.125000
array of the most recent [[streamingqueryprogress]] updates	recent progress	0.111111
number of columns of blocks in	num col blocks	1.000000
to configure	streaming	0.005025
tests whether this instance contains a	param paramname	0.111111
in this dstream	streaming dstream	0.083333
data or table	sql data frame	0.005348
object	object	0.138889
return its	core	0.003021
the year of a given	year	0.040000
a python topicandpartition to map	streaming topic and partition init topic partition	0.055556
decay	decay	1.000000
and count of the	core	0.003021
error	error	1.000000
value to a mllib vector if possible	param type converters to vector value	0.333333
forget about past terminated queries so that :func	sql streaming query manager reset terminated	0.200000
selector model	selector model	1.000000
random	mllib word2vec	0.125000
new :class column for distinct count of	count distinct	0.040000
value of	ml cross validator	0.166667
of this instance this	ml param	0.009524
use for loading	java mlreader	0.250000
evaluates the model on a test dataset	ml linear regression model evaluate dataset	1.000000
this obj assume that	object size obj	0.040000
a densevector with singular	singular	0.015625
an rdd comprised of vectors containing i i	random rdds gamma vector rdd	0.166667
__init__(self formula=none	ml rformula init formula	0.500000
merge the values for each key using	by key func numpartitions partitionfunc	0.066667
transforms a python parammap	java params transfer	0.125000
broadcast a read-only variable to	spark context broadcast	0.125000
:class dataframe	data frame writer	0.070423
min number of partitions for	min partitions	0.200000
__init__(self inputcol=none outputcol=none)	abs scaler init inputcol outputcol	1.000000
mark this rdd for	core rdd	0.003460
containing elements	core spark	0.010309
or	core status tracker get	0.500000
:py attr itemscol	items col value	1.000000
summary e g residuals mse r-squared of model	ml linear regression model summary	1.000000
finding frequent items for columns possibly	data frame freq items cols support	0.166667
current [[dataframe]] and perform the specified aggregation	pivot pivot_col values	0.050000
rawpredictioncol	rawpredictioncol	1.000000
dataframe as	sql data frame	0.005348
a vector or	vector	0.019231
for this stringindexer	ml string indexer	0.166667
dummy params instance used as	ml param params dummy	0.111111
0] for feature selection by percentile	mllib chi sq selector set percentile percentile	0.200000
this tests a	tests test	0.018519
can be used again to wait	streaming query manager reset	0.011905
matrix to the new mllib-local representation	matrix as	0.500000
group	external group	0.045455
value pairs or two separate arrays of	ml linalg	0.030303
libsvm format into an rdd of labeledpoint	mlutils load lib svmfile sc path	0.125000
feature selection by	chi sq selector	0.166667
perform a right outer join of	full outer join other numpartitions	0.111111
extract the minutes of	sql minute	0.050000
n elements from an rdd ordered	core rdd take ordered	0.050000
range of offsets from a single	range	0.030303
with scipy sparse matrices if scipy is available	sci	0.142857
from an rdd ordered in ascending order or	rdd take ordered	0.050000
internal use only create a new	sql hive context create	0.083333
original column	model original	0.062500
rdd an rdd of	mllib power iteration clustering train cls rdd	0.250000
again	sql streaming query	0.011765
set number	mllib streaming kmeans set	0.142857
find all globals names read or written	extract code globals	0.125000
duplicates	duplicates	1.000000
a dummy params instance used as a placeholder	ml param params dummy	0.111111
lda keeplastcheckpoint is set	ml distributed ldamodel get	0.066667
deserialized objects from the input stream	serializer load stream stream	0.333333
if false default do not catch assertionerrors	catch_assertions	0.166667
set number of batches after	streaming kmeans set	0.142857
that has no partitions or	core spark context empty	0.333333
returns the mean	mean	0.103448
that all the	object size	0.032258
of the dataframe in a	data frame	0.005000
write() save path	pipeline model save path	1.000000
decayfactor	decayfactor	0.714286
limits	limit	0.076923
windowing column	column over window	0.333333
function	function obj name	0.500000
stop the execution of the	context stop	0.125000
this thread until the group id	group groupid	0.142857
this obj assume that all the objects	core external merger object size obj	0.040000
can be	streaming	0.005025
can be used again	sql	0.002525
context	streaming context	0.055556
computes the levenshtein distance of	levenshtein left	0.058824
forget about past terminated queries	query manager reset terminated	0.200000
randomly generated uid and	cross validator	0.045455
evaluates the model on a test	ml linear regression model evaluate	1.000000
queries	sql streaming query manager	0.011905
new rdd	core rdd	0.003460
deviance for the fitted	ml generalized linear regression summary deviance	0.125000
elasticnetparam the elasticnet	elastic net	0.125000
compute the qr decomposition of this rowmatrix	mllib linalg row matrix tall skinny qr computeq	1.000000
parses a line in libsvm format into	mllib mlutils parse libsvm line line multiclass	0.111111
returns a new java	new java	0.166667
:py attr	value	0.871795
provides methods to	streaming	0.005025
which is a risk function corresponding to	regression summary	0.035714
:class dataframe as a temporary table	data frame as table df	0.333333
each rdd is generated by applying mappartitionswithindex()	map partitions with index f preservespartitioning	0.055556
note : experimental	bisecting kmeans summary	1.000000
rdd	rdd map	1.000000
memory for this obj assume that	size obj	0.040000
pipelinemodel create	pipeline model	0.071429
maxiter max number of iterations (>=	max iter	0.166667
memory	merger object size	0.032258
fillna	fillna	1.000000
queries so that :func	sql streaming query	0.011765
for binary or multiclass classification	numclasses categoricalfeaturesinfo	0.250000
params instances for the given	params	0.006623
convert to sparsematrix	ml linalg dense matrix to sparse	1.000000
the correlation	method	0.041667
adds an output option for	frame writer option key	1.000000
models that provide save() through their scala implementation	java saveable	0.333333
labels corresponding to	indexer model labels	0.166667
assumed to consist of key	key ascending numpartitions keyfunc	0.071429
an external database table via	url table mode	0.200000
partitions	partitions	0.466667
impurity="gini", numtrees=20	ml random forest classifier	0.046512
to an int if possible	ml param type converters to int	0.250000
parameters passed as a list of	core spark conf set all	0.125000
this instance with a randomly generated uid and	one vs rest model	0.058824
in the training set given the	distributed ldamodel training	0.034483
scale >= 0 or at integral part when	bround col	0.500000
stream query if this	stream	0.017544
rdd of labeledpoint	mllib mlutils load lib svmfile sc path numfeatures	0.125000
alias	alias	1.000000
labelcol label	label	0.071429
norm of a sparsevector	vector norm	0.055556
to a java pipelinemodel used for ml persistence	ml pipeline model to java	0.100000
calculates the approximate quantiles of numerical	approx quantile col probabilities relativeerror	0.166667
batch of jobs has started	batch started	0.333333
dependency on another module on a	module dependency on	0.142857
for the model	model	0.005587
sort the list based on	streaming test case sort result based on	0.333333
mean average precision map of all the queries	mllib ranking metrics mean average precision	1.000000
left singular	mllib linalg singular	0.017544
a right outer join of	rdd full outer join other	0.111111
elements in a numpy ndarray	linalg matrix to array	0.142857
applies standardization transformation on a	model transform	0.500000
queries so that :func	sql	0.002525
compute the number of	mllib linalg distributed matrix num	0.166667
retrieve gaussian distributions as a	ml gaussian mixture model gaussians df	0.166667
model intercept of binomial logistic regression	ml logistic regression model intercept	1.000000
of batches after which the centroids of	timeunit	0.025641
of active queries associated with this sqlcontext	manager active	0.066667
norm of a sparsevector	vector norm p	0.055556
of a dataframe as	sql data frame	0.005348
an rdd comprised	random rdds gamma vector rdd	0.166667
stratified sample without replacement based on the fraction	data frame sample	0.066667
a shortcut of write() save	ml mlwritable save	0.166667
param doc	doc	0.111111
with the keys of each tuple	keys	0.111111
elements to a forked external process	pipe command env checkcode	0.166667
driver as	local	0.038462
an input stream from an queue of rdds	queue stream rdds	1.000000
frame but not in another frame	sql data frame subtract other	0.333333
transforms the	transform	0.062500
first n rows	frame head n	1.000000
of products for all users the	products for users	0.250000
the content of the :class dataframe in	sql data frame writer	0.034884
base class for all test results	test result	0.200000
number of	block matrix num	0.100000
probabilitycol="probability", tol=0 01	probabilitycol	0.050000
weighted averaged f-measure	weighted fmeasure beta	1.000000
mixin for param standardization whether to standardize	has standardization	0.250000
internal use only create	hive context create	0.083333
train a decision tree model for	mllib decision tree train	0.333333
string column from one base to another	col frombase tobase	0.166667
of this	ml one vs rest model	0.111111
converts vector columns in an input dataframe to	convert vector columns from	0.166667
based on first	based on	0.111111
a local	spark context get local	0.333333
train a logistic regression model on the given	mllib logistic regression with sgd train cls	1.000000
the week number of a given date	weekofyear col	0.055556
an input stream that is to be used	stream ssc	0.090909
inherit documentation from its	mllib inherit doc cls	0.045455
are the right singular vectors of	mllib linalg singular	0.017544
kolmogorov-smirnov ks test for data sampled from a	stat statistics kolmogorov smirnov test data distname	0.111111
computes column-wise summary statistics	stat statistics col	0.200000
error which is defined as the	mllib regression	0.022727
underlying output data source	data stream writer format source	0.333333
calculates the norm of	linalg sparse vector norm p	0.066667
cluster centers represented as a	mllib kmeans model cluster centers	0.083333
as the specified table	as table name	1.000000
:class windowspec with the frame boundaries defined	sql window range	0.166667
half the weightage	half life halflife	0.166667
in the ensemble	tree ensemble	0.111111
a term to this	term	0.040000
the residual degrees of freedom for the null	regression summary residual degree of freedom null	0.333333
transform the rdd of document	transform document	0.250000
and profiles	core basic	0.066667
train the model	mllib streaming kmeans train	1.000000
table accessible via jdbc url url and connection	reader jdbc url table	0.166667
'with sparkcontext as	spark context	0.023256
vectors or transform the	hashing tf transform	0.045455
__init__(self	lda init	1.000000
represents a range of	offset range	0.047619
add a py or zip dependency for all	core spark context add py file path	0.166667
given spark runtime configuration property	runtime config	0.333333
squared error which is defined	mllib regression	0.022727
the accumulator's data type returning a	accumulator	0.012987
specifies the input schema	reader schema schema	0.333333
test that the final value of weights is	test	0.015152
matrix to a coordinatematrix	mllib linalg block matrix to coordinate matrix	0.333333
convert matrix	ml linalg matrix convert	0.166667
perform a left outer join of c{self}	left outer join other	0.111111
containing i i d samples drawn	numrows numcols numpartitions	0.125000
an rdd containing	rdd	0.003058
the specified table	table	0.031250
so that	reset	0.011236
:class dataframe with	sql data frame	0.005348
is set to a different value or	core spark context set	0.166667
representation of each word in vocabulary	word2vec fit data	0.200000
'x' to all mixture components	mllib gaussian mixture model predict soft x	0.142857
the observed data against the expected distribution	observed expected	0.166667
set the initial value	regression with sgd set initial	0.111111
first n rows to the console	show n truncate	0.333333
and profiles the method to_profile	core basic	0.066667
generates python code for	code name doc defaultvaluestr	0.111111
make predictions on batches of data	linear algorithm predict on	0.066667
the rdd	rdd partition	0.062500
returns the soundex encoding	soundex	0.043478
a large dataset and an item approximately find	ml lshmodel approx nearest neighbors dataset	0.166667
saves the content of the dataframe in a	data frame writer	0.014085
unified dstream from multiple dstreams of the	streaming streaming context union	0.111111
parallelize	parallelize	1.000000
returns all the	sql	0.002525
a model to be used for later scaling	standard scaler fit dataset	0.250000
of estimated	ml linear regression summary	0.285714
:class structtype	struct	0.166667
in similarity to "word"	synonyms word	0.166667
integral	integral	1.000000
test that the final value of weights	mllib streaming logistic regression with sgdtests test	0.111111
finding frequent items for	sql data frame freq items cols	0.166667
numslices	numslices	0.833333
gives the predicted	summary prediction col	1.000000
maxiter max number of iterations	max iter	0.166667
set a local property that	context set local property key value	0.200000
tree parameters	tree params	1.000000
storage type for	type cls	0.250000
get the root directory that contains	get root directory cls	0.333333
category	mllib multiclass	0.428571
norm of	vector norm p	0.055556
this dstream with the default storage level (c{memory_only})	streaming dstream cache	1.000000
that :func awaitanytermination() can be used again	sql	0.002525
from start	range between start	0.200000
are the right singular vectors of	linalg singular	0.017544
columns are the right singular vectors of the	mllib linalg singular	0.017544
the initial value of	set initial	0.111111
of training	ml generalized linear regression training summary	0.166667
finding frequent items for columns possibly	data frame freq items	0.166667
for new terminations	query manager reset	0.011905
to n	n	0.027778
densematrix	linalg matrices dense numrows numcols values	0.500000
decorator that makes a class inherit documentation from	inherit doc	0.045455
a given string	param	0.012500
specified partitioner	partition by numpartitions partitionfunc	0.250000
a param with a given string	has param	0.019231
topicdistributioncol or	topic distribution col	0.250000
number of rows	linalg row matrix num rows	0.200000
much of	external	0.013889
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 tol=1e-6 seed=none	init featurescol labelcol predictioncol maxiter	0.333333
sets the spark session to	session sparksession	0.083333
squared distance from a sparsevector or	linalg sparse vector squared distance other	0.166667
to a boolean	to boolean value	0.250000
python topicandpartition to	topic partition	0.055556
optional default value and	ml param params	0.013699
sets the given parameters in	param grid builder	0.055556
which is a dataframe	regression summary	0.035714
wait	streaming query manager reset	0.011905
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none	ml random forest classifier	0.023256
param with a	params has	0.019231
converts vector columns in	mllib mlutils convert vector columns	0.166667
creates an external table	sqlcontext create external table tablename	1.000000
maximum	max	0.071429
kolmogorov-smirnov ks test for data	mllib stat statistics kolmogorov smirnov test data distname	0.111111
current [[dataframe]] and perform	grouped data pivot pivot_col values	0.050000
every feature	linear	0.025641
nlargest	nlargest	1.000000
companion	params transfer	0.125000
memory	external merger	0.031250
get or compute the number	row matrix num	0.100000
python direct kafka stream messagehandler	kafka direct stream message handler	0.500000
columns that describes the sort order	data frame sort cols cols kwargs	0.142857
first date which is later than the value	dayofweek	0.037037
a python parammap into a	map to	0.125000
resets the configuration property for the given key	runtime config unset key	1.000000
of memory for this	external	0.013889
history	history	0.454545
sets the sql	sqlcontext	0.153846
text format or newline-delimited json	json path mode	0.125000
computes the area under	area under	0.166667
of	object size	0.032258
instance's params to the wrapped java	java params to	0.045455
a line in libsvm format into label indices	mlutils parse libsvm line line	0.111111
a new	wrapper new	0.333333
test that the model params are set correctly	kmeans test test model params	0.250000
a dict into a jvm map	scala map sc jm	0.200000
return	train rdd	0.166667
into disks	spill	0.038462
returns a java storagelevel based on	context get java	0.333333
save this model to	kmeans model save	0.500000
values	mllib	0.010526
two-sided p-value of estimated coefficients	ml linear regression summary p values	0.333333
this matrix to the	linalg dense matrix	0.083333
deviance for the fitted model	linear regression summary deviance	0.125000
set a local	context set local	1.000000
perform a right outer join of c{self}	rdd full outer join	0.111111
of this instance with a randomly generated	ml one vs rest	0.052632
output a python rdd of key-value pairs (of	core rdd save	0.037975
a java array of	ml java wrapper new java array	0.333333
model fitted by :py class stringindexer	string indexer model	1.000000
the dispatch to handle all function	core cloud pickler save function	0.142857
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto",	ml random forest classifier	0.023256
convert a list of column	sql to	0.041667
a model	linear regression	0.040000
write() save path	mlwritable save path	1.000000
line in libsvm format into label indices values	mllib mlutils parse libsvm line line multiclass	0.111111
tests whether this instance	params has param paramname	0.142857
of the :class dataframe in json format (json	sql data frame writer	0.011628
use only create	context create	0.083333
stageid	stageid	1.000000
has completed	completed	0.117647
extracts the embedded default param values and	params extract param map	0.333333
of indices back to a new column	index to	0.040000
training	linear regression training summary	0.500000
given value to scale	sql	0.002525
initial value of weights	set initial weights initialweights	0.333333
are the right singular vectors of the	linalg singular	0.017544
bound on the log	log	0.071429
orc files returning the result as	orc	0.083333
which should be smaller than or equal to	numiterations	0.050000
new accumulator	accumulator init	0.083333
basic operation test for dstream mapvalues	basic operation tests test	0.111111
setparams(self withmean=false withstd=true inputcol=none outputcol=none) sets params	set params withmean withstd inputcol outputcol	0.500000
python rdd	core rdd save as	0.037500
attr lda keeplastcheckpoint is set	ml distributed ldamodel	0.050000
or its	col	0.016393
smoothing=1 0 modeltype="multinomial", thresholds=none weightcol=none)	naive bayes	0.142857
this instance	params has	0.019231
determination	mllib regression metrics r2	0.166667
pearson's chi-squared goodness	mllib stat statistics chi sq	0.066667
whose columns are the left singular	singular	0.015625
given string by given separator but	split s separator	0.333333
tree (e g depth 0 means 1 leaf	mllib decision tree model	0.076923
stream query if this	sql data stream	0.031250
queries so	sql streaming query	0.011765
the number	indexed row matrix num	0.100000
elements with matching keys in c{self} and c{other}	join	0.034483
latest	latest	1.000000
a 'new api' hadoop	spark context new apihadoop	0.333333
train the model	mllib streaming linear regression with sgd train	1.000000
indices to select filter	selector model selected features	0.333333
initial value of	streaming logistic regression with sgd set initial	0.111111
an rdd comprised of vectors containing	mllib random rdds normal vector rdd	0.166667
that particular batch has half	half	0.058824
>= 0 or at integral part when	bround col	0.500000
of this instance this updates both	ml param params reset	0.166667
predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1	predictioncol probabilitycol	0.111111
decayfactor timeunit to configure	streaming	0.005025
from	load	0.111111
converts matrix columns in	mllib mlutils convert matrix columns to	0.166667
disks	core external group by	0.045455
create a python topicandpartition to map	streaming topic and partition init topic partition	0.055556
rdd is checkpointed and materialized either	rdd is checkpointed	0.166667
setparams(self featurescol="features", predictioncol="prediction", k=2	mixture set params featurescol predictioncol k	1.000000
saving	mlwriter	0.062500
degree	degree	1.000000
from	from	0.363636
deviance for the fitted	summary deviance	0.125000
comprised	random rdds normal vector	0.125000
a param with a given string	ml param params	0.013699
key and value	core	0.006042
non-streaming	write	0.071429
list of column names skipping	sql	0.005051
instance to a java pipelinemodel used	pipeline model to java	0.100000
contains the count	count	0.016949
queries so that :func	streaming query manager	0.011236
on a model with weights	streaming linear regression with	0.111111
the given parameters in this grid to fixed	ml param grid builder add grid param	0.250000
len with	len	0.071429
of memory	external merger object size	0.032258
a model from the input java	ml java estimator	0.200000
an associative and commutative reduce function	core rdd reduce by	0.125000
the dot product of two vectors	ml linalg dense vector dot other	0.090909
documentation of	ml	0.001835
comprised of vectors containing i i	random rdds	0.076923
specifies the input schema	frame reader schema schema	0.333333
a new accumulator with a	accumulator init	0.083333
that param	param	0.006250
points using the model trained	mllib tree ensemble model	0.058824
amount of time for a condition	condition	0.045455
receiver operating characteristic roc curve which is a	binary logistic regression summary roc	0.166667
runs and profiles	core	0.003021
changes the uid	reset uid	0.333333
executors if the	unpersist blocking	0.166667
sparkcontext which	streaming context spark	0.083333
and returns	sql	0.007576
column	model	0.005587
runs and profiles the method to_profile passed	core	0.003021
how much of memory for	size	0.009174
setparams(self mindocfreq=0 inputcol=none outputcol=none) sets params	set params mindocfreq inputcol outputcol	0.333333
only create a new hivecontext for	context create for	0.250000
statistic	stat	0.153846
a converter to drop	converter datatype	0.071429
a line in libsvm format into	mllib mlutils parse libsvm line line	0.111111
rdd by applying a function to each element	map f preservespartitioning	0.100000
items by creator	merger merge values iterator	0.333333
loads a csv file	reader csv path schema	0.666667
the objects	core	0.003021
the content of the dataframe in a	sql data frame writer	0.011628
the master	master	0.100000
dump	by spill	0.047619
final value of weights is close to	parameter accuracy	0.029412
original	ml min max scaler model original	0.062500
of possible outcomes for k classes classification	classes	0.034483
dense vector of 64-bit floats from a python	ml linalg vectors dense	0.166667
as a :class dataframe	data frame	0.015000
initmode	init mode	1.000000
dump	core external group by spill	0.047619
train a gradient-boosted trees model	mllib gradient boosted trees train	0.333333
setparams(self inputcol=none outputcol=none handleinvalid="error") sets params	set params inputcol outputcol handleinvalid	0.333333
obj assume that all the	external merger object size obj	0.040000
optional default value and user-supplied	ml param params	0.013699
transforms	params transfer param	0.250000
initial value of weights	sgd set initial weights	0.333333
evaluates the model on a test	generalized linear regression model evaluate	1.000000
the input data	data stream reader	0.200000
value to a mllib vector	to vector value	0.250000
to convert the java_model to a python	java_model	0.090909
list of conditions and returns	sql column otherwise value	0.050000
variance and count of the	core	0.003021
of indices back	index	0.041667
stratified sample without replacement based on	frame sample	0.066667
a paired rdd where the first element	matrix factorization model	0.043478
place	place	1.000000
used again to wait for new	sql streaming	0.010204
column expression representing a user defined function udf	udf f returntype	0.200000
infer schema from an rdd of	spark session infer schema rdd	0.250000
temporary table in the catalog	table df tablename	0.083333
minsupport=0 3 minconfidence=0 8 itemscol="items", predictioncol="prediction", numpartitions=none)	minsupport minconfidence itemscol predictioncol	1.000000
join of	join	0.068966
finding frequent items	frame freq items cols	0.166667
new java object	java wrapper new java obj java_class	0.333333
the training set given the	ldamodel training	0.034483
python function including lambda function	function name f	0.166667
classes	classes	0.172414
of	core	0.009063
have the same param	m2 param	0.125000
finding frequent items for columns	frame freq items cols support	0.166667
create	tests create	0.500000
the dot product	dot other	0.100000
rdd of key-value pairs (of	rdd save as	0.038462
this accumulator's	accumulator	0.012987
parquet	writer parquet	0.500000
load labeled points saved	mlutils load labeled points sc path minpartitions	0.250000
each original column during	min max scaler model original	0.062500
rdd 'x' has maximum membership in this model	mllib gaussian mixture model predict x	0.500000
this :class dataframe	data frame	0.015000
inputformat with arbitrary key and	inputformatclass	0.095238
perform a pearson's independence test using dataset	chi square test test dataset	0.333333
applying a function on rdds of the dstreams	context transform dstreams transformfunc	0.125000
a line in libsvm format	libsvm line line multiclass	0.333333
on the driver	on driver	0.333333
at pos in	pos	0.022222
the accumulator's value only usable in driver	accumulator value value	0.050000
:class dataframe to a data	sql data frame writer	0.011628
the week number of a given date as	weekofyear	0.043478
users for a	users	0.066667
current [[dataframe]] and perform	sql grouped data pivot pivot_col values	0.050000
can be used again to	reset	0.011236
train the model on	streaming logistic regression with sgd train on	0.333333
return a python wrapper of	ml	0.007339
identifier for the spark application	spark context application	1.000000
two block matrices together the matrices must have	mllib linalg block matrix add other	1.000000
text	writer text	0.333333
create a	create	0.051724
deviance for the null	null deviance	0.250000
note : developerapi	profiler	0.090909
build the union	context union	0.333333
that makes a class inherit documentation from	mllib inherit	0.045455
comprised of vectors containing	mllib random rdds normal vector	0.125000
between	v1 v2	1.000000
back to a new column	index to	0.040000
return sparkcontext which is associated	streaming streaming context spark	0.083333
tests whether this instance	param params has param paramname	0.142857
an rdd comprised of vectors	random rdds gamma vector rdd	0.166667
so	query	0.010753
a sparse vector using either a dictionary	vectors sparse size	0.166667
densevector with singular	mllib linalg singular	0.017544
number of rows	mllib linalg block matrix num rows	0.200000
gets a	params get	0.333333
again to wait for new terminations	query manager	0.011905
the residual degrees of freedom for	summary residual degree of freedom	0.125000
returns weighted true	mllib multiclass metrics weighted true	1.000000
list of terms to term frequency vectors or	tf	0.076923
day of the month of	sql dayofmonth	0.031250
to wait	query manager	0.011905
replace	replace	0.714286
list of active queries associated with this sqlcontext	streaming query manager active	0.066667
can be used	manager reset	0.011905
date column	next day date	0.100000
calculates the correlation of two columns of	method	0.041667
center for this model	model compute	0.133333
get the n elements from an rdd ordered	rdd take ordered num	1.000000
dataframe that	data frame to	0.250000
given path a shortcut of write() save path	ml pipeline model save path	0.200000
indices to select filter	chi sq selector model selected features	0.333333
deviance for the null	linear regression summary null deviance	0.250000
for the stream query if this is not	stream	0.017544
a neutral	zerovalue	0.076923
this params	param params	0.014925
transfer this instance to a java	to java	0.090909
fpgrowth	fpgrowth	0.714286
of vectors containing i i d samples drawn	std numrows	0.125000
a l{statcounter} object that captures	core rdd stats	0.083333
vector of 64-bit floats from a python list	ml linalg vectors	0.250000
from an rdd ordered	core rdd take ordered	0.050000
predict the label of	predict	0.034483
the spark session to	session sparksession	0.083333
float data type representing single precision floats	float type	1.000000
:func awaitanytermination() can be used	query manager reset	0.011905
ml instance	ml mlreader	0.111111
profile	profile	1.000000
parameters in this grid to	param grid builder base on	0.076923
of vectors containing i i d samples drawn	numrows numcols	0.125000
sets the given parameters in this grid to	ml param grid builder add grid	0.100000
indices	indices	1.000000
of iterations default	iterations	0.043478
vocab	vocab	1.000000
into disks	core	0.003021
create an input stream that pulls events from	create stream ssc hostname	0.200000
a :py attr countvectorizermodel	count vectorizer	0.166667
a :class dataframe representing the result	sqlquery	0.027027
extracts the embedded default param values and	param params extract param map	0.333333
convert matrix	mllib linalg matrix convert	0.166667
containing elements from start	core spark context range start	0.090909
returning the result	reader	0.040000
such as the spark fair	spark context	0.023256
the attempt numbers are correctly reported	core task context tests test attempt number	0.333333
set bandwidth of	set bandwidth bandwidth	0.142857
train a gradient-boosted trees model	mllib gradient boosted trees train regressor cls	0.333333
time of day in the given timezone returns	sql	0.002525
profile object is returned	basic profiler profile func	0.200000
value of	ml sqltransformer	0.250000
computes an fp-growth model that contains	mllib fpgrowth train cls data	0.100000
param with	param	0.012500
awaitanytermination() can be used again to wait for	sql streaming query manager reset	0.011905
kolmogorov-smirnov	kolmogorov smirnov	1.000000
:func awaitanytermination() can be used again to	query manager reset	0.011905
into disks	external group by spill	0.047619
column names skipping null	sql	0.005051
iterable this is used because the standard	iterable	0.125000
a streaming dataframe/dataset is written to	writer output mode outputmode	0.083333
the training set given the current	distributed ldamodel training	0.034483
'x' to all mixture	gaussian mixture model	0.052632
resulting rdd that contains a	rdd cogroup other numpartitions	0.066667
a :class dataframe representing the result of	sqlquery	0.027027
within	within	1.000000
improves on toy data with no	with sgdtests	0.200000
for this obj assume that all	object size obj	0.040000
sql storage type	sql type	0.250000
decode the unicode as utf-8	streaming utf8 decoder	1.000000
to	streaming query manager	0.011236
functions and a	core rdd	0.003460
representing the result of the	sqlquery	0.027027
gbtregression	gbtregression	0.714286
for loading	ml java mlreader	0.200000
lines text format or newline-delimited json <http //jsonlines	json path mode compression dateformat	0.166667
find	mllib word2vec model find	1.000000
to wait for new terminations	manager	0.011236
to wait	sql streaming query	0.011765
predicts	predict	0.034483
defined from start	between start	0.100000
loads a class generated by namedtuple	load namedtuple name fields	0.333333
that all the objects	core external	0.016129
[[pcamodel]] that contains the principal components of the	mllib pca fit data	0.166667
applies transformation on a	transformer transform	0.333333
:py attr implicitprefs	implicit prefs value	1.000000
of the :class dataframe as the specified table	sql data frame writer save as table	0.333333
starts at pos	pos	0.022222
returns an mlwriter instance for this ml	ml java mlwritable write	0.200000
the content of the	writer	0.040000
again to wait for new	streaming query	0.010526
be used again	sql streaming	0.010204
this instance contains a param with a	param	0.012500
minutes of a given date as	minute	0.040000
f1-measure	f1measure	0.166667
column standard deviation values	standard scaler model std	0.166667
that with new specified column names	to df	0.250000
default min number of partitions for hadoop rdds	default min partitions	0.250000
for every	linear model	0.066667
contents of the :class dataframe to a data	data frame writer save path	0.142857
save this model to the given	model save sc	1.000000
operation test for dstream groupbykey	operation tests test group	1.000000
converts matrix columns in an input dataframe from	mllib mlutils convert matrix columns to ml	0.166667
__init__(self mintokenlength=1 gaps=true pattern="\s+", inputcol=none outputcol=none tolowercase=true)	ml regex tokenizer init mintokenlength gaps pattern inputcol	1.000000
finding frequent items for columns	frame freq items	0.166667
of type	type	0.024390
stop the	streaming streaming context stop	0.125000
the name of the file to which	file	0.028571
infer schema from an rdd of	sql spark session infer schema rdd	0.250000
make predictions on a keyed	mllib streaming kmeans predict on values	1.000000
computes column-wise summary statistics for the input	stat statistics col	0.200000
first	first	1.000000
train the model on the incoming	train on	0.333333
return the number of fields	sql struct type len	0.200000
of labeledpoint	lib svmfile sc path numfeatures minpartitions	0.125000
with a randomly generated	train validation split	0.166667
the heap maintaining the heap	heap	0.047619
perform a left outer join of c{self}	rdd left outer join	0.111111
randomforestclassifier	random forest classification	1.000000
the model params are	model params	0.125000
conditions and returns one of multiple possible	sql column otherwise	0.050000
much of memory for this	external	0.013889
thread until the group id	group groupid	0.142857
the spark sink deployed on	ssc addresses storagelevel maxbatchsize	0.045455
labeled points	labeled points sc path	1.000000
this rdd and	core rdd	0.006920
levenshtein distance of the two given	levenshtein left right	0.058824
instance	has param	0.019231
week number of a given date as	weekofyear	0.043478
g depth 0 means 1 leaf	decision	0.052632
train the model on the	streaming linear regression with sgd train on	0.333333
squared error which is defined as the	regression	0.010000
test the python direct kafka rdd api with	stream tests test kafka rdd with	1.000000
__init__(self inverse=false inputcol=none outputcol=none)	ml dct init inverse inputcol outputcol	1.000000
table from	table tablename	0.083333
parameters in this grid	grid builder base	0.076923
a gradient-boosted	mllib gradient boosted	1.000000
in :py attr predictions	linear	0.025641
methods to set k decayfactor timeunit to configure	streaming	0.005025
parameters in this grid to	grid builder	0.055556
accumulator's	accumulator add	0.076923
them with extra values from	extra	0.023810
seed=none impurity="variance")	ml gbtregressor	0.333333
test	task context tests test	0.500000
norm of a sparsevector	linalg sparse vector norm	0.066667
for this obj assume that all the	obj	0.023810
contains a param with a given	param params	0.014925
test the python direct kafka stream api with	stream tests test kafka direct stream from	0.333333
to wait for	sql	0.002525
indexed	indexed	0.714286
specifies how data	data	0.011628
set number of batches after which	mllib streaming kmeans set	0.142857
return an javardd of object by unpickling	ml	0.001835
current idf vector	mllib idfmodel idf	0.333333
and profiles the method to_profile passed in a	core basic	0.066667
set initial centers should be set	kmeans set initial centers centers	0.200000
pass each value in the key-value	map	0.058824
transforms a python parammap into a	params transfer param map to	0.500000
the column mean values	mllib standard scaler model mean	0.125000
temporary table in the catalog	table df	0.083333
comprised of vectors containing i i d samples	mllib random rdds gamma vector	0.125000
data of	sql data stream	0.031250
per stage basis	profiler collector	0.142857
residual degrees of freedom for the	generalized linear regression summary residual degree of freedom	0.125000
or multiclass classification	classifier cls data numclasses categoricalfeaturesinfo	0.250000
reduced into	shuffle	0.166667
data source	sql data	0.024390
a multi-dimensional cube for the current :class dataframe	frame cube	0.055556
with singular	mllib linalg singular	0.017544
create a python topicandpartition to	topic partition	0.055556
the selector type of the chisqselector	selector type selectortype	0.333333
the md5 digest and	md5 col	0.333333
train the model on	sgd train on	0.333333
area under the	metrics area under	0.166667
containing a json string into a [[structtype]]	sql from json	0.166667
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini",	classifier	0.050000
class inherit documentation from its parents	inherit doc cls	0.045455
ml instance the	mlreader	0.037037
pearson correlation coefficient for col1 and	corr col1	0.333333
a column containing a json string into	json col	0.083333
contains a param with a	param params	0.014925
naive bayes classifiers	naive bayes	0.285714
of time for a condition	condition	0.045455
large dataset and an item approximately find at	lshmodel approx nearest neighbors dataset key numnearestneighbors distcol	0.166667
applying a function to	f	0.031579
dayofweek	dayofweek	0.185185
of day in utc	from utc	0.125000
__init__(self scalingvec=none	ml elementwise product init scalingvec	1.000000
broker	broker	0.600000
start position zero based	pos	0.022222
returns the date	date	0.037037
create a python topicandpartition to	init topic partition	0.055556
__init__(self formula=none	rformula init formula	0.500000
with an alias set	alias alias	0.333333
comprised of vectors containing i i d	mllib random rdds poisson	0.125000
setparams(self featurescol="features", labelcol="label",	set params featurescol labelcol	0.750000
predict the label of one or more examples	mllib decision tree model predict	1.000000
replacing a value with	replace to_replace	0.200000
columns for the given	columns	0.019608
create an input stream	create stream ssc hostname	0.200000
0] for feature selection by	mllib chi sq selector set	0.150000
for setting the spark context call site	sccall site sync	0.200000
on the incoming	on	0.111111
return as an dict	as dict recursive	1.000000
memory for this obj assume that	obj	0.023810
this model to transform input data	model	0.005587
and	streaming	0.005025
saved using rdd saveastextfile	path minpartitions	0.250000
this instance to a java onevsrest used for	ml one vs rest to java	0.166667
returns the greatest value of	sql greatest	0.055556
the receiver operating characteristic roc	roc	0.200000
relativesd	relativesd	0.833333
dot product of two	linalg dense vector dot other	0.058824
week number of a given	weekofyear col	0.055556
clusters	clustering	0.066667
dataframe representing	sqlquery	0.054054
be used with the spark sink deployed on	ssc addresses storagelevel maxbatchsize	0.045455
into disks	external group	0.045455
categoricalfeaturesinfo	categoricalfeaturesinfo	0.833333
the given path a shortcut of	ml	0.007339
coefficient of determination	regression metrics r2	0.166667
transform the rdd of document to	transform document	0.250000
can be used in sql	sql sqlcontext	0.095238
perform a left outer join	rdd left outer join	0.111111
test the stage ids are available	tests test stage id	1.000000
of vectors which this transforms	ml vector indexer	0.200000
wait until any of the queries	any termination timeout	0.166667
a java udf so	java	0.012195
of terms	ml ldamodel	0.111111
:py attr countvectorizermodel	count vectorizer	0.166667
property if not already set	set if missing key value	1.000000
column containing a json string into a	json col	0.083333
load a ridgeregressionmode	mllib ridge regression model load cls sc	1.000000
fast version of a	core heappushpop heap item	0.142857
of the current [[dataframe]] and perform the	pivot pivot_col values	0.050000
gets	param params get	0.500000
none by default	keyconverter	0.166667
of this instance with a randomly	ml one vs	0.142857
queries so that :func awaitanytermination() can be used	manager	0.011236
merge the values for each key	by key func numpartitions partitionfunc	0.066667
in	sql has	1.000000
get all values	get all	0.166667
the given path a shortcut of write()	ml mlwritable	0.142857
to all the jobs started by this thread	job	0.023810
classification	classification	0.500000
random forest model for classification	random forest	0.041667
for approximate distinct count of col	approx count distinct col rsd	0.066667
awaitanytermination() can be used again	streaming query	0.010526
as at text file using string representation	as text files prefix suffix	0.250000
predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0	predictioncol probabilitycol	0.333333
the given parameters in this grid	grid builder	0.055556
a class inherit documentation from its parents	inherit doc cls	0.045455
prints the first n rows to the console	frame show n truncate vertical	0.333333
an	cls	0.142857
a user defined function udf	udf f returntype	0.200000
decision tree models	decision tree model	0.050000
right outer join of	rdd full outer join	0.111111
set the selector type of the chisqselector	chi sq selector set selector type selectortype	0.333333
indexing categorical feature columns in a dataset	indexer	0.055556
a multi-dimensional cube for the current	data frame cube	0.055556
a local temporary view	temp view name	0.111111
sets the context	streaming context	0.055556
area under the receiver operating characteristic roc	classification metrics area under roc	0.500000
checkpoint data or	context get or	0.200000
aggregate the values	aggregate	0.111111
vector to the	sparse vector	0.062500
format into an rdd of labeledpoint	mlutils load lib svmfile	0.125000
keyfunc	keyfunc	1.000000
test that the model params	streaming kmeans test test model params	0.250000
the month of a given date as integer	sql dayofmonth col	0.031250
of the test method	stat chi sq test result method	0.250000
labeledpoint	load lib svmfile sc	0.125000
this vector to the	sparse vector	0.062500
tests whether	param params has param paramname	0.142857
9 0 95 0 99], quantilescol=none aggregationdepth=2)	fitintercept	0.058824
the column standard deviation values	standard scaler model std	0.166667
numfeatures	num features	0.333333
labeled points saved using	labeled points sc path minpartitions	0.250000
parses	sql expr str	0.125000
mean variance and count of the rdd's	core	0.003021
an rdd created by piping elements	rdd	0.003058
until any of	sql streaming query manager await any termination	0.142857
spark sink deployed	ssc addresses storagelevel maxbatchsize	0.045455
new	wrapper new	0.333333
column of corresponding string	string	0.041667
format (json lines text format or newline-delimited json	writer json path	0.125000
distance from a sparsevector	distance other	0.133333
used in sql statements	f returntype	0.125000
cost sum of	compute cost	0.142857
the stream query if this is not	data stream	0.028571
the :class dataframe to a data	sql data frame writer	0.011628
compare 2 ml params	compare params m1 m2	0.200000
accumulator's value only usable	core accumulator value	0.045455
returns micro-averaged label-based	multilabel metrics micro	1.000000
"func" and a	core	0.003021
generates python code	code name	0.111111
setparams(self p=2 0 inputcol=none outputcol=none)	ml normalizer set params p inputcol outputcol	1.000000
be used	manager	0.011236
load a model from	power iteration clustering model load cls sc	1.000000
all trees in the ensemble	mllib tree ensemble model	0.058824
load a ridgeregressionmode	mllib ridge regression model load cls sc path	1.000000
rdd as	rdd	0.003058
for new terminations	streaming query manager reset	0.011905
which is a risk function corresponding	mllib regression metrics	0.090909
id to all the jobs started	job	0.023810
save this model to the given	kmeans model save sc	1.000000
that	object size	0.032258
an input stream that pulls	stream ssc	0.090909
inputcol=none outputcol=none labels=none)	inputcol outputcol labels	1.000000
for every	mllib linear model	0.125000
thread such as the spark fair scheduler pool	spark	0.013158
call in the current call	call	0.142857
model	matrix factorization model	0.043478
setparams(self estimator=none estimatorparammaps=none evaluator=none trainratio=0 75	set params estimator estimatorparammaps evaluator trainratio	0.500000
that :func awaitanytermination() can be	sql streaming query manager	0.011905
over all trees in the ensemble	ensemble	0.100000
column of the current [[dataframe]] and perform	sql grouped data pivot pivot_col values	0.050000
parameters in this grid to	ml param grid builder	0.055556
a python object into an internal sql object	internal obj	0.500000
sets default params	ml param params set default	1.000000
tests whether this instance contains a	paramname	0.076923
the sql context to use for loading	ml java mlreader context sqlcontext	0.333333
into a python parammap	param map from	0.250000
a given	params	0.006623
the greatest value of	greatest	0.043478
the :class dataframe to a data source	sql data frame writer save path format	0.142857
containing i i d samples drawn	std numrows	0.125000
from a	offset	0.021739
accumulator's	core accumulator add	0.076923
the dataframe	sql data frame	0.005348
mixture	gaussian mixture model k	0.200000
dataframe	sql data frame writer	0.011628
that all	core external merger	0.032258
a shortcut of write()	ml pipeline	0.095238
pipelinemodel	pipeline model	0.071429
dummy params instance used as a placeholder	param params dummy	0.111111
until any of the queries on the associated	await any	0.142857
values	mllib linalg dense vector values	0.200000
lag	lag	1.000000
get total number	model total num	0.333333
is currently cached in-memory	catalog is cached	1.000000
named table accessible via jdbc url url and	reader jdbc url table	0.166667
configure the kmeans algorithm for fitting and	kmeans	0.025641
external database table via jdbc	jdbc url table mode properties	0.200000
waits for the termination of this query either	termination timeout	0.041667
pos in byte and is of length len	pos len	1.000000
in this	builder add	0.200000
for the termination of this query either	termination timeout	0.041667
to a mllib vector	to vector	0.250000
java parammap into a	java	0.012195
queries	sql streaming query	0.011765
a class inherit documentation from its parents	mllib inherit doc cls	0.045455
set the initial	initial	0.071429
a temporary table in the catalog	table df tablename	0.083333
matrix	linalg dense matrix	0.083333
windowduration	windowduration	1.000000
rows	count	0.016949
perform a left outer join of	left outer join other	0.111111
train the model on the incoming	mllib streaming kmeans train on	0.333333
returns an mlreader instance for this class	pipeline model read cls	1.000000
of write()	ml mlwritable	0.142857
a column containing a json string	sql from json col	0.083333
estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none): sets params for	set params estimator estimatorparammaps evaluator numfolds	0.333333
the dot product of two vectors we	linalg dense vector dot	0.058824
submit and test a single	spark submit tests test single	1.000000
a column of indices back to a	index to	0.040000
find all globals names read or written	cloud pickler extract code globals	0.125000
kmeans algorithm for fitting and predicting on incoming	kmeans	0.025641
returns true positive rate	metrics true positive rate	0.250000
test that the final value of weights is	logistic regression with sgdtests test	0.111111
value to a boolean	to boolean	0.250000
source format	format source	0.333333
elements in rdds in a sliding window	value and window windowduration slideduration numpartitions	0.076923
that	manager	0.011236
how much of	external merger	0.031250
matrix columns in	matrix columns from ml	0.142857
column scipy matrix from a dictionary of values	sci py tests scipy matrix size values	1.000000
test this should be	test	0.015152
receiver operating characteristic roc curve which is a	ml binary logistic regression summary roc	0.166667
index value pairs or two separate arrays of	ml linalg	0.030303
a batch of jobs	batch	0.068966
creates a copy of this instance	ml pipeline model copy extra	0.333333
:func	sql streaming query manager	0.011905
external merger will dump	external merger	0.031250
left outer join of c{self}	left outer join other numpartitions	0.111111
"zerovalue" which may	fold	0.076923
columns in	columns to	0.125000
python rdd of key-value pairs (of form	core rdd	0.010381
merge the values for each key using an	key func numpartitions partitionfunc	0.066667
most recent [[streamingqueryprogress]] updates for	recent progress	0.111111
instance's params	params	0.006623
an rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures minpartitions	0.125000
instance contains a param with a given	param params	0.014925
converts a labeledpoint to a string in	mllib mlutils convert labeled point to	0.250000
stream query if this is not set	sql data stream	0.031250
schema	schema schema	1.000000
spark fair	spark	0.013158
for	merger object	0.032258
:class datatype the data type string format equals	datatype string	0.111111
frequency vectors or transform the rdd	mllib hashing tf transform	0.045455
in "predictions" which gives the true label of	ml logistic regression summary label col	0.333333
a	ml param params	0.027397
transforms the	transformer transform	0.166667
local property set in this thread	spark context get local property key	0.066667
date datetime date data type	date type	1.000000
user and	user	0.055556
item by key out of a	item key	0.250000
checkpoint data or	get or	0.200000
a right outer join of	full outer join	0.111111
trained using multinomial/binary logistic	logistic	0.062500
format like '#,--#,--# --', rounded to	format number col	0.500000
pearson's chi-squared goodness of fit	stat statistics chi sq	0.066667
for a given product and returns a	product	0.029412
trigger for the stream query if this	sql data stream writer trigger	0.083333
accessible via jdbc url url and connection	reader jdbc url	0.250000
used again	streaming query	0.010526
streamingcontext from checkpoint data or create	get or create	0.111111
the accumulator's data	accumulator param	0.038462
converts matrix columns	convert matrix columns from ml	0.166667
the given parameters in this grid to fixed	param grid builder base	0.076923
mixture	mixture	0.368421
dump already partitioned data into	core external group by spill	0.047619
intercept computed for this model	model intercept	0.500000
return the column	model	0.005587
returns a :class dataframe representing the result	sql spark session sql sqlquery	0.250000
set number of batches after which the	streaming kmeans set	0.142857
freq	freq	0.833333
column that generates monotonically increasing 64-bit integers	monotonically increasing id	0.333333
set a local property that	set local property key	0.200000
statement=none)	statement	0.142857
a single script	single script	0.250000
converts vector columns in	convert vector columns to	0.166667
set a configuration property	core spark conf set key value	1.000000
rate for a given label	rate label	1.000000
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params for this	ml vector indexer set params maxcategories inputcol outputcol	0.333333
separate arrays of	ml	0.001835
estimated	generalized linear regression training summary	0.285714
called when processing of	streaming listener on	0.200000
of memory for this	merger	0.025641
test predicted values on a toy model	logistic regression with sgdtests test predictions	0.500000
set a local property that affects jobs submitted	context set local property key value	0.200000
the contents of the :class dataframe to a	frame writer	0.050000
converts vector columns in an	mlutils convert vector columns from	0.166667
residual degrees	generalized linear regression summary residual degree	0.500000
a python rdd	rdd	0.012232
columns in an input	columns from ml dataset	0.125000
ndcg value of all the queries	metrics ndcg	0.200000
rdd of labeledpoint	mllib mlutils load lib svmfile sc	0.125000
much	object size	0.032258
lines text format or newline-delimited json	json path	0.100000
key and	core	0.006042
tests whether this instance contains	param params has param paramname	0.142857
model to the input dataset with optional parameters	dataset params	0.166667
kmeans algorithm for fitting	streaming kmeans	0.035714
new sparkcontext at least the	spark context init	0.083333
instance with a randomly	split model	0.200000
a param with	has param	0.019231
week number	weekofyear col	0.055556
the global	global	0.111111
a param	param params has param	0.019231
to all the jobs started by	job	0.023810
right singular vectors of the	singular	0.015625
predict values for a single data point	predict	0.068966
count of col	count	0.033898
model fitted by als	alsmodel	0.071429
md5 digest and returns	sql md5	0.333333
as a temporary table	as table df tablename	0.250000
right outer join of c{self}	rdd full outer join other numpartitions	0.111111
param with a given string name	params has param	0.019231
the dot product of	dense vector dot	0.050000
based on the dataset in a data source	path source schema	1.000000
__init__(self inputcol=none outputcol=none)	ml max abs scaler init inputcol outputcol	1.000000
onevsrestmodel create and return a python	one vs rest model from	0.142857
the dot product of two vectors we support	dense vector dot	0.050000
this instance contains	has	0.011628
converts vector columns in	mlutils convert vector columns to ml dataset	0.166667
get the cluster centers represented as a	bisecting kmeans model cluster centers	0.095238
norm	linalg sparse vector norm	0.133333
convert matrix attributes which are array-like	mllib linalg matrix convert	0.166667
incrementing	task context	0.142857
or any hadoop-supported file system uri	file path	0.035714
ordered in ascending order or as specified by	ordered	0.076923
number of clusters	kmeans model k	0.250000
onevsrestmodel create and return a	one vs rest model	0.058824
then merges them with extra values from input	extra	0.023810
mintf	min tf	1.000000
list of names	names	0.050000
a function on each rdd of this dstream	kafka dstream transform func	0.500000
columns of blocks	blocks	0.076923
and	and	1.000000
local property set in this thread or null	get local property key	0.066667
paired rdd where the first element is	factorization model	0.043478
feature selection by number of	mllib chi sq selector set num	0.250000
an rdd comprised of vectors containing	mllib random rdds gamma vector rdd	0.166667
results immediately to the master as	locally	0.083333
a densematrix	linalg matrices dense numrows numcols values	0.250000
accumulator's	core accumulator value	0.045455
added through	core spark files	0.125000
scale < 0	scale	0.100000
mintokenlength=1 gaps=true pattern="\s+", inputcol=none outputcol=none tolowercase=true)	mintokenlength gaps pattern inputcol	1.000000
value for each original column during fitting	model original	0.062500
contents of the :class dataframe to	frame writer save path	0.066667
lda keeplastcheckpoint is	ldamodel	0.034483
new mllib-local representation	as	0.148148
param with a given string	ml param params has param	0.019231
the kolmogorov-smirnov ks	stat statistics kolmogorov smirnov	0.333333
setparams(self featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01	gaussian mixture set params featurescol predictioncol k probabilitycol	0.500000
the rdd's elements in	rdd	0.003058
wait for the execution	or timeout timeout	0.125000
value of	ml validator	1.000000
already partitioned data into disks	external group	0.045455
the stream query if this	stream writer	0.041667
of columns	columns	0.019608
instance contains a	params has param	0.019231
residual	residual	1.000000
converts matrix	mllib mlutils convert matrix	1.000000
by the items by key	by	0.014286
generate	generate logistic	1.000000
local representation this discards info about	local	0.038462
an array of	mllib matrix	0.047619
in libsvm format into	mlutils parse libsvm	0.125000
value	ml	0.001835
new rdd of int containing elements from start	core spark context range start	0.090909
paramname	paramname	0.384615
so that :func awaitanytermination() can	manager reset	0.011905
pyfiles	pyfiles	1.000000
initial value of	mllib streaming logistic regression with sgd set initial	0.111111
on all nodes or any hadoop-supported file	file path	0.035714
instance contains a	params	0.006623
a randomly generated uid	cross validator model	0.050000
given parameters in this grid to	ml param grid builder base on	0.076923
comprised	random rdds log normal	0.125000
transforms a	java params transfer param map	0.250000
parses a line in	line line multiclass	0.166667
pipelined maps >>> rdd = sc	pipelined rdd	1.000000
the levenshtein distance of the two given strings	levenshtein	0.045455
:func awaitanytermination() can	sql streaming query manager	0.011905
an rdd created by piping	core rdd	0.003460
<http //jsonlines	compression dateformat	1.000000
find the	find	0.111111
indicates whether this instance is of type	ml ldamodel is	0.066667
reciprocal condition number. all singular values	rcond	0.166667
two-sided p-value of estimated coefficients and	ml generalized linear regression training summary p values	0.333333
alias for na fill()	frame fillna value subset	0.166667
sets	to string set	1.000000
this instance with a randomly generated uid	one vs	0.125000
used again to wait for	query	0.010753
setparams(self minsupport=0 3 minconfidence=0 8 itemscol="items", predictioncol="prediction", numpartitions=none)	fpgrowth set params minsupport minconfidence itemscol predictioncol	1.000000
compare 2 ml	ml persistence test compare	0.166667
each rdds into the	dstream	0.031250
of users for a given product and	users product	0.142857
column	scaler	0.105263
the initial value of	regression with sgd set initial	0.111111
python rdd of key-value pairs	rdd	0.009174
with a given	param params has	0.019231
c{other}, return a resulting rdd that contains	core rdd cogroup other numpartitions	0.066667
data sampled from	data	0.011628
jsqlcontext	jsqlcontext	1.000000
parameters in this grid	grid builder add grid	0.100000
of indices to	ml chi sq selector model	0.166667
vector to the	mllib linalg sparse vector	0.111111
queries so	streaming query	0.010526
sets the given spark runtime configuration property	sql runtime config set	1.000000
of each instance as a	ml	0.001835
dummy params instance used as a placeholder to	params dummy	0.111111
list based on first	based on	0.111111
model for	model	0.011173
ordering columns in a	order by	0.142857
the old hadoop outputformat api mapred	save as hadoop dataset conf keyconverter valueconverter	0.083333
active queries associated with this sqlcontext >>>	streaming query manager active	0.066667
number of gaussians in mixture	mixture model k	0.200000
__init__(self degree=2 inputcol=none outputcol=none)	ml polynomial expansion init degree inputcol outputcol	1.000000
streamingcontext from checkpoint data or	streaming context get or	0.200000
predictioncol="prediction", maxiter=100 tol=1e-6 seed=none	predictioncol maxiter	0.200000
:func awaitanytermination() can be used	streaming query manager reset	0.011905
queries so that :func	sql streaming query manager reset	0.011905
c{sparkcontext addfile()}	get cls filename	1.000000
precision of all	precision	0.076923
instance contains a param with	ml param	0.009524
raw prediction scores into 0/1 predictions	linear classification model	0.076923
sample without replacement	sample	0.050000
track supported impurity measures	tree regressor params	0.250000
python rdd	rdd	0.012232
rdd	core rdd save as	0.037500
removes the specified table from the	uncache table tablename	0.250000
of indices to	ml	0.001835
scalingvec	scalingvec	0.833333
in the training set given the current parameter	distributed ldamodel training	0.034483
the left singular vectors of	mllib linalg singular	0.017544
pipelinemodel used for ml persistence	ml pipeline model	0.066667
local property set in	local property key	0.035714
converter to drop	converter datatype	0.071429
whose columns are the right singular	mllib linalg singular	0.017544
of memory for this	external merger	0.031250
get or compute the	mllib linalg distributed	0.333333
as specified by the optional key function	key	0.017857
elements in iterator	iterator	0.100000
uid of this instance this	ml param params reset uid	0.058824
op	op	1.000000
rdd by applying a function	f	0.021053
generates an rdd	uniform rdd sc size numpartitions seed	1.000000
changes the uid of this	ml param params reset uid newuid	0.058824
to pass else fail with	streaming test case eventually	0.500000
"predictions" which gives the true label	linear regression summary label col	0.333333
to wait	reset	0.011236
labels corresponding to indices to be	string indexer model labels	0.166667
contains a param with a given	param	0.012500
data of a streaming dataframe/dataset is written	sql data stream writer output mode outputmode	0.333333
row-oriented distributed matrix with	row matrix	0.200000
sets vector size default 100	mllib word2vec set vector size vectorsize	1.000000
of the observed data	observed	0.058824
the expected distribution	expected	0.076923
tree model for classification or	tree model	0.026316
such as the spark fair	spark	0.013158
value of the date	day date	0.100000
optional default value and user-supplied	params	0.006623
receiver operating characteristic roc curve which	ml binary logistic regression summary roc	0.166667
wait for the execution to stop return	streaming context await termination or timeout timeout	0.125000
the first n rows to the console	show n truncate vertical	0.333333
broadcast a	core spark context broadcast value	0.125000
returned	streaming py spark streaming test case	0.333333
streaming data	streaming	0.010050
spark session to	session sparksession	0.083333
restore an object of	restore name	0.333333
given path a shortcut of write() save	ml pipeline save	0.166667
a java array	wrapper new java array	0.333333
memory for	core external merger object	0.032258
hadoop configuration	hadoop	0.050000
elements in this	core	0.009063
table accessible via jdbc url url and	jdbc url table column	0.166667
this instance's params to the wrapped	params to	0.035714
specifies the behavior when data or table already	sql data frame writer mode savemode	0.071429
with a given string	param params has	0.019231
string	string	0.333333
for data	data distname	0.083333
sets the accumulator's value	accumulator value	0.050000
number of rows	row matrix num rows	0.200000
get the cluster	bisecting kmeans model cluster	0.333333
regression model with	regression with	0.200000
rdd	rdd map partitions with	1.000000
runs and profiles the method to_profile	core basic	0.066667
compute the sum	sum	0.125000
output by the given columns if	writer bucket by	0.100000
matrix columns in	matrix columns from	0.142857
with generated unique long ids	with unique id	1.000000
saves the content of the dataframe in a	sql data frame writer	0.011628
range of offsets from a single kafka	offset range	0.047619
creates a model from the input java	java estimator create	1.000000
create a new hivecontext for testing	create for testing	0.333333
accessor for the jvm spark sql context	sql sqlcontext ssql ctx	1.000000
set a java system	set system	1.000000
instance contains a param with a given	ml param params has	0.019231
create an input stream	create stream ssc	0.200000
bin	bin	1.000000
for cross	cross	0.142857
train a random forest model	mllib random forest train classifier	0.250000
dot product of two vectors we	mllib linalg dense vector dot	0.058824
of fit test of the observed data	test observed	0.090909
test the python direct kafka stream messagehandler	tests test kafka direct stream message handler	1.000000
minutes of a given date as integer	minute	0.040000
merge shuffled data together by aggregator	merger	0.025641
behavior when data or	sql data frame writer mode savemode	0.071429
residuals label - predicted	linear regression summary residuals	0.500000
value to a boolean if possible	param type converters to boolean	0.250000
value of weights is close to the	parameter accuracy	0.029412
the += operator adds a term	iadd term	0.142857
the dot product of two vectors we support	dot	0.040000
support __transient__ on new	cloud pickler save reduce func args state listitems	0.111111
for	sql	0.002525
fitting and	streaming	0.005025
set	streaming	0.005025
calculates the norm of	norm p	0.055556
maxheap version of a	max heap	0.200000
and a	core rdd	0.006920
value for each original	min max scaler model original	0.062500
number of partitions	num partitions	0.250000
so that :func awaitanytermination() can	streaming query	0.010526
named table accessible via jdbc url url	reader jdbc url table	0.166667
for new	streaming query manager reset	0.011905
convert this matrix to	mllib linalg matrix	0.333333
area under the	area under	0.166667
model which can perform an online update of	model	0.005587
from start	context range start	0.500000
summary of model	mllib tree ensemble model repr	1.000000
in "predictions" which gives the true label	logistic regression summary label	0.333333
which is later than	dayofweek	0.037037
the context to	streaming context	0.055556
a column scipy matrix from	mllib sci py tests scipy matrix size	0.090909
to set k	streaming	0.005025
save a linearregressionmodel	mllib linear regression model save sc path	1.000000
python topicandpartition to map to the java	topic and partition init topic partition	0.055556
how data of a	data	0.011628
of this rdd's	rdd	0.003058
convert a matrix from the new mllib-local	matrices from ml mat	0.333333
column for distinct count of col or cols	count distinct col	0.040000
trigger for	writer trigger	0.111111
adds a	core accumulator	0.030303
year of a given	sql year col	0.050000
generates an rdd comprised of vectors containing	random rdds poisson vector rdd sc mean	0.200000
defines the ordering columns in	spec order by	0.333333
of this instance	ml train	0.181818
be used again to wait	streaming	0.005025
param	params	0.006623
datatype the data type string	datatype string	0.111111
of key	key	0.017857
local temporary view with	temp view name	0.111111
instance	ml param params has param	0.019231
a temporary table in	table df	0.083333
selector type	mllib chi sq selector set selector type	0.111111
elements from an rdd ordered in ascending	core rdd take ordered	0.050000
saved using rdd saveastextfile	minpartitions	0.071429
in a :class windowspec	sql window	0.200000
groups the :class dataframe using the specified	data frame group by	0.200000
this instance contains	ml param params	0.013699
java onevsrestmodel create and return	one vs rest model from java	0.142857
the model params are set correctly	model params	0.125000
dstream	streaming streaming	0.047619
similarity to "word"	synonyms word	0.166667
number	block matrix num	0.100000
comprised of i i d samples from the	random rdds	0.038462
checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)	random forest classifier	0.022727
gets a param by its name	get param paramname	1.000000
configuration	core spark conf	0.055556
for feature selection by number of top features	chi sq selector set num top features	0.500000
docstring is not shown publicly	linalg row matrix init rows numrows numcols	0.333333
impurity="variance",	regressor	0.043478
with the frame boundaries defined from start	range between start	0.200000
as at text file using	as text files prefix suffix	0.250000
all globals names read or written to by	globals	0.076923
with the frame	range between	0.166667
value of	ml param has max	1.000000
scans all active values and count non zeros	nonzeros	0.375000
a logistic regression model	logistic regression	0.040000
compute the	mllib linalg indexed row matrix	0.250000
set the trigger for the stream	sql data stream writer trigger	0.083333
k	streaming	0.005025
sets params for	set params featurescol labelcol predictioncol	0.100000
items for	items cols support	0.125000
the new hadoop outputformat api mapreduce package	as new apihadoop dataset conf keyconverter valueconverter	0.142857
rdd's elements in	core rdd	0.003460
submit and test a single script on a	submit tests test single script on	0.500000
model scale paramter	aftsurvival regression model scale	1.000000
applies unit length normalization on	normalizer transform	0.500000
that :func	sql streaming query manager reset	0.011905
containing a json	from json	0.166667
receive accumulator updates in a daemon thread	update	0.055556
rdd as non-persistent and remove all	rdd unpersist	0.066667
columns that make up each	mllib linalg block matrix cols per	0.333333
test a script with	tests test	0.055556
k-means	kmeans train	0.333333
total log-likelihood	ml gaussian mixture summary log likelihood	0.142857
the l{sparkcontext} that this rdd was created	core rdd context	0.166667
mean squared error which is defined as	linear regression summary	0.013889
represents an entry of a	entry	0.250000
column or names into a jvm seq	seq sc cols converter	0.055556
for each original column during fitting	model original	0.062500
tokens in the training	ldamodel training	0.034483
param	m2 param	0.125000
predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1 0	predictioncol probabilitycol	0.111111
reduce	reduce	0.250000
data	data stream	0.028571
avg	avg	1.000000
computes column-wise summary statistics for the	statistics col stats	0.200000
mixin for param threshold threshold in binary classification	has threshold	0.250000
specified table as a :class dataframe	data frame reader table tablename	1.000000
results as	schema primitivesasstring prefersdecimal	0.500000
vectorizer	vectorizer	1.000000
used again to	query manager	0.011905
viewname	viewname	1.000000
converts vector columns in an input	mlutils convert vector columns to ml dataset	0.166667
get the n elements from an rdd ordered	core rdd take ordered num	1.000000
can be used again to wait	sql streaming query manager reset	0.011905
casesensitive=false) sets	words remover set	0.200000
to be placed into main	core modules to main	1.000000
awaitanytermination() can	streaming query manager	0.011236
ordered in ascending	ordered	0.076923
a data	data	0.011628
wait for	context await termination or timeout timeout	0.125000
local property that	local property key	0.035714
setparams(self min=0 0 max=1 0	scaler set params min max	1.000000
create an rdd	create	0.034483
test that the final value of weights is	sgdtests test	0.142857
dataframe	sql data frame	0.026738
1 0] for feature selection by	mllib chi sq selector	0.150000
computes hex value of the given column	hex col	0.166667
__init__(self formula=none featurescol="features",	init formula featurescol	1.000000
makes a class inherit documentation from its	mllib inherit	0.045455
calculates the length of	length col	0.050000
computes column-wise summary statistics for the input	mllib stat statistics col	0.200000
test this should be list of	test	0.015152
for	object	0.027778
importance of each feature	ml random forest classification model feature importances	0.250000
set the trigger for	trigger	0.071429
with a randomly generated	train validation split model	0.166667
for this	streaming	0.005025
is assumed to consist	ascending numpartitions keyfunc	0.100000
table accessible via jdbc url url	reader jdbc url table	0.166667
classes values which the	classes	0.034483
a paired rdd where the first element	matrix factorization	0.040000
so that :func awaitanytermination()	streaming query manager	0.011236
"zerovalue" which may be added	fold	0.076923
start inclusive to end inclusive	between start end	0.250000
this matrix to a rowmatrix	mllib linalg coordinate matrix to row matrix	0.333333
sort order	sort	0.111111
inside brackets pairs	brackets split	0.083333
called when a receiver has been started	listener on receiver started receiverstarted	1.000000
returns one of multiple possible	sql	0.002525
wait for the	streaming context await termination timeout	0.166667
sets the given parameters in this	ml param grid builder add	0.200000
setparams(self featurescol="features", labelcol="label",	ml linear regression set params featurescol labelcol	1.000000
defines the ordering columns in a	spec order by	0.333333
elements from an rdd ordered	rdd take ordered	0.050000
which is a	linear regression summary	0.013889
dstream by applying a function to each element	f preservespartitioning	0.100000
loads vectors saved using	load vectors sc path	0.333333
broadcast a read-only variable to the cluster	context broadcast value	0.125000
the given columns specified	sql data frame	0.005348
fdr	fdr fdr	1.000000
into	external group by spill	0.047619
valueconverter	valueconverter	0.833333
stream query if this	data stream writer	0.041667
train a random forest model for binary	mllib random forest train	0.250000
a jvm scala map from a	frame jmap jm	0.111111
the number of partitions	num partitions	0.250000
of each cluster	ml	0.001835
of conditions and returns	sql column otherwise value	0.050000
points saved using rdd saveastextfile	points sc path minpartitions	0.250000
for feature selection by number	mllib chi sq selector set num	0.250000
property	property	1.000000
configure the kmeans algorithm	kmeans	0.025641
the number of rows	linalg row matrix num rows	0.200000
function and	function	0.027778
submit and test a single	core spark submit tests test single	1.000000
table accessible via jdbc url url and	jdbc url table	0.090909
accumulator's	accumulator value value	0.050000
term frequency vectors or transform the rdd of	hashing tf transform	0.045455
the uid	param params reset uid	0.333333
copy all params defined on the class	param params copy params	0.200000
forget about past terminated queries so that	streaming query manager reset terminated	0.200000
mean squared error which is defined as the	regression	0.010000
param with a given	ml param params has param	0.019231
min value for each original column during	model original min	0.250000
maxiter max number	max iter	0.166667
c{self} and c{other}	join	0.034483
pearson's chi-squared goodness	stat statistics chi sq	0.066667
dictionary a list	size	0.036697
logistic regression	logistic regression	0.200000
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	decision tree regressor	0.058824
akaike's "an information criterion" aic	ml generalized linear regression summary aic	0.250000
set the selector type	selector set selector type	0.111111
size to	size	0.009174
of the month of a given date as	sql dayofmonth col	0.031250
setparams(self featurescol="features",	set params featurescol	0.800000
that has exactly numpartitions partitions	coalesce numpartitions	1.000000
this instance with	one vs	0.125000
params instances for the given param and assert	params m1	0.047619
columns are the right singular vectors of	singular	0.015625
external table based on the dataset in	external table tablename path	0.090909
output	writer bucket	0.500000
termination	termination timeout	0.041667
queries so that	streaming query manager	0.011236
data sampled from a continuous distribution	data	0.011628
how data	data	0.011628
computes average values for each numeric columns	grouped	0.035714
this instance	has param	0.019231
featurescol="features", labelcol="label", forceindexlabel=false) sets params for rformula	ml rformula set params formula featurescol labelcol forceindexlabel	0.500000
key and value class from	core spark	0.020619
unit length normalization	normalizer	0.166667
number of nonzero elements this	linalg sparse vector num	0.200000
two vectors we	linalg	0.022222
input dataset this is called by the	dataset	0.020408
sql context	context sqlcontext	0.083333
inherit	inherit	0.185185
zerovalue	zerovalue	0.384615
that :func awaitanytermination()	sql streaming	0.010204
the stream query if this is not set	stream	0.017544
ensure that daemon and workers terminate	core daemon tests test termination	0.166667
python direct kafka	kafka	0.222222
find synonyms of a word	model find synonyms word	1.000000
week number of a given date	weekofyear	0.043478
table based on the dataset in a	table tablename path	0.166667
again	manager	0.011236
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6	ml aftsurvival regression init featurescol labelcol predictioncol	0.500000
class to track supported random forest	random forest	0.041667
(json lines text format or newline-delimited json	writer json path	0.125000
number	indexed row matrix num	0.100000
scale	input offset scale	1.000000
:	regression	0.030000
as the spark fair scheduler	core spark context	0.011628
mean squared error which is defined	mllib regression	0.022727
day of the month of a given	sql dayofmonth col	0.031250
for	streaming	0.010050
a python rdd of key-value pairs	rdd save	0.038462
how much of	core external merger	0.032258
until any of the queries on the	any termination	0.142857
[[poweriterationclustering]]	power iteration	1.000000
or transform the rdd of document	hashing tf transform document	0.166667
functionality for statistic	stat	0.076923
save this model to the given	saveable save sc	1.000000
with a given	params has	0.019231
numuserblocks	numuserblocks	1.000000
to wait for new	manager	0.011236
create a multi-dimensional rollup for the current :class	sql data frame rollup	0.055556
system using the new hadoop outputformat api mapreduce	new apihadoop dataset conf keyconverter valueconverter	0.142857
loading	ml java mlreader	0.200000
'x' to all mixture	mixture model predict	0.125000
initial value	sgd set initial	0.111111
this frame but not in another frame	data frame subtract other	0.333333
a jvm scala map from a dict	sql data frame jmap jm	0.111111
sets the	set key value	0.500000
select filter	sq selector model selected features	0.333333
convert to	linalg dense matrix to	1.000000
the n elements from an rdd	core rdd take	0.200000
k decayfactor timeunit to configure the kmeans algorithm	kmeans	0.025641
to a java onevsrestmodel used for ml persistence	ml one vs rest model to java	1.000000
model on the	with	0.055556
so that :func awaitanytermination() can	sql streaming	0.010204
predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0	predictioncol probabilitycol	0.333333
__init__(self k=none	pca init k	1.000000
column into	sql column cast	1.000000
of the month of a given date	sql dayofmonth	0.031250
full class name	loader class cls clazz	0.333333
for a condition	condition	0.045455
of the file to which this rdd	file	0.028571
column containing a json	from json col	0.083333
transforms	java params transfer	0.250000
convert this vector to the new mllib-local representation	mllib linalg sparse vector as ml	0.333333
from start	between start	0.200000
convert this matrix to an indexedrowmatrix	linalg block matrix to indexed row matrix	0.333333
number of batches after which the centroids of	timeunit	0.025641
obtaining a test statistic result	stat test result	0.166667
:class column for distinct count of	count distinct	0.040000
outputformat api mapred	outputformatclass keyclass valueclass	0.250000
set the selector type of the	mllib chi sq selector set selector type	0.111111
dstream by applying reducebykey to	streaming dstream reduce by	0.076923
in the key-value pair rdd through a flatmap	core rdd flat map	0.333333
rdd was checkpointed not defined if	checkpoint	0.062500
value of	ml train	0.090909
converts matrix columns	mllib mlutils convert matrix columns	0.166667
dependency	dependency	0.320000
comprised of	random rdds poisson	0.125000
get number of nodes	num	0.008403
associative function "func" and a	core rdd	0.003460
value of	ml param has features col	1.000000
first n rows	head n	1.000000
param with	params	0.006623
the area	metrics area	0.333333
: deprecated in 2 1 use approx_count_distinct instead	sql approx count distinct col rsd	0.333333
java udf so	java	0.012195
for the stream query if this is	sql data stream	0.031250
tests whether this instance contains a param	params has param paramname	0.142857
resulting rdd that contains a tuple with the	rdd cogroup	0.066667
the list based on	based on key outputs	0.111111
randomly generated	cross validator model	0.050000
infer schema from an rdd of row	spark session infer schema rdd	0.250000
mark this rdd	core rdd	0.003460
of all params with their optionally default	ml param params explain params	0.166667
stats to stdout	profiler show	0.166667
even if users construct taskcontext instead of using	core task context	0.100000
seed	seed	0.666667
this instance	pipeline model	0.071429
the threshold	threshold value	0.166667
rdd 'x' has maximum membership in this model	gaussian mixture model	0.052632
data into	group by	0.041667
the stream query	sql data stream	0.031250
an fp-growth model that contains	mllib fpgrowth train cls data	0.100000
curve which is a dataframe	ml binary logistic regression	0.142857
multi-dimensional rollup for the current :class	frame rollup	0.055556
union	union	0.545455
create a python topicandpartition to map to the	partition init topic partition	0.055556
and commutative reduce function but	rdd reduce	0.071429
be used	streaming query manager	0.011236
in "predictions" which gives the probability of each	ml logistic regression summary probability col	0.166667
return a jvm seq of columns that describes	cols cols kwargs	0.090909
columns	columns from	0.125000
the list of	sql	0.005051
containing a json string into a [[structtype]]	from json	0.166667
__init__(self k=none	ml pca init k	1.000000
for which predictions are known	isotonic regression	0.090909
python object into an internal sql object	internal obj	0.500000
data into disks	external	0.013889
row	row	0.375000
the dot product	dot	0.080000
number	model num	0.333333
returns weighted false positive	mllib multiclass metrics weighted false positive	1.000000
comprised of vectors	random rdds poisson vector	0.125000
number of columns that make up each block	block matrix cols per block	0.333333
columns of blocks in	col blocks	0.250000
of specific kafkardd	streaming kafka rdd offset	0.500000
average values for each numeric columns	grouped data	0.035714
a new dstream by applying reducebykey to each	streaming dstream reduce by	0.076923
valueclass	valueclass	1.000000
so that	sql streaming	0.010204
be used again to	streaming query manager reset	0.011905
which is a dataframe having two	regression	0.010000
rdd contains the count of distinct	count	0.016949
the selector	set selector	0.333333
the underlying rdd with	mllib	0.010526
extract the year of a given date as	year	0.040000
the second is an	mllib	0.010526
property that affects jobs submitted	property key value	0.125000
feature selection by number	mllib chi sq selector set num	0.250000
table name	name	0.043478
listener	listener	1.000000
a param	param params has	0.019231
column mean	standard scaler model mean	0.125000
:class column for approximate distinct count of	approx count distinct col rsd	0.066667
centroids according to data	data decayfactor timeunit	0.500000
the date column	next day date	0.100000
total log-likelihood for this model on	gaussian mixture summary log likelihood	0.142857
name	name	0.565217
use l{sparkcontext broadcast()}	broadcast	0.052632
content of the	writer	0.040000
values for each key using an associative	by key func	0.062500
the current [[dataframe]] and perform the specified aggregation	pivot pivot_col values	0.050000
sort the list based on first value	streaming test case sort result based on key	0.333333
so that :func awaitanytermination() can be	manager reset	0.011905
hypothesis	result	0.125000
spark	spark context	0.023256
for	streaming query manager	0.011236
the points belongs to in this model	mllib bisecting kmeans model predict x	0.333333
of parameters	ml	0.001835
get or compute the number of rows	mllib linalg block matrix num rows	0.200000
a batch of jobs has started	batch started batchstarted	0.333333
model	generalized linear regression model	0.200000
accuracy/precision/recall objective history total iterations) of model	ml logistic regression model	0.500000
test the stage ids are available and incrementing	core task context tests test stage	1.000000
setparams(self estimator=none estimatorparammaps=none evaluator=none	estimator estimatorparammaps evaluator	0.250000
column	column	1.000000
mixin for param labelcol label	has label	1.000000
checkpointed and materialized either reliably or locally	checkpointed	0.083333
function to the value of each key-value	values f	0.062500
min value for each original column during fitting	model original min	0.250000
values	model	0.005587
current [[dataframe]] and	pivot pivot_col values	0.050000
this rdd was checkpointed not defined if	checkpoint	0.062500
word	word2vec	0.052632
weights computed for	weights	0.066667
used again to wait for	query manager reset	0.011905
this is called by the default implementation of	ml	0.001835
defines	window spec	0.166667
outputcol output column name	output col	0.500000
the libsvm format into an rdd of labeledpoint	mllib mlutils load lib svmfile sc	0.125000
that	core external merger object size	0.032258
format into an rdd of labeledpoint	lib svmfile sc	0.125000
sparsematrix	linalg matrices sparse numrows numcols colptrs rowindices	1.000000
java	get java	0.111111
or compute the number of rows	mllib linalg indexed row matrix num rows	0.200000
to the new mllib-local representation	as ml	0.333333
content of the :class dataframe in json	sql data frame	0.005348
of the :class dataframe in orc	data frame writer orc	0.200000
to an external database table via jdbc	jdbc url table mode properties	0.200000
residual degrees of freedom for the null model	summary residual degree of freedom null	0.333333
a python topicandpartition	streaming topic and partition init topic partition	0.055556
used again to wait for	streaming	0.005025
conduct pearson's chi-squared goodness	mllib stat statistics chi sq	0.066667
the initial value of	streaming logistic regression with sgd set initial	0.111111
binary	binary	0.615385
the area under the precision-recall curve	mllib binary classification metrics area under pr	0.333333
setparams(self estimator=none estimatorparammaps=none evaluator=none numfolds=3	estimator estimatorparammaps evaluator numfolds	0.200000
specified table	tablename	0.043478
column scipy matrix from a dictionary of	sci py tests scipy matrix size	0.090909
singular vectors of	linalg singular	0.035088
submitted from this thread such as the spark	core spark context	0.011628
globals names read or written to by	cloud pickler extract code globals	0.125000
an input stream that	stream	0.035088
the spark fair	spark	0.013158
train the model on the incoming dstream	streaming logistic regression with sgd train on dstream	1.000000
a range of	range	0.030303
jvm scala map from a	sql data frame jmap jm	0.111111
as the square root of the mean squared	root mean squared	0.333333
of this instance with a randomly generated uid	ml cross validator model	0.333333
degrees	degrees	1.000000
in a profile object is returned	basic profiler profile func	0.200000
transform	transform	0.437500
a term to this	core accumulator add term	0.066667
user defined function in python	user defined function	0.066667
compute the number of rows	mllib linalg indexed row matrix num rows	0.200000
in this grid	ml param grid builder add grid	0.100000
instance	params	0.006623
set a configuration	core spark conf set	0.100000
is to be used with the spark sink	storagelevel maxbatchsize	0.045455
load a model from the given path	model load cls sc path	0.500000
an input stream that pulls events	stream ssc hostname port	0.200000
compare 2 ml params instances for the given	compare params	0.200000
jvm seq	seq	0.043478
featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1 0	featurescol labelcol predictioncol probabilitycol	0.333333
transforms	params transfer param map	0.250000
a value to list of ints if possible	ml param type converters to list int	0.333333
instance contains a param	has	0.011628
of words closest in	ml word2vec	0.142857
so that :func awaitanytermination()	reset	0.011236
performs the kolmogorov-smirnov ks test for	stat statistics kolmogorov smirnov test	0.166667
this instance's params to	ml java params to	0.045455
calculates the md5 digest and returns	sql md5	0.333333
outer join	outer join	0.250000
stream api with start offset specified	stream from offset	0.500000
day of the month of	dayofmonth	0.027027
an rdd of	rdd	0.006116
squared distance from a sparsevector or 1-dimensional numpy	linalg sparse vector squared distance other	0.166667
of two vectors we	ml linalg dense	0.100000
:py attr inverse	inverse value	1.000000
python rdd of key-value	core rdd save as	0.037500
data points in	sizes	0.166667
explain	explain	1.000000
new	query manager	0.011905
average values for each numeric	sql grouped	0.043478
a text file	text path	0.333333
setparams(self scalingvec=none inputcol=none outputcol=none) sets params for	ml elementwise product set params scalingvec inputcol outputcol	0.333333
computes average values for each numeric columns for	sql grouped data	0.041667
so that :func awaitanytermination() can	reset	0.011236
an rdd of labeledpoint	load lib svmfile	0.125000
recommends the top "num"	recommend	0.153846
forest model for classification or	forest	0.083333
setparams(self labelcol="label",	set params labelcol	1.000000
again to wait for	sql streaming	0.010204
matching keys in c{self} and c{other}	join	0.034483
partitioned data into disks	core external	0.016129
default min number of partitions	context default min partitions	0.250000
all the :class streamingquery streamingqueries active	streaming query manager	0.011236
already partitioned	core external group by spill	0.047619
get depth of tree	tree	0.020833
single kafka topicandpartition	offset	0.021739
dump already partitioned data	by spill	0.047619
likelihood	likelihood dataset	1.000000
an rdd comprised of vectors containing	random rdds log normal vector rdd	0.166667
given string	param	0.012500
root directory that contains	root directory	0.333333
sql context to use for loading	java mlreader context sqlcontext	0.333333
parquet files returning the result	parquet	0.066667
extract the minutes of	minute col	0.050000
of this instance with	ml	0.011009
save this rdd	core rdd save	0.012658
the uid of this	ml param params reset uid newuid	0.058824
line in libsvm format into label indices values	parse libsvm line line	0.111111
add a py or zip dependency for all	add py file path	0.166667
both have the same param	m2 param	0.125000
or	tf	0.076923
'x' to all mixture	mllib gaussian mixture	0.045455
comprised of vectors containing i i d	random rdds log normal	0.125000
sparse matrix stored in csc	sparse matrix	0.250000
rdd 'x' to all mixture components	mllib gaussian mixture model predict soft x	0.142857
of memory for this	core external merger	0.032258
stepsize step size to be used for each	step size	0.125000
k decayfactor timeunit to configure the kmeans algorithm	streaming kmeans	0.035714
end exclusive	end	0.066667
containing elements from	core spark context range	0.142857
for this obj assume that all the objects	core external merger object size obj	0.040000
an associative and commutative reduce	core rdd reduce	0.083333
or compute the	mllib linalg block matrix	0.052632
of substr	substr str	0.125000
an external	external	0.027778
thread such as the spark fair	spark context	0.023256
obj	type obj	0.333333
of active queries associated with this sqlcontext	sql streaming query manager active	0.066667
globals names read or written to	cloud pickler extract code globals	0.125000
how data of a	data stream	0.028571
variance and count of	core rdd	0.003460
tables/views	tables	0.071429
into a jvm seq of column	seq	0.043478
hive	hive	1.000000
of this	core	0.003021
database table	table mode properties	0.200000
regression score	regression metrics	0.083333
all	core external merger	0.032258
as the spark	spark context	0.023256
using rdd[vector] saveastextfile	mllib mlutils	0.333333
__init__(self	generalized linear regression init	1.000000
field in :py attr predictions which	linear regression	0.040000
value of the given	col	0.016393
a given string	param params has	0.019231
wait	context await termination or timeout timeout	0.125000
list of conditions and returns one of	sql column otherwise value	0.050000
all other string options	options	0.111111
to	sql to	0.041667
hadoop configuration which is passed	hadoop	0.050000
return whether this	core	0.003021
to this params	param params	0.014925
daemon and workers terminate	core daemon tests test termination	0.166667
columns that describes the sort order	frame sort cols cols kwargs	0.142857
valuecontainsnull	valuecontainsnull	1.000000
again to wait	streaming query manager	0.011236
of active queries associated with this sqlcontext >>>	manager active	0.066667
add two values of	param add	0.250000
profile object is returned	profiler profile	0.200000
computes an fp-growth model that contains frequent	mllib fpgrowth train cls data minsupport numpartitions	0.100000
has half the weightage	half life halflife	0.166667
trigger for the stream query if	stream writer trigger	0.083333
instance contains a param with a given	param params has param	0.019231
transforms the embedded params to the companion	transfer params to	0.333333
of the current [[dataframe]] and perform	sql grouped data pivot pivot_col values	0.050000
a method for binary operator this	op name doc	0.166667
day	day	1.000000
the rdd of document	document	0.040000
initial	mllib streaming logistic regression with sgd set initial	0.111111
"predictions" which gives the predicted	linear regression summary prediction col	0.142857
print the first num elements of each	pprint num	0.250000
a dummy params instance used as a	ml param params dummy	0.111111
converting raw prediction scores into 0/1 predictions	linear classification model	0.076923
prefixspan >>> data = [	prefix span	0.166667
of labeledpoint	mllib mlutils load lib svmfile sc path numfeatures	0.125000
decorator that makes a class inherit documentation from	mllib inherit	0.045455
the :class dataframe	sql data frame	0.032086
left singular vectors of	singular	0.015625
rdd of key-value pairs (of	core rdd save as	0.037500
variance and count of the rdd's	core	0.003021
curve	binary classification metrics	0.500000
:py attr numbuckets	num buckets value	1.000000
function to each element	f preservespartitioning	0.200000
perform a right outer join	full outer join other numpartitions	0.111111
:class dataframe to a data	sql data frame writer save path	0.142857
the list based on first	based on	0.111111
final value of weights is close to the	parameter accuracy	0.029412
set the initial value of	logistic regression with sgd set initial	0.111111
on the executors if the	unpersist blocking	0.166667
ignore separators inside brackets pairs e g	ignore brackets split	0.250000
extract the minutes	minute col	0.050000
of indices	ml	0.005505
parameters in this grid to fixed values	param grid builder add grid param values	0.333333
contains a param with a given string name	ml param	0.009524
of int containing elements from	core spark context range	0.142857
content of the dataframe in a	data frame writer	0.014085
a right outer join of c{self}	rdd full outer join other numpartitions	0.111111
in rdd 'x' to all mixture	gaussian mixture	0.038462
comprised of	random rdds log normal vector	0.125000
first n elements in the	n	0.027778
compute similarities between columns of this matrix	mllib linalg row matrix column similarities threshold	1.000000
calculates the correlation of	col1 col2 method	0.055556
scalingvec=none inputcol=none outputcol=none)	scalingvec inputcol outputcol	1.000000
of each instance as a vector	ml	0.001835
n	n	0.333333
new dstream in which	streaming streaming	0.047619
'x' has maximum membership in	gaussian mixture	0.038462
comprised	random rdds poisson	0.125000
the dispatch to handle all function	core cloud pickler save function obj	0.142857
min=0 0 max=1 0 inputcol=none outputcol=none)	min max inputcol outputcol	1.000000
test the python direct kafka rdd api	stream tests test kafka rdd	0.500000
can be	sql streaming query	0.011765
log likelihood	log likelihood	0.250000
creates a local temporary view with this dataframe	data frame create temp view name	1.000000
already	group by spill	0.047619
adds an output option for	frame writer option key value	1.000000
a dense vector of 64-bit floats from	ml linalg vectors dense	0.166667
specific kafkardd	kafka rdd offset	0.500000
mixed with hasmaxiter hasinputcol and hasseed	other test	0.333333
a 'new api' hadoop	context new apihadoop	0.333333
the count of distinct elements	count	0.016949
norm of a sparsevector	linalg sparse vector norm p	0.066667
again to wait for new terminations	sql	0.002525
identifier	identifier	1.000000
values for each numeric	grouped	0.035714
on the log likelihood	ldamodel log likelihood dataset	0.142857
computes column-wise summary statistics for the input rdd[vector]	stat statistics col	0.200000
submit and test	submit tests test	0.285714
again	streaming	0.005025
impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	random forest classifier	0.022727
values for each key using	by key func	0.062500
instance contains	param params has param	0.019231
this instance contains	params has	0.019231
parquet file stream returning the	stream reader parquet path	0.083333
centers represented as a list of	centers	0.100000
convert a value to a boolean	to boolean	0.250000
load a	regression model load cls	1.000000
passed as a list of	core spark conf	0.055556
the minimum number	min count	0.076923
return an javardd of object by unpickling it	ml	0.001835
converts matrix columns in	mlutils convert matrix columns to	0.166667
save a	regression model save	0.500000
this	param params has	0.019231
each key using an associative and commutative reduce	reduce by key	0.333333
a new hivecontext for testing	for testing cls sparkcontext	0.333333
left outer join	left outer join	0.111111
to be used with the spark sink deployed	storagelevel maxbatchsize	0.045455
values	linalg dense vector values	0.200000
the content of the dataframe in a text	data frame writer text	0.200000
instance to a java	to java	0.090909
:class dataframe as a temporary table in the	data frame as table	0.333333
set the selector type of the	sq selector set selector type	0.111111
field by name in a	field name	0.166667
outer join of c{self}	outer join other numpartitions	0.333333
the deviance for the fitted model	summary deviance	0.125000
soundex encoding for	sql soundex	0.055556
returns	ml	0.001835
prediction a k	prediction	0.041667
fitintercept whether to fit	fit	0.100000
content of the :class dataframe in	data frame writer	0.028169
from flume	streaming flume utils	0.200000
using the old hadoop outputformat api	as hadoop dataset conf keyconverter valueconverter	0.083333
obj assume that all the objects	size obj	0.040000
the explained variance regression score	linear regression summary explained variance	0.333333
__init__(self	linear regression init	1.000000
:py attr censorcol	censor col value	1.000000
specifies timezone in utc offset	utcoffset timezone	1.000000
hadoop configuration which is passed	context hadoop	0.090909
size	size	0.091743
again to wait for	query manager reset	0.011905
set a local property that	context set local property key	0.200000
the count of	count	0.016949
with singular values in descending order	singular value decomposition s	0.250000
ml params instances for the	params m1	0.047619
list or pandas dataframe	local data schema	0.500000
checkpointinterval	checkpointinterval	1.000000
extract	regexp extract	0.500000
how much of memory for	external merger object size	0.032258
matrix to a rowmatrix	mllib linalg coordinate matrix to row matrix	0.333333
the jobs started	job	0.023810
used again to	streaming query manager reset	0.011905
them with extra values from input into	map extra	0.040000
the id of the stage that	stage id	0.500000
provides methods to set	streaming	0.005025
queries so	sql streaming query manager	0.011905
return the column	standard scaler model	0.090909
vector	linalg sparse vector	0.111111
:class dataframe to	frame writer save	0.066667
the standard deviation	rdd stdev	0.066667
distance from a sparsevector or	distance other	0.133333
__init__(self rawpredictioncol="rawprediction", labelcol="label", metricname="areaunderroc")	classification evaluator init rawpredictioncol labelcol metricname	1.000000
memory string	memory s	0.142857
convert this matrix	mllib linalg matrix	0.333333
predicted values on a toy	logistic regression with sgdtests	0.200000
the sum of	ml	0.001835
count of	core	0.003021
in this frame but not in another frame	data frame subtract other	0.333333
parammap into a java	param map to java	0.250000
x	x	1.000000
elements in seen in a sliding	windowduration slideduration	0.083333
called when processing of a	listener on output operation	0.166667
number of rows	linalg indexed row matrix num rows	0.200000
that	core external merger	0.032258
this rdd was created on	core rdd	0.003460
indicates whether this instance is	is	0.083333
all the	external merger	0.031250
to the	to	0.023077
variance	variance	1.000000
a json	json	0.043478
extract the week number of a given date	sql weekofyear	0.055556
of column names	sql	0.005051
multiple parameters passed as	conf set	0.200000
to an external database table via jdbc	jdbc url table mode	0.200000
of the date column	day date	0.100000
set multiple parameters passed as a list	core spark conf set	0.100000
that :func awaitanytermination() can be used	sql streaming query manager	0.011905
this instance with	one vs rest model	0.058824
multi-dimensional rollup	sql data frame rollup	0.055556
users	users	0.466667
all globals names read or written to by	cloud pickler extract code globals	0.125000
fields	fields	1.000000
two columns of a dataframe as	sql data frame corr col1	0.166667
output a python rdd	rdd save	0.038462
new terminations	sql streaming query manager	0.011905
dstream by applying reducebykey to each rdd	streaming dstream reduce by	0.076923
the log likelihood	log likelihood dataset	0.142857
this matrix to the new mllib-local representation	matrix as	0.250000
get or compute the number	mllib linalg row matrix num	0.125000
combfunc	combfunc	1.000000
the rdd as non-persistent and remove all blocks	rdd unpersist	0.066667
buckets the output by the given columns if	writer bucket by numbuckets col	0.200000
:py attr pattern	pattern value	1.000000
points belongs to in this model	bisecting kmeans model predict x	0.333333
number of gaussians in mixture	mllib gaussian mixture model k	0.200000
a list	core spark	0.020619
matrix columns in an input	matrix columns to ml	0.142857
singular vectors	mllib linalg singular	0.035088
trigger for the stream query	stream writer trigger	0.083333
obj assume that	object size obj	0.040000
with a given	param params has param	0.019231
densevector with singular values in descending order	singular value decomposition s	0.250000
a dataframe as	sql data frame corr col1 col2	0.166667
coefficient of determination	mllib regression metrics r2	0.166667
loads vectors saved using rdd[vector] saveastextfile	mlutils load vectors sc	1.000000
:class datatype the data type string format	datatype string s	0.111111
for the given user	user	0.055556
external table based	external table tablename path	0.090909
columns that make up each block	matrix cols per block	0.333333
transforms a java parammap into a python parammap	params transfer param map from java javaparammap	1.000000
which is a risk function	regression metrics	0.083333
an exception if any error is found	mllib linalg	0.026316
a new profiler using	new profiler	0.333333
of labeledpoint	load lib svmfile sc	0.125000
the soundex encoding	soundex col	0.055556
of int containing elements	core spark context	0.011628
save this model to the given	mllib kmeans model save sc	1.000000
list of functions registered in	list functions	0.250000
sets the accumulator's value only usable in	core accumulator value value	0.050000
this instance contains a	ml	0.001835
a class inherit documentation from its	inherit doc	0.045455
in which each rdd contains the count of	count by	0.100000
return a copy of the	core	0.003021
new feature vector with a subarray of	vector slicer	0.166667
paired rdd where	matrix factorization	0.040000
__init__(self numfeatures=1 << 18 binary=false	ml hashing tf init numfeatures binary	1.000000
all the	merger	0.025641
from checkpoint data or	streaming context get or	0.200000
column denoted by	getattr	0.166667
save this model to the	logistic regression model save	0.500000
test of the observed data against the	test observed	0.090909
total log-likelihood for	ml gaussian mixture summary log likelihood	0.142857
an rdd comprised of vectors	mllib random rdds normal vector rdd	0.166667
much of memory	core external merger	0.032258
format into an rdd of labeledpoint	load lib svmfile sc path	0.125000
number of	mllib linalg block matrix num	0.062500
matrix to a coordinatematrix	indexed row matrix to coordinate matrix	0.333333
sets	step size set	1.000000
to wait for new terminations	sql streaming query	0.011765
set number of batches after which	streaming kmeans set	0.142857
submitted from this thread such as the spark	core spark	0.010309
again to wait for new terminations	streaming query manager reset	0.011905
soundex encoding for a string >>>	soundex	0.043478
transforms the input dataset	transform dataset	1.000000
given parameters in this grid to fixed values	ml param grid builder base	0.076923
a param with	ml param params	0.013699
spark context	sc	0.062500
of this	ml	0.012844
be used again to	manager	0.011236
be used again to wait for new	sql	0.002525
residualstype	residualstype	1.000000
output a python rdd of key-value pairs (of	rdd save as	0.038462
the :class dataframe in json format	sql data frame	0.005348
value of	ml generalized linear	1.000000
new spark configuration	spark conf init	0.250000
loads orc files returning	reader orc path	0.200000
load labeled points	mllib mlutils load labeled points	1.000000
predicts rating for the given	mllib matrix factorization model predict	0.250000
params to the wrapped	ml java params to	0.045455
comprised of vectors containing	random rdds poisson vector	0.125000
used again	query manager reset	0.011905
the singularvaluedecomposition	value decomposition	0.200000
"zerovalue" which	rdd fold	0.125000
an external list for	external	0.013889
predictions which gives the predicted	generalized linear regression summary prediction	0.250000
infer schema from an rdd	session infer schema rdd	0.250000
partitioned data into	group by spill	0.047619
checkpointinterval=10 impurity="variance", seed=none	decision tree regressor	0.058824
multinomial	mllib	0.010526
as the spark fair	spark context	0.023256
be used again to wait for new	streaming query	0.010526
the ensemble	mllib tree ensemble model	0.058824
broker to map to	broker	0.100000
dstream by applying reducebykey	streaming dstream reduce by	0.076923
array-like or buffer	array_like dtype	0.166667
all the	external merger object	0.032258
the stream query	sql data stream writer	0.041667
representing the database table named table	table column	0.166667
a data source	data	0.011628
cachenodeids=false checkpointinterval=10 impurity="variance",	regressor	0.043478
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online", learningoffset=1024	init featurescol maxiter seed	0.250000
each point in rdd 'x' to all mixture	mllib gaussian mixture model	0.062500
so that :func awaitanytermination() can be used	manager reset	0.011905
docconcentration	doc concentration	1.000000
a specific topic and partition for	topic and partition	0.111111
for each numeric columns for each group	grouped	0.071429
extract the year of a	year	0.040000
of the :class dataframe to	data frame	0.005000
of memory for this obj assume that	obj	0.023810
unique id	id	0.083333
table based on the dataset	table tablename path	0.166667
decode	decoder	0.142857
update the centroids according to data	model update data decayfactor timeunit	1.000000
the initial value of weights	regression with sgd set initial weights initialweights	0.333333
large dataset and an item approximately find at	ml lshmodel approx nearest neighbors dataset	0.166667
file system using the new	as new	0.125000
that :func awaitanytermination() can be used again to	query manager	0.011905
used again to	sql streaming query manager	0.011905
for this	core external merger object	0.032258
number of instances in dataframe predictions	ml linear regression summary num instances	1.000000
sets params	set params featurescol labelcol predictioncol classifier	0.500000
python code for a	code name doc	0.111111
heap invariant	core heappop heap	0.142857
dataframe outputted by the model's transform method	linear regression summary predictions	0.200000
distinct elements in rdds in a sliding window	value and window windowduration slideduration	0.076923
feature	model feature	1.000000
value of the	col	0.016393
the content	writer	0.040000
featurescol=none labelcol=none predictioncol=none classifier=none): sets params for onevsrest	ml one vs rest set params featurescol	1.000000
a class inherit documentation	mllib inherit doc	0.045455
impurity="gini", numtrees=20 featuresubsetstrategy="auto",	ml random forest classifier	0.046512
the stream query if this is not	stream	0.017544
so that :func awaitanytermination() can be used again	manager	0.011236
retrieve gaussian distributions as a dataframe	gaussian mixture model gaussians df	0.166667
goodness of fit	stat statistics	0.125000
receiver has	receiver	0.153846
in the training set	training	0.029412
creates	spark session create	0.058824
a column of indices back to a new	index to	0.040000
stdev	stdev	0.238095
with the spark sink deployed	storagelevel maxbatchsize	0.045455
set number of	streaming kmeans set	0.142857
kolmogorov-smirnov ks test for data sampled	stat statistics kolmogorov smirnov test data	0.111111
for each key using an	key func numpartitions	0.066667
to this accumulator's value	accumulator	0.012987
table based	table tablename path	0.166667
max	abs scaler model max	1.000000
repeat	repeat	1.000000
mixin for param maxiter max number	has max iter	0.333333
the week number of a	sql weekofyear col	0.055556
test that the model params are set	test test model params	0.250000
by the default implementation of fit	ml estimator fit	0.083333
a local property that affects	local property key	0.035714
get or compute the number of rows	block matrix num rows	0.200000
of the :class dataframe in parquet	sql data frame writer parquet	0.200000
new	reset	0.011236
incremental step default 1	step	0.100000
impurity="variance", seed=none variancecol=none)	ml decision tree regressor	0.066667
tall	tall	1.000000
labels	labels	0.714286
trigger for the stream query if this is	sql data stream writer trigger	0.083333
which is defined as	mllib regression	0.022727
the precision-recall curve which is a	binary logistic regression summary pr	0.083333
columns are the left singular vectors of	mllib linalg singular	0.017544
with a function	sql user defined function	0.083333
dateformat	dateformat	1.000000
lda	distributed ldamodel get	0.066667
rdd of	core rdd save as	0.037500
setparams(self inputcol=none outputcol=none) sets params for this	set params inputcol outputcol	0.333333
sets the given parameters in this grid to	param grid builder add grid	0.100000
an input stream that pulls events from	stream ssc hostname port	0.200000
converts a sql datum into a user-type object	sql user defined type deserialize datum	0.333333
new class dataframe that with new specified	data frame to df	0.090909
the content of the :class dataframe in	sql data frame	0.016043
value in c{self} that is not contained	subtract other numpartitions	0.111111
multi-dimensional rollup for	data frame rollup	0.055556
of this instance	ml one vs rest model	0.111111
:py attr featuresubsetstrategy	feature subset strategy value	1.000000
given columns specified	sql data frame	0.005348
contents of the :class dataframe	frame writer	0.050000
logistic regression model	logistic regression	0.080000
spark fair scheduler pool	spark	0.013158
specifies the underlying output data source	frame writer format source	0.333333
stream query if this is not	stream	0.017544
rdd of key-value pairs (of form c{rdd[ k	core rdd	0.010381
given on each stratum	by col fractions seed	0.142857
length of a string	length	0.040000
string in	s	0.071429
load a model from the given path the	load cls sc path	0.200000
array of	ml	0.001835
how much of memory for	object size	0.032258
of rows in this :class dataframe	data frame count	0.333333
instance contains a param with a	param params has param	0.019231
the trigger for the stream	sql data stream writer trigger	0.083333
accumulator's value only	core accumulator	0.030303
as at text file using string representation of	as text files prefix suffix	0.250000
maxheap version of a heappop	max heap	0.200000
creates a :class dataframe from an	create	0.017241
that has no partitions or elements	spark context empty	0.333333
tests whether this instance contains a param	has param paramname	0.142857
two vectors	linalg dense	0.200000
least value of the list of	sql least	0.055556
new vector with 1 0 bias appended to	mllib mlutils append bias data	0.333333
python parammap into a	map to	0.125000
the ids of all active stages	core status tracker get active stage ids	0.250000
streaming dataframe/dataset is written	stream writer output mode outputmode	0.083333
make predictions on batches of data from a	linear algorithm predict on	0.066667
adds a term to this accumulator's	term	0.080000
comprised of vectors containing	mllib random rdds exponential vector	0.125000
create a new hivecontext for testing	context create for testing	0.333333
test the python direct kafka stream api	streaming kafka stream tests test kafka direct stream	0.250000
from the input	estimator	0.083333
vectors or transform the rdd of document to	hashing tf transform document	0.166667
year of a given date as integer	sql year col	0.050000
>>> arraytype(stringtype()) == arraytype(stringtype(), true)	sql array type init elementtype containsnull	1.000000
the cluster centers represented as a list	kmeans model cluster centers	0.090909
dm = densematrix(2 2 range 4	mllib linalg dense matrix	0.083333
likelihood	likelihood	1.000000
sort	sort result	0.333333
the features	features	0.043478
squared distance from a sparsevector or 1-dimensional	vector squared distance	0.166667
set the selector type of	mllib chi sq selector set selector type	0.111111
much of	merger object size	0.032258
model to make predictions on batches	mllib streaming linear algorithm predict on	0.066667
new terminations	manager reset	0.011905
queries so	query manager reset	0.011905
sort the list	sort result	0.333333
termination of this	termination timeout	0.041667
c{sparkcontext addfile()}	cls filename	1.000000
sets the spark session to use for loading	ml mlreader session sparksession	0.333333
dstreams in this	streaming streaming	0.047619
url of the sparkui instance started by	ui web url	1.000000
linalg	linalg	0.111111
regression model derived from a least-squares	regression model	0.031250
dot product	mllib linalg dense vector dot other	0.058824
'x' to all mixture	mllib gaussian mixture model predict	0.100000
models	model	0.011173
in this model	mllib bisecting kmeans model	0.333333
main entry point for spark streaming functionality	streaming context	0.055556
params to the wrapped java object and return	params to	0.035714
the current [[dataframe]] and	sql grouped data pivot pivot_col values	0.050000
that :func awaitanytermination() can be	reset	0.011236
values alias for na fill()	sql data frame fillna value subset	0.166667
resulting rdd that contains	rdd cogroup other	0.066667
setparams(self featurescol="features",	ml aftsurvival regression set params featurescol	1.000000
set a java system property such	context set system property cls key value	1.000000
model fitted by :py class word2vec	word2vec model	0.500000
of the rdd's elements in one	core	0.003021
list of functions registered in the	list functions	0.250000
to be used with the spark sink deployed	maxbatchsize	0.037037
convert a value to a boolean	to boolean value	0.250000
prints the first n rows to the console	data frame show n truncate vertical	0.333333
table accessible via jdbc url url	jdbc url table	0.090909
disks	by	0.014286
underlying output	writer	0.040000
accessible via jdbc url url and connection properties	jdbc url	0.200000
fp-growth model that contains	fpgrowth train cls data minsupport numpartitions	0.200000
:class column for distinct count	count distinct	0.040000
this rdd was created	core rdd	0.003460
squared distance from a sparsevector or 1-dimensional	linalg sparse vector squared distance	0.166667
python code for a shared param class	param gen param code name doc defaultvaluestr	0.333333
samples drawn	shape scale numrows	0.125000
driver	driver	0.545455
test that coefs are predicted accurately	tests test parameter accuracy	0.333333
qr	qr	1.000000
initial value of	logistic regression with sgd set initial	0.111111
set the selector type of the chisqselector	set selector type selectortype	0.333333
given a java onevsrestmodel	one vs rest model from java cls	0.200000
the companion	params transfer	0.125000
data or	sql data frame	0.005348
to convert the java_model to a python model	create model java_model	0.250000
does this configuration contain a given key?	spark conf contains key	0.333333
probabilitycol="probability", rawpredictioncol="rawprediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0	probabilitycol	0.150000
distributed matrix whose columns are the left singular	mllib linalg singular	0.017544
a param with a given string	param params has param	0.019231
the rdd's	core	0.003021
frequency vectors or	hashing tf	0.125000
parameters in this grid	grid builder base on	0.076923
distance from a	distance other	0.133333
return	mllib standard	0.125000
that all the	merger	0.025641
property that affects jobs submitted from this	property key	0.066667
md5 digest and	md5 col	0.333333
columns in an	columns to ml	0.125000
gives	summary	0.024390
the points belongs to in this model	mllib kmeans model predict x	0.333333
table based on the dataset in	table tablename path	0.166667
features i e length	features	0.043478
creates a table based on	sql catalog create external table tablename path	0.250000
a left outer join of c{self}	rdd left outer join	0.111111
"num" number of products for all users the	products for users num	0.200000
start	start	0.500000
probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1 0	probabilitycol	0.050000
a right outer join of	rdd full outer join other numpartitions	0.111111
compute	core rdd	0.003460
parameters in this grid to fixed values	ml param grid builder	0.055556
is also known as min-max normalization or rescaling	min	0.041667
predictions	linear	0.025641
matrix on the	matrix	0.015152
onevsrestmodel create and return a python wrapper of	ml one vs rest model	0.111111
calculates the length	length	0.040000
option of ensuring all received data has been	stopsparkcontext stopgracefully	0.050000
creates a new	init sparkcontext sparksession jsqlcontext	1.000000
test predicted values on a toy	logistic regression with sgdtests test predictions	0.500000
tests whether	params has param paramname	0.142857
note this docstring is not shown publicly	mllib linalg row matrix init rows numrows numcols	0.333333
python topicandpartition to map	streaming topic and partition init topic partition	0.055556
this vector to the new mllib-local representation	sparse vector as	0.333333
k=2 probabilitycol="probability", tol=0 01 maxiter=100	k probabilitycol	0.333333
gets a param by its name	ml param params get param paramname	1.000000
test that the	streaming logistic regression with sgdtests test	0.111111
linear regression model derived from a least-squares fit	linear regression model	0.066667
param with a given string	param params	0.014925
k decayfactor timeunit to configure the	streaming	0.005025
:class statcounter members as a	core stat counter as	0.333333
an input stream that is to	stream	0.017544
convert a dict into a	sql to	0.041667
of binomial	ml	0.003670
the sum for each numeric columns for	grouped data sum	0.083333
objective	ml linear regression training summary objective	1.000000
squared distance from a sparsevector	mllib linalg sparse vector squared distance other	0.166667
pivots a column of the current [[dataframe]] and	grouped data pivot pivot_col values	0.050000
dump	core external	0.016129
basic	streaming basic	1.000000
set	context set	0.125000
model which can perform an	model	0.005587
versionadded : 0 9 0	lasso with sgd	1.000000
block matrix other from this block matrix	mllib linalg block matrix	0.052632
multi-dimensional rollup for the current :class dataframe using	data frame rollup	0.055556
the greatest value of the list of column	greatest	0.043478
save this model to	naive bayes model save	0.500000
:class dataframe with the	sql data frame	0.005348
paired rdd	matrix factorization model	0.043478
table accessible via jdbc url url	reader jdbc url table column lowerbound	0.166667
comprised of vectors containing i i d	mllib random rdds gamma	0.125000
variance and count of	rdd	0.003058
from this thread such as the spark	spark	0.013158
wait until any	query manager await any termination timeout	0.166667
values for each key using	key func	0.066667
value pairs or two separate arrays of indices	ml linalg	0.030303
dictionary a	size	0.036697
wait for new terminations	query manager reset	0.011905
with a given string	params has param	0.019231
batch of jobs has completed	batch completed	0.333333
a line in libsvm format into label	mllib mlutils parse libsvm line line multiclass	0.111111
this rdd's	rdd	0.003058
the binary value of the given column	bin col	0.333333
the dot product of two vectors	linalg dense vector dot	0.058824
the selector type of the	set selector type	0.111111
to their vector representations	get vectors	0.142857
a file to be downloaded	file path recursive	0.500000
for indexing categorical feature columns	indexer	0.055556
values from this instance to another instance	copy values to extra	0.333333
python code for	code name doc	0.111111
as the specified table	writer save as table name	1.000000
levenshtein distance of the two given strings	levenshtein	0.045455
performs the kolmogorov-smirnov ks test for data sampled	stat statistics kolmogorov smirnov test data distname	0.111111
unary	unary	1.000000
id is the rdd id	id	0.083333
be used again to wait for	sql streaming query	0.011765
a function to each partition	f	0.010526
an associative and commutative reduce function but return	core rdd reduce	0.083333
returns	sql data frame	0.016043
:func awaitanytermination() can be used again to	query manager	0.011905
enable 'with sparkcontext as	core spark context	0.011628
the model improves on toy data with no	streaming logistic regression with sgdtests	0.200000
python module	module	0.111111
tcp server to receive accumulator updates	update server	0.333333
labeledpoint	lib svmfile sc path numfeatures minpartitions	0.125000
synonyms	synonyms	0.750000
the week number	weekofyear col	0.055556
add a py or	add py file	0.166667
right	right	1.000000
values for each numeric columns for	sql grouped data avg	0.058824
type datatype	datatype	0.045455
a right outer join of c{self}	full outer join	0.111111
maxiter=100 regparam=0 0 tol=1e-6 rawpredictioncol="rawprediction", fitintercept=true standardization=true threshold=0	maxiter	0.166667
get or compute the number of cols	mllib linalg row matrix num cols	0.333333
missingvalue	missingvalue	1.000000
calculates the norm of a sparsevector	sparse vector norm	0.066667
extract the minutes of a given date	minute	0.040000
the observed	observed	0.058824
table accessible via jdbc url url	jdbc url table column lowerbound	0.166667
create	context create	0.083333
new accumulator with	accumulator init aid	0.083333
model's transform method	summary predictions	0.230769
sort the list based on first value	case sort result based on key outputs	0.333333
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini",	classifier	0.050000
can be used again to wait for	streaming query manager reset	0.011905
memory for this obj assume that all	object size obj	0.040000
left singular vectors	mllib linalg singular	0.017544
this block matrix this	mllib linalg block matrix	0.052632
py4j gateway	gateway	0.166667
applies unit length normalization on a vector	mllib normalizer transform vector	1.000000
of labeledpoint	load lib svmfile sc path numfeatures minpartitions	0.125000
test the python direct kafka	tests test kafka direct	1.000000
of characters as a hexadecimal number	sql unhex col	0.142857
maintaining the heap invariant	core heappop heap	0.142857
svd	svd	1.000000
using the new hadoop outputformat api mapreduce package	new apihadoop dataset conf keyconverter valueconverter	0.142857
returns accuracy equals to the	mllib multiclass metrics accuracy	0.166667
exposes information about spark stages	spark stage info	1.000000
depth of tree (e	tree	0.020833
flume	flume utils	0.200000
to save	cloud pickler save	0.166667
returns	mllib multilabel metrics	1.000000
a new java object	new java obj java_class	0.333333
reg	reg	0.833333
the input	sql data frame reader	0.111111
rdd is checkpointed and materialized either reliably	rdd is checkpointed	0.166667
with this spark job	core spark	0.010309
transforms a python parammap	params transfer param	0.250000
span	span	1.000000
grid	grid	0.875000
set a java system property such as spark	spark context set system property cls key value	1.000000
column containing a json string	sql from json col	0.083333
containing a json string into a [[structtype]] or	json	0.043478
sets	word2vec set	0.857143
ids of all active stages	tracker get active stage ids	0.250000
awaitanytermination() can be used again	manager reset	0.011905
code for	code name	0.111111
this context to	streaming context	0.055556
in a profile object is returned	profiler profile func	0.200000
n	n truncate	0.250000
get or compute the number	linalg row matrix num	0.100000
itemscol	items col	0.500000
wait until any of	manager await any termination timeout	0.166667
can be	streaming query	0.010526
convert this vector	vector	0.019231
to	core accumulator	0.030303
returns its elements in a numpy ndarray	ml linalg matrix to array	0.166667
tree model for classification	tree model	0.026316
get spark_user for user who is running sparkcontext	context spark user	0.250000
set pipeline	ml pipeline set	1.000000
make predictions on batches of data	algorithm predict on	0.066667
train the model on the	train on	0.333333
return number of nodes	num nodes	1.000000
for distinct count of col	count distinct	0.040000
sort	spark streaming test case sort result	0.333333
loads	load	0.111111
vector class for passing data to mllib users	vector	0.019231
a lower bound on the log likelihood of	ml ldamodel log likelihood dataset	0.166667
again to	streaming query	0.010526
called when processing	streaming streaming listener on	0.200000
the norm	dense vector norm	0.333333
evaluates the output with	evaluator evaluate dataset	0.142857
indicates whether this instance is of type	ml ldamodel is distributed	0.066667
sizes including input and output layers	model layers	1.000000
an existing rdd	samplingratio	0.100000
has started	started outputoperationstarted	0.125000
the dot product of two vectors we support	dot other	0.050000
the index of the original	index	0.041667
copy	copy extra	0.333333
comprised of vectors containing i	random rdds log normal	0.125000
so that :func awaitanytermination()	streaming	0.005025
return the	streaming py spark streaming	0.333333
the trigger for the stream query	stream writer trigger	0.083333
a converter to drop	converter	0.052632
tests whether	ml param params has param paramname	0.142857
the greatest value of	sql greatest	0.055556
the year of a given date as	sql year col	0.050000
attempt numbers are correctly reported	context tests test attempt number	0.333333
by step every element	step numslices	0.333333
this instance with a randomly generated	train validation split	0.166667
the sample covariance of col1 and	covar samp col1	0.250000
tests a	tests	0.100000
singular vectors	linalg singular	0.035088
that :func awaitanytermination() can	manager reset	0.011905
+= operator adds a term to this accumulator's	accumulator iadd term	0.142857
this context	streaming context	0.055556
given string name	params has param	0.019231
:py attr trainratio	train ratio value	1.000000
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10	lda init featurescol maxiter seed	0.250000
already partitioned	external	0.013889
awaitanytermination() can be used	sql streaming query	0.011765
the deviance for the	linear regression summary deviance	0.125000
datum	datum	1.000000
any hadoop file system using the old hadoop	hadoop file path	1.000000
how much of memory for this	external merger	0.031250
java_model to a python	java_model	0.090909
from an rdd	rdd take	0.200000
specifies the input	sql data frame reader	0.111111
= densematrix(2 2 range 4	linalg dense matrix repr	0.142857
for which predictions are known	regression	0.010000
list of columns for the given table/view in	list columns	0.166667
or compute the number of rows	mllib linalg row matrix num rows	0.200000
the param	param	0.012500
tree (e	tree model	0.026316
the deviance for	deviance	0.111111
returns	spark session sql	0.250000
names of tables in the	table names	0.066667
streaming dataframe/dataset is written to a streaming	stream writer output mode outputmode	0.083333
set bandwidth of each	mllib stat kernel density set bandwidth bandwidth	0.142857
and refresh all	refresh	0.100000
parquet files returning	parquet	0.066667
all nodes or any hadoop-supported	path	0.020408
distinct	distinct col	0.333333
for multiclass classification	multiclass classification	1.000000
awaitanytermination() can be used	query manager reset	0.011905
column of the current [[dataframe]] and	data pivot pivot_col values	0.050000
again	query manager	0.011905
and then merges them with extra values	map extra	0.040000
the centroids of that particular batch has	timeunit	0.025641
this matrix to a coordinatematrix	row matrix to coordinate matrix	0.333333
:func awaitanytermination() can be used again to	sql	0.002525
list based on	based on key	0.111111
frame boundaries defined from start	between start	0.100000
n rows	n	0.055556
be used again to wait	sql streaming query	0.011765
returns a densevector with singular	linalg singular	0.017544
called when a receiver	streaming listener on receiver	0.500000
initialweights	initial weights	0.250000
approximate distinct count of col	approx count distinct col	0.071429
splits=none	splits	0.166667
left outer join of c{self} and c{other}	core rdd left outer join	0.200000
wrap this udf with a function	function	0.027778
matrix to a coordinatematrix	matrix to coordinate matrix	0.333333
data source	source schema	0.181818
partial objects	pickler save partial obj	0.125000
comprised of vectors containing	mllib random rdds gamma vector	0.125000
uid	reset uid	0.333333
range of	range	0.030303
model coefficients of binomial logistic regression	ml logistic regression model coefficients	1.000000
note : experimental	rdd count approx distinct relativesd	1.000000
computes an fp-growth model that contains	mllib fpgrowth train cls data minsupport numpartitions	0.100000
for which predictions are known	ml isotonic regression model	0.125000
this rdd which is assumed	core rdd	0.003460
new profiler using class	collector new profiler	0.333333
the sql context to use for loading	ml mlreader context sqlcontext	0.333333
the dataframe	data frame writer	0.014085
this standardscaler	ml standard scaler	0.200000
the soundex encoding for a	sql soundex col	0.055556
clustering	clustering	0.333333
multi-dimensional cube for the current :class	frame cube	0.055556
cluster centers represented as a list	model cluster centers	0.090909
given product and returns a list of	product	0.029412
sets vector	word2vec set vector	1.000000
smaller than or equal to	numiterations	0.050000
a decorator that makes a class inherit documentation	inherit doc	0.045455
first n elements in	n	0.027778
sample without replacement based on	data frame sample	0.066667
that maps a column of indices back to	index to	0.040000
timeunit to configure the	streaming	0.005025
sort the	test case sort result	0.333333
a py or zip	py file	0.066667
for instances that provide javamlreader	java mlreadable	0.250000
internal use only create a new hivecontext for	sql hive context create for	0.250000
buckets	numbuckets col	0.500000
params to the wrapped java	ml java params to	0.045455
queries	sql streaming query manager reset	0.011905
a temporary table in	table	0.031250
large dataset and an item approximately find	lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
load a model	load cls sc	0.214286
index value pairs or two	linalg	0.044444
points from the population should be a	mllib stat kernel density	0.066667
the accumulator's value only usable	core accumulator value	0.045455
setparams(self featurescol="features", predictioncol="prediction",	ml gaussian mixture set params featurescol predictioncol	1.000000
instance contains	params has param	0.019231
a json file	json path	0.100000
returns the root	metrics	0.041667
this obj assume that	obj	0.023810
returns the schema	schema	0.033333
sort	frame sort	0.250000
compute	linalg coordinate matrix	0.250000
character in matching	matching	0.111111
returns the latest model	mllib streaming linear algorithm latest model	0.500000
to any hadoop file system using the l{org	save as sequence file path	0.500000
note : experimental	rformula model	1.000000
:func	manager reset	0.011905
fast version of a	heappushpop heap item	0.142857
of active queries associated with	query manager active	0.066667
model	decision tree model	0.050000
py or zip dependency for all tasks to	py file	0.066667
the :class dataframe using the specified	data frame	0.005000
a column that generates monotonically increasing 64-bit integers	sql monotonically increasing id	0.333333
degrees of freedom	summary degrees of freedom	1.000000
the embedded params to the companion java object	params transfer params to java	0.500000
of	ml java params	0.125000
data or table	data	0.011628
comprised of vectors containing i	random rdds	0.076923
by number n	n	0.027778
all mixture components	mixture model predict soft	0.142857
a model to the input dataset	dataset	0.040816
format into an rdd of labeledpoint	mlutils load lib svmfile sc path	0.125000
given a large dataset and an item approximately	ml lshmodel approx nearest neighbors dataset key	0.166667
finding frequent items for columns possibly with false	data frame freq items cols	0.166667
dictionary a list of index value pairs	init size	0.066667
file to be	file path	0.035714
or newline-delimited json <http //jsonlines	json path mode compression dateformat	0.166667
__init__(self inputcol=none outputcol=none labels=none)	string init inputcol outputcol labels	1.000000
of the dataframe in	sql data frame	0.005348
save this model to the given	mllib java saveable save sc	1.000000
vector conduct pearson's chi-squared goodness of fit	mllib stat statistics chi sq	0.066667
until any of the queries	manager await any termination	0.142857
rdd is checkpointed	rdd is checkpointed	0.166667
splits the given string by given separator but	s separator	0.333333
:py attr fwe	fwe value	1.000000
sets	validator params set	1.000000
them with extra values from input	extra	0.023810
params to the wrapped java object and return	java params to	0.045455
to a java	to java	0.136364
the index of the original partition	index	0.041667
string column from one base to	col frombase tobase	0.166667
in	ml param grid builder	0.055556
parameters in this grid to fixed	ml param grid builder	0.055556
instance contains a param	params has	0.019231
week number of	sql weekofyear col	0.055556
prints the first n rows to the console	frame show n	0.333333
setparams(self estimator=none estimatorparammaps=none evaluator=none trainratio=0 75	validation split set params estimator estimatorparammaps evaluator trainratio	0.500000
of the :class dataframe as the	data frame writer save as	0.071429
convert a value to an int	to int value	0.250000
operation test for dstream mapvalues	operation tests test	0.111111
use only create a new hivecontext for testing	context create for testing	0.333333
sets	net param set	1.000000
setparams(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6	set params featurescol labelcol predictioncol	0.100000
sets the given parameters in this grid	param grid builder base	0.076923
specifies the underlying output	sql data stream writer	0.041667
and count of the rdd's elements	rdd	0.003058
given path a shortcut of write()	ml pipeline	0.095238
train a decision tree model	mllib decision tree train regressor cls data categoricalfeaturesinfo	0.333333
of freedom for the	of freedom	0.166667
the old hadoop	save as hadoop	0.142857
from start to end exclusive increased by	range start end	0.333333
returns the number of	num	0.008403
average values for each numeric columns for each	sql grouped data	0.041667
sets params	set params featurescol labelcol predictioncol	0.100000
copy of the rdd partitioned	rdd partition	0.062500
access fields by name or	struct type getitem key	0.200000
that all the	core external merger object size	0.032258
"zerovalue"	rdd fold	0.125000
item off the heap maintaining the heap	heap	0.047619
dataframe that stores	ml alsmodel	0.222222
in :py attr predictions which gives	generalized linear regression summary	0.090909
rescale each feature individually to a common	max scaler	0.200000
use for loading	ml java mlreader	0.200000
used again	streaming query manager reset	0.011905
inputformat with	inputformatclass keyclass valueclass	0.125000
changes the uid	reset uid newuid	0.333333
values and then merges them with extra	map extra	0.040000
all params with their optionally default values	params explain params	0.250000
sets window	mllib word2vec set window	1.000000
which	logistic regression summary	0.090909
of memory	core external merger object	0.032258
mean values	mean	0.034483
length len	len	0.071429
paired rdd	matrix factorization	0.040000
new dstream in which each rdd is	streaming streaming	0.047619
creating rdds comprised of i i d	random rdds	0.012821
file system using the	file path	0.035714
new dstream in which each	streaming streaming	0.047619
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 tol=1e-6	init featurescol labelcol predictioncol maxiter	0.333333
kolmogorov-smirnov ks	mllib stat statistics kolmogorov smirnov	0.333333
convert this vector to	mllib linalg sparse vector	0.111111
queries	streaming query manager reset	0.011905
setparams(self formula=none featurescol="features", labelcol="label",	featurescol labelcol	0.047619
that :func awaitanytermination() can be used	streaming query manager	0.011236
an rdd comprised of vectors	mllib random rdds exponential vector rdd	0.166667
content of	writer	0.040000
infer schema from	spark session infer schema	1.000000
outputoperationcompleted	outputoperationcompleted	1.000000
converts matrix columns	mllib mlutils convert matrix columns to ml	0.166667
nonnegative	nonnegative	1.000000
sparkcontext is initialized	core spark context ensure initialized	0.333333
compute the number of rows	num rows	0.200000
for every feature	mllib linear	0.166667
return a	core rdd	0.006920
to be placed into	core modules to	1.000000
active stages	get active	0.333333
of the :class dataframe in parquet	data frame writer parquet	0.200000
given a java onevsrest create and	one vs rest from java cls	0.200000
get the	core rdd get	0.250000
set	streaming kmeans set	0.142857
again to wait	streaming	0.005025
new dstream by applying reducebykey	streaming dstream reduce by	0.076923
returns an mlreader instance for	ml mlreadable read cls	0.250000
note : experimental	rdd mean approx timeout confidence	1.000000
rdd 'x' to all mixture	gaussian mixture	0.038462
computes an fp-growth model that	mllib fpgrowth train cls	0.100000
called when processing of	listener on	0.200000
number of columns that make up each block	matrix cols per block	0.333333
of this instance with a randomly generated uid	ml cross validator	0.166667
given string by given separator but	s separator	0.333333
curve which is a dataframe having	binary logistic regression	0.142857
ml instances that provide :py class javamlwriter	java mlwritable	0.500000
a range of offsets	offset range	0.047619
generates an rdd	exponential vector rdd sc mean	1.000000
python topicandpartition to map to the java	streaming topic and partition init topic partition	0.055556
predicts rating for the given user and product	mllib matrix factorization model predict user product	1.000000
instance contains	ml param params has param	0.019231
persist its values across operations after the first	rdd persist storagelevel	0.166667
__init__(self inputcol=none outputcol=none	to string init inputcol outputcol	1.000000
all the	external	0.013889
pos in byte	pos	0.022222
mathfunction by name	mathfunction name doc	1.000000
resulting rdd that contains a tuple with the	core rdd cogroup other numpartitions	0.066667
multiple parameters passed as a list of	core spark conf	0.055556
the model	mllib streaming logistic regression	0.500000
tcp server to	server	0.142857
elements from start	core spark context range start	0.090909
for each key using an associative	key func numpartitions partitionfunc	0.066667
computes the levenshtein distance of the	levenshtein left right	0.058824
:class dataframe out into	sql data frame	0.005348
the accumulator's value only usable in driver	accumulator value	0.050000
or c{other}, return a resulting rdd that contains	rdd cogroup	0.066667
wait for the	streaming streaming context await termination timeout	0.166667
model derived from	model	0.016760
so that :func	query	0.010753
the number of rows	mllib linalg indexed row matrix num rows	0.200000
test that the	test	0.030303
of labeledpoint	mllib mlutils load lib svmfile sc path	0.125000
in the training set given the	training	0.029412
the :class dataframe to a data source	data frame writer save path format	0.142857
configure the kmeans algorithm for fitting	kmeans	0.025641
input stream	stream stream	0.500000
files	files	1.000000
:class dataframe using the specified columns	data frame	0.005000
values	scaler	0.052632
weights computed for every feature	mllib linear model weights	0.250000
this vector to the	linalg sparse vector	0.111111
new rdd by applying a function to each	f	0.010526
with the spark sink deployed on a	ssc addresses storagelevel maxbatchsize	0.045455
the specified table	table name	0.333333
representation	linalg	0.044444
all globals names read or written to by	core cloud pickler extract code globals	0.125000
for this obj assume that all	size obj	0.040000
this instance's params to the	java params to	0.045455
a right outer join of c{self} and c{other}	core rdd full outer join	0.200000
dummy params instance used as	params dummy	0.111111
creates a model	create	0.017241
wait	query manager reset	0.011905
dump already partitioned data into	spill	0.038462
perform a pearson's independence test using	chi square test test	0.333333
are the left singular vectors of	linalg singular	0.017544
of the list	sql	0.005051
can be used again to wait for new	sql streaming query	0.011765
load labeled points	mlutils load labeled points	1.000000
output a python rdd of key-value pairs (of	core rdd	0.010381
does this configuration contain a given key?	core spark conf contains key	0.333333
system using the old hadoop	hadoop	0.050000
finding frequent items for	data frame freq items	0.166667
return	mllib	0.010526
a given string name	param params has param	0.019231
wait until any of the queries on	query manager await any termination timeout	0.166667
an rdd with the keys of	rdd keys	0.250000
performs the kolmogorov-smirnov ks test for data sampled	mllib stat statistics kolmogorov smirnov test data	0.111111
column mean values	standard scaler model mean	0.125000
data into	core external group	0.045455
extract the day of the month of	dayofmonth	0.027027
awaitanytermination()	query	0.010753
of fit test of the observed	test observed	0.090909
sets	prediction col set	1.000000
function translate any character in the srccol by	sql translate srccol	1.000000
the content of the :class dataframe	sql data frame writer	0.058140
implementation of	ml	0.001835
completed	completed	0.411765
much	external merger	0.031250
the precision-recall curve which is a dataframe containing	binary logistic regression summary pr	0.083333
sparkcontext is	spark context	0.023256
vector to the new mllib-local representation	mllib linalg sparse vector as	0.333333
seed=none impurity="gini",	classifier	0.050000
add a py	core spark context add py file path	0.166667
replacing a value with another value	replace to_replace value subset	1.000000
specified by the optional key	key	0.017857
of deserialized objects from	serializer load	0.083333
queries so	streaming query manager reset	0.011905
the current [[dataframe]] and perform the	sql grouped data pivot pivot_col values	0.050000
contains	param params has	0.019231
thresholds thresholds in multi-class	thresholds	0.071429
the deviance for the	ml generalized linear regression summary deviance	0.125000
new :class dataframe sorted by the	data frame sort	0.125000
sort the list based on first value	sort result based on	0.333333
it	java_stage	0.444444
forget about past terminated	manager reset terminated	0.200000
synonyms of a	synonyms	0.125000
create a java	java	0.012195
the initial value of weights	with sgd set initial weights	0.333333
converts vector columns in	mlutils convert vector columns from	0.166667
distinct count	count distinct	0.080000
the cluster centers represented as a list of	mllib bisecting kmeans model cluster centers	0.083333
add a py or zip dependency for	context add py file	0.166667
function scaled loss + regularization at each iteration	history	0.181818
python topicandpartition	init topic partition	0.055556
convert this matrix to the new mllib-local representation	dense matrix as ml	0.333333
sets	features set	1.000000
the date	date	0.074074
of memory for	external	0.013889
compute the dot product of two vectors	ml linalg dense vector dot other	0.090909
list	sql	0.005051
set sample	set sample sample	0.333333
a streaming :class dataframe from	data stream	0.028571
glom	glom	1.000000
an	mllib power iteration clustering train cls	1.000000
the initial value	sgd set initial	0.111111
the dstreams	dstreams transformfunc	0.125000
of ensuring all received data has been processed	stopsparkcontext stopgracefully	0.050000
broadcast a read-only	spark context broadcast	0.125000
generate	streaming logistic regression with sgdtests generate	1.000000
buckets	numbuckets	0.142857
attr lda	ldamodel get	0.066667
sets the accumulator's value only	accumulator value	0.050000
local property that affects jobs submitted	local property key	0.035714
whose columns are the right singular vectors	linalg singular	0.017544
of row used in python repl	sql row repr	0.250000
all params with their optionally default	params explain params	0.250000
iterations default	iterations	0.043478
a dictionary of values	size values	0.250000
a dummy params instance used as a placeholder	param params dummy	0.111111
featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online", learningoffset=1024	featurescol maxiter seed	0.200000
new accumulator with	accumulator	0.012987
of names of tables in	table names	0.066667
comprised of vectors containing i i d	random rdds	0.076923
fast version of a heappush followed by a	heappushpop heap item	0.142857
queries so that :func awaitanytermination() can be used	query manager	0.011905
on first	on	0.037037
or transform the rdd of	tf transform	0.045455
resets the configuration	sql runtime config unset	0.142857
the model to make predictions on batches of	mllib streaming linear algorithm predict on	0.066667
list of names of tables in	names	0.050000
awaitanytermination() can be used	streaming	0.005025
of columns that describes the sort	frame sort cols cols kwargs	0.142857
column mean	mean	0.034483
boundaries in increasing order for which predictions are	isotonic regression model boundaries	0.333333
input dataset this is called by	dataset	0.020408
month	dayofmonth	0.027027
host	host	0.833333
greatest value of the	greatest	0.043478
large dataset and an item approximately find at	ml lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
wrapper for user-defined function registration	udfregistration	1.000000
to term frequency vectors or	tf	0.076923
start inclusive to end inclusive	range between start end	0.250000
on	on key outputs	0.333333
an array of	mllib	0.010526
the week number of	weekofyear col	0.055556
queries	streaming query manager	0.011236
setparams(self mindocfreq=0 inputcol=none outputcol=none) sets params for	set params mindocfreq inputcol outputcol	0.333333
of this instance this updates	ml param params	0.013699
partial	save partial obj	0.125000
fdr	fdr	1.000000
original column during fitting	original	0.047619
kmeans algorithm for fitting	kmeans	0.025641
merge the values for each key	key func	0.066667
convert to sparsematrix	dense matrix to sparse	1.000000
already partitioned data into disks	external	0.013889
for this obj assume	obj	0.023810
transform the	transform	0.062500
model's transform method	ml logistic regression summary predictions	0.200000
inputcol=none outputcol=none	inputcol outputcol relativeerror	1.000000
a number in	sql conv	0.250000
all the objects	size	0.009174
sort	py spark streaming test case sort result	0.333333
generates an rdd comprised of vectors containing	mllib random rdds gamma vector rdd sc	0.200000
the norm of	ml linalg dense vector norm p	0.333333
sort	sort	0.666667
infer schema from an rdd of row or	spark session infer schema rdd	0.250000
group the values	group	0.025641
queries so that :func awaitanytermination() can be	query manager	0.011905
for feature selection by percentile	chi sq selector set percentile percentile	0.200000
for a given product and returns	product	0.029412
new dstream in which each rdd is generated	streaming streaming context	0.032258
data	core external group	0.045455
which is a risk function corresponding to	linear regression summary	0.013889
eager	eager	1.000000
this vector	vector	0.019231
call java function	mllib call java func sc func	1.000000
area	classification metrics area	0.333333
of the rdd	rdd partition by	0.062500
the initial value of weights	sgd set initial weights	0.333333
the levenshtein distance of	sql levenshtein	0.058824
compute the dot product of two	ml linalg dense vector dot	0.090909
values of the accumulator's data	accumulator	0.012987
binary operator this	op name doc	0.166667
this	core accumulator add	0.076923
that makes a class inherit documentation from its	inherit doc	0.045455
the	streaming	0.005025
create a sparse vector using either a	vectors sparse	0.166667
and commutative reduce function	core rdd reduce	0.083333
return a new dstream by applying reducebykey to	streaming dstream reduce by key	0.076923
an input stream that is to	stream ssc addresses storagelevel	0.166667
all values as	all	0.083333
from checkpoint data or create	streaming context get or create	0.200000
or create global	or create cls	1.000000
java pipeline create	pipeline from java	0.142857
given	param params has	0.019231
into	external group	0.045455
levenshtein distance of the	sql levenshtein left right	0.058824
:class dataframe as	data frame writer save as	0.071429
infer schema from an rdd of row or	sql spark session infer schema rdd	0.250000
find all globals names read or written to	core cloud pickler extract code globals	0.125000
returns the explained variance regression score	regression metrics explained variance	0.333333
each original column during	ml min max scaler model original	0.062500
given	sql	0.022727
wrapper of	ml java params	0.125000
property set in this thread or	property key	0.066667
cluster centers represented as a list	mllib bisecting kmeans model cluster centers	0.083333
how much of memory	core	0.003021
module	module	0.666667
in the	mllib	0.010526
for the stream	data stream	0.028571
table accessible via jdbc url url and	jdbc url table column lowerbound	0.166667
an rdd of labeledpoint	lib svmfile sc path numfeatures	0.125000
contains a param with a given string name	ml param params	0.013699
set a configuration property if not already set	core spark conf set if missing	0.500000
indices back	index	0.041667
this matrix to a rowmatrix	coordinate matrix to row matrix	0.333333
the documentation of	ml param	0.009524
center for this model	model	0.011173
attr predictions which gives the predicted	linear regression summary prediction	0.142857
as the spark	core spark context	0.011628
extract the week number of a	sql weekofyear	0.055556
for the stream query if this is	data stream	0.028571
into disks	by	0.014286
to an external database table via	url table mode properties	0.200000
either recreate	checkpointpath setupfunc	1.000000
pulls events from flume	streaming flume utils	0.200000
input	frame reader	0.400000
matrix columns in an input dataframe from the	matrix columns to ml	0.142857
wait for new	streaming	0.005025
any hadoop file system using the old hadoop	as hadoop	0.142857
matrix to a rowmatrix	linalg coordinate matrix to row matrix	0.333333
dump already partitioned	group by	0.041667
sets	elementwise product set	1.000000
in libsvm format into label indices values	mllib mlutils parse libsvm	0.125000
test that the final value of weights is	with sgdtests test	0.111111
seed random seed	seed	0.111111
name	ml param params has param	0.019231
decimal decimal decimal data type	decimal type	1.000000
the day of the month of a	sql dayofmonth	0.031250
'cogroup' between	cogroup	0.166667
returns a paired	matrix factorization model	0.043478
this bucketizer	ml bucketizer	0.250000
converts vector columns in an input dataframe	mlutils convert vector columns from ml dataset	0.166667
accumulator's value only usable in driver program	core accumulator	0.030303
list of names of tables	table names	0.066667
that	to	0.007692
instance to a	to	0.015385
wait	streaming context await termination timeout	0.166667
returns the least value of the list	least	0.043478
values	values	0.400000
by :func	sql streaming	0.010204
registered with the dispatch to handle all function	cloud pickler save function obj name	0.142857
:py attr learningoffset	learning offset value	1.000000
property if not already set	set if missing key	1.000000
alsmodel	alsmodel	0.357143
converts vector columns in an	mllib mlutils convert vector columns to	0.166667
the soundex encoding for a string >>> df	sql soundex	0.055556
on a flume	flume	0.071429
used again to wait	streaming query manager reset	0.011905
for this imputer	imputer	0.100000
setparams(self labelcol="label", featurescol="features",	ml generalized linear regression set params labelcol featurescol	1.000000
labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256	labelcol predictioncol maxdepth	0.333333
get or compute the number of rows	matrix num rows	0.200000
sparkcontext	context spark	0.083333
a column of the current [[dataframe]] and perform	data pivot pivot_col values	0.050000
predict values for a single data point or	predict	0.068966
cost sum of squared distances	cost	0.105263
risk function corresponding to the expected value of	ml	0.001835
the correlation of two	corr col1 col2 method	0.055556
test that coefs are predicted accurately by fitting	with tests test parameter accuracy	0.333333
only create a new hivecontext for	hive context create for	0.250000
new dstream by applying reducebykey to each	streaming dstream reduce by key func numpartitions	0.076923
boolean	boolean	0.875000
a value to an int if possible	param type converters to int	0.250000
using the new hadoop outputformat api mapreduce	new apihadoop dataset conf keyconverter valueconverter	0.142857
implicitprefs	implicit prefs	1.000000
add a py	add py file	0.166667
class	cls	0.047619
set	utils set	1.000000
code for	code	0.071429
converts matrix columns	convert matrix columns from	0.166667
delim	delim	1.000000
sliding window of	window	0.037037
frequency vectors or	tf	0.076923
the selector type of the	chi sq selector set selector type	0.111111
which is defined	regression	0.010000
sparse vector using either a	mllib linalg vectors sparse	0.166667
of this instance this updates both	ml param	0.009524
that :func awaitanytermination() can be	manager	0.011236
and return	core	0.009063
linear regression model	linear regression model	0.066667
modified to support __transient__ on	cloud pickler save reduce func args state listitems	0.111111
a matrix from the new mllib-local representation	mllib linalg matrices from ml	0.333333
is vector conduct pearson's chi-squared goodness	mllib stat statistics chi sq	0.066667
can be used again to	streaming query manager	0.011236
on a model with weights	mllib streaming linear regression with	0.111111
predict values for a single data	predict x	0.033898
matrix columns in an input dataframe	matrix columns from	0.142857
sliding window over	value and window windowduration slideduration numpartitions	0.076923
spark fair scheduler	spark context	0.023256
a script with a dependency on	dependency	0.040000
weights is close to	parameter accuracy	0.029412
setparams(self labelcol="label",	linear regression set params labelcol	1.000000
used with the spark sink	ssc addresses storagelevel maxbatchsize	0.045455
using l{rdd saveaspicklefile} method	spark context pickle file name	1.000000
path a shortcut of write() save path	ml one vs rest save path	0.200000
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online",	ml lda init featurescol maxiter seed	0.250000
curve which is a dataframe having two fields	binary logistic regression summary	0.111111
:func awaitanytermination() can be used again	sql streaming	0.010204
convert matrix attributes which are array-like or	ml linalg matrix convert	0.166667
an associative and commutative reduce function	core rdd reduce	0.083333
convert each python object into java	rdd to java	0.333333
field in "predictions" which gives the features of	ml linear regression summary features col	0.166667
date	next day date	0.100000
to	core	0.003021
computes an	mllib	0.010526
find synonyms of	word2vec model find synonyms	0.333333
numpartitions	numpartitions	0.625000
dataframe from	from	0.045455
:class dataframe to a	frame	0.034483
current [[dataframe]] and perform the	grouped data pivot pivot_col values	0.050000
infer schema from an rdd of row	session infer schema rdd samplingratio	0.250000
that :func awaitanytermination() can be	streaming query	0.010526
prediction scores into 0/1 predictions	mllib linear classification model	0.142857
finding frequent items for columns	sql data frame freq items cols	0.166667
of memory for this obj assume	merger object size obj	0.040000
create a multi-dimensional rollup for the	sql data frame rollup	0.055556
test that coefs are predicted accurately by fitting	regression with tests test parameter accuracy	0.333333
this obj assume	core external merger object size obj	0.040000
for the termination of	termination timeout	0.041667
started	started outputoperationstarted	0.125000
this model	generalized linear regression model	0.200000
distributed matrix on	matrix	0.015152
loads vectors saved	load vectors sc path	0.333333
of the rdd's elements	core rdd	0.003460
awaitanytermination() can be used again to	streaming query manager	0.011236
objects	merger object size	0.032258
for params	params	0.006623
inputcol input	input	0.090909
clusters	kmeans model k	0.250000
get the root	root	0.035714
param with	ml param	0.009524
behavior when data	data frame writer mode savemode	0.071429
squared distance from a sparsevector	mllib linalg sparse vector squared distance	0.166667
that all the objects	core	0.003021
c{other}, return a resulting rdd that contains a	core rdd cogroup other numpartitions	0.066667
param with a given string name	ml	0.001835
python object	obj	0.047619
train the model	streaming kmeans train	1.000000
stream returning	stream reader	0.076923
the embedded	param params	0.014925
columns are the right singular vectors of the	linalg singular	0.017544
this dataset checkpointing can be used to truncate	frame checkpoint eager	0.071429
so that :func awaitanytermination() can be	streaming	0.005025
sets the given spark runtime configuration property	runtime config set	1.000000
into disks	by spill	0.047619
contains a param with a given string	param params has	0.019231
column name for the	col	0.016393
items for columns	items cols support	0.125000
given a java pipeline create and return a	pipeline from java cls	0.200000
depth of tree (e g	tree model	0.026316
format into an rdd of labeledpoint	mllib mlutils load lib svmfile sc path numfeatures	0.125000
the kmeans algorithm for fitting and	streaming kmeans	0.035714
a param	ml param params has	0.019231
access fields	sql struct type getitem key	0.200000
again to wait for	sql	0.002525
model that has a vector	model	0.005587
compute the dot product of two vectors	ml linalg dense vector dot	0.090909
dict	dict	1.000000
offsetrange of specific	offset ranges	0.166667
load a model	model load cls sc	0.250000
deviance for the	deviance	0.111111
column for distinct count of	count distinct col	0.040000
n elements from an rdd ordered in ascending	core rdd take ordered	0.050000
number of nonzero elements this	mllib linalg sparse vector num	0.200000
contains a param with	ml param params	0.013699
for this udt	sql user defined	0.500000
mean variance and count of the rdd's elements	core	0.003021
this grid to	add grid	0.500000
columns if specified	sql data frame	0.005348
value of	ml param has seed	1.000000
the year of a given	sql year	0.050000
regression score	linear regression summary	0.013889
range	offset range	0.047619
an numpy ndarray	array	0.100000
train the model on the incoming dstream	mllib streaming kmeans train on dstream	1.000000
__init__(self	ml tokenizer init	1.000000
applies transformation on a vector	mllib vector transformer transform vector	1.000000
awaitanytermination()	streaming	0.005025
comprised of vectors containing i i	mllib random rdds poisson vector	0.125000
content of the :class dataframe in	sql data frame	0.016043
this matrix to a rowmatrix	mllib linalg indexed row matrix to row matrix	0.333333
or compute the number of rows	linalg row matrix num rows	0.200000
dump already partitioned data into disks	by spill	0.047619
prefix of string in doc tests to	prefix	0.083333
comprised of vectors	mllib random rdds normal vector	0.125000
computes average values for each numeric columns	grouped data avg	0.058824
return an rdd containing all pairs of elements	rdd	0.003058
returns weighted true	multiclass metrics weighted true	1.000000
function to the value of each key-value	map values f	0.125000
note : experimental	core spark context binary files path minpartitions	1.000000
of memory for this obj assume	core external merger object size obj	0.040000
can be used again to wait	sql streaming query	0.011765
two columns of a dataframe	data frame	0.005000
dump already	core external	0.016129
the python direct kafka stream	kafka direct stream	0.277778
the minimum number of times a	min count	0.076923
mixture	mllib gaussian mixture model	0.125000
an rdd comprised of	mllib random rdds log normal vector rdd	0.166667
tree <http //en wikipedia org/wiki/decision_tree_learning>_	tree regressor	0.058824
this sqltransformer	sqltransformer	0.125000
represents singular	singular	0.015625
number	sparse vector num	0.200000
set	set	0.106509
the approximate	approx	0.047619
all mixture	gaussian mixture	0.038462
precision-recall curve which is a dataframe containing	binary logistic regression summary pr	0.083333
data types	type	0.073171
the distinct	distinct	0.055556
version	heap	0.047619
classes values which the label can take	classes	0.034483
the input	stream reader	0.076923
converts matrix columns in	convert matrix columns to ml	0.166667
how	object size	0.032258
given parameters in this grid to	param grid builder base on	0.076923
on a	mllib streaming logistic regression with	0.500000
limit	limit	0.461538
in this grid to fixed	grid builder add grid param	0.250000
python topicandpartition to map to	streaming topic and partition init topic partition	0.055556
converts a	mlutils convert	0.166667
value	add	0.035714
return an rdd	rdd	0.006116
the levenshtein distance of the two given strings	sql levenshtein left right	0.058824
for each key using an associative	key func	0.066667
a local representation this discards	local	0.038462
rdd of key-value pairs	core rdd save	0.037975
parses a column containing a json string into	sql from json col	0.083333
python wrapper of	ml java params from	0.250000
the given block matrix other from this	linalg	0.022222
start offset	from offset	0.125000
compute the sum	data sum	0.333333
date	sql date	0.333333
sets the given parameters in	grid builder	0.055556
instance is of type distributedldamodel	ml ldamodel is	0.066667
sql storage	sql	0.002525
new dstream in which each rdd	streaming streaming context	0.032258
indicates whether this instance is of	ml ldamodel is distributed	0.066667
the kolmogorov-smirnov ks test for data	stat statistics kolmogorov smirnov test data distname	0.111111
norm of a sparsevector	mllib linalg sparse vector norm p	0.083333
the sum of	ml bisecting	0.066667
until any of the queries on the associated	sql streaming query manager await any	0.142857
__init__(self inputcol=none outputcol=none seed=none numhashtables=1)	lsh init inputcol outputcol seed numhashtables	1.000000
dict into a jvm map	scala map sc jm	0.200000
first date which is later than	dayofweek	0.037037
sets the sql context to use for loading	java mlreader context sqlcontext	0.333333
hadoop file system using the old hadoop	save as hadoop	0.142857
to fit an intercept term	fit intercept	0.250000
:func awaitanytermination()	query manager	0.011905
embedded	params	0.006623
worker	worker	1.000000
a paired	factorization	0.038462
levenshtein distance of the two	sql levenshtein	0.058824
contains a param with a given string name	params has param	0.019231
the training set	training	0.029412
1 0] for feature selection by percentile	mllib chi sq selector set percentile percentile	0.200000
represents a	offset	0.021739
for this obj assume that all the	size obj	0.040000
for the test this should be list	test	0.015152
of byte array that starts at pos	pos	0.022222
generates python code for a	code name	0.111111
tree	tree	0.375000
that :func	streaming query manager	0.011236
to a java pipelinemodel used for ml	ml pipeline model to java	0.100000
the first n rows to the console	data frame show n	0.333333
:func	query manager	0.011905
layers	multilayer perceptron classification model	0.500000
seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto",	random forest classifier	0.022727
tree ensemble	tree ensemble	0.055556
the log likelihood	log likelihood	0.125000
udf with a function	function	0.027778
dump	core	0.003021
the month	dayofmonth	0.027027
of this dstream and other	other numpartitions	0.083333
and profiles the method to_profile passed	core basic	0.066667
params for	params featurescol	0.500000
a list of numpy arrays	ml bisecting	0.066667
explained variance regression score	linear regression summary explained variance	0.333333
columns are the left singular	mllib linalg singular	0.017544
the accumulator's value	core accumulator	0.030303
length of a string	sql length col	0.050000
returns a :class dataframe representing	spark session sql sqlquery	0.250000
comprised of vectors containing i i d samples	random rdds gamma	0.125000
the file	file	0.028571
save this model to the given path	java saveable save sc path	1.000000
__init__(self labelcol="label",	generalized linear regression init labelcol	1.000000
commutative reduce	rdd reduce by	0.200000
used again to wait	streaming query manager	0.011236
returns the documentation of	ml	0.001835
basic operation test for dstream mapvalues	streaming basic operation tests test map values	1.000000
gaussians in mixture	gaussian mixture model k	0.200000
given product and returns a list of rating	product	0.029412
left outer join of c{self} and	core rdd left outer join other	0.200000
and vector	get vectors	0.142857
pairs in this dstream	streaming dstream	0.055556
train the model on	linear regression with sgd train on	0.333333
the area under	classification metrics area under	0.166667
defines	spec	0.076923
the left singular vectors of the singularvaluedecomposition	mllib linalg singular value decomposition	0.250000
the dot product	linalg dense vector dot	0.058824
with	has	0.011628
two vectors we support	linalg dense vector	0.200000
code for a shared param class	param gen param code	0.333333
bucketize rows into one or more time windows	timecolumn windowduration slideduration starttime	1.000000
test the partition	tests test partition	1.000000
convert this matrix to a coordinatematrix	mllib linalg block matrix to coordinate matrix	0.333333
labeledpoint	load lib svmfile sc path numfeatures	0.125000
called when processing of a batch	listener on batch	0.333333
rdd of labeledpoint	lib svmfile	0.125000
inputformat with arbitrary key and value class from	inputformatclass keyclass valueclass	0.125000
coefs are predicted accurately by fitting on toy	parameter accuracy	0.029412
outer join	outer join other	0.333333
the minutes of a given date as integer	minute	0.040000
returns a java storagelevel based on a	core spark context get java	0.333333
rdd is	rdd is	1.000000
log probability of	ml distributed ldamodel log prior	1.000000
input	estimator	0.083333
binary operator this object is on right side	reverse op name doc	1.000000
__init__(self featurescol="features", labelcol="label",	regression init featurescol labelcol	1.000000
instance contains a	param	0.012500
seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)	ml random forest classifier	0.023256
line in libsvm format into label	parse libsvm line line multiclass	0.111111
copy of the rdd	rdd partition by	0.062500
recall	recall	1.000000
orc files returning the	orc	0.083333
soft	soft	0.833333
the	object	0.027778
returns a new	java wrapper new	0.333333
"zerovalue" which may be added	fold by	0.125000
results immediately to	locally func	0.142857
basic operation test for dstream groupbykey	basic operation tests test group	1.000000
a unique identifier for the spark application	core spark context application id	0.500000
i d samples drawn	shape scale numrows	0.125000
contains a	params has param	0.019231
training set given the current parameter estimates	ldamodel training	0.034483
:func awaitanytermination() can be used	streaming	0.005025
a dummy params instance used as a	param params dummy	0.111111
root mean squared error which is defined	linear regression summary	0.013889
dataset	dataset featurescol labelcol	1.000000
of memory for this obj assume that all	size obj	0.040000
index	index	0.250000
2 ml params instances for	params m1	0.047619
the mean	metrics mean	0.333333
find synonyms of a word	word2vec model find synonyms word	1.000000
stream query if this is not set it	sql data stream writer	0.041667
set a configuration property if not already set	spark conf set if missing key value	0.500000
predict values	predict	0.068966
number of	dense vector num	0.166667
date which is later than the value of	dayofweek	0.037037
the correlation of two	method	0.041667
return	standard scaler	0.076923
arbitrary key and	core spark	0.020619
a sparse vector using either	linalg vectors sparse	0.166667
instance contains a param with a given	param	0.012500
parses a column	col	0.016393
block matrix other from this	mllib linalg	0.026316
synonyms of	synonyms	0.125000
set the selector type of the	set selector type	0.111111
the output by the given	writer bucket by	0.100000
collection to form an rdd using xrange	core spark context parallelize c numslices	0.250000
term to	core accumulator add term	0.066667
on a flume	streaming flume	0.111111
copies param	ml param	0.009524
compute the	mllib linalg distributed matrix	0.333333
singular vectors of the singularvaluedecomposition	mllib linalg singular value decomposition	0.250000
the given	mllib	0.010526
kmeans algorithm for fitting and predicting on	streaming kmeans	0.035714
value of	ml param has max iter	1.000000
add a file to be	context add file path	0.333333
two columns of a dataframe as	sql data frame	0.005348
losstype="logistic", maxiter=20	gbtclassifier	0.076923
trees model for classification or regression	trees	0.066667
a term	add term	0.066667
a function rdd[x] -> rdd[y]	transform function	0.166667
a param with a	param params has param	0.019231
of fit test of	test	0.015152
compute the standard deviation	core rdd stdev	0.066667
can	streaming query	0.010526
defined on	ml param	0.009524
the accumulator's value only	core accumulator	0.030303
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	regressor	0.043478
this instance contains	params	0.006623
load a model from the given path	loader load cls sc path	1.000000
regexp	regexp	0.461538
a list of active queries associated with	manager active	0.066667
value for each original column during fitting	original	0.047619
compute	linalg indexed row matrix	0.250000
of the	core rdd	0.003460
large dataset and an item approximately	ml lshmodel approx nearest neighbors dataset	0.166667
featurescol="features", labelcol="label", predictioncol="prediction",	featurescol labelcol predictioncol	0.727273
points to their nearest center for this model	model	0.011173
set a java system property such as spark	spark context set system property cls	1.000000
contents of the :class dataframe to	frame	0.034483
for each numeric	sql grouped data	0.083333
dummy params instance used as	param params dummy	0.111111
into an rdd of labeledpoint	mllib mlutils load lib svmfile sc	0.125000
for efficiency can also update c{value1}	value1 value2	0.250000
such as the spark fair scheduler	core spark	0.010309
given value to scale decimal places using	sql	0.002525
the python direct kafka stream api with start	kafka direct stream from	0.125000
similarities between columns of this	column similarities threshold	0.333333
for all test results	test result	0.200000
the training set given the	training	0.029412
generates an rdd	normal vector rdd sc	1.000000
a	core spark	0.030928
for efficiency can also update c{value1} in place	in place value1 value2	0.500000
the explained variance regression score	mllib regression metrics explained variance	0.333333
save a	core cloud pickler save	0.166667
two	linalg dense vector	0.200000
checkpointinterval=10 impurity="variance",	regressor	0.043478
this instance contains a param with a given	param	0.012500
load a java model from the given	java loader load java cls sc	0.200000
set initial centers should be set before	initial centers centers weights	0.200000
that :func	sql streaming query manager	0.011905
which is a risk function	regression	0.020000
for which predictions are	isotonic regression model	0.100000
a dataframe with two fields threshold	threshold	0.036364
adds input	stream reader	0.076923
the stream query if this is not set	sql data stream	0.031250
of this instance with a randomly generated	ml train validation split model	1.000000
__init__(self	fpgrowth init	1.000000
globals names read or written to by codeblock	cloud pickler extract code globals cls	1.000000
two-sided p-value of estimated coefficients and	ml linear regression summary p values	0.333333
such as the spark	core spark	0.010309
waits for the termination of this query	termination	0.035714
until any	manager await any	0.142857
an rdd of labeledpoint	mllib mlutils load lib svmfile	0.125000
value of	ml quantile discretizer	1.000000
awaitanytermination() can	sql streaming	0.010204
data or table already	data	0.011628
inputcol=none outputcol=none handleinvalid="error")	inputcol outputcol handleinvalid	1.000000
return an rdd created by piping elements to	core rdd	0.003460
directory	directory	0.833333
dstream by applying 'left	left	0.066667
wrap this udf with a function	sql user defined function	0.083333
average values for each numeric columns	sql grouped data avg	0.058824
frame boundaries from start inclusive to end inclusive	range between start end	0.250000
retrieve gaussian	gaussian	0.100000
inst	inst	1.000000
dispatch to handle all function types	cloud pickler save function obj name	0.142857
metaclass for datatype	data type singleton	1.000000
checkcode	checkcode	1.000000
property that affects jobs submitted from this thread	property key	0.066667
be used again to wait	sql	0.002525
a randomly	cross validator	0.045455
infer schema from an rdd of	sql sqlcontext infer schema rdd	0.250000
a python topicandpartition to map to	and partition init topic partition	0.055556
broadcast a	context broadcast	0.125000
date1 and date2	date1 date2	1.000000
metrics	metrics	0.208333
broadcast a read-only variable to	context broadcast value	0.125000
of fields in	sql	0.002525
get depth of tree (e g depth	tree model	0.026316
dot product of two vectors	linalg dense vector dot	0.058824
sample without replacement based on the	data frame sample	0.066667
accessible via jdbc url url and connection properties	reader jdbc url	0.250000
default min number of partitions for hadoop	core spark context default min partitions	0.250000
basic operation test for dstream groupbykey	basic operation tests test	0.111111
standard deviation of this rdd's elements	core rdd stdev	0.066667
test	tests test count	1.000000
value in the key-value	map values	0.166667
this instance contains	has param	0.019231
matrix to	matrix	0.030303
much of memory	external merger object	0.032258
the srccol	srccol	0.142857
value for each original	original	0.047619
a python wrapper of	ml java params	0.125000
java_model to a python model	discretizer create model java_model	0.250000
a python rdd of key-value	rdd	0.009174
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance",	decision tree regressor	0.058824
naivebayes	naive bayes	0.142857
map	map	0.470588
with the ordering defined	order by	0.142857
creates an external table based on the	sqlcontext create external table tablename path	0.250000
a python topicandpartition to map to	partition init topic partition	0.055556
param	has	0.011628
only create a	sql hive context create	0.083333
__init__(self withmean=false withstd=true inputcol=none outputcol=none)	ml standard scaler init withmean withstd inputcol outputcol	1.000000
list of columns for the given table/view in	catalog list columns	0.166667
contains a param with a	ml param params has param	0.019231
a densevector with singular	mllib linalg singular	0.017544
a python rdd of key-value pairs	core rdd save	0.037975
vector representation of each word in vocabulary	mllib word2vec fit	0.200000
convert to	ml linalg dense matrix to	1.000000
note : experimental	regression evaluator	1.000000
ensemble	tree ensemble	0.166667
how	core external merger	0.032258
return sparkcontext which is associated with this streamingcontext	spark context	0.023256
to support __transient__ on new objects	cloud pickler save reduce func args state listitems	0.111111
default or minimized false	ml evaluator is larger better	0.166667
a lower bound on the log likelihood	ldamodel log likelihood dataset	0.142857
how much of	external merger object	0.032258
returns the root mean	metrics	0.041667
'with sparkcontext as sc app' syntax	core spark context exit type value trace	0.333333
:class dataframe to a data	data frame writer save path format	0.142857
vectors saved using	vectors sc path	0.333333
setparams(self inputcol=none outputcol=none) sets params for	set params inputcol outputcol	0.333333
converts vector columns in	convert vector columns from ml dataset	0.166667
partial objects do not serialize correctly in	partial obj	0.125000
the model's transform method	regression summary predictions	0.200000
matrix other from this block matrix	mllib linalg block matrix	0.052632
containing a json string into a	json	0.043478
an input stream that	stream ssc	0.181818
onevsrest create and return a python	one vs rest from	0.142857
this broadcast on the executors if the	core broadcast unpersist blocking	0.500000
to stdout	core profiler show	0.166667
adds a term to this accumulator's value	term	0.080000
partitioned data into	core	0.003021
instance to a java onevsrest used for	ml one vs rest to java	0.166667
array containing the ids of all	stage ids	0.055556
the python module	module cls	0.333333
containing the ids of all active stages	status tracker get active stage ids	0.250000
prefix	prefix	0.500000
value of	ml param has raw prediction col	1.000000
fp-growth model that contains frequent	fpgrowth train cls	0.200000
param with a given	params has param	0.019231
the second is an array	mllib matrix	0.047619
perform a left outer join of c{self} and	core rdd left outer join	0.200000
bound on the log	ldamodel log	0.125000
dot product of two vectors we support	mllib linalg dense vector dot	0.058824
dataframe outputted by the model's transform method	ml logistic regression summary predictions	0.200000
of the :class dataframe in	sql data frame writer	0.034884
the same param	param	0.006250
number of partitions in rdd >>> rdd =	core rdd get num partitions	1.000000
for na fill()	sql data frame fillna value subset	0.166667
:func awaitanytermination() can be	streaming query manager	0.011236
given data type json	datatype json	0.333333
converts vector columns in an input dataframe from	convert vector columns to ml dataset	0.166667
the specified string value that match regexp	sql regexp	0.125000
a l{statcounter} object that captures	rdd stats	0.083333
a function	function	0.027778
the stream	data stream	0.028571
the selector type of the chisqselector	set selector type selectortype	0.333333
number of columns that make up each	block matrix cols per	0.333333
table/view in the specified	tablename	0.043478
stream query if this is not	data stream	0.028571
libsvm format into an rdd of labeledpoint	mllib mlutils load lib svmfile sc path	0.125000
dump	group	0.025641
compare 2 ml types	compare	0.071429
option for	reader option key	0.500000
much of memory for	core external merger	0.032258
adds	core accumulator add	0.076923
create a python topicandpartition	partition init topic partition	0.055556
hadoop outputformat api mapreduce package	apihadoop dataset conf keyconverter valueconverter	0.500000
sets	output col set	1.000000
of nodes summed over all trees in	nodes	0.037037
the l{sparkcontext} that this rdd was	core rdd context	0.166667
setparams(self featurescol="features", labelcol="label", predictioncol="prediction",	ml logistic regression set params featurescol labelcol predictioncol	1.000000
grid to fixed values	grid param values	1.000000
is later than the value of the	dayofweek	0.037037
in rdd >>> rdd = sc	core rdd get	0.250000
using an associative and commutative reduce function	core rdd reduce by	0.125000
recommends the	factorization model recommend	0.250000
this model	linear regression model	0.133333
create a multi-dimensional rollup for the current	sql data frame rollup	0.055556
be used again to wait for	sql	0.002525
create an rdd for dataframe from a	create from	0.500000
load a java model from the	mllib java loader load java cls sc	0.200000
set	spark conf set	0.111111
a file to	file path	0.035714
:func awaitanytermination() can be used again	streaming query	0.010526
names	table names	0.066667
for which predictions	regression	0.010000
a resulting rdd that contains	core rdd cogroup other numpartitions	0.066667
until any of the queries on the associated	any termination	0.142857
hint	hint	1.000000
add a py or zip dependency	spark context add py file	0.166667
number of partitions to use during reduce tasks	core rdd default reduce partitions	0.166667
each cluster for each training data point	gaussian mixture summary	0.200000
given parameters in this grid	ml param grid builder base	0.076923
pairs or two separate arrays of	ml	0.001835
correlation of two columns of a	col2 method	0.055556
that daemon and workers terminate on	core daemon tests test termination	0.166667
this obj assume that all the	object size obj	0.040000
this vector to the new mllib-local representation	mllib linalg sparse vector as	0.333333
in byte and is of length len	len	0.071429
ascending	ascending	1.000000
tokens in the training set given	training	0.029412
given a java onevsrest	one vs rest from java cls	0.200000
paired rdd where	matrix factorization model	0.043478
timestamp datetime datetime data type	timestamp type	1.000000
approximately find at most k items which have	lshmodel approx	0.100000
linear model that has	linear model	0.066667
a streaming dataframe/dataset is written to	stream writer output mode outputmode	0.083333
new terminations	reset	0.011236
recommends the top	matrix factorization model recommend	0.250000
of c{self} and	core rdd	0.006920
convert a dict into a jvm map	to scala map sc jm	1.000000
cluster centers represented as a	mllib bisecting kmeans model cluster centers	0.083333
computes column-wise summary statistics	statistics col stats	0.200000
average values for each numeric columns for	grouped data	0.035714
null values alias for na fill()	frame fillna value subset	0.166667
this instance's params to the wrapped java	java params to	0.045455
of names of	names	0.050000
the companion	java params transfer	0.125000
strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none) sets params	set params strategy missingvalue inputcols outputcols	0.200000
a dictionary	init size	0.066667
using the old hadoop outputformat api mapred	hadoop dataset conf keyconverter valueconverter	0.083333
a list of active queries associated with this	streaming query manager active	0.066667
text file	text path	0.333333
recent	recent	1.000000
used again to wait	query manager reset	0.011905
boundaries defined from start	between start	0.100000
text format or newline-delimited json	json	0.043478
underlying sql storage type for	sql type cls	0.250000
or none if the stage info	stage info	0.142857
for this obj assume that	external merger object size obj	0.040000
dataframe outputted by the model's transform method	summary predictions	0.153846
resets the	runtime config unset	0.142857
inherited by any streaminglinearalgorithm	streaming linear algorithm	0.076923
the content of the :class dataframe to	sql data frame writer	0.011628
the points belongs to in	predict x	0.033898
profiling	profiler	0.090909
union	context union	0.333333
rdd an rdd	power iteration clustering train cls rdd	0.250000
called when processing	listener on	0.200000
to	sql	0.002525
parammap into a	to	0.007692
so that :func awaitanytermination()	streaming query manager reset	0.011905
saves the content of the :class dataframe in	sql data frame	0.005348
infer schema from an rdd	session infer schema rdd samplingratio	0.250000
rdd an rdd of	clustering train cls rdd	0.250000
0 modeltype="multinomial", thresholds=none weightcol=none)	ml naive bayes	0.250000
maxheap variant of _siftdown	core siftdown max heap startpos pos	1.000000
the cluster centers represented as a list of	model cluster centers	0.060606
the accumulator's data type returning a new value	accumulator	0.012987
given a java object	java	0.012195
globals names read or written to	pickler extract code globals	0.125000
of memory for this obj assume that all	core external merger object size obj	0.040000
set the selector type	sq selector set selector type	0.111111
value of	ml tree regressor	1.000000
file with the	file	0.028571
point to programming	session	0.050000
set the initial value of weights	regression with sgd set initial weights	0.333333
the points belongs to	predict x	0.033898
extract the minutes of	sql minute col	0.050000
create a python topicandpartition to map to	and partition init topic partition	0.055556
the mean variance and count of the rdd's	core rdd	0.003460
name of the test method	mllib stat chi sq test result method	0.250000
left outer join	rdd left outer join	0.111111
contents of the :class dataframe to a	frame writer	0.050000
jvm seq of column	seq sc cols converter	0.055556
named table accessible via jdbc url url and	reader jdbc url table column	0.166667
partitioned data into	spill	0.038462
squared distance from a sparsevector or 1-dimensional	linalg sparse vector squared distance other	0.166667
list of columns for	list columns	0.166667
returns a	spark session sql	0.250000
calculates the correlation	col2 method	0.055556
squared distance from a	linalg sparse vector squared distance other	0.166667
which is a	regression	0.030000
a single script file calling a global	script with local functions	0.125000
with the spark sink deployed	ssc addresses storagelevel maxbatchsize	0.045455
densematrix whose columns are the right singular vectors	singular	0.015625
variance and count of the rdd's elements	core rdd	0.003460
returns subset accuracy	mllib multilabel metrics subset accuracy	1.000000
the value of spark sql	sql sqlcontext get	0.333333
agg	agg	0.833333
standard deviation of this rdd's elements	stdev	0.047619
weights for each tree	ensemble model tree weights	0.333333
for	sql streaming query manager reset	0.011905
"func" and a	core rdd	0.003460
an 'old' hadoop	core spark context hadoop	0.333333
correlation of two columns of	col1 col2 method	0.055556
on the log likelihood	ldamodel log likelihood	0.142857
sort the list based on first value	sort result based on key	0.333333
object into java	py2java sc obj	0.333333
as a temporary table in	as table df	0.250000
this matrix to a coordinatematrix	mllib linalg indexed row matrix to coordinate matrix	0.333333
heap maintaining the heap invariant	core heappop heap	0.142857
of terms to term frequency vectors or transform	tf transform	0.045455
sets params for	set params featurescol labelcol predictioncol classifier	0.500000
computes the levenshtein distance	levenshtein left right	0.058824
queries so that :func awaitanytermination()	sql streaming	0.010204
python rdd of key-value	core rdd save	0.037975
sampled subset of	sample withreplacement fraction seed	0.333333
save this model to the given path	logistic regression model save sc path	1.000000
a python object into an internal sql object	to internal obj	0.500000
create an input stream that pulls events from	create stream	0.200000
get or compute the number of cols	mllib linalg block matrix num cols	0.333333
used again to wait for new terminations	manager reset	0.011905
field in :py attr predictions which gives the	generalized linear regression summary	0.090909
mixin for param fitintercept whether to fit	has fit	1.000000
the norm of	vector norm	0.055556
this	merger	0.025641
extract the year	year	0.040000
the soundex encoding for a string	sql soundex col	0.055556
sum of	ml bisecting kmeans model	0.076923
internal use only create a new	hive context create	0.083333
awaitanytermination() can be used	sql	0.002525
udfregistration	sqlcontext	0.038462
return a l{statcounter} object that captures the mean	stats	0.055556
of this dataset checkpointing	checkpoint eager	0.250000
a new dstream in which	streaming streaming context	0.032258
a large dataset and an item	nearest neighbors dataset	0.333333
only create a new	context create	0.083333
hex	hex	0.833333
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none	regressor	0.043478
:class dataframe	frame writer save path format	0.066667
them with extra	extra	0.023810
function to each	f	0.010526
evaluate	evaluate	1.000000
centroids of that particular batch	timeunit	0.025641
:class dataframe replacing a value with another value	data frame replace to_replace value subset	1.000000
of estimated coefficients	ml generalized linear regression training summary	0.166667
is not contained	rdd subtract	0.333333
left multiplies this blockmatrix by other, another blockmatrix	mllib linalg block matrix multiply other	0.200000
given parameters in this grid	builder add grid	0.100000
a dummy params instance used as	params dummy	0.111111
hadoop	hadoop	0.300000
of the accumulator's data type returning a	accumulator	0.012987
be used again to wait for	streaming query manager reset	0.011905
be used again to wait for new terminations	manager	0.011236
testing	testing cls sparkcontext	1.000000
number of columns of blocks in the blockmatrix	mllib linalg block matrix num col blocks	1.000000
much of	size	0.009174
sparksession	sparksession	0.625000
forget about past terminated	sql streaming query manager reset terminated	0.200000
wait for new	manager	0.011236
the given parameters in this grid to fixed	builder add grid param	0.250000
an rdd comprised of	random rdds poisson vector rdd	0.166667
a condition to	condition	0.045455
the underlying :class sparkcontext	sql spark session	0.166667
for this dct	dct	0.125000
of the month of a given date	dayofmonth col	0.031250
of this dstream	streaming kafka dstream	0.250000
conditions and returns one	sql column otherwise	0.050000
jarray	jarray	1.000000
containing the distinct elements in this rdd	core rdd distinct numpartitions	0.250000
extract the day of the month of a	sql dayofmonth	0.031250
model params are set correctly	model params	0.125000
params instances for the given param	params m1	0.047619
from start	range start	0.500000
number of nonzero elements	linalg sparse vector num	0.200000
in csv format at the specified path	writer csv path mode compression sep	1.000000
by the default implementation of	ml estimator	0.125000
debug	debug	1.000000
separate arrays of indices and	ml linalg	0.030303
multiclass classification	classifier cls data numclasses categoricalfeaturesinfo	0.250000
property that affects jobs submitted from	property key	0.066667
infer schema from an rdd of row or	session infer schema rdd	0.250000
returns accuracy equals to the total	metrics accuracy	0.166667
single	single	0.777778
converts vector columns in	convert vector columns to ml	0.166667
an input from tcp source	context socket text stream	0.500000
can	query manager	0.011905
broadcast a	context broadcast value	0.125000
an rdd ordered in ascending order or	core rdd take ordered	0.050000
ordered in ascending order or as specified	take ordered	0.125000
newline-delimited json	writer json	0.125000
comprised	mllib random rdds	0.125000
a dependency	dependency	0.120000
representing a multiclass classification	linear classification	0.142857
specifies how data	sql data	0.024390
this instance's params to the wrapped java object	params to	0.035714
generates python code for a shared param class	param gen param code name doc	0.333333
again to wait for new terminations	streaming query	0.010526
param with a	ml param params	0.013699
casesensitive	casesensitive	1.000000
__init__(self splits=none inputcol=none	ml bucketizer init splits inputcol outputcol	1.000000
the dataframe in	sql data frame writer	0.011628
to each element	preservespartitioning	0.181818
that :func awaitanytermination() can be used	manager	0.011236
into label indices	mllib mlutils parse	0.250000
restore an object of namedtuple	restore name	0.333333
statistic functions with :class dataframe	data frame stat functions	0.333333
add a py	add py	0.166667
of determination	regression metrics r2	0.166667
this obj assume that all the	merger object size obj	0.040000
intercept computed	intercept	0.090909
parses a line in libsvm format into label	mlutils parse libsvm line line multiclass	0.111111
such as the spark	core spark context	0.011628
documentation of all params with their optionally	ml param params explain params	0.166667
get the residuals	summary residuals residualstype	0.333333
returns the receiver operating characteristic roc	roc	0.100000
wrapper	wrapper	1.000000
back to a new	index to	0.040000
as the square root of the mean	root mean	0.250000
of gaussians in mixture	gaussian mixture	0.038462
an iterator of deserialized batches lists	without unbatching	0.125000
creates an external table based	create external table tablename path	0.250000
pipelinemodel create and return a python	pipeline model	0.071429
so that :func awaitanytermination() can be used	streaming	0.005025
impurity="variance", seed=none	ml decision tree regressor	0.066667
specification that defines the partitioning ordering	spec	0.076923
that :func awaitanytermination() can be	manager reset	0.011905
__init__(self withmean=false withstd=true inputcol=none outputcol=none)	standard scaler init withmean withstd inputcol outputcol	1.000000
wait a given	timeout	0.071429
already partitioned data into	external	0.013889
maxheap version of	max heap item	0.250000
function to the value	values f	0.062500
this instance contains	ml param	0.009524
week number of a given date as	sql weekofyear col	0.055556
value to an int if possible	param type converters to int	0.250000
specifies the behavior when data or	sql data frame writer mode savemode	0.071429
feature selection by	mllib chi sq selector set	0.200000
expressions and returns	sql	0.002525
the files in disks	core external	0.016129
generated by applying a function on each rdd	transform func	0.117647
checkpointed version of this dataset checkpointing can	frame checkpoint eager	0.071429
as an :class pyspark rdd of :class row	data frame rdd	0.500000
until any of the queries on	await any termination	0.142857
a user defined function udf	sql udf f returntype	0.200000
names of tables	names	0.050000
min value for each original	ml min max scaler model original min	0.250000
find the minimum item	min key	0.333333
a left	rdd left	0.333333
embedded params to	params to	0.035714
the n elements from an rdd ordered	rdd take ordered	0.050000
sets the accumulator's value only usable	core accumulator	0.030303
sets the accumulator's	core accumulator	0.030303
return the column standard deviation	mllib standard scaler model std	0.166667
given path	path	0.071429
setparams(self p=2	ml normalizer set params p	1.000000
transforms	ml java params transfer	0.250000
output a python rdd of	core rdd save as	0.037500
this matrix to an indexedrowmatrix	matrix to indexed row matrix	0.333333
year of	year	0.040000
number n	n	0.055556
to term frequency vectors or transform the	mllib hashing tf transform	0.045455
again to wait	streaming query	0.010526
lowercase	lowercase	1.000000
for params shared by them	params copy	0.083333
given parameters in this grid to fixed values	grid builder base	0.076923
note : experimental	count approx timeout confidence	1.000000
save path	save path	1.000000
summary (e g	summary	0.024390
vector columns in an input dataframe to	vector columns	0.071429
again to	streaming query manager reset	0.011905
awaitanytermination() can be used again	sql	0.002525
python rdd of key-value	core rdd	0.010381
later than	dayofweek	0.037037
number	number	1.000000
of	ml param params	0.013699
comprised of vectors	mllib random rdds exponential	0.125000
is set to a different value or	context set	0.125000
a line in libsvm format into	parse libsvm line line	0.111111
a local property	local property key value	0.076923
queue	queue	0.857143
spark sink	addresses storagelevel maxbatchsize	0.045455
rdd get	rdd get	0.200000
line in libsvm format into	parse libsvm line line multiclass	0.111111
a row	row	0.062500
sets	features col set	1.000000
assumed to consist of	ascending numpartitions keyfunc	0.100000
get the cluster centers represented as	kmeans model cluster centers	0.090909
resulting rdd that contains a	core rdd cogroup other	0.066667
used again to wait	sql streaming query manager	0.011905
the residual degrees	regression summary residual degree	0.500000
total number of clusters	mllib kmeans model k	0.250000
aggregationdepth=2):	svc	0.285714
so that :func awaitanytermination()	sql streaming query manager	0.011905
register a java udf	register java	0.166667
how much of memory	core external	0.016129
used with the spark sink	storagelevel maxbatchsize	0.045455
:class dataframe to	sql data frame	0.010695
classification problem in multinomial logistic regression	mllib logistic regression model	0.083333
partial objects	cloud pickler save partial obj	0.125000
:func awaitanytermination() can be used again to	sql streaming query manager reset	0.011905
the stream will start and stop	test case take dstream	0.250000
in this	streaming	0.005025
operation	operation	1.000000
param with a	has param	0.019231
sample	data frame sample	0.066667
partial objects do	pickler save partial obj	0.125000
curve	ml binary	1.000000
terminations	reset	0.011236
multiclass classification	linear classification	0.142857
be used again to	streaming query	0.010526
get the cluster centers represented as a list	bisecting kmeans model cluster centers	0.095238
deviance for	regression summary deviance	0.125000
inherit documentation from	mllib inherit doc	0.045455
choose one directory for spill	core external merger get spill dir	0.500000
an rdd created by piping elements to	rdd	0.003058
smoothing=1 0 modeltype="multinomial", thresholds=none weightcol=none)	ml naive bayes	0.250000
of the date	next day date	0.100000
are the left singular vectors	linalg singular	0.017544
key-value pair rdd through a flatmap	rdd flat map	0.333333
this model to transform	model	0.005587
the receiver operating characteristic roc curve which	binary logistic regression summary roc	0.166667
parquet file stream returning the result	stream reader parquet path	0.083333
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online",	lda set params featurescol maxiter seed	0.250000
to an int if possible	param type converters to int	0.250000
a densevector with singular	linalg singular	0.017544
given a java pipelinemodel	pipeline model from java cls	0.200000
private java model produced by	java classification model	0.333333
"predictions" which gives the features	linear regression summary features col	0.333333
function and	user defined function	0.066667
__init__(self	kmeans init	1.000000
the l{sparkcontext} that	context	0.022727
submit and test a script	spark submit tests test	0.090909
system using the new	as new	0.125000
this vector to	vector	0.019231
maxheap version of a heappop	max heap item	0.250000
a list of numpy arrays	ml	0.001835
featurescol="features", predictioncol="prediction", k=2	featurescol predictioncol k	1.000000
predicts	model predict	0.333333
into	core external	0.016129
the sparkui instance started by	ui web	0.333333
evaluates the model on a test	linear regression model evaluate	1.000000
an fp-growth model that	mllib fpgrowth train cls	0.100000
compute	linalg block matrix	0.052632
create a python topicandpartition to map to the	topic and partition init topic partition	0.055556
sizes	sizes	0.833333
consist	ascending numpartitions keyfunc	0.100000
fill the datatype	datatype	0.045455
of deserialized batches lists of objects from the	serializer load stream without unbatching	0.200000
vector columns	vector columns from ml dataset	0.142857
inserts the	insert into	0.500000
separate arrays of indices	ml linalg	0.030303
calculates the norm of a	mllib linalg sparse vector norm p	0.083333
each original column	scaler model original	0.062500
comprised	mllib random rdds exponential	0.125000
fast version of a heappush	heappushpop heap item	0.142857
values for each numeric columns	sql grouped	0.043478
into a python	from	0.045455
get the cluster centers represented as	cluster centers	0.090909
it will convert each python object into	rdd to	0.200000
messagehandler	message handler	1.000000
udf with a function	sql user defined function	0.083333
rdd partitioned	rdd partition	0.062500
this	accumulator add	0.076923
extract a specific group matched by a java	extract str pattern idx	0.333333
rdd of points using the model trained	mllib tree ensemble model	0.058824
norm of a	linalg sparse vector norm	0.066667
of indices	ml chi sq	0.100000
comprised of vectors containing i i d	mllib random rdds exponential	0.125000
into an rdd of labeledpoint	mllib mlutils load lib svmfile	0.125000
this matrix to a coordinatematrix	linalg indexed row matrix to coordinate matrix	0.333333
much	size	0.009174
the kolmogorov-smirnov ks test	stat statistics kolmogorov smirnov test	0.166667
submit and test a single script on	submit tests test single script on	0.500000
sort	test case sort result	0.333333
contains	contains	1.000000
c{self} that is not contained in	core rdd subtract other numpartitions	0.111111
copy of the rdd	rdd partition	0.062500
the rdd's elements in one	core rdd	0.003460
comprised of vectors containing i i d samples	mllib random rdds log normal	0.125000
of freedom of	of freedom	0.166667
load a java model from the	loader load java cls sc	0.200000
even if users construct taskcontext instead of	task context new cls	0.333333
:class dataframestatfunctions for statistic functions	stat	0.076923
create an input stream that pulls events from	utils create stream ssc hostname port storagelevel	0.200000
sets the accumulator's	accumulator value value	0.050000
stream returning the result as a :class dataframe	data stream reader	0.200000
for each original column during fitting	ml min max scaler model original	0.062500
setparams(self strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none) sets params for	set params strategy missingvalue inputcols outputcols	0.200000
make predictions on a dstream	kmeans predict on dstream	1.000000
output a python rdd	core rdd save	0.037975
set initial centers should be	mllib streaming kmeans set initial centers centers weights	0.200000
transfer this instance's params	java params	0.200000
intercept computed for this model	linear model intercept	1.000000
convert this matrix to an indexedrowmatrix	mllib linalg coordinate matrix to indexed row matrix	0.333333
rdds	rdds	0.857143
the selector type of	set selector type	0.111111
none if the stage info	stage info	0.142857
suffix	suffix	1.000000
wait for the execution to stop	await termination timeout	0.166667
number of	sparse vector num	0.200000
which each rdd contains the count	count by	0.100000
underlying sql storage type for	sql type	0.250000
storagelevel based on a	core spark context get	0.333333
adds two block matrices together the matrices	mllib linalg block	0.111111
compute the dot product of	linalg dense vector dot	0.058824
an rdd	rdd	0.048930
of names of tables	sqlcontext table names	0.066667
original column during	min max scaler model original	0.062500
cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1	ml random forest classifier	0.023256
set the initial	with sgd set initial	0.111111
the stream query	data stream	0.028571
featurescol="features", labelcol="label",	featurescol labelcol	0.857143
compute	rdd	0.003058
the given value numbits right	right col numbits	0.500000
decorator that makes a class inherit documentation from	inherit	0.037037
vectors or transform the rdd of document to	mllib hashing tf transform document	0.166667
the :class dataframe in	sql data frame	0.016043
:py attr selectortype	selector type value	1.000000
python topicandpartition	partition init topic partition	0.055556
and name	core	0.003021
degree s of freedom of the hypothesis test	mllib stat test result degrees of freedom	1.000000
java onevsrestmodel	one vs rest model from java	0.142857
into	map to	0.125000
for feature selection by	chi sq selector	0.166667
resets	runtime config unset	0.142857
using string representations of elements	compressioncodecclass	0.111111
again to wait for new terminations	sql streaming	0.010204
save this model to the	model save	0.200000
and	reader	0.040000
into disks	core external group	0.045455
sampled subset of this rdd	rdd sample withreplacement fraction seed	1.000000
note : experimental	rdd sum approx timeout confidence	1.000000
which is a	logistic regression	0.040000
residuals	summary residuals residualstype	0.333333
an rdd ordered in ascending order or as	rdd take ordered	0.050000
tests whether this instance	paramname	0.076923
current status	status	0.111111
given string name	param params	0.014925
params shared	params copy	0.083333
drops the global	drop global	1.000000
a lower bound on the log likelihood of	ml ldamodel log likelihood	0.166667
names skipping null values	sql	0.005051
compute the sum for each numeric columns	sql grouped data sum	0.083333
maxiter=100	maxiter	0.083333
extract the week number of a given date	sql weekofyear col	0.055556
or none if the stage	stage	0.062500
value to list	to list	0.250000
cachenodeids=false checkpointinterval=10 losstype="logistic",	gbtclassifier	0.076923
even if users construct taskcontext instead of	core task context new cls	0.333333
half the	half	0.058824
newline-delimited json	json path mode	0.125000
:func awaitanytermination() can be used again to wait	reset	0.011236
two columns of a dataframe	data frame corr col1 col2	0.166667
:class dataframe as non-persistent and remove all blocks	sql data frame unpersist	1.000000
the given parameters in this	builder add	0.200000
nodes or any hadoop-supported	path	0.020408
mean squared error which	mllib regression	0.022727
all mixture	mllib gaussian mixture model predict	0.100000
collect each rdds into the returned list	py spark streaming test case collect dstream	1.000000
object or none if the stage	stage	0.062500
a local property set in this thread or	context get local property key	0.066667
cluster	cluster	0.571429
group the values for each key in	group by key numpartitions	0.333333
current	current	1.000000
compute the dot product	vector dot other	0.100000
for this obj assume that all	merger object size obj	0.040000
columns in an input dataframe from the :py	columns to	0.125000
data sampled from a continuous	data distname	0.083333
implementation of fit	ml estimator fit	0.083333
be used again to wait	manager reset	0.011905
partitioned data into	external group by	0.045455
functionality for statistic functions with :class dataframe	data frame stat functions	0.333333
max abs vector	scaler model max abs	1.000000
setparams(self featurescol="features",	aftsurvival regression set params featurescol	1.000000
the test	test	0.030303
the month of a given date as	dayofmonth col	0.031250
taskcontext	core task context	0.200000
back to a	index to	0.040000
tree (e g depth 0 means 1	tree	0.020833
only create a new hivecontext for testing	context create for testing	0.333333
for each key in	by key numpartitions	0.111111
dump already partitioned data into	external	0.013889
columns of a dataframe as	sql data frame corr	0.166667
distributed model to a local representation	ml distributed ldamodel to local	0.111111
database table via	url table	0.200000
transforms the input dataset	transformer transform dataset	1.000000
load a model from	java loader load cls sc	0.250000
class inherit documentation from	mllib inherit doc cls	0.045455
for this obj assume	size obj	0.040000
column scipy matrix from a dictionary of	sci py tests scipy matrix	0.090909
"zerovalue" which may	rdd fold by	0.125000
set to	set	0.005917
values for each key using an	key func numpartitions	0.066667
specification that defines	spec	0.076923
that :func awaitanytermination() can be used again	streaming	0.005025
sort the list	streaming test case sort result	0.333333
centers represented as a	centers	0.150000
a local property set in this thread	local property key	0.035714
of this query that	streaming query	0.010526
the given data type json string	parse datatype json string	0.333333
multi-dimensional cube	data frame cube	0.055556
computes hex value of the given column which	hex col	0.166667
0 95 0 99], quantilescol=none aggregationdepth=2):	fitintercept	0.058824
make predictions on a dstream	mllib streaming kmeans predict on dstream	1.000000
k-means algorithm return the	kmeans train rdd	0.333333
broadcast a read-only variable	core spark context broadcast value	0.125000
line in libsvm format into label indices values	mlutils parse libsvm line line	0.111111
that all	size	0.009174
distinct count	count distinct col	0.080000
percentile	percentile	1.000000
create a column scipy matrix from	mllib sci py tests scipy matrix	0.090909
java pipelinemodel	pipeline model from java	0.142857
creates an external	sql sqlcontext create external	1.000000
objects	external merger object	0.032258
name for column	col	0.016393
each point in rdd 'x' has maximum membership	mllib gaussian mixture	0.045455
awaitanytermination() can be used again	sql streaming	0.010204
sets	bucketizer set	1.000000
of this	ml train	0.181818
group id to all the jobs started by	job	0.023810
new	sql streaming query manager reset	0.011905
matrix to a rowmatrix	coordinate matrix to row matrix	0.333333
an upper triangular matrix r in a qr	linalg qrdecomposition r	0.333333
residual degrees of freedom for	summary residual degree of freedom	0.125000
rating for the given	mllib matrix factorization	0.250000
the accumulator's value only usable in driver	core accumulator	0.030303
end	end	0.333333
loaddefaults	loaddefaults	1.000000
matrix other from this block matrix	linalg block matrix	0.052632
sample	sample	0.300000
creating rdds comprised of	random rdds	0.012821
the root mean squared error which is	mllib regression	0.022727
computes column-wise summary statistics	mllib stat statistics col	0.200000
returns a java storagelevel based	get java	0.111111
predicted ratings for input user and	mllib matrix factorization model predict all user_product	0.050000
loads a csv file	reader csv path	0.666667
instance's params	java params	0.200000
the sort	sort	0.111111
of columns that make up each	mllib linalg block matrix cols per	0.333333
python parammap into a java parammap	to java pyparammap	0.250000
number of batches after which the centroids	timeunit	0.025641
while tracking	preservespartitioning	0.090909
new dstream in which each rdd contains	by	0.014286
computes the levenshtein distance of the two	sql levenshtein	0.058824
of iterations default 1 which	iterations	0.043478
returns an mlwriter instance for this ml	ml mlwritable write	0.200000
sort the list	test case sort result	0.333333
the sql context to use for saving	mlwriter context sqlcontext	0.333333
cluster for each training data point	gaussian mixture summary	0.200000
used again to wait for	streaming query manager reset	0.011905
produced by the	ml clustering	0.100000
extract a specific group matched by a	sql regexp extract str pattern idx	0.333333
a matrix from the new mllib-local	matrices from ml	0.333333
batches of data from	algorithm	0.090909
of points using the model trained	mllib logistic regression model	0.083333
create a	tests create	0.500000
performs the kolmogorov-smirnov ks	stat statistics kolmogorov smirnov	0.333333
seed=none numtrees=20	ml random forest	0.071429
flume	streaming flume	0.222222
line in libsvm format into label indices values	parse libsvm line line multiclass	0.111111
returns the soundex encoding for a	sql soundex	0.055556
already partitioned data	external group by spill	0.047619
the values for each key using an associative	key func	0.066667
mixin for param stepsize step size to be	has step size	0.333333
computes column-wise summary statistics for the input rdd[vector]	statistics col stats rdd	0.200000
convert the vector into an numpy ndarray	linalg vector to array	1.000000
products for all users	products for users	0.250000
the documentation of	ml	0.001835
densematrix(2 2 range 4	dense matrix repr	0.142857
of the current [[dataframe]] and	sql grouped data pivot pivot_col values	0.050000
in the training set	distributed ldamodel training	0.034483
the month of	dayofmonth	0.027027
be used again to wait for	sql streaming query manager reset	0.011905
queries so that :func awaitanytermination() can	manager reset	0.011905
parses the expression string into the column that	sql expr str	0.125000
new dstream by applying reducebykey	streaming dstream reduce by key func	0.076923
can be used again to	sql	0.002525
array-like or buffer to array	to array array_like dtype	0.166667
memory for this obj assume	object size obj	0.040000
labels corresponding to indices	model labels	0.166667
be used again to wait for	manager reset	0.011905
point in rdd 'x' to all mixture components	gaussian mixture model predict soft	0.142857
queries so that :func	streaming	0.005025
return a	core	0.021148
mindf	mindf	1.000000
that :func awaitanytermination() can be used	streaming	0.005025
isnull	isnull	1.000000
tree (e g depth 0 means 1 leaf	mllib decision tree	0.166667
__init__(self	rformula init	0.500000
spark context call site	sccall site sync	0.200000
top features	top features	0.500000
of the :class dataframe to a	frame writer	0.050000
regression score	mllib regression metrics	0.090909
int containing elements	core spark context	0.011628
of each class as a vector	ml	0.001835
a dataframe with two fields threshold precision curve	binary logistic regression summary precision by threshold	0.166667
name	ml param params	0.013699
of this instance with the	ml param	0.009524
udf with a function	defined function	0.066667
specifies the underlying output	frame writer format	0.333333
this tests	tests test	0.018519
configuration	configuration	1.000000
of the month	sql dayofmonth	0.031250
so that :func awaitanytermination() can	manager	0.011236
values for each numeric columns	sql grouped data	0.041667
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params	ml vector indexer set params maxcategories inputcol outputcol	0.333333
rdd, a list	schema samplingratio verifyschema	0.029412
checkpointinterval=10 impurity="variance", seed=none	regressor	0.043478
a default otherwise	spark	0.013158
of offsets from a single	offset	0.021739
an rdd of labeledpoint	mlutils load lib svmfile	0.125000
the most recent [[streamingqueryprogress]] updates for this query	streaming query recent progress	0.500000
can be used again to wait for	query	0.010753
the sum of	ml bisecting kmeans model	0.076923
python rdd of key-value	rdd save	0.038462
queries so that :func awaitanytermination() can be	streaming	0.005025
dictionary a list of	init size	0.066667
an error	mllib mllib streaming	0.500000
:py attr stepsize	step size value	1.000000
awaitanytermination() can be	sql streaming	0.010204
set bandwidth of each sample	mllib stat kernel density set bandwidth bandwidth	0.142857
rows	row	0.062500
of labeledpoint	load lib svmfile	0.125000
wrapper	java	0.012195
contains a param with a	params has param	0.019231
test that coefs are predicted accurately by	streaming linear regression with tests test parameter accuracy	0.333333
contains a param with a given string name	ml param params has param	0.019231
column or names into a jvm seq of	seq sc cols converter	0.055556
on the dataset in a data source	source schema	0.090909
each key	by key	0.052632
implementation of	ml estimator	0.125000
configure the kmeans algorithm for fitting and predicting	kmeans	0.025641
for given binary operator	sql bin op name doc	0.200000
singular values in descending order	linalg singular value decomposition s	0.250000
such as the spark fair scheduler pool	spark context	0.023256
save a	save	0.062500
changes the uid	param params reset uid	0.333333
this matrix to a rowmatrix	row matrix to row matrix	0.333333
dump already partitioned data into disks	external	0.013889
database table named table	table column	0.166667
return sparkcontext which is associated with this streamingcontext	context spark context	0.500000
model fitted by :py class vectorindexer	vector indexer model	0.200000
total log-likelihood for this model on the	gaussian mixture summary log likelihood	0.142857
returns accuracy equals	mllib multiclass metrics accuracy	0.166667
to term frequency vectors or transform the	hashing tf transform	0.045455
session	session	0.350000
output a python rdd of key-value pairs	core rdd save	0.037975
to convert the java_model to a python model	ml quantile discretizer create model java_model	0.250000
number of possible outcomes for	num	0.008403
content of the :class dataframe as	data frame writer save as	0.071429
statistic functions	frame stat	0.250000
number of rows	block matrix num rows	0.200000
"predictions" which gives the true label	linear regression summary label	0.333333
a resulting rdd that contains a tuple with	rdd cogroup other	0.066667
returns a :class dataframe representing the	sql sqlquery	0.250000
featurescol="features", maxiter=20 seed=none checkpointinterval=10	featurescol maxiter seed	0.200000
of the points belongs	predict x	0.033898
a new profiler using class	profiler collector new profiler	0.333333
computes the levenshtein distance of the two	sql levenshtein left	0.058824
disks	core external group	0.045455
model to the input dataset	dataset	0.040816
boundaries from start	start	0.045455
fits a model to the input dataset	estimator fit dataset	1.000000
app	app	1.000000
week number of a given date as	weekofyear col	0.055556
or an rdd of points using the model	model	0.005587
this instance contains a	params has	0.019231
finding frequent items for columns possibly	sql data frame freq items cols support	0.166667
are the left singular vectors of the	mllib linalg singular	0.017544
the content of the :class dataframe in orc	sql data frame writer orc	0.200000
configured value for some key or return a	core spark conf get key defaultvalue	1.000000
perform a right outer join of	full outer join	0.111111
set bandwidth of each sample	set bandwidth bandwidth	0.142857
dstream by applying a function to	f	0.010526
accumulator's value	core accumulator add	0.076923
to wait for new terminations	sql streaming query manager	0.011905
a character in matching	matching	0.111111
used again to	streaming query manager	0.011236
range	range	0.181818
does this configuration	core spark conf	0.055556
instance is	is	0.083333
train the model on the	logistic regression with sgd train on	0.333333
doc	doc	0.555556
the chisqselector	selectortype	0.125000
positive rate for a	positive rate	0.166667
squared distance from a sparsevector	linalg sparse vector squared distance	0.166667
number	linalg dense vector num	0.333333
an expression that gets a	sql column get	0.142857
that getting	test tc	1.000000
set multiple parameters passed as	conf set all	0.250000
until any of the	any	0.083333
the sum for each numeric columns for each	grouped data sum	0.083333
test that the model params are set	mllib streaming kmeans test test model params	0.250000
value of	ml isotonic regression	0.111111
only create a new	create	0.017241
attr lda keeplastcheckpoint is set to	ldamodel get	0.066667
>>> (maptype(stringtype(), integertype())	sql map type init keytype valuetype valuecontainsnull	1.000000
be used again to wait	sql streaming query manager reset	0.011905
rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures	0.125000
a list of conditions and	column otherwise	0.200000
make sure user configuration is respected spark-19307	spark submit tests test user configuration	1.000000
norm of a	mllib linalg sparse vector norm p	0.083333
how much of memory for	merger object	0.032258
the year of a	sql year	0.050000
partitioned data into disks	external group	0.045455
boosted trees model	boosted trees	0.166667
to a data source	sql data	0.024390
of model	tree model	0.026316
a jvm seq of	seq sc	0.055556
mat	mat	0.833333
create a converter to	create converter	0.166667
model	naive bayes model	0.500000
a unique identifier for the spark application	spark context application id	0.500000
data into disks	external group	0.045455
squared error which is defined as the	linear regression summary	0.013889
for the test this should be list of	test	0.015152
calculates a lower bound on the log likelihood	ldamodel log likelihood	0.142857
converts a labeledpoint to a string	mlutils convert labeled point to	0.250000
labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1 0	labelcol predictioncol probabilitycol	0.166667
close to the desired value	parameter accuracy	0.029412
this instance's params to	params to	0.035714
create an input stream	create stream ssc hostname port	0.200000
number of	mllib linalg sparse vector num	0.200000
leaf	decision	0.052632
closest in similarity to "word"	synonyms word	0.166667
generated	validation	0.285714
wait for the execution to stop return	streaming streaming context await termination or timeout timeout	0.125000
bound on the log likelihood of	ml ldamodel log likelihood dataset	0.166667
param	has param	0.019231
param with	ml param params has param	0.019231
given columns if specified	sql data frame	0.005348
transforms	params transfer	0.250000
recommends	mllib matrix factorization model recommend	0.250000
contains a param	ml	0.001835
this instance with a randomly generated uid and	cross validator model	0.050000
return	core	0.036254
lambda function	function name f	0.166667
generates an rdd	poisson vector rdd sc mean	1.000000
database table named table	table	0.031250
the content of the non-streaming :class dataframe out	sql data frame write	0.071429
private mixin for instances that provide javamlreader	java mlreadable	0.250000
initial value	initial	0.071429
to configure the	streaming	0.005025
get all values as a list of	spark conf get all	0.166667
initial value	with sgd set initial	0.111111
aggregate the values	rdd aggregate	0.250000
return sparkcontext which is associated with	streaming streaming context spark	0.083333
aid	aid	1.000000
from start to end exclusive increased	context range start end	0.333333
"zerovalue" which may be	rdd fold	0.125000
data from a dstream	dstream	0.031250
between	between	1.000000
timestamp datetime datetime	timestamp	0.166667
set a local property that affects jobs	context set local property key value	0.200000
converts matrix columns in	convert matrix columns	0.166667
a condition to pass else fail with	test case eventually condition	0.333333
line in libsvm format into label	mllib mlutils parse libsvm line line	0.111111
curve which is a dataframe	binary logistic regression summary	0.111111
for each key using an associative	by key func numpartitions	0.062500
set bandwidth of each	stat kernel density set bandwidth bandwidth	0.142857
ensemble	ensemble model	0.117647
value of	ml bucketizer	0.250000
start	add months start	1.000000
list of names of tables in the	names	0.050000
an rdd comprised of	random rdds log normal vector rdd	0.166667
impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	ml random forest classifier	0.023256
a left outer join of c{self}	rdd left outer join other numpartitions	0.111111
new terminations	streaming query	0.010526
match regexp	regexp	0.076923
again to wait	query	0.010753
so	sql streaming query manager reset	0.011905
f-measure	beta	0.166667
inverse	inverse	0.857143
returns accuracy equals to	mllib multiclass metrics accuracy	0.166667
creates a :class dataframe from an	sql spark session create	0.142857
in the key-value pair rdd through a flatmap	rdd flat map values	0.333333
the uid of	ml param params reset uid	0.058824
converts matrix columns in an input dataframe	mllib mlutils convert matrix columns to ml dataset	0.166667
contents of the :class dataframe to a	frame writer save path	0.066667
hadoop configuration which is passed in as	spark context hadoop	0.090909
tokenizer	ml tokenizer	0.333333
convert this matrix to a rowmatrix	matrix to row matrix	0.333333
so that	streaming query	0.010526
array containing the ids of all active stages	status tracker get active stage ids	0.250000
parses	expr	0.076923
broadcast a read-only variable to the	core spark context broadcast	0.125000
a python rdd of	rdd save as	0.038462
replacing a value with another	replace to_replace	0.200000
levenshtein distance of the two given strings	sql levenshtein	0.058824
kmeans algorithm for fitting and predicting	kmeans	0.025641
set each dstreams in this	streaming	0.005025
variant of _siftdown	siftdown	0.166667
coefficients	coefficients	1.000000
:class dataframe	frame	0.068966
dump already	by spill	0.047619
selector type of	selector set selector type	0.111111
script with a dependency	dependency	0.120000
value for efficiency can also update c{value1}	value1 value2	0.250000
fast	core heappushpop	1.000000
can be used again to wait	query manager	0.011905
property set in this thread or null if	property key	0.066667
of the :class dataframe	sql data frame writer save	0.083333
train the model	streaming linear regression with sgd train	1.000000
mixin for param fitintercept whether to	has	0.011628
how much of memory for	core	0.003021
threshold threshold	threshold	0.018182
list of columns for the given table/view	list columns	0.166667
in the ensemble	mllib tree ensemble	0.111111
loads a parquet file stream returning the	stream reader parquet path	0.083333
the java loader	mllib java loader java loader	0.333333
parses a line in libsvm format into	parse libsvm line line multiclass	0.111111
representation of each word in vocabulary	mllib word2vec fit	0.200000
second is an array of	mllib matrix	0.047619
densematrix whose columns are the right singular vectors	mllib linalg singular	0.017544
'x' to all mixture	gaussian mixture	0.038462
create a multi-dimensional cube	sql data frame cube	0.055556
file system using the old hadoop outputformat api	as hadoop dataset conf keyconverter valueconverter	0.083333
wait for the	await termination timeout	0.166667
mark the rdd	core rdd	0.003460
new rdd that is reduced into numpartitions partitions	rdd coalesce numpartitions shuffle	0.500000
parse a	sql parse	0.500000
the input dataset this	dataset	0.020408
load a model from the given	mllib kmeans model load cls sc	0.333333
the min value	min	0.041667
a	accumulator	0.012987
of tree	tree model	0.026316
or replaces a	or replace	0.500000
sparkcontext which is associated with this streamingcontext	context spark context	0.500000
train a lda model	mllib lda train cls rdd k maxiterations	1.000000
that	object	0.027778
system using the new hadoop outputformat api mapreduce	as new apihadoop dataset conf keyconverter valueconverter	0.142857
javasparkcontext instance optional	jsc	1.000000
saves the contents of the	save path format mode partitionby	0.200000
create a new dstream in which	streaming streaming context transform	0.066667
names of tables	sqlcontext table names	0.066667
given	params has param	0.019231
saved using rdd	sc path minpartitions	0.250000
the initial value	with sgd set initial	0.111111
the standard deviation of this rdd's	stdev	0.047619
return an numpy ndarray	linalg dense matrix to array	1.000000
get all values as a list of	conf get all	0.166667
computes the max value	max	0.071429
l{sparkcontext} that	context	0.022727
return sparkcontext which	streaming context spark	0.083333
converts matrix columns in	mlutils convert matrix columns to ml	0.166667
thresholds thresholds in multi-class classification	thresholds	0.071429
params are set	params	0.006623
comprised of	mllib random rdds normal vector	0.125000
the mean average precision map of	mean average precision	0.166667
new :class dataframe with each partition sorted by	data frame sort within partitions	1.000000
of columns that make up each block	matrix cols per block	0.333333
ml params instances	params	0.006623
inputcols input column names	input cols	0.500000
utils	utils	1.000000
or c{other}, return a resulting rdd that contains	rdd cogroup other	0.066667
broadcast	context broadcast	0.125000
and return the java object	java	0.012195
a param with a given string name	param params has	0.019231
defined on the class	param	0.006250
new :class column for approximate distinct count	approx count distinct col	0.071429
of the month of a given	dayofmonth col	0.031250
objective	linear regression training summary objective	1.000000
how	core external merger object	0.032258
rdd	rdd	0.155963
which the given date	date	0.037037
day of the month of a given date	sql dayofmonth	0.031250
prediction scores into 0/1 predictions	linear classification model	0.076923
the month of a given	sql dayofmonth col	0.031250
a jvm map	scala map sc jm	0.200000
the selector type of	chi sq selector set selector type	0.111111
:func awaitanytermination() can be used again to	streaming query	0.010526
an rdd comprised of vectors containing i i	mllib random rdds poisson vector rdd	0.166667
string column from one base	conv col frombase tobase	0.166667
instance contains	ml	0.001835
load a java model from the given path	loader load java cls sc path	1.000000
error which is	regression	0.010000
evaluates the output	ml evaluator evaluate dataset	0.285714
database table via	url table mode	0.200000
featurescol features column name	features col	0.250000
string name	ml param params has	0.019231
with two fields threshold recall curve	binary logistic regression summary recall by threshold	0.166667
in "predictions" which gives the features	linear regression summary features	0.333333
bisecting k-means algorithm return	bisecting kmeans train rdd	0.333333
contains a param	param params has	0.019231
the bisecting k-means algorithm return the	mllib bisecting kmeans train rdd	0.333333
ints	int value	0.500000
in "predictions" which gives the predicted value of	ml linear regression summary prediction col	0.500000
a unified dstream from multiple dstreams	streaming streaming context union	0.111111
called when processing of a batch	streaming streaming listener on batch	0.333333
java array	wrapper new java array	0.333333
deviance	deviance	0.555556
population should	stat kernel density	0.200000
components	soft x	1.000000
that has no partitions or elements	core spark context empty	0.333333
python rdd	core rdd	0.013841
into an rdd of labeledpoint	mlutils load lib svmfile	0.125000
of the rdd partitioned using the	rdd	0.003058
any hadoop file system using the l{org	sequence file path compressioncodecclass	0.500000
partition for	partition	0.066667
convert this matrix to an indexedrowmatrix	block matrix to indexed row matrix	0.333333
number of nonzero elements	dense vector num	0.166667
contains a param	has param	0.019231
gets a field by name in	get field name	0.333333
comprised of vectors containing	mllib random rdds poisson vector	0.125000
weighted averaged	weighted fmeasure	1.000000
of deserialized objects from the	serializer load	0.083333
new rdd	rdd	0.006116
format at the	compression	0.142857
create a python topicandpartition to	and partition init topic partition	0.055556
as	by	0.014286
checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1 0)	random forest classifier	0.022727
manager	manager	0.056180
the heap maintaining the heap invariant	core heappop heap	0.142857
given join	join	0.034483
already partitioned	by spill	0.047619
prefix of string	prefix f	0.142857
profile object is returned	basic profiler profile	0.200000
represents a range of offsets from a	offset range	0.047619
:class dataframe out	sql data frame	0.005348
next memory limit if the memory	next limit	0.200000
compute the dot product of	dot other	0.050000
again to wait for new terminations	query manager reset	0.011905
even if users construct taskcontext instead of using	task context new	0.333333
condition to pass else fail with an error	mllib mllib streaming test case eventually condition	0.333333
a paired	matrix factorization model	0.043478
the selector type of	selector set selector type	0.111111
events from flume	flume	0.071429
percentile	set percentile percentile	1.000000
that :func awaitanytermination()	manager reset	0.011905
values from this instance to another	copy values to extra	0.333333
value of	ml binarizer	1.000000
mean variance and count of the rdd's	core rdd	0.003460
nodes or any hadoop-supported file system	file path	0.035714
available on all nodes or any hadoop-supported file	file path	0.035714
ml params instances for the given param	params m1	0.047619
the given table/view in the specified database	tablename dbname	0.142857
comprised of vectors containing	random rdds log normal	0.125000
computes the area	area	0.181818
dataframe of probabilities	probability	0.100000
this context	streaming streaming context	0.032258
contents of the :class dataframe to a	frame writer save path format	0.066667
selector type	selector set selector type	0.111111
sets the accumulator's	accumulator	0.012987
list of indices to	ml chi	0.100000
the contents of the :class dataframe to	frame	0.034483
a :class datatype the data type string format	datatype string	0.111111
sum of	ml bisecting	0.066667
matrix from the new mllib-local representation	mllib linalg matrices from ml	0.333333
create an rdd that has no partitions or	spark context empty rdd	0.200000
csv file and	csv path schema	0.166667
later than the value of the date column	next day date dayofweek	0.333333
original column during fitting	min max scaler model original	0.062500
returns the soundex encoding for	sql soundex col	0.055556
of all params with their optionally default values	ml param params explain params	0.166667
for	core external merger	0.032258
converts vector columns in an input	convert vector columns to ml	0.166667
attr predictions which	generalized linear regression	0.090909
compute the dot product of two	dot	0.040000
sets params for rformula	ml rformula set params formula	1.000000
train the model on the incoming dstream	kmeans train on dstream	1.000000
that all the	merger object	0.032258
values for each key using an	by key func numpartitions	0.062500
calculates the norm of a	sparse vector norm	0.066667
the norm of a sparsevector	vector norm p	0.055556
an rdd comprised	mllib random rdds gamma vector rdd	0.166667
:py attr lda keeplastcheckpoint is	distributed ldamodel	0.052632
this matrix to an indexedrowmatrix	linalg block matrix to indexed row matrix	0.333333
create a java array	java wrapper new java array	0.333333
objects	serializer	0.125000
function by name	function name doc	0.500000
nodes in tree	tree	0.020833
this context to	streaming streaming context	0.032258
for the termination of this query	termination	0.035714
comprised of vectors containing i i	mllib random rdds poisson	0.125000
this instance with a randomly generated uid	one	0.058824
update the catalog	catalog	0.062500
defined on the class to	ml param	0.009524
function but return	core rdd	0.003460
matrix columns in an input dataframe	matrix columns to ml	0.142857
queue of	queue	0.142857
mark the rdd as non-persistent	core rdd unpersist	0.066667
set a local property	set local property key	0.200000
as the specified	writer save as	0.333333
matrix to the	matrix	0.030303
that :func	sql	0.002525
:class dataframe	grouped data	0.035714
the population	mllib stat kernel density	0.066667
block matrix other from this	linalg	0.022222
kafka partition id	partition	0.066667
save a file	core cloud pickler save file obj	1.000000
:class dataframe representing the database	sql data frame reader	0.111111
a sparse vector using either a	linalg vectors sparse	0.166667
of probabilities	probability	0.100000
python rdd of key-value	rdd save as	0.038462
setparams(self featurescol="features", predictioncol="prediction", maxiter=20 seed=none k=4 mindivisibleclustersize=1 0)	set params featurescol predictioncol maxiter seed	1.000000
the dot product of	vector dot other	0.050000
the entry point to programming spark with	spark session	0.100000
dataframe produced by the	ml clustering	0.100000
attr lda keeplastcheckpoint is set to	ml distributed ldamodel	0.050000
transforms	ml transformer transform	0.500000
partial	core cloud pickler save partial	0.125000
onevsrest create and return	one vs rest from	0.142857
chain two function together	chain f g	1.000000
a multi-dimensional cube for	frame cube	0.055556
how much of memory	merger	0.025641
create a converter to drop the names	create converter	0.166667
featurescol="features", predictioncol="prediction", k=2 initmode="k-means||", initsteps=2 tol=1e-4 maxiter=20 seed=none)	featurescol predictioncol k initmode	1.000000
components	model predict soft	1.000000
impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none	random forest classifier	0.022727
used again to wait for	manager reset	0.011905
ignore separators	ignore	0.100000
in the training set given the current	training	0.029412
checkpointinterval=10 seed=none impurity="gini",	classifier	0.050000
is checkpointed and materialized either reliably or locally	is checkpointed	0.142857
generates an rdd	log normal vector rdd sc mean	1.000000
to all the jobs started by this	job	0.023810
line in libsvm format into label indices	mllib mlutils parse libsvm line line multiclass	0.111111
set named options filter out those	option utils set opts schema	0.333333
set the initial value of weights	set initial weights	0.333333
centers represented as a list of numpy	centers	0.100000
with a given string name	ml param params has	0.019231
queries so that :func awaitanytermination() can	query manager reset	0.011905
test that	mllib streaming logistic regression with sgdtests test	0.111111
return a javardd of object by	core rdd	0.003460
get a local property set in	get local property key	0.066667
use only create a	hive context create	0.083333
split the list of values	values	0.050000
the length of a string or binary	sql length	0.050000
as the spark fair	core spark context	0.011628
in rdd 'x' to all mixture	mllib gaussian mixture model predict	0.100000
mlreadable	mlreadable	1.000000
again to wait for new	sql streaming query manager reset	0.011905
again to wait for new terminations	sql streaming query manager	0.011905
left outer join	rdd left outer join other numpartitions	0.111111
zips this rdd with generated unique long ids	core rdd zip with unique id	1.000000
contains	has param	0.019231
of memory for	core external merger	0.032258
multiclass classification	multiclass	0.071429
vector columns in an	vector columns from ml	0.142857
in the training set given	distributed ldamodel training	0.034483
column of predicted clusters in predictions	ml clustering summary prediction col	0.111111
memory for	core	0.003021
in :py attr predictions which gives the predicted	generalized linear regression summary prediction col	0.250000
c{other}	join	0.034483
the dispatch to handle all function	cloud pickler save function	0.142857
for	sql streaming query	0.011765
initial value of weights	sgd set initial weights initialweights	0.333333
returns an mlwriter instance	ml mlwritable write	0.200000
decayfactor timeunit to configure the kmeans algorithm	kmeans	0.025641
elements in rdds in a sliding window over	value and window windowduration slideduration numpartitions	0.076923
of memory	external merger object	0.032258
or list	oneatatime default	0.250000
the spark fair scheduler	spark	0.013158
to_profile passed in a profile object is returned	basic profiler profile func	0.200000
for each numeric columns for each	grouped data avg	0.058824
of the dataframe in	sql data frame writer	0.011628
return	streaming rdd	1.000000
week number of	sql weekofyear	0.055556
errors	errors	1.000000
the column mean values	standard scaler model mean	0.125000
write() save path	one vs rest save path	1.000000
java pipeline create and return a python wrapper	pipeline from java	0.142857
input schema	sql data frame reader schema schema	0.333333
submitted from this thread such as the spark	spark context	0.023256
note : experimental	core rdd count approx distinct relativesd	1.000000
can be used again to wait for new	streaming query	0.010526
large dataset and an item approximately	lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
curve	mllib binary classification	1.000000
the cluster centers represented as a	model cluster centers	0.090909
infer schema from an rdd	infer schema rdd samplingratio	0.250000
vectors which	vector indexer model	0.200000
each point in rdd 'x' to all mixture	gaussian mixture model	0.052632
infer schema from an rdd	sql spark session infer schema rdd	0.250000
residual degrees of freedom for the	linear regression summary residual degree of freedom	0.125000
accumulator's data type	accumulator	0.012987
dot product of two vectors we	vector dot	0.050000
the mean variance and count of	core	0.003021
copy of the rdd partitioned	rdd	0.003058
parameters in	param grid builder	0.055556
of write() save path	ml pipeline model save path	0.200000
months between	months between	0.333333
returns the least value	sql least	0.055556
to their nearest center for this model	model	0.011173
of memory	size	0.009174
left singular vectors of the	mllib linalg singular	0.017544
value2	value2	1.000000
fits	estimator fit	0.166667
passed as	conf	0.050000
of the dstreams	transform dstreams transformfunc	0.125000
predictioncol prediction	prediction	0.041667
this instance to	to extra	0.500000
wait for the execution to stop	termination or timeout timeout	0.125000
model fitted by :py class	scaler model	0.230769
inside brackets pairs e g	brackets split	0.083333
obj assume that all the	obj	0.023810
for approximate distinct count	approx count distinct col	0.071429
hadoop configuration which	context hadoop	0.090909
new hivecontext for	for	0.111111
broadcast a read-only variable to	core spark context broadcast	0.125000
[0 0 1 0] for feature selection by	chi sq selector set	0.125000
temporary table in	table df	0.083333
generates an rdd comprised of vectors	mllib random rdds poisson vector rdd sc mean	0.200000
that :func awaitanytermination()	sql	0.002525
python object into java	py2java sc obj	0.333333
wait for new terminations	sql streaming query manager	0.011905
spark sink	ssc addresses storagelevel maxbatchsize	0.045455
summary of model	model repr	1.000000
have the same param	m1 m2 param	0.125000
returns a java storagelevel based	java	0.012195
returns an mlwriter instance for this	java mlwritable write	0.200000
general pyspark tests that depend on numpy	num py tests	1.000000
date2	date2	0.833333
computes the area under the	metrics area under	0.166667
define a windowing column	sql column over window	0.333333
the libsvm format into an rdd of labeledpoint	mlutils load lib svmfile sc	0.125000
rdd as non-persistent and remove all	core rdd unpersist	0.066667
true if the table	tablename	0.043478
count of distinct elements in	count	0.016949
to be inherited by any streaminglinearalgorithm	linear algorithm	0.076923
parameters in this grid	param grid builder	0.055556
month of	sql dayofmonth col	0.031250
of labeledpoint	mlutils load lib svmfile sc path numfeatures	0.125000
a line in libsvm format into label	parse libsvm line line multiclass	0.111111
that :func awaitanytermination()	streaming query	0.010526
a shortcut of	ml	0.007339
value of	ml als	1.000000
number	num	0.218487
python topicandpartition to map to	and partition init topic partition	0.055556
operation test for dstream map	operation tests test map	0.333333
checkpointinterval=10 seed=none impurity="gini", numtrees=20	ml random forest classifier	0.023256
date which is later than the	dayofweek	0.037037
an event time	eventtime delaythreshold	0.333333
converts matrix columns in an input dataframe	mlutils convert matrix columns to ml	0.166667
threshold threshold in binary classification prediction	threshold	0.018182
for user who is running sparkcontext	spark context spark user	0.250000
creates an	sql sqlcontext create	0.500000
that particular batch has half the	half	0.058824
but not in another	subtract other	1.000000
then merges them with extra values	map extra	0.040000
accumulator's value	add	0.035714
fold	fold	0.384615
for the stream query if this	stream	0.017544
to a :class datatype the data type string	datatype string	0.111111
of columns	linalg block matrix cols	0.333333
test	test test	0.333333
the standard deviation of this rdd's	rdd stdev	0.066667
of indices back to a	index to	0.040000
the year of a given date	sql year col	0.050000
"predictions" which gives the probability	logistic regression summary probability col	0.333333
of iterations default 1 which should	iterations	0.043478
(i j s\ :sub ij\) tuples representing	k maxiterations	0.333333
count of	core rdd	0.003460
default	default	0.777778
again to wait for new	manager	0.011236
file system using the new	new	0.062500
area under the receiver operating characteristic roc	metrics area under roc	0.500000
0 1 0] for feature selection by fwe	mllib chi sq selector set fwe fwe	0.200000
terminations	streaming query manager reset	0.011905
to a certain time of day in utc	utc	0.050000
to	streaming query	0.010526
of each word in vocabulary	mllib word2vec fit data	0.200000
the rdd partitioned using the	rdd partition by	0.062500
the stream	stream	0.017544
file using string representations of elements	file path compressioncodecclass	0.333333
a new feature vector with a subarray of	vector slicer	0.166667
return a new dstream in which each	by	0.014286
containing a json string into a	from json	0.166667
a column	sql spark	0.125000
of rows in	count	0.016949
rdd partitioned using	rdd partition	0.062500
dump	external	0.013889
algorithm return	rdd	0.003058
optimizedocconcentration	optimize doc concentration	1.000000
convert a list	sql to	0.041667
the month of a given date as	sql dayofmonth	0.031250
awaitanytermination() can be	sql streaming query manager reset	0.011905
compute the dot product of two vectors	linalg dense vector dot other	0.058824
that	sql streaming	0.010204
extracts the embedded default param values and	param params extract param	0.333333
value of	ml param has variance	1.000000
a java model from the given	java cls	0.111111
a java	java javaparammap	0.125000
whose columns are the right singular	singular	0.015625
wait a given amount of time for	timeout catch_assertions	0.125000
the given data type json	parse datatype json	0.333333
month of a given date as	dayofmonth col	0.031250
for controlling	level	0.125000
data of	data stream	0.028571
this pca	pca	0.125000
value for each original column during fitting	max scaler model original	0.062500
a list of numpy	ml	0.001835
setparams(self k=none inputcol=none outputcol=none) set params	set params k inputcol outputcol	0.333333
for this idf	ml idf	0.250000
compute the sum for each numeric columns for	sql grouped data sum	0.083333
a python parammap into a java parammap	map to java pyparammap	0.250000
convert the java_model to a python model	ml quantile discretizer create model java_model	0.250000
or newline-delimited json	writer json path mode	0.125000
column or names into a jvm seq	seq	0.043478
to load a streaming :class dataframe from	data stream	0.028571
sparse vector using either a dictionary a	mllib linalg vectors sparse size	0.166667
of this instance with a randomly generated	ml one vs	0.142857
access fields by name or slice	struct type getitem key	0.200000
of top features	top features	0.500000
copy of the rdd partitioned using	rdd	0.003058
norm of	sparse vector norm	0.066667
all active stages	core status tracker get active	0.333333
converts matrix columns in	convert matrix columns to	0.166667
function to the value of each	f	0.021053
for which predictions are known	ml isotonic regression	0.111111
register a java	register java	0.166667
0] for feature selection by	mllib chi sq selector	0.150000
used again to wait for new terminations	sql streaming query manager reset	0.011905
columns that make up each	linalg block matrix cols per	0.333333
__init__(self	vector slicer init	1.000000
write()	pipeline model	0.071429
add a py or zip dependency	core spark context add py file	0.166667
deviation values	model std	0.500000
:py attr lda keeplastcheckpoint is set	ml distributed ldamodel get	0.066667
extra params	extra	0.142857
from this block	linalg block	0.076923
of this instance this updates	ml param	0.009524
of memory for this obj assume	size obj	0.040000
an	power iteration clustering train cls	1.000000
for this obj assume that all the objects	merger object size obj	0.040000
be used again	sql streaming query manager	0.011905
the libsvm format into an rdd of labeledpoint	lib svmfile sc path numfeatures minpartitions	0.125000
of write()	ml pipeline	0.095238
correlation of two columns of a	corr col1 col2 method	0.055556
value to a boolean	to boolean value	0.250000
the :class dataframe to a data	data frame	0.005000
make predictions on a keyed dstream	streaming kmeans predict on values dstream	1.000000
collect the distributed matrix on the driver as	local matrix	0.250000
the distributed matrix on the driver as	to local matrix	0.250000
another module	module	0.111111
load the gaussianmixturemodel from disk	mllib gaussian mixture model load cls sc path	1.000000
returns a :class dataframe representing the	spark session sql sqlquery	0.250000
__init__(self	lsh init	1.000000
commutative reduce	reduce by	0.200000
duplicate rows removed optionally only considering certain columns	duplicates subset	0.500000
gets summary	summary	0.024390
given string	ml param params has param	0.019231
na fill()	sql data frame fillna value subset	0.166667
first n rows to the console	data frame show n truncate	0.333333
decayfactor timeunit	streaming	0.005025
returns an mlreader instance for this class	mlreadable read cls	0.200000
this blockmatrix by other, another	multiply other	0.200000
to a data	sql data	0.024390
batches	streaming linear algorithm	0.076923
in a profile object is returned	profiler profile	0.200000
:py attr linkpower	link power value	1.000000
given parameters in	ml param grid builder	0.055556
contains a param with	param params	0.014925
attr predictions	generalized linear	0.200000
maxmemoryinmb	max memory in mb	1.000000
create an input stream that pulls events from	utils create stream ssc hostname	0.200000
boundaries from start	range between start	0.200000
dtype	dtype	1.000000
use only create a new hivecontext for testing	hive context create for testing cls sparkcontext	0.333333
a specific topic and partition for kafka	topic and partition	0.111111
inside brackets pairs e	brackets	0.058824
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online", learningoffset=1024	lda set params featurescol maxiter seed	0.250000
of	ml param	0.028571
test prediction on a model with weights already	streaming linear regression with tests test prediction	0.500000
pipeline create	pipeline from	0.142857
another timestamp that corresponds to the same time	timestamp timestamp tz	0.166667
the right singular vectors	singular	0.015625
a shortcut of write() save	ml one vs rest save	0.166667
lines text format or newline-delimited json	json	0.043478
data	group by spill	0.047619
probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1	probabilitycol	0.050000
model produced by [[poweriterationclustering]]	power iteration clustering model	0.500000
greatest value of the	sql greatest	0.055556
area under the receiver operating characteristic roc	area under roc	0.500000
in "predictions" which	regression summary	0.107143
importance of each feature	ml gbtregression model feature importances	0.250000
which predictions are	ml isotonic regression	0.111111
get depth of tree (e g depth 0	tree model	0.026316
converts matrix columns in an	convert matrix columns from ml	0.166667
new	sql streaming	0.010204
add two values of the accumulator's data type	accumulator param add	0.333333
forget about past terminated queries	terminated	0.100000
modeltype="multinomial", thresholds=none weightcol=none)	ml naive bayes	0.250000
dataframe in	data frame	0.005000
the day of the month of	dayofmonth	0.027027
returns the greatest value of the list	sql greatest	0.055556
which is assumed to consist of key	key ascending numpartitions keyfunc	0.071429
python code for a shared param class	ml param gen param code name doc defaultvaluestr	0.333333
partial objects do not serialize	cloud pickler save partial obj	0.125000
on rdds of the dstreams	transform dstreams transformfunc	0.125000
computes	compute	0.181818
threshold recall curve	ml binary logistic regression summary recall by threshold	0.166667
used again to wait	sql streaming	0.010204
comprised of	mllib random rdds	0.125000
name of the test	test	0.015152
the right singular vectors of	linalg singular	0.017544
first n rows to the console	sql data frame show n truncate vertical	0.333333
return an iterator that contains all	to local iterator	0.333333
spark sink deployed	storagelevel maxbatchsize	0.045455
setparams(self	ml als set params rank	1.000000
of gaussians in mixture	mixture model	0.066667
tokens in the training	training	0.029412
elements in a numpy ndarray	ml linalg matrix to array	0.166667
na	na	0.833333
writer	writer	0.200000
set bandwidth of each sample defaults	set bandwidth bandwidth	0.142857
dataframe	data frame	0.045000
'x' to all mixture components	mllib gaussian mixture model predict soft	0.142857
the += operator adds a term to	iadd term	0.142857
for this bucketizer	bucketizer	0.125000
set multiple parameters passed as a list	core spark conf set all	0.125000
contents of the :class dataframe	frame	0.034483
top	top	0.857143
tol=1e-6 seed=none layers=none blocksize=128 stepsize=0 03 solver="l-bfgs", initialweights=none)	ml multilayer perceptron classifier	0.142857
this class wraps a function rdd[x] -> rdd[y]	transform function	0.166667
a param with	param	0.012500
model from the	mllib power iteration clustering model	0.500000
for pipeline	ml pipeline	0.047619
the threshold	threshold distcol	0.500000
__init__(self formula=none featurescol="features", labelcol="label", forceindexlabel=false)	rformula init formula featurescol labelcol forceindexlabel	1.000000
the stream query if this is	data stream writer	0.041667
ordinal out of a list or gets an	sql column get	0.142857
mixture	mllib gaussian mixture	0.090909
list of indices to	ml chi sq	0.100000
ignore separators inside brackets	sql ignore brackets split	0.250000
logistic	logistic	0.687500
the trigger for the	writer trigger	0.111111
random	word2vec	0.052632
this configuration contain a given key?	conf contains key	0.333333
the log likelihood of	ml ldamodel log likelihood dataset	0.166667
large dataset and an item approximately find	ml lshmodel approx nearest neighbors dataset key	0.166667
this	params	0.006623
term to this accumulator's	term	0.040000
sampling ratio or no sampling default	samplingratio	0.100000
test that the model	kmeans test test model	1.000000
new dstream by applying reducebykey to each rdd	streaming dstream reduce by key func	0.076923
the population should	mllib stat kernel density	0.066667
splits str around pattern pattern is	split str pattern	0.333333
to make predictions on batches of data from	linear algorithm predict on	0.066667
or compute the number	indexed row matrix num	0.100000
how much of memory for this	external merger object	0.032258
length of a string or	length col	0.050000
the datatype with types inferred from obj	type obj datatype	0.333333
wrap this udf with a function and	sql user defined function	0.083333
converts matrix columns in	mllib mlutils convert matrix columns from ml	0.166667
test that the	streaming kmeans test test	0.500000
again to wait	sql	0.002525
squared distance from	linalg sparse vector squared distance other	0.166667
ranges	ranges	0.454545
for the spark	core spark	0.010309
pos	pos	0.133333
create a python topicandpartition to map	topic and partition init topic partition	0.055556
with a given	param params	0.014925
queries so that	sql streaming query	0.011765
test that	kmeans test test	0.500000
py or	py file	0.066667
maximized true default or minimized false	evaluator is larger better	0.166667
all merged items as iterator	external merger items	1.000000
dstream by applying a function to	map f	0.037037
value to a mllib vector	to vector	0.250000
submit and test a script with a	spark submit tests test	0.090909
sort the list based on first	streaming test case sort result based on key	0.333333
already partitioned data into disks	external group by	0.045455
aggregate the values of each	rdd aggregate by	1.000000
a multi-dimensional rollup	sql data frame rollup	0.055556
carry over its keys	algorithm	0.090909
a multiclass classification model	linear classification model	0.076923
week number of	weekofyear	0.043478
columns that describes the sort order	sql data frame sort cols cols kwargs	0.142857
of this dataset checkpointing can be used to	frame checkpoint eager	0.071429
this tokenizer	ml tokenizer	0.333333
of the dataframe	data frame	0.005000
instance contains a param with a given	params	0.006623
returns a paired rdd where	factorization model	0.043478
the norm of	mllib linalg sparse vector norm	0.083333
the points belongs	predict x	0.033898
__init__(self minsupport=0 3 minconfidence=0 8 itemscol="items", predictioncol="prediction", numpartitions=none)	ml fpgrowth init minsupport minconfidence itemscol predictioncol	1.000000
removes the specified table from the	catalog uncache table tablename	0.250000
to be used with the spark sink deployed	ssc addresses storagelevel maxbatchsize	0.045455
python rdd of key-value pairs (of	rdd save	0.038462
certain time of day in utc	from utc	0.125000
for na fill()	data frame fillna value subset	0.166667
alias for na fill()	data frame fillna value subset	0.166667
increased by step every element	step numslices	0.333333
rdd, a list or	schema samplingratio verifyschema	0.029412
predict the	predict x	0.033898
returned	spark streaming test case	0.333333
this matrix to	matrix	0.015152
the :class dataframe as	data frame writer save as	0.071429
the precision-recall curve which is a dataframe	ml binary logistic regression summary pr	0.083333
the dispatch to handle all function	cloud pickler save function obj name	0.142857
standardization whether to standardize the training features	standardization	0.076923
until any of the	manager await any	0.142857
finding frequent items for columns possibly with	sql data frame freq items cols support	0.166667
fallback to save_string	core cloud pickler save memoryview obj	1.000000
set named options filter out those the	utils set opts schema	0.333333
represents a row-oriented distributed matrix with	row matrix	0.200000
instance to a java pipelinemodel used	ml pipeline model to java	0.100000
a java array	java wrapper new java array	0.333333
a resulting rdd that contains a tuple with	rdd cogroup other numpartitions	0.066667
the input	estimator	0.083333
a python parammap into a java	param map to java	0.250000
from a single kafka topicandpartition	offset	0.021739
given a java pipelinemodel create and return a	pipeline model from java cls	0.200000
convert a dict into a	to	0.007692
the number of fields	struct type len	0.200000
configure the kmeans algorithm for fitting	streaming kmeans	0.035714
driver returns none	driver	0.090909
original column	min max scaler model original	0.062500
blocks	blocks	0.538462
the new	new	0.062500
validation	validation	0.714286
comprised of vectors containing i	mllib random rdds gamma vector	0.125000
mean values	standard scaler model mean	0.125000
sort	sql data frame sort	0.250000
a param with a given string name	param params has param	0.019231
so that :func awaitanytermination() can be used	reset	0.011236
estimator	estimator	0.500000
the rdd's elements	rdd	0.003058
outer join of c{self}	outer join	0.250000
__init__(self labelcol="label", featurescol="features", predictioncol="prediction",	init labelcol featurescol predictioncol	1.000000
the year of a given date as	year	0.040000
this vector	linalg sparse vector	0.111111
the slideduration in seconds	slide duration	0.333333
this udf with a function and	user defined function	0.066667
comprised of vectors containing	mllib random rdds gamma	0.125000
adds a	add	0.035714
lower bound on the log likelihood of	ml ldamodel log likelihood	0.166667
a new rdd of int containing elements from	core spark context range	0.142857
is checkpointed and materialized either	is checkpointed	0.142857
max abs vector	model max abs	1.000000
storage	storage	1.000000
function without	values f	0.062500
vectors containing i i d samples drawn	shape scale numrows	0.125000
binary or multiclass classification	cls data numclasses categoricalfeaturesinfo	0.250000
converter	converter datatype	0.071429
__init__(self inputcol=none outputcol=none	hash lsh init inputcol outputcol	1.000000
according to data	data decayfactor	1.000000
params shared by them	params	0.006623
into	by	0.014286
params instances for the given param and	params	0.006623
the index of the original partition	map partitions with index	0.100000
sparse vector using either a dictionary	vectors sparse size	0.166667
column for approximate distinct count of col	approx count distinct col	0.071429
this dct	ml dct	0.250000
that :func awaitanytermination() can be used	sql	0.002525
returns a paired rdd	factorization	0.038462
with a	params has	0.019231
with	param	0.012500
nodes summed over all	nodes	0.074074
over all trees in the ensemble	tree ensemble model	0.076923
__init__(self inputcol=none outputcol=none seed=none numhashtables=1)	min hash lsh init inputcol outputcol seed numhashtables	1.000000
specified schema	schema	0.033333
the mean variance and count of the	rdd	0.003058
new	sql streaming query	0.011765
new java object	wrapper new java obj java_class	0.333333
used again to wait for new	sql	0.002525
converts vector columns in an input dataframe	mllib mlutils convert vector columns to ml	0.166667
of a streaming dataframe/dataset is written to	writer output mode outputmode	0.083333
:class dataframe representing the database table	sql data frame	0.005348
sample covariance of col1 and	covar samp col1	0.250000
python topicandpartition to map to the java related	init topic partition	0.055556
estimatorparammaps	estimatorparammaps	1.000000
commutative reduce	rdd reduce	0.142857
loads a text file	text path	0.333333
the final value of weights is close	parameter accuracy	0.029412
the result count to the number	num	0.008403
of columns that describes the sort order	frame sort cols cols kwargs	0.142857
of memory for this obj assume	obj	0.023810
instance for params shared	params copy	0.083333
ids of all	stage ids	0.055556
for the test	test	0.015152
converts vector columns in	mlutils convert vector columns from ml dataset	0.166667
for this tokenizer	tokenizer	0.125000
special result iterable this is used because the	result iterable	0.500000
converts vector columns in	mllib mlutils convert vector columns from ml dataset	0.166667
return an rdd created by	core rdd	0.003460
merges them with extra values	extra	0.023810
an input from tcp source hostname port data	streaming context socket text stream hostname port	1.000000
stream query if	data stream	0.028571
model to make predictions on batches of	linear algorithm predict on	0.066667
of the current [[dataframe]] and perform the	sql grouped data pivot pivot_col values	0.050000
an object is	obj	0.023810
test that the final	test	0.015152
queries so that :func awaitanytermination() can be used	reset	0.011236
this model instance	model	0.022346
wait for the execution to stop	streaming streaming context await termination timeout	0.166667
objective	objective	1.000000
underlying output	frame writer	0.050000
create a new accumulator with a	accumulator init	0.083333
the dot product of two	dense vector dot	0.050000
new :class column for approximate distinct count of	approx count distinct	0.071429
returns an array of the	sql	0.002525
sort the list	case sort result	0.333333
the least value of the list of column	sql least	0.055556
for each key using	by key	0.052632
again to wait	streaming query manager reset	0.011905
recommends the top	model recommend	0.250000
test of the observed data	test observed	0.090909
a jvm scala map from	data frame jmap jm	0.111111
the year of a	year col	0.050000
decision tree-based ensemble algorithms parameters	tree ensemble params	1.000000
soundex encoding for a string >>> df	soundex	0.043478
stdout id is the rdd id	core profiler show id	0.333333
sqlcontext	sql sqlcontext	0.047619
return sparkcontext which is associated with this streamingcontext	streaming streaming context spark context	0.500000
sample without replacement	data frame sample	0.066667
over all trees in the ensemble	mllib tree ensemble	0.111111
with new specified column names	df	0.111111
awaitanytermination() can be	query manager	0.011905
are the right singular vectors	mllib linalg singular	0.017544
operation test for dstream groupbykey	operation tests test group by key	1.000000
until any of the queries on	sql streaming query manager await any	0.142857
for data sampled	data distname	0.083333
already partitioned data into disks	core external group by spill	0.047619
table via jdbc	jdbc url table mode	0.200000
that :func awaitanytermination()	query manager	0.011905
pca	ml pca	0.333333
the list based on first value	based on	0.111111
a csv file and	csv path schema sep encoding	0.166667
characters as a hexadecimal number	sql unhex col	0.142857
deprecated use mappartitionswithindex instead	core rdd map partitions with split f preservespartitioning	0.500000
optional parameters	params	0.006623
in tree including	tree model	0.026316
the cluster	kmeans model cluster	0.333333
columns are the left singular vectors of the	mllib linalg singular	0.017544
value of	ml rformula	0.500000
setparams(self	train validation split set params	1.000000
buckets the output by the given columns if	writer bucket by numbuckets	0.200000
brackets pairs e g	brackets split	0.083333
how much of	core external merger object	0.032258
the importance of each feature	ml decision tree classification model feature importances	0.250000
dot product of two vectors we	linalg dense vector dot	0.058824
count the number	count	0.016949
ignore separators inside brackets pairs e g	sql ignore brackets split	0.250000
finding frequent items for columns possibly with false	frame freq items cols support	0.166667
the given date	date	0.037037
elasticnetparam	elastic net	0.125000
instance contains a param with a given string	has param	0.019231
the levenshtein distance of	levenshtein	0.045455
predictioncol="prediction", labelcol="label",	predictioncol labelcol	1.000000
the :class dataframe as the specified table	sql data frame writer save as table	0.333333
list of names of	table names	0.066667
so that :func	sql streaming	0.010204
partitioned	core external group by spill	0.047619
sql user-defined type udt for vector	vector udt	1.000000
:class dataframe, using the given join expression	sql data frame join	0.500000
of tables/views	tables	0.071429
fp-growth model that contains frequent	fpgrowth train cls data minsupport numpartitions	0.200000
the :class dataframe using the specified columns so	data frame	0.005000
null model	null	0.125000
returns true positive rate for a given label	metrics true positive rate label	1.000000
generates an rdd comprised of	random rdds exponential vector rdd sc mean	0.200000
queries so that :func awaitanytermination() can	streaming	0.005025
as	row as	1.000000
the sum for each numeric columns for each	sql grouped data sum	0.083333
"predictions" which gives	linear regression summary	0.041667
converts matrix columns in an input dataframe from	convert matrix columns to ml	0.166667
new column of corresponding string values	string	0.041667
word2vec model	word2vec model	0.500000
of columns that describes the sort	data frame sort cols cols kwargs	0.142857
a param	has param	0.019231
the accumulator's value only usable in driver	core accumulator value	0.045455
elasticnetparam the elasticnet mixing parameter in	elastic net	0.125000
the python module	module	0.111111
boundaries in increasing order for which	ml isotonic regression model boundaries	0.333333
a python topicandpartition to map to the java	topic and partition init topic partition	0.055556
distributed model to a local	distributed ldamodel to local	0.111111
specifies how data of a	data stream	0.028571
arbitrary key and value	core spark context	0.023256
available on all nodes or any hadoop-supported	path	0.020408
max	max abs scaler model max	1.000000
containing a json string into	from json	0.166667
of all active	active	0.125000
list of functions registered in the specified	list functions	0.250000
ordered in ascending order or	take ordered	0.125000
:class windowspec with the	sql window range between	0.166667
a :class datatype the data type string	datatype string s	0.111111
returns true	metrics true	1.000000
number of columns that make up each block	mllib linalg block matrix cols per block	0.333333
checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1 0)	ml random forest classifier	0.023256
including lambda function	function name	0.166667
the soundex encoding for a	soundex	0.043478
@param input dataset for the test	case test func input func expected sort	0.333333
the precision-recall curve which is a	ml binary logistic regression summary pr	0.083333
multi-dimensional cube for the current :class dataframe	frame cube	0.055556
comprised of vectors containing i i	random rdds poisson vector	0.125000
to wait for new	query	0.010753
sets	multiclass classification evaluator set	1.000000
iterator	iterator	0.600000
do profiling on the	core profiler profile	0.500000
'x' has maximum membership in this model	mllib gaussian mixture model predict x	0.500000
list of tables/views in the specified	list tables	0.250000
for k classes classification	classes	0.034483
external list for	external list	0.166667
the given parameters in this grid to fixed	ml param grid builder base	0.076923
leaf nodes	nodes	0.037037
the given spark runtime configuration property	runtime config	0.333333
vector columns	vector columns from ml	0.142857
count of the rdd's elements	rdd	0.003058
setparams(self featurescol="features", predictioncol="prediction", maxiter=20 seed=none k=4 mindivisibleclustersize=1 0)	bisecting kmeans set params featurescol predictioncol maxiter seed	1.000000
wait a given amount	timeout catch_assertions	0.125000
parses a column containing a json	sql from json col	0.083333
squared distance	squared distance	0.142857
accumulator's value only usable in driver program	accumulator value value	0.050000
dump already partitioned	core external group by	0.045455
setparams(self strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none)	strategy missingvalue inputcols outputcols	0.333333
block	linalg block	0.076923
with a randomly generated uid	cross validator	0.045455
partitionby	partitionby	1.000000
content of the :class dataframe in orc	sql data frame writer orc	0.200000
:class rdd, a list or a :class	schema samplingratio verifyschema	0.029412
of users	users	0.066667
how data of	sql data stream	0.031250
block matrix other from this block matrix this	mllib linalg block matrix	0.052632
start to end exclusive increased by step	start end step	1.000000
hexadecimal number	sql unhex col	0.142857
:py attr estimator	estimator value	1.000000
key function	key	0.017857
transforms a python parammap	ml java params transfer	0.125000
matrix to an indexedrowmatrix	block matrix to indexed row matrix	0.333333
setparams(self formula=none featurescol="features", labelcol="label", forceindexlabel=false)	featurescol labelcol forceindexlabel	0.200000
siftdown	siftdown	0.833333
numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	ml random forest	0.071429
string	param params has param	0.019231
saves the content of the dataframe in a	data frame	0.005000
much of memory for this obj assume that	core external merger object size obj	0.040000
class dataframe that	data frame to	0.250000
optionally pass in an existing sparkconf handle	_jconf	0.166667
a python rdd of	rdd save	0.038462
the output by the given columns	writer bucket by	0.100000
table via	url table mode	0.200000
create an input stream that pulls	create stream ssc hostname	0.200000
function to	map values f	0.125000
returns a new	new	0.062500
tests whether this	has param paramname	0.142857
the :class dataframe to a	frame writer save	0.066667
be used again to wait for new	query manager reset	0.011905
a py or zip dependency for all	py	0.050000
elements in seen in	windowduration slideduration	0.083333
setparams(self seed=none) sets params for this test	ml other test params set params seed	1.000000
to all mixture components	gaussian mixture model predict soft x	0.142857
saved using	path minpartitions	0.250000
aggregate the values of each key	rdd aggregate by key	1.000000
add a py or	core spark context add py file path	0.166667
awaitanytermination() can be used again to wait for	query manager reset	0.011905
table named table	table column	0.166667
stopwords=none casesensitive=false) sets	words remover set	0.200000
dump already partitioned	external	0.013889
this instance contains a param with	param params	0.014925
data	sql data	0.097561
an rdd comprised of vectors containing i i	random rdds poisson vector rdd	0.166667
accumulator's	accumulator value	0.050000
estimated coefficients and intercept	linear regression summary	0.013889
samplingratio	samplingratio	0.500000
column	mllib	0.010526
computes an fp-growth model that contains frequent itemsets	mllib fpgrowth train cls data minsupport numpartitions	0.100000
variance and count	core rdd	0.003460
convert this distributed	ml distributed	0.200000
stream returning the result as a :class dataframe	sql data stream reader	0.250000
paired rdd where the first element	matrix factorization model	0.043478
get total number of	model total num	0.333333
returns weighted averaged	multiclass metrics weighted fmeasure	1.000000
object by pyrolite whenever	object rdd	0.500000
compute the dot product	dense vector dot	0.100000
a param	ml param params	0.013699
returns a :class dataframe representing the result of	sql sqlcontext sql sqlquery	0.250000
which predictions are	isotonic regression model	0.100000
using the new hadoop outputformat api mapreduce	as new apihadoop dataset conf keyconverter valueconverter	0.142857
contains a param with a given	has param	0.019231
be used again	manager reset	0.011905
matrix other from this block matrix this	mllib linalg block matrix	0.052632
so it can be used in sql statements	name f returntype	0.125000
seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto",	ml random forest classifier	0.023256
the standard deviation of	rdd stdev	0.066667
for this obj assume that all the	merger object size obj	0.040000
basic operation test for dstream mapvalues	basic operation tests test map values	1.000000
any hadoop file system using the new	as new	0.125000
wait a given	timeout catch_assertions	0.125000
field in :class structtype	struct field	0.500000
pearson's independence test using dataset	chi square test test dataset featurescol labelcol	0.333333
queries so that :func awaitanytermination()	sql streaming query manager reset	0.011905
current [[dataframe]] and perform the	sql grouped data pivot pivot_col values	0.050000
a java storagelevel based on	context get java	0.333333
the dot product of two	dot	0.040000
finding frequent items	data frame freq items	0.166667
registered with the dispatch to handle all function	core cloud pickler save function obj name	0.142857
batches of data from a	mllib streaming linear algorithm	0.166667
get	rdd get	0.200000
changes the uid	params reset uid	0.333333
and refresh all the cached the	refresh	0.100000
this instance contains a param with a given	ml param params	0.013699
dump already partitioned data	spill	0.038462
user defined	user defined	0.333333
set multiple parameters passed as a list	spark conf set	0.111111
get all values as a list	core spark conf get all	0.166667
a new sparkcontext at least	spark context init	0.083333
the square root of the mean squared error	root mean squared error	0.500000
for approximate distinct count	approx count distinct col rsd	0.066667
paired	matrix factorization	0.040000
the input java	java estimator	0.200000
reduce	core rdd default reduce	1.000000
restore an object of	core restore name fields value	0.333333
flatmapvalues	flat map values	1.000000
memory	core external merger object size	0.032258
test that the final value	sgdtests test	0.142857
delaythreshold	delaythreshold	1.000000
column for distinct	distinct col	0.166667
line in libsvm format into label	mllib mlutils parse libsvm line line multiclass	0.111111
in which each rdd contains the count	count by	0.100000
singular	singular	0.140625
comprised of vectors containing	mllib random rdds log normal vector	0.125000
for each numeric columns for	grouped data avg	0.058824
for params shared by	params copy	0.083333
initialized or	initialized	0.125000
defined function	defined function	0.066667
copy of the rdd partitioned using	rdd partition	0.062500
residual degrees	regression summary residual degree	0.500000
python topicandpartition to map to the java	topic partition	0.055556
for approximate distinct count of	approx count distinct col	0.071429
value of weights is close to the desired	parameter accuracy	0.029412
the content of the dataframe in a text	sql data frame writer text	0.200000
if observed is vector conduct pearson's chi-squared goodness	stat statistics chi sq	0.066667
a function on each rdd of this	transform func	0.058824
set initial centers should	streaming kmeans set initial centers centers weights	0.200000
batches of data from a	linear algorithm	0.076923
partitions to use during reduce tasks (e	core rdd default reduce partitions	0.166667
the length of	length	0.040000
evaluates the model on a test dataset	linear regression model evaluate dataset	1.000000
format (json lines text format or newline-delimited json	json	0.043478
condition to pass else fail with	test case eventually condition	0.333333
the union	union	0.090909
idfmodel	idfmodel	0.833333
already partitioned data into disks	group by	0.041667
:class windowspec with the frame boundaries defined from	sql window range	0.166667
return string prefix-time suffix	streaming rdd to file name prefix suffix timestamp	1.000000
list of	ml bisecting kmeans	0.062500
returns the value of spark sql	sql sqlcontext get	0.333333
find	word2vec model find	1.000000
a table	table tablename	0.083333
to make predictions on batches of data	algorithm predict on	0.066667
called when processing of a batch of jobs	streaming streaming listener on batch	0.333333
names of tables in the database dbname	table names dbname	0.500000
an	numpartitions	0.125000
given a java object	params from java	0.333333
and profiles the method	core basic	0.066667
creates a :class dataframe from an	spark session create	0.058824
jobs started by this thread until the	job	0.023810
finding frequent items for columns possibly with	freq items cols	0.166667
the initial value of weights	logistic regression with sgd set initial weights	0.333333
saves the contents of the	path format mode partitionby	0.200000
extract the minutes of a given date	sql minute col	0.050000
python wrapper of	ml java params	0.125000
returns a function	f	0.010526
convert this vector to the	mllib linalg sparse vector	0.111111
the threshold that	threshold value	0.166667
can be used again to wait for	streaming	0.005025
inherit documentation from	inherit doc cls	0.045455
list based on first value	based on	0.111111
column names	sql	0.005051
singularvaluedecomposition	value decomposition v	0.250000
wait until any of the queries on	await any termination timeout	0.166667
extract the year of a	sql year col	0.050000
binary mathfunction by name	binary mathfunction name doc	1.000000
dot product	dot	0.080000
rowmatrix	row	0.125000
libsvm format into label	mlutils parse libsvm	0.125000
table via	url table mode properties	0.200000
the year of a given date as	year col	0.050000
mean variance and	core rdd	0.003460
array that starts at pos in	pos	0.022222
two columns of a dataframe	data frame corr col1	0.166667
on first	on key	0.333333
spark sink deployed on a	addresses storagelevel maxbatchsize	0.045455
the attempt numbers are correctly reported	test attempt number	0.333333
in utc returns	sql from utc	0.142857
job of a batch has started	started outputoperationstarted	0.125000
adds a term to	add term	0.066667
the dot product of two vectors we	ml linalg dense vector dot	0.090909
clusters	power iteration clustering model k	0.200000
returns an mlwriter instance for this ml instance	mlwritable write	0.200000
this query either by :func	streaming	0.005025
rdd, a list or a :class	schema samplingratio verifyschema	0.029412
for which predictions are	ml isotonic regression	0.111111
param	params has param	0.019231
with a given string name	ml param params	0.013699
this obj assume that all the	obj	0.023810
values of each	by	0.014286
column scipy matrix from a dictionary	mllib sci py tests scipy matrix size	0.090909
sets default params	param params set default	1.000000
of deserialized batches lists of objects from	serializer load stream without unbatching	0.200000
python rdd of key-value pairs (of form	core rdd save	0.037975
inputformat with arbitrary key and value class from	inputformatclass	0.095238
checkpointpath	checkpointpath	1.000000
dispatch to handle all function	core cloud pickler save function	0.142857
from this thread such as the spark	spark context	0.023256
convert this matrix to the	mllib linalg dense matrix	0.083333
in this :class dataframe	data frame	0.005000
term to	accumulator add term	0.066667
compute	core	0.003021
curve which is	ml binary logistic regression	0.142857
python rdd of key-value pairs (of form c{rdd[	rdd save as	0.038462
of two columns of a dataframe as	sql data frame corr	0.166667
numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")	numbuckets inputcol outputcol	1.000000
set multiple parameters passed as a list	spark conf	0.058824
deviance for the null model	ml generalized linear regression summary null deviance	0.250000
used	manager	0.011236
data type	sql parse datatype	0.333333
representing	sqlquery	0.027027
with new specified column	df	0.111111
a class inherit documentation from	mllib inherit	0.045455
probabilities	probability	0.100000
table accessible via jdbc url url and connection	reader jdbc url table column	0.166667
observed tokens in the training set given	training	0.029412
sets	random forest params set	1.000000
get the offsetrange of specific	offset ranges	0.166667
so it can be used in sql statements	f returntype	0.125000
how data of	sql data	0.024390
a given string name	ml	0.001835
with a randomly	split	0.125000
handleinvalid	handle invalid	1.000000
:class dataframereader	session	0.050000
new rdd by applying a function to	f	0.021053
__init__(self featurescol="features", labelcol="label",	ml decision tree classifier init featurescol labelcol	1.000000
used	streaming query manager	0.011236
a right outer join of c{self} and c{other}	core rdd full outer join other numpartitions	0.200000
the norm	norm p	0.166667
the sql context to use for saving	ml mlwriter context sqlcontext	0.333333
gets	get estimator param	1.000000
precision-recall curve which is a dataframe containing two	ml binary logistic regression summary pr	0.083333
invalid	invalid	1.000000
the index	index	0.041667
sets the accumulator's	accumulator value	0.050000
a local property that affects jobs submitted	local property key value	0.076923
the centroids according	decayfactor timeunit	0.250000
or compute the number	mllib linalg distributed matrix num	0.166667
creates a :class dataframe from an	session create	0.058824
a converter to drop the names of fields	converter	0.052632
number of nonzero	linalg sparse vector num	0.200000
in "predictions" which gives the true label	logistic regression summary label col	0.333333
an external table based on the dataset in	external table tablename path	0.090909
parameters in this grid to fixed values	builder add grid param values	0.333333
train a random forest model for binary or	mllib random forest train classifier	0.250000
a resulting rdd that contains a tuple	core rdd cogroup other	0.066667
each original column during fitting	scaler model original	0.062500
of	ml bisecting kmeans	0.125000
wait for	streaming query manager reset	0.011905
obj assume that all the objects	external merger object size obj	0.040000
stats	stats	0.277778
each rdds into	dstream n block	0.333333
column denoted by name	getattr name	1.000000
find all globals names read or written	pickler extract code globals	0.125000
distance from a sparsevector or 1-dimensional	distance	0.095238
interface used to load a :class dataframe	data frame	0.005000
observed data against the expected	observed expected	0.166667
to wait for new terminations	sql streaming	0.010204
importances	importances	1.000000
rating for the	mllib matrix factorization	0.250000
dataset for the test this	test	0.015152
the configuration property for the given key	key	0.017857
the column	standard scaler model	0.090909
with the spark sink deployed on a	storagelevel maxbatchsize	0.045455
checkpoint data or	streaming context get or	0.200000
key using an associative and commutative reduce function	rdd reduce by key	0.333333
py or zip dependency for all tasks to	py file path	0.066667
instance	param params has param	0.019231
path the model	path	0.010204
exception that	query exception	0.500000
vector columns in	vector columns to ml	0.142857
:func awaitanytermination() can be used	reset	0.011236
the offsetrange of specific kafkardd	kafka rdd offset ranges	0.333333
data type	datatype	0.045455
instance's params to the wrapped java object and	params to	0.035714
converts matrix columns in an input dataframe	convert matrix columns to ml	0.166667
instance for this	ml one vs rest	0.052632
from start to end exclusive increased by	context range start end	0.333333
converts matrix columns in an input dataframe	mlutils convert matrix columns from ml dataset	0.166667
set the trigger for the stream query if	stream writer trigger	0.083333
convert this matrix to	mllib linalg dense matrix	0.083333
called when processing of a batch	streaming listener on batch	0.333333
levenshtein distance of the two given strings	sql levenshtein left	0.058824
the contents of the :class dataframe to	frame writer save	0.066667
this instance with a randomly generated	train validation split model	0.166667
__init__(self inputcol=none outputcol=none seed=none numhashtables=1)	init inputcol outputcol seed numhashtables	1.000000
comprised of vectors	mllib random rdds normal	0.125000
wait	manager reset	0.011905
called when a	listener on	0.200000
nodes summed over all trees	nodes	0.074074
globals names read or written to by	core cloud pickler extract code globals	0.125000
and combiner	core	0.003021
add a py or	spark context add py file	0.166667
transforms the input dataset with optional parameters	ml transformer transform dataset params	1.000000
sort the list	streaming py spark streaming test case sort result	0.333333
the optional key function	key	0.017857
level to persist its values across operations after	core rdd persist storagelevel	0.166667
pairs or two separate arrays of indices and	ml linalg	0.030303
dump	by	0.014286
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini",	classifier	0.050000
of the dataframe in a	sql data frame	0.005348
initial value of weights	initial weights	0.250000
called when	streaming streaming listener on	0.200000
waits for the termination	termination timeout	0.041667
comprised of vectors containing i	mllib random rdds exponential vector	0.125000
output a python rdd of	rdd	0.012232
to that user	user	0.055556
an input from tcp source hostname port data	streaming streaming context socket text stream hostname port	1.000000
is close to the desired	parameter accuracy	0.029412
again to wait for	query manager	0.011905
merge the values for each key	by key func numpartitions	0.062500
get or compute the number of	mllib linalg indexed row matrix num	0.125000
metric	metric	1.000000
an rdd of row or	rdd	0.006116
the correlation of two columns of a	col2 method	0.055556
of active queries associated with this sqlcontext	streaming query manager active	0.066667
convert the vector into	linalg vector to	1.000000
matrix to an indexedrowmatrix	mllib linalg block matrix to indexed row matrix	0.333333
from an :class rdd, a	schema samplingratio verifyschema	0.029412
given path a shortcut of write() save path	ml mlwritable save path	0.200000
the stream query if this	sql data stream writer	0.041667
note : experimental	one vs rest model	0.058824
list of indices	ml chi	0.100000
an rdd of points using the model trained	mllib logistic regression model	0.083333
load a partition from disk then sort	merge sorted items index	0.250000
the current [[dataframe]] and perform the	grouped data pivot pivot_col values	0.050000
feature selection by number	chi sq selector set num	0.250000
creates an external table based on the dataset	sqlcontext create external table tablename path	0.250000
the input dataset	dataset	0.081633
so	manager reset	0.011905
sample without	data frame sample	0.066667
the :class dataframe to a data	data frame writer save path	0.142857
so that :func awaitanytermination() can be used	query	0.010753
memory	memory	1.000000
newline-delimited json	writer json path	0.125000
levenshtein distance of the two	levenshtein left right	0.058824
values for each numeric	grouped data	0.035714
a python topicandpartition to map to the	and partition init topic partition	0.055556
current date as a date column	current date	1.000000
outputcols	outputcols	1.000000
:class dataframe to a data source	data frame writer save path format	0.142857
to wait for	sql streaming query	0.011765
comprised of vectors containing i i d	mllib random rdds	0.083333
with singular	linalg singular	0.017544
and count	rdd	0.003058
the elements in seen in a sliding	windowduration slideduration	0.083333
loader	loader	0.625000
hadoop file system using the old hadoop	hadoop	0.050000
call java	call java	1.000000
error which is defined as the	regression	0.010000
python rdd of key-value pairs (of	core rdd save as	0.037500
or compute the number of	linalg block matrix num	0.100000
parses the	json_string	0.125000
in "predictions" which gives the true label	linear regression summary label	0.333333
of the date column	next day date	0.100000
a range of offsets from	offset range	0.047619
convergencetol	convergencetol	1.000000
returns accuracy equals to	metrics accuracy	0.166667
id to all the jobs	job	0.023810
hostname port data is received using	hostname port storagelevel	0.500000
the probability of obtaining a test statistic result	stat test result	0.166667
tree (e g depth	tree	0.020833
java_model	java_model	0.454545
skipping	sql	0.005051
converts vector columns in an input dataframe from	convert vector columns to ml	0.166667
in the ensemble	ensemble model	0.117647
transfer this instance's params to the	ml java params to	0.045455
partial objects do not	cloud pickler save partial	0.125000
squared distance from a	ml linalg sparse vector squared distance	0.166667
"num" number of	num	0.016807
or [[arraytype]] of [[structtype]]s with the specified schema	schema options	0.125000
the idf vector	ml idfmodel idf	0.333333
the values for each key using an	key func numpartitions	0.066667
table named table accessible via jdbc url url	jdbc url table column	0.166667
stream query if this is not	sql data stream	0.031250
to the same time of day	to	0.007692
int containing elements	core	0.003021
extract a specific group matched by a	extract str pattern idx	0.333333
return an rdd containing all pairs	rdd	0.003058
contains a param with a	ml param	0.009524
context to use for	context	0.090909
computes column-wise summary statistics for	mllib stat statistics col	0.200000
a method for given binary operator	sql bin op name doc	0.200000
a keyed	values	0.050000
impurity="variance", seed=none	tree regressor	0.058824
already partitioned data into	core external group by spill	0.047619
accumulator with a	accumulator init	0.083333
cluster centers represented as a list	kmeans model cluster centers	0.090909
approximate distinct count	approx count distinct	0.071429
get a local	spark context get local	0.333333
instance contains a param with a	param	0.012500
adds a	core	0.003021
converts vector columns in an input dataframe from	mllib mlutils convert vector columns to ml	0.166667
of the rdd's	core	0.003021
as a temporary table	as table	0.200000
generates an rdd	uniform vector rdd sc	1.000000
format or newline-delimited json	json path	0.100000
that :func awaitanytermination() can be used again	reset	0.011236
as a :class column	data frame getitem item	0.250000
specified database	dbname	0.090909
set the initial value of	sgd set initial	0.111111
with a given	ml param params has	0.019231
from start to end exclusive increased	spark context range start end	0.333333
columns that describes	cols cols kwargs	0.090909
restore an object of namedtuple	core restore name fields	0.333333
any streaminglinearalgorithm	streaming linear algorithm	0.076923
fitintercept	fitintercept	0.294118
with extra	extra	0.023810
compute the dot product of two vectors	mllib linalg dense vector dot	0.058824
a local property set in this	core spark context get local property key	0.066667
the first n	n	0.027778
generates an rdd comprised of vectors containing	random rdds uniform vector rdd sc	0.200000
non-streaming :class dataframe out into	sql data frame write	0.071429
return a resulting rdd that contains a tuple	rdd cogroup other numpartitions	0.066667
partitions to use during reduce tasks (e g	rdd default reduce partitions	0.166667
this thread such as the spark fair	spark context	0.023256
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20	ml random forest classifier	0.023256
timeunit to configure	streaming	0.005025
or names into a jvm seq of column	seq sc cols	0.055556
contents of the :class dataframe to a	frame writer save	0.066667
can be used again	manager reset	0.011905
set multiple parameters passed as	conf set	0.200000
akaike's "an information criterion" aic for the	generalized linear regression summary aic	0.250000
featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor", quantileprobabilities=[0	featurescol labelcol predictioncol	0.090909
of the month of	dayofmonth col	0.031250
a right outer join of c{self} and	core rdd full outer join other	0.200000
groups	group by	0.041667
mean squared error which is defined as	mllib regression	0.022727
t-statistic of estimated coefficients and intercept	ml generalized linear regression training summary t values	1.000000
paired rdd where the	factorization	0.038462
fields in obj	sql	0.002525
dot product of two vectors we support	vector dot other	0.050000
cluster	model cluster	0.333333
linear model that has a vector of coefficients	linear model	0.066667
with a given string name	params has param	0.019231
this stringindexer	ml string indexer	0.166667
that :func	query	0.010753
the accumulator's value only	accumulator	0.012987
dataframestatfunctions for statistic functions	frame stat	0.250000
a function on rdds of the dstreams	transform dstreams transformfunc	0.125000
can be	sql streaming	0.010204
the week number of a given date	sql weekofyear col	0.055556
wait for new terminations	reset	0.011236
outputformat api mapred	outputformatclass keyclass	0.250000
a python topicandpartition	topic and partition init topic partition	0.055556
test that	tests test	0.018519
algorithm return the	train rdd	0.166667
call java function	call java func sc func	1.000000
parses the expression string into	expr	0.076923
two block matrices together the matrices	linalg block	0.076923
mixin for param rawpredictioncol raw prediction a k	has raw prediction	0.333333
of object	ml	0.001835
of	external	0.013889
again to	sql streaming	0.010204
get or compute the number of	row matrix num	0.100000
model with weights already	mllib streaming linear regression with	0.111111
will run the query as fast	processingtime once	0.166667
a dependency on	dependency	0.040000
the java_model to a python	java_model	0.090909
the column standard	mllib standard scaler	0.100000
again to wait for new	query manager reset	0.011905
instance contains a param	ml param params has	0.019231
much of memory for this	core external merger object size	0.032258
python direct kafka stream transform get offsetranges	kafka direct stream transform get offset ranges	0.500000
temp	temp	0.750000
class	profiler collector	0.142857
this instance with a randomly generated uid	one vs rest model	0.058824
generates an rdd comprised of	random rdds log normal vector rdd sc mean	0.200000
which is a risk function corresponding	linear regression summary	0.013889
sets	isotonic regression set	1.000000
datasets	dataseta datasetb	0.500000
first n rows to the console	frame show n truncate vertical	0.333333
function without changing	f	0.010526
the model params	model params	0.125000
contains a param with	ml param params has	0.019231
the first n elements in	n	0.027778
correlation of two	col2 method	0.055556
squared error which is	regression	0.010000
if the table	tablename	0.043478
new :class column for approximate	approx	0.047619
size to be used	size	0.009174
note this docstring is not shown publicly	linalg indexed row matrix init rows numrows numcols	0.333333
column	scaler model	0.153846
broadcast a read-only variable to the cluster	context broadcast	0.125000
how much of memory	object	0.027778
number of rows	indexed row matrix num rows	0.200000
squared distance from a sparsevector or 1-dimensional	mllib linalg sparse vector squared distance other	0.166667
cluster centers represented as a list of numpy	cluster centers	0.060606
+= operator adds a term to this	accumulator iadd term	0.142857
this model instance	linear regression model	0.133333
globals names read or written to by codeblock	code globals cls	1.000000
dstream in which each	by	0.014286
that :func	streaming query	0.010526
get all values	spark conf get all	0.166667
for a shared param class	ml param gen param	0.333333
all trees in the ensemble	ensemble model	0.117647
area under the	classification metrics area under	0.166667
sets params for	set params	0.137931
compute the number	indexed row matrix num	0.100000
wait a given amount of time for a	timeout	0.071429
for converting raw prediction scores into 0/1 predictions	linear classification model	0.076923
compare 2 ml types asserting	test compare	0.166667
users for	users	0.066667
__init__(self strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none):	init strategy missingvalue inputcols outputcols	1.000000
vectors or transform the	mllib hashing tf transform	0.045455
elements in seen in a sliding window of	window windowduration slideduration	0.333333
partitioned data	core external group by spill	0.047619
single script file calling	script with local functions	0.125000
observed is vector conduct pearson's chi-squared goodness of	mllib stat statistics chi sq	0.066667
memory	object	0.027778
model with weights already set	mllib streaming linear regression with	0.111111
can be used again	sql streaming query	0.011765
of offsets from a single kafka	offset	0.021739
columns that describes the sort	frame sort cols cols kwargs	0.142857
the given data type json string	datatype json string	0.333333
value of	ml tree classifier params	1.000000
that user	model user	0.250000
into an rdd of labeledpoint	lib svmfile sc path numfeatures minpartitions	0.125000
hadoop	context hadoop	0.090909
instance	instance	1.000000
used again to wait for new terminations	streaming query manager	0.011236
stores user factors in two columns id and	ml alsmodel user factors	1.000000
this grid to fixed values	add grid param values	1.000000
of weights is close to the	parameter accuracy	0.029412
a jvm seq of column	seq sc cols converter	0.055556
concentration	concentration	1.000000
wait until any of the	sql streaming query manager await any termination timeout	0.166667
the current idf vector	mllib idfmodel idf	0.333333
specified schema	schema options	0.125000
be	sql streaming	0.010204
boosted trees model for classification	boosted trees	0.166667
can be	streaming query manager	0.011236
awaitanytermination() can be used again to wait	sql streaming query manager	0.011905
distinct count of	count distinct col	0.080000
dependencies for debugging	to debug string	0.200000
sort order	frame sort	0.250000
be smaller than or equal to	numiterations	0.050000
of the points belongs to in this model	mllib bisecting kmeans model predict x	0.333333
input schema	reader schema schema	0.333333
threshold	datasetb threshold distcol	0.500000
this thread such as the spark fair scheduler	core spark	0.010309
database table via jdbc	jdbc url table	0.090909
note : experimental	one vs rest	0.068966
total log-likelihood for this	gaussian mixture summary log likelihood	0.142857
accumulator with	accumulator	0.012987
and commutative reduce	core rdd reduce	0.083333
so that :func awaitanytermination() can be used	sql streaming query manager reset	0.011905
hadoop configuration	spark context hadoop	0.090909
load a model from the	mllib power iteration clustering model load cls sc	0.500000
selector	set selector	0.333333
compare 2 ml types asserting	compare	0.071429
number of partitions for hadoop rdds when not	partitions	0.066667
comprised of	random rdds gamma	0.125000
a given string	ml	0.001835
a dummy params instance used as a	params dummy	0.111111
convert a dict	to	0.007692
the month of a given date	sql dayofmonth	0.031250
fp-growth model that contains	fpgrowth train cls data minsupport	0.200000
return a javardd of	core rdd	0.003460
soundex encoding for	soundex	0.043478
get or compute	linalg coordinate matrix	0.250000
the centroids according to	decayfactor timeunit	0.250000
comprised of vectors containing i i d	random rdds gamma vector	0.125000
calculates the approximate quantiles of	approx	0.047619
of indices back to	index to	0.040000
checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto",	ml random forest classifier	0.023256
data	external	0.013889
returns a new java object	wrapper new java obj java_class	0.333333
returns	multiclass metrics	0.750000
number of	ml clustering	0.100000
instance	one vs rest	0.034483
dataset for the test this should be list	test	0.015152
columns of a dataframe	data frame corr col1	0.166667
9 0 95 0 99], quantilescol=none aggregationdepth=2):	fitintercept	0.058824
+= operator adds a term to this	iadd term	0.142857
extract a specific group matched by	sql regexp extract str pattern idx	0.333333
any hadoop file system using the old hadoop	save as hadoop	0.142857
set multiple parameters passed as a list of	core spark conf set	0.100000
wrapped method and saves actual input keyword arguments	core keyword	1.000000
threshold threshold in binary classification prediction in	threshold	0.018182
two separate arrays of	ml linalg	0.030303
and commutative reduce	reduce	0.041667
in tree including	decision tree model	0.050000
the partition id	partition id	0.333333
of byte array that starts at pos in	pos	0.022222
cachenodeids=false checkpointinterval=10 losstype="logistic", maxiter=20	gbtclassifier	0.076923
queries so that :func awaitanytermination() can be used	streaming query	0.010526
tokenizer	tokenizer	0.750000
coefs are predicted accurately by	parameter accuracy	0.029412
value to a	to	0.015385
for binary or multiclass classification	data numclasses categoricalfeaturesinfo	0.250000
and return it	core	0.003021
dataframe produced by the model's transform method	clustering summary predictions	0.500000
saves the contents of the	mode partitionby	0.066667
of memory for this	core external merger object	0.032258
fits a model	ml estimator fit	0.083333
loss	loss	1.000000
comprised of vectors containing i i d samples	random rdds normal vector	0.125000
string in libsvm format	libsvm	0.090909
sets	gbtregressor set	1.000000
term frequency vectors or	hashing tf	0.125000
this obj assume that all	external merger object size obj	0.040000
an input stream	stream ssc hostname port	0.200000
rdd is generated by applying mappartitionswithindex() to each	map partitions with index f preservespartitioning	0.055556
compute the standard deviation of this	stdev	0.047619
parammap into a	param map to	0.125000
for feature selection by	mllib chi sq selector set	0.200000
enable 'with sparkcontext as sc app' syntax	spark context exit type value trace	0.333333
smaller than or equal	numiterations	0.050000
test that the model predicts correctly on toy	test test predict on model	0.500000
convert this distributed model to a local	ml distributed ldamodel to local	0.111111
rdd an rdd	cls rdd	0.250000
input dataset this is called	dataset	0.020408
convert a matrix from the new mllib-local representation	mllib linalg matrices from ml mat	0.333333
finding frequent items for columns	data frame freq items	0.166667
brackets	brackets split	0.083333
the year	sql year	0.050000
temp method for comparing instances	ml ldatest compare m1 m2	0.250000
vectors	linalg vectors	0.333333
word	word	0.777778
removes the specified table from the in-memory cache	sql sqlcontext uncache table tablename	0.250000
load a	svmmodel load cls sc	0.200000
decorator that makes a class inherit documentation from	mllib inherit doc	0.045455
wait for the execution to stop return true	await termination or timeout timeout	0.125000
return an rdd created	rdd	0.003058
setparams(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6	aftsurvival regression set params featurescol labelcol predictioncol	0.500000
python function including lambda function	function name	0.166667
term frequency vectors or transform the rdd	tf transform	0.045455
left singular vectors of the singularvaluedecomposition	mllib linalg singular value decomposition	0.250000
setparams(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor",	ml aftsurvival regression set params featurescol labelcol predictioncol	0.500000
predicting on	streaming	0.005025
drops the	drop	0.222222
every	linear	0.025641
given block matrix other from this	mllib linalg	0.026316
multi-dimensional cube for the current :class dataframe using	data frame cube	0.055556
compute the number of	mllib linalg row matrix num	0.125000
much of memory for this	size	0.009174
for this tokenizer	ml tokenizer	0.333333
abs	abs	0.833333
params	params featurescol labelcol	0.500000
:class dataframe whose schema starts with a	data	0.011628
set	mllib stat kernel density set	1.000000
format into an rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures minpartitions	0.125000
contains a param	param	0.012500
the word2vec model's vocabulary	mllib word2vec	0.125000
iterator that contains all	local iterator	0.333333
an rdd created by piping elements to	core rdd	0.003460
vector columns in an input dataframe to	vector columns from ml dataset	0.142857
wait for new terminations	sql streaming	0.010204
a	rdd	0.003058
sql storage type for this udt	user defined type sql type	0.500000
the spark session to use for loading	ml mlreader session sparksession	0.333333
awaitanytermination() can be used again to	manager reset	0.011905
converts vector columns in an input	mllib mlutils convert vector columns from ml	0.166667
passed as	conf set all	0.250000
number of columns that make up each block	linalg block matrix cols per block	0.333333
class generated by namedtuple	load namedtuple name fields	0.333333
computes an fp-growth model that contains frequent	mllib fpgrowth train cls	0.100000
the uid of this instance	ml param params reset uid newuid	0.058824
extract the year	year col	0.050000
and name or provide	core	0.003021
initialized	ensure initialized	0.333333
finding frequent items for	frame freq items cols support	0.166667
in the training set given the current parameter	training	0.029412
or equal	numiterations	0.050000
how much of memory for	merger	0.025641
:class dataframe as a temporary table in the	data frame as table df tablename	0.333333
finding frequent items for columns possibly with	data frame freq items	0.166667
the initial value	mllib streaming logistic regression with sgd set initial	0.111111
profiler_cls	ctx	0.166667
replace null values alias for na fill()	frame fillna value subset	0.166667
test the python direct kafka stream api	tests test kafka direct stream from	0.333333
infer schema from an rdd of	spark session infer schema rdd samplingratio	0.250000
tree (e g	tree model	0.026316
points belongs to in this model	kmeans model predict x	0.333333
until any of the queries on	any termination	0.142857
shut down the sparkcontext	spark context stop	1.000000
which predictions are	regression model	0.031250
columns on the	data frame	0.010000
to make predictions on the	predict on	0.058824
the minimum number of times a token	min count	0.076923
an rdd that has no partitions	core spark context empty rdd	0.200000
max	ml max abs scaler model max	1.000000
json	json	0.391304
extracts the embedded default param values and	params extract param	0.333333
stop the underlying :class	session stop	1.000000
already partitioned	group by	0.041667
of users for a given product	users product	0.142857
the initial value of weights	sgd set initial weights initialweights	0.333333
can be used	streaming query	0.010526
runs the bisecting k-means algorithm return the model	mllib bisecting kmeans train rdd k maxiterations mindivisibleclustersize	0.333333
of each word in vocabulary	word2vec fit	0.200000
this udt	user defined type	0.250000
the dot product of two vectors we	dense vector dot other	0.050000
adds	accumulator add	0.076923
the sparkcontext	spark context	0.023256
returns a	sql	0.002525
dstream by applying reducebykey to	streaming dstream reduce by key func	0.076923
to a local	to local	0.125000
output a python rdd of key-value pairs	rdd	0.009174
be used again to wait for new terminations	sql streaming	0.010204
result as a :class dataframe	data	0.034884
infer schema from an rdd of row	infer schema rdd samplingratio	0.250000
prediction a	prediction	0.041667
represents an entry of a	matrix entry	0.250000
for the stream query if this	sql data stream writer	0.041667
contains a param with	ml	0.001835
value of	ml multiclass classification evaluator	0.500000
wait until any of the queries	query manager await any termination timeout	0.166667
multi-dimensional rollup for the	data frame rollup	0.055556
that with new specified column	to df	0.250000
wait for	sql streaming	0.010204
accumulator's value only usable	core accumulator	0.030303
of conditions and returns one of multiple	sql column otherwise value	0.050000
inserts the content of	writer insert into	0.333333
in :py attr predictions which gives the predicted	linear regression summary prediction	0.142857
this distributed	distributed	0.111111
__init__(self inverse=false	dct init inverse	1.000000
operation test for dstream countbyvalue	operation tests test count by	1.000000
as a temporary table in the	as table df	0.250000
can be used	sql	0.002525
converts matrix columns in an input dataframe from	convert matrix columns to	0.166667
param and validates the ownership	params resolve param param	0.333333
predict the label of one	predict	0.034483
sets params for	set params featurescol labelcol	0.125000
can be used in sql statements	name f returntype	0.125000
create an	session create	0.117647
compute the number of	linalg row matrix num	0.100000
to all mixture	gaussian mixture	0.038462
how much of memory for this obj assume	object size obj	0.040000
matrix whose columns are the left singular vectors	linalg singular	0.017544
return the	streaming	0.005025
of	ml chi sq	0.100000
finding frequent items	freq items	0.166667
returns a new java object	ml java wrapper new java obj java_class	0.333333
generates	sc	0.125000
for the stream query if this	data stream	0.028571
the underlying output	writer format	0.333333
so that :func awaitanytermination() can be	sql streaming query manager	0.011905
using an associative and commutative reduce	core rdd reduce	0.083333
of the month of a	sql dayofmonth col	0.031250
to a java pipelinemodel used	pipeline model to java	0.100000
of each class as a	ml	0.001835
given string name	ml param params has	0.019231
:func awaitanytermination() can be used	sql streaming query manager	0.011905
a dataframe that stores item	alsmodel item	0.250000
wait for	or timeout timeout	0.125000
awaitanytermination() can	sql streaming query	0.011765
create a column scipy matrix from	sci py tests scipy matrix	0.090909
mindocfreq	min doc freq	1.000000
given path a shortcut of write() save	ml pipeline model save	0.166667
awaitanytermination() can be used	query	0.010753
the	external merger object size	0.032258
create a new sparkcontext	spark context init	0.083333
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params	set params maxcategories inputcol outputcol	0.333333
singular vectors of the singularvaluedecomposition	linalg singular value decomposition	0.250000
of the points belongs to in this model	model predict x	0.250000
value of	ml lda	0.357143
in "predictions" which gives the features of	ml linear regression summary features	0.166667
create an input stream that pulls events from	create stream ssc hostname port storagelevel	0.200000
set named options filter out those the value	utils set opts schema	0.333333
test the stage ids are available and incrementing	core task context tests test stage id	1.000000
point in rdd 'x' to all mixture components	mllib gaussian mixture model predict soft x	0.142857
used with the spark sink deployed on	storagelevel maxbatchsize	0.045455
:class rdd, a list	schema samplingratio verifyschema	0.029412
create a unified dstream from multiple dstreams	streaming streaming context union	0.111111
of this rdd's elements	rdd	0.003058
the pearson correlation coefficient for col1 and	corr col1	0.333333
names	sql	0.005051
the java_model to a python model	create model java_model	0.250000
variance	core	0.003021
of the date column	date	0.037037
contains a	ml	0.001835
value of	ml bisecting kmeans	0.125000
job of a batch has completed	completed outputoperationcompleted	0.125000
formula=none featurescol="features", labelcol="label", forceindexlabel=false) sets params	set params formula featurescol labelcol forceindexlabel	0.166667
is set	context set	0.125000
the training set given the current parameter estimates	distributed ldamodel training	0.034483
initialized	initialized	0.750000
how	external merger object	0.032258
of columns for the given table/view in the	columns	0.019608
rdd's elements in one operation	core rdd	0.003460
a multi-dimensional rollup for the current :class dataframe	sql data frame rollup	0.055556
streamingcontext	context	0.022727
comprised of vectors	random rdds exponential vector	0.125000
of each word in vocabulary	mllib word2vec fit	0.200000
memory for this obj assume that all	size obj	0.040000
jobs has started	started batchstarted	0.250000
into	group by spill	0.047619
the table	tablename	0.043478
'x' to all mixture	mllib gaussian mixture model	0.062500
underlying output data source	sql data stream writer format source	0.333333
column mean	mllib standard scaler model mean	0.125000
predict the	predict	0.068966
named table	table column lowerbound	0.166667
of that particular batch has half the	half	0.058824
attr lda keeplastcheckpoint	distributed ldamodel	0.052632
sort the list based on first	test case sort result based on	0.333333
multiclass	numclasses	0.111111
predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01	predictioncol k probabilitycol	0.333333
dump already partitioned data into	group by spill	0.047619
inputcols input	input	0.090909
:class dataframe, using the given	data frame	0.005000
a given	ml param params	0.013699
which predictions are known	ml isotonic regression	0.111111
kolmogorov-smirnov ks test for data sampled from a	mllib stat statistics kolmogorov smirnov test data	0.111111
into an rdd of labeledpoint	load lib svmfile sc	0.125000
rformula	rformula	0.666667
compare 2 ml params instances for the	compare params m1 m2	0.200000
values for each key	by key numpartitions	0.111111
buckets the output by the given columns	writer bucket by numbuckets	0.200000
labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0	labelcol predictioncol maxdepth	0.333333
:py attr keeplastcheckpoint	keep last checkpoint value	1.000000
if the table is currently cached in-memory	catalog is cached tablename	0.250000
for this :class dataframe	sql data frame	0.005348
a parquet file stream returning the result	stream reader parquet path	0.083333
__init__(self featurescol="features", labelcol="label", predictioncol="prediction",	linear regression init featurescol labelcol predictioncol	1.000000
original column during fitting	max scaler model original	0.062500
compute the number of cols	mllib linalg coordinate matrix num cols	0.333333
of estimated	ml generalized linear regression training summary	0.333333
according to	decayfactor	0.142857
note : experimental	train validation split model	0.166667
residuals deviance pvalues	generalized linear regression	0.090909
registers a python function including lambda function as	register function name f	1.000000
set sample points from the population	mllib stat kernel density set sample sample	0.333333
via jdbc	jdbc url	0.200000
array containing the ids of	stage ids	0.055556
the :class dataframe to a data source	sql data frame writer	0.011628
embedded params to the companion	params transfer params to	0.333333
the first n rows to the console	data frame show n truncate vertical	0.333333
partitioned data into disks	by spill	0.047619
passed as a list	spark conf set all	0.125000
convert this matrix to the new mllib-local representation	linalg matrix as ml	1.000000
with the spark sink	maxbatchsize	0.037037
companion	transfer	0.142857
the model	generalized linear regression model	0.200000
for statistic functions	frame stat	0.250000
the old hadoop	as hadoop	0.142857
this matrix to the new mllib-local representation	matrix as ml	0.250000
count of the rdd's elements in one	core rdd	0.003460
range of offsets from	range	0.030303
set initial centers should be set	set initial centers centers weights	0.200000
this instance to a java pipelinemodel used	pipeline model to java	0.100000
so	manager	0.011236
sets	max scaler set	1.000000
vectors of the singularvaluedecomposition	value decomposition v	0.250000
a given	params has	0.019231
sc app' syntax	exit type value trace	1.000000
to wait for new terminations	reset	0.011236
sorts this rdd which is assumed	core rdd sort by	0.200000
normalizer	normalizer	0.833333
merge the values for each key using an	by key func numpartitions partitionfunc	0.066667
:class dataframe to a	frame writer save path	0.066667
binary or multiclass classification	data numclasses categoricalfeaturesinfo	0.250000
levenshtein distance of the two given	sql levenshtein	0.058824
value of	ml logistic regression	0.222222
the dot product	dense vector dot	0.100000
comprised of vectors containing i i d	random rdds exponential	0.125000
impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1	random forest classifier	0.022727
in mixture	mixture model	0.066667
broadcast	broadcast	0.368421
each	func	0.125000
the trigger	writer trigger	0.111111
from this	linalg	0.022222
computes column-wise summary statistics	mllib stat statistics col stats	0.200000
only create a	create	0.017241
assert both have the same param	m1 m2 param	0.125000
function	function	0.333333
the mean	rdd	0.003058
broadcast variable	broadcast	0.052632
decorator that makes a class inherit documentation	mllib inherit doc cls	0.045455
output a python rdd of key-value pairs	core rdd save as	0.037500
log of	ml	0.001835
parameters passed as	conf set all	0.250000
thread such as the spark fair scheduler	core spark	0.010309
list of columns for	catalog list columns	0.166667
returns the root mean squared error which	linear regression summary	0.013889
enable 'with sparkcontext as sc app' syntax	core spark context exit type value trace	0.333333
a	ml param params has param	0.038462
a value to an int if possible	param type converters to int value	0.250000
rdd 'x' has maximum membership in this model	mllib gaussian mixture model	0.062500
partial objects do	core cloud pickler save partial	0.125000
the	merger object	0.032258
residual degrees of freedom for	regression summary residual degree of freedom	0.125000
left singular vectors of	linalg singular	0.017544
copy all	copy	0.166667
given data type json string	sql parse datatype json string	0.333333
wait for new terminations	streaming query	0.010526
for each original column during	ml min max scaler model original	0.062500
the values for each key using an	by key func numpartitions	0.062500
the root mean squared error which is defined	regression	0.010000
of labeledpoint	lib svmfile	0.125000
a java	java wrapper new java	0.166667
maxheap variant of _siftdown	core siftdown max heap startpos	1.000000
any hadoop-supported file system uri	file path	0.035714
accuracy equals	accuracy	0.076923
use only create a new hivecontext for testing	create for testing cls	0.333333
for aggregator by	doc	0.111111
ensuring all received data has been processed	stopsparkcontext stopgracefully	0.050000
setparams(self formula=none featurescol="features",	formula featurescol	0.200000
zips	zip	0.125000
the norm of a	norm	0.041667
sets	hot encoder set	1.000000
the trigger for the stream	data stream writer trigger	0.083333
streams with option of ensuring all received data	stopsparkcontext stopgracefully	0.050000
saves the content of the dataframe in	sql data frame	0.005348
of tree (e g depth	tree model	0.026316
wrapper of transformeddstream to transform on kafka rdd	kafka transformed dstream	0.333333
obj	obj	0.119048
libsvm format into an rdd of labeledpoint	mlutils load lib svmfile sc	0.125000
get all values as	core spark conf get all	0.166667
of model	decision tree model	0.050000
string in the format supported	s	0.071429
dot product of two	ml linalg dense vector dot other	0.090909
an :class rdd,	schema samplingratio verifyschema	0.029412
perform a right outer join of	rdd full outer join	0.111111
function	function obj	0.500000
underlying output	data frame writer format	0.333333
compare 2 ml	test compare	0.166667
the norm of a sparsevector	mllib linalg sparse vector norm	0.083333
n elements from an rdd	core rdd take	0.200000
that all the objects	merger	0.025641
a given	has param	0.019231
variance and count of the rdd's elements in	core rdd	0.003460
the given parameters in this grid	param grid builder add grid	0.100000
convert this vector to the	linalg sparse vector	0.111111
forest model	forest	0.083333
__init__(self inputcol=none outputcol=none)	tokenizer init inputcol outputcol	1.000000
of estimated coefficients and	ml generalized linear regression training summary	0.166667
this instance contains a param with a given	params	0.006623
the underlying output data source	writer format source	0.333333
test for data sampled from a	test data	0.166667
load a model from the given	mllib matrix factorization model load cls sc	0.333333
which are array-like or buffer	array_like dtype	0.166667
and profiles the method to_profile	core	0.003021
the value of spark sql configuration	sql sqlcontext get conf	0.333333
add a py or zip	core spark context add py	0.166667
data into	external group	0.045455
a range of offsets from a single	range	0.030303
decision	decision	0.631579
out into external storage	sql	0.002525
setparams(self	als set params	1.000000
multinomial logistic	mllib logistic	0.200000
merge the values for each key	by key	0.026316
python parammap into a	param map to	0.125000
find synonyms of a	word2vec model find synonyms	0.333333
if computeu was set to be true	u	0.111111
in rdd 'x' to all mixture components	gaussian mixture model predict soft x	0.142857
cluster centers represented as	mllib bisecting kmeans model cluster centers	0.083333
elements in one	core	0.003021
get all values as a list of key-value	conf get all	0.166667
this distributed model to a local	ml distributed ldamodel to local	0.111111
:func	streaming query	0.010526
awaitanytermination() can be used again to	query manager reset	0.011905
converts matrix columns in an input	mllib mlutils convert matrix columns to	0.166667
return all partitioned	core external merger external	1.000000
a configuration property if not already set	spark conf set if missing	0.500000
this instance contains a param with a given	param params has	0.019231
with	params	0.006623
spark sink deployed on	addresses storagelevel maxbatchsize	0.045455
datasetb	datasetb	1.000000
make predictions on a	kmeans predict on	0.500000
the column standard deviation	standard scaler model std	0.166667
return a copy of the rdd	core rdd partition by	0.333333
evaluates a list of conditions and returns	sql column otherwise value	0.050000
awaitanytermination() can be used again to wait	query	0.010753
sensitive	sensitive	1.000000
converts matrix columns	mllib mlutils convert matrix columns to ml dataset	0.166667
average values for each numeric	grouped	0.035714
"predictions" which gives the true label of	ml linear regression summary label	0.333333
returns the least value of	least	0.043478
on first	on key outputs	0.333333
for each key using	key func	0.066667
test predicted values on a toy model	with sgdtests test predictions	0.500000
by applying a function	f	0.031579
loads a parquet	parquet	0.066667
awaitanytermination()	reset	0.011236
converts vector columns in	mllib mlutils convert vector columns to	0.166667
checkpointinterval=10 seed=none impurity="gini", numtrees=20	random forest classifier	0.022727
after position pos	str pos	0.250000
seed=none layers=none blocksize=128 stepsize=0 03 solver="l-bfgs", initialweights=none)	multilayer perceptron classifier	0.333333
dataframe with two fields threshold recall curve	ml binary logistic regression summary recall by threshold	0.166667
and then merges them with extra values from	map extra	0.040000
query as fast	processingtime once	0.166667
get or compute the	mllib linalg coordinate matrix	0.250000
degrees of freedom	generalized linear regression summary degrees of freedom	1.000000
root mean squared error which is	regression	0.010000
parses a line in libsvm format into label	mlutils parse libsvm line line	0.111111
frequency vectors or transform the rdd of document	mllib hashing tf transform document	0.166667
buckets the output by the given columns	writer bucket by numbuckets col	0.200000
dispatch to handle all function types	cloud pickler save function	0.142857
:py attr numuserblocks	num user blocks value	1.000000
them with extra values	extra	0.023810
catalog	catalog	0.500000
evaluator for multiclass classification	multiclass metrics	0.250000
fp-growth model that contains frequent itemsets	fpgrowth train cls data minsupport numpartitions	0.200000
new rdd that is	rdd coalesce	0.500000
2 ml types asserting that they are equivalent	pipelines m1 m2	0.166667
:func awaitanytermination() can be used	sql streaming query manager reset	0.011905
each original column during fitting	min max scaler model original	0.062500
the content of the :class dataframe	data frame	0.025000
the explained variance regression score	regression metrics explained variance	0.333333
a lower bound on the log	log	0.071429
get or create global	get or create	0.111111
or minimized false	ml evaluator is larger better	0.166667
of this instance with a randomly generated uid	ml one vs	0.142857
partial objects do not serialize correctly in	core cloud pickler save partial obj	0.125000
converts	mllib mlutils convert	0.833333
for rformula	rformula	0.111111
extract the year of a given	year	0.040000
the standard deviation of this rdd's elements	stdev	0.047619
wait for the execution	timeout timeout	0.125000
loads a csv file and	reader csv path schema sep encoding	0.333333
loads a csv file stream	stream reader csv path	0.500000
returns the :class statcounter members as a dict	core stat counter as dict sample	0.333333
create an input stream that pulls events from	create stream ssc hostname port	0.200000
this instance to a java pipelinemodel used for	ml pipeline model to java	0.100000
scalingvec=none	scalingvec	0.166667
for which	isotonic regression	0.090909
a new	java wrapper new	0.333333
a new dstream in which each rdd is	streaming streaming	0.047619
__init__(self featurescol="features",	regression init featurescol	1.000000
completed	completed outputoperationcompleted	0.125000
an object into	obj	0.023810
stepsize step size to be used	step size	0.125000
cluster centers represented as a list of	mllib kmeans model cluster centers	0.083333
this instance	ml param params	0.013699
any hadoop file system using the new	save as new	0.125000
convert this matrix to a coordinatematrix	row matrix to coordinate matrix	0.333333
wait until any of the	any termination timeout	0.166667
note : experimental	core spark context binary records path recordlength	1.000000
in c{self} that is not contained in c{other}	subtract other numpartitions	0.111111
a text file at the specified path	text path compression	0.333333
variance and	core	0.003021
dstream by applying reducebykey to each rdd	streaming dstream reduce by key	0.076923
on a model	streaming linear regression	0.333333
queue	streaming context queue	0.500000
model trained	mllib tree ensemble model	0.058824
perform a left outer join of c{self}	left outer join	0.111111
left outer join of	left outer join other	0.111111
rdd of key-value pairs (of	core rdd	0.010381
convert a value to a boolean if possible	param type converters to boolean	0.250000
get number of trees in ensemble	mllib tree ensemble model num trees	1.000000
adds input options	frame reader options	1.000000
in sql statements	name f returntype	0.125000
parses a line in libsvm format	libsvm line line multiclass	0.333333
of value	value	0.008547
with a function and attach docstring from func	function wrapped	0.333333
output a python rdd of	rdd save	0.038462
a new dstream by applying a function to	map f	0.037037
specifies how data of	sql data	0.024390
a term to	term	0.040000
the column	standard	0.071429
sql context to use for saving	mlwriter context sqlcontext	0.333333
is later than the value of	dayofweek	0.037037
modules	modules	0.833333
maxtermspertopic	maxtermspertopic	1.000000
column standard	mllib standard scaler	0.100000
distinct	distinct numpartitions	0.142857
format or newline-delimited json <http //jsonlines	writer json path mode compression dateformat	0.166667
__init__(self	ml imputer init	1.000000
that :func awaitanytermination() can be used again	manager	0.011236
optional	ml param params	0.013699
mean variance and count of the	rdd	0.003058
for the termination	termination	0.035714
current status of the	status	0.111111
private java model produced by a	java classification model	0.333333
the ensemble	ensemble	0.100000
to term frequency vectors or transform the	tf transform	0.045455
create an input stream that pulls events	utils create stream ssc hostname	0.200000
map of words to their vector representations	word2vec model get vectors	0.166667
can be used again	streaming query	0.010526
array containing the ids of all active stages	get active stage ids	0.250000
all nodes or any hadoop-supported file system uri	file path	0.035714
version of a heappush followed	heap item	0.125000
awaitanytermination() can be used again to wait for	sql streaming query manager	0.011905
d	d	0.625000
until any of the queries on the associated	sql streaming query manager await any termination	0.142857
a left outer join of c{self} and	core rdd left outer join other numpartitions	0.200000
in c{self} and c{other}	join	0.034483
this instance contains a param with a given	has param	0.019231
the companion	ml java params transfer	0.125000
paired rdd where	factorization model	0.043478
creates	sql spark session create	0.142857
the query	streaming query	0.010526
transforms a python parammap	java params transfer param	0.250000
sort the list based on	sort result based on	0.333333
this instance with a randomly generated	cross validator model	0.050000
java parammap into	java javaparammap	0.125000
indices=none names=none)	indices names	1.000000
terms to term frequency vectors or	tf	0.076923
with option of ensuring all received data	stopsparkcontext stopgracefully	0.050000
track supported random forest	random forest	0.041667
session to use for loading	ml mlreader session	1.000000
number of possible outcomes for k classes	num classes	0.500000
finding frequent items for	frame freq items	0.166667
:func	query manager reset	0.011905
norm of	ml linalg dense vector norm p	0.333333
class inherit documentation	mllib inherit doc	0.045455
'new api' hadoop	context new apihadoop	0.333333
broadcast a read-only variable to the cluster returning	broadcast value	0.125000
norm of	linalg sparse vector norm	0.066667
the dot product of two	dot other	0.050000
java storagelevel based	get java	0.111111
new dstream by applying reducebykey to each rdd	streaming dstream reduce by	0.076923
instance contains a param	param params has param	0.019231
computes the singular	compute	0.181818
given path a shortcut of write() save path	ml pipeline save path	0.200000
an rdd	log normal vector rdd	1.000000
is	basic profiler	1.000000
predicts rating for the given user and	mllib matrix factorization model predict user	0.500000
a param with a given	ml param params has	0.019231
in rdd 'x' to all mixture	mixture model predict	0.125000
of nonzero elements	ml	0.001835
passed as a list of	spark conf set	0.111111
a converter	converter	0.052632
of obtaining a test statistic result	stat test result	0.166667
sample without replacement based on the fraction given	frame sample	0.066667
content of the :class dataframe in json	sql data frame writer	0.011628
__init__(self featurescol="features", labelcol="label",	decision tree classifier init featurescol labelcol	1.000000
return the column standard	standard scaler model	0.090909
objective	regression training summary objective	1.000000
name of the file to which this	file	0.028571
list of names of tables in	sqlcontext table names	0.066667
that starts at pos in byte and is	pos	0.022222
java storagelevel	java	0.012195
as a text file using string representations	as text file path compressioncodecclass	0.500000
utils for generating linear data	linear data generator	1.000000
results as a	schema primitivesasstring prefersdecimal	0.500000
given path a shortcut of write() save	ml one vs rest save	0.166667
results immediately to the master as	locally func	0.142857
rdd	core rdd	0.031142
function to the value of each key-value pairs	values f	0.062500
containing only the elements that satisfy predicate	filter f	0.500000
finding frequent items for	sql data frame freq items	0.166667
returns an mlwriter instance for this ml instance	ml pipeline model write	1.000000
be used with the spark sink	ssc addresses storagelevel maxbatchsize	0.045455
dump	external group	0.045455
min-max normalization or rescaling	min	0.041667
used again to wait for new terminations	streaming query manager reset	0.011905
set a local property	set local property key value	0.200000
the optional key	key	0.017857
return a new dstream by applying reducebykey to	streaming dstream reduce by key func numpartitions	0.076923
approximate quantiles	approx	0.047619
all mixture	gaussian mixture model	0.052632
of rows of blocks	row blocks	0.250000
the dot product of	dot other	0.050000
to a new	to	0.007692
a param with a given string	param params has	0.019231
return the	rdd	0.003058
the indexedrowmatrix	mllib linalg indexed row matrix	0.250000
inputformat with arbitrary key and value class	inputformatclass keyclass	0.125000
for distinct	distinct col	0.166667
do profiling on the function func	core profiler profile func	1.000000
a field by name in	field name	0.166667
months between date1 and date2	months between date1 date2	0.333333
of deserialized batches lists of	stream without unbatching	0.125000
return whether this rdd is checkpointed and	core rdd is checkpointed	0.333333
average values for each numeric columns for	sql grouped data	0.041667
a temporary table	table df	0.083333
into a java	param map to java	0.250000
can be used again to wait for	reset	0.011236
a streamingcontext from checkpoint data or create a	streaming context get or create cls	1.000000
be used	reset	0.011236
returns accuracy equals to	multiclass metrics accuracy	0.166667
performs the kolmogorov-smirnov ks test for	mllib stat statistics kolmogorov smirnov test	0.166667
the spark sink	addresses storagelevel maxbatchsize	0.045455
accumulator's value only usable	accumulator value value	0.050000
infer schema from an rdd	spark session infer schema rdd	0.250000
threshold if any used for converting raw prediction	threshold	0.018182
stream query	data stream writer	0.041667
as spark executor memory this must	core spark	0.010309
list of numpy	ml bisecting kmeans model	0.076923
abs vector	abs	0.166667
stop the execution of the streams	context stop	0.125000
elements in one operation	core	0.003021
for every feature	model	0.005587
this instance with a randomly	one vs rest	0.034483
parses a column containing a json string	from json col	0.083333
to all mixture	mixture	0.052632
collect	collect	0.625000
function	user defined function	0.066667
for data sampled from a continuous	data distname	0.083333
registered with the dispatch to handle all function	core cloud pickler save function	0.142857
mean variance and count of	rdd	0.003058
adds a	accumulator add	0.076923
the underlying output	sql data frame writer format	0.333333
all mixture components	gaussian mixture model predict soft	0.142857
an rdd of points using the model trained	logistic regression model	0.083333
this	ml param	0.009524
predictions	generalized linear	0.200000
scale <	scale	0.100000
param in the user-supplied param	param	0.006250
comprised of vectors containing i i d	random rdds exponential vector	0.125000
class inherit documentation from its	mllib inherit	0.045455
returns a :class dataframe representing the result of	sqlcontext sql sqlquery	0.250000
saves the contents of the :class dataframe to	frame writer save path format mode partitionby	0.500000
until any of the queries on	await any	0.142857
the stream query if	stream	0.017544
that :func awaitanytermination()	reset	0.011236
dot product of	linalg dense vector dot	0.058824
value of weights is close to	parameter accuracy	0.029412
the spark sink deployed on	addresses storagelevel maxbatchsize	0.045455
text file using string representations of elements	text file path compressioncodecclass	0.166667
hadoop configuration which is	spark context hadoop	0.090909
so that :func awaitanytermination() can be used	sql streaming query manager	0.011905
cluster centers represented as a list of	kmeans model cluster centers	0.060606
"zerovalue" which may be	fold	0.076923
a shared param class	ml param gen param	0.333333
for new	manager	0.011236
test the python direct kafka stream api with	tests test kafka direct stream from	0.333333
compressioncodecclass	compressioncodecclass	0.555556
of two	ml linalg dense	0.200000
binary classification	binary classification	0.500000
of this instance with a	ml	0.007339
for the stream query if this	data stream writer	0.041667
returns the greatest value of the list	greatest	0.043478
full description of model	ml tree ensemble model to debug string	1.000000
checkpointinterval=10 impurity="gini", numtrees=20	ml random forest classifier	0.023256
instance contains a	ml param params has param	0.019231
thread such as the spark fair scheduler pool	core spark context	0.011628
a sparse vector using either	vectors sparse	0.166667
test that the final	mllib streaming logistic regression with sgdtests test	0.111111
file system using the new	save as new	0.125000
number of users for a given product and	users product num	0.333333
rdd's elements in one	core rdd	0.003460
the expected value of	ml linear	0.066667
a local representation this discards info about the	local	0.038462
returns an mlwriter instance for this ml instance	ml java mlwritable write	0.200000
lower bound on the log likelihood	log likelihood	0.125000
convert a value to an int if possible	param type converters to int value	0.250000
root mean squared error which	linear regression summary	0.013889
python code	code	0.071429
save a	mllib linear regression model save	0.500000
generated by applying mappartitionswithindex()	map partitions with index f preservespartitioning	0.055556
to programming	session	0.050000
comprised	random rdds log	0.125000
dump already partitioned data	group	0.025641
with a function	function	0.027778
a map of words to their vector representations	model get vectors	0.142857
new profiler using class profiler_cls	core profiler collector new profiler ctx	0.333333
number in	sql conv	0.250000
an input stream that pulls events from	stream ssc	0.090909
test that the model predicts correctly on	kmeans test test predict on model	0.500000
chi squared selector model	chi sq selector model	0.500000
to wait for new	streaming query	0.010526
:func awaitanytermination() can be used	manager reset	0.011905
:py attr	lda	0.600000
0 1 0] for feature selection by	mllib chi sq selector set	0.150000
creates or replaces a local temporary view	create or replace temp view name	0.500000
perform a left	rdd left	0.333333
tcp server to receive accumulator updates in	update server	0.333333
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance",	regressor	0.043478
from an rdd ordered in ascending	core rdd take ordered	0.050000
generates an rdd comprised	mllib random rdds exponential vector rdd sc mean	0.200000
python object into java	mllib py2java sc obj	0.333333
applies standardization transformation on a vector	transform vector	1.000000
checkpoint data or create	streaming context get or create	0.200000
test that the	test training and prediction	0.500000
specific	offset	0.021739
the weightage	life halflife	0.500000
'old' hadoop	context hadoop	0.090909
"predictions" which gives the probability of	ml logistic regression summary probability col	0.166667
values for each numeric columns for	grouped data	0.035714
a java	new java	0.166667
create a new accumulator with	accumulator init aid	0.083333
a param with a	has	0.011628
create a multi-dimensional cube for the	sql data frame cube	0.055556
basic operation test for dstream groupbykey	basic operation tests test group by	1.000000
sets vector size	word2vec set vector size	1.000000
infer schema from	session infer schema	1.000000
levenshtein distance of the two	sql levenshtein left right	0.058824
a new java	new java	0.166667
is checkpointed and materialized either reliably or	is checkpointed	0.142857
load a model from the	matrix factorization model load cls sc	0.333333
returns the soundex encoding for a string	sql soundex col	0.055556
tests whether	has param paramname	0.142857
for the termination of this query	termination timeout	0.041667
a new spark configuration	core spark conf init loaddefaults _jvm	0.250000
wait for new	query manager	0.011905
which is a risk function corresponding to the	mllib regression	0.045455
names of tables in	names	0.050000
recommends	recommend	0.153846
memory	external	0.013889
return	scaler	0.052632
create a python topicandpartition to map to	partition init topic partition	0.055556
awaitanytermination() can be used	query manager	0.011905
of features corresponding to that user	model user features	0.333333
returns accuracy equals to the total number	mllib multiclass metrics accuracy	0.166667
mixin for param featurescol features column name	has features col	1.000000
performs the kolmogorov-smirnov ks test for data	stat statistics kolmogorov smirnov test data distname	0.111111
again	query manager reset	0.011905
save	mllib linear regression model save	0.500000
and generates a :py attr countvectorizermodel	count vectorizer	0.166667
chi squared selector	chi sq selector	0.041667
creates a :class dataframe from	create	0.017241
much of	external merger object	0.032258
python list to java type array	to jarray gateway jtype arr	0.500000
be used again to wait for	reset	0.011236
string column from one base to another	conv col frombase tobase	0.166667
a converter to	converter	0.052632
of the month of a given date	dayofmonth	0.027027
this model instance	kmeans model	0.090909
the length of a	sql length col	0.050000
again to	manager reset	0.011905
probability	probability col	0.500000
explained variance regression score	ml linear regression summary explained variance	0.333333
values of each key	by key	0.026316
test this should	test	0.015152
with a	param params has	0.019231
boundaries in increasing order for which predictions	isotonic regression model boundaries	0.333333
pairs	pairs	0.714286
instance to a java onevsrest used	one vs rest to java	0.166667
newline-delimited json <http //jsonlines	writer json path mode compression dateformat	0.166667
the embedded params to the companion java object	java params transfer params to java	0.500000
mixin for param outputcol output column name	has output col	1.000000
with the frame boundaries defined from	range between	0.166667
represents a range	offset range	0.047619
fitted model	generalized linear regression	0.090909
the index of	index	0.041667
two-sided p-value	p values	1.000000
for this	external	0.013889
returns the :class statcounter members as a	core stat counter as	0.333333
seed=none impurity="gini", numtrees=20	ml random forest classifier	0.023256
collections and generates a :py attr countvectorizermodel	count vectorizer	0.166667
operation	rdd	0.003058
applying c{f}	f	0.010526
jobs started by this thread	job	0.023810
a large dataset and an item approximately	ml lshmodel approx nearest neighbors dataset key numnearestneighbors	0.166667
indexing categorical feature columns	indexer	0.055556
'old' hadoop	core spark context hadoop	0.333333
comprised of vectors containing i	random rdds normal	0.125000
the label of one or more examples	mllib decision tree model	0.076923
in :py attr predictions which gives the	generalized linear regression summary	0.090909
of key-value pairs	pairs	0.142857
hadoop configuration which	hadoop	0.050000
words to their vector representations	get vectors	0.142857
value1	value1	1.000000
the values for each key using an associative	by key	0.026316
return a resulting rdd that contains a	core rdd cogroup other numpartitions	0.066667
randn	randn	1.000000
binary operator this object is on right side	sql reverse op name doc	1.000000
of active queries associated with this	sql streaming query manager active	0.066667
params mixed with hasmaxiter hasinputcol and hasseed	test params	0.500000
calculates a lower bound on the log	ldamodel log	0.125000
ndcg value of all the queries	ndcg	0.100000
java parammap into a python	from java javaparammap	0.500000
local property set in this thread or	get local property key	0.066667
note : experimental	binary logistic regression training summary	1.000000
and add the new item	item	0.062500
return a l{statcounter} object that captures the	stats	0.055556
sets the accumulator's value only usable in driver	accumulator value value	0.050000
list	of list	0.333333
of the dstreams	context transform dstreams transformfunc	0.125000
global	global	0.777778
to	query manager	0.011905
this instance contains a param with	param params has param	0.019231
can be used again to	query	0.010753
of a batch of	batch	0.068966
again to	sql streaming query	0.011765
instance to a java pipelinemodel used for	ml pipeline model to java	0.100000
content of the non-streaming :class dataframe out into	sql data frame write	0.071429
mean squared error which is	mllib regression	0.022727
recommends the top "num"	model recommend	0.250000
to configure the kmeans algorithm for	kmeans	0.025641
convert this vector	mllib linalg vector	0.333333
comprised of vectors containing i i	random rdds normal vector	0.125000
this obj assume that all the	core external merger object size obj	0.040000
string name	params has	0.019231
to use for loading	java mlreader	0.250000
year of a	year	0.040000
for the stream query if this	stream writer	0.041667
defines	window spec range	1.000000
set a local property that affects	set local property key value	0.200000
of top	top	0.142857
to an indexedrowmatrix	to indexed row	1.000000
average values for each numeric columns for	sql grouped	0.043478
to track supported random forest parameters	random forest params	0.250000
a decorator that makes a class inherit documentation	mllib inherit	0.045455
creates	sparkcontext sparksession jsqlcontext	0.500000
:class column for distinct count of col or	count distinct col	0.040000
get the rdd's current storage level	core rdd get storage level	1.000000
results immediately to the	locally func	0.142857
as a :class dataframe	sql data	0.024390
create an input stream that pulls events	utils create stream ssc hostname port storagelevel	0.200000
converts matrix columns in an input	mllib mlutils convert matrix columns	0.166667
the ids	stage ids	0.055556
:func awaitanytermination() can be	sql streaming query manager reset	0.011905
a new profiler using class	core profiler collector new profiler	0.333333
into type datatype	cast datatype	1.000000
to use	core spark context	0.011628
a parquet file stream returning	stream reader parquet path	0.083333
or compute the number	mllib linalg coordinate matrix num	0.166667
with a given	has	0.011628
function	function name	0.166667
the position of the first occurrence of substr	substr	0.071429
of predicted clusters	ml clustering summary prediction	0.333333
for each key	key	0.035714
instance for params shared by	params	0.006623
fpr	fpr	1.000000
collect the distributed matrix on the	matrix	0.015152
from start to end exclusive increased by step	range start end step	1.000000
id of the stage that	stage id	0.500000
mixin for param outputcol output	has output	1.000000
checkpoint the dstream operations for	checkpoint	0.062500
streaming dataframe/dataset is written to	stream writer output mode outputmode	0.083333
a param with a given string	ml param params has param	0.019231
densematrix >>> dm = densematrix(2 2 range 4	linalg dense matrix	0.083333
of words closest	ml word2vec	0.142857
line in libsvm format into label indices	parse libsvm line line	0.111111
thread until the group	group	0.025641
already partitioned data into disks	core external group	0.045455
a right outer join of c{self}	full outer join other numpartitions	0.111111
saves the contents of the :class dataframe	frame writer save path format mode partitionby	0.500000
all the jobs started by this	job	0.023810
of active queries associated with this sqlcontext >>>	streaming query manager active	0.066667
convert this vector to the new mllib-local representation	vector as	0.250000
and commutative reduce function	rdd reduce	0.071429
test the	stream tests test	0.437500
converts vector columns in an input	convert vector columns from ml	0.166667
boundaries from start inclusive to end inclusive	range between start end	0.250000
used again	manager	0.011236
predicting	streaming	0.005025
the :class dataframe in json format (json lines	sql data frame writer	0.011628
from this block matrix	linalg block matrix	0.052632
threshold	threshold value	0.166667
this accumulator's value	add	0.035714
:py attr predictions which gives	generalized linear regression summary	0.090909
reference	java_model	0.090909
new	ml java wrapper new	0.250000
system using the old hadoop	as hadoop	0.142857
from start to end exclusive	context range start end	0.333333
instance contains a param with a given	param params has	0.019231
using the new hadoop outputformat api mapreduce package	as new apihadoop dataset conf keyconverter valueconverter	0.142857
of vectors containing i i d samples drawn	shape scale numrows	0.125000
fits a java model to the	java estimator fit java	0.333333
the objects	core external merger object	0.032258
gets a param	param params get param	1.000000
densematrix	linalg	0.022222
samples drawn	numrows numcols	0.125000
number	count	0.016949
sparkcontext which is associated with this streamingcontext	streaming context spark context	0.500000
an rdd containing all pairs of elements with	rdd	0.003058
get or create global taskcontext	core task context get or create cls	0.250000
the offsetrange of specific	offset ranges	0.166667
max abs vector	max abs	0.500000
comprised	random rdds normal	0.125000
does this configuration	spark conf	0.058824
column	mllib standard scaler model	0.100000
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params for this	indexer set params maxcategories inputcol outputcol	0.333333
view	view	0.833333
mlreader for	mlreader	0.037037
return a javardd of object by unpickling	core	0.003021
a python rdd of key-value	rdd save as	0.038462
approximately find at	ml lshmodel approx	0.125000
range of offsets from	offset range	0.047619
load	load	0.666667
data or table	data frame	0.005000
of numpy arrays	ml bisecting kmeans	0.062500
the list based on	based on key	0.111111
the :class dataframe to a data source	data frame writer save	0.083333
attr predictions which gives the predicted	linear regression summary prediction col	0.142857
the contents of the :class dataframe to	frame writer save path	0.066667
can be used again to wait for new	query	0.010753
this obj assume that all the objects	external merger object size obj	0.040000
the correlation of	method	0.041667
for this	merger object size	0.032258
algorithm return the	rdd	0.003058
awaitanytermination() can be used again to wait	query manager reset	0.011905
the accumulator's value only usable	accumulator value	0.050000
of the :class dataframe to	frame	0.034483
:py attr predictions which gives the predicted	linear regression summary prediction	0.142857
this :class dataframe as pandas pandas dataframe	data frame to pandas	0.200000
returns an mlreader instance for this	mlreadable read cls	0.200000
of the dataframe in a text	sql data frame writer text	0.200000
computes an fp-growth model	mllib fpgrowth train cls data	0.100000
into label indices values	mlutils parse	0.250000
that daemon and workers terminate	core daemon tests test termination	0.166667
explained variance regression score	regression metrics explained variance	0.333333
list of functions registered in the specified database	list functions dbname	1.000000
dataframe that with new	data frame to df	0.090909
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto",	random forest classifier	0.022727
number	dense vector num	0.333333
create an input stream that	create stream ssc hostname	0.200000
min value for each original column	ml min max scaler model original min	0.250000
return	mllib standard scaler model	0.100000
list of	ml bisecting	0.066667
:func awaitanytermination() can	sql	0.002525
the residual degrees of freedom	regression summary residual degree of freedom	0.250000
name	name name	1.000000
with	ml param params has	0.019231
test that coefs are predicted accurately by	linear regression with tests test parameter accuracy	0.333333
appname	appname	1.000000
:class dataframe representing	sqlquery	0.027027
the second is an	mllib matrix	0.047619
compute the dot product of two	ml linalg dense vector dot other	0.090909
a range of offsets from a	range	0.030303
for this obj assume	core external merger object size obj	0.040000
the rdd as non-persistent and	core rdd unpersist	0.066667
function on each rdd of this dstream	dstream transform func	0.500000
the spark sink deployed on a	maxbatchsize	0.037037
all	external merger object	0.032258
be used again to	sql streaming query manager	0.011905
set bandwidth of each	density set bandwidth bandwidth	0.142857
make predictions on batches of data	mllib streaming linear algorithm predict on	0.066667
accumulator's data type returning a new	accumulator param	0.038462
converts vector columns in an input	convert vector columns from ml dataset	0.166667
of words closest in similarity	ml word2vec	0.142857
or compute the	mllib linalg distributed	0.333333
given string	ml param params has	0.019231
of vectors which	ml vector	0.200000
set a configuration property if not already set	spark conf set if missing	0.500000
"num" number of users for a given product	users product num	0.333333
test that the	logistic regression with sgdtests test	0.111111
note : experimental	binary classification evaluator	1.000000
names of tables	table names	0.066667
deserializes streams written by string getbytes	utf8deserializer	1.000000
a model with weights already set	linear regression with	0.111111
"zerovalue" which may be added to the	fold	0.076923
compute the number	row matrix num	0.100000
points saved using	points sc path minpartitions	0.250000
selector type	set selector type	0.111111
or its default value	col	0.016393
new accumulator with a	accumulator init	0.083333
resulting rdd that contains	rdd cogroup other numpartitions	0.066667
boolean	boolean value	0.500000
by applying mappartitionswithindex() to each rdds	map partitions with index f preservespartitioning	0.055556
given string name	ml	0.001835
this configuration contain a given key?	core spark conf contains key	0.333333
active	active	0.750000
param and validates the ownership	ml param params resolve param param	0.333333
submit and	core spark submit	1.000000
of binomial logistic	ml logistic	0.333333
to make predictions on	predict on	0.117647
l{sparkcontext} that this rdd was	core rdd context	0.166667
instance contains a	param params	0.014925
new profiler using class profiler_cls	collector new profiler ctx	0.333333
:class rdd, a	schema samplingratio verifyschema	0.029412
stop the execution of	stop	0.052632
timeunit to configure the kmeans algorithm for fitting	kmeans	0.025641
separator	separator	1.000000
create an	spark session create	0.117647
configure the kmeans algorithm for fitting and	streaming kmeans	0.035714
to the expected value of	ml	0.001835
or compute the number of cols	mllib linalg row matrix num cols	0.333333
disks	external group by	0.045455
contains a	param params has	0.019231
multilabel	multilabel	0.833333
curve which is a dataframe having	ml binary logistic regression summary	0.125000
setparams(self k=none inputcol=none outputcol=none) set params for	set params k inputcol outputcol	0.333333
random forest model for classification or	random forest	0.041667
this instance with a	one vs	0.125000
evaluator for regression	regression metrics	0.083333
wait for new terminations	query manager	0.011905
queries so that :func awaitanytermination() can be	manager reset	0.011905
pearson's independence test using dataset	chi square test test dataset	0.333333
value of	ml multilayer	1.000000
set master url to connect to	set master value	1.000000
used again to wait for new	manager	0.011236
a converter to drop the	converter datatype	0.071429
load	mllib mlutils load	1.000000
checkpoint data or create	get or create	0.111111
user who is running sparkcontext	core spark context spark user	0.250000
function to get or create global taskcontext	core task context get or create	0.250000
create a multi-dimensional cube	data frame cube	0.055556
scale	logistic input offset scale	1.000000
given	params has	0.019231
this instance contains a param	ml param params has	0.019231
new :class dataframe replacing	data frame replace to_replace	0.100000
a :class dataframe from an :class rdd, a	schema samplingratio verifyschema	0.029412
whether this instance is	is	0.083333
a :class windowspec	window	0.037037
get a local	core spark context get local	0.333333
computes the levenshtein distance of the two	levenshtein left right	0.058824
converter to drop	converter	0.052632
external list for	external list of	0.166667
computes hex value of	sql hex col	0.166667
calculates the approximate quantiles of numerical columns	approx quantile col probabilities relativeerror	0.166667
of the importance of each feature	ml gbtclassification model feature importances	0.250000
the year	year col	0.050000
terminated	terminated	0.500000
rdd	core rdd save	0.037975
this instance contains a	param params	0.014925
convert the vector into an numpy ndarray	vector to array	1.000000
decorator that makes a class inherit documentation	inherit	0.037037
a multi-dimensional cube for	sql data frame cube	0.055556
support	support	1.000000
number of rows of	num row	0.500000
this coordinatematrix	linalg coordinate matrix	0.250000
column of predicted clusters	ml clustering summary prediction col	0.111111
field by name in a structfield	field name	0.166667
in rdd >>> rdd	core rdd	0.003460
intercept computed for this model	mllib linear model intercept	1.000000
the elements in seen in a	windowduration slideduration	0.083333
the word2vec	mllib word2vec	0.125000
the libsvm format into an rdd of labeledpoint	lib svmfile	0.125000
sparkcontext at	spark context	0.023256
numfeatures=1 << 18 binary=false	numfeatures binary	1.000000
dump the profile stats into directory	spark context dump profiles	1.000000
the contents of the :class dataframe	frame	0.034483
of two vectors we	ml linalg	0.030303
an input stream that	stream ssc hostname port	0.200000
a list	spark	0.013158
count	count	0.203390
the names of fields in	sql	0.002525
sets	set	0.786982
dataframe outputted by the model's transform method	regression summary predictions	0.200000
stop	streaming context stop	0.125000
return sparkcontext which is associated with this streamingcontext	streaming context spark context	0.500000
matrix columns	matrix columns to ml dataset	0.142857
an exception if any	mllib linalg	0.026316
time over this dstream	streaming dstream	0.027778
is close to	parameter accuracy	0.029412
rules	rules	1.000000
:py attr forceindexlabel	force index label value	1.000000
returns an active query	sql streaming	0.010204
awaitanytermination() can be	streaming query	0.010526
operation test for dstream combinebykey	operation tests test combine by key	1.000000
stop the execution of the	stop	0.052632
rdd as non-persistent and remove all blocks for	rdd unpersist	0.066667
already partitioned	external group by spill	0.047619
:py attr lda keeplastcheckpoint is set to	ml distributed ldamodel	0.050000
given combine functions and a neutral	zerovalue	0.076923
forget about past terminated queries	sql streaming query manager reset terminated	0.200000
recommends the top	factorization model recommend	0.250000
url	url	0.461538
partial objects do not serialize correctly in python2	save partial	0.125000
table accessible via jdbc url url and connection	jdbc url table	0.090909
source vectors	data	0.011628
passed as	conf set	0.200000
all the queries	mllib ranking metrics	0.250000
of the month of	sql dayofmonth	0.031250
consist of key value pairs	key ascending numpartitions keyfunc	0.071429
creates a local temporary view with this	create temp view name	1.000000
of the singularvaluedecomposition if computeu was set	value decomposition u	0.100000
an external	external list of	0.166667
a param with a given string name	params has	0.019231
py or zip dependency	py	0.050000
set initial centers should be set	mllib streaming kmeans set initial centers centers	0.200000
decision tree <http //en wikipedia org/wiki/decision_tree_learning>_	decision tree regressor	0.058824
from start to	context range start	0.500000
the minutes of a given	minute	0.040000
of length len	len	0.071429
onevsrestmodel create	one vs rest	0.034483
left outer join of c{self} and	core rdd left outer join	0.200000
table and update	tablename	0.043478
evaluator for ranking algorithms	ranking metrics	0.333333
of this instance with	ml one	0.166667
for each numeric columns for each	sql grouped data	0.083333
of the mean	mean	0.034483
a python rdd of key-value pairs (of form	rdd save	0.038462
object	obj	0.071429
recommends the	recommend	0.153846
external database table	table mode	0.200000
all the objects	core external merger	0.032258
multi-dimensional rollup	frame rollup	0.055556
setparams(self featurescol="features", labelcol="label",	ml logistic regression set params featurescol labelcol	1.000000
data type	type	0.341463
configuration contain a given key?	core spark conf contains key	0.333333
in the ensemble	mllib tree ensemble model	0.058824
checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1	random forest classifier	0.022727
database table via jdbc	jdbc url table mode properties	0.200000
converts matrix columns	mllib mlutils convert matrix columns from ml dataset	0.166667
of this	ml param params	0.013699
key-value pair rdd through a flatmap function	rdd flat map values f	0.333333
data into	group	0.025641
params instances for the given param and	params m1 m2	0.047619
with start offset	from offset	0.125000
sum for each numeric columns	grouped data sum	0.083333
columns in an	columns	0.078431
the day of the month of a	dayofmonth	0.027027
to this params	ml param params	0.013699
the termination of this query	termination timeout	0.041667
from this block	mllib linalg block	0.111111
returns	sql spark session sql	0.250000
is to be used with the spark sink	ssc addresses storagelevel maxbatchsize	0.045455
this model instance	ml generalized linear regression model	0.166667
awaitanytermination()	streaming query	0.010526
registered with the dispatch to handle all function	cloud pickler save function obj	0.142857
system using the old hadoop outputformat api	hadoop dataset conf keyconverter valueconverter	0.083333
ignore separators inside brackets	ignore brackets split	0.250000
which is defined as the	linear regression summary	0.013889
represents	offset	0.021739
create a new rdd of int containing elements	core	0.003021
parses the expression string into	sql expr str	0.125000
next memory limit if the	external sorter next limit	0.200000
this dstream and other	other numpartitions	0.083333
bandwidth of each	bandwidth bandwidth	0.125000
the square root of	root	0.035714
input	reader	0.080000
parameters in this grid to fixed	grid builder base on	0.076923
expansion	expansion	1.000000
from the param	param	0.006250
sort the list based on first	streaming test case sort result based on	0.333333
python direct kafka stream api with start	kafka direct stream	0.055556
basic operation test for dstream flatmapvalues	streaming basic operation tests test flat map values	1.000000
the correlation of two columns	method	0.041667
active queries associated with this	manager active	0.066667
creates a :class dataframe	sql spark session create	0.142857
computes the levenshtein distance of the two given	sql levenshtein left right	0.058824
instance to a java onevsrest used for ml	one vs rest to java	0.166667
a python topicandpartition to	init topic partition	0.055556
load a java model from	mllib java loader load java cls sc	0.200000
given string	params has	0.019231
much of memory for this	core external merger	0.032258
the given database	dbname	0.045455
calculates a lower bound on the log likelihood	log likelihood dataset	0.142857
of c{self} and c{other}	core	0.006042
the model	java model	0.333333
represents a specific topic and partition for	topic and partition	0.111111
value for each original column during	original	0.047619
that :func awaitanytermination()	streaming	0.005025
records as a list of :class row	data frame collect	0.500000
comprised of vectors containing	random rdds normal	0.125000
rows in	count	0.016949
initial value of weights	logistic regression with sgd set initial weights	0.333333
infer schema from	sql spark session infer schema	1.000000
specified by the optional key function	key	0.017857
get or compute the number of rows	mllib linalg indexed row matrix num rows	0.200000
has reported an error	error receivererror	0.500000
performs the kolmogorov-smirnov ks	mllib stat statistics kolmogorov smirnov	0.333333
a model with weights	streaming linear regression with	0.111111
a profile object is returned	basic profiler profile	0.200000
with extra values	map extra	0.040000
extracts the embedded default param values and	ml param params extract param	0.333333
transforms a python	ml java params transfer param map	0.250000
inputcols	input cols	0.500000
generates an rdd comprised of i i d	random rdds uniform rdd sc size numpartitions seed	0.200000
l{sparkconf} object setting spark properties	conf	0.050000
values for each key	key func numpartitions partitionfunc	0.066667
dataframestatfunctions for statistic functions	stat	0.076923
format (json lines text format or newline-delimited json	json path	0.100000
as a temporary table in the catalog	as table df tablename	0.250000
new dstream by applying reducebykey to each	streaming dstream reduce by key	0.076923
indicates whether this instance is	ldamodel is	0.200000
with a randomly generated	cross validator model	0.050000
set the initial value of	streaming logistic regression with sgd set initial	0.111111
decode the	decoder s	0.500000
data into	core external group by spill	0.047619
an rdd with the keys	rdd keys	0.250000
__init__(self estimator=none estimatorparammaps=none evaluator=none	cross validator init estimator estimatorparammaps evaluator	1.000000
the column	mllib standard scaler	0.100000
comprised of vectors containing i	random rdds log	0.125000
for the stream query if this is	stream writer	0.041667
write()	pipeline	0.105263
used again to wait for new terminations	query manager	0.011905
hash	hash	1.000000
checkpointed	checkpointed	0.500000
col	col	0.081967
the norm of	norm p	0.055556
the levenshtein distance	sql levenshtein	0.058824
defines an event time	eventtime delaythreshold	0.333333
the values for each key	by key func numpartitions	0.062500
prints the first n rows to the console	show n truncate vertical	0.333333
version of a heappush followed by a heappop	heap	0.047619
in rdd 'x' to all mixture components	mllib gaussian mixture model predict soft x	0.142857
contains no	is empty	1.000000
sqltransformer	sqltransformer	0.750000
returns a	sql spark session sql	0.250000
a python topicandpartition to map to the java	init topic partition	0.055556
the python direct kafka stream transform get offsetranges	kafka direct stream transform get offset ranges	0.500000
rdd, a list or a :class pandas	schema samplingratio verifyschema	0.029412
different value or cleared	description interruptoncancel	0.166667
rdd partitioned using	rdd	0.003058
makes a class inherit documentation from its	mllib inherit doc cls	0.045455
each original	model original	0.062500
into	spill	0.038462
seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	ml random forest classifier	0.023256
converts matrix columns in an	mllib mlutils convert matrix columns to ml dataset	0.166667
config	config	1.000000
levenshtein distance of the two given strings	levenshtein left right	0.058824
squared distance from a sparsevector or	vector squared distance other	0.166667
an rdd comprised of vectors	random rdds log normal vector rdd	0.166667
matrix columns in an input dataframe	matrix columns from ml	0.142857
topic	topic	0.857143
a file	file path	0.035714
jobs started	job	0.023810
:func awaitanytermination()	streaming	0.005025
comprised of vectors	mllib random rdds poisson vector	0.125000
a param with a given	ml param	0.009524
column scipy matrix from a	sci py tests scipy matrix	0.090909
python wrapper of	ml	0.009174
java object	params from java	0.333333
an rdd comprised of vectors containing i	mllib random rdds normal vector rdd	0.166667
the bisecting k-means	mllib bisecting kmeans train	0.500000
this block	linalg block	0.076923
a data source	sql data	0.024390
defined on	param	0.006250
until any of the queries on the associated	manager await any termination	0.142857
register a java udf so it can be	register java	0.166667
of the file to	file	0.028571
embedded params to the companion java object	java params transfer params to java	0.500000
returns weighted false positive	multiclass metrics weighted false positive	1.000000
converts matrix columns in an	mllib mlutils convert matrix columns to	0.166667
awaitanytermination() can be used again to wait for	query manager	0.011905
2 ml params instances for the given param	params m1	0.047619
keep	keep	1.000000
mean variance and	rdd	0.003058
dump already partitioned data	group by spill	0.047619
or transform	tf transform	0.045455
this instance contains	ml param params has param	0.019231
items for columns possibly with	items cols	0.125000
with the dispatch to handle all function types	cloud pickler save function obj	0.142857
finding frequent items for columns possibly	frame freq items cols	0.166667
sorts this rdd which is assumed to	core rdd sort by	0.200000
py or zip dependency for all	py	0.050000
column scipy matrix from a	mllib sci py tests scipy matrix size	0.090909
value of	ml gaussian mixture	1.000000
norm of a sparsevector	sparse vector norm	0.066667
the dot product of two	linalg dense vector dot other	0.058824
greatest value of the list of column names	greatest	0.043478
+= operator	iadd	0.142857
columns in an input dataframe from the :py	columns	0.039216
rdd as non-persistent and	core rdd unpersist	0.066667
column for distinct count of col or	count distinct	0.040000
rdd get offsetranges	rdd get offset ranges	1.000000
partition of	partition	0.066667
to wait for	sql streaming query manager	0.011905
python rdd of key-value pairs (of form	rdd save as	0.038462
hadoop-supported file	file path	0.035714
ratio	ratio	1.000000
norm of a	mllib linalg sparse vector norm	0.083333
local property that affects jobs submitted from this	local property key	0.035714
number of	ml linalg dense vector num	0.250000
all globals names read or written	extract code globals	0.125000
sql context to use for loading	ml mlreader context sqlcontext	0.333333
large dataset and an item	nearest neighbors dataset key numnearestneighbors distcol	0.333333
awaitanytermination() can be used again to	sql streaming	0.010204
ordered list of labels corresponding	ml string indexer model labels	0.066667
sets	generalized linear regression set	1.000000
predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256	predictioncol maxdepth	0.250000
keys in c{self} and c{other}	join	0.034483
parquet file stream returning	stream reader parquet path	0.083333
a matrix from the new mllib-local representation	mllib linalg matrices from	0.333333
all the objects	merger object	0.032258
any hadoop file system using the l{org	as sequence file path	0.500000
feature selection by fdr	chi sq selector set fdr fdr	0.200000
computes the levenshtein distance	levenshtein	0.045455
class inherit documentation from its parents	inherit doc	0.045455
index	map partitions with index	0.100000
of words	ml word2vec	0.142857
to wait for	streaming	0.005025
elements in iterator	iterator key reverse	0.200000
:func awaitanytermination() can be used again to	sql streaming	0.010204
comprised	mllib random rdds poisson vector	0.125000
accumulator's value	core accumulator value value	0.050000
the input schema	reader schema schema	0.333333
set number of	set	0.005917
how	size	0.009174
grid to fixed	grid param	1.000000
product and returns a list of	product	0.029412
configure the kmeans algorithm for fitting and predicting	streaming kmeans	0.035714
of the current [[dataframe]] and perform the	grouped data pivot pivot_col values	0.050000
sample points from	sample sample	0.333333
area under the receiver operating characteristic roc curve	binary classification metrics area under roc	1.000000
specifies the underlying output	writer	0.040000
conditions and returns	sql column otherwise value	0.050000
average values for each numeric columns	sql grouped data	0.041667
this distributed model to a local	distributed ldamodel to local	0.111111
note : experimental	chi sq selector model	0.500000
awaitanytermination() can be used again to wait	manager reset	0.011905
deviance for the null model	generalized linear regression summary null deviance	0.250000
standard deviation of this	rdd stdev	0.066667
the initial value of	mllib streaming logistic regression with sgd set initial	0.111111
of partitions to use during reduce tasks (e	core rdd default reduce partitions	0.166667
a resulting rdd that contains a	rdd cogroup other	0.066667
inputcol=none outputcol=none seed=none numhashtables=1)	inputcol outputcol seed numhashtables	1.000000
the norm of	ml linalg sparse vector norm p	0.333333
start position int or column	startpos	0.166667
are the left singular vectors of the singularvaluedecomposition	linalg singular value decomposition	0.250000
each original	scaler model original	0.062500
the :class dataframe	data frame writer	0.070423
returns micro-averaged label-based precision	multilabel metrics micro precision	1.000000
how much of memory for	merger object size	0.032258
much of memory for this obj assume	external merger object size obj	0.040000
or two	mllib linalg	0.026316
of this instance	ml param params reset	0.166667
test the python direct kafka stream api with	streaming kafka stream tests test kafka direct stream	0.125000
the stream query if this is	sql data stream writer	0.041667
a column of the current [[dataframe]] and	pivot pivot_col values	0.050000
model for classification or	model	0.005587
create a new hivecontext for	create for	0.250000
python topicandpartition	topic partition	0.055556
by this thread until the group id	group groupid	0.142857
range of offsets from a	offset range	0.047619
subclass of params	params	0.013245
list of functions registered in the specified	catalog list functions	0.250000
sets	tree classifier params set	1.000000
area under the receiver operating characteristic roc curve	mllib binary classification metrics area under roc	1.000000
the :class dataframe to a data	data frame writer save path format	0.142857
training	generalized linear regression training summary	0.142857
perform a right	rdd full	0.333333
initial	sgd set initial	0.111111
a dictionary a list of	size	0.036697
ownership	params resolve	0.333333
keys in c{self} and c{other}	join other numpartitions	0.071429
rdd	rdd map partitions	1.000000
the week number of a given	weekofyear col	0.055556
sep	sep	1.000000
default implementation of	ml	0.001835
an javardd of object by unpickling it	ml	0.001835
of the :class dataframe	data frame writer save	0.083333
minimum number of times a token must	min count	0.076923
this model	model predict	0.333333
word	mllib word2vec	0.125000
file system using the old hadoop	as hadoop	0.142857
this instance with a randomly generated uid	cross validator	0.045455
total log-likelihood for	gaussian mixture summary log likelihood	0.142857
name of the test method	stat chi sq test result method	0.250000
a param with	params	0.006623
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance",	tree regressor	0.058824
awaitanytermination() can be used again to wait	manager	0.011236
returns accuracy equals to the total number	multiclass metrics accuracy	0.166667
changes the uid of	ml param params reset uid	0.058824
__init__(self mindocfreq=0 inputcol=none outputcol=none)	idf init mindocfreq inputcol outputcol	1.000000
the root directory that contains	get root directory cls	0.333333
already	core	0.003021
unhex	unhex	1.000000
to all mixture	gaussian mixture model predict	0.100000
optional default value and user-supplied value in	ml param params	0.013699
rdd contains no elements	core rdd is empty	0.083333
curve which is a	binary logistic regression summary	0.111111
data into	group by spill	0.047619
model	factorization model	0.043478
called when processing of a job of	listener on output operation	0.166667
every feature	mllib linear	0.166667
features	features	0.347826
the java_model to a python model	ml quantile discretizer create model java_model	0.250000
convert a value to a boolean if possible	ml param type converters to boolean	0.250000
be used again to wait for new terminations	manager reset	0.011905
of	ml java wrapper	1.000000
of names of tables in the database dbname	names dbname	0.500000
representation	mllib linalg dense vector	1.000000
values for each key	by key func	0.062500
k=none inputcol=none outputcol=none)	k inputcol outputcol	1.000000
numtrees=20 featuresubsetstrategy="auto", seed=none	random forest	0.041667
+= operator adds a term to this accumulator's	iadd term	0.142857
an fp-growth model that contains frequent	mllib fpgrowth train cls data minsupport numpartitions	0.100000
based on first value	based on key	0.111111
the index of the	with index	0.100000
using the new	as new	0.125000
instance is of type	ml ldamodel is	0.066667
finding frequent items for columns	data frame freq items cols support	0.166667
representing the result of the given	sqlquery	0.027027
sqltype() into class	cls	0.047619
active queries associated with this sqlcontext >>> sq	sql streaming query manager active	0.066667
loads a class generated by namedtuple	namedtuple name fields	0.333333
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10	lda init featurescol maxiter seed	0.250000
and vector	model get vectors	0.142857
setparams(self featurescol="features", labelcol="label",	linear regression set params featurescol labelcol	1.000000
mixin for param rawpredictioncol raw	has raw	1.000000
an :class rdd, a list or a	schema samplingratio verifyschema	0.029412
comprised	random rdds log normal vector	0.125000
iterator of deserialized batches lists of	stream without unbatching	0.125000
train a decision tree model	mllib decision tree train regressor cls	0.333333
a temporary table in	table df tablename	0.083333
create	spark session create	0.117647
register a java udf so	register java	0.166667
instance contains a param	has param	0.019231
the uid of this instance this	ml param params reset uid	0.058824
the underlying output	sql data stream writer format	0.333333
0] for feature selection by fwe	chi sq selector set fwe fwe	0.200000
python direct kafka rdd api	kafka rdd	0.285714
numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1 0)	ml random forest	0.071429
model improves on toy data with no of	mllib streaming logistic regression with sgdtests	0.200000
in this	param grid builder add	0.200000
the += operator adds a term to this	core accumulator iadd term	0.142857
all trees in the ensemble	ensemble	0.100000
a term to this accumulator's value	term	0.040000
as the spark fair scheduler pool	core spark	0.010309
training	training	0.176471
this rdd's elements	core rdd	0.006920
memory for this obj assume	external merger object size obj	0.040000
if possible	ml param type converters	1.000000
month of a given date as	sql dayofmonth col	0.031250
word2vec	word2vec	0.368421
curve which is a	ml binary logistic regression summary	0.125000
context to use for saving	ml java mlwriter context	1.000000
rdd of labeledpoint	load lib svmfile sc path numfeatures minpartitions	0.125000
parameters in this grid to fixed	grid builder add grid param	0.250000
to wait for new terminations	sql	0.002525
by applying 'full	full	0.066667
all	merger	0.051282
function on each rdd of this dstream	kafka dstream transform func	0.500000
of one or more examples	decision tree model	0.050000
the levenshtein distance	levenshtein left	0.058824
java ml instance the default	java mlreader java	0.333333
in this model	kmeans model	0.181818
a list	core spark context	0.011628
fits a model	estimator fit	0.166667
the accumulator's	accumulator value	0.050000
contains a param with a given string name	param	0.012500
which the centroids of	timeunit	0.025641
in this grid to fixed	param grid builder add grid param	0.250000
the :class dataframe using the	sql data frame	0.005348
:func awaitanytermination() can be used again to	query	0.010753
term frequency vectors or transform	tf transform	0.045455
until any of	manager await any termination	0.142857
given a java onevsrest create and return a	one vs rest from java cls	0.200000
setparams(self formula=none featurescol="features",	featurescol	0.031250
this model	model	0.061453
has half	half	0.058824
the levenshtein distance of the two	levenshtein left right	0.058824
decisiontreeregressor	decision tree	0.076923
forest model for classification	forest	0.083333
the current [[dataframe]] and	pivot pivot_col values	0.050000
sets vector size default 100	word2vec set vector size vectorsize	1.000000
in this model	bisecting kmeans model	0.250000
use only create	sql hive context create	0.083333
set the initial	set initial	0.111111
rank of	ml alsmodel rank	1.000000
"zerovalue" which may	fold by	0.125000
summary of	summary	0.024390
test that the	regression with sgdtests test	0.111111
predicted clusters in	clustering summary prediction	0.333333
regression model with l2-regularization	mllib ridge regression with sgd	1.000000
versionadded : 0 9 0	naive bayes	0.142857
the model trained	tree ensemble model	0.038462
convert a value to list	to list	0.250000
comprised of vectors containing i	random rdds gamma	0.125000
returns the	mllib	0.010526
called when a receiver	listener on receiver	0.500000
to receive accumulator updates in a daemon thread	update	0.055556
contains a param with a given string	params has param	0.019231
seed=none	maxiter seed	0.500000
returns micro-averaged label-based	mllib multilabel metrics micro	1.000000
the contents of the :class dataframe to a	frame	0.034483
"zerovalue" which may be added to the	rdd fold	0.125000
probability of obtaining a test statistic	mllib stat test	0.166667
this instance with a randomly	cross validator model	0.050000
partial objects do not serialize correctly	partial obj	0.125000
train a decision tree model for	mllib decision tree train regressor cls data	0.333333
returns an mlreader instance for this	ml mlreadable read cls	0.250000
for data sampled from a continuous distribution	data distname	0.083333
new dstream by applying reducebykey to each rdd	streaming dstream reduce by key	0.076923
average precision map	average precision	0.500000
quantiles of numerical columns	quantile col probabilities relativeerror	0.166667
pipelinemodel create and return a python wrapper	pipeline model from	0.142857
whose columns are the left singular	mllib linalg singular	0.017544
precision-recall curve which is a dataframe containing	ml binary logistic regression summary pr	0.083333
terms to term frequency vectors or transform	hashing tf transform	0.045455
calculates the norm of a sparsevector	norm p	0.055556
selector type of the	mllib chi sq selector set selector type	0.111111
importance of each feature	ml gbtclassification model feature importances	0.250000
access fields	struct type getitem key	0.200000
left singular vectors of	mllib linalg singular	0.017544
calculates the norm of	sparse vector norm p	0.066667
items for columns possibly with false positives using	items	0.066667
timeunit to configure the kmeans algorithm	kmeans	0.025641
starts at pos in byte and is	pos	0.022222
two separate arrays of indices	ml	0.001835
content of the :class dataframe in	data frame	0.010000
this dstream	streaming kafka dstream	0.250000
behavior when data or table already exists	sql data frame writer mode savemode	0.071429
into a jvm map	scala map sc jm	0.200000
until any of the	await any termination	0.142857
track supported random forest parameters	random forest params	0.250000
table accessible via jdbc url url	reader jdbc url table column	0.166667
obj assume that all the	core external merger object size obj	0.040000
the greatest value of the list of	greatest	0.043478
term to this accumulator's	add term	0.066667
number	linalg block matrix num	0.100000
:func awaitanytermination()	query manager reset	0.011905
partitions default 1 use a small	partitions numpartitions	0.500000
setparams(self	fpgrowth set params	1.000000
ignore	ignore	0.700000
model that has a vector of coefficients	model	0.005587
print the first num elements	pprint num	0.250000
this instance	one vs rest	0.034483
used again to	query manager reset	0.011905
of the month	sql dayofmonth col	0.031250
stratified sample without replacement based on the	sample	0.050000
return the	standard scaler model	0.090909
the :class dataframe as the specified	sql data frame writer save as	0.071429
combine functions and a	core	0.003021
only create a new hivecontext for testing	hive context create for testing cls	0.333333
scaling	scaling	1.000000
sql statements	f returntype	0.125000
note : experimental	logistic regression training summary	0.500000
rdd of key-value	rdd save as	0.038462
find the minimum item in this rdd	core rdd min key	0.333333
return an rdd created by	rdd	0.003058
of this instance	ml param params	0.013699
the full class name	class cls	0.333333
sets window	word2vec set window	1.000000
vector columns in an input dataframe to the	vector columns from ml	0.142857
stream query if this	stream writer	0.041667
the dstreams	context transform dstreams transformfunc	0.125000
model produced by	clustering model	1.000000
that :func awaitanytermination() can	manager	0.011236
the += operator adds a term to this	iadd term	0.142857
save this rdd as a	core rdd save as	0.012500
so that :func awaitanytermination() can	sql streaming query manager reset	0.011905
a copy	copy extra	0.333333
parses a column containing a json string into	from json col	0.083333
loads	reader	0.120000
key-value pair rdd through a flatmap function without	rdd flat map values f	0.333333
current [[dataframe]] and	grouped data pivot pivot_col values	0.050000
end exclusive increased	end	0.066667
this vector to the new mllib-local representation	sparse vector as ml	0.333333
transforms a python parammap into	ml java params transfer param map to	0.500000
shut down the	stop	0.052632
another timestamp that corresponds	timestamp timestamp tz	0.333333
rawpredictioncol raw	raw	0.166667
broadcast a read-only variable to the cluster	broadcast	0.052632
frame boundaries from start inclusive to end inclusive	between start end	0.125000
instance contains a param with a given string	param params has	0.019231
collect each rdds into the returned list	spark streaming test case collect dstream n	1.000000
set a configuration property	core spark conf set key	1.000000
computes column-wise summary statistics for the input rdd[vector]	stat statistics col stats	0.200000
compute the dot product	linalg dense vector dot other	0.058824
converts vector columns in an input	convert vector columns to	0.166667
in the key-value	map	0.058824
a large dataset and an item approximately find	ml lshmodel approx nearest neighbors dataset key	0.166667
vector columns in an input dataframe to the	vector columns	0.071429
evaluates	ml java evaluator evaluate dataset	0.333333
dump already partitioned	spill	0.038462
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20	random forest classifier	0.022727
+= operator adds a term to this accumulator's	core accumulator iadd term	0.142857
train a decision tree model	mllib decision tree train regressor	0.333333
:class dataframe in json format (json	sql data frame	0.005348
curve which is a dataframe having two	binary logistic regression summary	0.111111
the square root of the mean	root mean	0.250000
method for comparing instances	ml ldatest compare m1 m2	0.250000
a line in libsvm format into label	mlutils parse libsvm line line	0.111111
columns	linalg block matrix cols	0.333333
featurescol="features", labelcol="label",	formula featurescol labelcol	0.400000
behavior when data or table	sql data frame writer mode savemode	0.071429
add a file to be downloaded	context add file path recursive	1.000000
a resulting rdd that contains a tuple	rdd cogroup other numpartitions	0.066667
parameters in this grid to fixed values	ml param grid builder base on	0.076923
contains a param with a given string	ml param	0.009524
pipeline create and return a python wrapper	pipeline	0.052632
new rdd that has exactly numpartitions partitions	rdd repartition numpartitions	1.000000
system	partition	0.066667
function without	f	0.010526
a dictionary a list of	init size	0.066667
the termination of this	termination	0.035714
inputcol=none outputcol=none)	inputcol outputcol	0.666667
tables	tables	0.357143
java_model to a python model	quantile discretizer create model java_model	0.250000
predict values for	predict	0.068966
exposes information about	info	0.142857
in this context	streaming context	0.055556
:func awaitanytermination() can be used	streaming query	0.010526
1 leaf	mllib decision	0.125000
awaitanytermination() can be used	streaming query manager	0.011236
value of	ml param decision tree	1.000000
params to	params to	0.035714
a sliding window over	value and window windowduration slideduration numpartitions	0.076923
the residual degrees of freedom for	generalized linear regression summary residual degree of freedom	0.125000
vector or	vector	0.019231
the	ml	0.001835
columns of a dataframe as	sql data frame	0.005348
this	merger object size	0.032258
name	param params has	0.019231
selector type	sq selector set selector type	0.111111
the minutes of a	sql minute col	0.050000
block matrix	block matrix	0.500000
in which each	by	0.014286
for distinct count of col or	count distinct col	0.040000
of the :class dataframe to a	frame writer save	0.066667
the stream query if	sql data stream	0.031250
mb	mb	1.000000
__init__(self labelcol="label", featurescol="features", predictioncol="prediction",	regression init labelcol featurescol predictioncol	1.000000
data of a streaming dataframe/dataset is written to	data stream writer output mode outputmode	0.333333
values	standard scaler model	0.090909
basic operation test for dstream countbyvalue	basic operation tests test count by	1.000000
calculates the norm of a sparsevector	linalg sparse vector norm	0.066667
number of gaussians in mixture	mllib gaussian mixture	0.045455
which	ml isotonic regression model	0.125000
return the column	standard scaler	0.076923
or compute the	mllib linalg coordinate matrix	0.250000
rdd[vector] saveastextfile	mllib mlutils	0.333333
the current [[dataframe]] and perform the specified	pivot pivot_col values	0.050000
vectors containing i i d samples drawn	numrows numcols	0.125000
wait for new	query	0.010753
impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1 0)	random forest classifier	0.022727
mark the rdd as non-persistent and remove	core rdd unpersist	0.066667
:class dataframe using the specified	data frame	0.005000
in the specified database	dbname	0.090909
converts a labeledpoint to a	mlutils convert labeled point to	0.250000
>>> schema = _parse_schema_abstract("a b c d")	sql infer schema	1.000000
of the points belongs to in this model	bisecting kmeans model predict x	0.333333
the greatest value of the	sql greatest	0.055556
basic operation test for dstream groupbykey	basic operation tests test group by key	1.000000
can be used again	sql streaming query manager reset	0.011905
an rdd comprised of	mllib random rdds poisson vector rdd	0.166667
wrap this udf with a function	defined function	0.066667
a sparse vector using either a dictionary a	linalg vectors sparse size	0.166667
setparams(self inputcol=none outputcol=none) sets params for this tokenizer	ml tokenizer set params inputcol outputcol	1.000000
norm of	ml linalg sparse vector norm	0.333333
memory for this obj assume that all	external merger object size obj	0.040000
the norm of a sparsevector	mllib linalg sparse vector norm p	0.083333
submit and test	core spark submit tests test	0.363636
of params mixed with hasmaxiter hasinputcol and hasseed	test params	0.500000
right singular vectors of the singularvaluedecomposition	mllib linalg singular value decomposition v	0.250000
given string	has param	0.019231
libsvm format into an rdd of labeledpoint	load lib svmfile sc path numfeatures minpartitions	0.125000
of columns that make up each	matrix cols per	0.333333
the topics described by weighted terms	mllib ldamodel describe topics maxtermspertopic	0.333333
a local property set	get local property key	0.066667
matrix whose columns are the left singular	linalg singular	0.017544
number of features i e length	model num features	1.000000
pair rdd through a flatmap	rdd flat	1.000000
infer schema from an rdd of	session infer schema rdd samplingratio	0.250000
assumed to consist of key value	key ascending numpartitions keyfunc	0.071429
column from one base to	conv col frombase tobase	0.166667
be used	manager reset	0.011905
save a linearregressionmodel	save sc path	0.500000
parses the expression string	expr	0.076923
vector to	mllib linalg sparse vector	0.111111
a py or	py	0.050000
a term to	add term	0.066667
this docstring is not shown publicly	linalg coordinate matrix init entries numrows numcols	0.333333
solver	solver	1.000000
line	line line multiclass	0.166667
__init__(self rawpredictioncol="rawprediction", labelcol="label", metricname="areaunderroc")	binary classification evaluator init rawpredictioncol labelcol metricname	1.000000
computes column-wise summary	mllib linalg row matrix compute column summary	1.000000
to combine	combine	0.142857
sets	hashing tf set	1.000000
output a python rdd of key-value	core rdd save	0.037975
code for a	code name doc	0.111111
usercol	user col	1.000000
this matrix to	dense matrix	0.076923
the length	length	0.040000
is set to a different value or	spark context set	0.166667
:func awaitanytermination() can be used again	streaming	0.005025
the block	block	0.090909
resolves a param and validates	param param	0.100000
tree model for	tree model	0.026316
given parameters in this	builder add	0.200000
paired rdd where the first	factorization model	0.043478
queries so that :func awaitanytermination()	query manager	0.011905
fits a model to the	ml estimator fit	0.083333
the stream query if this is	sql data stream	0.031250
already partitioned	core external group	0.045455
trait for multivariate statistical summary of a	multivariate statistical summary	0.250000
queries so that :func	manager reset	0.011905
save this model to the given path	kmeans model save sc path	1.000000
create an rdd	session create	0.117647
word2vec model's vocabulary default	word2vec	0.052632
squared distance from a sparsevector or 1-dimensional	ml linalg sparse vector squared distance	0.166667
set initial centers should be set before	streaming kmeans set initial centers centers	0.200000
serializes a stream of	serializer	0.062500
passed as a list	spark conf set	0.111111
of memory for	merger object size	0.032258
value of	ml count vectorizer	1.000000
a new dstream in which each rdd	streaming streaming context transform	0.066667
should be smaller than or equal	numiterations	0.050000
for	streaming query manager reset	0.011905
calculates the correlation of two columns of a	corr col1 col2 method	0.055556
rawpredictioncol="rawprediction", labelcol="label", metricname="areaunderroc")	rawpredictioncol labelcol metricname	1.000000
the cluster centers represented as a list of	mllib kmeans model cluster centers	0.083333
for loading	ml mlreader	0.222222
dump the profile stats into directory	dump profiles	1.000000
other from this block matrix this	linalg block matrix	0.052632
test the stage ids are available	tests test stage	1.000000
build the union	union	0.090909
iterable	iterable	0.625000
is assumed to consist of key value	key ascending numpartitions keyfunc	0.071429
update	update	0.333333
can	streaming query manager	0.011236
clean up all	merger cleanup	0.333333
dot product of two	vector dot	0.050000
a range of offsets from a single kafka	offset range	0.047619
description	description	1.000000
sample without replacement based on the fraction given	data frame sample	0.066667
a new dstream by applying reducebykey to	streaming dstream reduce by key	0.076923
the rdd partitioned	rdd partition by	0.062500
inputformat with arbitrary key and value	inputformatclass	0.095238
prediction tasks regression	prediction	0.041667
load a model from	mllib kmeans model load cls sc	0.333333
set bandwidth of each sample defaults to	set bandwidth bandwidth	0.142857
the length of a string	length	0.040000
date column	date	0.037037
contains a param with a	has	0.011628
category if specified	multiclass	0.142857
bias	bias	1.000000
version of a heappush followed by a	heap item	0.125000
is	profiler	0.090909
0 1 0] for feature selection by	mllib chi sq selector	0.150000
returns a paired rdd	factorization model	0.043478
memory for	external merger object size	0.032258
possible outcomes for k classes	classes	0.034483
dstream by applying reducebykey to each	streaming dstream reduce by key	0.076923
external sort when the	core external	0.016129
this block matrix	linalg block matrix	0.052632
how data of a streaming dataframe/dataset is written	sql data stream writer output mode outputmode	0.333333
a column of the current [[dataframe]] and perform	sql grouped data pivot pivot_col values	0.050000
model fitted by bisectingkmeans	bisecting kmeans model	0.250000
decision tree	decision tree	0.538462
load labeled points saved using	mllib mlutils load labeled points sc path minpartitions	0.250000
and name or	core	0.003021
input dataset	dataset	0.081633
sets the given parameters in this grid	param grid builder add grid	0.100000
index keys are categorical feature indices column indices	indexer model category maps	1.000000
that :func awaitanytermination()	query manager reset	0.011905
param with a given string name	params	0.006623
all	merger object	0.032258
makes a class inherit documentation from its parents	inherit doc cls	0.045455
predict values for a single data point or	predict x	0.033898
the second is an array of	mllib	0.010526
train the model on the incoming	logistic regression with sgd train on	0.333333
contains	ml param params has param	0.019231
original column during	model original	0.062500
:py attr inputcols	input cols value	1.000000
field in "predictions" which gives the features of	ml linear regression summary features	0.166667
partial objects	cloud pickler save partial	0.125000
this dstream	kafka dstream	0.250000
ordered list of labels corresponding to	ml string indexer model labels	0.066667
__init__(self estimator=none estimatorparammaps=none evaluator=none	init estimator estimatorparammaps evaluator	1.000000
until any of the	sql streaming query manager await any termination	0.142857
python direct kafka stream	kafka direct stream	0.277778
g depth 0 means 1 leaf	mllib decision	0.125000
matrix to a coordinatematrix	block matrix to coordinate matrix	0.333333
returns the soundex encoding for a string	soundex col	0.055556
generates an rdd	vector rdd sc mean	1.000000
this docstring is not shown publicly	mllib linalg row matrix init rows numrows numcols	0.333333
make sure user configuration is respected spark-19307	user configuration	1.000000
ordered in ascending order	take ordered	0.125000
queries so that :func awaitanytermination() can	sql streaming query	0.011765
deviance for the fitted model	regression summary deviance	0.125000
get number of trees in	num trees	0.500000
called when processing of a batch of	streaming listener on batch	0.333333
this matrix to an indexedrowmatrix	block matrix to indexed row matrix	0.333333
vectors or transform the rdd	tf transform	0.045455
that all the objects	object	0.027778
year of a given date as	sql year	0.050000
contents of this :class dataframe as pandas pandas	to pandas	0.166667
for	core external	0.016129
load a	load cls sc	0.142857
initial value of weights	with sgd set initial weights initialweights	0.333333
terminations	query manager	0.011905
converts matrix columns in	mllib mlutils convert matrix columns from	0.166667
the uid	param params reset uid newuid	0.333333
a jvm scala map from a dict	frame jmap jm	0.111111
is an array of	mllib	0.010526
accessible via jdbc url url and	reader jdbc url	0.250000
sum for each numeric columns for	sql grouped data sum	0.083333
exception	exception	0.750000
obtaining a test	test	0.015152
fast version of a	heappushpop heap	0.142857
a new spark configuration	core spark conf init	0.250000
column mean values	mean	0.034483
create a column scipy matrix from a dictionary	sci py tests scipy matrix size	0.090909
converts vector columns in an input dataframe	convert vector columns from	0.166667
for each key	key func numpartitions	0.066667
the	sql	0.010101
the records as a list of :class row	data frame collect	0.500000
testing	testing cls	1.000000
create a python topicandpartition to map to	topic and partition init topic partition	0.055556
tests whether	paramname	0.076923
that :func awaitanytermination() can be used again to	sql	0.002525
calculates the length of	sql length	0.050000
training set given the current parameter	ldamodel training	0.034483
an rdd containing all pairs	rdd	0.003058
number	classification model num	1.000000
a new java object	java wrapper new java obj java_class	0.333333
column name	col	0.147541
correlation of two columns	col1 col2 method	0.055556
given a java pipeline create	pipeline from java cls	0.200000
for each numeric	grouped	0.071429
of memory for this obj assume that	size obj	0.040000
the key-value pair rdd through a flatmap	core rdd flat map values	0.333333
the index of the original	partitions with index	0.100000
data	sql data stream	0.031250
in this grid to	builder add grid	0.100000
product and returns	product	0.029412
an fp-growth model that contains frequent itemsets	mllib fpgrowth train cls data minsupport	0.100000
new spark configuration	spark conf init loaddefaults	0.250000
option of ensuring all received	stopsparkcontext stopgracefully	0.050000
instance contains a	has	0.011628
returns a :class	spark session sql	0.250000
the initial value of weights	logistic regression with sgd set initial weights initialweights	0.333333
the initial	set initial	0.111111
value of	ml random forest params	1.000000
returns weighted averaged f-measure	mllib multiclass metrics weighted fmeasure beta	1.000000
with	param params has param	0.019231
used again to wait for new terminations	sql	0.002525
and another frame	all other	1.000000
already	core external group by	0.045455
formula=none featurescol="features", labelcol="label", forceindexlabel=false)	formula featurescol labelcol forceindexlabel	0.400000
to wait for	streaming query manager	0.011236
new map column	map	0.058824
load labeled points	load labeled points sc	1.000000
left	rdd left	0.333333
the current [[dataframe]] and perform	sql grouped data pivot pivot_col values	0.050000
to the input dataset this is	dataset	0.020408
the test this should be list of	test	0.015152
of terms to term frequency vectors or	mllib hashing tf	0.125000
predicts rating for	matrix factorization model predict	0.250000
to conversion	conversion	0.142857
returns the receiver operating characteristic roc curve which	ml binary logistic regression summary roc	0.166667
model	model base	1.000000
partial objects do not	save partial	0.125000
generates python code for a shared param class	param gen param code name	0.333333
from checkpoint data or create	streaming streaming context get or create	0.200000
this thread such as the spark	spark context	0.023256
the :class dataframe in orc	sql data frame writer orc	0.200000
each	summary	0.024390
handler	handler	1.000000
cluster centers represented as a list of	model cluster centers	0.060606
used again to wait for new terminations	streaming query	0.010526
of the most recent [[streamingqueryprogress]] updates	recent progress	0.111111
second is an array of	mllib	0.010526
matrix columns in an input dataframe from the	matrix columns to ml dataset	0.142857
save each rdd in this dstream	streaming dstream save	1.000000
in multinomial	mllib	0.010526
of params mixed with hasmaxiter hasinputcol and hasseed	other test params	0.500000
1 0] for feature selection by percentile	chi sq selector set percentile percentile	0.200000
memory for this obj assume that all	core external merger object size obj	0.040000
sort the list based on	sort result based on key outputs	0.333333
only	only	0.833333
invalidates and refreshes all the cached data and	catalog refresh by	0.200000
numhashtables	numhashtables	1.000000
a function to each element	map f preservespartitioning	0.200000
a local property set	context get local property key	0.066667
which is a risk function	mllib regression	0.045455
containing i i d samples drawn	numrows numcols	0.125000
residual degrees of freedom for the null model	linear regression summary residual degree of freedom null	0.333333
train the model on	streaming linear regression with sgd train on	0.333333
a value to a boolean if possible	ml param type converters to boolean	0.250000
of each class	ml	0.001835
a	accumulator add	0.076923
transforms a	params transfer	0.250000
this grid to fixed	add grid param	1.000000
represents an entry of	entry	0.250000
test the python direct kafka stream foreachrdd get	stream tests test kafka direct stream foreach get	1.000000
python code for a	code	0.071429
frequency vectors or transform the rdd	tf transform	0.045455
the selector type of the	mllib chi sq selector set selector type	0.111111
contains a	param params	0.014925
lasso	lasso	1.000000
term frequency vectors or transform the rdd of	mllib hashing tf transform	0.045455
key	by key	0.026316
can be used again to wait	manager reset	0.011905
saves the contents of	format mode partitionby	0.100000
represents a row	row	0.062500
creating rdds comprised of i	random rdds	0.012821
converts matrix columns in an input dataframe from	mllib mlutils convert matrix columns to ml dataset	0.166667
accuracy equals to the total	accuracy	0.076923
residuals mse r-squared of model on	ml linear regression model	0.166667
numtrees=20 featuresubsetstrategy="auto",	random forest	0.083333
directory	path	0.010204
perform a right outer join of	rdd full outer join other numpartitions	0.111111
set bandwidth of each sample defaults to 1	stat kernel density set bandwidth bandwidth	0.142857
:py attr numfeatures	num features value	1.000000
context to use for saving	mlwriter context	1.000000
all values as a list	all	0.083333
centers	centers centers	1.000000
a boolean	boolean	0.125000
classification problem in multinomial logistic regression	mllib logistic regression	0.250000
create an rdd for dataframe from	create from	0.500000
the training set given the current parameter	distributed ldamodel training	0.034483
:py attr numpartitions	num partitions value	1.000000
data of a	sql data stream	0.031250
param with a given string name	params has	0.019231
property set in	property key	0.066667
bandwidth	bandwidth bandwidth	0.125000
columns that make up each block	block matrix cols per block	0.333333
the greatest value of the	greatest	0.043478
create an rdd that has no partitions or	core spark context empty rdd	0.200000
a right outer join of c{self} and c{other}	core rdd full outer join other	0.200000
sets mincount the minimum number of times a	set min count mincount	0.250000
the rdd's current storage level	storage level	0.500000
__init__(self min=0 0 max=1 0	min max scaler init min max	1.000000
instance contains a	ml param params has	0.019231
squared distance from a	ml linalg sparse vector squared distance other	0.166667
wait for new terminations	streaming query manager reset	0.011905
a new hivecontext for testing	for testing	0.333333
transforms a python parammap into a	java params transfer param map to	0.500000
given	params	0.006623
alternating least squares als matrix factorization	als	0.142857
get the root directory that contains	get root directory	0.333333
the index	partitions with index	0.100000
length of a string or	sql length	0.050000
test that the model	mllib streaming kmeans test test model	1.000000
this rdd and another one	core rdd add other	1.000000
casesensitive=false) sets	stop words remover set	0.200000
adds a term	add term	0.066667
the top "num" number	num	0.016807
can be used again to wait for	sql streaming query manager	0.011905
obj assume that all	external merger object size obj	0.040000
0 99], quantilescol=none aggregationdepth=2):	fitintercept	0.058824
return the column	scaler model	0.153846
stream query if this is	stream	0.017544
centers represented as	centers	0.150000
return a resulting rdd that contains a	rdd cogroup other numpartitions	0.066667
jvm scala map from a	data frame jmap jm	0.111111
in 2 1 use approx_count_distinct instead	sql approx count distinct col rsd	0.333333
soundex encoding for a	soundex col	0.055556
the accumulator's value only usable	core accumulator	0.030303
inputcol	inputcol	0.833333
make predictions on batches	mllib streaming linear algorithm predict on	0.066667
of indices to	ml chi	0.100000
the importance of each feature	ml decision tree regression model feature importances	0.250000
mixin for param inputcol input	has input	0.500000
a multi-dimensional cube for the current	frame cube	0.055556
that generates monotonically increasing 64-bit integers	monotonically increasing id	0.333333
resulting rdd that contains a tuple with the	core rdd cogroup other	0.066667
@param input dataset for the test	test case test func input func expected sort	0.333333
configuration property if not already set	conf set if missing key	1.000000
which are array-like or buffer to array	to array array_like dtype	0.166667
of partitions to use during reduce tasks (e	rdd default reduce partitions	0.166667
against the expected	expected	0.076923
fast version of a heappush followed by	core heappushpop heap	0.142857
using the model trained	logistic regression model	0.083333
a class generated by namedtuple	namedtuple name fields	0.333333
from this	mllib linalg	0.026316
that makes a class inherit documentation	mllib inherit doc cls	0.045455
get or compute the number of rows	indexed row matrix num rows	0.200000
returns the precision-recall curve which is a	ml binary logistic regression summary pr	0.083333
the name of the file to which this	file	0.028571
be used with the spark sink deployed	storagelevel maxbatchsize	0.045455
train the model on	logistic regression with sgd train on	0.333333
ldamodel	ldamodel	0.206897
soundex encoding for a	sql soundex	0.055556
return the	standard	0.071429
for indexing categorical feature columns in	indexer	0.055556
to wait for new	streaming query manager reset	0.011905
variance and count of the rdd's elements	rdd	0.003058
instance to a java pipelinemodel used for	pipeline model to java	0.100000
residual degrees	summary residual degree	0.500000
partitioned data into	external	0.013889
converts vector columns in an input dataframe from	mllib mlutils convert vector columns to ml dataset	0.166667
sets the given parameters in this grid	grid builder add grid	0.100000
for distinct count of col	count distinct col	0.040000
find norm of the given vector	linalg vectors norm vector p	1.000000
instance for params	params	0.006623
streaming dataframe/dataset is written to a streaming	writer output mode outputmode	0.083333
model with weights	streaming linear regression with	0.111111
from checkpoint data or create	context get or create	0.200000
decision tree <http //en wikipedia org/wiki/decision_tree_learning>_	decision tree classifier	0.500000
efficiency can also update c{value1} in place	in place value1 value2	0.500000
in "predictions" which gives the features of	ml linear regression summary features col	0.166667
deviance for the null model	regression summary null deviance	0.250000
stop the execution	stop	0.052632
set a configuration property if not already set	spark conf set if missing key	0.500000
approximately find at most k items which	lshmodel approx	0.100000
this vector	mllib linalg sparse vector	0.111111
of possible outcomes for k classes classification problem	classes	0.034483
sql	sql to	0.083333
the cluster centers represented as a	bisecting kmeans model cluster centers	0.095238
underlying :class sparkcontext	sql spark session	0.166667
sparkcontext	streaming streaming context spark	0.083333
convert this matrix to a coordinatematrix	matrix to coordinate matrix	0.333333
first n rows to the console	frame show n	0.333333
that :func awaitanytermination() can	sql streaming query manager reset	0.011905
a new rdd of int containing elements	core spark context	0.011628
to the java related object	init host port	0.200000
dump already	external group	0.045455
queries	manager	0.011236
rdd as a	rdd	0.003058
obj assume that all the objects	obj	0.023810
of this instance this	ml param params	0.013699
of tree (e g depth 0 means	tree model	0.026316
new item	item	0.062500
for kmeans	ml kmeans	0.250000
index of the original	index	0.041667
contains a	ml param params	0.013699
pattern	pattern	0.857143
given parameters in this grid to fixed	ml param grid builder base on	0.076923
a [[pcamodel]] that contains the principal components of	mllib pca fit data	0.166667
a java parammap into	java javaparammap	0.125000
boosted trees model for classification or	boosted trees	0.166667
binomial logistic	logistic	0.125000
labeled points saved using rdd saveastextfile	labeled points sc path minpartitions	0.250000
the column standard deviation values	mllib standard scaler model std	0.166667
average values for each numeric	grouped data	0.035714
the correlation of two columns	col2 method	0.055556
load a model from the	load cls sc	0.214286
that	external	0.013889
generates an rdd comprised of vectors	random rdds normal vector rdd sc	0.200000
setparams(self featurescol="features", predictioncol="prediction",	set params featurescol predictioncol	1.000000
recovers all the	recover	0.142857
the query as fast	processingtime once	0.166667
the square root of the	root	0.035714
p	p	1.000000
the spark fair scheduler	spark context	0.023256
get the	rdd get	0.200000
sets the given parameters in this grid to	grid builder base	0.076923
that all	object	0.027778
create a multi-dimensional rollup	data frame rollup	0.055556
setparams(self featurescol="features", predictioncol="prediction",	mixture set params featurescol predictioncol	1.000000
for this stringindexer	string indexer	0.250000
the cluster centers represented as a list of	bisecting kmeans model cluster centers	0.047619
tokens in the training set given the	distributed ldamodel training	0.034483
train or predict a linear regression model on	linear regression with sgd	0.500000
so that	manager	0.011236
list of conditions and	column otherwise	0.200000
of this instance	ml pipeline model	0.066667
number of rows	mllib linalg indexed row matrix num rows	0.200000
makes a class inherit documentation from	inherit doc	0.045455
a new dstream in which	streaming streaming context transform	0.066667
fit test	test	0.015152
test that the final value of	test	0.015152
deviance for	summary deviance	0.125000
possible outcomes for k classes classification problem	classes	0.034483
the month of a given	dayofmonth col	0.031250
into disks	core external	0.016129
param with a given string name	has param	0.019231
for multiclass classification evaluator	multiclass classification evaluator	0.500000
it can be used in sql statements	name f returntype	0.125000
this vector to the	vector	0.019231
of functions registered in the specified database	functions dbname	0.500000
to save	save	0.062500
a list of predicted ratings for input	mllib matrix factorization model predict all user_product	0.050000
load a model	mllib java loader load cls sc	0.250000
:func awaitanytermination()	streaming query manager reset	0.011905
which is	mllib regression	0.068182
for each key using	by key func numpartitions	0.062500
by a	core	0.006042
feature selection by	mllib chi sq selector	0.200000
adds an input option for	stream reader option key value	0.333333
this ml instance	ml pipeline	0.047619
configuration	conf init loaddefaults	1.000000
to the same time	to	0.007692
squared	sq	0.166667
the given path the	path	0.010204
value pairs or two	linalg	0.044444
set the selector type	selector type	0.100000
the output by	writer bucket by	0.100000
values from this instance to another	values to extra	0.333333
prediction a k a confidence column name	prediction col	0.142857
with	range	0.030303
reducebykey to each	reduce	0.041667
topicdistributioncol or its	topic distribution col	0.250000
the behavior when	frame writer mode savemode	0.333333
this model instance	ml gaussian mixture model	0.500000
extracts the embedded default param values and user-supplied	param params extract param	0.333333
to all mixture components	mllib gaussian mixture model predict soft	0.142857
comprised of vectors containing i	random rdds gamma vector	0.125000
for	model	0.033520
used	query	0.010753
generic function to	createcombiner mergevalue mergecombiners numpartitions	0.500000
of terms or	ml ldamodel	0.111111
sparse vector using either a dictionary	mllib linalg vectors sparse size	0.166667
this	external	0.013889
dictionary of values	values	0.050000
that	streaming query	0.010526
validates the block	block	0.090909
the :class dataframe to	frame writer save	0.066667
of predicted clusters in	ml clustering summary prediction	0.333333
maxiter=100 regparam=0 0 tol=1e-6 rawpredictioncol="rawprediction", fitintercept=true standardization=true	maxiter	0.166667
load a model from the given path	matrix factorization model load cls sc path	1.000000
contents of this :class dataframe as pandas pandas	pandas	0.090909
to wait for	query manager reset	0.011905
a string in libsvm format	libsvm p	0.250000
value of	ml logistic	0.333333
again to wait for new terminations	query	0.010753
queries so that :func awaitanytermination() can be used	sql streaming query manager	0.011905
an rdd of labeledpoint	load lib svmfile sc path numfeatures minpartitions	0.125000
for each numeric	grouped data	0.071429
names of fields in	sql	0.002525
root mean squared error which is defined as	linear regression summary	0.013889
the minutes of a given	minute col	0.050000
converts a labeledpoint	mlutils convert labeled point	1.000000
defined as the square root of the mean	root mean	0.250000
used again	manager reset	0.011905
of each instance	ml	0.005505
each point in rdd 'x' has maximum membership	gaussian mixture	0.038462
the training set given	training	0.029412
features corresponding	features	0.043478
the count	count	0.016949
converts vector columns in an	mllib mlutils convert vector columns from	0.166667
restore an object	core restore name fields value	0.333333
a rowmatrix	row	0.125000
a new	ml java wrapper new	0.250000
set the selector	selector	0.100000
lines text format or newline-delimited json	writer json path	0.125000
of rows	count	0.016949
wait until any of the	await any termination timeout	0.166667
creates a new sqlcontext	sqlcontext init sparkcontext sparksession	1.000000
columns that describes the sort	data frame sort cols cols kwargs	0.142857
by the optional key function	key	0.017857
set multiple parameters passed as a list	core spark conf	0.055556
how data of a	sql data stream	0.031250
rdd 'x' to all mixture components	mixture model predict soft x	0.142857
product and	product	0.029412
a matrix from the new mllib-local	matrices from ml mat	0.333333
this matrix to the new mllib-local representation	linalg dense matrix as	0.333333
batch of jobs has	batch	0.068966
a java model from the	java cls	0.111111
getitem	getitem	1.000000
with matching keys in c{self} and c{other}	join other numpartitions	0.071429
components	model predict soft x	1.000000
text format or newline-delimited json	writer json path	0.125000
rdd 'x' to all mixture	mixture model predict	0.125000
rdd of int containing elements from start to	core spark context range start	0.090909
paired rdd where the first element is the	factorization	0.038462
transfer this instance's params to the wrapped java	ml java params to	0.045455
rdd of key-value pairs (of form	core rdd	0.010381
cost sum of squared distances of	cost x	0.142857
of the :class dataframe as the	sql data frame writer save as	0.071429
is checkpointed and materialized	is checkpointed	0.142857
one	core rdd	0.003460
n rows to the	n truncate vertical	0.250000
value to list of ints if possible	ml param type converters to list int value	0.333333
of model	mllib tree ensemble model	0.058824
size to be used for each iteration	size	0.009174
queries so that :func	streaming query manager reset	0.011905
signed shift the given value numbits right	shift right col numbits	1.000000
or	get id	1.000000
unset	unset	1.000000
or compute the number of rows	indexed row matrix num rows	0.200000
for each numeric columns for each group	sql grouped	0.086957
name	ml param	0.009524
term to this accumulator's	core accumulator add term	0.066667
all	core external	0.016129
stop the execution of the streams with option	stop	0.052632
dstreams	dstreams transformfunc	0.125000
with a function	defined function	0.066667
convert this matrix to a rowmatrix	indexed row matrix to row matrix	0.333333
python direct kafka rdd messagehandler	kafka rdd message handler	1.000000
merge the values for each key using	key func numpartitions	0.066667
foreachrdd get offsetranges	foreach get offset ranges	1.000000
the context	streaming streaming context	0.032258
from the population should be a	mllib stat kernel density	0.066667
ignore separators	sql ignore	0.333333
params to the wrapped java	params to	0.035714
of this instance this updates both	ml	0.001835
converts	mlutils convert	0.833333
attr lda keeplastcheckpoint is set	distributed ldamodel get	0.066667
the database table named table	table column lowerbound	0.166667
field in "predictions" which gives the true label	logistic regression summary label col	0.333333
table accessible via jdbc url url and	reader jdbc url table	0.166667
collect each rdds into the returned list	test case collect dstream n block	1.000000
using an associative and commutative reduce	core rdd reduce by	0.125000
on a spark	core spark	0.010309
number of nonzero	linalg dense vector num	0.166667
"num" number	num	0.016807
a column scipy matrix from a dictionary	sci py tests scipy matrix	0.090909
columns	columns	0.196078
input schema	data frame reader schema schema	0.333333
convert each python object into	mllib to	0.250000
the += operator adds a term	accumulator iadd term	0.142857
extract the day of the month	sql dayofmonth col	0.031250
greatest value of the list	sql greatest	0.055556
test predicted values on a toy	streaming logistic regression with sgdtests test	0.111111
computes average values for each numeric columns	sql grouped data avg	0.058824
words closest in	word2vec	0.052632
to a java pipelinemodel used for ml	pipeline model to java	0.100000
python parammap into a java parammap	param map to java pyparammap	0.250000
boundaries in increasing order for which predictions	regression model boundaries	0.333333
default min number of partitions for	core spark context default min partitions	0.250000
is received using	storagelevel	0.100000
rdd of key-value	rdd	0.009174
a given string name	ml param	0.009524
value of	ml chi	0.400000
format	format	0.888889
bisecting k-means	bisecting kmeans train	0.500000
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params	vector indexer set params maxcategories inputcol outputcol	0.333333
partition	partition	0.600000
another one	add other	1.000000
underlying output	data frame writer	0.014085
set bandwidth of each sample	kernel density set bandwidth bandwidth	0.142857
return a resulting rdd that contains a	core rdd cogroup other	0.066667
create a python topicandpartition	topic and partition init topic partition	0.055556
min value for each	min	0.041667
used	sql streaming	0.010204
list of functions registered in	catalog list functions	0.250000
observed tokens in the training set given	ldamodel training	0.034483
local non-distributed model fitted by :py class lda	local ldamodel	1.000000
each of the points belongs to in	predict x	0.033898
note : experimental	core rdd mean approx timeout confidence	1.000000
inherit documentation	inherit	0.037037
finding frequent items for	data frame freq items cols support	0.166667
local property that affects jobs submitted from	local property key	0.035714
save a	linear regression model save	0.500000
into	parse	0.071429
the area under the	metrics area under	0.166667
the partition	partition	0.066667
all globals names read or written to	pickler extract code globals	0.125000
and profiles	core	0.003021
partial objects do not	pickler save partial obj	0.125000
error which is defined	regression	0.010000
of	rdd	0.006116
dot product of two vectors we	dot other	0.050000
a spark	core spark	0.020619
the threshold if any used for	threshold	0.018182
month of a given date	sql dayofmonth col	0.031250
generates an rdd comprised of	mllib random rdds gamma vector rdd sc	0.200000
the minutes of a given date as	sql minute	0.050000
resulting rdd that contains	core rdd cogroup	0.066667
mlreader for :py class javaparams	mlreader	0.037037
returns a list of active queries associated with	sql streaming query manager active	0.066667
the python direct kafka stream transform	kafka direct stream transform	0.500000
old hadoop outputformat api mapred	hadoop dataset conf keyconverter valueconverter	0.083333
the mean variance and count	core	0.003021
fast version of	heappushpop heap	0.142857
dump already partitioned data into	by	0.014286
the list of column	sql	0.005051
evaluates the output	evaluator evaluate dataset	0.285714
that :func awaitanytermination() can be used	query	0.010753
accumulator's value only usable in driver	core accumulator value	0.045455
generates python code for a shared param class	ml param gen param code name doc defaultvaluestr	0.333333
list	catalog list	0.500000
return sparkcontext	streaming streaming context spark	0.083333
that match regexp	sql regexp	0.125000
vec	vec	0.833333
array containing the ids of all active	active stage ids	0.200000
transforms a python	java params transfer	0.125000
table named table	table	0.031250
python topicandpartition to map to the	topic partition	0.055556
until any of the queries on	sql streaming query manager await any termination	0.142857
dot product of two vectors	dot	0.040000
concatenates multiple input string columns together into	concat ws sep	0.500000
conditions and returns one of multiple possible	sql column otherwise value	0.050000
a line in libsvm format into label indices	mllib mlutils parse libsvm line line	0.111111
specifies some hint on the current	hint name	1.000000
into disks	group by	0.041667
compute the number	mllib linalg indexed row matrix num	0.125000
specified	tablename	0.043478
this dstream and other	other	0.033333
specifies the behavior when data or table	data frame writer mode savemode	0.071429
contains a param with	param params has	0.019231
log likelihood	ldamodel log likelihood	0.142857
labels corresponding to indices	string indexer model labels	0.166667
prefix of string in doc	prefix	0.083333
an rdd comprised of vectors	random rdds exponential vector rdd	0.166667
of terms to term frequency vectors or	tf	0.076923
for every feature	linear	0.025641
model to make predictions on batches	linear algorithm predict on	0.066667
timeunit to configure the kmeans algorithm for	kmeans	0.025641
test	tests test map partitions	1.000000
and value class from	core spark	0.020619
sets the sql context to use for saving	ml mlwriter context sqlcontext	0.333333
set initial centers should be set	initial centers centers weights	0.200000
get a local property set in	core spark context get local property key	0.066667
train a decision tree model for	mllib decision tree train regressor cls	0.333333
value	get	0.021739
is	context	0.022727
function and	sql user defined function	0.083333
csv file and	csv path	0.166667
is close to the desired value	parameter accuracy	0.029412
feature	linear model	0.066667
a column scipy matrix from a dictionary of	sci py tests scipy matrix size	0.090909
sets the given spark runtime configuration property	runtime config set key value	1.000000
current [[dataframe]] and perform	pivot pivot_col values	0.050000
and commutative reduce function but	reduce	0.041667
transforms a python	params transfer	0.125000
dataframe, using the given join expression	join	0.034483
tests whether this instance contains a	ml param params has param paramname	0.142857
that starts at pos in byte and	pos	0.022222
create a unified dstream from multiple dstreams of	streaming streaming context union	0.111111
code for a shared param class	param gen param code name doc defaultvaluestr	0.333333
specified table as	reader table tablename	0.500000
creates a new sqlcontext	sql sqlcontext init sparkcontext sparksession jsqlcontext	1.000000
how much of memory	external merger object size	0.032258
new	sql streaming query manager	0.011905
to the input dataset this	dataset	0.020408
cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1	ml random forest classifier	0.023256
a list of numpy	ml bisecting kmeans model	0.076923
transforms a python parammap	params transfer	0.125000
:func	streaming query manager	0.011236
finding frequent items for columns possibly with false	frame freq items	0.166667
the model's transform method	ml linear regression summary predictions	0.200000
py or zip	py file	0.066667
save	linear regression model save	0.500000
set a	context set	0.125000
cluster centers represented as a list	cluster centers	0.090909
remember	remember	1.000000
the cluster centers represented as a	mllib kmeans model cluster centers	0.083333
a temporary table in the	table df	0.083333
gives the	linear	0.076923
prediction tasks regression and	prediction	0.041667
data type	parse datatype	0.333333
tests whether this instance contains a	param params has param paramname	0.142857
labelcol="label", predictioncol="prediction", maxiter=100 tol=1e-6	labelcol predictioncol maxiter	0.333333
savemode	savemode	1.000000
matching keys in c{self} and c{other}	join other	0.071429
root	root	0.214286
vectorsize	vectorsize	0.833333
representing the result	sqlquery	0.027027
the day of the month	sql dayofmonth	0.031250
be used again to wait for new terminations	reset	0.011236
of labeledpoint	mllib mlutils load lib svmfile	0.125000
:class dataframe using the specified columns so we	data frame	0.005000
decorator that makes a class inherit documentation	inherit doc	0.045455
the area under the precision-recall	classification metrics area under pr	0.333333
value to a boolean if possible	ml param type converters to boolean value	0.250000
of the non-streaming :class dataframe out	sql data frame write	0.071429
pretty printing of	str	0.090909
score	metrics	0.041667
convert a value to a mllib	to	0.007692
number	linalg row matrix num	0.100000
column names	cols	0.052632
convert matrix attributes which are array-like or	linalg matrix convert	0.166667
this instance with a randomly generated	one vs rest model	0.058824
objects	core external merger object	0.032258
name for column of predicted clusters in predictions	ml clustering summary prediction col	0.111111
:func awaitanytermination() can be used again	query manager reset	0.011905
:py attr featurescol	features col value	1.000000
a large dataset and an item	nearest neighbors dataset key numnearestneighbors	0.333333
table	table df tablename	0.083333
content of the dataframe in a text	sql data frame writer text	0.200000
sets	param set	1.000000
accumulator with a	accumulator init aid	0.083333
make predictions on batches	streaming linear algorithm predict on	0.066667
setupfunc	setupfunc	1.000000
for feature selection by number of	mllib chi sq selector set num	0.250000
a local property set in this thread or	local property key	0.035714
setparams(self	vectorizer set params	1.000000
a given string name	ml param params has param	0.019231
comprised of vectors containing i i d	mllib random rdds exponential vector	0.125000
"zerovalue" which may be	fold by	0.125000
for cross validator	cross validator	0.045455
points from the population should	mllib stat kernel density	0.066667
for fitting	streaming	0.005025
an rdd of points using the model trained	mllib tree ensemble model	0.058824
the selector type of the chisqselector	mllib chi sq selector set selector type selectortype	0.333333
from start to end exclusive	range start end	0.333333
extracts the embedded default param values and user-supplied	ml param params extract param map	0.333333
points belongs to in this model	model predict x	0.250000
and value	core spark context	0.023256
obj assume	obj	0.023810
column or names into a jvm seq	seq sc	0.055556
called when a receiver has been started	streaming listener on receiver started receiverstarted	1.000000
set the selector	sq selector set selector	0.333333
hadoop configuration which is passed	spark context hadoop	0.090909
fields	struct type len	0.200000
can be used	reset	0.011236
update	mllib streaming kmeans model update	0.500000
join expression	join	0.034483
wait for new terminations	manager reset	0.011905
counter	counter	1.000000
returns one of multiple possible result	sql	0.002525
term to this accumulator's value	core accumulator add term	0.066667
new class dataframe that with new specified column	data frame to df	0.090909
create a multi-dimensional rollup	sql data frame rollup	0.055556
the input data	sql data stream reader	0.250000
by	doc	0.222222
l{statcounter} object that captures the	stats	0.055556
distributed	distributed	0.777778
setparams(self stages=none) sets params for pipeline	pipeline set params stages	1.000000
set it will run the query as fast	processingtime once	0.166667
numbits	numbits	1.000000
data into	external group by	0.045455
numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1	random forest	0.041667
word2vec model's	word2vec	0.052632
mixin for instances that provide javamlreader	java mlreadable	0.250000
in a data source	source	0.105263
:func awaitanytermination() can be used again to	manager	0.011236
of labeledpoint	mlutils load lib svmfile sc	0.125000
return an numpy ndarray	ml linalg dense matrix to array	1.000000
the stage	stage	0.125000
stream api with	stream	0.017544
compute the number of cols	mllib linalg distributed matrix num cols	0.333333
types inferred from	type	0.024390
rdd as non-persistent and remove	rdd unpersist	0.066667
columns in an input	columns to ml	0.125000
select filter	model selected features	0.333333
loads json files and	reader json path	1.000000
set a java system property such as spark	core spark context set system property cls	1.000000
a value to a	to	0.015385
number of training	ml generalized linear regression training summary num	1.000000
0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance",	ml decision tree regressor	0.066667
string name	has param	0.019231
the least value of the list of column	least	0.043478
name	param params	0.014925
sets	scaler set	1.000000
are the left singular vectors of	singular	0.015625
get number of	model num	0.083333
inputformat with arbitrary key and	inputformatclass keyclass valueclass	0.125000
:class datatype the data type string	datatype string s	0.111111
ctx	ctx	0.833333
list of numpy arrays	ml bisecting kmeans	0.062500
this params	params	0.006623
data into disks	core external group by spill	0.047619
a jvm scala map from	sql data frame jmap jm	0.111111
matrix on the driver as	block matrix to local matrix	0.250000
rdd, a list or a :class pandas dataframe	data frame data schema samplingratio verifyschema	1.000000
mark the rdd as non-persistent	rdd unpersist	0.066667
0 weightcol=none aggregationdepth=2):	linear svc	0.285714
the minutes of a given date as	sql minute col	0.050000
of the :class dataframe	frame writer save path	0.066667
return a copy of the rdd	core rdd partition	0.333333
python list to java type array	sql to jarray gateway jtype arr	0.500000
and assert both have the same param	m1 m2 param	0.125000
value of the date	date	0.037037
statistic	mllib stat	1.000000
wait until any of	streaming query manager await any termination timeout	0.166667
how much of memory for this obj assume	external merger object size obj	0.040000
this	params has param	0.019231
new :class column for approximate distinct count of	approx count distinct col rsd	0.066667
a given string name	param params	0.014925
standard	standard	0.428571
the :class dataframe using the specified	sql data frame	0.005348
parses a line in	line line	0.166667
how much of	external	0.013889
the dot product of two	mllib linalg dense vector dot	0.058824
this streamingcontext	context	0.022727
set	sql option utils set	1.000000
in libsvm format into label indices values	parse libsvm	0.125000
converts vector columns in an input dataframe from	mlutils convert vector columns to ml dataset	0.166667
external list	external list of	0.166667
returns	sql next	1.000000
randomly	cross validator	0.045455
adds an input option for	reader option key value	0.500000
columns are the left singular vectors of the	linalg singular	0.017544
heappop	heappop	1.000000
method for binary operator this	op name doc	0.166667
predict the label of one or more examples	mllib decision tree model predict x	1.000000
deviance for the fitted	regression summary deviance	0.125000
property that affects jobs submitted from this	property key value	0.125000
active queries associated with this sqlcontext >>>	query manager active	0.066667
sample points	sample sample	0.333333
squared distance from	mllib linalg sparse vector squared distance other	0.166667
instance	params has	0.019231
whose columns are the left singular vectors of	singular	0.015625
norm	sparse vector norm p	0.133333
converts vector columns in an	convert vector columns to ml dataset	0.166667
term to this accumulator's value	add term	0.066667
of the :class dataframe to a data	sql data frame	0.005348
a batch of jobs has	batch	0.068966
a multi-dimensional rollup for the current :class dataframe	data frame rollup	0.055556
so that :func awaitanytermination() can be	streaming query	0.010526
the initial value of	sgd set initial	0.111111
each rdds into	dstream n	0.333333
"predictions" which gives the features of	ml linear regression summary features col	0.166667
train the model on the incoming	streaming kmeans train on	0.333333
the test this should be list of lists	test	0.015152
pickler	pickler	1.000000
that :func awaitanytermination() can	query manager reset	0.011905
tol=1e-6 seed=none layers=none blocksize=128 stepsize=0 03 solver="l-bfgs", initialweights=none)	multilayer perceptron classifier	0.333333
:class pyspark sql	sql	0.005051
test predicted values on a toy	mllib streaming logistic regression with sgdtests test predictions	0.500000
stream will start and stop	test case take dstream	0.250000
size to be	size	0.009174
temp table from catalog	temp table tablename	0.500000
calculates the norm	norm	0.125000
user and the second is an array of	mllib	0.010526
create an rdd for	session create	0.117647
checkpointinterval=10 impurity="variance",	ml decision tree regressor	0.066667
use only create a new	hive context create	0.083333
set multiple parameters passed as a list of	core spark conf	0.055556
computes hex value	sql hex col	0.166667
the soundex encoding for	soundex col	0.055556
i d samples drawn	numrows numcols	0.125000
maintaining the heap	heap	0.047619
a line in libsvm format	libsvm line line	0.333333
predictioncol="prediction",	predictioncol	0.821429
of functions registered in	functions	0.071429
used again to wait for new	sql streaming query manager reset	0.011905
imputer	imputer	0.600000
a class inherit documentation from its parents	mllib inherit doc	0.045455
rawpredictioncol raw prediction	raw prediction	0.200000
population should be a	stat kernel density	0.200000
returns a dummy params instance used as a	params dummy	0.111111
rdd into a	core rdd	0.003460
or compute the	mllib linalg	0.052632
increased by step	step	0.100000
return its path	core	0.003021
a paired rdd where the first element is	factorization model	0.043478
creates a	sparkcontext sparksession	0.500000
s	s	0.357143
final	final	1.000000
standard deviation of this	stdev	0.047619
a local property that	local property key	0.035714
decision tree parameters	decision tree params	1.000000
vector representation of each word in vocabulary	mllib word2vec fit data	0.200000
smallest value and add the new item	item	0.062500
extract the year of a given date	year	0.040000
a text file stream and	stream reader text path	0.333333
blocking	blocking	0.714286
which	regression model	0.031250
matrix to the new mllib-local representation	mllib linalg dense matrix as ml	0.333333
awaitanytermination() can be used again	streaming query manager reset	0.011905
the norm	mllib linalg sparse vector norm	0.083333
instance contains	param params has	0.019231
the greatest value of the list	sql greatest	0.055556
local property set in this	context get local property key	0.066667
get the offsetrange of specific kafkardd	streaming kafka rdd offset ranges	0.333333
with extra values from input	map extra	0.040000
ridgeregressionmode	ridge regression model	1.000000
create a python topicandpartition	streaming topic and partition init topic partition	0.055556
the values for each key using an associative	by key func numpartitions partitionfunc	0.066667
specified table or view as a :class dataframe	sqlcontext table tablename	0.142857
sets the sql context to use for	context sqlcontext	0.083333
java_model to a python model	create model java_model	0.250000
conduct pearson's chi-squared goodness of	mllib stat statistics chi sq	0.066667
features	features col	0.250000
experimental	timeout confidence	1.000000
until any of the queries on the associated	any	0.083333
applying a function on each rdd of this	transform func	0.058824
wait	timeout	0.142857
probabilitycol="probability", rawpredictioncol="rawprediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256	probabilitycol	0.150000
__init__(self inputcol=none outputcol=none	string init inputcol outputcol	1.000000
load labeled points saved using rdd saveastextfile	load labeled points sc path minpartitions	0.250000
the residual degrees of freedom for the	regression summary residual degree of freedom	0.125000
value for each original	max scaler model original	0.062500
__init__(self featurescol="features", labelcol="label", predictioncol="prediction",	ml decision tree classifier init featurescol labelcol predictioncol	1.000000
compute the number of rows	linalg block matrix num rows	0.200000
combine the items by creator and combiner	core merger merge values iterator	0.166667
average precision map of	average precision	0.500000
the embedded params to the companion java object	transfer params to java	0.500000
enable 'with sparkcontext as	spark context	0.023256
is an	mllib	0.010526
multinomial logistic regression	mllib logistic regression	0.250000
saves the content of the dataframe in a	sql data frame	0.005348
column standard deviation	standard scaler model std	0.166667
dstream by applying reducebykey	streaming dstream reduce by key func numpartitions	0.076923
of training	ml linear regression training summary	0.500000
converts matrix columns in an input dataframe from	mlutils convert matrix columns to ml dataset	0.166667
rdd of key-value pairs (of form c{rdd[	rdd save as	0.038462
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto",	random forest classifier	0.022727
:class column for distinct count of	count distinct col	0.040000
key and value class	core	0.006042
sql storage type for	type sql type	0.250000
already partitioned data	core	0.003021
norm of a	vector norm	0.055556
mlwriter for	mlwriter	0.062500
of pairs split the list of values	values	0.050000
sparkcontext is initialized or	spark context ensure initialized	0.333333
even if users construct taskcontext instead of using	task context	0.142857
wait for	termination or timeout timeout	0.125000
columns are the left singular vectors	mllib linalg singular	0.017544
ensuring all received data	stopsparkcontext stopgracefully	0.050000
week number	weekofyear	0.043478
statements	returntype	0.071429
column into type datatype	sql column cast datatype	1.000000
v1	v1	1.000000
of this dataset checkpointing can	frame checkpoint eager	0.071429
new spark configuration	core spark conf init	0.250000
array of features	features	0.043478
be used for later scaling	standard scaler fit dataset	0.250000
hadoop configuration which is passed in as	hadoop	0.050000
version of	heap item	0.125000
list of	ml	0.005505
"predictions" which	regression summary	0.107143
a large dataset and an item approximately	ml lshmodel approx nearest neighbors dataset key	0.166667
adds a term to this accumulator's	add term	0.066667
it will run the query as fast	processingtime once	0.166667
format into an rdd of labeledpoint	lib svmfile sc path numfeatures minpartitions	0.125000
pairs or two separate arrays of indices	ml linalg	0.030303
note : developerapi	vector transformer	0.250000
rdd storage level	storagelevel	0.100000
new :class column for distinct count of	count distinct col	0.040000
features corresponding to	features	0.043478
the initial	with sgd set initial	0.111111
dataset in a data source	source	0.105263
:class dataframe using	sql data frame	0.005348
the year of a given date as integer	sql year col	0.050000
blockmatrix	linalg block matrix	0.210526
sparkcontext as	core spark context	0.023256
blockmatrix by other, another blockmatrix	mllib linalg block matrix multiply other	0.200000
a resulting rdd that contains a	core rdd cogroup other	0.066667
dump already partitioned data into disks	external group by	0.045455
converts vector columns	mlutils convert vector columns from ml	0.166667
drop the names of fields in	sql	0.002525
predictioncol="prediction", k=2	predictioncol k	1.000000
pearson's independence test using dataset	chi square test test dataset featurescol	0.333333
use only create a new hivecontext for testing	sql hive context create for testing cls sparkcontext	0.333333
parameters in this grid to	ml param grid builder base on	0.076923
expr	expr	0.384615
columns that describes the sort order	sort cols cols kwargs	0.142857
:class statcounter members as a dict	core stat counter as dict sample	0.333333
already partitioned data	group by spill	0.047619
batches of data	linear algorithm	0.076923
get a local property set	core spark context get local property key	0.066667
dot product of two vectors we	mllib linalg dense vector dot other	0.058824
set each dstreams in this context to	streaming context	0.055556
list or gets an item by key out	sql column get item key	1.000000
time of day in utc returns	sql from utc	0.142857
generate	streaming logistic regression with sgdtests generate logistic input	1.000000
instance with a randomly generated	cross validator model	0.050000
partitioned data into disks	external group by spill	0.047619
as a temporary table in the catalog	as table df	0.250000
length of a	sql length col	0.050000
be used	sql streaming	0.010204
minimum number of times a token must appear	min count	0.076923
instance's params to	ml java params to	0.045455
this matrix to an indexedrowmatrix	mllib linalg coordinate matrix to indexed row matrix	0.333333
residual degrees of freedom for the	regression summary residual degree of freedom	0.125000
train a random forest model for binary or	mllib random forest train classifier cls	0.250000
column of the current [[dataframe]] and perform the	data pivot pivot_col values	0.050000
create a python topicandpartition	topic partition	0.055556
save this model to	bayes model save	0.500000
is to be used with the spark sink	maxbatchsize	0.037037
name for column of predicted clusters in	ml clustering summary prediction col	0.111111
thresholds	thresholds	0.500000
test the python direct kafka rdd api	streaming kafka stream tests test kafka rdd	0.500000
function "func" and a	core rdd	0.003460
the java loader the default	mllib java loader java loader	0.333333
a single script on a cluster	single script on cluster	0.500000
save this model to the	kmeans model save	0.500000
create a new spark configuration	spark conf init loaddefaults	0.250000
returns an mlreader instance for this class	one vs rest read cls	1.000000
or names into a jvm seq of column	seq sc	0.055556
model	tree ensemble model	0.038462
this instance contains a param with a	ml param params has param	0.019231
lib	lib	1.000000
of	ml distributed ldamodel	0.100000
for col1	col1	0.111111
for the termination of this query either by	termination timeout	0.041667
this instance's params to the wrapped	ml java params to	0.045455
whose columns are the left singular	linalg singular	0.017544
vector conduct pearson's chi-squared	chi sq	0.111111
orc	writer orc	0.500000
optional default value and user-supplied value in a	ml param params	0.013699
paths to files added through l{sparkcontext	spark files	0.250000
hadoop	spark context hadoop	0.090909
regparam regularization parameter (>= 0)	reg	0.166667
points	points	1.000000
return	streaming	0.005025
a new sparkcontext at	spark context init	0.083333
sets the given spark runtime configuration property	runtime config set key	1.000000
convert a list of column or	sql to	0.041667
squared distance from a sparsevector	squared distance	0.142857
this matrix to a coordinatematrix	mllib linalg block matrix to coordinate matrix	0.333333
comprised of vectors containing	random rdds normal vector	0.125000
column for approximate	approx	0.047619
__init__(self n=2 inputcol=none outputcol=none)	ml ngram init n inputcol outputcol	1.000000
the :class dataframe	sql data frame writer	0.046512
comprised of vectors containing i i d samples	mllib random rdds normal vector	0.125000
compare 2 ml types asserting that	test compare	0.166667
value in c{self} that is not contained in	rdd subtract other numpartitions	0.111111
right outer join of	rdd full outer join other	0.111111
retrieve gaussian	ml gaussian	0.333333
particular batch has half the weightage	half life halflife	0.166667
this instance to a	to	0.015385
results immediately to the master	locally	0.083333
dot product of two	dot other	0.050000
the training set given the current parameter estimates	training	0.029412
a param with a given string name	has param	0.019231
the list based on	based on	0.111111
update	kmeans model update	0.500000
adds	frame	0.034483
returns	sql spark	0.250000
is	ldamodel is distributed	0.200000
of this instance	ml param	0.019048
a dummy params instance used as	ml param params dummy	0.111111
of the :class dataframe to a data	data frame writer save	0.083333
get pipeline	pipeline get	1.000000
converts vector columns in an	mlutils convert vector columns to	0.166667
fast version	heappushpop heap item	0.142857
singular vectors of the	linalg singular	0.035088
number	row matrix num	0.100000
the new item	item	0.062500
total log-likelihood for this model on the	ml gaussian mixture summary log likelihood	0.142857
text format or newline-delimited json	writer json path mode	0.125000
jobs has started	started	0.055556
coordinatematrix	matrix	0.015152
operation test for dstream mapvalues	operation tests test map	0.333333
:py attr lda keeplastcheckpoint is set	distributed ldamodel get	0.066667
stopwordremover	stopwords casesensitive	1.000000
test the python direct kafka stream api	tests test kafka direct stream	0.250000
model's transform method	ml linear regression summary predictions	0.200000
python topicandpartition to map	init topic partition	0.055556
set the selector	mllib chi sq selector set selector	0.333333
vector	ml linalg vector	0.500000
with the keys of each	keys	0.111111
rdd >>> rdd = sc	rdd get	0.200000
mlreader for :py	mlreader	0.037037
of a dataframe	data frame corr col1	0.166667
make predictions on a keyed	streaming kmeans predict on values	1.000000
statements	function name javaclassname returntype	1.000000
alternating least squares matrix factorization	als	0.142857
load a model from the	factorization model load cls sc	0.333333
a python topicandpartition to map to the java	and partition init topic partition	0.055556
with option of ensuring all received	stopsparkcontext stopgracefully	0.050000
mixin for param support	has support	1.000000
results immediately	locally func	0.142857
explained variance regression score	mllib regression metrics explained variance	0.333333
transfer this instance to a	to	0.015385
whose columns are the left singular vectors of	linalg singular	0.017544
onevsrestmodel create and return a python wrapper	one vs rest	0.034483
for	external merger	0.031250
the	external merger	0.031250
into disks	group by spill	0.047619
set the initial value	sgd set initial	0.111111
one operation	rdd	0.003058
convert matrix attributes	ml linalg matrix convert	0.166667
how much of memory for	core external merger	0.032258
pipeline create and return	pipeline from	0.142857
comprised of vectors	random rdds gamma	0.125000
predicted	summary prediction	0.500000
rsd	rsd	1.000000
function rdd[x] -> rdd[y]	transform function	0.166667
creates a model from the input	estimator create	1.000000
abstract class representing a multiclass classification model	classification model	0.166667
minimum splits in dataset	minsplits	1.000000
number of training	ml linear regression training summary	0.500000
of	ml linalg sparse	1.000000
an rdd ordered in ascending order or as	core rdd take ordered	0.050000
compute	linalg distributed	0.333333
columns are the right singular	singular	0.015625
memory for this	external	0.013889
matrix columns	matrix columns from ml	0.142857
java pipeline create and return	pipeline from java	0.142857
the given parameters in this grid to	grid builder base on	0.076923
the input param belongs to	param	0.006250
instance contains a	ml param	0.009524
maximum item in this rdd	core rdd max key	0.333333
cluster for each training	summary	0.024390
each stratum	by col fractions seed	0.142857
rdd of key-value pairs	rdd save	0.038462
are array-like or buffer	array_like dtype	0.166667
given database	dbname	0.045455
transforms a python	ml java params transfer	0.125000
duration	duration	1.000000
model with weights already set	linear regression with	0.111111
containing elements from start to end exclusive increased	core spark context range start end	0.166667
of two	ml linalg	0.030303
arg2	arg2	1.000000
:class dataframe representing the result of	sqlquery	0.027027
the :class dataframe to a	frame writer save path	0.066667
lambda function	function name	0.166667
l{statcounter} object that captures the mean variance and	stats	0.055556
results immediately to the master as a dictionary	locally	0.083333
into a	core	0.003021
rdd to	rdd	0.003058
train the model on the	mllib streaming kmeans train on	0.333333
of the month of a	sql dayofmonth	0.031250
columns in an input dataframe from the :py	columns to ml	0.125000
input dataset	input	0.090909
with extra values	extra	0.023810
verify	tests	0.100000
returned	test case	0.333333
create a java array	wrapper new java array	0.333333
or c{other}, return a resulting rdd that contains	core rdd cogroup	0.066667
in rdd >>> rdd =	core rdd get	0.250000
content	writer	0.040000
value of	ml hashing tf	1.000000
a term to this	accumulator add term	0.066667
converts vector columns in an input dataframe to	mllib mlutils convert vector columns from ml dataset	0.166667
in which each rdd contains the	by	0.014286
wait for the	await termination or timeout timeout	0.125000
curve which is	binary logistic regression	0.142857
note : experimental	task context	0.142857
forceindexlabel	force index label	1.000000
to persist its values across operations after the	core rdd persist storagelevel	0.166667
model which can perform an online	model	0.005587
key and	core spark context	0.023256
partial objects do	cloud pickler save partial obj	0.125000
set initial centers should be set	initial centers centers	0.200000
dependent variable given a vector or	linear regression model base	0.142857
the items by creator and combiner	core merger merge values iterator	0.166667
return the number of fields	struct type len	0.200000
of the observed data against	observed	0.058824
curve	binary logistic	1.000000
count of col or	count	0.016949
extract a specific group matched by	regexp extract str pattern idx	0.333333
converts matrix columns in an input	mlutils convert matrix columns	0.166667
a left outer join	rdd left outer join other	0.111111
of memory for this obj assume that all	external merger object size obj	0.040000
model params are	model params	0.125000
this model	linear model	0.066667
driver as	matrix to local	0.500000
a character in matching	matching replace	0.250000
the :class dataframe as the specified table	data frame writer save as table name	0.333333
dot	dot	0.200000
be used again to wait for new	streaming	0.005025
ensemble	mllib tree ensemble model	0.117647
already partitioned data into disks	core external group by	0.045455
specific topic and partition	topic and partition	0.111111
a list of conditions and returns	sql column otherwise value	0.050000
parameters passed as a list of	core spark conf set	0.100000
of labeledpoint	lib svmfile sc path numfeatures	0.125000
be used again to wait for new terminations	streaming query	0.010526
extract	sql regexp extract	0.500000
key-value pair rdd through a flatmap function without	core rdd flat map values f	0.333333
broadcast a read-only variable to the cluster	core spark context broadcast value	0.125000
return an rdd containing	core rdd	0.003460
columns in	columns to ml	0.125000
tests whether this instance	ml param params has param paramname	0.142857
:class pyspark sql types longtype	sql sqlcontext range start end step numpartitions	0.083333
set the selector type of	selector set selector type	0.111111
that all	external merger	0.031250
computes column-wise	mllib linalg row matrix compute column	1.000000
including lambda function	function	0.055556
of nodes summed over all	nodes	0.037037
an input stream that is to	stream ssc addresses	0.166667
convert a number in	sql conv	0.250000
or compute the number	mllib linalg row matrix num	0.125000
json file	json path	0.100000
the training set given the current	training	0.029412
columns in	columns	0.078431
with the dispatch to handle all function types	cloud pickler save function obj name	0.142857
list of index value pairs or two	mllib linalg	0.026316
deviance for the null model	null deviance	0.250000
for loading	mlreader	0.111111
sqlquery	sqlquery	0.135135
of fit test	test	0.015152
a batch of jobs has completed	batch completed batchcompleted	0.333333
a spark	spark	0.026316
bound on the log likelihood	log likelihood	0.125000
convert this matrix to a rowmatrix	coordinate matrix to row matrix	0.333333
from an rdd	rdd	0.003058
__init__(self	ml generalized linear regression init	1.000000
paired rdd where the	factorization model	0.043478
cross	ml cross	0.333333
the ldamodel	mllib ldamodel	0.500000
rdd which is assumed	rdd	0.003058
that	merger object size	0.032258
model on	with	0.055556
set initial centers should	mllib streaming kmeans set initial centers centers	0.200000
is not contained in c{other}	core rdd subtract	0.333333
:func awaitanytermination() can be used again to wait	streaming	0.005025
to select filter	sq selector model selected features	0.333333
count of distinct elements	count	0.016949
relative	relative	1.000000
optimize	optimize	1.000000
format (json lines text format or newline-delimited json	writer json path mode	0.125000
the result as a :class dataframe	sql data	0.024390
to be used with the spark sink	storagelevel maxbatchsize	0.045455
in the training set given the current parameter	ldamodel training	0.034483
the libsvm format into an rdd of labeledpoint	lib svmfile sc	0.125000
the sparkcontext	core spark context	0.011628
all the objects	core external merger object size	0.032258
k-means algorithm return	kmeans train rdd	0.333333
set a configuration property if not already set	core spark conf set if missing key	0.500000
of the :class dataframe to	sql data frame	0.005348
rdd's	core rdd	0.006920
it will convert each python object into	to	0.023077
for new terminations	reset	0.011236
the month of a	sql dayofmonth col	0.031250
extract a specific group matched by	extract str pattern idx	0.333333
the log likelihood	ldamodel log likelihood dataset	0.142857
optional default value and user-supplied value in	params	0.006623
memory	object size	0.032258
predicted values on a toy	with sgdtests	0.200000
the deviance for	generalized linear regression summary deviance	0.125000
for	core	0.003021
cartesian()	cartesian	1.000000
the log likelihood	ldamodel log likelihood	0.142857
input dataset this is	dataset	0.020408
different profilers on a per stage basis	profiler collector	0.142857
norm	linalg sparse vector norm p	0.133333
lassomodel	lasso model	1.000000
test a single script	tests test single script	0.500000
set initial centers should be set before	kmeans set initial centers centers	0.200000
sets window size default 5	word2vec set window size windowsize	1.000000
target instance	to	0.007692
an rdd comprised of vectors	random rdds normal vector rdd	0.166667
depth 0 means 1 leaf	mllib decision	0.125000
compute the dot product	mllib linalg dense vector dot	0.058824
submit and test a script with a	submit tests test	0.142857
inserts the content of the	writer insert into	0.333333
in "predictions" which gives	logistic regression summary	0.090909
of each word in vocabulary	word2vec fit data	0.200000
sparkcontext	core spark context	0.011628
generates an rdd comprised	random rdds normal vector rdd sc	0.200000
update the centroids according to data	mllib streaming kmeans model update data decayfactor timeunit	1.000000
gamma	gamma	1.000000
that makes a class inherit documentation from	inherit doc	0.045455
a sliding window of	window	0.037037
underlying	sql	0.015152
a certain time of day in utc	utc	0.050000
:py attr lda	ml distributed ldamodel get	0.066667
and commutative reduce	rdd reduce	0.071429
c{other}, return a resulting rdd that contains	rdd cogroup	0.066667
:func awaitanytermination() can	manager	0.011236
json_string	json_string	0.625000
be used again to wait for	manager	0.011236
curve which is a dataframe having two fields	binary logistic regression	0.142857
computes column-wise summary statistics for the input rdd[vector]	mllib stat statistics col stats rdd	0.200000
forget about past terminated queries so	query manager reset terminated	0.200000
computes column-wise summary statistics for the input	statistics col stats rdd	0.200000
model to be used for later scaling	mllib standard scaler fit dataset	0.250000
convert a matrix from the new mllib-local	matrices from ml	0.333333
predictioncol="prediction", maxiter=100 tol=1e-6	predictioncol maxiter	0.200000
multinomial logistic regression	mllib logistic regression model	0.083333
boosted	boosted	1.000000
conditions and returns one of multiple possible result	sql column otherwise value	0.050000
choose one directory for spill by	core external sorter get path	1.000000
saves the content of the dataframe	data frame	0.005000
operation test for dstream	operation tests test	0.555556
text format or newline-delimited json <http //jsonlines	writer json path mode compression dateformat	0.166667
result	result	0.750000
the blockmatrix	mllib linalg block matrix	0.105263
elements in	core rdd	0.003460
comprised of vectors containing	random rdds exponential vector	0.125000
called when processing of	streaming streaming listener on output operation	0.166667
a local property that	local property key value	0.076923
a param with a given string	ml param	0.009524
how much of memory	merger object size	0.032258
the uid	uid newuid	0.333333
checkpointinterval=10 impurity="gini", numtrees=20	random forest classifier	0.022727
columns are the right singular	linalg singular	0.017544
the += operator	accumulator iadd	0.500000
load a model from	mllib java loader load cls sc	0.250000
g accuracy/precision/recall objective history total iterations) of	ml logistic regression	0.111111
this instance contains a param with	has	0.011628
binary or multiclass	classifier cls data numclasses	0.250000
returns accuracy equals	metrics accuracy	0.166667
resets the configuration property for the given	runtime config unset	0.142857
aggregate the values of each key	aggregate by key	1.000000
returns micro-averaged label-based recall	multilabel metrics micro recall	1.000000
input data source format	sql data stream reader format source	0.333333
converts vector columns in an input dataframe to	mllib mlutils convert vector columns from	0.166667
pipelinemodel used	pipeline model	0.071429
operation test for dstream countbyvalue	operation tests test	0.111111
statistics	statistics	0.545455
deployed on a flume	streaming flume	0.111111
creates a table based on	catalog create external table tablename path	0.250000
a text file	text file path	0.500000
scipy	scipy	1.000000
so that :func awaitanytermination()	manager reset	0.011905
data or	sql data	0.024390
minutes of a given date as integer	minute col	0.050000
print	print	1.000000
rdd's	core	0.006042
given	ml param params	0.013699
load labeled points saved	load labeled points sc path minpartitions	0.250000
in rdds in a sliding window	value and window windowduration slideduration	0.076923
rdd of key-value	core rdd save as	0.037500
resets the configuration	runtime config unset	0.142857
master	master	0.600000
of layer sizes including input and output layers	ml multilayer perceptron classification model layers	1.000000
for	query manager	0.011905
the items by creator	merger merge values iterator	0.333333
input option for	stream reader option key value	0.333333
test python direct kafka	kafka stream tests test kafka	1.000000
used	query manager	0.011905
__init__(self inverse=false inputcol=none outputcol=none)	dct init inverse inputcol outputcol	1.000000
of the non-streaming :class dataframe out into external	sql data frame write	0.071429
fp-growth model that contains frequent	fpgrowth train cls data	0.200000
svmfile	svmfile	1.000000
max	model max	1.000000
f function	f	0.021053
all values as a	all	0.083333
label of one or more examples	mllib decision tree model	0.076923
:py attr layers	layers value	1.000000
set the initial value of weights	logistic regression with sgd set initial weights	0.333333
in :py attr predictions which gives the predicted	linear regression summary prediction col	0.142857
qrdecomposition	qrdecomposition	0.833333
setparams(self maxcategories=20 inputcol=none outputcol=none) sets params for this	vector indexer set params maxcategories inputcol outputcol	0.333333
setparams(self scalingvec=none inputcol=none outputcol=none) sets params for this	product set params scalingvec inputcol outputcol	0.333333
an input stream that is to be	stream ssc addresses	0.166667
return sparkcontext which is associated with this	spark	0.013158
compute the dot product of two	mllib linalg dense vector dot	0.058824
line in libsvm format into label indices	mlutils parse libsvm line line	0.111111
for new	manager reset	0.011905
get or compute the	mllib linalg block matrix	0.052632
strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none):	strategy missingvalue inputcols outputcols	0.333333
python object into an internal sql object	type to internal obj	0.500000
the week number of	sql weekofyear	0.055556
topics described	describe topics maxtermspertopic	0.333333
for each numeric columns for each	grouped	0.071429
mixin for param elasticnetparam the	has elastic net param	0.200000
returns the least value of the list of	sql least	0.055556
create an input stream that pulls events from	utils create stream	0.200000
value of	ml vector	0.200000
matrix stored in csc format	matrix	0.015152
checkpoint data or create	context get or create	0.200000
file system	file path	0.035714
python topicandpartition to map to the java related	and partition init topic partition	0.055556
:py attr lda keeplastcheckpoint is	ldamodel get	0.066667
again	streaming query manager reset	0.011905
length of a string or binary expression	length	0.040000
of a dataframe as	sql data frame corr col1 col2	0.166667
function and attach docstring from func	user defined function wrapped	0.333333
:class column for approximate distinct count of col	approx count distinct col	0.071429
sort the list based on	case sort result based on	0.333333
version of this dataset checkpointing can be used	frame checkpoint eager	0.071429
removes the specified table from	sqlcontext uncache table tablename	0.250000
each key	key	0.035714
dictionary of values	size values	0.250000
embedded params to the companion	ml java params transfer params to	0.333333
specified string value that match regexp	regexp	0.076923
gets a field by name in a structfield	get field name	0.333333
this instance with	one vs rest	0.034483
elasticnetparam the elasticnet mixing	elastic net	0.125000
whose columns are the left singular vectors	mllib linalg singular	0.017544
predicted values on a toy model	streaming logistic regression with sgdtests	0.200000
a param with a given	ml param params	0.013699
how	external	0.013889
creates an external table based on	sqlcontext create external table tablename path	0.250000
:py attr quantileprobabilities	quantile probabilities value	1.000000
returns	sql from	0.500000
this model instance	gaussian mixture model	0.052632
:py attr lda keeplastcheckpoint	ml distributed ldamodel	0.050000
decision tree	decision tree model	0.050000
given path a shortcut of write()	ml one vs rest	0.052632
document collections and generates a :py attr countvectorizermodel	count vectorizer	0.166667
min value for each original column during fitting	ml min max scaler model original min	0.250000
fields threshold	threshold	0.036364
a right	rdd full	0.333333
parameters passed as	conf set	0.200000
compute the mean	mean	0.034483
model fitted by :py class standardscaler	standard scaler model	0.090909
converts matrix columns in an input dataframe from	mllib mlutils convert matrix columns	0.083333
the deviance for the fitted	generalized linear regression summary deviance	0.125000
for list	of list	0.333333
binary or multiclass	numclasses	0.111111
the libsvm format into an rdd of labeledpoint	mlutils load lib svmfile	0.125000
dataset and an	dataset key numnearestneighbors	1.000000
attr predictions which gives the predicted	generalized linear regression summary prediction col	0.250000
return number of nodes	model num nodes	1.000000
on	regression with	0.200000
by applying c{f}	key by f	1.000000
how much of	object	0.027778
create a new accumulator	accumulator	0.012987
python rdd of key-value pairs (of form c{rdd[	rdd	0.009174
squared distance from	ml linalg sparse vector squared distance other	0.166667
length of a string or binary	length	0.040000
of memory for	size	0.009174
determination	regression metrics r2	0.166667
sets the accumulator's value only usable	core accumulator value	0.045455
:func awaitanytermination() can be used	query	0.010753
for feature selection by number of top	mllib chi sq selector set num top	0.500000
to make predictions on batches of data from	mllib streaming linear algorithm predict on	0.066667
python topicandpartition to	topic and partition init topic partition	0.055556
the rdd as non-persistent	rdd unpersist	0.066667
the stream query if this is not set	data stream writer	0.041667
set bandwidth of each sample defaults	density set bandwidth bandwidth	0.142857
get a local property set in this thread	get local property key	0.066667
sparkstageinfo object or none if the stage info	stage info	0.142857
and	core external	0.016129
of points using the model trained	tree ensemble model	0.038462
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini",	classifier	0.050000
instance contains a param with a given string	ml param	0.009524
a sliding window	value and window windowduration slideduration numpartitions	0.076923
table accessible via jdbc url url and	reader jdbc url table column lowerbound	0.166667
key	key	0.214286
generates an rdd comprised of	mllib random rdds normal vector rdd sc	0.200000
this accumulator's	core accumulator	0.030303
sliding window	value and window windowduration slideduration numpartitions	0.076923
wait a given amount of time	timeout	0.071429
pulls events from flume	flume utils	0.200000
estimated	linear regression summary	0.027778
class representing a multiclass classification	linear classification	0.142857
sum for each numeric columns for each group	grouped data sum	0.083333
cost sum of squared distances of points to	cost	0.105263
non-streaming :class dataframe out into external storage	sql data frame write	0.071429
dot product of two vectors	dot other	0.050000
choose one directory for spill	external merger get spill dir	0.500000
matrix columns in an input dataframe to	matrix columns from	0.142857
to programming spark with	spark session	0.100000
any hadoop file system using the l{org	sequence file path	0.500000
to get or create global taskcontext	core task context get or create cls	0.250000
training set given	training	0.029412
return an iterator of deserialized objects from the	core serializer load	0.500000
to term frequency vectors or transform	mllib hashing tf transform	0.045455
specifies the input data source format	sql data stream reader format source	0.333333
number of possible	num	0.008403
trigger for the stream query if this	data stream writer trigger	0.083333
mean average precision map	mean average precision	0.166667
queries so that :func awaitanytermination() can be used	sql streaming	0.010204
each tree	tree	0.020833
c{other}	join other numpartitions	0.071429
param with a given string name	ml param params has param	0.019231
__init__(self min=0 0 max=1 0 inputcol=none outputcol=none)	min max scaler init min max inputcol outputcol	1.000000
:class windowspec	window	0.074074
generates an rdd comprised of vectors containing i	random rdds log normal vector rdd sc mean	0.200000
local property set in this	get local property key	0.066667
dstream by applying reducebykey to each rdd	streaming dstream reduce by key func numpartitions	0.076923
to end exclusive increased by step	end step	0.500000
a streamingcontext from checkpoint data or create a	context get or create cls	1.000000
until any of the queries on the	streaming query manager await any termination	0.142857
vectors or transform	mllib hashing tf transform	0.045455
count the	count	0.016949
and then merges them with extra values from	extra	0.023810
data into	core external	0.016129
wait	sql streaming query manager reset	0.011905
from the	from ml	0.500000
an array	mllib	0.010526
node	node	1.000000
for each original	ml min max scaler model original	0.062500
a python topicandpartition to	partition init topic partition	0.055556
spark	core spark	0.030928
can be used again to wait	sql streaming query manager	0.011905
compute the dot product of two vectors	vector dot other	0.050000
cluster centers represented as a list	mllib kmeans model cluster centers	0.083333
sets	handle invalid set	1.000000
rowmatrix	linalg row matrix	0.200000
create an input stream that	create stream ssc	0.200000
matrix to	dense matrix	0.076923
sum for each numeric	grouped data sum	0.083333
dot product of two	mllib linalg dense vector dot other	0.058824
inclusive starting offset	fromoffset	1.000000
new :class dataframe with	data frame	0.005000
stopwords=none casesensitive=false) sets	set	0.005917
dstream by applying reducebykey to	streaming dstream reduce by key func numpartitions	0.076923
rdd of document to rdd of term	document	0.040000
dataframe as	sql data frame corr col1 col2	0.166667
set initial centers should be set	kmeans set initial centers centers weights	0.200000
queries so that :func awaitanytermination()	query	0.010753
total	total	0.777778
for this	object size	0.032258
underlying :class sparkcontext	spark session	0.100000
all active stages	status tracker get active	0.333333
array of features corresponding	features	0.043478
converts vector columns in an	convert vector columns from	0.166667
of the list of column names skipping null	sql	0.005051
a local property set	core spark context get local property key	0.066667
files in	core	0.003021
factor	factor	1.000000
attr lda keeplastcheckpoint is set to	distributed ldamodel get	0.066667
converts vector columns in an input dataframe from	mlutils convert vector columns to	0.166667
id	id	0.583333
column scipy matrix from a	mllib sci py tests scipy matrix	0.090909
c{other}, return a resulting rdd that contains a	core rdd cogroup other	0.066667
so that :func	streaming	0.005025
labels corresponding to indices to be	indexer model labels	0.166667
col2	col2	1.000000
resets the	sql runtime config unset	0.142857
model improves on toy data with no of	logistic regression with sgdtests	0.200000
or	streaming streaming context get or	0.200000
timeunit to configure the kmeans algorithm for	streaming kmeans	0.035714
paired	factorization model	0.043478
save this model to the given path	mllib logistic regression model save sc path	1.000000
:func	manager	0.011236
disks	core external group by spill	0.047619
returns accuracy equals to the total	mllib multiclass metrics accuracy	0.166667
which is a dataframe having two fields fpr	logistic regression	0.040000
associative and	core rdd	0.003460
of the rdd partitioned using	rdd partition by	0.062500
user-defined type udt for	udt	0.500000
transforms a python	params transfer param	0.250000
fits	ml estimator fit	0.083333
of corresponding string	string	0.041667
watermark for this :class dataframe a watermark tracks	sql data frame with watermark	1.000000
the column mean	scaler model mean	0.125000
already partitioned data into	core external	0.016129
calculates the correlation of two columns of	col1 col2 method	0.055556
get pipeline stages	ml pipeline get stages	1.000000
batch of jobs has started	batch started batchstarted	0.333333
aic	aic	1.000000
the levenshtein distance of the two given strings	sql levenshtein left	0.058824
be used again to wait for	query	0.010753
points from the	mllib	0.010526
set the initial value of weights	initial weights initialweights	0.333333
get number	num	0.016807
vector columns in an input dataframe from the	vector columns to ml dataset	0.142857
returns the	sql	0.005051
partial objects	pickler save partial	0.125000
column for approximate distinct count of col	approx count distinct col rsd	0.066667
tests whether this instance contains a param with	param paramname	0.111111
a shortcut of write()	ml mlwritable	0.142857
for this	core external merger	0.032258
create a new	hive context create	0.083333
convert a value to a	to	0.015385
given path the	path	0.010204
force	force	1.000000
an int	int value	0.500000
user factors in two columns id and	user factors	1.000000
get the cluster centers represented as a	model cluster centers	0.090909
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10	lda set params featurescol maxiter seed	0.250000
method	method	0.208333
so that :func awaitanytermination() can	sql streaming query	0.011765
can be used	sql streaming	0.010204
arbitrary key and	core spark context	0.023256
them	external	0.013889
jvm seq	seq sc cols converter	0.055556
return the first	spark streaming	0.333333
param with	params has	0.019231
given string name	param	0.012500
by the model's transform method	ml logistic regression summary predictions	0.200000
__init__(self	init formula	0.500000
file	path schema sep	0.666667
set a local property that affects	set local property key	0.200000
restore an object	core restore	0.333333
outputted by the model's transform method	linear regression summary predictions	0.200000
linear regression model	linear regression	0.080000
a text file using string representations	text file path compressioncodecclass	0.166667
returns an mlreader instance	ml mlreadable read cls	0.250000
data or	data frame	0.005000
the mean variance	core rdd	0.003460
get a local	context get local	0.333333
returns a function with	f	0.010526
a param with	params has param	0.019231
converts vector columns in	convert vector columns to ml dataset	0.166667
window over	window	0.037037
based on first	based on key outputs	0.111111
in the training	training	0.029412
function on rdds of the dstreams	transform dstreams transformfunc	0.125000
converts matrix columns in	convert matrix columns from	0.166667
python broker to map	streaming broker	0.200000
to all mixture	mllib gaussian mixture model	0.062500
receive accumulator updates in a	update	0.055556
that :func awaitanytermination() can be used again to	manager	0.011236
average	average	1.000000
an rdd that has no partitions	spark context empty rdd	0.200000
smoothing	smoothing	1.000000
create a multi-dimensional rollup for the current :class	data frame rollup	0.055556
stop	streaming streaming context stop	0.125000
into class	cls	0.047619
dependency on another module on a cluster	module dependency on cluster	0.500000
memory for	object	0.027778
the content of the non-streaming :class dataframe	data frame write	0.166667
as non-persistent and remove all	unpersist	0.083333
applying a function on each rdd	transform func	0.117647
this docstring is not shown publicly	linalg indexed row matrix init rows numrows numcols	0.333333
gets a field by name in a	get field name	0.333333
value of	ml param has num features	1.000000
vector columns	vector columns to ml	0.142857
summed over all trees in the ensemble	ensemble	0.100000
train a decision tree model for	mllib decision tree train regressor	0.333333
load a java	loader load java cls sc	0.200000
set initial centers should be set	set initial centers centers	0.200000
default min number of partitions for	spark context default min partitions	0.250000
the termination of	termination timeout	0.041667
:class dataframe as a temporary table in the	data frame as table df	0.333333
an external table	external table tablename	0.500000
values for each key using an	by key func	0.062500
for cross validator	ml cross validator	0.166667
comprised of vectors containing	random rdds exponential	0.125000
line in libsvm format into	mllib mlutils parse libsvm line line	0.111111
for the null	null	0.062500
curve which is a dataframe having	binary logistic regression summary	0.111111
queries so that	sql streaming query manager	0.011905
the vector	linalg vector	0.200000
the index of the original	with index	0.100000
squared distance from	vector squared distance	0.166667
create a new dstream in which each	streaming streaming	0.047619
set initial centers should be	mllib streaming kmeans set initial centers centers	0.200000
contains a param with a given string name	param params has	0.019231
wrapper for the model in jvm	model wrapper	1.000000
comprised of vectors containing i i d samples	mllib random rdds exponential vector	0.125000
with a dependency	dependency	0.120000
the given join expression	join	0.034483
to save	core cloud pickler save	0.166667
count of distinct elements in rdds in a	count	0.016949
java system	system	0.142857
generate	logistic regression with sgdtests generate logistic	1.000000
the output	writer bucket	0.500000
two fields threshold recall curve	binary logistic regression summary recall by threshold	0.166667
a function on each rdd of	transform func	0.058824
with a given string name	param params has	0.019231
of the accumulator's data	accumulator param	0.038462
the area under the	classification metrics area under	0.166667
params instances for the given param	params m1 m2	0.047619
predicted values on a toy model	mllib streaming logistic regression with sgdtests	0.200000
transforms a python parammap into a java	ml java params transfer param map to java	1.000000
pyspark storagelevel	storage level storagelevel	1.000000
this	ml param params has param	0.019231
__init__(self splits=none inputcol=none outputcol=none handleinvalid="error")	bucketizer init splits inputcol outputcol handleinvalid	1.000000
from	from ml	0.500000
value of	ml word2vec	0.285714
much of	merger	0.025641
py	py file	0.066667
terms to term frequency vectors or transform the	mllib hashing tf transform	0.045455
create an input stream that pulls	create stream	0.200000
separators inside brackets pairs	brackets split	0.083333
squared error which	mllib regression	0.022727
and profiles the	core	0.003021
produced	clustering	0.133333
columns in an input dataframe from the	columns	0.039216
vectors which	vector	0.019231
that :func awaitanytermination()	sql streaming query manager	0.011905
dump already partitioned	group by spill	0.047619
and count of	core rdd	0.003460
__init__(self	ml count vectorizer init	1.000000
wait until any	sql streaming query manager await any termination timeout	0.166667
instance contains a	param params has	0.019231
sample without replacement based on the	frame sample	0.066667
computes a [[pcamodel]] that contains the principal components	mllib pca fit data	0.166667
prefix of string in doc tests	prefix	0.083333
awaitanytermination() can be used	streaming query manager reset	0.011905
matrix columns in an input dataframe to	matrix columns	0.071429
mean variance	core	0.003021
optional default value and	param params	0.014925
merge the values for each key using	key func	0.066667
list of	ml chi	0.100000
external database table via jdbc	jdbc url table	0.090909
a param	params has	0.019231
n elements from an rdd	core rdd	0.003460
data sampled from a	data	0.011628
:func awaitanytermination() can	sql streaming query	0.011765
number of possible outcomes for k classes	model num classes	0.500000
all globals names read or written to by	extract code globals	0.125000
comprised of vectors containing i i d samples	random rdds gamma vector	0.125000
can	streaming	0.005025
internal use only create a new	create	0.017241
can be used again to wait for new	reset	0.011236
number of classes values which the label can	ml java classification model num classes	0.250000
a new spark configuration	spark conf init	0.250000
of this instance	ml one vs rest	0.052632
python topicandpartition to map to the java	and partition init topic partition	0.055556
new dstream by applying a function	f	0.010526
points belongs to	predict x	0.033898
to configure the kmeans algorithm for fitting	streaming kmeans	0.035714
for	mllib linear model	0.125000
the :class dataframe	sql data frame writer save	0.083333
pipeline	ml pipeline	0.142857
decision tree model	decision tree model	0.050000
used again to wait for	manager	0.011236
or cleared	description interruptoncancel	0.166667
a profile object is returned	profiler profile	0.200000
sql statements	returntype	0.071429
nodes in tree including	decision tree model	0.050000
boundaries	boundaries	0.714286
user and the second is an array of	mllib matrix	0.047619
with matching keys in c{self} and c{other}	join other	0.071429
computes column-wise summary statistics for	stat statistics col	0.200000
curve which	ml binary logistic regression summary	0.250000
for each key	key func	0.066667
write() save	pipeline model save	1.000000
parses the expression string into	expr str	0.125000
ensure	ensure	1.000000
squared error which	linear regression summary	0.013889
iterations default 1 which should be	iterations	0.043478
original	scaler model original	0.062500
:py attr lda keeplastcheckpoint is set to	ldamodel get	0.066667
rate for a	rate	0.090909
first n rows	data frame head n	1.000000
a dependency on another module on	module dependency on	0.142857
table	table column	0.166667
to wait for new terminations	manager reset	0.011905
string name	ml param params	0.013699
resulting rdd that contains a tuple with	core rdd cogroup other	0.066667
in "predictions"	summary	0.073171
losstype="logistic",	gbtclassifier	0.076923
local property that affects	local property key	0.035714
a column containing a json string into a	from json col	0.083333
number of months	months	0.125000
centers	centers centers weights	1.000000
left outer join	left outer join other	0.111111
of nonzero elements this	ml	0.001835
sets the accumulator's value only usable in	accumulator value	0.050000
wait for the execution to stop return	await termination or timeout timeout	0.125000
in the training set given	training	0.029412
gets	rformula get	1.000000
of the rdd partitioned using	rdd	0.003058
used with the spark sink	addresses storagelevel maxbatchsize	0.045455
the given parameters in this grid	ml param grid builder base	0.076923
returns true if	sql	0.002525
training	regression training summary	1.000000
rep	str pattern replacement	1.000000
parameters in this	grid builder add	0.200000
a function and attach docstring from func	function wrapped	0.333333
globals names read or written	extract code globals	0.125000
an input stream that pulls events from	stream	0.017544
value to a mllib	to	0.007692
much of memory	core external merger object	0.032258
this instance contains a param with a given	ml param params has	0.019231
depth of tree	tree model	0.026316
underlying :class sparkcontext	spark session spark context	1.000000
frequency vectors or transform the rdd of document	hashing tf transform document	0.166667
entry point to programming	session	0.050000
vectors saved	vectors	0.083333
fixed	param	0.006250
instance contains a param with a	params has	0.019231
class inherit documentation from	inherit doc	0.045455
standardscaler	standard scaler	0.076923
so that :func awaitanytermination() can	streaming query manager reset	0.011905
this instance contains a param	has param	0.019231
given data type string to a	sql	0.002525
param with a given	has	0.011628
compute the standard deviation of this	rdd stdev	0.066667
aggregate	aggregate	0.666667
onevsrest create	one vs rest from	0.142857
that has to be inherited by any streaminglinearalgorithm	streaming linear algorithm	0.076923
converts vector columns	convert vector columns from	0.166667
indices to select filter	selected features	0.333333
helper for setting the spark context call site	sccall site sync	0.200000
create an input stream that pulls	utils create stream ssc	0.200000
and other	other	0.033333
mixin for param thresholds thresholds in	has thresholds	0.250000
function and	defined function	0.066667
the levenshtein distance of the two	levenshtein left	0.058824
used again to wait for new	query manager reset	0.011905
set master url to connect to	conf set master value	1.000000
find norm of the given vector	mllib linalg vectors norm vector p	1.000000
train the model on	streaming kmeans train on	0.333333
of vectors which this	ml vector indexer model	0.250000
create a new sparkcontext at least	spark context init	0.083333
a left outer join of c{self}	left outer join other numpartitions	0.111111
c{self} that is not contained	rdd subtract other numpartitions	0.111111
convert this distributed model to	ml distributed ldamodel to	0.166667
starts at pos in byte and is of	pos	0.022222
current [[dataframe]] and perform the specified aggregation	grouped data pivot pivot_col values	0.050000
the given key	key	0.017857
new java	wrapper new java	0.166667
of nonzero	ml linalg dense	0.100000
all the	object size	0.032258
defined from start inclusive to end inclusive	between start end	0.125000
wrap this udf with a function	user defined function	0.066667
queries so that :func awaitanytermination() can	sql streaming query manager reset	0.011905
that :func awaitanytermination() can be used again to	query	0.010753
in the stream will start and stop	test case take dstream	0.250000
kafka topicandpartition	offset	0.021739
date truncated to the unit specified by	trunc date	0.500000
used again to wait for new	sql streaming query	0.011765
test for data sampled	test data	0.166667
create a converter to drop	create converter	0.166667
a local property set in this thread	spark context get local property key	0.066667
gets the	get	0.021739
of two vectors we support	ml linalg dense	0.100000
checkpointinterval=10 losstype="logistic",	gbtclassifier	0.076923
large dataset and an item	nearest neighbors dataset key numnearestneighbors	0.333333
removes the specified table from the in-memory cache	sql catalog uncache table tablename	0.250000
to wait	query manager reset	0.011905
setparams(self statement=none) sets params for this sqltransformer	ml sqltransformer set params statement	1.000000
:class rdd, a list or a	schema samplingratio verifyschema	0.029412
already partitioned data	core external group	0.045455
occurrence of substr	substr str	0.125000
compute the	mllib linalg row matrix	0.250000
to	sql streaming	0.010204
of predicted ratings for input user and	mllib matrix factorization model predict all user_product	0.050000
only create a new	sql hive context create	0.083333
a dataframe with two fields threshold recall curve	binary logistic regression summary recall by threshold	0.166667
wait for the execution	await termination timeout	0.166667
of the rdd's elements	rdd	0.003058
queue	streaming streaming context queue	0.500000
paired rdd where the first element	matrix factorization	0.040000
squared distance from a	squared distance	0.142857
a sql datum into a user-type object	user defined type deserialize datum	0.333333
tree	mllib decision tree	0.166667
of rows of	row	0.062500
how much of	object size	0.032258
distance from a sparsevector or 1-dimensional numpy array	distance	0.095238
the :class dataframe in	data frame writer	0.028169
local property set in this thread or	spark context get local property key	0.066667
an rdd comprised	mllib random rdds normal vector rdd	0.166667
system	system	0.714286
python topicandpartition to map to	partition init topic partition	0.055556
for each key	key func numpartitions partitionfunc	0.066667
instance contains a param with a	params	0.006623
dump already partitioned	by	0.014286
broker to map to the	broker	0.100000
provides methods to set k decayfactor timeunit to	streaming	0.005025
a configuration property if not already set	core spark conf set if missing key	0.500000
an upper triangular matrix r in a qr	mllib linalg qrdecomposition r	0.333333
return a new dstream by applying reducebykey to	streaming dstream reduce by	0.076923
one	rdd	0.003058
leaders	leaders	1.000000
setparams(self	quantile discretizer set params	1.000000
the underlying output	stream writer	0.041667
until any of the queries on the associated	query manager await any	0.142857
java storagelevel based	java	0.012195
the current [[dataframe]] and perform the specified aggregation	grouped data pivot pivot_col values	0.050000
params mixed with hasmaxiter hasinputcol and hasseed	other test params	0.500000
ensuring all received	stopsparkcontext stopgracefully	0.050000
how much of memory	core external merger object size	0.032258
setparams(self minsupport=0 3 minconfidence=0 8 itemscol="items", predictioncol="prediction", numpartitions=none)	set params minsupport minconfidence itemscol predictioncol	1.000000
function and attach docstring from func	function wrapped	0.333333
restore an object	core restore name fields	0.333333
return a new dstream by applying reducebykey	streaming dstream reduce by	0.076923
ignore separators inside	ignore	0.100000
note : experimental	binary logistic regression summary	0.111111
for new	sql streaming query manager	0.011905
params	params copy	0.083333
:class dataframe representing	sql data frame reader	0.111111
is generated by applying mappartitionswithindex()	map partitions with index f preservespartitioning	0.055556
greatest value	greatest	0.043478
a	cls	0.047619
convert matrix	linalg matrix convert	0.166667
a numpy ndarray	linalg matrix to array	0.142857
ordered in ascending order or as specified by	take ordered	0.125000
tests whether this instance contains	paramname	0.076923
labeled points saved	labeled points sc path minpartitions	0.250000
specified table or	sqlcontext table tablename	0.142857
mean variance and count of	core	0.003021
dictionary	size	0.036697
the dot product of two vectors we	vector dot	0.050000
an external list	external list	0.166667
the	model	0.005587
the	scaler model	0.076923
representation of row used in python repl	sql row repr	0.250000
log likelihood	log likelihood dataset	0.142857
instance's params to the wrapped java object	ml java params to	0.045455
generates an rdd comprised of vectors containing	mllib random rdds poisson vector rdd sc mean	0.200000
by applying c{f}	by f	1.000000
of the :class dataframe to the	data frame	0.005000
of two columns of a dataframe as	sql data frame	0.005348
train a decision tree model for regression	mllib decision tree train regressor cls	0.333333
a batch has completed	completed outputoperationcompleted	0.125000
a py or zip dependency	py file path	0.066667
singular	mllib linalg singular	0.052632
sets the given parameters in this grid	grid builder	0.055556
an external database table via jdbc	jdbc url table mode	0.200000
with a given string name	has param	0.019231
which is defined	linear regression summary	0.013889
which is a dataframe having	logistic regression	0.040000
generates an rdd comprised of vectors containing i	mllib random rdds exponential vector rdd sc mean	0.200000
or compute	linalg distributed matrix	0.333333
produced	ml clustering	0.100000
applies standardization transformation on	standard scaler model transform	0.500000
each dstreams in this context	streaming streaming context	0.032258
set initial centers should be	set initial centers centers	0.200000
be used again to	sql streaming query manager reset	0.011905
least value of the	least	0.043478
column	standard	0.071429
adds output	frame writer	0.050000
content of the dataframe in	sql data frame	0.005348
exponential	exponential	1.000000
called when processing of a	listener on	0.200000
new :class column for distinct count	count distinct	0.040000
forget about past terminated queries so	streaming query manager reset terminated	0.200000
right outer join	rdd full outer join other numpartitions	0.111111
precision of	precision	0.076923
of two columns of a dataframe as	sql data frame corr col1 col2	0.166667
broadcast a read-only	broadcast value	0.125000
spark session to use for loading	ml mlreader session sparksession	0.333333
an external database table	table	0.031250
singular	linalg singular	0.052632
provided buckets the buckets	buckets	0.111111
for binary operator this	op name doc	0.166667
all globals names read or written to	cloud pickler extract code globals	0.125000
can be used	manager	0.011236
the :class statcounter members as a	core stat counter as	0.333333
parameters passed as a list	spark conf	0.058824
given a large dataset and an item	nearest neighbors dataset key	0.333333
stratified sample	sample	0.050000
field in "predictions" which gives the predicted	linear regression summary prediction	0.142857
a model	streaming linear regression	0.333333
predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false	predictioncol maxdepth	0.250000
convert matrix attributes which are array-like or	mllib linalg matrix convert	0.166667
so that	streaming query manager	0.011236
be	streaming query manager	0.011236
a multi-dimensional rollup for the current :class	sql data frame rollup	0.055556
sets the accumulator's value only usable in	accumulator	0.012987
a model to the input dataset this	dataset	0.020408
setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online",	set params featurescol maxiter seed	0.250000
only create a new hivecontext for testing	create for testing cls	0.333333
sampled subset of this :class dataframe	data frame sample withreplacement fraction seed	1.000000
a left outer join of c{self}	rdd left outer join other	0.111111
precision of all the queries	precision	0.076923
dump already partitioned data into disks	core external group	0.045455
type for	type	0.024390
sets	kmeans set	0.363636
checkpointinterval=10 losstype="logistic",	ml gbtclassifier	0.095238
set of expressions and returns	sql	0.002525
partial objects do not serialize correctly in python2	partial obj	0.125000
profiling on the	profiler	0.090909
generates an rdd comprised of vectors containing i	mllib random rdds normal vector rdd sc	0.200000
be	manager	0.011236
the latest	mllib streaming kmeans latest	0.500000
return an	core	0.006042
to set k decayfactor timeunit to configure	streaming	0.005025
right outer join	full outer join other	0.111111
itemscol items	items	0.066667
memory	external merger object size	0.032258
keys in c{self} and c{other}	join other	0.071429
approximate distinct count of col	approx count distinct col rsd	0.066667
rowmatrix stored	linalg row matrix	0.200000
deserialized batches lists of	without unbatching	0.125000
mean	mean	0.413793
a large dataset and an item approximately find	lshmodel approx nearest neighbors dataset key	0.166667
test predicted values on a toy model	mllib streaming logistic regression with sgdtests test	0.111111
for new	reset	0.011236
month of a given date as integer	sql dayofmonth	0.031250
kolmogorov-smirnov ks test	stat statistics kolmogorov smirnov test	0.166667
sparkcontext which is associated	spark	0.013158
of the month of	dayofmonth	0.027027
setparams(self estimator=none estimatorparammaps=none evaluator=none	set params estimator estimatorparammaps evaluator	1.000000
java parammap into a	java javaparammap	0.125000
creates or replaces a	create or replace	0.500000
collect each rdds into the returned list	py spark streaming test case collect dstream n	1.000000
distance from a sparsevector or 1-dimensional	distance other	0.133333
to the same time of	to	0.007692
training	training summary	1.000000
a param with a	param	0.012500
labels corresponding to indices to	indexer model labels	0.166667
for which predictions are	regression model	0.031250
rdd	rdd rdd	1.000000
month of a given	dayofmonth col	0.031250
to the same	to	0.007692
of this instance with the	ml	0.003670
sets	gbtclassifier set	1.000000
the name of the file	file	0.028571
broadcast	spark context broadcast value	0.125000
values alias for na fill()	data frame fillna value subset	0.166667
output a python rdd of key-value	rdd save	0.038462
the norm of a	mllib linalg sparse vector norm	0.083333
the rdd partitioned using	rdd partition	0.062500
set number of batches	set	0.005917
a single script on a	single script on	0.250000
true positive rate	true positive rate	0.400000
the profile stats to stdout	profiler show	0.166667
:class sparkstageinfo object or none if the stage	stage	0.062500
the given data type	sql parse datatype	0.333333
depth of tree	tree	0.020833
queries so that :func awaitanytermination() can be used	sql streaming query manager reset	0.011905
content of the :class dataframe in	sql data frame writer	0.034884
root directory that contains files added through c{sparkcontext	core spark files get root directory cls	1.000000
the levenshtein distance of	levenshtein left	0.058824
new profiler using	new profiler	0.333333
the right singular vectors of the	singular	0.015625
saves the content of the dataframe	sql data frame writer	0.011628
java storagelevel based on a pyspark storagelevel	core spark context get java storage level storagelevel	0.500000
converts vector columns in an input	mlutils convert vector columns from ml dataset	0.166667
k=2 probabilitycol="probability", tol=0 01 maxiter=100 seed=none)	k probabilitycol	0.333333
compute the dot product of two	dot other	0.050000
intercept computed for	intercept	0.090909
instance to a java pipelinemodel used for ml	ml pipeline model to java	0.100000
decorator that makes a class inherit documentation	mllib inherit	0.045455
test a single script file calling	tests test script with local functions	0.333333
a range of offsets from a single	offset range	0.047619
the selector	selector	0.100000
given value to	sql	0.002525
for each numeric columns	sql grouped data avg	0.058824
tokens in the training	distributed ldamodel training	0.034483
train a gaussian mixture clustering	mllib gaussian mixture train cls rdd k convergencetol	0.500000
int containing elements from start to end exclusive	core spark context range start end	0.166667
:param rdd an rdd	iteration clustering train cls rdd	0.250000
attr lda keeplastcheckpoint is	ldamodel	0.034483
calculates the correlation of two	method	0.041667
number of rows	matrix num rows	0.200000
matrix attributes which are array-like	matrix	0.030303
converts vector columns in	convert vector columns	0.166667
the absolute path of a file added through	core spark files	0.125000
a right outer join of c{self}	rdd full outer join other	0.111111
an rdd of labeledpoint	mlutils load lib svmfile sc path numfeatures	0.125000
sets mincount the minimum number of	set min count mincount	0.250000
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	ml decision tree regressor	0.066667
external table based on	external table tablename path	0.090909
python parammap into a	to	0.007692
default implementation of fit	ml estimator fit	0.083333
converts vector columns in an input dataframe	mlutils convert vector columns to	0.166667
the length of a string or binary expression	sql length	0.050000
the column mean	mean	0.034483
that :func awaitanytermination() can be	query manager reset	0.011905
load labeled points	mllib mlutils load labeled points sc	1.000000
until any of the queries on the	manager await any termination	0.142857
computes column-wise summary statistics for the	mllib stat statistics	0.125000
content of the :class dataframe as the specified	data frame writer save as	0.071429
numfeatures number of features	num features	0.333333
dataset for the test this should be	test	0.015152
the norm of	vector norm p	0.055556
of predicted ratings for input user and product	matrix factorization model predict all user_product	0.050000
contains a param with a given	param params has param	0.019231
modlist to be placed into main	modules to main modlist	0.333333
format into an rdd of labeledpoint	lib svmfile sc path numfeatures	0.125000
and update the catalog	catalog	0.062500
computes the levenshtein distance of the two given	sql levenshtein left	0.058824
the first n rows to the console	frame show n truncate vertical	0.333333
that makes a class inherit documentation from	mllib inherit doc cls	0.045455
test that the final value of weights	streaming logistic regression with sgdtests test	0.111111
:class dataframe in parquet	sql data frame writer parquet	0.200000
number of clusters	mllib power iteration clustering model k	0.200000
in mixture	mixture model k	0.200000
python topicandpartition to map to the java	partition init topic partition	0.055556
given columns	sql data frame	0.010695
curve which is a dataframe having	ml binary logistic regression	0.142857
or compute the number of cols	mllib linalg indexed row matrix num cols	0.333333
true positive rate for	true positive rate	0.200000
dstream	kafka dstream	0.500000
until any of the queries on the associated	manager await any	0.142857
ngram	ngram	1.000000
list of indices	ml chi sq	0.100000
param with a	param	0.012500
a text	text	0.230769
by applying mappartitionswithindex() to each	map partitions with index f preservespartitioning	0.055556
the underlying output	sql data frame writer	0.011628
this udf with a function	sql user defined function	0.083333
vector size default 100	vector size vectorsize	1.000000
internal use only create a new	context create	0.083333
return an rdd created	core rdd	0.003460
field in "predictions" which gives the	logistic regression summary	0.090909
of [[structtype]]s with the specified schema	schema options	0.125000
matrix columns in	matrix columns from ml dataset	0.142857
sets	ml one hot encoder set	1.000000
add two values of the	add	0.035714
this obj assume	merger object size obj	0.040000
database table	table mode	0.200000
original column during fitting	ml min max scaler model original	0.062500
ignore the	core ignore	0.500000
convert this vector	linalg vector	0.200000
inputformat with arbitrary key and value class	inputformatclass keyclass valueclass	0.125000
a python topicandpartition to map	partition init topic partition	0.055556
string	string s	0.333333
feature	model	0.005587
restore an object of namedtuple	core restore name	0.333333
gaussian distributions as a	ml gaussian mixture model gaussians df	0.166667
the column mean	mllib standard scaler model mean	0.125000
of points using the model trained	mllib tree ensemble model	0.058824
a pearson's independence test using	ml chi square test test	0.333333
add a	core spark context add	0.500000
and	core spark	0.020619
stats to stdout id is the rdd id	profiler show id	0.333333
that all	core external merger object	0.032258
value of	ml has items	1.000000
calculates a lower bound on the log	log	0.071429
objects	object size	0.032258
partitioned	external	0.013889
centers represented as a list of numpy arrays	centers	0.100000
data into disks	core	0.003021
model fitted by :class isotonicregression	isotonic regression model	0.100000
specified by the param	param	0.006250
a list of	ml bisecting kmeans model	0.076923
from checkpoint data or create	get or create	0.111111
map or	or	0.142857
path a shortcut of write() save path	ml pipeline save path	0.200000
kolmogorov-smirnov ks test for data sampled	stat statistics kolmogorov smirnov test data distname	0.111111
a py or zip dependency	py	0.050000
month of a given	sql dayofmonth	0.031250
resolve	resolve	1.000000
based on the	path	0.010204
sum for each numeric columns for each	grouped data sum	0.083333
which	regression metrics	0.083333
logistic regression model on	logistic regression with	0.250000
the objects	object	0.027778
prints the first n rows to the console	sql data frame show n truncate	0.333333
this instance	one	0.117647
in disks	core external	0.016129
for the stream	stream	0.017544
ensure that daemon and workers terminate on	core daemon tests test termination	0.166667
a given	param params has param	0.019231
__init__(self featurescol="features", labelcol="label",	tree classifier init featurescol labelcol	1.000000
a python rdd of key-value	core rdd	0.010381
sets	binarizer set	1.000000
sets the threshold	set threshold	0.500000
be used again to wait for new terminations	query	0.010753
number of times	count	0.016949
offsets from a single kafka	offset	0.021739
__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10	ml lda init featurescol maxiter seed	0.250000
the objects	external	0.013889
of int containing elements	core spark	0.010309
new	java wrapper new	0.333333
or transform the	mllib hashing tf transform	0.045455
:py attr lda keeplastcheckpoint is set to	distributed ldamodel get	0.066667
which predictions are known	regression	0.010000
words closest in similarity to	word2vec	0.052632
load a model from the	mllib java loader load cls sc	0.250000
creates an external table based on the	create external table tablename path	0.250000
specific topic and partition for	topic and partition	0.111111
perform a left outer join of c{self} and	core rdd left outer join other numpartitions	0.200000
outputformat api	outputformatclass	0.111111
rdd contains all the elements in seen in	windowduration slideduration	0.083333
already	core external group by spill	0.047619
number	java classification model num	1.000000
of a streaming dataframe/dataset is written to a	writer output mode outputmode	0.083333
__init__(self featurescol="features",	aftsurvival regression init featurescol	1.000000
configuration	spark conf	0.058824
this matrix to the new mllib-local representation	dense matrix as	0.333333
sets	ml index to string set	1.000000
regression model on the	regression with	0.200000
property that affects jobs submitted from this thread	property key value	0.125000
of terms to term frequency vectors or transform	mllib hashing tf transform	0.045455
applies standardization transformation on a	mllib standard scaler model transform	0.500000
all mixture components	gaussian mixture model predict soft x	0.142857
output a python rdd of key-value pairs (of	rdd	0.009174
rows of	row	0.062500
of vectors	ml vector	0.200000
get a local property set in this	local property key	0.035714
dump	core external group by	0.045455
dataframe produced by the	clustering	0.066667
column scipy matrix from a dictionary of	mllib sci py tests scipy matrix	0.090909
a local property that affects jobs	local property key	0.035714
param and assert both have the same param	m2 param	0.125000
model fitted by :class randomforestregressor	random forest regression model	1.000000
columns for the	columns	0.019608
sets random	word2vec set	0.142857
values from this instance to another instance for	copy values to extra	0.333333
list of pairs split the list of values	values	0.050000
of clusters	mllib power iteration clustering model k	0.200000
for each key	by key func	0.062500
create a	sql hive context create	0.083333
sets the given parameters in	builder	0.090909
as a temporary table in the catalog	as table	0.200000
text format or newline-delimited json	json path	0.100000
sets the spark	sparksession	0.125000
rdds of the dstreams	dstreams transformfunc	0.125000
an rdd previously saved using l{rdd saveaspicklefile} method	core spark context pickle file name minpartitions	0.250000
add a py or zip	core spark context add py file	0.166667
the count of distinct elements in	count	0.016949
saves the contents	path format mode partitionby	0.200000
:class pyspark sql types longtype column	sql sqlcontext range start end step numpartitions	0.083333
commutative reduce function but	reduce	0.041667
another	other	0.200000
hadoop configuration which is passed in as a	hadoop	0.050000
the log likelihood of	ml ldamodel log likelihood	0.166667
catch_assertions	catch_assertions	0.833333
buckets	buckets	0.666667
in c{self} that is not contained	core rdd subtract other numpartitions	0.111111
raw	raw	0.833333
function to the value of	f	0.021053
internal use only create a	hive context create	0.083333
least value of the list of column names	sql least	0.055556
feature	mllib linear	0.166667
with a given string name	param params	0.014925
estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none)	estimator estimatorparammaps evaluator numfolds	0.200000
second is an array	mllib	0.010526
comprised	mllib random rdds log normal	0.125000
any additional metadata default none	metadata	0.500000
of blocks	blocks	0.153846
as a temporary table in the	as table	0.200000
2 ml params instances for the given	params m1	0.047619
interface for saving the content of the non-streaming	write	0.071429
an rdd created by piping elements to a	rdd	0.003058
2 ml params instances for the given	params m1 m2	0.047619
that makes a class inherit documentation from	inherit	0.037037
and the second is an array	mllib matrix	0.047619
mixture	gaussian mixture model	0.105263
distributions as a dataframe	mixture model gaussians df	0.333333
of	matrix	0.015152
into	mlutils parse	0.250000
replacing	replace to_replace	0.200000
a left outer	rdd left outer	0.333333
python direct kafka stream transform get	kafka direct stream transform get	0.500000
the sparsevector	sparse vector	0.062500
'x' has maximum membership in this model	gaussian mixture model predict x	0.500000
dataframe in a	sql data frame	0.005348
the accumulator's value only usable in	accumulator	0.012987
norm	linalg dense vector norm p	0.333333
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)	tree regressor	0.058824
:py attr aggregationdepth	aggregation depth value	1.000000
train a gradient-boosted trees model for regression	mllib gradient boosted trees train regressor cls data	0.333333
the cluster centers represented as a list	mllib bisecting kmeans model cluster centers	0.083333
format at	compression	0.142857
for each key using	key func numpartitions partitionfunc	0.066667
dump already partitioned	group	0.025641
copy all params defined on the	param params copy params	0.200000
will convert each python object into java	mllib to java	0.333333
dayofmonth	dayofmonth	0.135135
transform the rdd of document to rdd of	transform document	0.250000
object is on right side	sql reverse	1.000000
of model	tree ensemble model	0.038462
summary of	repr	0.222222
with the frame boundaries defined	range between	0.166667
same param	m2 param	0.125000
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic", maxiter=20	gbtclassifier	0.076923
initialized or not	initialized	0.125000
left	left	0.400000
instance	param params has	0.019231
maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic",	gbtclassifier	0.076923
computes column-wise summary statistics for the input rdd[vector]	statistics	0.090909
the number of	num	0.033613
awaitanytermination() can be used again	sql streaming query manager	0.011905
compute the number	linalg indexed row matrix num	0.100000
of the rdd's elements in one operation	core	0.003021
events from flume	flume utils	0.200000
stringindexer	ml string indexer	0.166667
wait for new	sql streaming query	0.011765
a given string	param params	0.014925
out of a list or gets an	column get	0.200000
objective function scaled loss + regularization at	regression training summary objective history	0.500000
two fields threshold	threshold	0.036364
an upper triangular matrix r in	linalg qrdecomposition r	0.333333
the selector type of	mllib chi sq selector set selector type	0.111111
dictionary a list of index	size	0.036697
conversion happens	conversion tests	1.000000
serializes a	serializer	0.062500
__init__(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor",	aftsurvival regression init featurescol labelcol predictioncol	0.500000
test the	tests test	0.148148
sets vector	mllib word2vec set vector	1.000000
partial objects do not serialize correctly in python2	pickler save partial	0.125000
a param with a given string name	params has param	0.019231
to the wrapped java object and return the	to	0.007692
a list of :class	data frame	0.005000
using an associative and commutative reduce function	reduce by	0.200000
list of column names skipping null	sql	0.005051
drawn	shape scale numrows	0.125000
value of	ml gaussian	0.333333
so that :func awaitanytermination()	sql	0.002525
column	standard scaler	0.076923
over all trees in the ensemble	tree ensemble	0.111111
setparams(self withmean=false withstd=true inputcol=none outputcol=none) sets params for	set params withmean withstd inputcol outputcol	0.500000
summed over all trees in the ensemble	ensemble model	0.117647
length of a string	sql length	0.050000
partitioned data into disks	group	0.025641
recovers all	recover	0.142857
to persist its values across operations after	rdd persist storagelevel	0.166667
the given path a shortcut of write() save	ml one vs rest save	0.166667
given parameters in this grid	grid builder base on	0.076923
this model instance	ml kmeans model	0.500000
dataframe in	data frame writer	0.014085
variance and	rdd	0.003058
multilabel classification	multilabel	0.166667
set a local property that affects jobs submitted	set local property key	0.200000
gets frequent sequences	mllib prefix span model freq sequences	1.000000
specified table or view as a	sqlcontext table tablename	0.142857
for each original	model original	0.062500
regression score	ml linear regression summary	0.142857
to approximately	lshmodel approx	0.100000
params instances for	params m1 m2	0.047619
value of :py attr	ml lda	0.642857
a dictionary a list of index	init size	0.066667
to set k decayfactor timeunit to	streaming	0.005025
get the cluster centers represented as	bisecting kmeans model cluster centers	0.095238
of the month of a given	sql dayofmonth col	0.031250
matrix this	matrix	0.015152
save this model to the given path	bayes model save sc path	1.000000
the levenshtein distance of the two given	levenshtein left right	0.058824
creates	sqlcontext create	0.500000
partitioned data	by spill	0.047619
this model	gaussian mixture model	0.052632
inputformat	inputformatclass	0.095238
a resulting rdd that contains	rdd cogroup other numpartitions	0.066667
left outer join of c{self} and c{other}	core rdd left outer join other	0.200000
it will convert each python object into java	mllib to java	0.333333
string name	param	0.012500
line in libsvm format into	mllib mlutils parse libsvm line line multiclass	0.111111
list of tables/views	list tables	0.250000
convert this vector to the new mllib-local representation	mllib linalg vector as ml	1.000000
choose one directory for spill	external sorter get path	0.333333
a map of words to their vector representations	word2vec model get vectors	0.166667
saved using rdd[vector] saveastextfile	mllib mlutils	0.333333
values for each key using	key func numpartitions partitionfunc	0.066667
in the training set given the current	ldamodel training	0.034483
dot product of two	dot	0.040000
with a randomly	cross validator	0.045455
returns accuracy equals to the total number of	multiclass metrics accuracy	0.166667
seed	seed seed	1.000000
based on	context get	0.333333
in which each rdd contains	by	0.014286
product and returns a list of rating	product	0.029412
given	has param	0.019231
dataframe in a	data frame writer	0.014085
much	merger object	0.032258
dt	dt	1.000000
minimum item	min key	0.333333
slicer	slicer	1.000000
a left outer join of c{self} and c{other}	core rdd left outer join other numpartitions	0.200000
null model	generalized linear regression summary null	0.250000
gets	get	0.195652
calculates the norm of	vector norm	0.055556
root mean squared error which is defined	regression	0.010000
the elements in iterator	iterator key	0.200000
for this imputer	ml imputer	0.250000
parses the expression string into the	sql expr str	0.125000
convert a list	to	0.007692
add a py or zip dependency	core spark context add py file path	0.166667
n rows	n truncate vertical	0.250000
globals names read or written to by codeblock	globals cls	1.000000
new dstream by applying a function to	map f	0.037037
join of c{self}	join other	0.142857
active queries associated with this sqlcontext >>> sq	manager active	0.066667
column of the current [[dataframe]] and perform	data pivot pivot_col values	0.050000
in one operation	rdd	0.003058
from this block matrix this	linalg block matrix	0.052632
vector columns in an input dataframe	vector columns	0.142857
transforms a python parammap	ml java params transfer param map	0.250000
awaitanytermination() can be used again	streaming	0.005025
database	dbname	0.181818
return a new dstream by applying a function	f	0.010526
the minutes of	sql minute col	0.050000
timeunit to	streaming	0.005025
this vector	sparse vector	0.062500
the rdd	rdd	0.009174
compare 2 ml	compare	0.142857
function	func sc func	1.000000
batchcompleted	batchcompleted	1.000000
property	defaultvalue	0.166667
this is called by the default implementation of	ml estimator	0.125000
create a python topicandpartition to map	topic partition	0.055556
rdd for dataframe from a	from	0.045455
set	core spark conf set	0.100000
steps	steps	1.000000
cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20	ml random forest classifier	0.023256
boolean data type	boolean type	1.000000
an input stream	stream ssc hostname	0.200000
its elements in a numpy ndarray	linalg matrix to array	0.142857
variance and count	core	0.003021
test that coefs are predicted accurately	streaming linear regression with tests test parameter accuracy	0.333333
with singular	singular	0.015625
adds two block matrices together the matrices	linalg block	0.076923
the	core external	0.016129
each key using an associative and	by key	0.026316
len	len	0.500000
much of memory	merger	0.025641
variance and count of the	core rdd	0.003460
amount of time for a condition to	condition	0.045455
called by the default implementation of	ml	0.001835
dstream in which each rdd contains	by	0.014286
test that	test training and	0.500000
be used again to wait for new terminations	streaming query manager reset	0.011905
the minimum number of times	min count	0.076923
a python rdd of key-value pairs (of form	core rdd save as	0.037500
as pandas pandas	pandas	0.090909
the levenshtein distance of the two	sql levenshtein left	0.058824
set initial centers should	initial centers centers weights	0.200000
the n elements from an rdd ordered in	rdd take ordered	0.050000
a column scipy matrix from a	sci py tests scipy matrix size	0.090909
new hivecontext for testing	for testing cls	0.333333
__init__(self splits=none	bucketizer init splits	1.000000
on a	logistic regression with	0.250000
every feature	linear model	0.066667
that separates positive predictions from negative predictions	mllib linear classification model	0.142857
external storage systems (e	reader	0.040000
returns recall or recall for a given label	metrics recall label	1.000000
the dataframe in a text	sql data frame writer text	0.200000
is defined as the square root of	root	0.035714
this instance contains a	param params has param	0.019231
rdd's elements	rdd	0.009174
contains a param with a	param params has param	0.019231
matrix columns in	matrix columns to ml	0.142857
for approximate distinct count of	approx count distinct col rsd	0.066667
finding frequent items for columns possibly with false	sql data frame freq items cols	0.166667
add a py or zip dependency for all	core spark context add py	0.166667
this vector to	linalg sparse vector	0.111111
sets	variance col set	1.000000
later than the value	dayofweek	0.037037
soundex encoding for a string >>>	soundex col	0.055556
converts matrix columns in an	convert matrix columns from ml dataset	0.166667
new spark configuration	core spark conf init loaddefaults	0.250000
columns for the given table/view in the	columns	0.019608
train a gradient-boosted trees model	mllib gradient boosted trees train classifier cls	0.333333
to	sql streaming query	0.011765
of a dataframe	data frame corr col1 col2	0.166667
a term to	accumulator add term	0.066667
for this obj assume that all	obj	0.023810
train a gradient-boosted trees model	mllib gradient boosted trees train classifier cls data	0.333333
sort the	streaming py spark streaming test case sort result	0.333333
expression representing a user defined function udf	udf f returntype	0.200000
until any of the queries	streaming query manager await any	0.142857
sparse matrix stored in	sparse matrix	0.250000
trainratio	train ratio	1.000000
a jvm seq of	seq sc cols	0.055556
the initial value of weights	streaming logistic regression with sgd set initial weights	0.333333
the standard deviation of	stdev	0.047619
compare 2	persistence test compare	0.166667
create a new hivecontext for testing	sql hive context create for testing	0.333333
the md5 digest and	md5	0.125000
the rdd partitioned using the	rdd	0.003058
ssql	ssql	1.000000
that is	coalesce	0.142857
load a java model from the given	mllib java loader load java cls sc	0.200000
merge the values for each key using an	by key	0.026316
problem in multinomial logistic regression	mllib logistic regression model	0.083333
that all the objects	core external merger object	0.032258
load labeled points saved using rdd	mllib mlutils load labeled points sc path minpartitions	0.250000
square root of the mean squared error	root mean squared error	0.500000
the += operator adds a term to	core accumulator iadd term	0.142857
with another value	value	0.008547
table in	table df	0.083333
a paired rdd where the first element	factorization	0.038462
uid of this instance	ml param params reset uid newuid	0.058824
representation of each word in vocabulary	word2vec fit	0.200000
param	param params	0.014925
much of	core external merger object	0.032258
convert this matrix to an indexedrowmatrix	coordinate matrix to indexed row matrix	0.333333
queries so that :func awaitanytermination() can be	sql streaming	0.010204
is vector conduct pearson's chi-squared goodness of fit	stat statistics chi sq	0.066667
this instance is of type distributedldamodel	ml ldamodel is distributed	0.066667
calculates the norm	dense vector norm p	0.333333
given parameters in this grid to	grid builder	0.055556
aggregates	aggregate zerovalue seqop combop	1.000000
k decayfactor timeunit to configure	streaming	0.005025
the underlying output data source	stream writer format source	0.333333
mincount the minimum number	min count mincount	0.200000
version of a heappush followed	heap	0.047619
calculates the correlation of two	col1 col2 method	0.055556
__init__(self featurescol="features", predictioncol="prediction", maxiter=20 seed=none k=4 mindivisibleclustersize=1 0)	init featurescol predictioncol maxiter seed	1.000000
rformula	ml rformula	0.250000
value of	ml param has output col	1.000000
param with a given	param params	0.014925
get a local	local	0.038462
of trees in	trees	0.066667
function	defined function	0.066667
none if the stage	stage	0.062500
merge the values for each key using an	by key func	0.062500
given path a shortcut of write() save	ml mlwritable save	0.166667
parses a column containing a json string into	json col	0.083333
model from the input	estimator	0.083333
load labeled points saved using	load labeled points sc path minpartitions	0.250000
compute the standard deviation of this rdd's elements	rdd stdev	0.066667
sets the given parameters in this grid to	ml param grid builder base on	0.076923
is called by the default implementation of	ml estimator	0.125000
partitioned data into	group by	0.041667
matrix columns	matrix columns from ml dataset	0.142857
comprised of vectors containing i i d	random rdds normal vector	0.125000
cogroup	cogroup	0.833333
stop the execution of the streams with	streaming streaming context stop	0.125000
__init__(self	ml decision tree classifier init	1.000000
saves	writer	0.040000
to a different value or cleared	description interruptoncancel	0.166667
called when a receiver has been started	streaming streaming listener on receiver started receiverstarted	1.000000
new hadoop outputformat api mapreduce	as new apihadoop dataset conf keyconverter valueconverter	0.142857
iterator of deserialized batches lists	without unbatching	0.125000
test for data sampled from a	test data distname	0.166667
a new accumulator	accumulator init	0.083333
path a shortcut of write()	ml pipeline model	0.066667
this thread such as the spark	spark	0.013158
an input stream	stream	0.035088
the standard deviation of this	stdev	0.047619
get the n	num	0.008403
(e g depth 0 means 1 leaf	mllib decision	0.125000
sqlcontext	sqlcontext	0.461538
value of	ml min	1.000000
paired	matrix factorization model	0.043478
that all	core external merger object size	0.032258
of products for all users	products for users	0.250000
add a py or zip	context add py file path	0.166667
class inherit documentation from	mllib inherit	0.045455
model fitted by decisiontreeclassifier	decision tree classification model	1.000000
output a python rdd of key-value pairs (of	core rdd save as	0.037500
correlation of two columns of a	method	0.041667
for approximate distinct count of col	approx count distinct col	0.071429
computes column-wise summary statistics	stat statistics col stats rdd	0.200000
returns	sql sqlcontext	0.047619
all globals names read or written	cloud pickler extract code globals	0.125000
tests whether this instance	param paramname	0.111111
a new dstream in which each	streaming streaming context	0.032258
fp-growth model that contains frequent	fpgrowth train cls data minsupport	0.200000
predictioncol="prediction", k=2 initmode="k-means||", initsteps=2 tol=1e-4 maxiter=20 seed=none)	predictioncol k initmode	1.000000
an	train cls	0.500000
squared distance from a sparsevector or 1-dimensional numpy	linalg sparse vector squared distance	0.166667
key-value	map values	0.166667
wait for the	context await termination or timeout timeout	0.125000
the day of the month	dayofmonth	0.027027
this	merger object	0.032258
probability of obtaining a test	test	0.015152
numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1	ml random forest	0.071429
to all mixture components	mixture model predict soft x	0.142857
use only create a	create	0.017241
a sliding window	window	0.037037
can be used in sql statements	returntype	0.071429
comprised of vectors containing i i d	random rdds normal	0.125000
return a callsite	core	0.003021
streaming dataframe/dataset is written to a streaming sink	writer output mode outputmode	0.083333
squared distance from a sparsevector or 1-dimensional	mllib linalg sparse vector squared distance	0.166667
fast version of a heappush followed by a	heappushpop heap	0.142857
has to be inherited by any streaminglinearalgorithm	streaming linear algorithm	0.076923
model	bayes model	1.000000
sets	size set	1.000000
vector columns in an	vector columns to	0.142857
data into disks	group by spill	0.047619
pipelinemodel create and return a	pipeline model	0.071429
streamingcontext from checkpoint data or create	streaming streaming context get or create	0.200000
the :class dataframe to a data	sql data frame writer save path format	0.142857
term frequency tf vectors to tf-idf vectors	mllib idfmodel transform	0.142857
values of the accumulator's	accumulator	0.012987
a left outer join of	rdd left outer join other	0.111111
transforms the input dataset	ml transformer transform dataset	1.000000
containing i i d samples drawn	shape scale numrows	0.125000
contains a param	ml param	0.009524
with the dispatch to handle all	core cloud pickler save	0.166667
saves	format mode partitionby	0.200000
matrix to a coordinatematrix	mllib linalg indexed row matrix to coordinate matrix	0.333333
each value in c{self} that	other numpartitions	0.083333
returns the soundex encoding for a	sql soundex col	0.055556
a new dstream by applying reducebykey	streaming dstream reduce by key func	0.076923
a hexadecimal number	sql unhex col	0.142857
setparams(self splits=none inputcol=none outputcol=none handleinvalid="error") sets params	set params splits inputcol outputcol handleinvalid	0.500000
applies transformation on a	mllib vector transformer transform	0.500000
compute the dot product	mllib linalg dense vector dot other	0.058824
length of a string or binary expression	length col	0.050000
loads a csv file stream and	stream reader csv path schema	0.500000
an rdd comprised of vectors containing	mllib random rdds log normal vector rdd	0.166667
to list of ints if possible	ml param type converters to list int	0.333333
featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0	featurescol labelcol predictioncol maxdepth	0.500000
predictions which gives the	linear regression summary	0.013889
contains a param with	params has param	0.019231
array that starts at pos in byte	pos	0.022222
sets the given parameters in this	builder add	0.200000
right singular vectors of	singular	0.015625
comprised	random rdds gamma	0.125000
converts matrix	mlutils convert matrix	1.000000
all values as a list of key-value pairs	all	0.083333
two fields threshold precision curve	ml binary logistic regression summary precision by threshold	0.166667
field in :py attr predictions	generalized linear	0.200000
missing	missing	1.000000
number of	count	0.016949
unique identifier for the spark application	spark context application id	0.500000
the right singular vectors	linalg singular	0.017544
get total number of nodes summed over all	model total num nodes	0.250000
given user and product	user product	0.250000
"predictions" which gives the features of each instance	ml linear regression summary features col	0.166667
much	merger object size	0.032258
a local property set in	context get local property key	0.066667
chain two function together	core chain f g	1.000000
user defined function	user defined function	0.066667
mean squared	mean squared	0.500000
kolmogorov-smirnov ks test for data	stat statistics kolmogorov smirnov test data distname	0.111111
dump already partitioned	core external group	0.045455
as session	spark session	0.100000
until any of the queries	query manager await any termination	0.142857
frequency vectors or transform the rdd of document	tf transform document	0.166667
files added through c{sparkcontext	core spark files get	0.125000
the area under the	area under	0.166667
broadcast on the executors if	broadcast unpersist blocking	0.500000
in this	sql	0.002525
rdd 'x' has maximum membership in this model	gaussian mixture model predict	0.100000
right outer join	full outer join	0.111111
load a java model from the	java loader load java cls sc	0.200000
squared	sparse vector squared	1.000000
in "predictions" which gives the features	linear regression summary features col	0.333333
new :class dataframe sorted by the specified	data frame sort	0.125000
columns that describes the sort	sort cols cols kwargs	0.142857
can be used again to	streaming	0.005025
stored	matrix	0.015152
soundex encoding for a string	soundex col	0.055556
onevsrestmodel create and return	one vs rest	0.034483
objects	external merger	0.031250
pipelinemodel	pipeline model from	0.142857
the java_model to a python model	quantile discretizer create model java_model	0.250000
at the	compression	0.071429
the current [[dataframe]] and perform	data pivot pivot_col values	0.050000
frequency tf vectors to tf-idf vectors	mllib idfmodel transform	0.142857
that	sql streaming query manager	0.011905
calculates the length	sql length	0.050000
column containing a json string into	sql from json col	0.083333
an exception	mllib	0.010526
contains a	param	0.012500
memory for	core external	0.016129
for this sqltransformer	ml sqltransformer	0.250000
for the stream query if this is not	stream writer	0.041667
an external list for	external list of	0.166667
loads a json file stream and	stream reader json path	1.000000
an rdd of labeledpoint	lib svmfile	0.125000
list or gets an	column get	0.200000
computes the sum of	ml	0.001835
:py class stringindexer	string indexer	0.250000
stream query if	sql data stream writer	0.041667
rdd of labeledpoint	mllib mlutils load lib svmfile	0.125000
the left singular	mllib linalg singular	0.017544
featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6	featurescol labelcol predictioncol	0.090909
add a py	spark context add py	0.166667
set number of batches after which	set	0.005917
persist	persist	1.000000
dump	dump	1.000000
number of nonzero	mllib linalg dense vector num	0.200000
of that particular batch has half	half	0.058824
computes column-wise summary statistics for the	statistics col	0.200000
sql storage type for	sql type	0.250000
to stdout id is the rdd id	profiler show id	0.333333
stream returning the	stream reader	0.076923
generate	with sgdtests generate logistic input offset	1.000000
or compute the number of rows	matrix num rows	0.200000
the python direct kafka stream api with	kafka direct stream from	0.125000
to a local representation	to local	0.125000
year	year	0.240000
to this accumulator's value	accumulator add	0.076923
database table named table	table column lowerbound	0.166667
so that :func awaitanytermination() can be used again	sql streaming query manager	0.011905
new dstream by applying reducebykey to	streaming dstream reduce by	0.076923
dump already partitioned	by spill	0.047619
function to	f	0.052632
applies standardization transformation on a	standard scaler model transform	0.500000
the file to which this rdd	file	0.028571
null	generalized linear regression summary null	0.250000
:py attr initmode	init mode value	1.000000
return the column mean values	mean	0.034483
for	external merger object size	0.032258
initial	streaming logistic regression with sgd set initial	0.111111
to wait	sql streaming query manager	0.011905
for new terminations	manager reset	0.011905
to a :class datatype the data type string	datatype string s	0.111111
points saved	points sc path minpartitions	0.250000
operation test for dstream groupbykey	operation tests test group by	1.000000
lshparams	lshparams	1.000000
wait for the execution	streaming streaming context await termination timeout	0.166667
this instance to a java	to java	0.090909
outputcol output	output	0.166667
kolmogorov-smirnov ks test for data sampled	mllib stat statistics kolmogorov smirnov test data	0.111111
makes a class inherit documentation	inherit doc	0.045455
spark session to use for	session sparksession	0.083333
again to wait for new terminations	sql streaming query	0.011765
for binary or multiclass	numclasses	0.111111
and the second is an array of	mllib matrix	0.047619
unpickling it will convert each python object into	to	0.023077
vector class for passing data to	vector	0.019231
so that :func awaitanytermination() can	query manager	0.011905
ignorenulls	ignorenulls	1.000000
this idf	idf	0.111111
until any of the queries	sql streaming query manager await any	0.142857
of two vectors	ml linalg dense vector	0.200000
new accumulator	accumulator init aid	0.083333
calculates the norm of	mllib linalg sparse vector norm p	0.083333
featurescol=none labelcol=none predictioncol=none classifier=none): sets params for onevsrest	ml one vs rest set params featurescol labelcol	1.000000
term frequency vectors or transform the rdd of	tf transform	0.045455
subset	subset	1.000000
java onevsrest	one vs rest from java	0.142857
week number	sql weekofyear col	0.055556
a new dstream in which each rdd is	streaming streaming context	0.032258
new dstream in which each	by	0.014286
int containing elements from	core spark context range	0.142857
of the singularvaluedecomposition	value decomposition v	0.250000
labeledpoint to a string in libsvm format	labeled point to libsvm p	0.500000
converter	converter	0.315789
c{self} and	core rdd	0.006920
rating for the given	matrix factorization	0.040000
python direct kafka rdd get	kafka rdd get	1.000000
matrix to the new mllib-local representation	linalg dense matrix as	0.333333
compute the number of rows	indexed row matrix num rows	0.200000
a python topicandpartition to map to	streaming topic and partition init topic partition	0.055556
the month of	sql dayofmonth col	0.031250
that makes a class inherit documentation from its	inherit	0.037037
accumulator's value	core	0.003021
dump already	core external group by	0.045455
return the topics described by weighted terms	mllib ldamodel describe topics maxtermspertopic	0.333333
the left singular vectors	linalg singular	0.017544
from the population should be	mllib stat kernel density	0.066667
specifies the underlying output	stream writer	0.041667
a param with	ml param	0.009524
generates an rdd	gamma vector rdd sc	1.000000
rdd's current storage level	storage level	0.500000
an associative and	core	0.003021
column scipy matrix from a dictionary	mllib sci py tests scipy matrix	0.090909
test the	kafka stream tests test	0.437500
dump	group by	0.041667
of nonzero elements	ml linalg dense vector	0.100000
all partitioned items as iterator	external merger external items	1.000000
of the :class dataframe as	data frame writer save as	0.071429
an external database table via jdbc	jdbc url table mode properties	0.200000
vectors or	hashing tf	0.125000
partitioned	core external	0.016129
how much of	external merger object size	0.032258
month of a	sql dayofmonth col	0.031250
column standard deviation values	mllib standard scaler model std	0.166667
dot product of two vectors we	ml linalg dense vector dot	0.090909
checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto",	ml random forest classifier	0.023256
until any	await any	0.142857
lda keeplastcheckpoint is	ml distributed ldamodel get	0.066667
c{other}, return a resulting rdd that contains a	rdd cogroup	0.066667
count of the rdd's	rdd	0.003058
the selector	selector set selector	0.333333
learning	learning	1.000000
the values for each key using an	key func numpartitions partitionfunc	0.066667
stream api with	stream from	0.250000
is assumed to consist of	ascending numpartitions keyfunc	0.100000
to wait for new	reset	0.011236
much of memory for	external merger object size	0.032258
the dependent variable given a vector	mllib linear regression model base	0.200000
pandas dataframe	data frame data	0.142857
test the partition	task context tests test partition	1.000000
create an input stream	create stream	0.200000
get all values as a list of key-value	core spark conf get all	0.166667
summary statistics	summary statistics	1.000000
"predictions" which gives the features	linear regression summary features	0.333333
schema	schema options	0.125000
"zerovalue" which may be added to the result	fold by	0.125000
uid of this instance this updates both	ml param params reset uid newuid	0.058824
elements	core rdd	0.017301
new spark configuration	core spark conf init loaddefaults _jvm _jconf	0.250000
:func awaitanytermination()	sql streaming query	0.011765
a line in libsvm format into	parse libsvm line line multiclass	0.111111
ml	ml	0.009174
"predictions" which gives the	linear regression summary	0.041667
:class dataframe in json format	sql data frame	0.005348
predict values	predict x	0.033898
an object	obj identifier	0.333333
distributed	ml distributed	0.200000
internal use only create a new hivecontext for	hive context create for	0.250000
the given parameters in this grid to fixed	param grid builder	0.055556
python rdd of key-value pairs (of form c{rdd[	core rdd save	0.037975
this blockmatrix by other,	multiply	0.100000
that :func awaitanytermination() can be	sql	0.002525
applies unit length normalization on a	mllib normalizer transform	0.500000
pipelinemodel create and return	pipeline model from	0.142857
the training set	distributed ldamodel training	0.034483
test the python direct kafka stream messagehandler	stream tests test kafka direct stream message handler	1.000000
this matrix to a rowmatrix	matrix to row matrix	0.333333
seed=none impurity="gini", numtrees=20	random forest classifier	0.022727
conduct pearson's chi-squared goodness of fit test of	mllib stat statistics chi sq test	0.166667
:py attr	quantile	0.166667
into an rdd of labeledpoint	load lib svmfile sc path numfeatures	0.125000
total log-likelihood for this model	ml gaussian mixture summary log likelihood	0.142857
(json lines text format or newline-delimited json	writer json	0.125000
on	mllib streaming logistic regression with	0.500000
of :class	data frame	0.010000
class inherit documentation from its	mllib inherit doc	0.045455
or newline-delimited json	json	0.043478
a value to an int if possible	ml param type converters to int value	0.250000
of two vectors	ml linalg	0.030303
column containing a json string	json col	0.083333
even if users construct taskcontext instead	core task context new cls	0.333333
awaitanytermination() can be used again	reset	0.011236
comprised of vectors containing i i	mllib random rdds exponential vector	0.125000
this accumulator's value	core accumulator add	0.076923
returns an active query from this	sql streaming	0.010204
converts matrix columns in an input dataframe to	convert matrix columns from ml	0.166667
key-value pair rdd through a flatmap	rdd flat map values	0.333333
:py attr mindf	min df value	1.000000
test that	with tests test	1.000000
"predictions" which gives the true label	logistic regression summary label col	0.333333
value of the date column	day date	0.100000
train a random forest model for	mllib random forest train	0.250000
local	spark context get local	0.333333
convert this vector to	sparse vector	0.062500
learningoffset	learning offset	1.000000
partial objects do not serialize	partial obj	0.125000
compute the number of	linalg indexed row matrix num	0.100000
converts vector columns in an input dataframe from	convert vector columns	0.083333
compute the	rdd	0.003058
given parameters in this grid to	param grid builder add grid	0.100000
partitioned	core external group by	0.045455
partial objects do not serialize	partial	0.076923
keyed dstream	values dstream	0.250000
all the jobs started	job	0.023810
and value class	core spark	0.020619
should be maximized true default or minimized false	evaluator is larger better	0.166667
test that the model params are set	kmeans test test model params	0.250000
setparams(self stages=none) sets params	set params stages	0.500000
