core		_exception_message	excp	return the message from an exception as either a str or unicode object supports both
core		first_spark_call		return a callsite representing the first spark call in the current call stack
core		_get_local_dirs	sub	get all the directories
core	Merger	mergeValues	iterator	combine the items by creator and combiner
core	Merger	mergeCombiners	iterator	merge the combined items by mergecombiner
core	Merger	items		return the merged items ad iterator
core	ExternalMerger	_get_spill_dir	n	choose one directory for spill by number n
core	ExternalMerger	_next_limit		return the next memory limit if the memory is not released
core	ExternalMerger	mergeValues	iterator	combine the items by creator and combiner
core	ExternalMerger	_partition	key	return the partition for key
core	ExternalMerger	_object_size	obj	how much of memory for this obj assume that all the objects
core	ExternalMerger	mergeCombiners	iterator limit	merge k v pair by mergecombiner
core	ExternalMerger	_spill		dump already partitioned data into disks
core	ExternalMerger	items		return all merged items as iterator
core	ExternalMerger	_external_items		return all partitioned items as iterator
core	ExternalMerger	_recursive_merged_items	index	merge the partitioned items and return the as iterator if one partition can not be fit in memory then them will be
core	ExternalMerger	_cleanup		clean up all the files in disks
core	ExternalSorter	_get_path	n	choose one directory for spill by number n
core	ExternalSorter	_next_limit		return the next memory limit if the memory is not released
core	ExternalSorter	sorted	iterator key reverse	sort the elements in iterator do external sort when the memory goes above the limit
core	ExternalList	_spill		dump the values into disk
core	ExternalGroupBy	_spill		dump already partitioned data into disks
core	ExternalGroupBy	_merge_sorted_items	index	load a partition from disk then sort and group by key
core	StatusTracker	getJobIdsForGroup	jobGroup	return a list of all known jobs in a particular job group if
core	StatusTracker	getActiveStageIds		returns an array containing the ids of all active stages
core	StatusTracker	getActiveJobsIds		returns an array containing the ids of all active jobs
core	StatusTracker	getJobInfo	jobId	returns a :class sparkjobinfo object or none if the job info could not be found or was garbage collected
core	StatusTracker	getStageInfo	stageId	returns a :class sparkstageinfo object or none if the stage info could not be found or was garbage collected
core	SparkFiles	get	cls filename	get the absolute path of a file added through c{sparkcontext addfile()}
core	SparkFiles	getRootDirectory	cls	get the root directory that contains files added through c{sparkcontext
core	TaskContext	__new__	cls	even if users construct taskcontext instead of using get give them the singleton
core	TaskContext	__init__		construct a taskcontext use get instead
core	TaskContext	_getOrCreate	cls	internal function to get or create global taskcontext
core	TaskContext	get	cls	return the currently active taskcontext this can be called inside of
core	TaskContext	stageId		the id of the stage that this task belong to
core	TaskContext	partitionId		the id of the rdd partition that is computed by this task
core	TaskContext	attemptNumber		" how many times this task has been attempted
core	TaskContext	taskAttemptId		an id that is unique to this task attempt within the same sparkcontext no two task attempts will share the same attempt id
core		_find_spark_home		find the spark_home
core	Serializer	dump_stream	iterator stream	serialize an iterator of objects to the output stream
core	Serializer	load_stream	stream	return an iterator of deserialized objects from the input stream
core	Serializer	_load_stream_without_unbatching	stream	return an iterator of deserialized batches lists of objects from the input stream
core	FramedSerializer	dumps	obj	serialize an object into a byte array
core	FramedSerializer	loads	obj	deserialize an object from a byte array
core		_restore	name fields value	restore an object of namedtuple
core		_hack_namedtuple	cls	make class generated by namedtuple picklable
core		_hijack_namedtuple		hack namedtuple() to make it picklable
core	ProfilerCollector	new_profiler	ctx	create a new profiler using class profiler_cls
core	ProfilerCollector	add_profiler	id profiler	add a profiler for rdd id
core	ProfilerCollector	dump_profiles	path	dump the profile stats into directory path
core	ProfilerCollector	show_profiles		print the profile stats to stdout
core	Profiler	profile	func	do profiling on the function func
core	Profiler	stats		return the collected profiling stats pstats stats
core	Profiler	show	id	print the profile stats to stdout id is the rdd id
core	Profiler	dump	id path	dump the profile into path id is the rdd id
core	BasicProfiler	profile	func	runs and profiles the method to_profile passed in a profile object is returned
core	Broadcast	__init__	sc value pickle_registry path	should not be called directly by users -- use l{sparkcontext broadcast()}
core	Broadcast	value		return the broadcasted value
core	Broadcast	unpersist	blocking	delete cached copies of this broadcast on the executors if the
core	Broadcast	destroy		destroy all data and metadata related to this broadcast variable
core		chain	f g	chain two function together
core	TaskContextTests	test_stage_id		test the stage ids are available and incrementing as expected
core	TaskContextTests	test_partition_id		test the partition id
core	TaskContextTests	test_attempt_number		verify the attempt numbers are correctly reported
core	TaskContextTests	test_tc_on_driver		verify that getting the taskcontext on the driver returns none
core	DaemonTests	test_termination_stdin		ensure that daemon and workers terminate when stdin is closed
core	DaemonTests	test_termination_sigterm		ensure that daemon and workers terminate on sigterm
core	SparkSubmitTests	createTempFile	name content dir	create a temp file with the given name and content and return its path
core	SparkSubmitTests	createFileInZip	name content ext dir	create a zip archive containing a file with the given content and return its path
core	SparkSubmitTests	test_single_script		submit and test a single script file
core	SparkSubmitTests	test_script_with_local_functions		submit and test a single script file calling a global function
core	SparkSubmitTests	test_module_dependency		submit and test a script with a dependency on another module
core	SparkSubmitTests	test_module_dependency_on_cluster		submit and test a script with a dependency on another module on a cluster
core	SparkSubmitTests	test_package_dependency		submit and test a script with a dependency on a spark package
core	SparkSubmitTests	test_package_dependency_on_cluster		submit and test a script with a dependency on a spark package on a cluster
core	SparkSubmitTests	test_single_script_on_cluster		submit and test a single script on a cluster
core	SparkSubmitTests	test_user_configuration		make sure user configuration is respected spark-19307
core	SparkConf	__init__	loadDefaults _jvm _jconf	create a new spark configuration
core	SparkConf	set	key value	set a configuration property
core	SparkConf	setIfMissing	key value	set a configuration property if not already set
core	SparkConf	setMaster	value	set master url to connect to
core	SparkConf	setAppName	value	set application name
core	SparkConf	setSparkHome	value	set path where spark is installed on worker nodes
core	SparkConf	setExecutorEnv	key value pairs	set an environment variable to be passed to executors
core	SparkConf	setAll	pairs	set multiple parameters passed as a list of key-value pairs
core	SparkConf	get	key defaultValue	get the configured value for some key or return a default otherwise
core	SparkConf	getAll		get all values as a list of key-value pairs
core	SparkConf	contains	key	does this configuration contain a given key?
core	SparkConf	toDebugString		returns a printable version of the configuration as a list of key=value pairs one per line
core		heappush	heap item	push item onto heap maintaining the heap invariant
core		heappop	heap	pop the smallest item off the heap maintaining the heap invariant
core		heapreplace	heap item	pop and return the current smallest value and add the new item
core		heappushpop	heap item	fast version of a heappush followed by a heappop
core		heapify	x	transform list into a heap in-place in o(len x time
core		_heappop_max	heap	maxheap version of a heappop
core		_heapreplace_max	heap item	maxheap version of a heappop followed by a heappush
core		_heapify_max	x	transform list into a maxheap in-place in o(len x time
core		_siftdown_max	heap startpos pos	maxheap variant of _siftdown
core		_siftup_max	heap pos	maxheap variant of _siftup
core		merge	iterables key reverse	merge multiple sorted inputs into a single sorted output
core		nsmallest	n iterable key	find the n smallest elements in a dataset
core		nlargest	n iterable key	find the n largest elements in a dataset
core	CloudPickler	save_memoryview	obj	fallback to save_string
core	CloudPickler	save_buffer	obj	fallback to save_string
core	CloudPickler	save_module	obj	save a module as an import
core	CloudPickler	save_codeobject	obj	save a code object
core	CloudPickler	save_function	obj name	registered with the dispatch to handle all function types
core	CloudPickler	save_function_tuple	func	pickles an actual func object
core	CloudPickler	extract_code_globals	cls co	find all globals names read or written to by codeblock co
core	CloudPickler	extract_func_data	func	turn the function into a tuple of data necessary to recreate it
core	CloudPickler	save_inst	obj	inner logic to save instance based off pickle save_inst
core	CloudPickler	save_itemgetter	obj	itemgetter serializer needed for namedtuple support
core	CloudPickler	save_reduce	func args state listitems	modified to support __transient__ on new objects
core	CloudPickler	save_partial	obj	partial objects do not serialize correctly in python2 x -- this fixes the bugs
core	CloudPickler	save_file	obj	save a file
core	CloudPickler	save_ufunc	obj	hack function for saving numpy ufunc objects
core	CloudPickler	inject_addons		plug in system register additional pickling functions if modules already loaded
core		_modules_to_main	modList	force every module in modlist to be placed into main
core		_fill_function	func globals defaults dict	fills in the rest of function data into the skeleton function object that were created via _make_skel_func()
core		_make_skel_func	code closures base_globals	creates a skeleton function object that contains just the provided code and the correct number of cells in func_closure
core		_load_class	cls d	loads additional properties into class cls
core		_load_namedtuple	name fields	loads a class generated by namedtuple
core		launch_gateway	conf	launch jvm gateway
core	Accumulator	__init__	aid value accum_param	create a new accumulator with a given initial value and accumulatorparam object
core	Accumulator	__reduce__		custom serialization saves the zero value from our accumulatorparam
core	Accumulator	value		get the accumulator's value only usable in driver program
core	Accumulator	value	value	sets the accumulator's value only usable in driver program
core	Accumulator	add	term	adds a term to this accumulator's value
core	Accumulator	__iadd__	term	the += operator adds a term to this accumulator's value
core	AccumulatorParam	zero	value	provide a "zero value" for the type compatible in dimensions with the provided c{value} (e
core	AccumulatorParam	addInPlace	value1 value2	add two values of the accumulator's data type returning a new value for efficiency can also update c{value1} in place and return it
core		_start_update_server		start a tcp server to receive accumulator updates in a daemon thread and returns it
core		since	version	a decorator that annotates a function to append the version of spark the function was added
core		copy_func	f name sinceversion doc	returns a function with same code globals defaults closure and name or provide a new name
core		keyword_only	func	a decorator that forces keyword arguments in the wrapped method and saves actual input keyword arguments in _input_kwargs
core		worker	sock	called by a worker process after the fork()
core		portable_hash	x	this function returns consistent hash code for builtin types especially for none and tuple with none
core		_parse_memory	s	parse a memory string in the format supported by java e g 1g 200m and
core		ignore_unicode_prefix	f	ignore the 'u' prefix of string in doc tests to make it works
core	RDD	id		a unique id for this rdd within its sparkcontext
core	RDD	context		the l{sparkcontext} that this rdd was created on
core	RDD	cache		persist this rdd with the default storage level (c{memory_only})
core	RDD	persist	storageLevel	set this rdd's storage level to persist its values across operations after the first time it is computed
core	RDD	unpersist		mark the rdd as non-persistent and remove all blocks for it from memory and disk
core	RDD	checkpoint		mark this rdd for checkpointing it will be saved to a file inside the
core	RDD	isCheckpointed		return whether this rdd is checkpointed and materialized either reliably or locally
core	RDD	localCheckpoint		mark this rdd for local checkpointing using spark's existing caching layer
core	RDD	isLocallyCheckpointed		return whether this rdd is marked for local checkpointing
core	RDD	getCheckpointFile		gets the name of the file to which this rdd was checkpointed not defined if rdd is checkpointed locally
core	RDD	map	f preservesPartitioning	return a new rdd by applying a function to each element of this rdd
core	RDD	flatMap	f preservesPartitioning	return a new rdd by first applying a function to all elements of this rdd and then flattening the results
core	RDD	mapPartitions	f preservesPartitioning	return a new rdd by applying a function to each partition of this rdd
core	RDD	mapPartitionsWithIndex	f preservesPartitioning	return a new rdd by applying a function to each partition of this rdd while tracking the index of the original partition
core	RDD	mapPartitionsWithSplit	f preservesPartitioning	deprecated use mappartitionswithindex instead
core	RDD	getNumPartitions		returns the number of partitions in rdd >>> rdd = sc
core	RDD	filter	f	return a new rdd containing only the elements that satisfy a predicate
core	RDD	distinct	numPartitions	return a new rdd containing the distinct elements in this rdd
core	RDD	sample	withReplacement fraction seed	return a sampled subset of this rdd
core	RDD	randomSplit	weights seed	randomly splits this rdd with the provided weights
core	RDD	takeSample	withReplacement num seed	return a fixed-size sampled subset of this rdd
core	RDD	_computeFractionForSampleSize	sampleSizeLowerBound total withReplacement	returns a sampling rate that guarantees a sample of size >= samplesizelowerbound 99
core	RDD	union	other	return the union of this rdd and another one
core	RDD	intersection	other	return the intersection of this rdd and another one the output will
core	RDD	__add__	other	return the union of this rdd and another one
core	RDD	repartitionAndSortWithinPartitions	numPartitions partitionFunc ascending keyfunc	repartition the rdd according to the given partitioner and within each resulting partition sort records by their keys
core	RDD	sortByKey	ascending numPartitions keyfunc	sorts this rdd which is assumed to consist of key value pairs
core	RDD	sortBy	keyfunc ascending numPartitions	sorts this rdd by the given keyfunc >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
core	RDD	glom		return an rdd created by coalescing all elements within each partition into a list
core	RDD	cartesian	other	return the cartesian product of this rdd and another one that is the rdd of all pairs of elements c{ a b } where c{a} is in c{self} and
core	RDD	groupBy	f numPartitions partitionFunc	return an rdd of grouped items
core	RDD	pipe	command env checkCode	return an rdd created by piping elements to a forked external process
core	RDD	foreach	f	applies a function to all elements of this rdd
core	RDD	foreachPartition	f	applies a function to each partition of this rdd
core	RDD	collect		return a list that contains all of the elements in this rdd
core	RDD	reduce	f	reduces the elements of this rdd using the specified commutative and associative binary operator
core	RDD	treeReduce	f depth	reduces the elements of this rdd in a multi-level tree pattern
core	RDD	fold	zeroValue op	aggregate the elements of each partition and then the results for all the partitions using a given associative function and a neutral "zero value
core	RDD	aggregate	zeroValue seqOp combOp	aggregate the elements of each partition and then the results for all the partitions using a given combine functions and a neutral "zero
core	RDD	treeAggregate	zeroValue seqOp combOp depth	aggregates the elements of this rdd in a multi-level tree pattern
core	RDD	max	key	find the maximum item in this rdd
core	RDD	min	key	find the minimum item in this rdd
core	RDD	sum		add up the elements in this rdd
core	RDD	count		return the number of elements in this rdd
core	RDD	stats		return a l{statcounter} object that captures the mean variance and count of the rdd's elements in one operation
core	RDD	histogram	buckets	compute a histogram using the provided buckets the buckets
core	RDD	mean		compute the mean of this rdd's elements
core	RDD	variance		compute the variance of this rdd's elements
core	RDD	stdev		compute the standard deviation of this rdd's elements
core	RDD	sampleStdev		compute the sample standard deviation of this rdd's elements (which corrects for bias in estimating the standard deviation by dividing by
core	RDD	sampleVariance		compute the sample variance of this rdd's elements which corrects for bias in estimating the variance by dividing by n-1 instead of n
core	RDD	countByValue		return the count of each unique value in this rdd as a dictionary of value count pairs
core	RDD	top	num key	get the top n elements from an rdd
core	RDD	takeOrdered	num key	get the n elements from an rdd ordered in ascending order or as specified by the optional key function
core	RDD	take	num	take the first num elements of the rdd
core	RDD	first		return the first element in this rdd
core	RDD	isEmpty		returns true if and only if the rdd contains no elements at all
core	RDD	saveAsNewAPIHadoopDataset	conf keyConverter valueConverter	output a python rdd of key-value pairs (of form c{rdd[ k v ]}) to any hadoop file system using the new hadoop outputformat api mapreduce package
core	RDD	saveAsNewAPIHadoopFile	path outputFormatClass keyClass valueClass	output a python rdd of key-value pairs (of form c{rdd[ k v ]}) to any hadoop file system using the new hadoop outputformat api mapreduce package
core	RDD	saveAsHadoopDataset	conf keyConverter valueConverter	output a python rdd of key-value pairs (of form c{rdd[ k v ]}) to any hadoop file system using the old hadoop outputformat api mapred package
core	RDD	saveAsHadoopFile	path outputFormatClass keyClass valueClass	output a python rdd of key-value pairs (of form c{rdd[ k v ]}) to any hadoop file system using the old hadoop outputformat api mapred package
core	RDD	saveAsSequenceFile	path compressionCodecClass	output a python rdd of key-value pairs (of form c{rdd[ k v ]}) to any hadoop file system using the l{org
core	RDD	saveAsPickleFile	path batchSize	save this rdd as a sequencefile of serialized objects the serializer
core	RDD	saveAsTextFile	path compressionCodecClass	save this rdd as a text file using string representations of elements
core	RDD	collectAsMap		return the key-value pairs in this rdd to the master as a dictionary
core	RDD	keys		return an rdd with the keys of each tuple
core	RDD	values		return an rdd with the values of each tuple
core	RDD	reduceByKey	func numPartitions partitionFunc	merge the values for each key using an associative and commutative reduce function
core	RDD	reduceByKeyLocally	func	merge the values for each key using an associative and commutative reduce function but return the results immediately to the master as a dictionary
core	RDD	countByKey		count the number of elements for each key and return the result to the master as a dictionary
core	RDD	join	other numPartitions	return an rdd containing all pairs of elements with matching keys in c{self} and c{other}
core	RDD	leftOuterJoin	other numPartitions	perform a left outer join of c{self} and c{other}
core	RDD	rightOuterJoin	other numPartitions	perform a right outer join of c{self} and c{other}
core	RDD	fullOuterJoin	other numPartitions	perform a right outer join of c{self} and c{other}
core	RDD	partitionBy	numPartitions partitionFunc	return a copy of the rdd partitioned using the specified partitioner
core	RDD	combineByKey	createCombiner mergeValue mergeCombiners numPartitions	generic function to combine the elements for each key using a custom set of aggregation functions
core	RDD	aggregateByKey	zeroValue seqFunc combFunc numPartitions	aggregate the values of each key using given combine functions and a neutral "zero value"
core	RDD	foldByKey	zeroValue func numPartitions partitionFunc	merge the values for each key using an associative function "func" and a neutral "zerovalue" which may be added to the result an
core	RDD	groupByKey	numPartitions partitionFunc	group the values for each key in the rdd into a single sequence
core	RDD	flatMapValues	f	pass each value in the key-value pair rdd through a flatmap function without changing the keys this also retains the original rdd's
core	RDD	mapValues	f	pass each value in the key-value pair rdd through a map function without changing the keys this also retains the original rdd's
core	RDD	groupWith	other	alias for cogroup but with support for multiple rdds
core	RDD	cogroup	other numPartitions	for each key k in c{self} or c{other}, return a resulting rdd that contains a tuple with the list of values for that key in c{self} as
core	RDD	sampleByKey	withReplacement fractions seed	return a subset of this rdd sampled by key via stratified sampling
core	RDD	subtractByKey	other numPartitions	return each key value pair in c{self} that has no pair with matching key in c{other}
core	RDD	subtract	other numPartitions	return each value in c{self} that is not contained in c{other}
core	RDD	keyBy	f	creates tuples of the elements in this rdd by applying c{f}
core	RDD	repartition	numPartitions	return a new rdd that has exactly numpartitions partitions
core	RDD	coalesce	numPartitions shuffle	return a new rdd that is reduced into numpartitions partitions
core	RDD	zip	other	zips this rdd with another one returning key-value pairs with the first element in each rdd second element in each rdd etc
core	RDD	zipWithIndex		zips this rdd with its element indices
core	RDD	zipWithUniqueId		zips this rdd with generated unique long ids
core	RDD	name		return the name of this rdd
core	RDD	setName	name	assign a name to this rdd
core	RDD	toDebugString		a description of this rdd and its recursive dependencies for debugging
core	RDD	getStorageLevel		get the rdd's current storage level
core	RDD	_defaultReducePartitions		returns the default number of partitions to use during reduce tasks (e g groupby)
core	RDD	lookup	key	return the list of values in the rdd for key key this operation
core	RDD	_to_java_object_rdd		return a javardd of object by unpickling it will convert each python object into java object by pyrolite whenever the
core	RDD	countApprox	timeout confidence	note : experimental
core	RDD	sumApprox	timeout confidence	note : experimental
core	RDD	meanApprox	timeout confidence	note : experimental
core	RDD	countApproxDistinct	relativeSD	note : experimental
core	RDD	toLocalIterator		return an iterator that contains all of the elements in this rdd
core	StatCounter	asDict	sample	returns the :class statcounter members as a dict
core	SparkContext	__init__	master appName sparkHome pyFiles	create a new sparkcontext at least the master and app name should be set
core	SparkContext	_initialize_context	jconf	initialize sparkcontext in function to allow subclass specific initialization
core	SparkContext	_ensure_initialized	cls instance gateway conf	checks whether a sparkcontext is initialized or not
core	SparkContext	__enter__		enable 'with sparkcontext as sc app sc ' syntax
core	SparkContext	__exit__	type value trace	enable 'with sparkcontext as sc app' syntax
core	SparkContext	getOrCreate	cls conf	get or instantiate a sparkcontext and register it as a singleton object
core	SparkContext	setLogLevel	logLevel	control our loglevel this overrides any user-defined log settings
core	SparkContext	setSystemProperty	cls key value	set a java system property such as spark executor memory this must
core	SparkContext	version		the version of spark on which this application is running
core	SparkContext	applicationId		a unique identifier for the spark application
core	SparkContext	uiWebUrl		return the url of the sparkui instance started by this sparkcontext
core	SparkContext	startTime		return the epoch time when the spark context was started
core	SparkContext	defaultParallelism		default level of parallelism to use when not given by user (e g for
core	SparkContext	defaultMinPartitions		default min number of partitions for hadoop rdds when not given by user
core	SparkContext	stop		shut down the sparkcontext
core	SparkContext	emptyRDD		create an rdd that has no partitions or elements
core	SparkContext	range	start end step numSlices	create a new rdd of int containing elements from start to end exclusive increased by step every element
core	SparkContext	parallelize	c numSlices	distribute a local python collection to form an rdd using xrange
core	SparkContext	pickleFile	name minPartitions	load an rdd previously saved using l{rdd saveaspicklefile} method
core	SparkContext	textFile	name minPartitions use_unicode	read a text file from hdfs a local file system available on all nodes or any hadoop-supported file system uri and return it as an
core	SparkContext	wholeTextFiles	path minPartitions use_unicode	read a directory of text files from hdfs a local file system available on all nodes or any hadoop-supported file system
core	SparkContext	binaryFiles	path minPartitions	note : experimental
core	SparkContext	binaryRecords	path recordLength	note : experimental
core	SparkContext	sequenceFile	path keyClass valueClass keyConverter	read a hadoop sequencefile with arbitrary key and value writable class from hdfs a local file system available on all nodes or any hadoop-supported file system uri
core	SparkContext	newAPIHadoopFile	path inputFormatClass keyClass valueClass	read a 'new api' hadoop inputformat with arbitrary key and value class from hdfs a local file system available on all nodes or any hadoop-supported file system uri
core	SparkContext	newAPIHadoopRDD	inputFormatClass keyClass valueClass keyConverter	read a 'new api' hadoop inputformat with arbitrary key and value class from an arbitrary hadoop configuration which is passed in as a python dict
core	SparkContext	hadoopFile	path inputFormatClass keyClass valueClass	read an 'old' hadoop inputformat with arbitrary key and value class from hdfs a local file system available on all nodes or any hadoop-supported file system uri
core	SparkContext	hadoopRDD	inputFormatClass keyClass valueClass keyConverter	read an 'old' hadoop inputformat with arbitrary key and value class from an arbitrary hadoop configuration which is passed in as a python dict
core	SparkContext	union	rdds	build the union of a list of rdds
core	SparkContext	broadcast	value	broadcast a read-only variable to the cluster returning a l{broadcast<pyspark
core	SparkContext	accumulator	value accum_param	create an l{accumulator} with the given initial value using a given l{accumulatorparam} helper object to define how to add values of the
core	SparkContext	addFile	path recursive	add a file to be downloaded with this spark job on every node
core	SparkContext	addPyFile	path	add a py or zip dependency for all tasks to be executed on this
core	SparkContext	setCheckpointDir	dirName	set the directory under which rdds are going to be checkpointed the
core	SparkContext	_getJavaStorageLevel	storageLevel	returns a java storagelevel based on a pyspark storagelevel
core	SparkContext	setJobGroup	groupId description interruptOnCancel	assigns a group id to all the jobs started by this thread until the group id is set to a different value or cleared
core	SparkContext	setLocalProperty	key value	set a local property that affects jobs submitted from this thread such as the spark fair scheduler pool
core	SparkContext	getLocalProperty	key	get a local property set in this thread or null if it is missing see
core	SparkContext	sparkUser		get spark_user for user who is running sparkcontext
core	SparkContext	cancelJobGroup	groupId	cancel active jobs for the specified group see l{sparkcontext setjobgroup}
core	SparkContext	cancelAllJobs		cancel all jobs that have been scheduled or are running
core	SparkContext	statusTracker		return :class statustracker object
core	SparkContext	runJob	rdd partitionFunc partitions allowLocal	executes the given partitionfunc on the specified set of partitions returning the result as an array of elements
core	SparkContext	show_profiles		print the profile stats to stdout
core	SparkContext	dump_profiles	path	dump the profile stats into directory path
streaming	StreamingListener	onReceiverStarted	receiverStarted	called when a receiver has been started
streaming	StreamingListener	onReceiverError	receiverError	called when a receiver has reported an error
streaming	StreamingListener	onReceiverStopped	receiverStopped	called when a receiver has been stopped
streaming	StreamingListener	onBatchSubmitted	batchSubmitted	called when a batch of jobs has been submitted for processing
streaming	StreamingListener	onBatchStarted	batchStarted	called when processing of a batch of jobs has started
streaming	StreamingListener	onBatchCompleted	batchCompleted	called when processing of a batch of jobs has completed
streaming	StreamingListener	onOutputOperationStarted	outputOperationStarted	called when processing of a job of a batch has started
streaming	StreamingListener	onOutputOperationCompleted	outputOperationCompleted	called when processing of a job of a batch has completed
streaming		utf8_decoder	s	decode the unicode as utf-8
streaming	KafkaUtils	createStream	ssc zkQuorum groupId topics	create an input stream that pulls messages from a kafka broker
streaming	KafkaUtils	createDirectStream	ssc topics kafkaParams fromOffsets	note : experimental
streaming	KafkaUtils	createRDD	sc kafkaParams offsetRanges leaders	note : experimental
streaming	OffsetRange	__init__	topic partition fromOffset untilOffset	create an offsetrange to represent range of offsets
streaming	TopicAndPartition	__init__	topic partition	create a python topicandpartition to map to the java related object
streaming	Broker	__init__	host port	create a python broker to map to the java related object
streaming	KafkaRDD	offsetRanges		get the offsetrange of specific kafkardd
streaming	KafkaDStream	foreachRDD	func	apply a function to each rdd in this dstream
streaming	KafkaDStream	transform	func	return a new dstream in which each rdd is generated by applying a function on each rdd of this dstream
streaming	KafkaMessageAndMetadata	__init__	topic partition offset key	python wrapper of kafka messageandmetadata
streaming		rddToFileName	prefix suffix timestamp	return string prefix-time suffix
streaming		utf8_decoder	s	decode the unicode as utf-8
streaming	KinesisUtils	createStream	ssc kinesisAppName streamName endpointUrl	create an input stream that pulls messages from a kinesis stream this uses the
streaming	PySparkStreamingTestCase	_take	dstream n	return the first n elements in the stream will start and stop
streaming	PySparkStreamingTestCase	_collect	dstream n block	collect each rdds into the returned list
streaming	PySparkStreamingTestCase	_test_func	input func expected sort	@param input dataset for the test this should be list of lists
streaming	PySparkStreamingTestCase	_sort_result_based_on_key	outputs	sort the list based on first value
streaming	BasicOperationTests	test_map		basic operation test for dstream map
streaming	BasicOperationTests	test_flatMap		basic operation test for dstream faltmap
streaming	BasicOperationTests	test_filter		basic operation test for dstream filter
streaming	BasicOperationTests	test_count		basic operation test for dstream count
streaming	BasicOperationTests	test_reduce		basic operation test for dstream reduce
streaming	BasicOperationTests	test_reduceByKey		basic operation test for dstream reducebykey
streaming	BasicOperationTests	test_mapValues		basic operation test for dstream mapvalues
streaming	BasicOperationTests	test_flatMapValues		basic operation test for dstream flatmapvalues
streaming	BasicOperationTests	test_glom		basic operation test for dstream glom
streaming	BasicOperationTests	test_mapPartitions		basic operation test for dstream mappartitions
streaming	BasicOperationTests	test_countByValue		basic operation test for dstream countbyvalue
streaming	BasicOperationTests	test_groupByKey		basic operation test for dstream groupbykey
streaming	BasicOperationTests	test_combineByKey		basic operation test for dstream combinebykey
streaming	KafkaStreamTests	test_kafka_stream		test the python kafka stream api
streaming	KafkaStreamTests	test_kafka_direct_stream		test the python direct kafka stream api
streaming	KafkaStreamTests	test_kafka_direct_stream_from_offset		test the python direct kafka stream api with start offset specified
streaming	KafkaStreamTests	test_kafka_rdd		test the python direct kafka rdd api
streaming	KafkaStreamTests	test_kafka_rdd_with_leaders		test the python direct kafka rdd api with leaders
streaming	KafkaStreamTests	test_kafka_rdd_get_offsetRanges		test python direct kafka rdd get offsetranges
streaming	KafkaStreamTests	test_kafka_direct_stream_foreach_get_offsetRanges		test the python direct kafka stream foreachrdd get offsetranges
streaming	KafkaStreamTests	test_kafka_direct_stream_transform_get_offsetRanges		test the python direct kafka stream transform get offsetranges
streaming	KafkaStreamTests	test_kafka_direct_stream_transform_with_checkpoint		test the python direct kafka stream transform with checkpoint correctly recovered
streaming	KafkaStreamTests	test_kafka_rdd_message_handler		test python direct kafka rdd messagehandler
streaming	KafkaStreamTests	test_kafka_direct_stream_message_handler		test the python direct kafka stream messagehandler
streaming		utf8_decoder	s	decode the unicode as utf-8
streaming	FlumeUtils	createStream	ssc hostname port storageLevel	create an input stream that pulls events from flume
streaming	FlumeUtils	createPollingStream	ssc addresses storageLevel maxBatchSize	creates an input stream that is to be used with the spark sink deployed on a flume agent
streaming	DStream	context		return the streamingcontext associated with this dstream
streaming	DStream	count		return a new dstream in which each rdd has a single element generated by counting each rdd of this dstream
streaming	DStream	filter	f	return a new dstream containing only the elements that satisfy predicate
streaming	DStream	flatMap	f preservesPartitioning	return a new dstream by applying a function to all elements of
streaming	DStream	map	f preservesPartitioning	return a new dstream by applying a function to each element of dstream
streaming	DStream	mapPartitions	f preservesPartitioning	return a new dstream in which each rdd is generated by applying mappartitions() to each rdds of this dstream
streaming	DStream	mapPartitionsWithIndex	f preservesPartitioning	return a new dstream in which each rdd is generated by applying mappartitionswithindex() to each rdds of this dstream
streaming	DStream	reduce	func	return a new dstream in which each rdd has a single element generated by reducing each rdd of this dstream
streaming	DStream	reduceByKey	func numPartitions	return a new dstream by applying reducebykey to each rdd
streaming	DStream	combineByKey	createCombiner mergeValue mergeCombiners numPartitions	return a new dstream by applying combinebykey to each rdd
streaming	DStream	partitionBy	numPartitions partitionFunc	return a copy of the dstream in which each rdd are partitioned using the specified partitioner
streaming	DStream	foreachRDD	func	apply a function to each rdd in this dstream
streaming	DStream	pprint	num	print the first num elements of each rdd generated in this dstream
streaming	DStream	mapValues	f	return a new dstream by applying a map function to the value of each key-value pairs in this dstream without changing the key
streaming	DStream	flatMapValues	f	return a new dstream by applying a flatmap function to the value of each key-value pairs in this dstream without changing the key
streaming	DStream	glom		return a new dstream in which rdd is generated by applying glom() to rdd of this dstream
streaming	DStream	cache		persist the rdds of this dstream with the default storage level (c{memory_only})
streaming	DStream	persist	storageLevel	persist the rdds of this dstream with the given storage level
streaming	DStream	checkpoint	interval	enable periodic checkpointing of rdds of this dstream
streaming	DStream	groupByKey	numPartitions	return a new dstream by applying groupbykey on each rdd
streaming	DStream	countByValue		return a new dstream in which each rdd contains the counts of each distinct value in each rdd of this dstream
streaming	DStream	saveAsTextFiles	prefix suffix	save each rdd in this dstream as at text file using string representation of elements
streaming	DStream	transform	func	return a new dstream in which each rdd is generated by applying a function on each rdd of this dstream
streaming	DStream	transformWith	func other keepSerializer	return a new dstream in which each rdd is generated by applying a function on each rdd of this dstream and 'other' dstream
streaming	DStream	repartition	numPartitions	return a new dstream with an increased or decreased level of parallelism
streaming	DStream	_slideDuration		return the slideduration in seconds of this dstream
streaming	DStream	union	other	return a new dstream by unifying data of another dstream with this dstream
streaming	DStream	cogroup	other numPartitions	return a new dstream by applying 'cogroup' between rdds of this dstream and other dstream
streaming	DStream	join	other numPartitions	return a new dstream by applying 'join' between rdds of this dstream and other dstream
streaming	DStream	leftOuterJoin	other numPartitions	return a new dstream by applying 'left outer join' between rdds of this dstream and other dstream
streaming	DStream	rightOuterJoin	other numPartitions	return a new dstream by applying 'right outer join' between rdds of this dstream and other dstream
streaming	DStream	fullOuterJoin	other numPartitions	return a new dstream by applying 'full outer join' between rdds of this dstream and other dstream
streaming	DStream	_jtime	timestamp	convert datetime or unix_timestamp into time
streaming	DStream	slice	begin end	return all the rdds between 'begin' to 'end' both included begin, end could be datetime
streaming	DStream	window	windowDuration slideDuration	return a new dstream in which each rdd contains all the elements in seen in a sliding window of time over this dstream
streaming	DStream	reduceByWindow	reduceFunc invReduceFunc windowDuration slideDuration	return a new dstream in which each rdd has a single element generated by reducing all elements in a sliding window over this dstream
streaming	DStream	countByWindow	windowDuration slideDuration	return a new dstream in which each rdd has a single element generated by counting the number of elements in a window over this dstream
streaming	DStream	countByValueAndWindow	windowDuration slideDuration numPartitions	return a new dstream in which each rdd contains the count of distinct elements in rdds in a sliding window over this dstream
streaming	DStream	groupByKeyAndWindow	windowDuration slideDuration numPartitions	return a new dstream by applying groupbykey over a sliding window
streaming	DStream	reduceByKeyAndWindow	func invFunc windowDuration slideDuration	return a new dstream by applying incremental reducebykey over a sliding window
streaming	DStream	updateStateByKey	updateFunc numPartitions initialRDD	return a new "state" dstream where the state for each key is updated by applying the given function on the previous state of the key and the new values of the key
streaming	StreamingContext	__init__	sparkContext batchDuration jssc	create a new streamingcontext
streaming	StreamingContext	_jduration	seconds	create duration object given number of seconds
streaming	StreamingContext	getOrCreate	cls checkpointPath setupFunc	either recreate a streamingcontext from checkpoint data or create a new streamingcontext
streaming	StreamingContext	getActive	cls	return either the currently active streamingcontext (i e if there is a context started
streaming	StreamingContext	getActiveOrCreate	cls checkpointPath setupFunc	either return the active streamingcontext i e currently started but not stopped
streaming	StreamingContext	sparkContext		return sparkcontext which is associated with this streamingcontext
streaming	StreamingContext	start		start the execution of the streams
streaming	StreamingContext	awaitTermination	timeout	wait for the execution to stop
streaming	StreamingContext	awaitTerminationOrTimeout	timeout	wait for the execution to stop return true if it's stopped or
streaming	StreamingContext	stop	stopSparkContext stopGraceFully	stop the execution of the streams with option of ensuring all received data has been processed
streaming	StreamingContext	remember	duration	set each dstreams in this context to remember rdds it generated in the last given duration
streaming	StreamingContext	checkpoint	directory	sets the context to periodically checkpoint the dstream operations for master fault-tolerance
streaming	StreamingContext	socketTextStream	hostname port storageLevel	create an input from tcp source hostname port data is received using
streaming	StreamingContext	textFileStream	directory	create an input stream that monitors a hadoop-compatible file system for new files and reads them as text files
streaming	StreamingContext	binaryRecordsStream	directory recordLength	create an input stream that monitors a hadoop-compatible file system for new files and reads them as flat binary files with records of
streaming	StreamingContext	queueStream	rdds oneAtATime default	create an input stream from an queue of rdds or list in each batch
streaming	StreamingContext	transform	dstreams transformFunc	create a new dstream in which each rdd is generated by applying a function on rdds of the dstreams
streaming	StreamingContext	union		create a unified dstream from multiple dstreams of the same type and same slide duration
streaming	StreamingContext	addStreamingListener	streamingListener	add a [[org apache spark streaming scheduler streaminglistener]] object for
ml	ALS	__init__	rank maxIter regParam numUserBlocks	__init__(self rank=10 maxiter=10 regparam=0 1 numuserblocks=10 numitemblocks=10 implicitprefs=false alpha=1 0 usercol="user", itemcol="item", seed=none ratingcol="rating", nonnegative=false checkpointinterval=10 intermediatestoragelevel="memory_and_disk", finalstoragelevel="memory_and_disk", coldstartstrategy="nan")
ml	ALS	setParams	rank maxIter regParam numUserBlocks	setparams(self rank=10 maxiter=10 regparam=0 1 numuserblocks=10 numitemblocks=10 implicitprefs=false alpha=1 0 usercol="user", itemcol="item", seed=none ratingcol="rating", nonnegative=false checkpointinterval=10 intermediatestoragelevel="memory_and_disk", finalstoragelevel="memory_and_disk", coldstartstrategy="nan")
ml	ALS	setRank	value	sets the value of :py attr rank
ml	ALS	getRank		gets the value of rank or its default value
ml	ALS	setNumUserBlocks	value	sets the value of :py attr numuserblocks
ml	ALS	getNumUserBlocks		gets the value of numuserblocks or its default value
ml	ALS	setNumItemBlocks	value	sets the value of :py attr numitemblocks
ml	ALS	getNumItemBlocks		gets the value of numitemblocks or its default value
ml	ALS	setNumBlocks	value	sets both :py attr numuserblocks and :py attr numitemblocks to the specific value
ml	ALS	setImplicitPrefs	value	sets the value of :py attr implicitprefs
ml	ALS	getImplicitPrefs		gets the value of implicitprefs or its default value
ml	ALS	setAlpha	value	sets the value of :py attr alpha
ml	ALS	getAlpha		gets the value of alpha or its default value
ml	ALS	setUserCol	value	sets the value of :py attr usercol
ml	ALS	getUserCol		gets the value of usercol or its default value
ml	ALS	setItemCol	value	sets the value of :py attr itemcol
ml	ALS	getItemCol		gets the value of itemcol or its default value
ml	ALS	setRatingCol	value	sets the value of :py attr ratingcol
ml	ALS	getRatingCol		gets the value of ratingcol or its default value
ml	ALS	setNonnegative	value	sets the value of :py attr nonnegative
ml	ALS	getNonnegative		gets the value of nonnegative or its default value
ml	ALS	setIntermediateStorageLevel	value	sets the value of :py attr intermediatestoragelevel
ml	ALS	getIntermediateStorageLevel		gets the value of intermediatestoragelevel or its default value
ml	ALS	setFinalStorageLevel	value	sets the value of :py attr finalstoragelevel
ml	ALS	getFinalStorageLevel		gets the value of finalstoragelevel or its default value
ml	ALS	setColdStartStrategy	value	sets the value of :py attr coldstartstrategy
ml	ALS	getColdStartStrategy		gets the value of coldstartstrategy or its default value
ml	ALSModel	rank		rank of the matrix factorization model
ml	ALSModel	userFactors		a dataframe that stores user factors in two columns id and
ml	ALSModel	itemFactors		a dataframe that stores item factors in two columns id and
ml	ALSModel	recommendForAllUsers	numItems	returns top numitems items recommended for each user for all users
ml	ALSModel	recommendForAllItems	numUsers	returns top numusers users recommended for each item for all items
ml		_jvm		returns the jvm view associated with sparkcontext must be called
ml	Identifiable	_randomUID	cls	generate a unique unicode id for the object the default implementation
ml	MLWriter	save	path	save the ml instance to the input path
ml	MLWriter	overwrite		overwrites if the output path already exists
ml	MLWriter	context	sqlContext	sets the sql context to use for saving
ml	MLWriter	session	sparkSession	sets the spark session to use for saving
ml	JavaMLWriter	save	path	save the ml instance to the input path
ml	JavaMLWriter	overwrite		overwrites if the output path already exists
ml	JavaMLWriter	context	sqlContext	sets the sql context to use for saving
ml	JavaMLWriter	session	sparkSession	sets the spark session to use for saving
ml	MLWritable	write		returns an mlwriter instance for this ml instance
ml	MLWritable	save	path	save this ml instance to the given path a shortcut of write() save path
ml	JavaMLWritable	write		returns an mlwriter instance for this ml instance
ml	MLReader	load	path	load the ml instance from the input path
ml	MLReader	context	sqlContext	sets the sql context to use for loading
ml	MLReader	session	sparkSession	sets the spark session to use for loading
ml	JavaMLReader	load	path	load the ml instance from the input path
ml	JavaMLReader	context	sqlContext	sets the sql context to use for loading
ml	JavaMLReader	session	sparkSession	sets the spark session to use for loading
ml	JavaMLReader	_java_loader_class	cls clazz	returns the full class name of the java ml instance the default
ml	JavaMLReader	_load_java_obj	cls clazz	load the peer java object of the ml instance
ml	MLReadable	read	cls	returns an mlreader instance for this class
ml	MLReadable	load	cls path	reads an ml instance from the input path a shortcut of read() load path
ml	JavaMLReadable	read	cls	returns an mlreader instance for this class
ml	JavaPredictionModel	numFeatures		returns the number of features the model was trained on if unknown returns -1
ml		_to_java_object_rdd	rdd	return an javardd of object by unpickling it will convert each python object into java object by pyrolite whenever the
ml		_py2java	sc obj	convert python object into java
ml		callJavaFunc	sc func	call java function
ml		inherit_doc	cls	a decorator that makes a class inherit documentation from its parents
ml	ClusteringSummary	predictionCol		name for column of predicted clusters in predictions
ml	ClusteringSummary	predictions		dataframe produced by the model's transform method
ml	ClusteringSummary	featuresCol		name for column of features in predictions
ml	ClusteringSummary	k		the number of clusters the model was trained with
ml	ClusteringSummary	cluster		dataframe of predicted cluster centers for each training data point
ml	ClusteringSummary	clusterSizes		size of number of data points in each cluster
ml	GaussianMixtureModel	weights		weight for each gaussian distribution in the mixture
ml	GaussianMixtureModel	gaussiansDF		retrieve gaussian distributions as a dataframe
ml	GaussianMixtureModel	hasSummary		indicates whether a training summary exists for this model instance
ml	GaussianMixtureModel	summary		gets summary e g cluster assignments cluster sizes of the model trained on the
ml	GaussianMixture	__init__	featuresCol predictionCol k probabilityCol	__init__(self featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01 maxiter=100 seed=none)
ml	GaussianMixture	setParams	featuresCol predictionCol k probabilityCol	setparams(self featurescol="features", predictioncol="prediction", k=2 probabilitycol="probability", tol=0 01 maxiter=100 seed=none)
ml	GaussianMixture	setK	value	sets the value of :py attr k
ml	GaussianMixture	getK		gets the value of k
ml	GaussianMixtureSummary	probabilityCol		name for column of predicted probability of each cluster in predictions
ml	GaussianMixtureSummary	probability		dataframe of probabilities of each cluster for each training data point
ml	GaussianMixtureSummary	logLikelihood		total log-likelihood for this model on the given data
ml	KMeansModel	clusterCenters		get the cluster centers represented as a list of numpy arrays
ml	KMeansModel	computeCost	dataset	return the k-means cost sum of squared distances of points to their nearest center for this model on the given data
ml	KMeansModel	hasSummary		indicates whether a training summary exists for this model instance
ml	KMeansModel	summary		gets summary e g cluster assignments cluster sizes of the model trained on the
ml	KMeans	__init__	featuresCol predictionCol k initMode	__init__(self featurescol="features", predictioncol="prediction", k=2 initmode="k-means||", initsteps=2 tol=1e-4 maxiter=20 seed=none)
ml	KMeans	setParams	featuresCol predictionCol k initMode	setparams(self featurescol="features", predictioncol="prediction", k=2 initmode="k-means||", initsteps=2 tol=1e-4 maxiter=20 seed=none) sets params for kmeans
ml	KMeans	setK	value	sets the value of :py attr k
ml	KMeans	getK		gets the value of k
ml	KMeans	setInitMode	value	sets the value of :py attr initmode
ml	KMeans	getInitMode		gets the value of initmode
ml	KMeans	setInitSteps	value	sets the value of :py attr initsteps
ml	KMeans	getInitSteps		gets the value of initsteps
ml	BisectingKMeansModel	clusterCenters		get the cluster centers represented as a list of numpy arrays
ml	BisectingKMeansModel	computeCost	dataset	computes the sum of squared distances between the input points and their corresponding cluster centers
ml	BisectingKMeansModel	hasSummary		indicates whether a training summary exists for this model instance
ml	BisectingKMeansModel	summary		gets summary e g cluster assignments cluster sizes of the model trained on the
ml	BisectingKMeans	__init__	featuresCol predictionCol maxIter seed	__init__(self featurescol="features", predictioncol="prediction", maxiter=20 seed=none k=4 mindivisibleclustersize=1 0)
ml	BisectingKMeans	setParams	featuresCol predictionCol maxIter seed	setparams(self featurescol="features", predictioncol="prediction", maxiter=20 seed=none k=4 mindivisibleclustersize=1 0)
ml	BisectingKMeans	setK	value	sets the value of :py attr k
ml	BisectingKMeans	getK		gets the value of k or its default value
ml	BisectingKMeans	setMinDivisibleClusterSize	value	sets the value of :py attr mindivisibleclustersize
ml	BisectingKMeans	getMinDivisibleClusterSize		gets the value of mindivisibleclustersize or its default value
ml	LDAModel	isDistributed		indicates whether this instance is of type distributedldamodel
ml	LDAModel	vocabSize		vocabulary size number of terms or words in the vocabulary
ml	LDAModel	topicsMatrix		inferred topics where each topic is represented by a distribution over terms
ml	LDAModel	logLikelihood	dataset	calculates a lower bound on the log likelihood of the entire corpus
ml	LDAModel	logPerplexity	dataset	calculate an upper bound on perplexity lower is better
ml	LDAModel	describeTopics	maxTermsPerTopic	return the topics described by their top-weighted terms
ml	LDAModel	estimatedDocConcentration		value for :py attr lda docconcentration estimated from data
ml	DistributedLDAModel	toLocal		convert this distributed model to a local representation this discards info about the
ml	DistributedLDAModel	trainingLogLikelihood		log likelihood of the observed tokens in the training set given the current parameter estimates
ml	DistributedLDAModel	logPrior		log probability of the current parameter estimate
ml	DistributedLDAModel	getCheckpointFiles		if using checkpointing and :py attr lda keeplastcheckpoint is set to true then there may
ml	LDA	__init__	featuresCol maxIter seed checkpointInterval	__init__(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online", learningoffset=1024 0 learningdecay=0 51 subsamplingrate=0 05 optimizedocconcentration=true docconcentration=none topicconcentration=none topicdistributioncol="topicdistribution", keeplastcheckpoint=true):
ml	LDA	setParams	featuresCol maxIter seed checkpointInterval	setparams(self featurescol="features", maxiter=20 seed=none checkpointinterval=10 k=10 optimizer="online", learningoffset=1024 0 learningdecay=0 51 subsamplingrate=0 05 optimizedocconcentration=true docconcentration=none topicconcentration=none topicdistributioncol="topicdistribution", keeplastcheckpoint=true):
ml	LDA	setK	value	sets the value of :py attr k
ml	LDA	getK		gets the value of :py attr k or its default value
ml	LDA	setOptimizer	value	sets the value of :py attr optimizer
ml	LDA	getOptimizer		gets the value of :py attr optimizer or its default value
ml	LDA	setLearningOffset	value	sets the value of :py attr learningoffset
ml	LDA	getLearningOffset		gets the value of :py attr learningoffset or its default value
ml	LDA	setLearningDecay	value	sets the value of :py attr learningdecay
ml	LDA	getLearningDecay		gets the value of :py attr learningdecay or its default value
ml	LDA	setSubsamplingRate	value	sets the value of :py attr subsamplingrate
ml	LDA	getSubsamplingRate		gets the value of :py attr subsamplingrate or its default value
ml	LDA	setOptimizeDocConcentration	value	sets the value of :py attr optimizedocconcentration
ml	LDA	getOptimizeDocConcentration		gets the value of :py attr optimizedocconcentration or its default value
ml	LDA	setDocConcentration	value	sets the value of :py attr docconcentration
ml	LDA	getDocConcentration		gets the value of :py attr docconcentration or its default value
ml	LDA	setTopicConcentration	value	sets the value of :py attr topicconcentration
ml	LDA	getTopicConcentration		gets the value of :py attr topicconcentration or its default value
ml	LDA	setTopicDistributionCol	value	sets the value of :py attr topicdistributioncol
ml	LDA	getTopicDistributionCol		gets the value of :py attr topicdistributioncol or its default value
ml	LDA	setKeepLastCheckpoint	value	sets the value of :py attr keeplastcheckpoint
ml	LDA	getKeepLastCheckpoint		gets the value of :py attr keeplastcheckpoint or its default value
ml	HasSupport	setMinSupport	value	sets the value of :py attr minsupport
ml	HasSupport	getMinSupport		gets the value of minsupport or its default value
ml	HasConfidence	setMinConfidence	value	sets the value of :py attr minconfidence
ml	HasConfidence	getMinConfidence		gets the value of minconfidence or its default value
ml	HasItemsCol	setItemsCol	value	sets the value of :py attr itemscol
ml	HasItemsCol	getItemsCol		gets the value of itemscol or its default value
ml	FPGrowthModel	freqItemsets		dataframe with two columns * items - itemset of the same type as the input column
ml	FPGrowthModel	associationRules		data with three columns * antecedent - array of the same type as the input column
ml	FPGrowth	__init__	minSupport minConfidence itemsCol predictionCol	__init__(self minsupport=0 3 minconfidence=0 8 itemscol="items", predictioncol="prediction", numpartitions=none)
ml	FPGrowth	setParams	minSupport minConfidence itemsCol predictionCol	setparams(self minsupport=0 3 minconfidence=0 8 itemscol="items", predictioncol="prediction", numpartitions=none)
ml	Estimator	_fit	dataset	fits a model to the input dataset this is called by the default implementation of fit
ml	Estimator	fit	dataset params	fits a model to the input dataset with optional parameters
ml	Transformer	_transform	dataset	transforms the input dataset
ml	Transformer	transform	dataset params	transforms the input dataset with optional parameters
ml	LinearRegression	__init__	featuresCol labelCol predictionCol maxIter	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 regparam=0 0 elasticnetparam=0 0 tol=1e-6 fitintercept=true standardization=true solver="auto", weightcol=none aggregationdepth=2)
ml	LinearRegression	setParams	featuresCol labelCol predictionCol maxIter	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 regparam=0 0 elasticnetparam=0 0 tol=1e-6 fitintercept=true standardization=true solver="auto", weightcol=none aggregationdepth=2)
ml	LinearRegressionModel	summary		gets summary e g residuals mse r-squared of model on
ml	LinearRegressionModel	hasSummary		indicates whether a training summary exists for this model instance
ml	LinearRegressionModel	evaluate	dataset	evaluates the model on a test dataset
ml	LinearRegressionSummary	predictions		dataframe outputted by the model's transform method
ml	LinearRegressionSummary	predictionCol		field in "predictions" which gives the predicted value of the label at each instance
ml	LinearRegressionSummary	labelCol		field in "predictions" which gives the true label of each instance
ml	LinearRegressionSummary	featuresCol		field in "predictions" which gives the features of each instance as a vector
ml	LinearRegressionSummary	explainedVariance		returns the explained variance regression score
ml	LinearRegressionSummary	meanAbsoluteError		returns the mean absolute error which is a risk function corresponding to the expected value of the absolute error
ml	LinearRegressionSummary	meanSquaredError		returns the mean squared error which is a risk function corresponding to the expected value of the squared error
ml	LinearRegressionSummary	rootMeanSquaredError		returns the root mean squared error which is defined as the square root of the mean squared error
ml	LinearRegressionSummary	r2		returns r^2^, the coefficient of determination
ml	LinearRegressionSummary	residuals		residuals label - predicted value
ml	LinearRegressionSummary	numInstances		number of instances in dataframe predictions
ml	LinearRegressionSummary	devianceResiduals		the weighted residuals the usual residuals rescaled by the square root of the instance weights
ml	LinearRegressionSummary	coefficientStandardErrors		standard error of estimated coefficients and intercept
ml	LinearRegressionSummary	tValues		t-statistic of estimated coefficients and intercept
ml	LinearRegressionSummary	pValues		two-sided p-value of estimated coefficients and intercept
ml	LinearRegressionTrainingSummary	objectiveHistory		objective function scaled loss + regularization at each iteration
ml	LinearRegressionTrainingSummary	totalIterations		number of training iterations until termination
ml	IsotonicRegression	__init__	featuresCol labelCol predictionCol weightCol	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", weightcol=none isotonic=true featureindex=0):
ml	IsotonicRegression	setParams	featuresCol labelCol predictionCol weightCol	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", weightcol=none isotonic=true featureindex=0): set the params for isotonicregression
ml	IsotonicRegression	setIsotonic	value	sets the value of :py attr isotonic
ml	IsotonicRegression	getIsotonic		gets the value of isotonic or its default value
ml	IsotonicRegression	setFeatureIndex	value	sets the value of :py attr featureindex
ml	IsotonicRegression	getFeatureIndex		gets the value of featureindex or its default value
ml	IsotonicRegressionModel	boundaries		boundaries in increasing order for which predictions are known
ml	IsotonicRegressionModel	predictions		predictions associated with the boundaries at the same index monotone because of isotonic regression
ml	TreeEnsembleParams	setSubsamplingRate	value	sets the value of :py attr subsamplingrate
ml	TreeEnsembleParams	getSubsamplingRate		gets the value of subsamplingrate or its default value
ml	TreeRegressorParams	setImpurity	value	sets the value of :py attr impurity
ml	TreeRegressorParams	getImpurity		gets the value of impurity or its default value
ml	RandomForestParams	setNumTrees	value	sets the value of :py attr numtrees
ml	RandomForestParams	getNumTrees		gets the value of numtrees or its default value
ml	RandomForestParams	setFeatureSubsetStrategy	value	sets the value of :py attr featuresubsetstrategy
ml	RandomForestParams	getFeatureSubsetStrategy		gets the value of featuresubsetstrategy or its default value
ml	DecisionTreeRegressor	__init__	featuresCol labelCol predictionCol maxDepth	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)
ml	DecisionTreeRegressor	setParams	featuresCol labelCol predictionCol maxDepth	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", seed=none variancecol=none)
ml	DecisionTreeModel	numNodes		return number of nodes of the decision tree
ml	DecisionTreeModel	depth		return depth of the decision tree
ml	DecisionTreeModel	toDebugString		full description of model
ml	TreeEnsembleModel	trees		trees in this ensemble warning these have null parent estimators
ml	TreeEnsembleModel	getNumTrees		number of trees in ensemble
ml	TreeEnsembleModel	treeWeights		return the weights for each tree
ml	TreeEnsembleModel	totalNumNodes		total number of nodes summed over all trees in the ensemble
ml	TreeEnsembleModel	toDebugString		full description of model
ml	DecisionTreeRegressionModel	featureImportances		estimate of the importance of each feature
ml	RandomForestRegressor	__init__	featuresCol labelCol predictionCol maxDepth	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", subsamplingrate=1 0 seed=none numtrees=20 featuresubsetstrategy="auto")
ml	RandomForestRegressor	setParams	featuresCol labelCol predictionCol maxDepth	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", subsamplingrate=1 0 seed=none numtrees=20 featuresubsetstrategy="auto")
ml	RandomForestRegressionModel	trees		trees in this ensemble warning these have null parent estimators
ml	RandomForestRegressionModel	featureImportances		estimate of the importance of each feature
ml	GBTRegressor	__init__	featuresCol labelCol predictionCol maxDepth	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false subsamplingrate=1 0 checkpointinterval=10 losstype="squared", maxiter=20 stepsize=0 1 seed=none impurity="variance")
ml	GBTRegressor	setParams	featuresCol labelCol predictionCol maxDepth	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false subsamplingrate=1 0 checkpointinterval=10 losstype="squared", maxiter=20 stepsize=0 1 seed=none impurity="variance")
ml	GBTRegressor	setLossType	value	sets the value of :py attr losstype
ml	GBTRegressor	getLossType		gets the value of losstype or its default value
ml	GBTRegressionModel	featureImportances		estimate of the importance of each feature
ml	GBTRegressionModel	trees		trees in this ensemble warning these have null parent estimators
ml	AFTSurvivalRegression	__init__	featuresCol labelCol predictionCol fitIntercept	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor", quantileprobabilities=[0 01 0 05 0 1 0 25 0 5 0 75 0 9 0 95 0 99], quantilescol=none aggregationdepth=2)
ml	AFTSurvivalRegression	setParams	featuresCol labelCol predictionCol fitIntercept	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", fitintercept=true maxiter=100 tol=1e-6 censorcol="censor", quantileprobabilities=[0 01 0 05 0 1 0 25 0 5 0 75 0 9 0 95 0 99], quantilescol=none aggregationdepth=2):
ml	AFTSurvivalRegression	setCensorCol	value	sets the value of :py attr censorcol
ml	AFTSurvivalRegression	getCensorCol		gets the value of censorcol or its default value
ml	AFTSurvivalRegression	setQuantileProbabilities	value	sets the value of :py attr quantileprobabilities
ml	AFTSurvivalRegression	getQuantileProbabilities		gets the value of quantileprobabilities or its default value
ml	AFTSurvivalRegression	setQuantilesCol	value	sets the value of :py attr quantilescol
ml	AFTSurvivalRegression	getQuantilesCol		gets the value of quantilescol or its default value
ml	AFTSurvivalRegressionModel	scale		model scale paramter
ml	GeneralizedLinearRegression	__init__	labelCol featuresCol predictionCol family	__init__(self labelcol="label", featurescol="features", predictioncol="prediction", family="gaussian", link=none fitintercept=true maxiter=25 tol=1e-6 regparam=0 0 weightcol=none solver="irls", linkpredictioncol=none variancepower=0 0 linkpower=none)
ml	GeneralizedLinearRegression	setParams	labelCol featuresCol predictionCol family	setparams(self labelcol="label", featurescol="features", predictioncol="prediction", family="gaussian", link=none fitintercept=true maxiter=25 tol=1e-6 regparam=0 0 weightcol=none solver="irls", linkpredictioncol=none variancepower=0 0 linkpower=none)
ml	GeneralizedLinearRegression	setFamily	value	sets the value of :py attr family
ml	GeneralizedLinearRegression	getFamily		gets the value of family or its default value
ml	GeneralizedLinearRegression	setLinkPredictionCol	value	sets the value of :py attr linkpredictioncol
ml	GeneralizedLinearRegression	getLinkPredictionCol		gets the value of linkpredictioncol or its default value
ml	GeneralizedLinearRegression	setLink	value	sets the value of :py attr link
ml	GeneralizedLinearRegression	getLink		gets the value of link or its default value
ml	GeneralizedLinearRegression	setVariancePower	value	sets the value of :py attr variancepower
ml	GeneralizedLinearRegression	getVariancePower		gets the value of variancepower or its default value
ml	GeneralizedLinearRegression	setLinkPower	value	sets the value of :py attr linkpower
ml	GeneralizedLinearRegression	getLinkPower		gets the value of linkpower or its default value
ml	GeneralizedLinearRegressionModel	summary		gets summary e g residuals deviance pvalues of model on
ml	GeneralizedLinearRegressionModel	hasSummary		indicates whether a training summary exists for this model instance
ml	GeneralizedLinearRegressionModel	evaluate	dataset	evaluates the model on a test dataset
ml	GeneralizedLinearRegressionSummary	predictions		predictions output by the model's transform method
ml	GeneralizedLinearRegressionSummary	predictionCol		field in :py attr predictions which gives the predicted value of each instance
ml	GeneralizedLinearRegressionSummary	rank		the numeric rank of the fitted linear model
ml	GeneralizedLinearRegressionSummary	degreesOfFreedom		degrees of freedom
ml	GeneralizedLinearRegressionSummary	residualDegreeOfFreedom		the residual degrees of freedom
ml	GeneralizedLinearRegressionSummary	residualDegreeOfFreedomNull		the residual degrees of freedom for the null model
ml	GeneralizedLinearRegressionSummary	residuals	residualsType	get the residuals of the fitted model by type
ml	GeneralizedLinearRegressionSummary	nullDeviance		the deviance for the null model
ml	GeneralizedLinearRegressionSummary	deviance		the deviance for the fitted model
ml	GeneralizedLinearRegressionSummary	dispersion		the dispersion of the fitted model
ml	GeneralizedLinearRegressionSummary	aic		akaike's "an information criterion" aic for the fitted model
ml	GeneralizedLinearRegressionTrainingSummary	numIterations		number of training iterations
ml	GeneralizedLinearRegressionTrainingSummary	solver		the numeric solver used for training
ml	GeneralizedLinearRegressionTrainingSummary	coefficientStandardErrors		standard error of estimated coefficients and intercept
ml	GeneralizedLinearRegressionTrainingSummary	tValues		t-statistic of estimated coefficients and intercept
ml	GeneralizedLinearRegressionTrainingSummary	pValues		two-sided p-value of estimated coefficients and intercept
ml	TestParams	setParams	seed	setparams(self seed=none) sets params for this test
ml	OtherTestParams	setParams	seed	setparams(self seed=none) sets params for this test
ml	EvaluatorTests	test_java_params		this tests a bug fixed by spark-18274 which causes multiple copies of a params instance in python to be linked to the same java instance
ml	PersistenceTest	_compare_params	m1 m2 param	compare 2 ml params instances for the given param and assert both have the same param value and parent
ml	PersistenceTest	_compare_pipelines	m1 m2	compare 2 ml types asserting that they are equivalent
ml	LDATest	_compare	m1 m2	temp method for comparing instances
ml	JavaClassificationModel	numClasses		number of classes values which the label can take
ml	LinearSVC	__init__	featuresCol labelCol predictionCol maxIter	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 regparam=0 0 tol=1e-6 rawpredictioncol="rawprediction", fitintercept=true standardization=true threshold=0 0 weightcol=none aggregationdepth=2):
ml	LinearSVC	setParams	featuresCol labelCol predictionCol maxIter	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 regparam=0 0 tol=1e-6 rawpredictioncol="rawprediction", fitintercept=true standardization=true threshold=0 0 weightcol=none aggregationdepth=2):
ml	LinearSVCModel	coefficients		model coefficients of linear svm classifier
ml	LinearSVCModel	intercept		model intercept of linear svm classifier
ml	LogisticRegression	__init__	featuresCol labelCol predictionCol maxIter	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 regparam=0 0 elasticnetparam=0 0 tol=1e-6 fitintercept=true threshold=0 5 thresholds=none probabilitycol="probability", rawpredictioncol="rawprediction", standardization=true weightcol=none aggregationdepth=2 family="auto")
ml	LogisticRegression	setParams	featuresCol labelCol predictionCol maxIter	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 regparam=0 0 elasticnetparam=0 0 tol=1e-6 fitintercept=true threshold=0 5 thresholds=none probabilitycol="probability", rawpredictioncol="rawprediction", standardization=true weightcol=none aggregationdepth=2 family="auto")
ml	LogisticRegression	setThreshold	value	sets the value of :py attr threshold
ml	LogisticRegression	getThreshold		get threshold for binary classification
ml	LogisticRegression	setThresholds	value	sets the value of :py attr thresholds
ml	LogisticRegression	getThresholds		if :py attr thresholds is set return its value
ml	LogisticRegression	setFamily	value	sets the value of :py attr family
ml	LogisticRegression	getFamily		gets the value of :py attr family or its default value
ml	LogisticRegressionModel	coefficients		model coefficients of binomial logistic regression
ml	LogisticRegressionModel	intercept		model intercept of binomial logistic regression
ml	LogisticRegressionModel	summary		gets summary (e g accuracy/precision/recall objective history total iterations) of model
ml	LogisticRegressionModel	hasSummary		indicates whether a training summary exists for this model instance
ml	LogisticRegressionModel	evaluate	dataset	evaluates the model on a test dataset
ml	LogisticRegressionSummary	predictions		dataframe outputted by the model's transform method
ml	LogisticRegressionSummary	probabilityCol		field in "predictions" which gives the probability of each class as a vector
ml	LogisticRegressionSummary	labelCol		field in "predictions" which gives the true label of each instance
ml	LogisticRegressionSummary	featuresCol		field in "predictions" which gives the features of each instance as a vector
ml	LogisticRegressionTrainingSummary	objectiveHistory		objective function scaled loss + regularization at each iteration
ml	LogisticRegressionTrainingSummary	totalIterations		number of training iterations until termination
ml	BinaryLogisticRegressionSummary	roc		returns the receiver operating characteristic roc curve which is a dataframe having two fields fpr tpr with
ml	BinaryLogisticRegressionSummary	areaUnderROC		computes the area under the receiver operating characteristic roc curve
ml	BinaryLogisticRegressionSummary	pr		returns the precision-recall curve which is a dataframe containing two fields recall precision with (0
ml	BinaryLogisticRegressionSummary	fMeasureByThreshold		returns a dataframe with two fields threshold f-measure curve with beta = 1
ml	BinaryLogisticRegressionSummary	precisionByThreshold		returns a dataframe with two fields threshold precision curve
ml	BinaryLogisticRegressionSummary	recallByThreshold		returns a dataframe with two fields threshold recall curve
ml	TreeClassifierParams	setImpurity	value	sets the value of :py attr impurity
ml	TreeClassifierParams	getImpurity		gets the value of impurity or its default value
ml	DecisionTreeClassifier	__init__	featuresCol labelCol predictionCol probabilityCol	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", seed=none)
ml	DecisionTreeClassifier	setParams	featuresCol labelCol predictionCol probabilityCol	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", seed=none)
ml	DecisionTreeClassificationModel	featureImportances		estimate of the importance of each feature
ml	RandomForestClassifier	__init__	featuresCol labelCol predictionCol probabilityCol	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1 0)
ml	RandomForestClassifier	setParams	featuresCol labelCol predictionCol probabilityCol	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0)
ml	RandomForestClassificationModel	featureImportances		estimate of the importance of each feature
ml	RandomForestClassificationModel	trees		trees in this ensemble warning these have null parent estimators
ml	GBTClassifier	__init__	featuresCol labelCol predictionCol maxDepth	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic", maxiter=20 stepsize=0 1 seed=none subsamplingrate=1 0)
ml	GBTClassifier	setParams	featuresCol labelCol predictionCol maxDepth	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic", maxiter=20 stepsize=0 1 seed=none subsamplingrate=1 0)
ml	GBTClassifier	setLossType	value	sets the value of :py attr losstype
ml	GBTClassifier	getLossType		gets the value of losstype or its default value
ml	GBTClassificationModel	featureImportances		estimate of the importance of each feature
ml	GBTClassificationModel	trees		trees in this ensemble warning these have null parent estimators
ml	NaiveBayes	__init__	featuresCol labelCol predictionCol probabilityCol	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1 0 modeltype="multinomial", thresholds=none weightcol=none)
ml	NaiveBayes	setParams	featuresCol labelCol predictionCol probabilityCol	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1 0 modeltype="multinomial", thresholds=none weightcol=none)
ml	NaiveBayes	setSmoothing	value	sets the value of :py attr smoothing
ml	NaiveBayes	getSmoothing		gets the value of smoothing or its default value
ml	NaiveBayes	setModelType	value	sets the value of :py attr modeltype
ml	NaiveBayes	getModelType		gets the value of modeltype or its default value
ml	NaiveBayesModel	pi		log of class priors
ml	NaiveBayesModel	theta		log of class conditional probabilities
ml	MultilayerPerceptronClassifier	__init__	featuresCol labelCol predictionCol maxIter	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 tol=1e-6 seed=none layers=none blocksize=128 stepsize=0 03 solver="l-bfgs", initialweights=none)
ml	MultilayerPerceptronClassifier	setParams	featuresCol labelCol predictionCol maxIter	setparams(self featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 tol=1e-6 seed=none layers=none blocksize=128 stepsize=0 03 solver="l-bfgs", initialweights=none)
ml	MultilayerPerceptronClassifier	setLayers	value	sets the value of :py attr layers
ml	MultilayerPerceptronClassifier	getLayers		gets the value of layers or its default value
ml	MultilayerPerceptronClassifier	setBlockSize	value	sets the value of :py attr blocksize
ml	MultilayerPerceptronClassifier	getBlockSize		gets the value of blocksize or its default value
ml	MultilayerPerceptronClassifier	setStepSize	value	sets the value of :py attr stepsize
ml	MultilayerPerceptronClassifier	getStepSize		gets the value of stepsize or its default value
ml	MultilayerPerceptronClassifier	setSolver	value	sets the value of :py attr solver
ml	MultilayerPerceptronClassifier	getSolver		gets the value of solver or its default value
ml	MultilayerPerceptronClassifier	setInitialWeights	value	sets the value of :py attr initialweights
ml	MultilayerPerceptronClassifier	getInitialWeights		gets the value of initialweights or its default value
ml	MultilayerPerceptronClassificationModel	layers		array of layer sizes including input and output layers
ml	MultilayerPerceptronClassificationModel	weights		the weights of layers
ml	OneVsRestParams	setClassifier	value	sets the value of :py attr classifier
ml	OneVsRestParams	getClassifier		gets the value of classifier or its default value
ml	OneVsRest	__init__	featuresCol labelCol predictionCol classifier	__init__(self featurescol="features", labelcol="label", predictioncol="prediction", classifier=none)
ml	OneVsRest	setParams	featuresCol labelCol predictionCol classifier	setparams(self featurescol=none labelcol=none predictioncol=none classifier=none): sets params for onevsrest
ml	OneVsRest	copy	extra	creates a copy of this instance with a randomly generated uid and some extra params
ml	OneVsRest	write		returns an mlwriter instance for this ml instance
ml	OneVsRest	save	path	save this ml instance to the given path a shortcut of write() save path
ml	OneVsRest	read	cls	returns an mlreader instance for this class
ml	OneVsRest	_from_java	cls java_stage	given a java onevsrest create and return a python wrapper of it
ml	OneVsRest	_to_java		transfer this instance to a java onevsrest used for ml persistence
ml	OneVsRestModel	copy	extra	creates a copy of this instance with a randomly generated uid and some extra params
ml	OneVsRestModel	write		returns an mlwriter instance for this ml instance
ml	OneVsRestModel	save	path	save this ml instance to the given path a shortcut of write() save path
ml	OneVsRestModel	read	cls	returns an mlreader instance for this class
ml	OneVsRestModel	_from_java	cls java_stage	given a java onevsrestmodel create and return a python wrapper of it
ml	OneVsRestModel	_to_java		transfer this instance to a java onevsrestmodel used for ml persistence
ml	Evaluator	_evaluate	dataset	evaluates the output
ml	Evaluator	evaluate	dataset params	evaluates the output with optional parameters
ml	Evaluator	isLargerBetter		indicates whether the metric returned by :py meth evaluate should be maximized true default or minimized false
ml	JavaEvaluator	_evaluate	dataset	evaluates the output
ml	BinaryClassificationEvaluator	__init__	rawPredictionCol labelCol metricName	__init__(self rawpredictioncol="rawprediction", labelcol="label", metricname="areaunderroc")
ml	BinaryClassificationEvaluator	setMetricName	value	sets the value of :py attr metricname
ml	BinaryClassificationEvaluator	getMetricName		gets the value of metricname or its default value
ml	BinaryClassificationEvaluator	setParams	rawPredictionCol labelCol metricName	setparams(self rawpredictioncol="rawprediction", labelcol="label", metricname="areaunderroc") sets params for binary classification evaluator
ml	RegressionEvaluator	__init__	predictionCol labelCol metricName	__init__(self predictioncol="prediction", labelcol="label", metricname="rmse")
ml	RegressionEvaluator	setMetricName	value	sets the value of :py attr metricname
ml	RegressionEvaluator	getMetricName		gets the value of metricname or its default value
ml	RegressionEvaluator	setParams	predictionCol labelCol metricName	setparams(self predictioncol="prediction", labelcol="label", metricname="rmse") sets params for regression evaluator
ml	MulticlassClassificationEvaluator	__init__	predictionCol labelCol metricName	__init__(self predictioncol="prediction", labelcol="label", metricname="f1")
ml	MulticlassClassificationEvaluator	setMetricName	value	sets the value of :py attr metricname
ml	MulticlassClassificationEvaluator	getMetricName		gets the value of metricname or its default value
ml	MulticlassClassificationEvaluator	setParams	predictionCol labelCol metricName	setparams(self predictioncol="prediction", labelcol="label", metricname="f1") sets params for multiclass classification evaluator
ml	ParamGridBuilder	addGrid	param values	sets the given parameters in this grid to fixed values
ml	ParamGridBuilder	baseOn		sets the given parameters in this grid to fixed values
ml	ParamGridBuilder	build		builds and returns all combinations of parameters specified by the param grid
ml	ValidatorParams	setEstimator	value	sets the value of :py attr estimator
ml	ValidatorParams	getEstimator		gets the value of estimator or its default value
ml	ValidatorParams	setEstimatorParamMaps	value	sets the value of :py attr estimatorparammaps
ml	ValidatorParams	getEstimatorParamMaps		gets the value of estimatorparammaps or its default value
ml	ValidatorParams	setEvaluator	value	sets the value of :py attr evaluator
ml	ValidatorParams	getEvaluator		gets the value of evaluator or its default value
ml	CrossValidator	__init__	estimator estimatorParamMaps evaluator numFolds	__init__(self estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none)
ml	CrossValidator	setParams	estimator estimatorParamMaps evaluator numFolds	setparams(self estimator=none estimatorparammaps=none evaluator=none numfolds=3 seed=none): sets params for cross validator
ml	CrossValidator	setNumFolds	value	sets the value of :py attr numfolds
ml	CrossValidator	getNumFolds		gets the value of numfolds or its default value
ml	CrossValidator	copy	extra	creates a copy of this instance with a randomly generated uid and some extra params
ml	CrossValidatorModel	copy	extra	creates a copy of this instance with a randomly generated uid and some extra params
ml	TrainValidationSplit	__init__	estimator estimatorParamMaps evaluator trainRatio	__init__(self estimator=none estimatorparammaps=none evaluator=none trainratio=0 75 seed=none)
ml	TrainValidationSplit	setParams	estimator estimatorParamMaps evaluator trainRatio	setparams(self estimator=none estimatorparammaps=none evaluator=none trainratio=0 75 seed=none):
ml	TrainValidationSplit	setTrainRatio	value	sets the value of :py attr trainratio
ml	TrainValidationSplit	getTrainRatio		gets the value of trainratio or its default value
ml	TrainValidationSplit	copy	extra	creates a copy of this instance with a randomly generated uid and some extra params
ml	TrainValidationSplitModel	copy	extra	creates a copy of this instance with a randomly generated uid and some extra params
ml	Pipeline	setStages	value	set pipeline stages
ml	Pipeline	getStages		get pipeline stages
ml	Pipeline	setParams	stages	setparams(self stages=none) sets params for pipeline
ml	Pipeline	copy	extra	creates a copy of this instance
ml	Pipeline	write		returns an mlwriter instance for this ml instance
ml	Pipeline	save	path	save this ml instance to the given path a shortcut of write() save path
ml	Pipeline	read	cls	returns an mlreader instance for this class
ml	Pipeline	_from_java	cls java_stage	given a java pipeline create and return a python wrapper of it
ml	Pipeline	_to_java		transfer this instance to a java pipeline used for ml persistence
ml	PipelineModel	copy	extra	creates a copy of this instance
ml	PipelineModel	write		returns an mlwriter instance for this ml instance
ml	PipelineModel	save	path	save this ml instance to the given path a shortcut of write() save path
ml	PipelineModel	read	cls	returns an mlreader instance for this class
ml	PipelineModel	_from_java	cls java_stage	given a java pipelinemodel create and return a python wrapper of it
ml	PipelineModel	_to_java		transfer this instance to a java pipelinemodel used for ml persistence
ml	Binarizer	__init__	threshold inputCol outputCol	__init__(self threshold=0 0 inputcol=none outputcol=none)
ml	Binarizer	setParams	threshold inputCol outputCol	setparams(self threshold=0 0 inputcol=none outputcol=none)
ml	Binarizer	setThreshold	value	sets the value of :py attr threshold
ml	Binarizer	getThreshold		gets the value of threshold or its default value
ml	LSHParams	setNumHashTables	value	sets the value of :py attr numhashtables
ml	LSHParams	getNumHashTables		gets the value of numhashtables or its default value
ml	LSHModel	approxNearestNeighbors	dataset key numNearestNeighbors distCol	given a large dataset and an item approximately find at most k items which have the closest distance to the item
ml	LSHModel	approxSimilarityJoin	datasetA datasetB threshold distCol	join two datasets to approximately find all pairs of rows whose distance are smaller than the threshold
ml	BucketedRandomProjectionLSH	__init__	inputCol outputCol seed numHashTables	__init__(self inputcol=none outputcol=none seed=none numhashtables=1 bucketlength=none)
ml	BucketedRandomProjectionLSH	setParams	inputCol outputCol seed numHashTables	setparams(self inputcol=none outputcol=none seed=none numhashtables=1 bucketlength=none) sets params for this bucketedrandomprojectionlsh
ml	BucketedRandomProjectionLSH	setBucketLength	value	sets the value of :py attr bucketlength
ml	BucketedRandomProjectionLSH	getBucketLength		gets the value of bucketlength or its default value
ml	Bucketizer	__init__	splits inputCol outputCol handleInvalid	__init__(self splits=none inputcol=none outputcol=none handleinvalid="error")
ml	Bucketizer	setParams	splits inputCol outputCol handleInvalid	setparams(self splits=none inputcol=none outputcol=none handleinvalid="error") sets params for this bucketizer
ml	Bucketizer	setSplits	value	sets the value of :py attr splits
ml	Bucketizer	getSplits		gets the value of threshold or its default value
ml	Bucketizer	setHandleInvalid	value	sets the value of :py attr handleinvalid
ml	Bucketizer	getHandleInvalid		gets the value of :py attr handleinvalid or its default value
ml	CountVectorizer	__init__	minTF minDF vocabSize binary	__init__(self mintf=1 0 mindf=1 0 vocabsize=1 << 18 binary=false inputcol=none outputcol=none)
ml	CountVectorizer	setParams	minTF minDF vocabSize binary	setparams(self mintf=1 0 mindf=1 0 vocabsize=1 << 18 binary=false inputcol=none outputcol=none)
ml	CountVectorizer	setMinTF	value	sets the value of :py attr mintf
ml	CountVectorizer	getMinTF		gets the value of mintf or its default value
ml	CountVectorizer	setMinDF	value	sets the value of :py attr mindf
ml	CountVectorizer	getMinDF		gets the value of mindf or its default value
ml	CountVectorizer	setVocabSize	value	sets the value of :py attr vocabsize
ml	CountVectorizer	getVocabSize		gets the value of vocabsize or its default value
ml	CountVectorizer	setBinary	value	sets the value of :py attr binary
ml	CountVectorizer	getBinary		gets the value of binary or its default value
ml	CountVectorizerModel	vocabulary		an array of terms in the vocabulary
ml	DCT	__init__	inverse inputCol outputCol	__init__(self inverse=false inputcol=none outputcol=none)
ml	DCT	setParams	inverse inputCol outputCol	setparams(self inverse=false inputcol=none outputcol=none) sets params for this dct
ml	DCT	setInverse	value	sets the value of :py attr inverse
ml	DCT	getInverse		gets the value of inverse or its default value
ml	ElementwiseProduct	__init__	scalingVec inputCol outputCol	__init__(self scalingvec=none inputcol=none outputcol=none)
ml	ElementwiseProduct	setParams	scalingVec inputCol outputCol	setparams(self scalingvec=none inputcol=none outputcol=none) sets params for this elementwiseproduct
ml	ElementwiseProduct	setScalingVec	value	sets the value of :py attr scalingvec
ml	ElementwiseProduct	getScalingVec		gets the value of scalingvec or its default value
ml	HashingTF	__init__	numFeatures binary inputCol outputCol	__init__(self numfeatures=1 << 18 binary=false inputcol=none outputcol=none)
ml	HashingTF	setParams	numFeatures binary inputCol outputCol	setparams(self numfeatures=1 << 18 binary=false inputcol=none outputcol=none) sets params for this hashingtf
ml	HashingTF	setBinary	value	sets the value of :py attr binary
ml	HashingTF	getBinary		gets the value of binary or its default value
ml	IDF	__init__	minDocFreq inputCol outputCol	__init__(self mindocfreq=0 inputcol=none outputcol=none)
ml	IDF	setParams	minDocFreq inputCol outputCol	setparams(self mindocfreq=0 inputcol=none outputcol=none) sets params for this idf
ml	IDF	setMinDocFreq	value	sets the value of :py attr mindocfreq
ml	IDF	getMinDocFreq		gets the value of mindocfreq or its default value
ml	IDFModel	idf		returns the idf vector
ml	Imputer	__init__	strategy missingValue inputCols outputCols	__init__(self strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none):
ml	Imputer	setParams	strategy missingValue inputCols outputCols	setparams(self strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none) sets params for this imputer
ml	Imputer	setOutputCols	value	sets the value of :py attr outputcols
ml	Imputer	getOutputCols		gets the value of :py attr outputcols or its default value
ml	Imputer	setStrategy	value	sets the value of :py attr strategy
ml	Imputer	getStrategy		gets the value of :py attr strategy or its default value
ml	Imputer	setMissingValue	value	sets the value of :py attr missingvalue
ml	Imputer	getMissingValue		gets the value of :py attr missingvalue or its default value
ml	ImputerModel	surrogateDF		returns a dataframe containing inputcols and their corresponding surrogates which are used to replace the missing values in the input dataframe
ml	MaxAbsScaler	__init__	inputCol outputCol	__init__(self inputcol=none outputcol=none)
ml	MaxAbsScaler	setParams	inputCol outputCol	setparams(self inputcol=none outputcol=none) sets params for this maxabsscaler
ml	MaxAbsScalerModel	maxAbs		max abs vector
ml	MinHashLSH	__init__	inputCol outputCol seed numHashTables	__init__(self inputcol=none outputcol=none seed=none numhashtables=1)
ml	MinHashLSH	setParams	inputCol outputCol seed numHashTables	setparams(self inputcol=none outputcol=none seed=none numhashtables=1) sets params for this minhashlsh
ml	MinMaxScaler	__init__	min max inputCol outputCol	__init__(self min=0 0 max=1 0 inputcol=none outputcol=none)
ml	MinMaxScaler	setParams	min max inputCol outputCol	setparams(self min=0 0 max=1 0 inputcol=none outputcol=none)
ml	MinMaxScaler	setMin	value	sets the value of :py attr min
ml	MinMaxScaler	getMin		gets the value of min or its default value
ml	MinMaxScaler	setMax	value	sets the value of :py attr max
ml	MinMaxScaler	getMax		gets the value of max or its default value
ml	MinMaxScalerModel	originalMin		min value for each original column during fitting
ml	MinMaxScalerModel	originalMax		max value for each original column during fitting
ml	NGram	__init__	n inputCol outputCol	__init__(self n=2 inputcol=none outputcol=none)
ml	NGram	setParams	n inputCol outputCol	setparams(self n=2 inputcol=none outputcol=none) sets params for this ngram
ml	NGram	setN	value	sets the value of :py attr n
ml	NGram	getN		gets the value of n or its default value
ml	Normalizer	__init__	p inputCol outputCol	__init__(self p=2 0 inputcol=none outputcol=none)
ml	Normalizer	setParams	p inputCol outputCol	setparams(self p=2 0 inputcol=none outputcol=none)
ml	Normalizer	setP	value	sets the value of :py attr p
ml	Normalizer	getP		gets the value of p or its default value
ml	OneHotEncoder	__init__	dropLast inputCol outputCol	__init__(self droplast=true inputcol=none outputcol=none)
ml	OneHotEncoder	setParams	dropLast inputCol outputCol	setparams(self droplast=true inputcol=none outputcol=none) sets params for this onehotencoder
ml	OneHotEncoder	setDropLast	value	sets the value of :py attr droplast
ml	OneHotEncoder	getDropLast		gets the value of droplast or its default value
ml	PolynomialExpansion	__init__	degree inputCol outputCol	__init__(self degree=2 inputcol=none outputcol=none)
ml	PolynomialExpansion	setParams	degree inputCol outputCol	setparams(self degree=2 inputcol=none outputcol=none) sets params for this polynomialexpansion
ml	PolynomialExpansion	setDegree	value	sets the value of :py attr degree
ml	PolynomialExpansion	getDegree		gets the value of degree or its default value
ml	QuantileDiscretizer	__init__	numBuckets inputCol outputCol relativeError	__init__(self numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")
ml	QuantileDiscretizer	setParams	numBuckets inputCol outputCol relativeError	setparams(self numbuckets=2 inputcol=none outputcol=none relativeerror=0 001 handleinvalid="error")
ml	QuantileDiscretizer	setNumBuckets	value	sets the value of :py attr numbuckets
ml	QuantileDiscretizer	getNumBuckets		gets the value of numbuckets or its default value
ml	QuantileDiscretizer	setRelativeError	value	sets the value of :py attr relativeerror
ml	QuantileDiscretizer	getRelativeError		gets the value of relativeerror or its default value
ml	QuantileDiscretizer	setHandleInvalid	value	sets the value of :py attr handleinvalid
ml	QuantileDiscretizer	getHandleInvalid		gets the value of :py attr handleinvalid or its default value
ml	QuantileDiscretizer	_create_model	java_model	private method to convert the java_model to a python model
ml	RegexTokenizer	__init__	minTokenLength gaps pattern inputCol	__init__(self mintokenlength=1 gaps=true pattern="\s+", inputcol=none outputcol=none tolowercase=true)
ml	RegexTokenizer	setParams	minTokenLength gaps pattern inputCol	setparams(self mintokenlength=1 gaps=true pattern="\s+", inputcol=none outputcol=none tolowercase=true) sets params for this regextokenizer
ml	RegexTokenizer	setMinTokenLength	value	sets the value of :py attr mintokenlength
ml	RegexTokenizer	getMinTokenLength		gets the value of mintokenlength or its default value
ml	RegexTokenizer	setGaps	value	sets the value of :py attr gaps
ml	RegexTokenizer	getGaps		gets the value of gaps or its default value
ml	RegexTokenizer	setPattern	value	sets the value of :py attr pattern
ml	RegexTokenizer	getPattern		gets the value of pattern or its default value
ml	RegexTokenizer	setToLowercase	value	sets the value of :py attr tolowercase
ml	RegexTokenizer	getToLowercase		gets the value of tolowercase or its default value
ml	SQLTransformer	setParams	statement	setparams(self statement=none) sets params for this sqltransformer
ml	SQLTransformer	setStatement	value	sets the value of :py attr statement
ml	SQLTransformer	getStatement		gets the value of statement or its default value
ml	StandardScaler	__init__	withMean withStd inputCol outputCol	__init__(self withmean=false withstd=true inputcol=none outputcol=none)
ml	StandardScaler	setParams	withMean withStd inputCol outputCol	setparams(self withmean=false withstd=true inputcol=none outputcol=none) sets params for this standardscaler
ml	StandardScaler	setWithMean	value	sets the value of :py attr withmean
ml	StandardScaler	getWithMean		gets the value of withmean or its default value
ml	StandardScaler	setWithStd	value	sets the value of :py attr withstd
ml	StandardScaler	getWithStd		gets the value of withstd or its default value
ml	StandardScalerModel	std		standard deviation of the standardscalermodel
ml	StandardScalerModel	mean		mean of the standardscalermodel
ml	StringIndexer	__init__	inputCol outputCol handleInvalid	__init__(self inputcol=none outputcol=none handleinvalid="error")
ml	StringIndexer	setParams	inputCol outputCol handleInvalid	setparams(self inputcol=none outputcol=none handleinvalid="error") sets params for this stringindexer
ml	StringIndexerModel	labels		ordered list of labels corresponding to indices to be assigned
ml	IndexToString	__init__	inputCol outputCol labels	__init__(self inputcol=none outputcol=none labels=none)
ml	IndexToString	setParams	inputCol outputCol labels	setparams(self inputcol=none outputcol=none labels=none) sets params for this indextostring
ml	IndexToString	setLabels	value	sets the value of :py attr labels
ml	IndexToString	getLabels		gets the value of :py attr labels or its default value
ml	StopWordsRemover	__init__	inputCol outputCol stopWords caseSensitive	__init__(self inputcol=none outputcol=none stopwords=none casesensitive=false)
ml	StopWordsRemover	setParams	inputCol outputCol stopWords caseSensitive	setparams(self inputcol=none outputcol=none stopwords=none casesensitive=false) sets params for this stopwordremover
ml	StopWordsRemover	setStopWords	value	sets the value of :py attr stopwords
ml	StopWordsRemover	getStopWords		gets the value of :py attr stopwords or its default value
ml	StopWordsRemover	setCaseSensitive	value	sets the value of :py attr casesensitive
ml	StopWordsRemover	getCaseSensitive		gets the value of :py attr casesensitive or its default value
ml	StopWordsRemover	loadDefaultStopWords	language	loads the default stop words for the given language
ml	Tokenizer	__init__	inputCol outputCol	__init__(self inputcol=none outputcol=none)
ml	Tokenizer	setParams	inputCol outputCol	setparams(self inputcol=none outputcol=none) sets params for this tokenizer
ml	VectorAssembler	__init__	inputCols outputCol	__init__(self inputcols=none outputcol=none)
ml	VectorAssembler	setParams	inputCols outputCol	setparams(self inputcols=none outputcol=none) sets params for this vectorassembler
ml	VectorIndexer	__init__	maxCategories inputCol outputCol	__init__(self maxcategories=20 inputcol=none outputcol=none)
ml	VectorIndexer	setParams	maxCategories inputCol outputCol	setparams(self maxcategories=20 inputcol=none outputcol=none) sets params for this vectorindexer
ml	VectorIndexer	setMaxCategories	value	sets the value of :py attr maxcategories
ml	VectorIndexer	getMaxCategories		gets the value of maxcategories or its default value
ml	VectorIndexerModel	numFeatures		number of features i e length of vectors which this transforms
ml	VectorIndexerModel	categoryMaps		feature value index keys are categorical feature indices column indices
ml	VectorSlicer	__init__	inputCol outputCol indices names	__init__(self inputcol=none outputcol=none indices=none names=none)
ml	VectorSlicer	setParams	inputCol outputCol indices names	setparams(self inputcol=none outputcol=none indices=none names=none): sets params for this vectorslicer
ml	VectorSlicer	setIndices	value	sets the value of :py attr indices
ml	VectorSlicer	getIndices		gets the value of indices or its default value
ml	VectorSlicer	setNames	value	sets the value of :py attr names
ml	VectorSlicer	getNames		gets the value of names or its default value
ml	Word2Vec	__init__	vectorSize minCount numPartitions stepSize	__init__(self vectorsize=100 mincount=5 numpartitions=1 stepsize=0 025 maxiter=1 seed=none inputcol=none outputcol=none windowsize=5 maxsentencelength=1000)
ml	Word2Vec	setParams	vectorSize minCount numPartitions stepSize	setparams(self mincount=5 numpartitions=1 stepsize=0 025 maxiter=1 seed=none inputcol=none outputcol=none windowsize=5 maxsentencelength=1000)
ml	Word2Vec	setVectorSize	value	sets the value of :py attr vectorsize
ml	Word2Vec	getVectorSize		gets the value of vectorsize or its default value
ml	Word2Vec	setNumPartitions	value	sets the value of :py attr numpartitions
ml	Word2Vec	getNumPartitions		gets the value of numpartitions or its default value
ml	Word2Vec	setMinCount	value	sets the value of :py attr mincount
ml	Word2Vec	getMinCount		gets the value of mincount or its default value
ml	Word2Vec	setWindowSize	value	sets the value of :py attr windowsize
ml	Word2Vec	getWindowSize		gets the value of windowsize or its default value
ml	Word2Vec	setMaxSentenceLength	value	sets the value of :py attr maxsentencelength
ml	Word2Vec	getMaxSentenceLength		gets the value of maxsentencelength or its default value
ml	Word2VecModel	getVectors		returns the vector representation of the words as a dataframe with two fields word and vector
ml	Word2VecModel	findSynonyms	word num	find "num" number of words closest in similarity to "word"
ml	PCA	__init__	k inputCol outputCol	__init__(self k=none inputcol=none outputcol=none)
ml	PCA	setParams	k inputCol outputCol	setparams(self k=none inputcol=none outputcol=none) set params for this pca
ml	PCA	setK	value	sets the value of :py attr k
ml	PCA	getK		gets the value of k or its default value
ml	PCAModel	pc		returns a principal components matrix
ml	PCAModel	explainedVariance		returns a vector of proportions of variance explained by each principal component
ml	RFormula	__init__	formula featuresCol labelCol forceIndexLabel	__init__(self formula=none featurescol="features", labelcol="label", forceindexlabel=false)
ml	RFormula	setParams	formula featuresCol labelCol forceIndexLabel	setparams(self formula=none featurescol="features", labelcol="label", forceindexlabel=false) sets params for rformula
ml	RFormula	setFormula	value	sets the value of :py attr formula
ml	RFormula	getFormula		gets the value of :py attr formula
ml	RFormula	setForceIndexLabel	value	sets the value of :py attr forceindexlabel
ml	RFormula	getForceIndexLabel		gets the value of :py attr forceindexlabel
ml	ChiSqSelector	__init__	numTopFeatures featuresCol outputCol labelCol	__init__(self numtopfeatures=50 featurescol="features", outputcol=none labelcol="label", selectortype="numtopfeatures", percentile=0 1 fpr=0 05 fdr=0 05 fwe=0 05)
ml	ChiSqSelector	setParams	numTopFeatures featuresCol outputCol labelCol	setparams(self numtopfeatures=50 featurescol="features", outputcol=none labelcol="labels", selectortype="numtopfeatures", percentile=0 1 fpr=0 05 fdr=0 05 fwe=0 05)
ml	ChiSqSelector	setSelectorType	value	sets the value of :py attr selectortype
ml	ChiSqSelector	getSelectorType		gets the value of selectortype or its default value
ml	ChiSqSelector	setNumTopFeatures	value	sets the value of :py attr numtopfeatures
ml	ChiSqSelector	getNumTopFeatures		gets the value of numtopfeatures or its default value
ml	ChiSqSelector	setPercentile	value	sets the value of :py attr percentile
ml	ChiSqSelector	getPercentile		gets the value of percentile or its default value
ml	ChiSqSelector	setFpr	value	sets the value of :py attr fpr
ml	ChiSqSelector	getFpr		gets the value of fpr or its default value
ml	ChiSqSelector	setFdr	value	sets the value of :py attr fdr
ml	ChiSqSelector	getFdr		gets the value of fdr or its default value
ml	ChiSqSelector	setFwe	value	sets the value of :py attr fwe
ml	ChiSqSelector	getFwe		gets the value of fwe or its default value
ml	ChiSqSelectorModel	selectedFeatures		list of indices to select filter
ml	ChiSquareTest	test	dataset featuresCol labelCol	perform a pearson's independence test using dataset
ml	Correlation	corr	dataset column method	compute the correlation matrix with specified method using dataset
ml	JavaWrapper	_create_from_java_class	cls java_class	construct this object from given java classname and arguments
ml	JavaWrapper	_new_java_obj	java_class	returns a new java object
ml	JavaWrapper	_new_java_array	pylist java_class	create a java array of given java_class type useful for
ml	JavaParams	_make_java_param_pair	param value	makes a java parm pair
ml	JavaParams	_transfer_params_to_java		transforms the embedded params to the companion java object
ml	JavaParams	_transfer_param_map_to_java	pyParamMap	transforms a python parammap into a java parammap
ml	JavaParams	_transfer_params_from_java		transforms the embedded params from the companion java object
ml	JavaParams	_transfer_param_map_from_java	javaParamMap	transforms a java parammap into a python parammap
ml	JavaParams	_empty_java_param_map		returns an empty java parammap reference
ml	JavaParams	_to_java		transfer this instance's params to the wrapped java object and return the java object
ml	JavaParams	_from_java	java_stage	given a java object create and return a python wrapper of it
ml	JavaParams	copy	extra	creates a copy of this instance with the same uid and some extra params
ml	JavaEstimator	_create_model	java_model	creates a model from the input java model reference
ml	JavaEstimator	_fit_java	dataset	fits a java model to the input dataset
ml	JavaModel	__init__	java_model	initialize this instance with a java model object
ml.param		_gen_param_header	name doc defaultValueStr typeConverter	generates the header part for shared variables
ml.param		_gen_param_code	name doc defaultValueStr	generates python code for a shared param class
ml.param	Param	_copy_new_parent	parent	copy the current param to a new parent must be a dummy param
ml.param	TypeConverters	identity	value	dummy converter that just returns value
ml.param	TypeConverters	toList	value	convert a value to a list if possible
ml.param	TypeConverters	toListFloat	value	convert a value to list of floats if possible
ml.param	TypeConverters	toListInt	value	convert a value to list of ints if possible
ml.param	TypeConverters	toListString	value	convert a value to list of strings if possible
ml.param	TypeConverters	toVector	value	convert a value to a mllib vector if possible
ml.param	TypeConverters	toFloat	value	convert a value to a float if possible
ml.param	TypeConverters	toInt	value	convert a value to an int if possible
ml.param	TypeConverters	toString	value	convert a value to a string if possible
ml.param	TypeConverters	toBoolean	value	convert a value to a boolean if possible
ml.param	Params	_copy_params		copy all params defined on the class to current object
ml.param	Params	params		returns all params ordered by name the default implementation
ml.param	Params	explainParam	param	explains a single param and returns its name doc and optional default value and user-supplied value in a string
ml.param	Params	explainParams		returns the documentation of all params with their optionally default values and user-supplied values
ml.param	Params	getParam	paramName	gets a param by its name
ml.param	Params	isSet	param	checks whether a param is explicitly set by user
ml.param	Params	hasDefault	param	checks whether a param has a default value
ml.param	Params	isDefined	param	checks whether a param is explicitly set by user or has a default value
ml.param	Params	hasParam	paramName	tests whether this instance contains a param with a given string name
ml.param	Params	getOrDefault	param	gets the value of a param in the user-supplied param map or its default value
ml.param	Params	extractParamMap	extra	extracts the embedded default param values and user-supplied values and then merges them with extra values from input into
ml.param	Params	copy	extra	creates a copy of this instance with the same uid and some extra params
ml.param	Params	_shouldOwn	param	validates that the input param belongs to this params instance
ml.param	Params	_resolveParam	param	resolves a param and validates the ownership
ml.param	Params	_dummy		returns a dummy params instance used as a placeholder to generate docs
ml.param	Params	_set		sets user-supplied params
ml.param	Params	_clear	param	clears a param from the param map if it has been explicitly set
ml.param	Params	_setDefault		sets default params
ml.param	Params	_copyValues	to extra	copies param values from this instance to another instance for params shared by them
ml.param	Params	_resetUid	newUid	changes the uid of this instance this updates both
ml.param	HasMaxIter	setMaxIter	value	sets the value of :py attr maxiter
ml.param	HasMaxIter	getMaxIter		gets the value of maxiter or its default value
ml.param	HasRegParam	setRegParam	value	sets the value of :py attr regparam
ml.param	HasRegParam	getRegParam		gets the value of regparam or its default value
ml.param	HasFeaturesCol	setFeaturesCol	value	sets the value of :py attr featurescol
ml.param	HasFeaturesCol	getFeaturesCol		gets the value of featurescol or its default value
ml.param	HasLabelCol	setLabelCol	value	sets the value of :py attr labelcol
ml.param	HasLabelCol	getLabelCol		gets the value of labelcol or its default value
ml.param	HasPredictionCol	setPredictionCol	value	sets the value of :py attr predictioncol
ml.param	HasPredictionCol	getPredictionCol		gets the value of predictioncol or its default value
ml.param	HasProbabilityCol	setProbabilityCol	value	sets the value of :py attr probabilitycol
ml.param	HasProbabilityCol	getProbabilityCol		gets the value of probabilitycol or its default value
ml.param	HasRawPredictionCol	setRawPredictionCol	value	sets the value of :py attr rawpredictioncol
ml.param	HasRawPredictionCol	getRawPredictionCol		gets the value of rawpredictioncol or its default value
ml.param	HasInputCol	setInputCol	value	sets the value of :py attr inputcol
ml.param	HasInputCol	getInputCol		gets the value of inputcol or its default value
ml.param	HasInputCols	setInputCols	value	sets the value of :py attr inputcols
ml.param	HasInputCols	getInputCols		gets the value of inputcols or its default value
ml.param	HasOutputCol	setOutputCol	value	sets the value of :py attr outputcol
ml.param	HasOutputCol	getOutputCol		gets the value of outputcol or its default value
ml.param	HasNumFeatures	setNumFeatures	value	sets the value of :py attr numfeatures
ml.param	HasNumFeatures	getNumFeatures		gets the value of numfeatures or its default value
ml.param	HasCheckpointInterval	setCheckpointInterval	value	sets the value of :py attr checkpointinterval
ml.param	HasCheckpointInterval	getCheckpointInterval		gets the value of checkpointinterval or its default value
ml.param	HasSeed	setSeed	value	sets the value of :py attr seed
ml.param	HasSeed	getSeed		gets the value of seed or its default value
ml.param	HasTol	setTol	value	sets the value of :py attr tol
ml.param	HasTol	getTol		gets the value of tol or its default value
ml.param	HasStepSize	setStepSize	value	sets the value of :py attr stepsize
ml.param	HasStepSize	getStepSize		gets the value of stepsize or its default value
ml.param	HasHandleInvalid	setHandleInvalid	value	sets the value of :py attr handleinvalid
ml.param	HasHandleInvalid	getHandleInvalid		gets the value of handleinvalid or its default value
ml.param	HasElasticNetParam	setElasticNetParam	value	sets the value of :py attr elasticnetparam
ml.param	HasElasticNetParam	getElasticNetParam		gets the value of elasticnetparam or its default value
ml.param	HasFitIntercept	setFitIntercept	value	sets the value of :py attr fitintercept
ml.param	HasFitIntercept	getFitIntercept		gets the value of fitintercept or its default value
ml.param	HasStandardization	setStandardization	value	sets the value of :py attr standardization
ml.param	HasStandardization	getStandardization		gets the value of standardization or its default value
ml.param	HasThresholds	setThresholds	value	sets the value of :py attr thresholds
ml.param	HasThresholds	getThresholds		gets the value of thresholds or its default value
ml.param	HasThreshold	setThreshold	value	sets the value of :py attr threshold
ml.param	HasThreshold	getThreshold		gets the value of threshold or its default value
ml.param	HasWeightCol	setWeightCol	value	sets the value of :py attr weightcol
ml.param	HasWeightCol	getWeightCol		gets the value of weightcol or its default value
ml.param	HasSolver	setSolver	value	sets the value of :py attr solver
ml.param	HasSolver	getSolver		gets the value of solver or its default value
ml.param	HasVarianceCol	setVarianceCol	value	sets the value of :py attr variancecol
ml.param	HasVarianceCol	getVarianceCol		gets the value of variancecol or its default value
ml.param	HasAggregationDepth	setAggregationDepth	value	sets the value of :py attr aggregationdepth
ml.param	HasAggregationDepth	getAggregationDepth		gets the value of aggregationdepth or its default value
ml.param	DecisionTreeParams	setMaxDepth	value	sets the value of :py attr maxdepth
ml.param	DecisionTreeParams	getMaxDepth		gets the value of maxdepth or its default value
ml.param	DecisionTreeParams	setMaxBins	value	sets the value of :py attr maxbins
ml.param	DecisionTreeParams	getMaxBins		gets the value of maxbins or its default value
ml.param	DecisionTreeParams	setMinInstancesPerNode	value	sets the value of :py attr mininstancespernode
ml.param	DecisionTreeParams	getMinInstancesPerNode		gets the value of mininstancespernode or its default value
ml.param	DecisionTreeParams	setMinInfoGain	value	sets the value of :py attr mininfogain
ml.param	DecisionTreeParams	getMinInfoGain		gets the value of mininfogain or its default value
ml.param	DecisionTreeParams	setMaxMemoryInMB	value	sets the value of :py attr maxmemoryinmb
ml.param	DecisionTreeParams	getMaxMemoryInMB		gets the value of maxmemoryinmb or its default value
ml.param	DecisionTreeParams	setCacheNodeIds	value	sets the value of :py attr cachenodeids
ml.param	DecisionTreeParams	getCacheNodeIds		gets the value of cachenodeids or its default value
ml.linalg		_vector_size	v	returns the size of the vector
ml.linalg	Vector	toArray		convert the vector into an numpy ndarray
ml.linalg	DenseVector	numNonzeros		number of nonzero elements this scans all active values and count non zeros
ml.linalg	DenseVector	norm	p	calculates the norm of a densevector
ml.linalg	DenseVector	dot	other	compute the dot product of two vectors we support
ml.linalg	DenseVector	squared_distance	other	squared distance of two vectors
ml.linalg	DenseVector	toArray		returns an numpy ndarray
ml.linalg	DenseVector	values		returns a list of values
ml.linalg	SparseVector	__init__	size	create a sparse vector using either a dictionary a list of index value pairs or two separate arrays of indices and
ml.linalg	SparseVector	numNonzeros		number of nonzero elements this scans all active values and count non zeros
ml.linalg	SparseVector	norm	p	calculates the norm of a sparsevector
ml.linalg	SparseVector	dot	other	dot product with a sparsevector or 1- or 2-dimensional numpy array
ml.linalg	SparseVector	squared_distance	other	squared distance from a sparsevector or 1-dimensional numpy array
ml.linalg	SparseVector	toArray		returns a copy of this sparsevector as a 1-dimensional numpy array
ml.linalg	Vectors	sparse	size	create a sparse vector using either a dictionary a list of index value pairs or two separate arrays of indices and
ml.linalg	Vectors	dense		create a dense vector of 64-bit floats from a python list or numbers
ml.linalg	Vectors	squared_distance	v1 v2	squared distance between two vectors
ml.linalg	Vectors	norm	vector p	find norm of the given vector
ml.linalg	Vectors	_equals	v1_indices v1_values v2_indices v2_values	check equality between sparse/dense vectors v1_indices and v2_indices assume to be strictly increasing
ml.linalg	Matrix	toArray		returns its elements in a numpy ndarray
ml.linalg	Matrix	_convert_to_array	array_like dtype	convert matrix attributes which are array-like or buffer to array
ml.linalg	DenseMatrix	__str__		pretty printing of a densematrix >>> dm = densematrix(2 2 range 4
ml.linalg	DenseMatrix	__repr__		representation of a densematrix >>> dm = densematrix(2 2 range 4
ml.linalg	DenseMatrix	toArray		return an numpy ndarray
ml.linalg	DenseMatrix	toSparse		convert to sparsematrix
ml.linalg	SparseMatrix	__str__		pretty printing of a sparsematrix >>> sm1 = sparsematrix(2 2 [0 2 3], [0 1 1], [2 3 4])
ml.linalg	SparseMatrix	__repr__		representation of a sparsematrix >>> sm1 = sparsematrix(2 2 [0 2 3], [0 1 1], [2 3 4])
ml.linalg	SparseMatrix	toArray		return an numpy ndarray
ml.linalg	Matrices	dense	numRows numCols values	create a densematrix
ml.linalg	Matrices	sparse	numRows numCols colPtrs rowIndices	create a sparsematrix
mllib	MatrixFactorizationModel	predict	user product	predicts rating for the given user and product
mllib	MatrixFactorizationModel	predictAll	user_product	returns a list of predicted ratings for input user and product pairs
mllib	MatrixFactorizationModel	userFeatures		returns a paired rdd where the first element is the user and the second is an array of features corresponding to that user
mllib	MatrixFactorizationModel	productFeatures		returns a paired rdd where the first element is the product and the second is an array of features corresponding to that product
mllib	MatrixFactorizationModel	recommendUsers	product num	recommends the top "num" number of users for a given product and returns a list of rating objects sorted by the predicted rating in
mllib	MatrixFactorizationModel	recommendProducts	user num	recommends the top "num" number of products for a given user and returns a list of rating objects sorted by the predicted rating in
mllib	MatrixFactorizationModel	recommendProductsForUsers	num	recommends the top "num" number of products for all users the
mllib	MatrixFactorizationModel	recommendUsersForProducts	num	recommends the top "num" number of users for all products the
mllib	MatrixFactorizationModel	rank		rank for the features in this model
mllib	MatrixFactorizationModel	load	cls sc path	load a model from the given path
mllib	ALS	train	cls ratings rank iterations	train a matrix factorization model given an rdd of ratings by users for a subset of products
mllib	ALS	trainImplicit	cls ratings rank iterations	train a matrix factorization model given an rdd of 'implicit preferences' of users for a subset of products
mllib	MLUtils	_parse_libsvm_line	line multiclass	parses a line in libsvm format into label indices values
mllib	MLUtils	_convert_labeled_point_to_libsvm	p	converts a labeledpoint to a string in libsvm format
mllib	MLUtils	loadLibSVMFile	sc path numFeatures minPartitions	loads labeled data in the libsvm format into an rdd of labeledpoint
mllib	MLUtils	saveAsLibSVMFile	data dir	save labeled data in libsvm format
mllib	MLUtils	loadLabeledPoints	sc path minPartitions	load labeled points saved using rdd saveastextfile
mllib	MLUtils	appendBias	data	returns a new vector with 1 0 bias appended to
mllib	MLUtils	loadVectors	sc path	loads vectors saved using rdd[vector] saveastextfile
mllib	MLUtils	convertVectorColumnsToML	dataset	converts vector columns in an input dataframe from the :py class pyspark
mllib	MLUtils	convertVectorColumnsFromML	dataset	converts vector columns in an input dataframe to the :py class pyspark
mllib	MLUtils	convertMatrixColumnsToML	dataset	converts matrix columns in an input dataframe from the :py class pyspark
mllib	MLUtils	convertMatrixColumnsFromML	dataset	converts matrix columns in an input dataframe to the :py class pyspark
mllib	Saveable	save	sc path	save this model to the given path
mllib	JavaSaveable	save	sc path	save this model to the given path
mllib	Loader	load	cls sc path	load a model from the given path the model should have been
mllib	JavaLoader	_java_loader_class	cls	returns the full class name of the java loader the default
mllib	JavaLoader	_load_java	cls sc path	load a java model from the given path
mllib	JavaLoader	load	cls sc path	load a model from the given path
mllib	LinearDataGenerator	generateLinearInput	intercept weights xMean xVariance	:param intercept bias factor the term c in x'w + c
mllib	LinearDataGenerator	generateLinearRDD	sc nexamples nfeatures eps	generate an rdd of labeledpoints
mllib		_to_java_object_rdd	rdd	return a javardd of object by unpickling it will convert each python object into java object by pyrolite whenever the
mllib		_py2java	sc obj	convert python object into java
mllib		callJavaFunc	sc func	call java function
mllib		callMLlibFunc	name	call api in pythonmllibapi
mllib	JavaModelWrapper	call	name	call method of java_model
mllib		inherit_doc	cls	a decorator that makes a class inherit documentation from its parents
mllib	BisectingKMeansModel	clusterCenters		get the cluster centers represented as a list of numpy arrays
mllib	BisectingKMeansModel	k		get the number of clusters
mllib	BisectingKMeansModel	predict	x	find the cluster that each of the points belongs to in this model
mllib	BisectingKMeansModel	computeCost	x	return the bisecting k-means cost sum of squared distances of points to their nearest center for this model on the given
mllib	BisectingKMeans	train	rdd k maxIterations minDivisibleClusterSize	runs the bisecting k-means algorithm return the model
mllib	KMeansModel	clusterCenters		get the cluster centers represented as a list of numpy arrays
mllib	KMeansModel	k		total number of clusters
mllib	KMeansModel	predict	x	find the cluster that each of the points belongs to in this model
mllib	KMeansModel	computeCost	rdd	return the k-means cost sum of squared distances of points to their nearest center for this model on the given
mllib	KMeansModel	save	sc path	save this model to the given path
mllib	KMeansModel	load	cls sc path	load a model from the given path
mllib	KMeans	train	cls rdd k maxIterations	train a k-means clustering model
mllib	GaussianMixtureModel	weights		weights for each gaussian distribution in the mixture where weights[i] is the weight for gaussian i and weights
mllib	GaussianMixtureModel	gaussians		array of multivariategaussian where gaussians[i] represents the multivariate gaussian normal distribution for gaussian i
mllib	GaussianMixtureModel	k		number of gaussians in mixture
mllib	GaussianMixtureModel	predict	x	find the cluster to which the point 'x' or each point in rdd 'x' has maximum membership in this model
mllib	GaussianMixtureModel	predictSoft	x	find the membership of point 'x' or each point in rdd 'x' to all mixture components
mllib	GaussianMixtureModel	load	cls sc path	load the gaussianmixturemodel from disk
mllib	GaussianMixture	train	cls rdd k convergenceTol	train a gaussian mixture clustering model
mllib	PowerIterationClusteringModel	k		returns the number of clusters
mllib	PowerIterationClusteringModel	assignments		returns the cluster assignments of this model
mllib	PowerIterationClusteringModel	load	cls sc path	load a model from the given path
mllib	PowerIterationClustering	train	cls rdd k maxIterations	:param rdd an rdd of (i j s\ :sub ij\) tuples representing the
mllib	StreamingKMeansModel	clusterWeights		return the cluster weights
mllib	StreamingKMeansModel	update	data decayFactor timeUnit	update the centroids according to data
mllib	StreamingKMeans	latestModel		return the latest model
mllib	StreamingKMeans	setK	k	set number of clusters
mllib	StreamingKMeans	setDecayFactor	decayFactor	set decay factor
mllib	StreamingKMeans	setHalfLife	halfLife timeUnit	set number of batches after which the centroids of that particular batch has half the weightage
mllib	StreamingKMeans	setInitialCenters	centers weights	set initial centers should be set before calling trainon
mllib	StreamingKMeans	setRandomCenters	dim weight seed	set the initial centres to be random samples from a gaussian population with constant weights
mllib	StreamingKMeans	trainOn	dstream	train the model on the incoming dstream
mllib	StreamingKMeans	predictOn	dstream	make predictions on a dstream
mllib	StreamingKMeans	predictOnValues	dstream	make predictions on a keyed dstream
mllib	LDAModel	topicsMatrix		inferred topics where each topic is represented by a distribution over terms
mllib	LDAModel	vocabSize		vocabulary size number of terms or terms in the vocabulary
mllib	LDAModel	describeTopics	maxTermsPerTopic	return the topics described by weighted terms
mllib	LDAModel	load	cls sc path	load the ldamodel from disk
mllib	LDA	train	cls rdd k maxIterations	train a lda model
mllib	FPGrowthModel	freqItemsets		returns the frequent itemsets of this model
mllib	FPGrowthModel	load	cls sc path	load a model from the given path
mllib	FPGrowth	train	cls data minSupport numPartitions	computes an fp-growth model that contains frequent itemsets
mllib	PrefixSpanModel	freqSequences		gets frequent sequences
mllib	PrefixSpan	train	cls data minSupport maxPatternLength	finds the complete set of frequent sequential patterns in the input sequences of itemsets
mllib	RandomRDDs	uniformRDD	sc size numPartitions seed	generates an rdd comprised of i i d samples from the
mllib	RandomRDDs	normalRDD	sc size numPartitions seed	generates an rdd comprised of i i d samples from the standard normal
mllib	RandomRDDs	logNormalRDD	sc mean std size	generates an rdd comprised of i i d samples from the log normal
mllib	RandomRDDs	poissonRDD	sc mean size numPartitions	generates an rdd comprised of i i d samples from the poisson
mllib	RandomRDDs	exponentialRDD	sc mean size numPartitions	generates an rdd comprised of i i d samples from the exponential
mllib	RandomRDDs	gammaRDD	sc shape scale size	generates an rdd comprised of i i d samples from the gamma
mllib	RandomRDDs	uniformVectorRDD	sc numRows numCols numPartitions	generates an rdd comprised of vectors containing i i d samples drawn
mllib	RandomRDDs	normalVectorRDD	sc numRows numCols numPartitions	generates an rdd comprised of vectors containing i i d samples drawn
mllib	RandomRDDs	logNormalVectorRDD	sc mean std numRows	generates an rdd comprised of vectors containing i i d samples drawn
mllib	RandomRDDs	poissonVectorRDD	sc mean numRows numCols	generates an rdd comprised of vectors containing i i d samples drawn
mllib	RandomRDDs	exponentialVectorRDD	sc mean numRows numCols	generates an rdd comprised of vectors containing i i d samples drawn
mllib	RandomRDDs	gammaVectorRDD	sc shape scale numRows	generates an rdd comprised of vectors containing i i d samples drawn
mllib	LinearModel	weights		weights computed for every feature
mllib	LinearModel	intercept		intercept computed for this model
mllib	LinearRegressionModelBase	predict	x	predict the value of the dependent variable given a vector or an rdd of vectors containing values for the independent variables
mllib	LinearRegressionModel	save	sc path	save a linearregressionmodel
mllib	LinearRegressionModel	load	cls sc path	load a linearregressionmodel
mllib	LinearRegressionWithSGD	train	cls data iterations step	train a linear regression model using stochastic gradient descent sgd
mllib	LassoModel	save	sc path	save a lassomodel
mllib	LassoModel	load	cls sc path	load a lassomodel
mllib	LassoWithSGD	train	cls data iterations step	train a regression model with l1-regularization using stochastic gradient descent
mllib	RidgeRegressionModel	save	sc path	save a ridgeregressionmode
mllib	RidgeRegressionModel	load	cls sc path	load a ridgeregressionmode
mllib	RidgeRegressionWithSGD	train	cls data iterations step	train a regression model with l2-regularization using stochastic gradient descent
mllib	IsotonicRegressionModel	predict	x	predict labels for provided features
mllib	IsotonicRegressionModel	save	sc path	save an isotonicregressionmodel
mllib	IsotonicRegressionModel	load	cls sc path	load an isotonicregressionmodel
mllib	IsotonicRegression	train	cls data isotonic	train an isotonic regression model on the given data
mllib	StreamingLinearAlgorithm	latestModel		returns the latest model
mllib	StreamingLinearAlgorithm	predictOn	dstream	use the model to make predictions on batches of data from a dstream
mllib	StreamingLinearAlgorithm	predictOnValues	dstream	use the model to make predictions on the values of a dstream and carry over its keys
mllib	StreamingLinearRegressionWithSGD	setInitialWeights	initialWeights	set the initial value of weights
mllib	StreamingLinearRegressionWithSGD	trainOn	dstream	train the model on the incoming dstream
mllib	MLLibStreamingTestCase	_eventually	condition timeout catch_assertions	wait a given amount of time for a condition to pass else fail with an error
mllib	SciPyTests	scipy_matrix	size values	create a column scipy matrix from a dictionary of values
mllib	StreamingKMeansTest	test_model_params		test that the model params are set correctly
mllib	StreamingKMeansTest	test_accuracy_for_single_center		test that parameters obtained are correct for a single center
mllib	StreamingKMeansTest	test_trainOn_model		test the model on toy data with four clusters
mllib	StreamingKMeansTest	test_predictOn_model		test that the model predicts correctly on toy data
mllib	StreamingKMeansTest	test_trainOn_predictOn		test that prediction happens on the updated model
mllib	StreamingLogisticRegressionWithSGDTests	generateLogisticInput	offset scale nPoints seed	generate 1 / (1 + exp(-x * scale + offset)) where
mllib	StreamingLogisticRegressionWithSGDTests	test_parameter_accuracy		test that the final value of weights is close to the desired value
mllib	StreamingLogisticRegressionWithSGDTests	test_convergence		test that weights converge to the required value on toy data
mllib	StreamingLogisticRegressionWithSGDTests	test_predictions		test predicted values on a toy model
mllib	StreamingLogisticRegressionWithSGDTests	test_training_and_prediction		test that the model improves on toy data with no of batches
mllib	StreamingLinearRegressionWithTests	test_parameter_accuracy		test that coefs are predicted accurately by fitting on toy data
mllib	StreamingLinearRegressionWithTests	test_parameter_convergence		test that the model parameters improve with streaming data
mllib	StreamingLinearRegressionWithTests	test_prediction		test prediction on a model with weights already set
mllib	StreamingLinearRegressionWithTests	test_train_prediction		test that error on test data improves as model is trained
mllib	LinearClassificationModel	setThreshold	value	sets the threshold that separates positive predictions from negative predictions
mllib	LinearClassificationModel	threshold		returns the threshold if any used for converting raw prediction scores into 0/1 predictions
mllib	LinearClassificationModel	clearThreshold		clears the threshold so that predict will output raw prediction scores
mllib	LinearClassificationModel	predict	test	predict values for a single data point or an rdd of points using the model trained
mllib	LogisticRegressionModel	numFeatures		dimension of the features
mllib	LogisticRegressionModel	numClasses		number of possible outcomes for k classes classification problem in multinomial logistic regression
mllib	LogisticRegressionModel	predict	x	predict values for a single data point or an rdd of points using the model trained
mllib	LogisticRegressionModel	save	sc path	save this model to the given path
mllib	LogisticRegressionModel	load	cls sc path	load a model from the given path
mllib	LogisticRegressionWithSGD	train	cls data iterations step	train a logistic regression model on the given data
mllib	LogisticRegressionWithLBFGS	train	cls data iterations initialWeights	train a logistic regression model on the given data
mllib	SVMModel	predict	x	predict values for a single data point or an rdd of points using the model trained
mllib	SVMModel	save	sc path	save this model to the given path
mllib	SVMModel	load	cls sc path	load a model from the given path
mllib	SVMWithSGD	train	cls data iterations step	train a support vector machine on the given data
mllib	NaiveBayesModel	predict	x	return the most likely class for a data vector
mllib	NaiveBayesModel	save	sc path	save this model to the given path
mllib	NaiveBayesModel	load	cls sc path	load a model from the given path
mllib	NaiveBayes	train	cls data lambda_	train a naive bayes model given an rdd of label features vectors
mllib	StreamingLogisticRegressionWithSGD	setInitialWeights	initialWeights	set the initial value of weights
mllib	StreamingLogisticRegressionWithSGD	trainOn	dstream	train the model on the incoming dstream
mllib	BinaryClassificationMetrics	areaUnderROC		computes the area under the receiver operating characteristic roc curve
mllib	BinaryClassificationMetrics	areaUnderPR		computes the area under the precision-recall curve
mllib	BinaryClassificationMetrics	unpersist		unpersists intermediate rdds used in the computation
mllib	RegressionMetrics	explainedVariance		returns the explained variance regression score
mllib	RegressionMetrics	meanAbsoluteError		returns the mean absolute error which is a risk function corresponding to the expected value of the absolute error loss or l1-norm loss
mllib	RegressionMetrics	meanSquaredError		returns the mean squared error which is a risk function corresponding to the expected value of the squared error loss or quadratic loss
mllib	RegressionMetrics	rootMeanSquaredError		returns the root mean squared error which is defined as the square root of the mean squared error
mllib	RegressionMetrics	r2		returns r^2^, the coefficient of determination
mllib	MulticlassMetrics	confusionMatrix		returns confusion matrix predicted classes are in columns they are ordered by class label ascending as in "labels"
mllib	MulticlassMetrics	truePositiveRate	label	returns true positive rate for a given label category
mllib	MulticlassMetrics	falsePositiveRate	label	returns false positive rate for a given label category
mllib	MulticlassMetrics	precision	label	returns precision or precision for a given label category if specified
mllib	MulticlassMetrics	recall	label	returns recall or recall for a given label category if specified
mllib	MulticlassMetrics	fMeasure	label beta	returns f-measure or f-measure for a given label category if specified
mllib	MulticlassMetrics	accuracy		returns accuracy equals to the total number of correctly classified instances out of the total number of instances
mllib	MulticlassMetrics	weightedTruePositiveRate		returns weighted true positive rate
mllib	MulticlassMetrics	weightedFalsePositiveRate		returns weighted false positive rate
mllib	MulticlassMetrics	weightedRecall		returns weighted averaged recall
mllib	MulticlassMetrics	weightedPrecision		returns weighted averaged precision
mllib	MulticlassMetrics	weightedFMeasure	beta	returns weighted averaged f-measure
mllib	RankingMetrics	precisionAt	k	compute the average precision of all the queries truncated at ranking position k
mllib	RankingMetrics	meanAveragePrecision		returns the mean average precision map of all the queries
mllib	RankingMetrics	ndcgAt	k	compute the average ndcg value of all the queries truncated at ranking position k
mllib	MultilabelMetrics	precision	label	returns precision or precision for a given label category if specified
mllib	MultilabelMetrics	recall	label	returns recall or recall for a given label category if specified
mllib	MultilabelMetrics	f1Measure	label	returns f1measure or f1measure for a given label category if specified
mllib	MultilabelMetrics	microPrecision		returns micro-averaged label-based precision
mllib	MultilabelMetrics	microRecall		returns micro-averaged label-based recall
mllib	MultilabelMetrics	microF1Measure		returns micro-averaged label-based f1-measure
mllib	MultilabelMetrics	subsetAccuracy		returns subset accuracy
mllib	VectorTransformer	transform	vector	applies transformation on a vector
mllib	Normalizer	transform	vector	applies unit length normalization on a vector
mllib	JavaVectorTransformer	transform	vector	applies transformation on a vector or an rdd[vector]
mllib	StandardScalerModel	transform	vector	applies standardization transformation on a vector
mllib	StandardScalerModel	setWithMean	withMean	setter of the boolean which decides
mllib	StandardScalerModel	setWithStd	withStd	setter of the boolean which decides
mllib	StandardScalerModel	withStd		returns if the model scales the data to unit standard deviation
mllib	StandardScalerModel	withMean		returns if the model centers the data before scaling
mllib	StandardScalerModel	std		return the column standard deviation values
mllib	StandardScalerModel	mean		return the column mean values
mllib	StandardScaler	fit	dataset	computes the mean and variance and stores as a model to be used for later scaling
mllib	ChiSqSelectorModel	transform	vector	applies transformation on a vector
mllib	ChiSqSelector	setNumTopFeatures	numTopFeatures	set numtopfeature for feature selection by number of top features
mllib	ChiSqSelector	setPercentile	percentile	set percentile [0 0 1 0] for feature selection by percentile
mllib	ChiSqSelector	setFpr	fpr	set fpr [0 0 1 0] for feature selection by fpr
mllib	ChiSqSelector	setFdr	fdr	set fdr [0 0 1 0] for feature selection by fdr
mllib	ChiSqSelector	setFwe	fwe	set fwe [0 0 1 0] for feature selection by fwe
mllib	ChiSqSelector	setSelectorType	selectorType	set the selector type of the chisqselector
mllib	ChiSqSelector	fit	data	returns a chisquared feature selector
mllib	PCA	__init__	k	:param k number of principal components
mllib	PCA	fit	data	computes a [[pcamodel]] that contains the principal components of the input vectors
mllib	HashingTF	setBinary	value	if true term frequency vector will be binary such that non-zero
mllib	HashingTF	indexOf	term	returns the index of the input term
mllib	HashingTF	transform	document	transforms the input document list of terms to term frequency vectors or transform the rdd of document to rdd of term
mllib	IDFModel	transform	x	transforms term frequency tf vectors to tf-idf vectors
mllib	IDFModel	idf		returns the current idf vector
mllib	IDF	fit	dataset	computes the inverse document frequency
mllib	Word2VecModel	transform	word	transforms a word to its vector representation
mllib	Word2VecModel	findSynonyms	word num	find synonyms of a word
mllib	Word2VecModel	getVectors		returns a map of words to their vector representations
mllib	Word2VecModel	load	cls sc path	load a model from the given path
mllib	Word2Vec	__init__		construct word2vec instance
mllib	Word2Vec	setVectorSize	vectorSize	sets vector size default 100
mllib	Word2Vec	setLearningRate	learningRate	sets initial learning rate default 0 025
mllib	Word2Vec	setNumPartitions	numPartitions	sets number of partitions default 1 use a small number for
mllib	Word2Vec	setNumIterations	numIterations	sets number of iterations default 1 which should be smaller than or equal to number of partitions
mllib	Word2Vec	setSeed	seed	sets random seed
mllib	Word2Vec	setMinCount	minCount	sets mincount the minimum number of times a token must appear to be included in the word2vec model's vocabulary default 5
mllib	Word2Vec	setWindowSize	windowSize	sets window size default 5
mllib	Word2Vec	fit	data	computes the vector representation of each word in vocabulary
mllib	ElementwiseProduct	transform	vector	computes the hadamard product of the vector
mllib	TreeEnsembleModel	predict	x	predict values for a single data point or an rdd of points using the model trained
mllib	TreeEnsembleModel	numTrees		get number of trees in ensemble
mllib	TreeEnsembleModel	totalNumNodes		get total number of nodes summed over all trees in the ensemble
mllib	TreeEnsembleModel	__repr__		summary of model
mllib	DecisionTreeModel	predict	x	predict the label of one or more examples
mllib	DecisionTreeModel	numNodes		get number of nodes in tree including leaf nodes
mllib	DecisionTreeModel	depth		get depth of tree (e g depth 0 means 1 leaf node depth 1
mllib	DecisionTreeModel	__repr__		summary of model
mllib	DecisionTree	trainClassifier	cls data numClasses categoricalFeaturesInfo	train a decision tree model for classification
mllib	DecisionTree	trainRegressor	cls data categoricalFeaturesInfo impurity	train a decision tree model for regression
mllib	RandomForest	trainClassifier	cls data numClasses categoricalFeaturesInfo	train a random forest model for binary or multiclass classification
mllib	RandomForest	trainRegressor	cls data categoricalFeaturesInfo numTrees	train a random forest model for regression
mllib	GradientBoostedTrees	trainClassifier	cls data categoricalFeaturesInfo loss	train a gradient-boosted trees model for classification
mllib	GradientBoostedTrees	trainRegressor	cls data categoricalFeaturesInfo loss	train a gradient-boosted trees model for regression
mllib.stat	TestResult	pValue		the probability of obtaining a test statistic result at least as extreme as the one that was actually observed assuming that the
mllib.stat	TestResult	degreesOfFreedom		returns the degree s of freedom of the hypothesis test
mllib.stat	TestResult	nullHypothesis		null hypothesis of the test
mllib.stat	ChiSqTestResult	method		name of the test method
mllib.stat	KernelDensity	setBandwidth	bandwidth	set bandwidth of each sample defaults to 1 0
mllib.stat	KernelDensity	setSample	sample	set sample points from the population should be a rdd
mllib.stat	KernelDensity	estimate	points	estimate the probability density at points
mllib.stat	Statistics	colStats	rdd	computes column-wise summary statistics for the input rdd[vector]
mllib.stat	Statistics	corr	x y method	compute the correlation matrix for the input rdd s using the specified method
mllib.stat	Statistics	chiSqTest	observed expected	if observed is vector conduct pearson's chi-squared goodness of fit test of the observed data against the expected distribution
mllib.stat	Statistics	kolmogorovSmirnovTest	data distName	performs the kolmogorov-smirnov ks test for data sampled from a continuous distribution
mllib.linalg	DistributedMatrix	numRows		get or compute the number of rows
mllib.linalg	DistributedMatrix	numCols		get or compute the number of cols
mllib.linalg	RowMatrix	__init__	rows numRows numCols	note this docstring is not shown publicly
mllib.linalg	RowMatrix	rows		rows of the rowmatrix stored as an rdd of vectors
mllib.linalg	RowMatrix	numRows		get or compute the number of rows
mllib.linalg	RowMatrix	numCols		get or compute the number of cols
mllib.linalg	RowMatrix	computeColumnSummaryStatistics		computes column-wise summary statistics
mllib.linalg	RowMatrix	computeCovariance		computes the covariance matrix treating each row as an observation
mllib.linalg	RowMatrix	computeGramianMatrix		computes the gramian matrix a^t a
mllib.linalg	RowMatrix	columnSimilarities	threshold	compute similarities between columns of this matrix
mllib.linalg	RowMatrix	tallSkinnyQR	computeQ	compute the qr decomposition of this rowmatrix
mllib.linalg	RowMatrix	computeSVD	k computeU rCond	computes the singular value decomposition of the rowmatrix
mllib.linalg	RowMatrix	computePrincipalComponents	k	computes the k principal components of the given row matrix
mllib.linalg	RowMatrix	multiply	matrix	multiply this matrix by a local dense matrix on the right
mllib.linalg	SingularValueDecomposition	U		returns a distributed matrix whose columns are the left singular vectors of the singularvaluedecomposition if computeu was set to be true
mllib.linalg	SingularValueDecomposition	s		returns a densevector with singular values in descending order
mllib.linalg	SingularValueDecomposition	V		returns a densematrix whose columns are the right singular vectors of the singularvaluedecomposition
mllib.linalg	IndexedRowMatrix	__init__	rows numRows numCols	note this docstring is not shown publicly
mllib.linalg	IndexedRowMatrix	rows		rows of the indexedrowmatrix stored as an rdd of indexedrows
mllib.linalg	IndexedRowMatrix	numRows		get or compute the number of rows
mllib.linalg	IndexedRowMatrix	numCols		get or compute the number of cols
mllib.linalg	IndexedRowMatrix	columnSimilarities		compute all cosine similarities between columns
mllib.linalg	IndexedRowMatrix	computeGramianMatrix		computes the gramian matrix a^t a
mllib.linalg	IndexedRowMatrix	toRowMatrix		convert this matrix to a rowmatrix
mllib.linalg	IndexedRowMatrix	toCoordinateMatrix		convert this matrix to a coordinatematrix
mllib.linalg	IndexedRowMatrix	toBlockMatrix	rowsPerBlock colsPerBlock	convert this matrix to a blockmatrix
mllib.linalg	IndexedRowMatrix	computeSVD	k computeU rCond	computes the singular value decomposition of the indexedrowmatrix
mllib.linalg	IndexedRowMatrix	multiply	matrix	multiply this matrix by a local dense matrix on the right
mllib.linalg	CoordinateMatrix	__init__	entries numRows numCols	note this docstring is not shown publicly
mllib.linalg	CoordinateMatrix	entries		entries of the coordinatematrix stored as an rdd of matrixentries
mllib.linalg	CoordinateMatrix	numRows		get or compute the number of rows
mllib.linalg	CoordinateMatrix	numCols		get or compute the number of cols
mllib.linalg	CoordinateMatrix	transpose		transpose this coordinatematrix
mllib.linalg	CoordinateMatrix	toRowMatrix		convert this matrix to a rowmatrix
mllib.linalg	CoordinateMatrix	toIndexedRowMatrix		convert this matrix to an indexedrowmatrix
mllib.linalg	CoordinateMatrix	toBlockMatrix	rowsPerBlock colsPerBlock	convert this matrix to a blockmatrix
mllib.linalg	BlockMatrix	__init__	blocks rowsPerBlock colsPerBlock numRows	note this docstring is not shown publicly
mllib.linalg	BlockMatrix	blocks		the rdd of sub-matrix blocks blockrowindex blockcolindex sub-matrix) that form this
mllib.linalg	BlockMatrix	rowsPerBlock		number of rows that make up each block
mllib.linalg	BlockMatrix	colsPerBlock		number of columns that make up each block
mllib.linalg	BlockMatrix	numRowBlocks		number of rows of blocks in the blockmatrix
mllib.linalg	BlockMatrix	numColBlocks		number of columns of blocks in the blockmatrix
mllib.linalg	BlockMatrix	numRows		get or compute the number of rows
mllib.linalg	BlockMatrix	numCols		get or compute the number of cols
mllib.linalg	BlockMatrix	cache		caches the underlying rdd
mllib.linalg	BlockMatrix	persist	storageLevel	persists the underlying rdd with the specified storage level
mllib.linalg	BlockMatrix	validate		validates the block matrix info against the matrix data (blocks) and throws an exception if any error is found
mllib.linalg	BlockMatrix	add	other	adds two block matrices together the matrices must have the
mllib.linalg	BlockMatrix	subtract	other	subtracts the given block matrix other from this block matrix this - other
mllib.linalg	BlockMatrix	multiply	other	left multiplies this blockmatrix by other, another blockmatrix
mllib.linalg	BlockMatrix	transpose		transpose this blockmatrix returns a new blockmatrix
mllib.linalg	BlockMatrix	toLocalMatrix		collect the distributed matrix on the driver as a densematrix
mllib.linalg	BlockMatrix	toIndexedRowMatrix		convert this matrix to an indexedrowmatrix
mllib.linalg	BlockMatrix	toCoordinateMatrix		convert this matrix to a coordinatematrix
mllib.linalg		_vector_size	v	returns the size of the vector
mllib.linalg	Vector	toArray		convert the vector into an numpy ndarray
mllib.linalg	Vector	asML		convert this vector to the new mllib-local representation
mllib.linalg	DenseVector	parse	s	parse string representation back into the densevector
mllib.linalg	DenseVector	numNonzeros		number of nonzero elements this scans all active values and count non zeros
mllib.linalg	DenseVector	norm	p	calculates the norm of a densevector
mllib.linalg	DenseVector	dot	other	compute the dot product of two vectors we support
mllib.linalg	DenseVector	squared_distance	other	squared distance of two vectors
mllib.linalg	DenseVector	toArray		returns an numpy ndarray
mllib.linalg	DenseVector	asML		convert this vector to the new mllib-local representation
mllib.linalg	DenseVector	values		returns a list of values
mllib.linalg	SparseVector	__init__	size	create a sparse vector using either a dictionary a list of index value pairs or two separate arrays of indices and
mllib.linalg	SparseVector	numNonzeros		number of nonzero elements this scans all active values and count non zeros
mllib.linalg	SparseVector	norm	p	calculates the norm of a sparsevector
mllib.linalg	SparseVector	parse	s	parse string representation back into the sparsevector
mllib.linalg	SparseVector	dot	other	dot product with a sparsevector or 1- or 2-dimensional numpy array
mllib.linalg	SparseVector	squared_distance	other	squared distance from a sparsevector or 1-dimensional numpy array
mllib.linalg	SparseVector	toArray		returns a copy of this sparsevector as a 1-dimensional numpy array
mllib.linalg	SparseVector	asML		convert this vector to the new mllib-local representation
mllib.linalg	Vectors	sparse	size	create a sparse vector using either a dictionary a list of index value pairs or two separate arrays of indices and
mllib.linalg	Vectors	dense		create a dense vector of 64-bit floats from a python list or numbers
mllib.linalg	Vectors	fromML	vec	convert a vector from the new mllib-local representation
mllib.linalg	Vectors	stringify	vector	converts a vector into a string which can be recognized by vectors
mllib.linalg	Vectors	squared_distance	v1 v2	squared distance between two vectors
mllib.linalg	Vectors	norm	vector p	find norm of the given vector
mllib.linalg	Vectors	parse	s	parse a string representation back into the vector
mllib.linalg	Vectors	_equals	v1_indices v1_values v2_indices v2_values	check equality between sparse/dense vectors v1_indices and v2_indices assume to be strictly increasing
mllib.linalg	Matrix	toArray		returns its elements in a numpy ndarray
mllib.linalg	Matrix	asML		convert this matrix to the new mllib-local representation
mllib.linalg	Matrix	_convert_to_array	array_like dtype	convert matrix attributes which are array-like or buffer to array
mllib.linalg	DenseMatrix	__str__		pretty printing of a densematrix >>> dm = densematrix(2 2 range 4
mllib.linalg	DenseMatrix	__repr__		representation of a densematrix >>> dm = densematrix(2 2 range 4
mllib.linalg	DenseMatrix	toArray		return an numpy ndarray
mllib.linalg	DenseMatrix	toSparse		convert to sparsematrix
mllib.linalg	DenseMatrix	asML		convert this matrix to the new mllib-local representation
mllib.linalg	SparseMatrix	__str__		pretty printing of a sparsematrix >>> sm1 = sparsematrix(2 2 [0 2 3], [0 1 1], [2 3 4])
mllib.linalg	SparseMatrix	__repr__		representation of a sparsematrix >>> sm1 = sparsematrix(2 2 [0 2 3], [0 1 1], [2 3 4])
mllib.linalg	SparseMatrix	toArray		return an numpy ndarray
mllib.linalg	SparseMatrix	asML		convert this matrix to the new mllib-local representation
mllib.linalg	Matrices	dense	numRows numCols values	create a densematrix
mllib.linalg	Matrices	sparse	numRows numCols colPtrs rowIndices	create a sparsematrix
mllib.linalg	Matrices	fromML	mat	convert a matrix from the new mllib-local representation
mllib.linalg	QRDecomposition	Q		an orthogonal matrix q in a qr decomposition
mllib.linalg	QRDecomposition	R		an upper triangular matrix r in a qr decomposition
sql		_create_function	name doc	create a function for aggregator by name
sql		_create_binary_mathfunction	name doc	create a binary mathfunction by name
sql		_create_window_function	name doc	create a window function by name
sql		approxCountDistinct	col rsd	note : deprecated in 2 1 use approx_count_distinct instead
sql		approx_count_distinct	col rsd	returns a new :class column for approximate distinct count of col
sql		broadcast	df	marks a dataframe as small enough for use in broadcast joins
sql		coalesce		returns the first column that is not null
sql		corr	col1 col2	returns a new :class column for the pearson correlation coefficient for col1 and col2
sql		covar_pop	col1 col2	returns a new :class column for the population covariance of col1 and col2
sql		covar_samp	col1 col2	returns a new :class column for the sample covariance of col1 and col2
sql		countDistinct	col	returns a new :class column for distinct count of col or cols
sql		first	col ignorenulls	aggregate function returns the first value in a group
sql		grouping	col	aggregate function indicates whether a specified column in a group by list is aggregated or not returns 1 for aggregated or 0 for not aggregated in the result set
sql		grouping_id		aggregate function returns the level of grouping equals to (grouping c1 << n-1 + (grouping c2 << n-2 +
sql		input_file_name		creates a string column for the file name of the current spark task
sql		isnan	col	an expression that returns true iff the column is nan
sql		isnull	col	an expression that returns true iff the column is null
sql		last	col ignorenulls	aggregate function returns the last value in a group
sql		monotonically_increasing_id		a column that generates monotonically increasing 64-bit integers
sql		nanvl	col1 col2	returns col1 if it is not nan or col2 if col1 is nan
sql		rand	seed	generates a random column with independent and identically distributed i i d samples
sql		randn	seed	generates a column with independent and identically distributed i i d samples from
sql		round	col scale	round the given value to scale decimal places using half_up rounding mode if scale >= 0 or at integral part when scale < 0
sql		bround	col scale	round the given value to scale decimal places using half_even rounding mode if scale >= 0 or at integral part when scale < 0
sql		shiftLeft	col numBits	shift the given value numbits left
sql		shiftRight	col numBits	signed shift the given value numbits right
sql		shiftRightUnsigned	col numBits	unsigned shift the given value numbits right
sql		spark_partition_id		a column for partition id
sql		expr	str	parses the expression string into the column that it represents >>> df
sql		struct		creates a new struct column
sql		greatest		returns the greatest value of the list of column names skipping null values
sql		least		returns the least value of the list of column names skipping null values
sql		when	condition value	evaluates a list of conditions and returns one of multiple possible result expressions
sql		log	arg1 arg2	returns the first argument-based logarithm of the second argument
sql		log2	col	returns the base-2 logarithm of the argument
sql		conv	col fromBase toBase	convert a number in a string column from one base to another
sql		factorial	col	computes the factorial of the given value
sql		lag	col count default	window function returns the value that is offset rows before the current row and defaultvalue if there is less than offset rows before the current row
sql		lead	col count default	window function returns the value that is offset rows after the current row and defaultvalue if there is less than offset rows after the current row
sql		ntile	n	window function returns the ntile group id (from 1 to n inclusive) in an ordered window partition
sql		current_date		returns the current date as a date column
sql		current_timestamp		returns the current timestamp as a timestamp column
sql		date_format	date format	converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument
sql		year	col	extract the year of a given date as integer
sql		quarter	col	extract the quarter of a given date as integer
sql		month	col	extract the month of a given date as integer
sql		dayofmonth	col	extract the day of the month of a given date as integer
sql		dayofyear	col	extract the day of the year of a given date as integer
sql		hour	col	extract the hours of a given date as integer
sql		minute	col	extract the minutes of a given date as integer
sql		second	col	extract the seconds of a given date as integer
sql		weekofyear	col	extract the week number of a given date as integer
sql		date_add	start days	returns the date that is days days after start >>> df = spark
sql		date_sub	start days	returns the date that is days days before start >>> df = spark
sql		datediff	end start	returns the number of days from start to end
sql		add_months	start months	returns the date that is months months after start >>> df = spark
sql		months_between	date1 date2	returns the number of months between date1 and date2
sql		to_date	col format	converts a :class column of :class pyspark sql types stringtype or
sql		to_timestamp	col format	converts a :class column of :class pyspark sql types stringtype or
sql		trunc	date format	returns date truncated to the unit specified by the format
sql		next_day	date dayOfWeek	returns the first date which is later than the value of the date column
sql		last_day	date	returns the last day of the month which the given date belongs to
sql		from_unixtime	timestamp format	converts the number of seconds from unix epoch 1970-01-01 00 00 00 utc to a string representing the timestamp of that moment in the current system time zone in the given
sql		unix_timestamp	timestamp format	convert time string with given pattern ('yyyy-mm-dd hh mm ss', by default) to unix time stamp in seconds using the default timezone and the default
sql		from_utc_timestamp	timestamp tz	given a timestamp which corresponds to a certain time of day in utc returns another timestamp that corresponds to the same time of day in the given timezone
sql		to_utc_timestamp	timestamp tz	given a timestamp which corresponds to a certain time of day in the given timezone returns another timestamp that corresponds to the same time of day in utc
sql		window	timeColumn windowDuration slideDuration startTime	bucketize rows into one or more time windows given a timestamp specifying column window
sql		crc32	col	calculates the cyclic redundancy check value crc32 of a binary column and returns the value as a bigint
sql		md5	col	calculates the md5 digest and returns the value as a 32 character hex string
sql		sha1	col	returns the hex string result of sha-1
sql		sha2	col numBits	returns the hex string result of sha-2 family of hash functions sha-224 sha-256 sha-384 and sha-512
sql		hash		calculates the hash code of given columns and returns the result as an int column
sql		concat		concatenates multiple input string columns together into a single string column
sql		concat_ws	sep	concatenates multiple input string columns together into a single string column using the given separator
sql		decode	col charset	computes the first argument into a string from a binary using the provided character set (one of 'us-ascii', 'iso-8859-1', 'utf-8', 'utf-16be', 'utf-16le', 'utf-16')
sql		encode	col charset	computes the first argument into a binary from a string using the provided character set (one of 'us-ascii', 'iso-8859-1', 'utf-8', 'utf-16be', 'utf-16le', 'utf-16')
sql		format_number	col d	formats the number x to a format like '#,--#,--# --', rounded to d decimal places
sql		format_string	format	formats the arguments in printf-style and returns the result as a string column
sql		instr	str substr	locate the position of the first occurrence of substr column in the given string
sql		substring	str pos len	substring starts at pos and is of length len when str is string type or returns the slice of byte array that starts at pos in byte and is of length len
sql		substring_index	str delim count	returns the substring from string str before count occurrences of the delimiter delim
sql		levenshtein	left right	computes the levenshtein distance of the two given strings
sql		locate	substr str pos	locate the position of the first occurrence of substr in a string column after position pos
sql		lpad	col len pad	left-pad the string column to width len with pad
sql		rpad	col len pad	right-pad the string column to width len with pad
sql		repeat	col n	repeats a string column n times and returns it as a new string column
sql		split	str pattern	splits str around pattern pattern is a regular expression
sql		regexp_extract	str pattern idx	extract a specific group matched by a java regex from the specified string column
sql		regexp_replace	str pattern replacement	replace all substrings of the specified string value that match regexp with rep
sql		initcap	col	translate the first letter of each word to upper case in the sentence
sql		soundex	col	returns the soundex encoding for a string >>> df = spark
sql		bin	col	returns the string representation of the binary value of the given column
sql		hex	col	computes hex value of the given column which could be :class pyspark sql types stringtype,
sql		unhex	col	inverse of hex interprets each pair of characters as a hexadecimal number
sql		length	col	calculates the length of a string or binary expression
sql		translate	srcCol matching replace	a function translate any character in the srccol by a character in matching
sql		create_map		creates a new map column
sql		array		creates a new array column
sql		array_contains	col value	collection function returns null if the array is null true if the array contains the given value and false otherwise
sql		explode	col	returns a new row for each element in the given array or map
sql		posexplode	col	returns a new row for each element with position in the given array or map
sql		get_json_object	col path	extracts json object from a json string based on json path specified and returns json string of the extracted json object
sql		json_tuple	col	creates a new row for a json column according to the given field names
sql		from_json	col schema options	parses a column containing a json string into a [[structtype]] or [[arraytype]] of [[structtype]]s with the specified schema
sql		to_json	col options	converts a column containing a [[structtype]] or [[arraytype]] of [[structtype]]s into a json string
sql		size	col	collection function returns the length of the array or map stored in the column
sql		sort_array	col asc	collection function sorts the input array in ascending or descending order according to the natural ordering of the array elements
sql	UserDefinedFunction	_wrapped		wrap this udf with a function and attach docstring from func
sql		udf	f returnType	creates a :class column expression representing a user defined function udf
sql	GroupedData	agg		compute aggregates and returns the result as a :class dataframe
sql	GroupedData	count		counts the number of records for each group
sql	GroupedData	mean		computes average values for each numeric columns for each group
sql	GroupedData	avg		computes average values for each numeric columns for each group
sql	GroupedData	max		computes the max value for each numeric columns for each group
sql	GroupedData	min		computes the min value for each numeric column for each group
sql	GroupedData	sum		compute the sum for each numeric columns for each group
sql	GroupedData	pivot	pivot_col values	pivots a column of the current [[dataframe]] and perform the specified aggregation
sql		_to_seq	sc cols converter	convert a list of column or names into a jvm seq of column
sql		_to_list	sc cols converter	convert a list of column or names into a jvm scala list of column
sql		_unary_op	name doc	create a method for given unary operator
sql		_bin_op	name doc	create a method for given binary operator
sql		_reverse_op	name doc	create a method for binary operator this object is on right side
sql	Column	getItem	key	an expression that gets an item at position ordinal out of a list or gets an item by key out of a dict
sql	Column	getField	name	an expression that gets a field by name in a structfield
sql	Column	substr	startPos length	return a :class column which is a substring of the column
sql	Column	isin		a boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments
sql	Column	alias		returns this column aliased with a new name or names in the case of expressions that return more than one column such as explode
sql	Column	cast	dataType	convert the column into type datatype
sql	Column	between	lowerBound upperBound	a boolean expression that is evaluated to true if the value of this expression is between the given columns
sql	Column	when	condition value	evaluates a list of conditions and returns one of multiple possible result expressions
sql	Column	otherwise	value	evaluates a list of conditions and returns one of multiple possible result expressions
sql	Column	over	window	define a windowing column
sql	SparkSession	__init__	sparkContext jsparkSession	creates a new sparksession
sql	SparkSession	newSession		returns a new sparksession as new session that has separate sqlconf registered temporary views and udfs but shared sparkcontext and
sql	SparkSession	sparkContext		returns the underlying :class sparkcontext
sql	SparkSession	version		the version of spark on which this application is running
sql	SparkSession	conf		runtime configuration interface for spark
sql	SparkSession	catalog		interface through which the user may create drop alter or query underlying databases tables functions etc
sql	SparkSession	udf		returns a :class udfregistration for udf registration
sql	SparkSession	range	start end step numPartitions	create a :class dataframe with single :class pyspark sql types longtype column named
sql	SparkSession	_inferSchemaFromList	data	infer schema from list of row or tuple
sql	SparkSession	_inferSchema	rdd samplingRatio	infer schema from an rdd of row or tuple
sql	SparkSession	_createFromRDD	rdd schema samplingRatio	create an rdd for dataframe from an existing rdd returns the rdd and schema
sql	SparkSession	_createFromLocal	data schema	create an rdd for dataframe from a list or pandas dataframe returns
sql	SparkSession	createDataFrame	data schema samplingRatio verifySchema	creates a :class dataframe from an :class rdd, a list or a :class pandas dataframe
sql	SparkSession	sql	sqlQuery	returns a :class dataframe representing the result of the given query
sql	SparkSession	table	tableName	returns the specified table as a :class dataframe
sql	SparkSession	read		returns a :class dataframereader that can be used to read data in as a :class dataframe
sql	SparkSession	readStream		returns a :class datastreamreader that can be used to read data streams as a streaming :class dataframe
sql	SparkSession	streams		returns a :class streamingquerymanager that allows managing all the :class streamingquery streamingqueries active on this context
sql	SparkSession	stop		stop the underlying :class sparkcontext
sql	SparkSession	__enter__		enable 'with sparksession builder getorcreate() as session app' syntax
sql	SparkSession	__exit__	exc_type exc_val exc_tb	enable 'with sparksession builder getorcreate() as session app' syntax
sql	DataFrame	rdd		returns the content as an :class pyspark rdd of :class row
sql	DataFrame	na		returns a :class dataframenafunctions for handling missing values
sql	DataFrame	stat		returns a :class dataframestatfunctions for statistic functions
sql	DataFrame	toJSON	use_unicode	converts a :class dataframe into a :class rdd of string
sql	DataFrame	registerTempTable	name	registers this rdd as a temporary table using the given name
sql	DataFrame	createTempView	name	creates a local temporary view with this dataframe
sql	DataFrame	createOrReplaceTempView	name	creates or replaces a local temporary view with this dataframe
sql	DataFrame	createGlobalTempView	name	creates a global temporary view with this dataframe
sql	DataFrame	write		interface for saving the content of the non-streaming :class dataframe out into external storage
sql	DataFrame	writeStream		interface for saving the content of the streaming :class dataframe out into external storage
sql	DataFrame	schema		returns the schema of this :class dataframe as a :class pyspark sql types structtype
sql	DataFrame	printSchema		prints out the schema in the tree format
sql	DataFrame	explain	extended	prints the logical and physical plans to the console for debugging purpose
sql	DataFrame	isLocal		returns true if the :func collect and :func take methods can be run locally without any spark executors
sql	DataFrame	isStreaming		returns true if this :class dataset contains one or more sources that continuously return data as it arrives
sql	DataFrame	show	n truncate vertical	prints the first n rows to the console
sql	DataFrame	checkpoint	eager	returns a checkpointed version of this dataset checkpointing can be used to truncate the
sql	DataFrame	withWatermark	eventTime delayThreshold	defines an event time watermark for this :class dataframe a watermark tracks a point
sql	DataFrame	hint	name	specifies some hint on the current dataframe
sql	DataFrame	count		returns the number of rows in this :class dataframe
sql	DataFrame	collect		returns all the records as a list of :class row
sql	DataFrame	toLocalIterator		returns an iterator that contains all of the rows in this :class dataframe
sql	DataFrame	limit	num	limits the result count to the number specified
sql	DataFrame	take	num	returns the first num rows as a :class list of :class row
sql	DataFrame	foreach	f	applies the f function to all :class row of this :class dataframe
sql	DataFrame	foreachPartition	f	applies the f function to each partition of this :class dataframe
sql	DataFrame	cache		persists the :class dataframe with the default storage level (c{memory_and_disk})
sql	DataFrame	persist	storageLevel	sets the storage level to persist the contents of the :class dataframe across operations after the first time it is computed
sql	DataFrame	storageLevel		get the :class dataframe's current storage level
sql	DataFrame	unpersist	blocking	marks the :class dataframe as non-persistent and remove all blocks for it from memory and disk
sql	DataFrame	coalesce	numPartitions	returns a new :class dataframe that has exactly numpartitions partitions
sql	DataFrame	repartition	numPartitions	returns a new :class dataframe partitioned by the given partitioning expressions the
sql	DataFrame	distinct		returns a new :class dataframe containing the distinct rows in this :class dataframe
sql	DataFrame	sample	withReplacement fraction seed	returns a sampled subset of this :class dataframe
sql	DataFrame	sampleBy	col fractions seed	returns a stratified sample without replacement based on the fraction given on each stratum
sql	DataFrame	randomSplit	weights seed	randomly splits this :class dataframe with the provided weights
sql	DataFrame	dtypes		returns all column names and their data types as a list
sql	DataFrame	columns		returns all column names as a list
sql	DataFrame	alias	alias	returns a new :class dataframe with an alias set
sql	DataFrame	crossJoin	other	returns the cartesian product with another :class dataframe
sql	DataFrame	join	other on how	joins with another :class dataframe, using the given join expression
sql	DataFrame	sortWithinPartitions		returns a new :class dataframe with each partition sorted by the specified column s
sql	DataFrame	sort		returns a new :class dataframe sorted by the specified column s
sql	DataFrame	_jseq	cols converter	return a jvm seq of columns from a list of column or names
sql	DataFrame	_jmap	jm	return a jvm scala map from a dict
sql	DataFrame	_jcols		return a jvm seq of columns from a list of column or column names if cols has only one list in it cols[0] will be used as the list
sql	DataFrame	_sort_cols	cols kwargs	return a jvm seq of columns that describes the sort order
sql	DataFrame	describe		computes statistics for numeric and string columns
sql	DataFrame	head	n	returns the first n rows
sql	DataFrame	first		returns the first row as a :class row
sql	DataFrame	__getitem__	item	returns the column as a :class column
sql	DataFrame	__getattr__	name	returns the :class column denoted by name
sql	DataFrame	select		projects a set of expressions and returns a new :class dataframe
sql	DataFrame	selectExpr		projects a set of sql expressions and returns a new :class dataframe
sql	DataFrame	filter	condition	filters rows using the given condition
sql	DataFrame	groupBy		groups the :class dataframe using the specified columns so we can run aggregation on them
sql	DataFrame	rollup		create a multi-dimensional rollup for the current :class dataframe using the specified columns so we can run aggregation on them
sql	DataFrame	cube		create a multi-dimensional cube for the current :class dataframe using the specified columns so we can run aggregation on them
sql	DataFrame	agg		aggregate on the entire :class dataframe without groups (shorthand for df
sql	DataFrame	union	other	return a new :class dataframe containing union of rows in this frame and another frame
sql	DataFrame	unionAll	other	return a new :class dataframe containing union of rows in this frame and another frame
sql	DataFrame	intersect	other	return a new :class dataframe containing rows only in both this frame and another frame
sql	DataFrame	subtract	other	return a new :class dataframe containing rows in this frame but not in another frame
sql	DataFrame	dropDuplicates	subset	return a new :class dataframe with duplicate rows removed optionally only considering certain columns
sql	DataFrame	dropna	how thresh subset	returns a new :class dataframe omitting rows with null values
sql	DataFrame	fillna	value subset	replace null values alias for na fill()
sql	DataFrame	replace	to_replace value subset	returns a new :class dataframe replacing a value with another value
sql	DataFrame	approxQuantile	col probabilities relativeError	calculates the approximate quantiles of numerical columns of a dataframe
sql	DataFrame	corr	col1 col2 method	calculates the correlation of two columns of a dataframe as a double value
sql	DataFrame	cov	col1 col2	calculate the sample covariance for the given columns specified by their names as a double value
sql	DataFrame	crosstab	col1 col2	computes a pair-wise frequency table of the given columns also known as a contingency
sql	DataFrame	freqItems	cols support	finding frequent items for columns possibly with false positives using the
sql	DataFrame	withColumn	colName col	returns a new :class dataframe by adding a column or replacing the existing column that has the same name
sql	DataFrame	withColumnRenamed	existing new	returns a new :class dataframe by renaming an existing column
sql	DataFrame	drop		returns a new :class dataframe that drops the specified column
sql	DataFrame	toDF		returns a new class dataframe that with new specified column names
sql	DataFrame	toPandas		returns the contents of this :class dataframe as pandas pandas dataframe
sql		_to_scala_map	sc jm	convert a dict into a jvm map
sql		to_str	value	a wrapper over str(), but converts bool values to lower case strings
sql	OptionUtils	_set_opts	schema	set named options filter out those the value is none
sql	DataFrameReader	format	source	specifies the input data source format
sql	DataFrameReader	schema	schema	specifies the input schema
sql	DataFrameReader	option	key value	adds an input option for the underlying data source
sql	DataFrameReader	options		adds input options for the underlying data source
sql	DataFrameReader	load	path format schema	loads data from a data source and returns it as a :classdataframe
sql	DataFrameReader	json	path schema primitivesAsString prefersDecimal	loads json files and returns the results as a :class dataframe
sql	DataFrameReader	table	tableName	returns the specified table as a :class dataframe
sql	DataFrameReader	parquet		loads parquet files returning the result as a :class dataframe
sql	DataFrameReader	text	paths	loads text files and returns a :class dataframe whose schema starts with a string column named "value", and followed by partitioned columns if there
sql	DataFrameReader	csv	path schema sep encoding	loads a csv file and returns the result as a :class dataframe
sql	DataFrameReader	orc	path	loads orc files returning the result as a :class dataframe
sql	DataFrameReader	jdbc	url table column lowerBound	construct a :class dataframe representing the database table named table accessible via jdbc url url and connection properties
sql	DataFrameWriter	mode	saveMode	specifies the behavior when data or table already exists
sql	DataFrameWriter	format	source	specifies the underlying output data source
sql	DataFrameWriter	option	key value	adds an output option for the underlying data source
sql	DataFrameWriter	options		adds output options for the underlying data source
sql	DataFrameWriter	partitionBy		partitions the output by the given columns on the file system
sql	DataFrameWriter	bucketBy	numBuckets col	buckets the output by the given columns if specified
sql	DataFrameWriter	sortBy	col	sorts the output in each bucket by the given columns on the file system
sql	DataFrameWriter	save	path format mode partitionBy	saves the contents of the :class dataframe to a data source
sql	DataFrameWriter	insertInto	tableName overwrite	inserts the content of the :class dataframe to the specified table
sql	DataFrameWriter	saveAsTable	name format mode partitionBy	saves the content of the :class dataframe as the specified table
sql	DataFrameWriter	json	path mode compression dateFormat	saves the content of the :class dataframe in json format (json lines text format or newline-delimited json <http //jsonlines
sql	DataFrameWriter	parquet	path mode partitionBy compression	saves the content of the :class dataframe in parquet format at the specified path
sql	DataFrameWriter	text	path compression	saves the content of the dataframe in a text file at the specified path
sql	DataFrameWriter	csv	path mode compression sep	saves the content of the :class dataframe in csv format at the specified path
sql	DataFrameWriter	orc	path mode partitionBy compression	saves the content of the :class dataframe in orc format at the specified path
sql	DataFrameWriter	jdbc	url table mode properties	saves the content of the :class dataframe to an external database table via jdbc
sql	Window	partitionBy		creates a :class windowspec with the partitioning defined
sql	Window	orderBy		creates a :class windowspec with the ordering defined
sql	Window	rowsBetween	start end	creates a :class windowspec with the frame boundaries defined from start inclusive to end inclusive
sql	Window	rangeBetween	start end	creates a :class windowspec with the frame boundaries defined from start inclusive to end inclusive
sql	WindowSpec	partitionBy		defines the partitioning columns in a :class windowspec
sql	WindowSpec	orderBy		defines the ordering columns in a :class windowspec
sql	WindowSpec	rowsBetween	start end	defines the frame boundaries from start inclusive to end inclusive
sql	WindowSpec	rangeBetween	start end	defines the frame boundaries from start inclusive to end inclusive
sql	SQLTests	test_column_name_encoding		ensure that created columns has str type consistently
sql	StreamingQuery	id		returns the unique id of this query that persists across restarts from checkpoint data
sql	StreamingQuery	runId		returns the unique id of this query that does not persist across restarts that is every
sql	StreamingQuery	name		returns the user-specified name of the query or null if not specified
sql	StreamingQuery	isActive		whether this streaming query is currently active or not
sql	StreamingQuery	awaitTermination	timeout	waits for the termination of this query either by :func query stop() or by an
sql	StreamingQuery	status		returns the current status of the query
sql	StreamingQuery	recentProgress		returns an array of the most recent [[streamingqueryprogress]] updates for this query
sql	StreamingQuery	lastProgress		returns the most recent :class streamingqueryprogress update of this streaming query or
sql	StreamingQuery	processAllAvailable		blocks until all available data in the source has been processed and committed to the sink
sql	StreamingQuery	stop		stop this streaming query
sql	StreamingQuery	explain	extended	prints the logical and physical plans to the console for debugging purpose
sql	StreamingQuery	exception		:return the streamingqueryexception if the query was terminated by an exception or none
sql	StreamingQueryManager	active		returns a list of active queries associated with this sqlcontext >>> sq = sdf
sql	StreamingQueryManager	get	id	returns an active query from this sqlcontext or throws exception if an active query with this name doesn't exist
sql	StreamingQueryManager	awaitAnyTermination	timeout	wait until any of the queries on the associated sqlcontext has terminated since the creation of the context or since :func resetterminated() was called
sql	StreamingQueryManager	resetTerminated		forget about past terminated queries so that :func awaitanytermination() can be used again to wait for new terminations
sql	DataStreamReader	format	source	specifies the input data source format
sql	DataStreamReader	schema	schema	specifies the input schema
sql	DataStreamReader	option	key value	adds an input option for the underlying data source
sql	DataStreamReader	options		adds input options for the underlying data source
sql	DataStreamReader	load	path format schema	loads a data stream from a data source and returns it as a :classdataframe
sql	DataStreamReader	json	path schema primitivesAsString prefersDecimal	loads a json file stream and returns the results as a :class dataframe
sql	DataStreamReader	parquet	path	loads a parquet file stream returning the result as a :class dataframe
sql	DataStreamReader	text	path	loads a text file stream and returns a :class dataframe whose schema starts with a string column named "value", and followed by partitioned columns if there
sql	DataStreamReader	csv	path schema sep encoding	loads a csv file stream and returns the result as a :class dataframe
sql	DataStreamWriter	outputMode	outputMode	specifies how data of a streaming dataframe/dataset is written to a streaming sink
sql	DataStreamWriter	format	source	specifies the underlying output data source
sql	DataStreamWriter	option	key value	adds an output option for the underlying data source
sql	DataStreamWriter	options		adds output options for the underlying data source
sql	DataStreamWriter	partitionBy		partitions the output by the given columns on the file system
sql	DataStreamWriter	queryName	queryName	specifies the name of the :class streamingquery that can be started with :func start
sql	DataStreamWriter	trigger	processingTime once	set the trigger for the stream query if this is not set it will run the query as fast
sql	DataStreamWriter	start	path format outputMode partitionBy	streams the contents of the :class dataframe to a data source
sql	RuntimeConfig	__init__	jconf	create a new runtimeconfig that wraps the underlying jvm object
sql	RuntimeConfig	set	key value	sets the given spark runtime configuration property
sql	RuntimeConfig	get	key default	returns the value of spark runtime configuration property for the given key assuming it is set
sql	RuntimeConfig	unset	key	resets the configuration property for the given key
sql	RuntimeConfig	_checkType	obj identifier	assert that an object is of type str
sql	Catalog	__init__	sparkSession	create a new catalog that wraps the underlying jvm object
sql	Catalog	currentDatabase		returns the current default database in this session
sql	Catalog	setCurrentDatabase	dbName	sets the current default database in this session
sql	Catalog	listDatabases		returns a list of databases available across all sessions
sql	Catalog	listTables	dbName	returns a list of tables/views in the specified database
sql	Catalog	listFunctions	dbName	returns a list of functions registered in the specified database
sql	Catalog	listColumns	tableName dbName	returns a list of columns for the given table/view in the specified database
sql	Catalog	createExternalTable	tableName path source schema	creates a table based on the dataset in a data source
sql	Catalog	createTable	tableName path source schema	creates a table based on the dataset in a data source
sql	Catalog	dropTempView	viewName	drops the local temporary view with the given view name in the catalog
sql	Catalog	dropGlobalTempView	viewName	drops the global temporary view with the given view name in the catalog
sql	Catalog	registerFunction	name f returnType	registers a python function including lambda function as a udf so it can be used in sql statements
sql	Catalog	isCached	tableName	returns true if the table is currently cached in-memory
sql	Catalog	cacheTable	tableName	caches the specified table in-memory
sql	Catalog	uncacheTable	tableName	removes the specified table from the in-memory cache
sql	Catalog	clearCache		removes all cached tables from the in-memory cache
sql	Catalog	refreshTable	tableName	invalidates and refreshes all the cached data and metadata of the given table
sql	Catalog	recoverPartitions	tableName	recovers all the partitions of the given table and update the catalog
sql	Catalog	refreshByPath	path	invalidates and refreshes all the cached data and the associated metadata for any dataframe that contains the given data source path
sql	Catalog	_reset		internal use only drop all existing databases (except "default"), tables partitions and functions and set the current database to "default"
sql		install_exception_handler		hook an exception handler into py4j which could capture some sql exceptions in java
sql		toJArray	gateway jtype arr	convert python list to java type array
sql	DataType	needConversion		does this type need to conversion between python object and internal sql object
sql	DataType	toInternal	obj	converts a python object into an internal sql object
sql	DataType	fromInternal	obj	converts an internal sql object into a native python object
sql	ArrayType	__init__	elementType containsNull	>>> arraytype(stringtype()) == arraytype(stringtype(), true)
sql	MapType	__init__	keyType valueType valueContainsNull	>>> (maptype(stringtype(), integertype())
sql	StructField	__init__	name dataType nullable metadata	>>> (structfield("f1", stringtype(), true)
sql	StructType	__init__	fields	>>> struct1 = structtype([structfield("f1", stringtype(), true)]) >>> struct2 = structtype([structfield("f1", stringtype(), true)])
sql	StructType	add	field data_type nullable metadata	construct a structtype by adding new elements to it to define the schema the method accepts
sql	StructType	__iter__		iterate the fields
sql	StructType	__len__		return the number of fields
sql	StructType	__getitem__	key	access fields by name or slice
sql	UserDefinedType	sqlType	cls	underlying sql storage type for this udt
sql	UserDefinedType	module	cls	the python module of the udt
sql	UserDefinedType	scalaUDT	cls	the class name of the paired scala udt (could be '', if there is no corresponding one)
sql	UserDefinedType	_cachedSqlType	cls	cache the sqltype() into class because it's heavy used in tointernal
sql	UserDefinedType	serialize	obj	converts the a user-type object into a sql datum
sql	UserDefinedType	deserialize	datum	converts a sql datum into a user-type object
sql		_ignore_brackets_split	s separator	splits the given string by given separator but ignore separators inside brackets pairs e g
sql		_parse_datatype_string	s	parses the given data type string to a :class datatype the data type string format equals
sql		_parse_datatype_json_string	json_string	parses the given data type json string
sql		_infer_type	obj	infer the datatype from obj
sql		_infer_schema	row	infer the schema from dict/namedtuple/object
sql		_has_nulltype	dt	return whether there is nulltype in dt or not
sql		_create_converter	dataType	create a converter to drop the names of fields in obj
sql		_split_schema_abstract	s	split the schema abstract into fields
sql		_parse_field_abstract	s	parse a field in schema abstract >>> _parse_field_abstract("a")
sql		_parse_schema_abstract	s	parse abstract into schema >>> _parse_schema_abstract("a b c")
sql		_infer_schema_type	obj dataType	fill the datatype with types inferred from obj >>> schema = _parse_schema_abstract("a b c d")
sql		_verify_type	obj dataType nullable	verify the type of obj against datatype raise a typeerror if they do not match
sql	Row	asDict	recursive	return as an dict
sql	Row	__call__		create new row object
sql	Row	__reduce__		returns a tuple so python knows how to pickle row
sql	Row	__repr__		printable representation of row used in python repl
sql	SQLContext	__init__	sparkContext sparkSession jsqlContext	creates a new sqlcontext
sql	SQLContext	_ssql_ctx		accessor for the jvm spark sql context
sql	SQLContext	getOrCreate	cls sc	get the existing sqlcontext or create a new one with given sparkcontext
sql	SQLContext	newSession		returns a new sqlcontext as new session that has separate sqlconf registered temporary views and udfs but shared sparkcontext and
sql	SQLContext	setConf	key value	sets the given spark sql configuration property
sql	SQLContext	getConf	key defaultValue	returns the value of spark sql configuration property for the given key
sql	SQLContext	udf		returns a :class udfregistration for udf registration
sql	SQLContext	range	start end step numPartitions	create a :class dataframe with single :class pyspark sql types longtype column named
sql	SQLContext	registerFunction	name f returnType	registers a python function including lambda function as a udf so it can be used in sql statements
sql	SQLContext	registerJavaFunction	name javaClassName returnType	register a java udf so it can be used in sql statements
sql	SQLContext	_inferSchema	rdd samplingRatio	infer schema from an rdd of row or tuple
sql	SQLContext	createDataFrame	data schema samplingRatio verifySchema	creates a :class dataframe from an :class rdd, a list or a :class pandas dataframe
sql	SQLContext	registerDataFrameAsTable	df tableName	registers the given :class dataframe as a temporary table in the catalog
sql	SQLContext	dropTempTable	tableName	remove the temp table from catalog
sql	SQLContext	createExternalTable	tableName path source schema	creates an external table based on the dataset in a data source
sql	SQLContext	sql	sqlQuery	returns a :class dataframe representing the result of the given query
sql	SQLContext	table	tableName	returns the specified table or view as a :class dataframe
sql	SQLContext	tables	dbName	returns a :class dataframe containing names of tables in the given database
sql	SQLContext	tableNames	dbName	returns a list of names of tables in the database dbname
sql	SQLContext	cacheTable	tableName	caches the specified table in-memory
sql	SQLContext	uncacheTable	tableName	removes the specified table from the in-memory cache
sql	SQLContext	clearCache		removes all cached tables from the in-memory cache
sql	SQLContext	read		returns a :class dataframereader that can be used to read data in as a :class dataframe
sql	SQLContext	readStream		returns a :class datastreamreader that can be used to read data streams as a streaming :class dataframe
sql	SQLContext	streams		returns a :class streamingquerymanager that allows managing all the :class streamingquery streamingqueries active on this context
sql	HiveContext	_createForTesting	cls sparkContext	internal use only create a new hivecontext for testing
sql	HiveContext	refreshTable	tableName	invalidate and refresh all the cached the metadata of the given table
