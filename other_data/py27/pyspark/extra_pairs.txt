vector indexer	class for indexing categorical feature columns in a dataset of vector
how	'any' or 'all'
n	int default 1. number of rows to return
min max scaler	rescale each feature individually to a common range [min max] linearly using column summary statistics which is also known as min-max normalization or rescaling
inferSchema	infers the input schema automatically from data. it requires one extra
topics	list of topic_name to consume
tableName	string name of the table
date type	date datetime date data type
col	str list
distCol	output column for storing the distance between each result row and the key
lda	latent dirichlet allocation lda a topic model designed for text documents
standard scaler	standardizes features by removing the mean and scaling to unit variance using column summary statistics on the samples in the
num py tests	general pyspark tests that depend on numpy
analysis exception	failed to analyze a sql query plan
path	path specifying the directory in which to save
options	options to control parsing. accepts the same options as the json datasource
min max scaler model	model fitted by :py class minmaxscaler
n	an integer
end	the end value exclusive
tree classifier params	private class to track supported impurity measures
pyFiles	collection of .zip or .py files to send to the cluster
indexed row	represents a row of an indexedrowmatrix
to_replace	bool int long float string list or dict
schema	an optional :class pyspark.sql.types.structtype for the input schema
runtime config	user-facing configuration api accessible through sparksession conf
path	path specifying the directory to which the model
mean	mean or lambda for the poisson distribution
test params	a subclass of params mixed with hasmaxiter hasinputcol and hasseed
upperBound	the maximum value of column used to decide partition stride
dataset	input dataset which is an instance of :py class pyspark.sql.dataframe
chi square test	note : experimental
numItems	max number of recommendations for each user
correlation	note : experimental
maxMalformedLogPerPartition	this parameter is no longer used since spark 2.2.0
size	size of the rdd
dbName	string name of the database to use. default to the current database
bisecting kmeans	a bisecting k-means algorithm based on the paper "a comparison of document clustering techniques" by steinbach karypis and kumar
idfmodel	model fitted by :py class idf
rdd	an rdd of row or tuple
offset range	represents a range of offsets from a single kafka topicandpartition
python only udt	user-defined type udt for examplepoint
chi sq selector	creates a chisquared feature selector
matrix factorization model	a matrix factorisation model trained by regularized alternating least-squares
data	list of row or tuple
properties	a dictionary of jdbc database connection arguments. normally at
delayThreshold	the minimum delay to wait to data to arrive late relative to the
data stream writer	interface used to write a streaming :class dataframe to external storage systems (e
auto serializer	choose marshal or pickle as serialization protocol automatically
streamName	kinesis stream name
method	string specifying the method to use for computing correlation
hive context	a variant of spark sql that integrates with data stored in hive
train validation split	note : experimental
java mlwritable	private mixin for ml instances that provide :py class javamlwriter
struct type	struct type consisting of a list of :class structfield
bounded float	bounded value is generated by approximate job with confidence and low bound and high bound
extended	boolean default false. if false, prints only the physical plan
data	an rdd of any kind of sql data representation(e.g. row tuple int boolean
cols	list of column names string or list of :class column expressions that grouped
dataset	the data used to compute the mean and variance
quantile discretizer	note : experimental
prefersDecimal	infers all floating-point values as a decimal type. if the values
spark session	the entry point to programming spark with the dataset and dataframe api
java prediction model	private java model for prediction tasks regression and classification
multilayer perceptron classifier	classifier trainer based on the multilayer perceptron
decision tree params	mixin for decision tree parameters
rformula model	note : experimental
kmeans model	model fitted by kmeans
ridge regression model	a linear regression model derived from a least-squares fit with an l_2 penalty term
auto batched serializer	choose the size of batch automatically based on the size of object
subset	optional list of column names to consider
naive bayes model	model fitted by naivebayes
mlreadable	mixin for instances that provide :py class mlreader
quote	sets the single character used for escaping quoted values where the
one vs rest	note : experimental
has tol	mixin for param tol the convergence tolerance for iterative algorithms (>= 0)
stsAssumeRoleArn	arn of iam role to assume when using sts sessions to read from
pairs	list of key-value pairs to set
java saveable	mixin for models that provide save() through their scala implementation
existing	string name of the existing column to rename
threshold	the threshold for the distance of row pairs
fpgrowth model	note : experimental
spark context	main entry point for spark functionality a sparkcontext represents the
path	path to hadoop file
addresses	list of host port s on which the spark sink is running
vec	a :py class pyspark.ml.linalg.vector
maxBatchSize	the maximum number of events to be pulled from the spark sink
numeric type	numeric data types
catalog	user-facing catalog api accessible through sparksession catalog
sc	spark context used for loading model files
string type	string data type
gateway	py4j gateway
random forest regressor	random forest <http //en wikipedia org/wiki/random_forest>_
imputer	note : experimental
samplingRatio	sampling ratio or no sampling default
col	name of column containing array
svmwith sgd	versionadded : 0 9 0
logistic regression	logistic regression
generalized linear regression summary	note : experimental
gaussian mixture	learning algorithm for gaussian mixtures using the expectation-maximization algorithm
path	optional string or a list of string for file-system backed data sources
pivot_col	name of the column to pivot
gateway	use an existing gateway and jvm otherwise a new jvm
seed	seed for the rng that generates the seed for the generator in each partition
sc	sparkcontext
linear regression	linear regression
path	optional string for file-system backed data sources
params	an optional param map that overrides embedded params
status tracker	low-level status reporting apis for monitoring job and stage progress
table	name of the table in the external database
grouped data	a set of methods for aggregations on a :class dataframe, created by :func dataframe
pipelined rdd	pipelined maps >>> rdd = sc
has raw prediction col	mixin for param rawpredictioncol raw prediction a k a confidence column name
pos	start position zero based
valueClass	fully qualified classname of value writable class
sparkHome	location where spark is installed on cluster nodes
java model wrapper	wrapper for the model in jvm
bisecting kmeans model	model fitted by bisectingkmeans
word2vec model	model fitted by :py class word2vec
basic profiler	basicprofiler is the default profiler which is implemented based on
col	string column in json format
gbtparams	private class to track supported gbt params
logistic regression model	model fitted by logisticregression
cols	list of :class column or column names to sort by
column	a column in a dataframe
dct	a feature transformer that takes the 1d discrete cosine transform of a real vector
storageLevel	storage level to use for storing the received objects (default is
value	value to check for in array
random forest regression model	model fitted by :class randomforestregressor
stop words remover	a feature transformer that filters out stop words from input
parallelism	number of concurrent requests this stream should send to the sink
nanValue	sets the string representation of a non-number value. if none is set it
mean	mean of the log normal distribution
has thresholds	mixin for param thresholds thresholds in multi-class classification to adjust the probability of predicting each class array must have length equal to the number of classes with values > 0 excepting that at most one value may be 0 the class with largest value p/t is predicted where p is the original probability of that class and t is the class's threshold
port	port of the slave machine to which the flume data will be sent
col	a name of a column or a list of names
loadDefaults	whether to load values from java system
logistic regression training summary	note : experimental
streaming query exception	exception that stopped a :class streamingquery
data frame	a distributed collection of data grouped into named columns
cols	list of column names string or list of :class column expressions that have
decision tree classification model	model fitted by decisiontreeclassifier
sci py tests	test both vector operations and mllib algorithms with scipy sparse matrices if scipy is available
how	str default inner. must be one of inner, cross, outer,
rdd	a resilient distributed dataset rdd the basic abstraction in spark
batched serializer	serializes a stream of objects in batches by calling its wrapped serializer with streams of objects
utf8deserializer	deserializes streams written by string getbytes
metadata	any additional metadata default none
schema	a :class pyspark.sql.types.structtype object
dstream	a discretized stream dstream the basic abstraction in spark streaming is a continuous sequence of rdds of the same type representing a
ldamodel	latent dirichlet allocation lda model
mat	a :py class pyspark.ml.linalg.matrix
dataset	a dataset that contains labels/observations and predictions
verifySchema	verify data types of every row against schema
block matrix	represents a distributed matrix in blocks of local matrices
conf	spark configuration passed to spark-submit
parameters	optional parameters
leaders	kafka brokers for each topicandpartition in offsetranges. may be an empty
seed	random seed
checkpointInterval	checkpoint interval for kinesis checkpointing. see the kinesis
processingTime	a processing time interval as a string e.g. '5 seconds', '1 minute'
kmeans model	a clustering model derived from the k-means method
fpgrowth	a parallel fp-growth algorithm to mine frequent itemsets
cols	list of column names string or expressions (:class column)
returnType	a :class pyspark.sql.types.datatype object
pca	a feature transformer that projects vectors to a low-dimensional space using pca
storage level	flags for controlling the storage of an rdd each storagelevel records whether to use memory
fpgrowth model	a fp-growth model for mining frequent itemsets using the parallel fp-growth algorithm
defaultValueStr	string representation of the default value
col	the column name of the numeric value to be formatted
path	directory to the input data files
doc	param doc
standard scaler model	model fitted by :py class standardscaler
_jconf	optionally pass in an existing sparkconf handle
multilabel metrics	evaluator for multilabel classification
decision tree model	a decision tree model for classification or regression
linear data generator	utils for generating linear data
lasso with sgd	versionadded : 0 9 0
catch_assertions	if false default do not catch assertionerrors
conf	hadoop job configuration passed in as a dict none by default
outputMode	specifies how data of a streaming dataframe/dataset is written to a
other	right side of the join
name	the table name
mode	specifies the behavior of the save operation when data already exists
gaussian mixture model	model fitted by gaussianmixture
has confidence	mixin for param confidence
params	additional values which need to be provided for
evaluator	base class for evaluators that compute metrics from predictions
zkQuorum	zookeeper quorum hostname port hostname port .
lshmodel	mixin for locality sensitive hashing lsh models
map type	map data type
expected	vector containing the expected categorical counts/relative
dateFormat	sets the string that indicates a date format. custom date formats
has solver	mixin for param solver the solver algorithm for optimization if this is not set or empty default value is 'auto'
data frame na functions	functionality for working with missing data in :class dataframe
vertical	if set to true print output rows vertically (one line
path	the path in a hadoop supported file system
user defined type	user-defined type udt
float type	float data type representing single precision floats
checkCode	whether or not to check the return value of the shell command
tree ensemble model	treeensemblemodel
naive bayes	versionadded : 0 9 0
double type	double data type representing double precision floats
samplingRatio	the sample ratio of rows used for inferring
python only point	an example class to demonstrate udt in only python
transformed dstream	transformeddstream is a dstream generated by an python function transforming each rdd of a dstream to another rdds
fraction	expected size of the sample as a fraction of this rdd's size
path	file or directory path in any hadoop-supported file
messageHandler	a function used to convert kafkamessageandmetadata. you can assess
cols	additional names optional . if col is a list it should be empty
storageLevel	rdd storage level
allowSingleQuotes	allows single quotes in addition to double quotes. if none is
nullValue	sets the string representation of a null value. if none is set it uses
numUsers	max number of recommendations for each item
accumulator server	a simple tcp server that intercepts shutdown() in order to interrupt our continuous polling on the handler
data stream reader	interface used to load a streaming :class dataframe from external storage systems (e
start	the start value
normalizer	normalize a vector to have unit norm using the given p-norm
to	the target instance
vector	vector or rdd of vector to be normalized
str	a column of :class pyspark.sql.types.stringtype
partition	partition id of this kafka message
one vs rest params	parameters for onevsrest and onevsrestmodel
kmeans	k-means clustering with a k-means++ like initialization mode (the k-means|| algorithm by bahmani et al)
eventTime	the name of the column that contains the event time of the row
cols	name of columns
multiclass metrics	evaluator for multiclass classification
gbtclassifier	gradient-boosted trees gbts <http //en wikipedia org/wiki/gradient_boosting>_
valueDecoder	a function used to decode value (default is utf8_decoder)
labeled point	class that represents the features and labels of a data point
seed	random seed default a random long integer
profiler_cls	a class of custom profiler used to do profiling
method	the correlation method. currently only supports "pearson"
vector assembler	a feature transformer that merges multiple columns into a vector column
metadata	a dict of information to be stored in metadata attribute of the
awsSecretKey	aws secretkey (default is none. if none will use
data	an rdd of any kind of sql data representation(e.g. :class row,
start	boundary start inclusive
numBuckets	the number of buckets to save
simple aggregator	simpleaggregator is useful for the cases that combiners have
linear regression model	a linear regression model derived from a least-squares fit
max abs scaler model	model fitted by :py class maxabsscaler
on	a string for the join column name a list of column names
data	an rdd[labeledpoint] containing the labeled dataset
numPartitions	number of partitions in the rdd (default sc.defaultparallelism)
col2	the name of the second column
dense vector	a dense vector represented by a value array we use numpy array for
max abs scaler	rescale each feature individually to range [-1 1] by dividing through the largest maximum absolute value in each feature
ignoreLeadingWhiteSpace	a flag indicating whether or not leading whitespaces from
fromOffset	inclusive starting offset
streaming kmeans model	clustering model which can perform an online update of the centroids
exprs	a dict mapping from column name string to aggregate functions string
count	number of row to extend
storageLevel	storage level to use for storing the received objects
recordLength	the length at which to split the records
mlwritable	mixin for ml instances that provide :py class mlwriter
awsAccessKeyId	aws accesskeyid (default is none. if none will use
col	name of column containing the struct or array of the structs
numPartitions	number of partitions in the rdd
atomic type	an internal type used to represent everything that is not null udts arrays structs and maps
wholeFile	parse records which may span multiple lines. if none is
mode	one of append, overwrite, error, ignore default error
example point	an example class to demonstrate udt in scala java and python
broadcast	a broadcast variable created with l{sparkcontext broadcast()}
loader	mixin for classes which can load saved models from files
enableDecompression	should netty server decompress input stream
dataset	input dataset which is an instance of
numCols	number of elements in each vector
chi sq test result	contains test results for the chi-squared hypothesis test
streaming query manager	a class to manage all the :class streamingquery streamingqueries active
ngram	a feature transformer that converts the input array of strings into an array of n-grams null
type converters	note : developerapi
data type	base class for data types
col1	the name of the first column. distinct items will make the first item of
linear svcmodel	note : experimental
udfregistration	wrapper for user-defined function registration
ranking metrics	evaluator for ranking algorithms
kafka dstream	a python wrapper of kafkadstream
svmmodel	model for support vector machines svms
cols	list of columns to group by
framed serializer	serializer that writes objects as a stream of length data pairs where c{length} is a 32-bit integer and data is c{length} bytes
naive bayes	naive bayes classifiers
cols	names of the columns to calculate frequent items for as a list or tuple of
key	a function used to generate key for comparing
binarizer	binarize a column of continuous features given a threshold
pcamodel	model fitted by [[pca]] that can project vectors to a low-dimensional space using pca
random forest params	private class to track supported random forest parameters
transformer	abstract class for transformers that transform one dataset into another
topic	topic name of this kafka message
other test params	a subclass of params mixed with hasmaxiter hasinputcol and hasseed
prefix span model	model fitted by prefixspan >>> data = [
linear svc	note : experimental
truncate	if set to true truncate strings longer than 20 chars by default
compressed serializer	compress the serialized data
binary classification evaluator	note : experimental
list tests	test mllib algorithms on plain lists to make sure they're passed through as numpy arrays
word2vec	word2vec trains a model of map string vector , i e transforms a word into a code for further
n	number of rows to show
paths	string or list of strings for input path s
col	name of column or expression
table	the name of the table
columnNameOfCorruptRecord	allows renaming the new field having malformed string
has probability col	mixin for param probabilitycol column name for predicted class conditional probabilities note not all models output well-calibrated probability estimates! these probabilities should be treated as confidences not precise probabilities
matrix udt	sql user-defined type udt for matrix
user defined function	user defined function in python
relativeSD	relative accuracy. smaller values create
isotonic regression model	model fitted by :class isotonicregression
spark conf	configuration for a spark application used to set various spark
vector udt	sql user-defined type udt for vector
sparse matrix	sparse matrix stored in csc format
data type singleton	metaclass for datatype
alias	strings of desired column names collects all positional arguments passed
random forest model	represents a random forest model
kafka transformed dstream	kafka specific wrapper of transformeddstream to transform on kafka rdd
decimal type	decimal decimal decimal data type
initialPositionInStream	in the absence of kinesis checkpoint info this is the
value	a list of transformers or estimators
param grid builder	builder for a param grid used in grid search-based model selection
lasso model	a linear regression model derived from a least-squares fit with an l_1 penalty term
row matrix	represents a row-oriented distributed matrix with no meaningful row indices
other	right side of the cartesian product
negativeInf	sets the string representation of a negative infinity value. if none
maxCharsPerColumn	defines the maximum number of characters allowed for any given
name	a name of the hint
alsmodel	model fitted by als
java model	base class for :py class models that wrap java/scala implementations
count vectorizer model	model fitted by :py class countvectorizer
weights	list of doubles as weights with which to split the dataframe. weights will
bisecting kmeans model	a clustering model derived from the bisecting k-means method
short type	short data type i e a signed 16-bit integer
has label col	mixin for param labelcol label column name
marshal serializer	serializes objects using python's marshal serializer http //docs
word2vec model	class for word2vec model
null type	null type
allowNumericLeadingZero	allows leading zeros in numbers e.g. 00012 . if none is
coordinate matrix	represents a matrix in coordinate format
rCond	reciprocal condition number. all singular values
minPartitions	min number of partitions
quoteAll	a flag indicating whether all values should always be enclosed in
integer type	int data type i e a signed 32-bit integer
vector indexer model	model fitted by :py class vectorindexer
param	param name or the param instance which must
dataset	the dataset to search for nearest neighbors of the key
param	a param with self-contained documentation
size	size of the vector
timestampFormat	sets the string that indicates a timestamp format. custom date
arr	python type list
random forest	learning algorithm for a random forest model for classification or regression
gaussian mixture model	a clustering model derived from the gaussian mixture model method
bisecting kmeans	a bisecting k-means algorithm based on the paper "a comparison of document clustering techniques" by steinbach karypis and kumar with modification to fit spark
random forest classifier	random forest <http //en wikipedia org/wiki/random_forest>_
transform function	this class wraps a function rdd[x] -> rdd[y] that was passed to dstream
utcoffset timezone	specifies timezone in utc offset
standard scaler	standardizes features by removing the mean and scaling to unit variance using column summary statistics on the samples in the training set
binary classification metrics	evaluator for binary classification
cross validator	k-fold cross validation performs model selection by splitting the dataset into a set of non-overlapping randomly partitioned folds which are used as separate training and test datasets
sci py tests	general pyspark tests that depend on scipy
y	an rdd of float of the same cardinality as x
external list of list	an external list for list
data_type	if present the datatype of the structfield to create
datasetA	one of the datasets to join
linear regression model base	a linear regression model
depth	suggested depth of the tree default 2
ignoreTrailingWhiteSpace	a flag indicating whether or not trailing whitespaces from
vector transformer	note : developerapi
java params	utility class to help create wrapper classes from java/scala implementations of pipeline components
extra	extra parameters
substr	a string
chi sq selector	note : experimental
spark files	resolves paths to files added through l{sparkcontext
external sorter	extenalsorter will divide the elements into chunks sort them in memory and dump them into disks finally merge them back
regex tokenizer	a regex based tokenizer that extracts tokens either by using the provided regex pattern in java dialect to split the text
has weight col	mixin for param weightcol weight column name if this is not set or empty we treat all instance weights as 1 0
java mlreadable	private mixin for instances that provide javamlreader
options	all other string options. you may want to provide a checkpointlocation
valueConverter	none by default
appName	a name for your job to display on the cluster web ui
value	int long float string or list
multivariate statistical summary	trait for multivariate statistical summary of a data matrix
mlwriter	utility class that can save ml instances
word	a word
bucketed random projection lsh	note : experimental
lowerBound	the minimum value of column used to decide partition stride
singular value decomposition	represents singular value decomposition svd factors
transform function serializer	this class implements a serializer for pythontransformfunction java objects
java mlreader	private specialization of :py class mlreader for :py class javaparams types
numNearestNeighbors	the maximum number of nearest neighbors
aftsurvival regression model	note : experimental
master	cluster url to connect to
column	the name of an integer column that will be used for partitioning
generalized linear regression model	note : experimental
linear regression training summary	note : experimental
support	the frequency with which to consider an item 'frequent'. default is 1%
cols	list of new column names string
sc	spark context used to save model data
url	a jdbc url of the form jdbc subprotocol subname
computeU	whether or not to compute u. if set to be
tokenizer	a tokenizer that converts the input string to lowercase and then splits it by white spaces
has support	mixin for param support
primitivesAsString	infers all primitive values as a string type. if none is set
seed	seed for the random number generator
sc	sparkcontext used to create the rdd
vector slicer	this class takes a feature vector and outputs a new feature vector with a subarray of the original features
vector	vector to be transformed
condition	a boolean :class column expression
ascending	boolean or list of boolean default true
groupId	the group id for this consumer
path	the path in any hadoop supported file system
pca	pca trains a model to project vectors to a lower dimensional space of the top :py attr k principal components
name	param name
has handle invalid	mixin for param handleinvalid how to handle invalid entries options are skip which will filter out rows with bad values or error which will throw an error more options may be added later
has threshold	mixin for param threshold threshold in binary classification prediction in range [0 1]
streaming logistic regression with sgd	train or predict a logistic regression model on streaming data
logistic regression with lbfgs	versionadded : 1 2 0
source	string name of the data source e.g. 'json', 'parquet'
external merger	external merger will dump the aggregated data into disks when memory usage goes above the limit then merge them together
conf	none by default
kolmogorov smirnov test result	contains test results for the kolmogorov-smirnov test
query execution exception	failed to execute a query
broker	represent the host and port info for a kafka broker
col2	the name of the second column. distinct items will make the column names
port	broker's port
scale	scale (> 0) parameter for the gamma distribution
pipeline	a simple pipeline which acts as an estimator a pipeline consists
word	a word or a vector representation of word
sparse vector	a simple sparse vector class for passing data to mllib users may
validator params	common params for trainvalidationsplit and crossvalidator
bucketizer	maps a column of continuous features to a column of feature buckets
message	actual message payload of this kafka message the return data is
untilOffset	exclusive ending offset
one vs rest model	note : experimental
rowsPerBlock	number of rows that make up each block
kafka rdd	a python wrapper of kafkardd to provide additional information on normal rdd
multivariate gaussian	represents a mu sigma tuple >>> m = multivariategaussian(vectors
external list	externallist can have many items which cannot be hold in memory in the same time
struct field	a field in :class structtype
newUid	new uid to use which is converted to unicode
has step size	mixin for param stepsize step size to be used for each iteration of optimization (>= 0)
hostname	hostname of the slave machine to which the flume data will be sent
isotonic regression model	regression model for isotonic regression
aftsurvival regression	note : experimental
binary type	binary byte array data type
partition	kafka partition id
recursive	turns the nested row as dict default false
has aggregation depth	mixin for param aggregationdepth suggested depth for treeaggregate (>= 2)
cross validator model	crossvalidatormodel contains the model with the highest average cross-validation metric across folds and uses this model to transform input data
pstats param	pstatsparam is used to merge pstats stats
format	'year', 'yyyy', 'yy' or 'month', 'mon', 'mm'
encoding	decodes the csv files by the given encoding type. if none is set
withReplacement	can elements be sampled multiple times replaced when sampled out
data	training data. rdd of list of string
data frame stat functions	functionality for statistic functions with :class dataframe
has reg param	mixin for param regparam regularization parameter (>= 0)
path	path to sequncefile
args	non-zero entries as a dictionary list of tuples
multilayer perceptron classification model	model fitted by multilayerperceptronclassifier
streaming context	main entry point for spark streaming functionality a streamingcontext
regression evaluator	note : experimental
col	column that defines strata
weights	weights for splits will be normalized if they don't sum to 1
schema	a :class pyspark.sql.types.datatype or a datatype string or a list of
spark job info	exposes information about spark jobs
index to string	a :py class transformer that maps a column of indices back to a new column of corresponding string values
allowBackslashEscapingAnyCharacter	allows accepting quoting of all character
has seed	mixin for param seed random seed
key	feature vector representing the item to search for
path	string or list of strings for input path s
probabilities	a list of quantile probabilities
decision tree regressor	decision tree <http //en wikipedia org/wiki/decision_tree_learning>_
dataset	a dataset that contains labels/observations and
distributed matrix	represents a distributively stored matrix backed by one or more rdds
f	python function
result iterable	a special result iterable this is used because the standard
value	int long float string or dict
dense matrix	column-major dense matrix
dir	directory to save the data
has prediction col	mixin for param predictioncol prediction column name
has elastic net param	mixin for param elasticnetparam the elasticnet mixing parameter in range [0 1] for alpha = 0 the penalty is an l2 penalty for alpha = 1 it is an l1 penalty
fpgrowth	note : experimental
saveable	mixin for models and transformers which may be saved as files
java vector transformer	wrapper for the model in jvm
hashing tf	maps a sequence of terms to their term frequencies using the hashing trick
rdd	an rdd[vector] for which column-wise summary statistics
idf	compute the inverse document frequency idf given a collection of documents
window	utility functions for defining window in dataframes
gbtclassification model	model fitted by gbtclassifier
path	string represents path to the json dataset
offsetRanges	list of offsetrange to specify topic partition [start end) to consume
fields	list of fields to extract
distName	string currently only "norm" is supported
streaming linear regression with sgd	train or predict a linear regression model on streaming data
polynomial expansion	perform feature expansion in a polynomial space as said in wikipedia of polynomial expansion
kafkaParams	additional params for kafka
bodyDecoder	a function used to decode body (default is utf8_decoder)
java evaluator	base class for :py class evaluators that wrap java/scala implementations
idfmodel	represents an idf model that can transform term frequency vectors
_jvm	internal parameter used to pass a handle to the
params	an optional param map that overrides embedded params. if a list/tuple of
min hash lshmodel	note : experimental
cartesian deserializer	deserializes the javardd cartesian() of two pythonrdds
numPartitions	the number of partitions of the dataframe
logistic regression with sgd	versionadded : 0 9 0
has num features	mixin for param numfeatures number of features
vector	vector or rdd of vector to be standardized
stsExternalId	external id that can be used to validate against the assumed iam
pair deserializer	deserializes the javardd zip() of two pythonrdds
x	an rdd of vector for which the correlation matrix is to be computed
has features col	mixin for param featurescol features column name
has fit intercept	mixin for param fitintercept whether to fit an intercept term
partitionBy	names of partitioning columns
vector	vector or rdd of vector to be transformed
format	optional string for format of the data source. default to 'parquet'
task context	note : experimental
condition	a :class column of :class types.booleantype
numPartitions	the number of partitions
dataset	an rdd of term frequency vectors
compression	compression codec to use when saving to file. this can be one of the
naive bayes model	model for naive bayes classifiers
prefix span	a parallel prefixspan algorithm to mine frequent sequential patterns
regression metrics	evaluator for regression
isotonic regression	isotonic regression
mean	mean or 1 / lambda for the exponential distribution
stsSessionName	name to uniquely identify sts sessions used to read from kinesis
default	default value
distCol	output column for storing the distance between each pair of rows. use
timeout	number of seconds to wait. default 30 seconds
numRows	number of vectors in the rdd
mean	mean for the log normal distribution
lda	versionadded : 1 5 0
ldamodel	a clustering model derived from the lda method
rating	represents a user product rating tuple
header	writes the names of columns as the first line. if none is set it uses
d	the n decimal places
colsPerBlock	number of columns that make up each block
source	string name of the data source which for now can be 'parquet'
k	number of leading singular values to keep (0 < k <= n)
data	an rdd of labeledpoint to be saved
cols	a string name of the column to drop or a
col1	the name of the first column
clustering summary	note : experimental
shape	shape (> 0) of the gamma distribution
gaussian mixture summary	note : experimental
condition	function that checks for termination conditions
tree ensemble model	private abstraction represents a tree ensemble model
java transformer	base class for :py class transformers that wrap java/scala implementations
string indexer	a label indexer that maps a string column of labels to an ml column of label indices
sccall site sync	helper for setting the spark context call site
random forest classification model	model fitted by randomforestclassifier
fractional type	fractional data types
array type	array data type
mlutils	helper methods to load save and pre-process data used in mllib
linear regression model	model fitted by :class linearregression
kernel density	estimate probability density at required points given an rdd of samples from the population
sqltransformer	implements the transforms which are defined by sql statement
als	alternating least squares matrix factorization
bucketed random projection lshmodel	note : experimental
schema	optional :class pyspark.sql.types.structtype for the input schema
wholeFile	parse one record which may span multiple lines per file. if none is
extra	extra param values
x	an rdd of term frequency vectors or a term frequency
local ldamodel	local non-distributed model fitted by :py class lda
streaming kmeans	provides methods to set k decayfactor timeunit to configure the kmeans algorithm for fitting and predicting on incoming dstreams
java wrapper	wrapper class for a java companion object
kmeans	versionadded : 0 9 0
scale	scale (> 0) of the gamma distribution
step	the incremental step default 1
profiler collector	this class keeps track of different profilers on a per stage basis
inputFormatClass	fully qualified classname of hadoop inputformat
keyClass	fully qualified classname of key writable class
seed	the seed for sampling
distributed ldamodel	distributed model fitted by :py class lda
args	active entries as a dictionary {index value ...},
linear model	a linear model that has a vector of coefficients and an intercept
conf	hadoop configuration passed in as a dict
isotonic regression	currently implemented using parallelized pool adjacent violators algorithm
predicates	a list of expressions suitable for inclusion in where clauses
conf	a l{sparkconf} object setting spark properties
cols	list of column names string . non-numeric columns are ignored
decision tree	learning algorithm for a decision tree model for classification or regression
qrdecomposition	represents qr factors
num	number of synonyms to find
parse exception	failed to parse a sql command
key	key payload of this kafka message can be null if this kafka message has no key
numFeatures	number of features which will be determined
values	list of values that will be translated to columns in the output dataframe
boolean type	boolean data type
col	string new name of the column
col	a :class column expression for the new column
tree ensemble params	mixin for decision tree-based ensemble algorithms parameters
sc	spark context
logistic regression summary	note : experimental
residualsType	the type of residuals which should be returned
decision tree regression model	model fitted by :class decisiontreeregressor
ridge regression with sgd	versionadded : 0 9 0
timestamp type	timestamp datetime datetime data type
idf	inverse document frequency idf
regionName	name of region used by the kinesis client library kcl to update
startPos	start position int or column
gradient boosted trees	learning algorithm for a gradient boosted trees model for classification or regression
fromOffsets	per-topic/partition kafka offsets defining the inclusive starting
keyConverter	none by default
decoder	a function used to decode value (default is utf8_decoder)
comment	sets the single character used for skipping lines beginning with this
gaussian mixture	gaussianmixture clustering
java classification model	private java model produced by a classifier
topic	kafka topic name
illegal argument exception	passed an illegal or inappropriate argument
data	rdd samples from the data
compressionCodecClass	none by default
end	boundary end inclusive
logistic regression model	classification model trained using multinomial/binary logistic regression
escapeQuotes	a flag indicating whether values containing quotes should always
format	the format used to save
chi sq selector model	represents a chi squared selector model
decision tree classifier	decision tree <http //en wikipedia org/wiki/decision_tree_learning>_
params	an optional param map that overrides embedded
positiveInf	sets the string representation of a positive infinity value. if none
default values tests	test :py class javaparams classes to see if their default param values match those in their scala counterparts
group by key	group a sorted iterator as [ k1 it1 k2 it2 ]
matrix	a local dense matrix whose number of rows must match the number of columns
sep	sets the single character as a separator for each field and value
kinesisAppName	kinesis application name used by the kinesis client library kcl to
merger	merge shuffled data together by aggregator
header	uses the first line as names of columns. if none is set it uses the
linear classification model	a private abstract class representing a multiclass classification model
nullable	whether the field to add should be nullable default true
aggregator	aggregator has tree functions to merge values into combiner
decision tree model	abstraction for decision tree models
external group by	group by the items by key if any partition of them can not been
byte type	byte data type i e a signed integer in a single byte
f	python function if used as a standalone function
eager	whether to checkpoint this dataframe immediately
environment	a dictionary of environment variables to set on
sqlcontext	the entry point for working with structured data rows and columns in spark in spark 1 x
integral type	integral data types
has checkpoint interval	mixin for param checkpointinterval set checkpoint interval (>= 1) or disable checkpoint -1 e g 10 means that the cache will get checkpointed every 10 iterations
elementwise product	outputs the hadamard product (i e the element-wise product) of each input vector
datasetB	another dataset to join
normalizer	normalizes samples individually to unit l\ :sup p\ norm for any 1 <= p < float('inf'), normalizes samples using
kafka message and metadata	kafka message and metadata information including topic partition offset and message
data frame reader	interface used to load a :class dataframe from external storage systems (e
word2vec	word2vec creates vector representation of words in a text corpus
javaClassName	fully qualified name of java class
path	string represents path to the json dataset or a list of paths
queryName	unique name for the query
java estimator	base class for :py class estimators that wrap java/scala implementations
elementwise product	scales each column of the vector with the supplied weight vector
gradient boosted trees model	represents a gradient-boosted tree model
value	a literal value or a :class column expression
spark stage info	exposes information about spark stages
binary logistic regression summary	note : experimental
path	path to sequence file
streaming linear algorithm	base class that has to be inherited by any streaminglinearalgorithm
name	name of the udf
options	all other string options
generalized linear regression	note : experimental
long type	long data type i e a signed 64-bit integer
gbtregression model	model fitted by :class gbtregressor
numSlices	the number of partitions of the new rdd
als	alternating least squares als matrix factorization
lshparams	mixin for locality sensitive hashing lsh algorithm parameters
ssc	streamingcontext object
pickle serializer	serializes objects using python's pickle serializer http //docs
batchSize	the number of python objects represented as a single
string indexer model	model fitted by :py class stringindexer
profiler	note : developerapi
java mlwriter	private specialization of :py class mlwriter for :py class javaparams types
outputFormatClass	fully qualified classname of hadoop outputformat
count vectorizer	extracts a vocabulary from document collections and generates a :py attr countvectorizermodel
thresh	int default none
cols	list of column names string or list of :class column expressions
window spec	a window specification that defines the partitioning ordering and frame boundaries
binary logistic regression training summary	note : experimental
has input cols	mixin for param inputcols input column names
min hash lsh	note : experimental
matrix entry	represents an entry of a coordinatematrix
cols	names of columns or expressions
data	source vectors
power iteration clustering	power iteration clustering pic a scalable graph clustering algorithm developed by [[http //www
linear regression with sgd	versionadded : 0 9 0
tree regressor params	private class to track supported impurity measures
params	components that take parameters this also provides an internal
topic and partition	represents a specific topic and partition for kafka
mlreader	utility class that can load ml instances
window	a :class windowspec
accumulator param	helper object that defines how to accumulate values of a given type
blocking	whether to block until unpersisting has completed
has input col	mixin for param inputcol input column name
shape	shape (> 0) parameter for the gamma distribution
has standardization	mixin for param standardization whether to standardize the training features before fitting the model
test result	base class for all test results
vectors	factory methods for working with vectors
sep	sets the single character as a separator for each field and value. if none is
pcamodel	model fitted by :py class pca transforms vectors to a lower dimensional space
update request handler	this handler will keep polling updates from the same socket until the server is shutdown
example point udt	user-defined type udt for examplepoint
dbName	string name of the database to use
model	abstract class for models that are fitted by estimators
offset	offset of this kafka message in the specific partition
one hot encoder	a one-hot encoder that maps a column of category indices to a column of binary vectors with at most a single one-value per row
colName	string name of the new column
has output col	mixin for param outputcol output column name
data frame writer	interface used to write a :class dataframe to external storage systems (e
estimator	abstract class for estimators that fit models to data
indexed row matrix	represents a row-oriented distributed matrix with indexed rows
row	a row in l{dataframe}
schema	a structtype or arraytype of structtype to use when parsing the json column
jsc	the javasparkcontext instance optional
relativeError	the relative target precision to achieve
maxColumns	defines a hard limit of how many columns a record can have. if none is
wholeFile	parse one record which may span multiple lines. if none is
k	number of principal components to keep
multiclass classification evaluator	note : experimental
length	length of the substring int or column
sc	sparkcontext object
extra	extra params to be copied
minSplits	minimum splits in dataset
host	broker's hostname
jtype	java type of element in array
java loader	mixin for classes which can load saved models using its scala implementation
kmeans summary	note : experimental
conf	sparkconf optional
serializer	the serializer for rdds
rformula	note : experimental
keyDecoder	a function used to decode key (default is utf8_decoder)
power iteration clustering model	model produced by [[poweriterationclustering]]
std	std for the log normal distribution
escape	sets the single character used for escaping quotes inside an already
train validation split model	note : experimental
standard scaler model	represents a standardscaler model that can transform vectors
extra	extra parameters to copy to the new instance
chi sq selector model	note : experimental
allowUnquotedFieldNames	allows unquoted json field names. if none is set
field	either the name of the field or a structfield object
std	standard deviation of the log normal distribution
has max iter	mixin for param maxiter max number of iterations (>= 0)
param type conversion tests	test that param type conversion happens
random rdds	generator methods for creating rdds comprised of i i d samples from
mode	allows a mode for dealing with corrupt records during parsing. if none is
conf	hadoop job configuration passed in as a dict
gbtregressor	gradient-boosted trees gbts <http //en wikipedia org/wiki/gradient_boosting>_
options	options to control converting. accepts the same options as the json datasource
path	path to the json object to extract
bisecting kmeans summary	note : experimental
linear regression summary	note : experimental
identifiable	object with a unique id
params	additional params overwriting embedded values
allowComments	ignores java/c++ style comment in json records. if none is set
generalized linear regression training summary	note : experimental
streaming query	a handle to a query that is executing continuously in the background as new data arrives
has items col	mixin for param itemscol items column name
imputer model	note : experimental
observed	it could be a vector containing the observed categorical
topics	dict of (topic_name -> numpartitions) to consume
adding accumulator param	an accumulatorparam that uses the + operators to add values designed for simple types
endpointUrl	url of kinesis service (e.g., https //kinesis.us-east-1.amazonaws.com)
accumulator	a shared variable that can be accumulated i e has a commutative and associative "add"
flattened values serializer	serializes a stream of list of pairs split the list of values which contain more than a certain number of objects to make them
has variance col	mixin for param variancecol column name for the biased sample variance of prediction
pipeline model	represents a compiled pipeline with transformers and fitted models
