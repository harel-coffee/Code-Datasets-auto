module	an array ||| mllib	count=1
function	square root [function_2] ||| [function_1] [function_2]	count=3
function	[function_1] has completed ||| [function_2] [function_1]	count=2
function	standard deviation of this ||| stdev	count=1
arg	type datatype ||| datatype	count=1
function	find the ||| find	count=1
function	positive ||| positive	count=3
function	inputcol=none outputcol=none [function] params for ||| [function]	count=1
function	[function_1] users ||| [function_1] [function_2]	count=3
function	schema ||| schema	count=6
arg	the threshold ||| threshold distcol	count=1
function	points ||| points	count=1
function	left ||| left	count=1
module	round the [module] ||| [module]	count=3
arg	file ||| path schema	count=2
module	the [module] :py attr ||| [module]	count=2
class	tree [class_2] ||| [class_2] [class_1]	count=1
module_class	the [module_1] [class_2] ||| [module_1] [class_2]	count=25
class	stores ||| alsmodel	count=2
class	be used again to ||| query	count=1
function	transform ||| transform	count=2
class	sets ||| polynomial expansion	count=1
arg	outputformat ||| outputformatclass keyclass	count=1
module	is an array of ||| mllib	count=1
function	[function_1] array ||| [function_1] [function_2]	count=5
function	vector columns in ||| vector columns	count=2
module_class	creates a [module_1] [class_2] uid and some ||| [module_1] [class_2]	count=4
function	the maximum ||| max	count=1
class	a [class_2] ||| [class_1] [class_2]	count=2
arg	the heap ||| heap	count=1
function	the null model ||| null	count=2
arg	inputformat with ||| inputformatclass keyclass	count=2
arg	featurescol="features", labelcol="label", predictioncol="prediction", ||| featurescol labelcol predictioncol	count=34
class	subsamplingrate=1 [class] ||| [class]	count=1
arg	input dataset ||| dataset	count=4
function	[function_1] rows ||| [function_2] [function_1]	count=10
function	applying 'full ||| full	count=1
module	list ||| sql	count=2
class	[class] randomly generated ||| [class] vs	count=1
arg	data source ||| source schema	count=2
module_class	[module_1] model ||| [module_1] linear regression [class_2]	count=1
function	register [function_2] ||| [function_2] [function_1]	count=3
class	the output ||| writer	count=2
function	of columns ||| columns	count=1
function	locate the position ||| locate	count=1
class	awaitanytermination() can be ||| streaming query manager	count=1
class	of tree (e g ||| tree model	count=1
class	in tree including ||| decision tree model	count=2
class	[class] app' ||| [class]	count=1
module	gets the [module] cachenodeids or ||| [module]	count=1
function	log [function_2] ||| [function_2] [function_1]	count=2
module	name ||| param	count=1
module	gets the [module] numfeatures ||| [module]	count=1
function_arg	save [arg_2] ||| [arg_2] [function_1]	count=4
module	fields in obj ||| sql	count=1
class	this ||| other	count=1
function	"zerovalue" which ||| fold	count=1
module	gets the [module] maxiter ||| [module]	count=1
arg	this dstream and other ||| other numpartitions	count=1
function	hook [function] into ||| install [function]	count=1
function	this instance is ||| is	count=1
class	a paired rdd ||| matrix factorization	count=1
class	how data ||| data stream	count=1
class	curve which is a ||| binary logistic regression	count=1
module	the [module] maxbins ||| [module]	count=1
module	with a given string ||| ml	count=1
arg	the input param belongs ||| param	count=1
function	java object ||| java	count=3
function	[function] arbitrary ||| new apihadoop [function]	count=3
module	this instance ||| ml param	count=1
module	function [module] the value ||| [module]	count=2
module	the [module] mintokenlength or ||| [module]	count=1
function	c{other} ||| join	count=1
function	parquet ||| parquet	count=2
function	test that ||| test training	count=1
class	regression score ||| linear regression summary	count=1
function	of rows [function_2] ||| [function_2] [function_1]	count=2
class	[class_1] [class_2] ||| [class_1] [class_2] subtract	count=1
class	how data of a ||| data stream	count=1
arg	min=0 0 max=1 0 ||| min max	count=1
function	much of ||| object	count=1
class	again ||| streaming	count=1
function	contains a param with ||| param	count=1
arg	a data source ||| source schema	count=2
function	hivecontext for testing ||| for testing	count=2
arg	observed data [arg_2] ||| [arg_2] [arg_1]	count=2
class	that :func awaitanytermination() can ||| streaming query	count=1
function	creates a [function_2] ||| [function_2] [function_1]	count=1
module	arrays of indices and ||| ml	count=1
module	:func awaitanytermination() can ||| sql	count=1
arg	file ||| path schema sep	count=2
class	this dataframe ||| data frame	count=1
class	with singular ||| singular	count=1
class	comprised of vectors ||| random rdds	count=6
function	window ||| window	count=5
function	[function] archive containing ||| create file in [function]	count=1
function	mean [function_2] ||| [function_1] [function_2]	count=6
function	[function_1] coordinatematrix ||| [function_1] [function_2]	count=2
function	python ||| from	count=1
arg	and other ||| other numpartitions	count=1
class	how much of memory ||| merger	count=1
class	[class_1] aggregationdepth=2): ||| [class_1] [class_2]	count=6
function_arg	[function_1] [arg_2] before ||| [function_1] [arg_2]	count=1
function	and refresh all ||| refresh	count=1
arg	a function [arg_2] ||| [arg_1] [arg_2]	count=2
function	memory for this ||| object size	count=1
function	set number of ||| set	count=1
function_arg	converts [arg_2] ||| [arg_2] [function_1]	count=3
class	checkpointinterval=10 impurity="variance", seed=none ||| decision tree regressor	count=1
module	the [module] finalstoragelevel or ||| [module]	count=1
function	add a [function_2] ||| [function_2] [function_1]	count=4
class	column [class_2] ||| [class_2] [class_1]	count=2
module	of indices ||| ml linalg	count=1
function	returns the number ||| num	count=1
class	this instance ||| params	count=1
function	create [function_2] ||| [function_2] [function_1]	count=7
function	the initial value of ||| initial	count=1
function	[function_1] [function_2] ||| [function_2] params [function_1]	count=1
class	[class_1] :class dataframe ||| [class_1] [class_2]	count=1
module	the [module] estimator or ||| [module]	count=1
module	of ||| ml param	count=4
class	rdd which ||| rdd	count=1
class	specifies the underlying output ||| stream writer	count=1
class	:class dataframe to ||| data frame	count=1
function_arg	[function_1] string str ||| [arg_2] [function_1]	count=1
arg	saves the ||| path format mode partitionby	count=1
class	vector ||| vectors	count=2
class	dataframe that stores ||| alsmodel	count=2
class	be ||| streaming	count=1
module	sizes [module] the ||| [module]	count=1
function	seed=none numhashtables=1) [function] params for ||| [function]	count=1
function	initial value ||| set initial	count=1
function	sql [function_2] ||| [function_1] [function_2]	count=2
module	[module] names ||| [module]	count=1
class	[class] when ||| [class]	count=1
class	already ||| external	count=1
function	java [function_2] ||| [function_2] [function_1]	count=6
module	the [module] cachenodeids or ||| [module]	count=1
arg	[arg] this dstream ||| [arg]	count=2
class	each ||| clustering summary	count=1
class	param [class_2] ||| [class_2] [class_1]	count=1
function	sort [function_2] ||| [function_1] [function_2]	count=4
class	__init__(self ||| min hash lsh	count=1
class	model ||| model	count=17
module	instance ||| ml	count=1
class	__init__(self ||| standard scaler	count=1
function	on a [function_2] ||| [function_1] [function_2]	count=1
module	gets the [module] outputcol ||| [module]	count=1
function	[function_1] of partitions ||| [function_1] [function_2]	count=3
function	a given string ||| has	count=1
class	count of the rdd's ||| rdd	count=1
class	gaussians in mixture ||| mixture model	count=1
function	of users ||| users	count=1
class	this instance with ||| one vs rest model	count=1
class	vector to ||| vector	count=2
class	randomly generated ||| cross validator	count=1
module	[module] minsupport ||| [module]	count=1
class	new ||| query manager	count=2
function	converts vector columns in ||| convert vector columns	count=2
module	the [module] intermediatestoragelevel ||| [module]	count=1
class	returns the ||| metrics	count=1
class	of the accumulator's ||| accumulator param	count=1
function	iterator that contains all ||| to local iterator	count=1
function	or ||| get or	count=1
function	all the objects ||| object size	count=1
arg	strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none) ||| strategy missingvalue inputcols outputcols	count=1
function	contains ||| param	count=1
function	the model params are ||| model params	count=1
module_class	the underlying data ||| sql data frame	count=2
function	with a given ||| has	count=1
function	tree ||| tree	count=2
function	data type json ||| parse datatype json	count=2
arg	the given ||| sc	count=3
function	default param ||| param map	count=1
module_class	standard error [module_1] [class_2] coefficients and intercept ||| [module_1] [class_2]	count=1
function	as the specified ||| save as	count=1
arg	predictioncol="prediction", labelcol="label", [arg_2] ||| [arg_2] [arg_1]	count=1
function	has started ||| started	count=2
class	type ||| type	count=1
function_arg	the group [arg_2] ||| [function_1] [arg_2]	count=1
function	[function] archive ||| create file in [function]	count=1
arg	to the input dataset ||| dataset	count=3
function_arg	setparams(self [arg_2] ||| [arg_2] [function_1]	count=49
module	and profiles the ||| core	count=1
class	of memory ||| external	count=1
class	the objects ||| external	count=1
function	the list based on ||| based on key	count=1
function	[function_1] hivecontext for ||| [function_2] [function_1]	count=2
function	[function_1] params for ||| [function_1] [function_2]	count=5
function	[function_1] rows ||| [function_1] [function_2]	count=10
function	the predicted ||| prediction col	count=2
arg	another ||| other	count=6
class	returns ||| multiclass metrics	count=3
arg	named table ||| table column lowerbound	count=1
class	produced ||| clustering	count=1
function	python direct kafka rdd ||| kafka rdd	count=2
class	of the rdd's elements ||| rdd	count=1
function_arg	[function_1] [arg_2] maxbins=32 mininstancespernode=1 mininfogain=0 ||| [function_1] [arg_2]	count=2
function	original column during ||| original	count=1
function	the [function] day ||| [function]	count=1
function	precision of all ||| precision	count=1
class	[class] sc ||| [class]	count=1
function	jobs started by this ||| job	count=1
function	commutative reduce function but ||| reduce	count=1
class	[class_1] data ||| [class_2] [class_1]	count=8
function	distinct count of ||| count distinct	count=4
module	gets the [module] elasticnetparam or ||| [module]	count=1
class	matrix to the ||| dense matrix	count=1
class	lda keeplastcheckpoint ||| distributed ldamodel	count=1
function_arg	functions registered [arg_2] ||| [function_1] [arg_2]	count=1
function	threshold that ||| threshold	count=1
arg	len with [arg_2] ||| sql rpad col [arg_1] [arg_2]	count=1
function	index of the original ||| partitions with index	count=1
class	context to ||| streaming context	count=1
function	sort the list ||| sort result	count=1
class	maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", ||| regressor	count=1
module_class	for the [module_1] [class_2] ||| [module_1] [class_2]	count=12
module	to this ||| core	count=1
function_arg	[function_1] version ||| [arg_2] [function_1]	count=1
function	code for ||| code	count=1
class	how data of ||| data	count=1
class	input ||| stream	count=2
function	script on [function_2] ||| [function_2] [function_1]	count=1
module	queries ||| sql	count=1
module_class	the [class_2] ||| [module_1] linalg [class_2]	count=2
class	values of the accumulator's ||| accumulator param	count=1
class	[class_1] [class_2] ||| [class_1] [class_2] train cls data iterations	count=1
class	sets ||| num features	count=1
function_arg	[function_1] specified database ||| [arg_2] [function_1]	count=5
arg	a function to ||| f	count=3
function	[function_1] messagehandler ||| [function_1] [function_2]	count=5
function	given string name ||| has param	count=1
class	already partitioned data ||| group by	count=1
class	paired rdd ||| factorization	count=1
function_arg	this grid [arg_2] ||| [arg_2] [function_1]	count=2
module	the [module] threshold or ||| [module]	count=2
function	conduct pearson's chi-squared ||| chi sq	count=1
class	with a function and ||| user defined function	count=1
module	the [module] featureindex ||| [module]	count=1
class	sets ||| index to string	count=1
class	a configuration ||| spark conf	count=2
class	and ||| reader	count=1
class	[class_1] gives the ||| [class_1] [class_2]	count=5
function	[function_1] vector size ||| [function_2] [function_1]	count=1
function	key using ||| by key	count=2
module_class	creates a [module_1] [class_2] and some ||| [module_1] [class_2]	count=4
module_class	of each [class_2] ||| [module_1] [class_2]	count=3
class	queries so that ||| streaming	count=1
function	by the ||| bucket by	count=1
module	this accumulator's value ||| core	count=1
function	stream foreachrdd get offsetranges ||| stream foreach get offset ranges	count=1
module	elements in ||| core	count=1
function	value count ||| count by value	count=1
function	the length of a ||| length	count=1
class	checkpointinterval=10 impurity="gini", numtrees=20 ||| random forest classifier	count=2
module	given value ||| sql	count=1
function	params instances ||| params	count=1
function	stream ||| stream	count=5
function	gets summary e g ||| summary	count=4
class	a list of ||| spark	count=1
class	of the accumulator's data ||| accumulator	count=1
function	get offsetranges ||| get offset ranges	count=6
function_arg	__init__(self [arg_2] ||| [function_1] [arg_2] checkpointinterval	count=1
class	attr lda keeplastcheckpoint ||| ldamodel	count=1
class	sets ||| ensemble params	count=1
function	false positive ||| false positive	count=2
arg	every element ||| numslices	count=1
module	the rdd's elements in ||| core	count=1
arg	maxiter=100 tol=1e-6 ||| maxiter	count=1
function	index of the original ||| with index	count=1
module_class	[module_1] columns on ||| [module_1] [class_2] writer	count=1
function	tables/views ||| tables	count=1
module_class	creates a copy [module_1] [class_2] some ||| [module_1] [class_2]	count=4
function	value of ||| get	count=1
function	compare ||| compare	count=2
class	impurity="gini", [class_2] ||| [class_2] [class_1]	count=16
module	column ||| mllib	count=1
module	the [module] statement ||| [module]	count=1
class	[class_1] from ||| [class_2] [class_1]	count=2
function	on a cluster ||| on cluster	count=3
function	model [function_2] ||| [function_2] [function_1]	count=12
arg	saved using rdd saveastextfile ||| sc path minpartitions	count=1
class	computes column-wise summary statistics ||| statistics	count=1
arg	formula=none featurescol="features", ||| featurescol	count=2
module	times and [module] it ||| [module]	count=1
function	the ||| object size	count=1
function	rows ||| row	count=1
function_arg	[function_1] by ||| [arg_2] [function_1]	count=6
function	rows ||| rows	count=3
function	[function_1] default value ||| [function_1] [function_2]	count=1
module	rdd's elements in one ||| core	count=1
function_arg	worker [arg_2] ||| [arg_2] [function_1]	count=1
function	an exception handler ||| exception handler	count=1
function_arg	[function_1] representing ||| [function_1] [arg_2]	count=1
arg	calculates the correlation of ||| col2 method	count=1
class	from the input java ||| java estimator	count=1
class	counts ||| grouped data	count=2
function	mindf ||| min df	count=1
class	number ||| row matrix	count=2
function	initialized or ||| initialized	count=1
class	clusters in predictions ||| clustering	count=1
arg	setparams(self formula=none featurescol="features", ||| formula featurescol	count=1
module	the [module] max ||| [module]	count=1
function	[function_1] columns ||| [function_1] [function_2]	count=10
function	a new [function_2] ||| [function_2] [function_1]	count=1
class	impurity="variance", ||| decision tree regressor	count=1
module	contains a param ||| param	count=1
class	stream query if this ||| data stream	count=1
class	checkpointinterval=10 impurity="variance", ||| tree regressor	count=1
function	get [function_2] ||| [function_2] [function_1]	count=4
module	so ||| sql	count=1
function	weighted averaged [function_2] ||| [function_2] [function_1]	count=1
function	hivecontext for [function_2] ||| [function_1] [function_2]	count=1
arg	[arg] family="gaussian", ||| labelcol [arg]	count=2
function	the [function] of this ||| [function]	count=1
module	and [module] it ||| [module]	count=1
function	into a java ||| map to java	count=1
function	[function] params ||| [function]	count=2
module_class	creates a [module_1] [class_2] ||| [module_1] [class_2] copy	count=6
function	between ||| between	count=1
class	on [class] ||| [class] submit	count=1
arg	used in sql statements ||| name f returntype	count=1
class	[class_1] and ||| [class_2] [class_1]	count=1
arg	an rdd ||| rdd samplingratio	count=2
function_arg	csv [arg_2] ||| [arg_2] [function_1]	count=3
class	columns on ||| data frame	count=2
function	[function_1] a converter ||| [function_1] [function_2]	count=1
class	curve which is a ||| binary logistic regression summary	count=1
arg	or ||| col	count=2
function_arg	synonyms of [arg_2] ||| [function_1] [arg_2]	count=1
class	pipelinemodel create and return ||| pipeline model	count=1
class	checkpointinterval=10 seed=none impurity="gini", numtrees=20 ||| random forest classifier	count=1
function	[function_1] hivecontext for ||| [function_1] [function_2]	count=2
class	is ||| profiler	count=1
function	[function_1] mathfunction ||| [function_2] [function_1]	count=1
module	and value class from ||| core	count=2
function	list of ||| list	count=3
module_class	[module_1] vector ||| [module_1] [class_2]	count=2
arg	saved using ||| sc path minpartitions	count=1
function	[function_1] specified table ||| [function_1] [function_2]	count=1
arg	given java_class type useful ||| pylist java_class	count=1
function	string ||| has param	count=1
arg	character in matching ||| matching	count=1
class	this ||| accumulator	count=1
function	stage ||| stage	count=2
function	compare 2 ml ||| compare	count=2
function_arg	distance between ||| distance v1 v2	count=1
function	ordered in ascending ||| ordered	count=1
arg	spark ||| sparksession	count=1
function	this ||| object size	count=1
function	centers represented as ||| centers	count=3
function	set number of batches ||| set	count=1
function	script with a dependency ||| dependency	count=3
arg	of key-value pairs ||| pairs	count=1
class	that :func ||| streaming	count=1
arg	numfeatures=1 << 18 binary=false ||| numfeatures binary	count=1
function	transform get ||| transform get	count=2
arg	saved ||| path minpartitions	count=1
module	of this instance with ||| ml	count=6
function	by ||| key by	count=1
function	receiver has ||| receiver	count=2
function	data type [function_2] ||| [function_1] [function_2]	count=2
class	the ||| model	count=1
arg	[arg_1] expected distribution ||| [arg_2] [arg_1]	count=1
module	the [module] metricname ||| [module]	count=2
function	[function_1] labeled points ||| [function_1] [function_2]	count=1
class	this ||| one vs rest model	count=1
arg	f function ||| f	count=2
function	based on first value ||| based on	count=1
function	ordering columns ||| order by	count=1
function	until any of ||| await any termination	count=1
arg	linearregressionmodel ||| sc path	count=2
class	[class_1] gives ||| [class_1] [class_2]	count=5
function	list of [function_2] ||| [function_2] [function_1]	count=11
function	log [function_2] ||| [function_1] [function_2]	count=2
class	into disks ||| external group	count=1
class	broker to map to ||| broker	count=1
arg	splits=none inputcol=none ||| splits inputcol outputcol	count=2
function	predicted ||| prediction	count=2
class	for saving ||| java mlwriter	count=1
module	gets the [module] featuresubsetstrategy or ||| [module]	count=1
class	sets ||| linear regression	count=3
class	logistic regression ||| logistic regression model	count=2
module	disks ||| core	count=1
function	area [function_2] ||| [function_2] [function_1]	count=2
class	[class_1] trees ||| [class_2] [class_1]	count=2
function	hadoop configuration which is ||| hadoop	count=1
module_class	[module_1] model ||| [module_1] gaussian mixture [class_2]	count=2
function	initial value [function_2] ||| [function_2] [function_1]	count=1
class	onevsrest create and return ||| one vs rest	count=1
class	this matrix to the ||| matrix	count=1
class	of the dataframe in ||| data frame writer	count=1
function	local [function] view ||| drop [function]	count=1
function	a java ||| java	count=10
module	[module] gaps ||| [module]	count=1
arg	the column ||| col	count=3
function	users for a ||| users	count=1
class	nodes in tree ||| tree	count=1
module_class	a random forest ||| mllib random forest	count=1
module	representation [module] the ||| [module]	count=1
class	mean squared error which ||| linear regression summary	count=1
function_arg	[function_1] parammap ||| [function_1] [arg_2]	count=6
arg	the specified schema ||| schema options	count=1
class	points using the model ||| model	count=1
function	inside brackets pairs e ||| brackets	count=1
module	[module] blocksize ||| [module]	count=1
class	which gives ||| regression summary	count=2
class	[class] row ||| [class]	count=3
function	[function_1] nodes ||| [function_1] [function_2]	count=1
function	[function_1] or its ||| [function_1] [function_2]	count=1
class	dump already partitioned ||| external group by	count=1
function	[function_1] create ||| [function_1] [function_2]	count=4
module	[module] seed ||| [module]	count=1
arg	is later than the ||| dayofweek	count=1
class	that :func awaitanytermination() can ||| query manager	count=1
function	iterations default 1 ||| iterations	count=1
function	creates ||| create	count=8
function	with a given ||| param	count=1
function	of top [function_2] ||| [function_2] [function_1]	count=1
arg	same param ||| m2 param	count=1
arg	an ||| cls	count=3
arg	saved ||| sc path minpartitions	count=1
function	probability ||| prior	count=1
module_class	[module_1] logistic ||| [module_1] [class_2]	count=12
function	recommends the ||| recommend	count=2
function	of cols ||| cols	count=5
module	: deprecated in ||| sql	count=1
module	null values ||| sql	count=2
class	can ||| manager	count=1
module	[module] outputcol ||| [module]	count=1
module	value ||| ml	count=1
arg	saves the content of ||| format mode partitionby	count=1
function	list of columns ||| list columns	count=1
module_class	[module_1] data source ||| [module_1] [class_2]	count=12
arg	[arg_1] seed=none numhashtables=1) ||| [arg_2] [arg_1]	count=1
arg	[arg_1] according ||| [arg_2] [arg_1]	count=4
arg	true iff [arg] ||| [arg]	count=2
module	the [module] droplast ||| [module]	count=1
class	for this tokenizer ||| tokenizer	count=1
module	[module] intermediatestoragelevel ||| [module]	count=1
function	inner logic [function] ||| [function]	count=1
arg	date1 and date2 ||| date1 date2	count=2
function	broadcast ||| broadcast	count=1
class	return an rdd ||| rdd	count=2
class	sets ||| variance col	count=1
function	memory ||| object	count=1
function	to all the jobs ||| job	count=1
module_class	cluster assignments [module_1] [class_2] trained on the ||| [module_1] [class_2]	count=1
function_arg	months between [arg_2] ||| [function_1] [arg_2]	count=2
class	dump already partitioned ||| group by	count=1
module	[module] link or ||| [module]	count=1
function	of the file ||| file	count=1
module	the [module] fdr ||| [module]	count=1
module	union [module] ||| [module]	count=3
class	given string ||| params	count=1
function_arg	[function_1] [arg_2] ||| [function_1] key [arg_2]	count=1
function	checkpoint ||| checkpoint	count=1
arg	featurescol="features", labelcol="label", [arg_2] ||| [arg_2] [arg_1]	count=25
module	returns all the ||| sql	count=1
function	set [function_2] ||| [function_2] [function_1]	count=2
function	left outer [function_2] ||| [function_1] [function_2]	count=3
arg	if the table ||| tablename	count=1
function	temporary ||| temp	count=2
function	[function_1] rate for ||| [function_1] [function_2]	count=2
function	positive [function_2] ||| [function_2] [function_1]	count=6
module_class	of [class_2] ||| [module_1] [class_2] coefficient standard errors	count=1
class	a paired ||| matrix factorization	count=1
module	this ||| ml	count=2
function	index of ||| map partitions with index	count=1
function_arg	binary mathfunction [arg_2] ||| [function_1] name [arg_2]	count=1
function	contains the count of ||| count	count=1
class	of ||| rdd	count=2
arg	numtopfeature for ||| numtopfeatures	count=1
arg	before [arg] occurrences ||| str delim [arg]	count=1
function	rdd messagehandler ||| rdd message handler	count=2
class	cluster assignments cluster sizes ||| bisecting kmeans	count=1
function	__init__(self ||| init	count=36
arg	starts at pos ||| pos	count=1
class	the stream query ||| data stream	count=1
function	to a string ||| to	count=1
function	iterator that contains all ||| local iterator	count=1
function	[function_1] list based ||| [function_1] [function_2]	count=1
class	which is a dataframe ||| regression	count=1
function_arg	fit test [arg_2] ||| [arg_2] [function_1]	count=4
module	function [module] the level ||| [module]	count=1
function	k-means [function] ||| [function]	count=6
class	model ||| regression	count=1
class	of the accumulator's data ||| accumulator param	count=1
arg	after [arg] >>> ||| [arg]	count=1
function	[function_1] partitions ||| [function_1] [function_2]	count=4
function	computes the singular ||| compute	count=2
function	infer schema from ||| infer schema	count=2
module_class	[module_1] model on ||| [module_1] linear regression [class_2]	count=1
function	of freedom of ||| of freedom	count=1
function	old hadoop ||| as hadoop	count=1
function	converts vector columns in ||| convert vector columns to ml	count=1
arg	for the given user ||| user	count=1
module	contains a ||| param	count=1
arg	the threshold ||| datasetb threshold	count=1
arg	applying c{f} ||| f	count=1
function	norm ||| norm	count=3
module	string name ||| ml	count=1
class	vectors ||| vectors	count=1
function	[function_1] statistics ||| [function_2] [function_1]	count=1
module_class	[module_1] columns ||| [module_1] [class_2]	count=3
function	tcp server ||| server	count=1
arg	value with another value ||| value subset	count=1
function	string ||| has	count=1
class	is ||| context	count=1
function	[function_1] grid to ||| [function_2] [function_1]	count=1
class	randomly ||| cross validator	count=1
arg	or multiclass ||| cls data numclasses	count=1
class	a randomly generated ||| train validation split	count=2
class	awaitanytermination() can be used ||| query manager	count=1
function	[function_1] type for ||| [function_1] [function_2]	count=2
arg	saved using ||| minpartitions	count=1
arg	correlation of two ||| col1 col2 method	count=1
arg	input param belongs ||| param	count=1
function	local representation ||| local	count=1
function	create a [function_2] ||| [function_2] [function_1]	count=7
function	from the ||| load stream	count=1
function	[function_1] of columns ||| [function_2] [function_1]	count=1
function	trigger for the ||| trigger	count=1
function	[function_1] null model ||| [function_2] [function_1]	count=5
arg	external database table ||| table	count=1
function	the selector type ||| set selector type	count=2
arg	of the observed ||| observed	count=1
function	converts matrix columns ||| convert matrix columns from	count=1
class	batches of ||| algorithm	count=1
function	[function] cluster ||| [function]	count=2
class	paramter ||| aftsurvival regression	count=1
arg	default 5 ||| windowsize	count=1
arg	predictioncol="prediction", labelcol="label", ||| predictioncol labelcol	count=2
class	does this configuration ||| spark conf	count=1
class	memory for ||| merger	count=1
function	foreachrdd get [function_2] ||| [function_2] [function_1]	count=1
arg	dataframe representing ||| sqlquery	count=2
function	load labeled ||| load labeled	count=2
module	[module] linkpower or ||| [module]	count=1
arg	[arg] number of ||| [arg]	count=3
module	the [module] maxcategories ||| [module]	count=1
module	gets the [module] statement or ||| [module]	count=1
function	values of each key ||| by key	count=1
arg	stages=none) ||| stages	count=1
function	number of times a ||| count	count=1
function	in utc ||| from utc	count=1
function_arg	[function_1] rep ||| [arg_2] [function_1]	count=3
arg	labelcol="label", [arg_2] ||| [arg_1] [arg_2] maxiter	count=3
class	to wait ||| query manager	count=1
function	loads vectors saved using ||| load vectors	count=1
function	directory ||| dir	count=1
module	gets the [module] layers or ||| [module]	count=1
module	much ||| core	count=1
module	the [module] impurity or ||| [module]	count=1
function	a py or zip ||| py	count=1
class	lda keeplastcheckpoint is ||| distributed ldamodel	count=1
function	[function_1] threshold that ||| [function_2] [function_1]	count=1
class	fields ||| struct type	count=1
class	:class dataframe, using the ||| data frame	count=1
function	[function_1] the null ||| [function_1] [function_2]	count=1
function	of type ||| type	count=1
function	of probabilities ||| probability	count=1
arg	word ||| word	count=1
function	[function_1] null ||| [function_1] [function_2]	count=1
class	onevsrest ||| one vs rest	count=2
class	:class dataframe [class_2] ||| [class_2] [class_1]	count=4
class	attr predictions ||| generalized linear	count=1
function	number of partitions ||| partitions	count=1
class	rowmatrix ||| row matrix	count=1
class	pulls events from flume ||| flume utils	count=1
function	[function_1] distinct count ||| [function_1] [function_2]	count=4
function	local ||| get local	count=1
class	[class_1] regression ||| mllib [class_1] [class_2] with	count=1
module	key and value ||| core	count=2
class	sparkcontext ||| spark context	count=3
module	[module] featureindex ||| [module]	count=1
class	this instance with a ||| one vs	count=1
function	numbuckets ||| num buckets	count=1
class	a [class_2] ||| [class_2] [class_1]	count=2
module_class	of [class_2] ||| [module_1] [class_2] vs rest copy	count=4
function	[function_1] foreachrdd get ||| [function_1] [function_2]	count=3
function	a left [function_2] ||| [function_1] [function_2]	count=4
function	vectors saved using ||| vectors	count=1
arg	[arg_1] numfolds=3 seed=none): ||| [arg_1] [arg_2]	count=2
function	[function_1] [function_2] ||| [function_1] ids for [function_2]	count=2
class	is ||| spark context	count=1
class	wait for new terminations ||| manager	count=1
class	vectors of the singularvaluedecomposition ||| value decomposition	count=1
class	of nodes in tree ||| decision tree	count=1
arg	correlation of two columns ||| method	count=1
function	in the key-value ||| map	count=1
module	of this rdd's ||| core	count=1
arg	end exclusive increased ||| end	count=1
function	[function_1] function ||| [function_2] [function_1]	count=1
function_arg	grid to [arg_2] ||| [function_1] [arg_2]	count=2
function	returns the soundex encoding ||| soundex	count=1
arg	[arg_1] [arg_2] ||| [arg_2] mode partitionby [arg_1]	count=6
arg	obj assume ||| obj	count=1
module	the [module] maxiter ||| [module]	count=1
module	much of ||| core	count=1
class	to use ||| spark context	count=1
function	parses ||| parse	count=1
arg	the given [arg_2] ||| [arg_1] [arg_2]	count=3
arg	source ||| source	count=1
module	a list of numpy ||| ml	count=1
arg	len [arg_2] ||| sql rpad col [arg_1] [arg_2]	count=1
function	given ||| param	count=1
function	[function_1] as a ||| [function_2] [function_1]	count=1
arg	external database table via ||| url table	count=1
class	model trained ||| tree ensemble model	count=2
arg	with the spark sink ||| storagelevel maxbatchsize	count=1
arg	smaller than or equal ||| numiterations	count=1
function	batch has completed ||| completed	count=1
function	sparkcontext which is associated ||| spark	count=1
module	instance contains ||| ml param	count=1
arg	[arg_1] end exclusive ||| [arg_1] [arg_2]	count=1
function	[function_1] a java ||| [function_1] [function_2]	count=14
class	used again ||| streaming	count=1
arg	term to this ||| term	count=1
function	[function_1] each ||| [function_2] [function_1]	count=1
function_arg	[function_1] given path ||| [function_1] cls sc [arg_2]	count=1
class	degrees ||| regression summary	count=1
function	[function_1] trees in ||| [function_2] [function_1]	count=1
function	given string name ||| has	count=1
function	new ||| init	count=2
function_arg	return a new [function_1] [arg_2] ||| [function_1] [arg_2]	count=6
function	vector columns in an ||| vector columns to	count=1
function	half the ||| half	count=1
arg	matching ||| matching replace	count=1
function	of all active ||| active	count=1
function	[function_1] a param ||| [function_1] [function_2]	count=1
arg	with the spark sink ||| maxbatchsize	count=1
class	impurity="variance", seed=none variancecol=none) ||| tree regressor	count=1
class	of model ||| decision tree model	count=1
function	[function_1] replaces ||| [function_1] [function_2]	count=2
function	locate the ||| locate	count=1
function	for [function_1] [function_2] ||| [function_1] [function_2]	count=2
class	return the column standard ||| standard scaler	count=1
module_class	gets the [module_1] [class_2] ||| [module_1] [class_2]	count=8
class	distributed ||| distributed	count=1
module	returns the ||| sql	count=2
function	:class column for distinct ||| distinct	count=1
class	vectors ||| vector	count=2
class	model from the input ||| estimator	count=1
module_class	the bisecting ||| mllib bisecting	count=1
module	gets the [module] standardization or ||| [module]	count=1
class	of this dstream ||| dstream	count=1
class	randomly generated uid and ||| cross validator model	count=1
function	changes the uid ||| reset uid	count=1
function	csv ||| csv	count=2
function	this instance ||| has	count=1
function_arg	between date1 ||| between date1	count=1
function	[function_1] vectors ||| [function_2] [function_1]	count=1
module	rdd's ||| core	count=2
class	return the ||| spark streaming	count=1
function	matrix columns ||| matrix columns to	count=1
class	incrementing as expected ||| task context	count=1
function	set a [function_2] ||| [function_2] [function_1]	count=2
function	[function_1] prefix ||| [function_2] [function_1]	count=1
module	given ||| param	count=1
function	receiver ||| receiver	count=2
class	word in ||| word2vec	count=1
function	checkpoint the dstream operations ||| checkpoint	count=1
function	[function_1] number of ||| [function_2] [function_1]	count=2
arg	function without changing the ||| f	count=1
function	make predictions on the ||| predict on	count=1
module	list of numpy ||| ml	count=1
class	data or table ||| data	count=1
function	[function_1] blocks ||| [function_1] col [function_2]	count=1
function	of memory ||| size	count=1
class	[class_1] a list ||| [class_2] [class_1]	count=6
function	creates a model ||| create	count=1
class	squared error which ||| linear regression summary	count=1
arg	[arg] maxdepth=5 ||| [arg]	count=4
class	of ||| external merger	count=2
arg	first occurrence of substr ||| substr	count=1
module	and name or provide ||| core	count=1
arg	instance to ||| to extra	count=2
function	right outer join of ||| full outer join	count=1
function	based on first ||| based on	count=1
function	given data type json ||| parse datatype json	count=1
function	class inherit documentation from ||| inherit	count=1
function_arg	parquet [arg_2] ||| [arg_2] [function_1]	count=5
function	if the stage info ||| stage info	count=1
class	context ||| context	count=2
module	of two ||| ml linalg	count=2
arg	outputformat api mapred ||| outputformatclass	count=1
function	this instance ||| param	count=1
class	data ||| group	count=1
function	the log [function_2] ||| [function_1] [function_2]	count=1
class	the stream query ||| stream	count=1
class	data ||| data stream	count=1
function	maximum ||| max	count=1
class	ml instance the ||| mlreader	count=1
function	number of rows of ||| num row	count=1
function	param with a given ||| param	count=1
function	computes the min value ||| min	count=1
arg	setparams(self estimator=none estimatorparammaps=none evaluator=none ||| estimator estimatorparammaps evaluator	count=1
function	parse [function_2] ||| [function_1] [function_2]	count=1
arg	estimator=none estimatorparammaps=none evaluator=none numfolds=3 ||| estimator estimatorparammaps evaluator numfolds	count=1
class	adds ||| accumulator	count=1
class	impurity="variance", subsamplingrate=1 [class] featuresubsetstrategy="auto") ||| [class]	count=1
class	already partitioned data into ||| external group by	count=1
class	again to wait ||| streaming query manager	count=1
function	rows of blocks in ||| row blocks	count=1
function	the ||| object	count=1
class	of gaussians in mixture ||| gaussian mixture	count=1
module	feature ||| mllib	count=1
function	name of the file ||| file	count=1
class	in [class] ||| [class]	count=1
class	for this ||| streaming	count=1
function_arg	from the [arg_2] ||| [arg_2] [function_1]	count=1
class	values ||| model	count=1
function	[function_1] squared ||| [function_2] [function_1]	count=7
arg	maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 ||| maxdepth	count=1
function_arg	[function_1] default 100 ||| [arg_2] [function_1]	count=3
function_arg	[function_1] featurescol="features", predictioncol="prediction", ||| [arg_2] [function_1]	count=1
class	to ||| query manager	count=2
module	[module] separate ||| ml [module]	count=3
arg	by step every element ||| step numslices	count=1
class	an rdd created ||| rdd	count=1
arg	function to [arg_2] ||| [arg_2] [arg_1]	count=2
class	can be ||| query	count=1
function	variancecol ||| variance col	count=1
module	gets the [module] mindf ||| [module]	count=1
function	a ||| param	count=2
function	based on ||| based on	count=1
function	a python parammap into ||| to	count=1
class	model with weights already ||| streaming linear regression with	count=1
function	true [function_2] ||| [function_1] [function_2]	count=7
arg	seed=none ||| maxiter seed	count=2
class	convert this vector to ||| sparse vector	count=1
function	parammap into ||| to	count=1
arg	maxcategories=20 inputcol=none outputcol=none) ||| maxcategories inputcol outputcol	count=2
module	returns all ||| sql	count=1
arg	[arg_1] featurescol="features", ||| [arg_1] [arg_2]	count=2
function	columns in ||| columns to	count=2
function	[function_1] each original ||| [function_2] [function_1]	count=2
function	minutes ||| minute	count=1
module	the [module] featuresubsetstrategy or ||| [module]	count=1
module_class	return the [class_2] ||| [module_1] [class_2] kmeans	count=1
class	partitioned ||| external group by	count=2
module_class	[module_1] [class_2] handleinvalid or its default ||| [module_1] [class_2] discretizer get handle invalid	count=1
module	the [module] intermediatestoragelevel or ||| [module]	count=1
arg	in the specified ||| tablename	count=1
module	that :func awaitanytermination() can ||| sql	count=1
function	nodes ||| nodes	count=2
class	udf with a function ||| defined function	count=1
module	return its ||| core	count=1
class	classification [class_2] ||| [class_1] [class_2]	count=1
class	sparkcontext at least ||| spark context	count=1
class	[class_1] svm classifier ||| [class_1] [class_2]	count=1
function	boundaries in increasing order ||| boundaries	count=1
class	can be used ||| streaming query manager	count=1
module	params ||| ml param	count=1
arg	profiler_cls ||| ctx	count=1
class	:py attr predictions ||| linear	count=1
module	path a shortcut of ||| ml	count=4
function_arg	[function_1] input dataset ||| [arg_2] [function_1]	count=1
module	[module] thresholds ||| [module]	count=1
class	into disks ||| external group by	count=1
function	[function] that ||| [function]	count=2
module_class	this [class_2] ||| [module_1] [class_2]	count=35
function	set the initial value ||| set initial	count=1
arg	[arg_1] with pad ||| [arg_2] [arg_1]	count=1
arg	[arg_1] to data ||| [arg_2] [arg_1]	count=6
class	the returned ||| spark streaming test case	count=1
class	from ||| spark context	count=1
class	input ||| estimator	count=1
class	to wait ||| streaming query manager	count=1
class	[class] master as ||| [class]	count=1
class	terminations ||| manager	count=1
arg	product and returns a ||| product	count=1
function_arg	note : [arg_2] ||| [arg_2] [function_1]	count=1
class	a :class [class] ||| [class]	count=1
function	[function_1] checkpointed and ||| [function_1] [function_2]	count=1
function	of ||| object size	count=1
function	[function_1] window ||| [function_2] [function_1]	count=1
module	in json ||| sql	count=1
class	this instance ||| pipeline model	count=1
module	names skipping ||| sql	count=2
function	[function] squared error ||| [function] squared	count=4
class	of this :class dataframe ||| data frame	count=2
class	onevsrestmodel create and return ||| one vs rest model	count=1
function	termination of this ||| termination	count=1
module	[module] inputcol ||| [module]	count=1
function	distance from ||| distance	count=2
function	that all the ||| size	count=1
class	the stream ||| data stream writer	count=1
function	number of possible ||| num	count=1
function	weights computed for ||| weights	count=1
arg	labelcol="label", [arg_2] ||| [arg_1] [arg_2] family	count=2
module_class	in multinomial [class_2] ||| [module_1] [class_2]	count=4
module	a given ||| ml	count=1
function	[function_1] a boolean ||| [function_2] [function_1]	count=4
function	specific ||| offset	count=1
function	with a given string ||| has	count=1
function_arg	fast version of ||| heappushpop heap item	count=1
function_arg	configuration [arg_2] ||| [function_1] key [arg_2]	count=1
class	so that :func awaitanytermination() ||| query	count=1
function	the initial value ||| initial	count=1
function	a param with ||| has param	count=1
class	sort ||| data frame	count=1
module	an exception if any ||| mllib	count=1
function	distributed matrix on ||| matrix	count=1
class	vectors which this transforms ||| vector indexer model	count=1
function_arg	grid [arg_2] ||| [function_1] [arg_2]	count=2
function	driver as ||| local	count=1
function	fields threshold ||| threshold	count=2
module	this instance contains ||| ml	count=1
class	model [class_2] ||| [class_2] [class_1]	count=9
arg	given user and product ||| user product	count=1
function_arg	a receiver [arg_2] ||| [function_1] error [arg_2]	count=1
class	set ||| streaming kmeans	count=1
arg	featurescol="features", [arg_2] ||| [arg_1] [arg_2] maxdepth	count=4
arg	[arg_1] numfolds=3 seed=none) ||| [arg_1] [arg_2]	count=1
arg	or multiclass classification ||| numclasses categoricalfeaturesinfo	count=1
function	join of c{self} ||| join	count=2
arg	right-pad the string column [arg_1] [arg_2] ||| sql rpad [arg_1] [arg_2]	count=1
class	the rdd's ||| rdd	count=1
arg	function ||| f	count=10
function	ml params instances for ||| params	count=1
class	objects ||| serializer	count=2
function	the month of a ||| dayofmonth	count=1
function	with a given string ||| param	count=1
module	gets the [module] thresholds ||| [module]	count=1
arg	and product ||| product	count=1
function	[function_1] squared error ||| [function_2] [function_1]	count=7
module	the [module] usercol ||| [module]	count=1
class	[class_1] session ||| sql [class_1] [class_2] enter	count=1
class	how data of ||| data stream	count=1
function	of nodes summed over ||| nodes	count=1
function	threshold [function_2] ||| [function_2] by [function_1]	count=2
module	gets the [module] fitintercept or ||| [module]	count=1
function_arg	[function_1] document to ||| [arg_2] [function_1]	count=1
arg	each rdds ||| dstream n block	count=1
class	[class_1] returning ||| [class_1] [class_2]	count=1
function	a [function_1] [function_2] ||| [function_1] [function_2]	count=42
function	this instance is ||| is distributed	count=1
function	destroy all data ||| destroy	count=1
module	goodness ||| mllib stat	count=1
class	basic ||| basic	count=9
class	ensemble ||| ensemble	count=2
function	average precision map of ||| average precision	count=1
class	for the stream ||| stream writer	count=1
arg	the srccol ||| srccol	count=1
function	of top features ||| top features	count=2
arg	predictioncol="prediction", maxiter=100 tol=1e-6 seed=none ||| predictioncol maxiter	count=1
arg	column from one base ||| col frombase tobase	count=1
module_class	from this [class_2] ||| [module_1] [class_2] subtract	count=1
function	[function_1] on a ||| [function_1] [function_2]	count=3
function	get all values as ||| get all	count=1
class	score ||| metrics	count=1
module	[module] numhashtables ||| [module]	count=1
function	for approximate distinct ||| approx count distinct	count=1
function	[function] using ||| fold [function]	count=3
function	the index ||| map partitions with index	count=1
arg	python topicandpartition to map ||| topic partition	count=1
function	converts vector columns in ||| convert vector columns from	count=1
function	java parammap ||| java	count=1
function	vector columns in an ||| vector columns from	count=1
class	in mixture ||| gaussian mixture	count=1
class	rdd >>> rdd ||| rdd	count=1
function	[function_1] converter to ||| [function_2] [function_1]	count=1
function	test of the ||| test	count=1
class	:class dataframe using ||| data frame	count=1
module	partitioned data ||| core	count=1
function	only create [function_2] ||| [function_1] [function_2]	count=2
module	given string name ||| param	count=1
arg	to by codeblock co ||| cls co	count=1
function	numhashtables=1) [function] params for ||| [function]	count=1
function	results immediately to the ||| locally	count=1
class	distributed model ||| distributed ldamodel	count=2
function	python parammap into a ||| param map to	count=1
class	batches ||| algorithm	count=1
module	[module] selectortype or ||| [module]	count=1
function	sets vector size ||| set vector size	count=3
class	and ||| rdd	count=1
function	count ||| count	count=6
function	[function_1] block ||| [function_1] [function_2]	count=3
function	:py attr formula ||| formula	count=1
function	a left [function_2] ||| [function_2] [function_1]	count=4
function	[function_1] storage type ||| [function_2] [function_1]	count=2
class	for this model ||| linear model	count=1
module	this ||| mllib linalg	count=1
class	partitioned data into ||| external group	count=1
class	__init__(self ||| vector slicer	count=1
arg	that [arg] ||| [arg]	count=2
function	model params ||| model params	count=2
module	a given string ||| ml	count=1
class	used again to ||| streaming	count=1
function	sample ||| sample	count=1
module	partitioned data into disks ||| core	count=1
class	the dataframe ||| data frame writer	count=1
module	zips [module] rdd with ||| [module]	count=1
class	[class] persists ||| streaming [class]	count=1
arg	from [arg_1] [arg_2] occurrences of the delimiter ||| [arg_1] [arg_2]	count=2
function	converts vector columns in ||| convert vector columns to	count=1
class	__init__(self ||| aftsurvival regression	count=1
module_class	cluster assignments cluster sizes [module_1] [class_2] the ||| [module_1] [class_2]	count=2
function_arg	orc [arg_2] ||| [function_1] [arg_2]	count=1
class	content of the dataframe ||| data frame	count=1
class	with a randomly generated ||| cross validator	count=1
arg	predictioncol="prediction", k=2 [arg_2] ||| [arg_2] [arg_1]	count=2
class	this model ||| kmeans model	count=3
function	dataframe as pandas pandas ||| pandas	count=1
module	the [module] labelcol or ||| [module]	count=1
arg	data sampled ||| data	count=1
function_arg	in parquet [arg_2] ||| [arg_2] [function_1]	count=4
function	columns in an input ||| columns	count=4
function	databases tables functions etc ||| catalog	count=1
function	a single [function_2] ||| [function_2] [function_1]	count=5
function	[function] local ||| [function] temp	count=1
function	create a java ||| java	count=1
class	distributed [class_2] ||| [class_2] [class_1]	count=1
arg	this instance [arg_2] ||| [arg_2] [arg_1]	count=2
function	the ids of all ||| stage ids	count=1
class	sparkcontext ||| spark	count=1
function	sql storage ||| sql	count=1
function	create an input stream ||| create stream	count=1
function	partition [function_2] ||| [function_1] [function_2]	count=2
class	columns ||| block matrix	count=1
function	[function] cluster assignments ||| [function]	count=2
function	which each [function_2] ||| [function_2] [function_1]	count=2
function	"zerovalue" which may be ||| fold	count=1
class	incrementing ||| task context	count=1
module_class	[module_1] broadcast variable ||| [module_1] [class_2]	count=2
module	of column names ||| sql	count=2
module	with a ||| param	count=1
class	returns a paired rdd ||| matrix factorization	count=1
function	the week number ||| weekofyear	count=1
function	[function_1] [function_2] ||| [function_1] [function_2]	count=1795
class	of the :class dataframe ||| data frame	count=6
function	length ||| length	count=1
module	[module_1] two vectors ||| [module_1] [module_2]	count=2
class	params ||| params	count=6
function	for this ||| object size	count=1
module	of this ||| core	count=1
function	[function_1] or ||| [function_2] [function_1]	count=2
module	statistic ||| mllib stat	count=1
class	a randomly generated ||| cross validator model	count=1
module	given data type ||| sql	count=1
class	paired ||| factorization	count=1
class	an input ||| stream reader	count=1
function	selector type of ||| set selector type	count=2
arg	vector or ||| vector	count=1
function	is checkpointed and ||| is checkpointed	count=2
module_class	from this [class_2] ||| [module_1] [class_2]	count=7
function	deviation values ||| std	count=1
class	spark fair scheduler ||| spark context	count=1
arg	or list in each ||| oneatatime	count=1
arg	[arg_1] indices=none names=none) ||| [arg_2] [arg_1]	count=1
class	new [class] as ||| [class]	count=1
function	[function_1] points ||| [function_2] [function_1]	count=2
function	[function_1] users ||| [function_2] [function_1]	count=3
arg	path ||| path mode	count=1
function	[function_1] checkpointed ||| [function_1] [function_2]	count=1
module_class	gets the [module_1] [class_2] its default value ||| [module_1] [class_2] discretizer get handle invalid	count=1
class	this model instance ||| generalized linear regression model	count=1
function_arg	extract a [arg_2] ||| [arg_2] [function_1]	count=1
module	get or compute the ||| mllib linalg	count=2
function	[function_1] temporary table ||| [function_1] [function_2]	count=2
function	leaf nodes ||| nodes	count=1
function	until any of the ||| await any termination	count=1
function	computes the max value ||| max	count=1
function	a unique ||| id	count=1
class	[class_1] sgd ||| [class_1] [class_2]	count=1
function	month of a ||| dayofmonth	count=1
class	spark fair scheduler pool ||| spark	count=1
class	this rdd ||| rdd	count=1
class	sets ||| tree regressor params	count=1
module	gets the [module] pattern ||| [module]	count=1
arg	or any hadoop-supported ||| path	count=2
function	a java storagelevel based ||| get java	count=1
function	the area ||| area	count=2
function	get the root ||| root	count=1
function	rate for ||| rate	count=1
function	until any ||| any termination	count=1
class	objective ||| linear regression training summary	count=1
class	to wait for new ||| manager	count=1
class	data into ||| group by	count=1
class	bucketizer ||| bucketizer	count=1
arg	true if the table ||| tablename	count=1
class	submit and ||| submit	count=6
class	how much of memory ||| external merger	count=1
class	of the query ||| streaming query	count=1
class	rating for the ||| matrix factorization	count=1
module	the [module] gaps ||| [module]	count=1
function	test this should be ||| test	count=1
arg	provided buckets the buckets ||| buckets	count=1
function	of columns [function_2] ||| [function_2] [function_1]	count=2
class	saving ||| mlwriter	count=1
class	terminations ||| streaming query	count=1
arg	new name ||| name	count=1
function	[function_1] stream api ||| [function_1] [function_2]	count=4
function	[function_1] user ||| [function_2] [function_1]	count=2
function	creates [function_2] ||| [function_2] [function_1]	count=7
module	rdd's elements ||| core	count=2
arg	pad ||| pad	count=1
function	to a ||| to	count=11
function	fields threshold precision ||| precision by threshold	count=1
arg	product ||| product	count=2
class	class ||| java	count=1
function_arg	[function_1] the chisqselector ||| [function_1] [arg_2]	count=4
module	gets the [module] degree or ||| [module]	count=1
class	already partitioned data into ||| group by	count=1
class	return the first ||| spark streaming	count=1
function	the standard deviation of ||| stdev	count=1
function_arg	[function_1] url url ||| [arg_2] [function_1]	count=1
class	binomial logistic [class_2] ||| [class_1] [class_2]	count=2
function	set number ||| set	count=1
arg	[arg_1] k=2 ||| params [arg_1] [arg_2]	count=1
function	[function_1] inside brackets ||| [function_1] [function_2]	count=1
function	register [function_2] ||| [function_1] [function_2]	count=3
module	[module] minconfidence or ||| [module]	count=1
arg	given name ||| name	count=1
class	events from flume ||| flume	count=1
module	string ||| ml param	count=2
class	compute the ||| rdd	count=1
arg	for binary or multiclass ||| cls data numclasses	count=1
class	paired rdd where the ||| factorization	count=1
arg	pos in ||| pos	count=1
function	number [function_2] ||| [function_1] [function_2]	count=11
arg	rdd an rdd ||| cls rdd	count=1
function	in [function_2] ||| [function_2] [function_1]	count=1
function_arg	return a new [function_1] [arg_2] dstream ||| streaming dstream [function_1] [arg_2]	count=2
function	used again to ||| reset	count=1
function_arg	[function_1] file ||| [arg_2] [function_1]	count=12
function	index of the ||| map partitions with index	count=1
function	[function_1] rowmatrix ||| [function_2] [function_1]	count=2
class	create a new accumulator ||| accumulator	count=1
function	loads ||| load	count=1
class	this rdd's ||| rdd	count=1
module	and value ||| core	count=2
class	to wait for new ||| streaming query manager	count=1
class	gradient-boosted trees ||| gradient boosted trees	count=4
class	generate ||| with sgdtests	count=1
function	get the offsetrange of ||| ranges	count=1
arg	multiclass [arg_2] ||| [arg_2] [arg_1]	count=1
class	cross validator ||| cross validator	count=2
class	paired ||| matrix factorization	count=1
arg	binary or multiclass ||| numclasses	count=1
function	this instance ||| has param	count=1
function	c{self} and c{other} ||| join	count=1
module	javardd of object ||| ml	count=1
arg	label ||| label	count=1
module_class	[module_1] incrementing ||| [module_1] [class_2]	count=2
function	of tables/views ||| tables	count=1
module	already partitioned ||| core	count=1
function	mathfunction ||| mathfunction	count=1
function	param with a ||| has	count=1
arg	codeblock [arg_2] ||| [arg_1] [arg_2]	count=1
function	null model ||| null	count=2
module	gets the [module] linkpower or ||| [module]	count=1
function	aggregationdepth ||| aggregation depth	count=1
function	nodes summed over all ||| nodes	count=2
arg	data against the expected ||| expected	count=1
function	table in ||| table	count=1
function	the file to which ||| file	count=1
function	sets params for ||| set params	count=8
function	left outer [function_2] ||| [function_2] [function_1]	count=3
function	logic [function] instance ||| [function]	count=1
arg	according [arg_2] ||| [arg_2] [arg_1]	count=2
module	for ||| mllib	count=1
function	embedded params to ||| params to	count=2
module	this ||| param	count=1
function	param ||| has	count=1
class	term frequency vectors or ||| hashing tf	count=1
function_arg	setparams(self [arg_2] ||| [function_1] [arg_2] maxdepth	count=3
arg	of the date column ||| date	count=1
function	to the same time ||| to	count=1
arg	the dstreams ||| dstreams transformfunc	count=1
class	specifies the underlying output ||| frame writer	count=1
function	count the number ||| count	count=1
module_class	[module_1] [class_2] ||| [module_1] [class_2] select	count=10
module_class	of [class_2] ||| [module_1] tree ensemble [class_2]	count=1
arg	[arg] seed=none ||| [arg] seed	count=1
module	computes the sum of ||| ml	count=1
function_arg	[function_1] [arg_2] maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 ||| regressor [function_1] [arg_2] maxdepth	count=1
arg	the date column ||| date	count=1
function	[function_1] vectors ||| [function_1] [function_2]	count=1
function_arg	start [arg_2] ||| [arg_2] [function_1]	count=1
function	data type ||| parse datatype	count=1
function	fpr ||| fpr	count=1
function	of memory for this ||| object	count=1
class	number ||| block matrix	count=1
function	the index of the ||| index	count=1
class	data or table ||| data frame	count=1
function	containing the ids ||| stage ids	count=1
module	[module] cachenodeids ||| [module]	count=1
arg	"zero value" ||| seqfunc combfunc numpartitions	count=1
module	the rdd's elements ||| core	count=1
function	from ||| load stream	count=1
function	loads a parquet ||| parquet	count=1
function	column ||| col	count=1
module	the [module] droplast or ||| [module]	count=1
function	generate ||| generate logistic input	count=2
class	stream query ||| data stream writer	count=1
module	of int containing elements ||| core	count=1
arg	setparams(self [arg_1] [arg_2] ||| [arg_1] [arg_2]	count=4
class	:py attr lda keeplastcheckpoint ||| distributed ldamodel	count=1
function	inside brackets ||| brackets	count=1
arg	probabilitycol="probability", tol=0 01 ||| probabilitycol	count=1
function	the value ||| get	count=1
function	[function_1] transform ||| [function_1] [function_2]	count=3
function	computes the ||| compute	count=2
function	accuracy equals to the ||| accuracy	count=1
module	the underlying rdd ||| mllib	count=1
class	accumulator's data ||| accumulator param	count=1
function	set the initial ||| initial	count=1
class	with a function ||| function	count=1
function	chain ||| chain	count=1
function	with a given ||| has param	count=1
module	second is an ||| mllib	count=1
arg	[arg] maxdepth=5 ||| featurescol [arg]	count=4
function	utc ||| from utc	count=1
arg	document to rdd ||| document	count=1
arg	f-measure ||| beta	count=1
class	logistic regression [class_2] ||| [class_2] [class_1]	count=2
function	[function_1] mathfunction ||| [function_1] [function_2]	count=1
function	[function_1] global ||| [function_2] [function_1]	count=2
function	or create ||| get or create	count=2
class	to use for saving ||| mlwriter	count=1
class	rdd of key-value ||| rdd	count=3
arg	first n elements ||| n	count=1
module	the [module] featurescol ||| [module]	count=1
module_class	[module_1] :py attr ||| [module_1] [class_2] discretizer get handle invalid	count=3
function	instance is ||| is	count=1
function_arg	in orc [arg_2] ||| [arg_2] [function_1]	count=4
class	model ||| ensemble model	count=1
module_class	[module_1] model ||| [module_1] logistic regression [class_2]	count=1
module	the [module] modeltype ||| [module]	count=1
module	the [module] featureindex or ||| [module]	count=1
function	converts ||| to	count=1
arg	than or equal to ||| numiterations	count=1
function	commutative reduce ||| reduce	count=2
module_class	[module_1] gaussianmixturemodel ||| [module_1] [class_2]	count=2
function	[function_1] an int ||| [function_1] [function_2]	count=4
function_arg	sets [arg_2] ||| [arg_2] [function_1]	count=2
arg	[arg] seed=none numhashtables=1) ||| [arg] seed	count=1
class	to ||| manager	count=1
class	queries ||| streaming	count=1
function	compute the mean ||| mean	count=1
function	[function_1] of blocks ||| [function_1] col [function_2]	count=1
arg	input dataset this ||| dataset	count=1
function	in libsvm format ||| libsvm	count=2
module	[module] initmode ||| [module]	count=1
arg	samples drawn ||| numrows numcols numpartitions	count=2
arg	applying a function ||| f	count=3
function	a field in ||| field	count=1
class	rdd as ||| rdd	count=1
arg	the threshold ||| datasetb threshold distcol	count=1
function	get the offsetrange ||| ranges	count=1
function	the selector [function_2] ||| [function_2] [function_1]	count=3
module	expression that [module] true ||| [module]	count=2
function	[function_1] rdd messagehandler ||| [function_1] [function_2]	count=1
function	creates or replaces a ||| create or replace	count=1
arg	heap maintaining the heap ||| heap	count=1
function	maxmemoryinmb ||| max memory in mb	count=1
function	data or [function_2] ||| [function_2] [function_1]	count=1
function	names ||| names	count=1
class	rating ||| matrix factorization	count=1
function	[function_1] rows of ||| [function_2] [function_1]	count=1
arg	[arg_1] [arg_2] maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 ||| [arg_1] [arg_2] maxdepth	count=12
function	libsvm format into ||| parse libsvm	count=2
class	left singular [class_2] ||| [class_2] [class_1]	count=1
function	that all ||| object size	count=1
function	that with new specified ||| to df	count=1
arg	path ||| path	count=25
arg	saves the content ||| mode partitionby	count=2
class	unique id [class] persists across ||| [class]	count=1
function	the index of the ||| map partitions with index	count=1
class	attr predictions ||| linear	count=1
module_class	delete cached copies of [module_1] [class_2] ||| [module_1] [class_2]	count=6
arg	inputcol=none ||| inputcol outputcol	count=4
arg	from a dstream ||| dstream	count=1
class	awaitanytermination() can be ||| streaming	count=1
arg	[arg_1] predictioncol="prediction", ||| [arg_1] [arg_2]	count=19
function_arg	[function_1] dataframe representing ||| [function_1] [arg_2]	count=2
class	terms or words ||| ldamodel	count=1
function	the [function] in ||| [function]	count=1
function	by ||| bucket by	count=1
class	accumulator's ||| accumulator	count=3
arg	inputformat with arbitrary key ||| inputformatclass keyclass	count=2
class	parallelism [class] ||| [class]	count=1
module	given ||| sql	count=9
class	for this model ||| model	count=3
arg	in sql statements ||| returntype	count=1
module	param ||| ml	count=1
module	of each instance ||| ml	count=3
module	[module_1] [module_2] a sparsematrix >>> sm1 ||| [module_2] [module_1] sparse matrix repr	count=1
class	a function ||| user defined function	count=1
arg	file and ||| path schema sep encoding	count=1
function	threshold ||| threshold	count=6
function	content of the non-streaming ||| write	count=1
function	columns in ||| columns to ml	count=2
module	the ||| mllib	count=28
class	so that :func ||| streaming	count=1
class	all ||| merger	count=2
function_arg	model [arg_2] ||| [function_1] [arg_2]	count=1
function	[function_1] json string ||| [function_1] [function_2]	count=3
class	aggregationdepth=2): ||| svc	count=2
arg	tests whether ||| paramname	count=1
arg	n ||| n truncate	count=1
module	new terminations ||| sql	count=1
class	this test ||| other test	count=2
class	awaitanytermination() can be used ||| streaming query	count=1
arg	event time ||| eventtime	count=1
class	function ||| defined function	count=1
class	data source ||| data	count=8
function	selector [function_2] ||| [function_1] [function_2]	count=3
class	category if specified ||| multiclass	count=2
function	labeled [function_2] ||| [function_2] [function_1]	count=1
class	with ||| with	count=1
class	randomly generated ||| train validation split model	count=1
arg	spark sink deployed ||| ssc addresses storagelevel maxbatchsize	count=1
module	two ||| linalg	count=4
arg	input param ||| param	count=1
class	create ||| spark session	count=2
function	signed shift ||| shift	count=1
arg	the input dataset this ||| dataset	count=1
class	query ||| query	count=1
arg	0 99], quantilescol=none aggregationdepth=2): ||| fitintercept	count=1
arg	function to each partition ||| f	count=1
arg	binary or multiclass classification ||| cls data numclasses categoricalfeaturesinfo	count=1
function	matrix columns in an ||| matrix columns from ml	count=1
function_arg	in orc [arg_2] ||| [function_1] [arg_2]	count=1
function	in the training set ||| training	count=1
class	as the spark ||| spark context	count=1
function	for approximate ||| approx	count=1
function	default min [function_2] ||| [function_2] [function_1]	count=2
function	[function_1] handler ||| sql [function_1] [function_2]	count=1
function	parse a [function_2] ||| [function_2] [function_1]	count=1
class	left singular vectors of ||| singular	count=1
function	[function] to ||| show [function]	count=3
function	profile stats ||| profiles	count=3
arg	start to [arg_2] ||| [arg_1] [arg_2]	count=5
class	to a data source ||| data	count=1
arg	start [arg_2] ||| [arg_1] [arg_2]	count=5
class	predictions ||| linear	count=1
function	return the java object ||| java	count=1
module	gets the [module] subsamplingrate or ||| [module]	count=1
class	with weights already set ||| with	count=1
arg	[arg_1] expected distribution ||| [arg_1] [arg_2]	count=1
function_arg	[function_1] value of ||| [function_1] [arg_2]	count=1
class	model by type ||| linear	count=1
function	stages ||| stages	count=2
class	generate ||| streaming logistic regression with sgdtests	count=1
function	all ||| size	count=1
function	stop the execution of ||| stop	count=1
class	this matrix ||| dense matrix	count=1
arg	or multiclass classification ||| data numclasses categoricalfeaturesinfo	count=1
arg	i d samples drawn ||| std numrows	count=1
class	batches of data from ||| streaming linear algorithm	count=1
class	a list of :class ||| data frame	count=1
class	[class] uid and ||| [class]	count=6
class	instance for ||| one vs rest	count=1
class	:func awaitanytermination() can ||| query	count=1
module	gets the [module] standardization ||| [module]	count=1
class	[class_1] evaluator ||| [class_1] [class_2]	count=2
module	an javardd of ||| ml	count=1
module	given ||| ml	count=1
function	of the non-streaming ||| write	count=1
function	a cluster ||| cluster	count=3
arg	input dataset ||| input	count=1
function	[function_1] transform get ||| [function_1] [function_2]	count=3
function	stream messagehandler ||| stream message handler	count=2
function	converts vector columns in ||| convert vector columns from ml	count=1
function	global [function_2] ||| [function_2] [function_1]	count=1
class	the unique id [class_1] [class_2] persists across restarts from ||| [class_1] [class_2]	count=1
function	the root ||| root	count=1
function	get number ||| num	count=2
function	number of columns ||| num	count=1
class	:class dataframe ||| session	count=1
class	should have been ||| loader	count=1
class	values ||| standard	count=1
function_arg	java model [arg_2] ||| [arg_2] [function_1]	count=1
module	pairs [module] ||| [module]	count=3
class	'with [class] ||| [class]	count=1
class	the spark fair ||| spark	count=1
class	pipelinemodel create and ||| pipeline model	count=1
arg	predictioncol="prediction", [arg_2] ||| [arg_2] [arg_1]	count=13
function	has half the ||| half	count=1
function	on toy data ||| on	count=2
function	norm of a sparsevector ||| norm	count=1
arg	the specified table ||| tablename	count=1
arg	given [arg_2] ||| [arg_1] [arg_2]	count=3
arg	spark sink deployed on ||| ssc addresses storagelevel maxbatchsize	count=1
function	stage info ||| stage info	count=1
module	can be ||| sql	count=1
class	of tree ||| tree model	count=1
function	offset ||| from offset	count=1
function_arg	[function_1] date1 ||| [function_1] [arg_2]	count=5
class	paired rdd where ||| matrix factorization model	count=1
arg	via ||| url	count=1
function	[function_1] under the ||| [function_1] [function_2]	count=5
class	terminations ||| streaming query manager	count=2
class	getorcreate() [class] app' ||| [class]	count=1
function	java array ||| new java array	count=2
module	out ||| sql	count=1
module	and return ||| core	count=3
module	[module] degree or ||| [module]	count=1
arg	database table via ||| url table	count=1
function	sets [function_2] ||| [function_2] [function_1]	count=15
function	wait ||| await termination or timeout	count=1
function	layers ||| layers	count=1
arg	[arg_1] pad ||| [arg_2] [arg_1]	count=3
class	for this udt ||| user defined	count=1
function	[function_1] outer ||| [function_1] [function_2]	count=6
class	wait ||| streaming query manager	count=2
arg	[arg_1] featurescol="features", predictioncol="prediction", ||| [arg_1] [arg_2] family	count=2
function	for distinct count of ||| count distinct	count=1
module	[module] censorcol or ||| [module]	count=1
function	ordered in ascending order ||| ordered	count=1
arg	probabilitycol="probability", tol=0 01 maxiter=100 ||| probabilitycol	count=1
class	classification [class_2] ||| [class_2] [class_1]	count=1
function	cluster centers represented as ||| cluster centers	count=3
arg	with another value ||| value subset	count=1
module	this ||| core	count=12
module	the [module] regparam or ||| [module]	count=1
function_arg	[function_1] experimental ||| [arg_2] [function_1]	count=1
arg	the input path ||| path	count=2
module	and profiles the method ||| core	count=1
function	add two values ||| add	count=1
function	labeledpoint ||| labeled point	count=1
function	or its default ||| col	count=1
function_arg	tables/views in [arg_2] ||| [arg_2] [function_1]	count=1
arg	dstream and other ||| other	count=1
arg	neutral [arg] ||| zerovalue [arg]	count=1
class	this [class_2] ||| [class_1] [class_2]	count=2
class	in the ensemble ||| tree ensemble model	count=2
function	new ||| reset	count=1
module	an exception if ||| mllib linalg	count=1
arg	given array [arg] map ||| [arg]	count=2
function	contains a param ||| has param	count=1
arg	function to each ||| f	count=1
function	nulltype ||| has nulltype	count=1
function	featureindex ||| feature index	count=1
function	the sort ||| sort	count=1
function	regexp ||| regexp	count=1
function	[function_1] table ||| [function_2] [function_1]	count=3
arg	setparams(self [arg] ||| [arg] inputcol	count=1
class	convert this distributed ||| distributed	count=1
function_arg	[function_1] a vector ||| [arg_2] [function_1]	count=1
arg	statement=none) ||| statement	count=1
function	from an [function_2] ||| [function_2] [function_1]	count=3
function	[function_1] coordinatematrix ||| [function_2] [function_1]	count=2
function	underlying sql storage ||| sql	count=1
function	aggregate the [function_2] ||| [function_1] [function_2]	count=2
function	is close ||| parameter accuracy	count=1
function	the square root of ||| root	count=1
function	that [function_2] ||| [function_1] [function_2]	count=3
function	[function_1] columns for ||| [function_1] [function_2]	count=2
class	sets the context to ||| context	count=1
module	of fields in obj ||| sql	count=1
class	__init__(self ||| decision tree classifier	count=1
function	load labeled [function_2] ||| [function_2] [function_1]	count=1
class	a sparse [class] using ||| sparse [class]	count=2
arg	dataset ||| dataset	count=3
function	contains a param with ||| has param	count=1
function	set a java system ||| set system	count=1
function	model [function_2] ||| [function_2] on [function_1]	count=1
class	sets ||| elementwise product	count=2
function_arg	grid to fixed ||| grid param	count=2
function	converts matrix columns ||| convert matrix columns to ml	count=1
module	out into ||| sql	count=1
function	[function_1] [function_2] ||| utils [function_1] polling [function_2]	count=2
function	based on first ||| based on key	count=2
module_class	of [class_2] ||| [module_1] [class_2]	count=106
module	zips [module] rdd ||| [module]	count=1
arg	data of another ||| other	count=1
arg	inputformat with arbitrary key ||| inputformatclass	count=2
class	a model with weights ||| streaming linear regression with	count=1
class	much of memory ||| external merger	count=1
function	prediction ||| prediction	count=1
arg	saves the ||| format mode partitionby	count=2
class	that :func awaitanytermination() ||| streaming	count=1
function	a java parammap into ||| java	count=1
module	contains a param with ||| ml	count=1
class	layers ||| multilayer perceptron classification model	count=1
function_arg	[function_1] observed data ||| [function_1] [arg_2]	count=2
function_arg	a new dstream by [function_1] [arg_2] this dstream ||| [function_1] [arg_2]	count=2
module	the [module] outputcol or ||| [module]	count=1
function	[function_1] of nodes ||| [function_2] [function_1]	count=1
function	[function_1] the driver ||| [function_2] [function_1]	count=1
arg	version of ||| heap	count=1
function	string format equals ||| string	count=1
module_class	the [class_2] ||| [module_1] [class_2] kmeans	count=1
function_arg	computes hex [arg_2] ||| [function_1] [arg_2]	count=4
module	of ||| core	count=3
function	destroy all data and ||| destroy	count=1
module_class	[module_1] words ||| [module_1] [class_2]	count=2
function_arg	[function_1] string ||| [arg_2] [function_1]	count=1
class	this ||| one vs rest	count=1
function	approximate distinct ||| approx count distinct	count=1
function	tables/views in ||| tables	count=1
function_arg	[function_1] version of ||| [arg_2] [function_1]	count=1
function	with the keys of ||| keys	count=1
class	:class dataframe to ||| frame	count=1
class	be used again to ||| query manager	count=1
class	sets ||| to string	count=1
function_arg	transforms the [arg_2] ||| [function_1] [arg_2]	count=2
class	regression ||| regression	count=8
arg	returns a function with ||| f	count=1
module	arrays of indices ||| ml linalg	count=1
arg	the given [arg_2] ||| [arg_2] [arg_1]	count=3
function	loads [function_2] ||| [function_1] [function_2]	count=3
function_arg	find synonyms [arg_2] ||| [arg_2] [function_1]	count=1
function	months between ||| months between	count=2
function	of names of tables ||| names	count=1
class	[class_1] on ||| [class_1] [class_2]	count=2
arg	database table ||| table mode properties	count=1
arg	the specified database ||| dbname	count=2
function	[function_1] distinct ||| [function_2] [function_1]	count=4
module_class	[module_1] [class_2] adding a column or ||| [module_1] [class_2] with column colname col	count=1
function	of memory ||| object	count=1
function	if computeu was set ||| u	count=1
class	of the singularvaluedecomposition ||| value decomposition	count=1
function	java model from ||| java	count=1
function	instance contains a ||| has param	count=1
class	parallelism [class] when not ||| [class]	count=1
function_arg	[function_1] labelcol="label", featurescol="features", ||| [arg_2] [function_1]	count=1
class	the right singular ||| singular	count=1
module	with a ||| ml	count=1
function_arg	[function_1] chisqselector ||| [arg_2] [function_1]	count=4
function	min value for ||| min	count=1
class	sets ||| cross validator	count=1
arg	for the given key ||| key	count=2
module	for spill by ||| core	count=1
module	this instance contains a ||| param	count=1
arg	wait a ||| timeout	count=1
function	sorts ||| sort by	count=1
module	containing union [module] this ||| [module]	count=1
module	and combiner ||| core	count=1
function	[function_1] window size ||| [function_2] [function_1]	count=1
class	including ||| model	count=1
class	k-means algorithm ||| kmeans	count=1
class	profiling on ||| profiler	count=1
function	new ||| save as new	count=1
module	[module] labelcol or ||| [module]	count=1
function	add a py ||| add py	count=1
arg	has been ||| receiverstarted	count=1
module	[module] :py ||| [module]	count=2
arg	[arg] containing a ||| [arg]	count=1
class	a :class windowspec ||| window	count=1
module	skipping null values ||| sql	count=2
function	underlying sql [function_2] ||| [function_2] [function_1]	count=2
class	regression model ||| regression	count=3
function	hadoop configuration ||| hadoop	count=1
module	the [module] cachenodeids ||| [module]	count=1
class	of this rdd ||| rdd	count=1
module	queries so ||| sql	count=1
function	returns the schema ||| schema	count=1
class	null ||| linear regression summary	count=1
module	the [module] itemscol ||| [module]	count=1
function_arg	receiver [arg_2] ||| [arg_2] [function_1]	count=2
class	this model ||| bisecting kmeans model	count=1
arg	path the ||| path	count=1
class	binomial logistic [class_2] ||| [class_2] [class_1]	count=2
function	inner logic [function] instance ||| [function]	count=1
function_arg	by other, [arg_2] ||| [arg_2] [function_1]	count=1
arg	[arg] inclusive) ||| [arg]	count=1
function	storage ||| storage	count=1
arg	at ||| compression	count=1
arg	version ||| heap item	count=1
function	that ||| to	count=1
function	blocks ||| blocks	count=2
class	into disks ||| by	count=1
function	the model params ||| model params	count=2
function	filter ||| filter	count=1
arg	existing rdd ||| samplingratio	count=1
function	[function_1] [function_2] of the ||| [function_1] [function_2]	count=4
module	accumulator's ||| core	count=1
class	for new ||| streaming query	count=1
arg	defines an event time ||| eventtime	count=1
class	task ||| task	count=1
function	parammap into ||| map to	count=1
function	false positive [function_2] ||| [function_1] [function_2]	count=1
arg	event time ||| eventtime delaythreshold	count=1
arg	buckets the ||| numbuckets col	count=1
arg	the centroids according to ||| decayfactor timeunit	count=1
function_arg	a new dstream by [function_1] [arg_2] ||| streaming dstream [function_1] [arg_2]	count=8
function	class generated by namedtuple ||| namedtuple	count=1
function	max value ||| max	count=1
class	cross [class_2] ||| [class_2] [class_1]	count=1
module	returns an array of ||| sql	count=1
function_arg	[function_1] [arg_2] 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 ||| regressor [function_1] [arg_2]	count=1
class	of memory for ||| merger	count=1
function	returns an mlwriter instance ||| write	count=1
class	spark configuration ||| spark conf	count=2
arg	from one base to ||| frombase tobase	count=1
function	to make predictions on ||| predict on	count=2
module	return an iterator ||| core	count=2
function	recovers all [function_2] ||| [function_2] [function_1]	count=1
function	sample without replacement based ||| sample	count=1
function	value in the key-value ||| map values	count=1
arg	user and [arg_2] ||| [arg_2] [arg_1]	count=1
function_arg	receiver has [arg_2] ||| [function_1] started [arg_2]	count=1
arg	[arg_1] probabilitycol="probability", rawpredictioncol="rawprediction", ||| [arg_1] [arg_2]	count=3
function_arg	return a new dstream [function_1] [arg_2] dstream ||| [function_1] [arg_2]	count=2
class	with a function ||| defined function	count=1
function	based on ||| based on key	count=2
function	that :func awaitanytermination() ||| reset	count=1
function	the model on ||| on model	count=1
function	load [function_2] ||| [function_1] [function_2]	count=2
class	this matrix to ||| matrix	count=1
function	hook [function_1] [function_2] into py4j which could ||| [function_1] [function_2]	count=1
class	for which predictions are ||| regression model	count=1
function	java parammap into a ||| java	count=1
function	extract a ||| extract	count=1
class	on a flume ||| flume	count=1
function	[function_1] mean squared ||| [function_2] [function_1]	count=1
function	ints ||| int	count=1
function	context to use ||| context	count=4
class	l1-norm ||| metrics	count=1
function	[function_1] brackets pairs ||| [function_1] [function_2]	count=1
function	in [function_2] ||| [function_1] [function_2]	count=1
function	number [function_2] ||| [function_2] [function_1]	count=12
class	id [class] ||| [class]	count=2
function	disks ||| spill	count=1
function	jvm seq ||| seq	count=1
function	[function_1] [function_2] archive containing ||| [function_1] [function_2]	count=1
function	on toy ||| on	count=2
function	true positive rate for ||| true positive rate	count=1
function	true positive [function_2] ||| [function_2] [function_1]	count=5
function	casesensitive=false) sets ||| set	count=1
function	instance to [function_2] ||| [function_2] [function_1]	count=2
function_arg	__init__(self [arg_2] ||| regressor [function_1] [arg_2] maxdepth	count=3
function	each original column during ||| original	count=1
class	awaitanytermination() ||| streaming	count=1
function	[function_1] stream transform ||| [function_2] [function_1]	count=2
class	which ||| logistic regression	count=1
module	gets the [module] :py attr ||| [module]	count=2
function	number of partitions ||| num partitions	count=3
class	terminations ||| streaming	count=1
arg	hadoop [arg] ||| path [arg]	count=3
function	value of spark ||| get	count=1
arg	column [arg_2] ||| [arg_1] [arg_2]	count=2
arg	labelcol="label", predictioncol="prediction", ||| labelcol predictioncol	count=25
function_arg	[function_1] [arg_2] 5 thresholds=none probabilitycol="probability", rawpredictioncol="rawprediction", ||| [function_1] [arg_2]	count=2
class	awaitanytermination() can be ||| query	count=1
function	columns in ||| columns from ml	count=2
class	dump already partitioned ||| external group	count=1
function_arg	test [arg_2] ||| [arg_2] [function_1]	count=7
arg	[arg_1] [arg_2] tol=1e-6 regparam=0 0 weightcol=none ||| [arg_1] [arg_2]	count=4
module	[module] separate arrays ||| ml [module]	count=3
module	variance ||| core	count=1
class	sets ||| step size	count=1
function	of the month ||| dayofmonth	count=1
class	this instance's params ||| java params	count=1
module	[module] initsteps ||| [module]	count=1
class	param [class_2] ||| [class_1] [class_2]	count=1
class	class dataframe ||| data frame	count=1
module_class	[module_1] a ||| [module_1] [class_2]	count=5
function	as non-persistent and remove ||| unpersist	count=1
function	prediction on ||| prediction	count=1
arg	and date2 ||| date2	count=1
function_arg	that is [arg_2] ||| [function_1] numpartitions [arg_2]	count=1
class	into ||| by	count=1
class	sparkcontext is ||| spark context	count=1
class	new ||| java wrapper	count=1
arg	runs ||| k maxiterations mindivisibleclustersize	count=1
class	of ||| merger	count=1
function	unique ||| id	count=1
module_class	[module_1] write() ||| [module_1] [class_2]	count=32
arg	given user [arg_2] ||| [arg_1] [arg_2]	count=1
function	[function_1] blocks ||| [function_2] [function_1]	count=4
function	squared ||| squared	count=6
class	stream returning ||| stream reader	count=2
arg	least the master ||| master	count=1
class	:func awaitanytermination() can be ||| streaming	count=1
function	the key-value ||| map	count=1
module	gets the [module] aggregationdepth or ||| [module]	count=1
module	contains a param with ||| ml param	count=1
class	predictions which gives ||| linear regression summary	count=1
function	dense ||| dense	count=1
module	mean ||| core	count=1
class	a densevector with singular ||| singular	count=1
function	returns the date ||| date	count=1
arg	of document to rdd ||| document	count=1
class	[class] persists ||| [class]	count=2
function	[function_1] to the ||| [function_2] [function_1]	count=3
function	[function_1] error ||| [function_1] [function_2]	count=10
arg	indices=none names=none) ||| indices names	count=1
function	the given data type ||| parse datatype	count=1
arg	k ||| k	count=2
function	corresponding to that user ||| user	count=1
class	spark [class_2] ||| [class_2] [class_1]	count=1
function	of names of ||| table names	count=1
function	k ||| k	count=4
function	parses the expression ||| expr	count=1
function	values of each ||| by	count=1
function	most recent [[streamingqueryprogress]] updates ||| recent progress	count=1
class	:class dataframe in ||| data frame	count=2
function	statement ||| statement	count=1
module	variance and count ||| core	count=1
function	compare [function_2] ||| [function_1] [function_2]	count=2
class	cachenodeids=false checkpointinterval=10 losstype="logistic", maxiter=20 ||| gbtclassifier	count=2
function	broadcast a ||| broadcast	count=1
module	this instance contains ||| ml param	count=1
module	of fields in ||| sql	count=1
class	can be ||| streaming query	count=1
module	other from this ||| mllib linalg	count=1
arg	increased by step ||| step	count=1
class	new accumulator with ||| accumulator	count=1
class	word2vec model's ||| word2vec	count=1
function	cols ||| cols	count=5
arg	boundaries defined from start ||| start	count=1
arg	:class rdd, a list ||| schema samplingratio verifyschema	count=1
arg	by step [arg_2] ||| [arg_2] [arg_1]	count=1
arg	single sequence ||| partitionfunc	count=1
arg	trainratio=0 75 ||| trainratio	count=1
function_arg	positive rate [arg_2] ||| [function_1] [arg_2]	count=1
function	and commutative reduce ||| reduce	count=1
function	[function_1] an indexedrowmatrix ||| [function_2] [function_1]	count=2
class	mean variance and count ||| rdd	count=1
class	a data source ||| data	count=1
class	used ||| query manager	count=2
class	words closest ||| word2vec	count=1
function	of fit test ||| test	count=1
class	udfregistration ||| sqlcontext	count=1
function	of freedom for ||| of freedom	count=1
function_arg	parquet file ||| parquet path	count=1
arg	[arg_1] by step ||| [arg_2] [arg_1]	count=3
function	the cluster [function_2] ||| [function_2] [function_1]	count=6
function	of features corresponding ||| features	count=1
function	the least value of ||| least	count=1
function	rdd get [function_2] ||| [function_1] [function_2]	count=1
arg	scalingvec=none inputcol=none outputcol=none) ||| scalingvec inputcol outputcol	count=2
arg	boundaries from start ||| start	count=1
function	partitioned data into ||| spill	count=1
function	string in libsvm format ||| libsvm	count=1
module	the [module] featuresubsetstrategy ||| [module]	count=1
function	registers ||| register	count=1
class	use l{sparkcontext broadcast()} ||| broadcast	count=1
function	on a [function_2] ||| [function_2] [function_1]	count=1
function	job [function_2] ||| [function_2] [function_1]	count=2
function	local representation this ||| local	count=1
class	can ||| streaming query	count=1
function_arg	[function_1] <http //jsonlines ||| [function_1] [arg_2]	count=4
function	[function_1] of each ||| [function_2] [function_1]	count=1
function_arg	transform [arg_2] ||| [arg_2] [function_1]	count=1
module	gets the [module] labelcol ||| [module]	count=1
function	into ||| to	count=1
class	rdd partitioned using ||| rdd	count=1
function	cost sum of ||| compute cost	count=2
module_class	[module_1] each training ||| [module_1] gaussian mixture [class_2]	count=1
function	from ||| load	count=1
class	the [class] ||| bisecting kmeans [class]	count=3
class	[class_1] tree ||| [class_1] [class_2]	count=2
arg	(from 1 [arg] inclusive) ||| [arg]	count=1
function	hivecontext for [function_2] ||| [function_2] [function_1]	count=1
class	test ||| context tests	count=1
class	into disks ||| external	count=1
class	instance ||| one vs rest	count=1
module_class	cluster assignments [module_1] [class_2] trained on the ||| [module_1] gaussian mixture [class_2]	count=1
module	to wait for new ||| sql	count=1
function	compare 2 ||| compare	count=2
class	dstream ||| kafka dstream	count=2
arg	param belongs ||| param	count=1
module_class	number of terms ||| ml ldamodel vocab	count=1
function	or newline-delimited json ||| json	count=1
class	words closest in similarity ||| word2vec	count=1
arg	or equal to ||| numiterations	count=1
class	compare ||| persistence test	count=1
function	index ||| partitions with index	count=1
class	model ||| generalized linear regression model	count=1
class	this [class] using ||| [class]	count=1
class	the unique [class_1] [class_2] persists across restarts from ||| [class_1] [class_2]	count=1
class	so that :func ||| streaming query	count=1
module	the [module] tolowercase or ||| [module]	count=1
class	sets ||| vector indexer	count=2
module	param with a ||| ml param	count=1
class	can be used ||| frame	count=1
arg	a [arg_1] [arg_2] ||| [arg_1] [arg_2]	count=4
module	a number in ||| sql	count=1
function	precision ||| precision	count=4
class	[class_1] trees ||| [class_1] [class_2]	count=2
function	with a ||| has param	count=1
module	a param with ||| ml param	count=1
function_arg	[function_1] to fixed ||| [function_1] [arg_2]	count=2
function	[function_1] script ||| [function_1] [function_2]	count=2
module	of two vectors we ||| ml linalg	count=1
function	key ||| by key	count=1
function	[function_1] params instances ||| [function_2] [function_1]	count=1
class	generate ||| logistic regression with sgdtests	count=1
function	selector type ||| set selector type	count=2
function	[function_1] distance ||| [function_1] [function_2]	count=2
function	single [function_2] ||| [function_1] [function_2]	count=5
class	paired rdd where ||| factorization model	count=1
arg	heap ||| heap	count=1
function	string ||| string	count=2
function_arg	__init__(self formula=none [arg_2] ||| [arg_2] [function_1]	count=3
class	dependent variable given ||| linear regression model base	count=1
function	applying [function] ||| [function]	count=1
class	comprised of i i ||| random rdds	count=3
function	drops the ||| drop	count=2
function	offsetrange [function_2] ||| [function_2] [function_1]	count=2
class	a [class] using stochastic ||| [class] with	count=1
function	a text ||| text	count=3
class	the input ||| estimator	count=1
arg	dictionary a list ||| size	count=4
function	droplast ||| drop last	count=1
function	parammap into a java ||| param map to java	count=1
function	[function_1] recall ||| [function_1] [function_2]	count=1
function	a param with a ||| has	count=1
arg	sql statements ||| name f returntype	count=1
function	topicdistributioncol ||| topic distribution	count=1
class	count of ||| rdd	count=1
function	test [function] ||| test train on [function]	count=1
arg	to be downloaded ||| recursive	count=1
class	predictions ||| generalized linear	count=1
module	the [module] a ||| [module]	count=1
class	group ||| group	count=1
arg	correlation of ||| col2 method	count=1
function	initialized ||| initialized	count=1
class	matrix attributes which ||| matrix	count=2
function	features corresponding [function_2] ||| [function_2] [function_1]	count=2
class	which is ||| regression	count=4
function	fit test of the ||| test	count=1
function	for ||| size	count=1
class	can be ||| query manager	count=1
arg	and some [arg] ||| [arg]	count=4
function	converts vector columns ||| convert vector columns to ml	count=1
module	gets the [module] max or ||| [module]	count=1
class	weightcol=none [class_2] ||| [class_2] [class_1]	count=2
module	arrays of indices and ||| ml linalg	count=1
arg	rdds of the dstreams ||| dstreams transformfunc	count=1
function	columns in an input ||| columns to	count=2
function_arg	[function_1] [arg_2] mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 ||| [function_1] [arg_2]	count=2
arg	this instance to ||| to extra	count=2
function	much ||| object size	count=1
module_class	creates a copy [module_1] [class_2] randomly generated uid and ||| [module_1] [class_2]	count=1
arg	a dictionary ||| size	count=4
function_arg	into [arg_2] ||| [function_1] [arg_2]	count=1
module_class	files in disks ||| core external	count=1
arg	string in the format ||| s	count=1
arg	input stream ||| stream	count=1
function	seconds [function] unix epoch ||| [function]	count=1
class	[class_1] configuration ||| [class_2] [class_1]	count=6
function_arg	[function_1] name ||| [arg_2] [function_1]	count=1
arg	true iff [arg] is ||| [arg]	count=2
module	[module] standardization or ||| [module]	count=1
arg	function without ||| f	count=1
function	using an associative ||| by	count=1
arg	[arg_1] [arg_2] occurrences of the delimiter ||| [arg_1] [arg_2]	count=2
class	in one ||| rdd	count=1
module_class	[module_1] a default ||| [module_1] [class_2]	count=2
class	are the left singular ||| singular	count=1
module	and c{other} ||| core	count=2
function	for column ||| col	count=1
class	which gives the ||| regression summary	count=2
class	onevsrestmodel create ||| one vs rest model	count=1
arg	[arg_1] with another ||| [arg_2] [arg_1]	count=2
function	start ||| add months	count=1
function	for the test ||| test	count=1
function_arg	[function_1] by ||| [function_1] name [arg_2]	count=8
function	params ||| params	count=7
function	of seconds [function] unix epoch ||| [function]	count=1
arg	input dataset for ||| input	count=1
class	stream query if this ||| stream	count=1
arg	the same param ||| param	count=1
arg	query as fast ||| processingtime once	count=1
class	again to ||| manager	count=1
class	this instance ||| one vs rest	count=1
class	[class] agent ||| [class]	count=3
class	term frequency vectors or ||| tf	count=1
class	this distributed model ||| distributed ldamodel	count=2
function_arg	[function_1] fixed values ||| [arg_2] [function_1]	count=2
arg	given path the ||| path	count=1
arg	file and ||| path	count=1
class	the :class dataframe to ||| data frame	count=1
function_arg	users [arg_2] ||| [function_1] [arg_2]	count=1
function	drops [function_2] ||| [function_1] [function_2]	count=2
function	much of memory ||| object size	count=1
module	a param ||| ml param	count=1
module	or [module] the ||| [module]	count=1
function	a jvm seq ||| seq	count=1
class	of gaussians in mixture ||| mixture	count=1
function	on the incoming ||| on	count=3
class	[class] randomly ||| [class] vs	count=1
class	for this [class_2] ||| [class_2] [class_1]	count=2
module	[module] indices or ||| [module]	count=1
function	trainratio ||| train ratio	count=1
class	for the stream query ||| stream	count=1
function	transform get [function_2] ||| [function_1] [function_2]	count=1
class	be used ||| streaming	count=1
function	jobs has started ||| started	count=1
function	a batch of jobs ||| batch	count=2
function	underlying sql storage type ||| sql type	count=1
function	column mean ||| mean	count=1
arg	input ||| input	count=1
function	initsteps ||| init steps	count=1
module	gets the [module] stepsize or ||| [module]	count=2
class	sets ||| decision tree params	count=3
class	enable 'with sparkcontext as ||| spark context	count=1
function	right outer join ||| full outer join	count=3
module	of nonzero ||| ml linalg	count=1
function	of key ||| key	count=1
function	[function_1] each key ||| [function_2] [function_1]	count=1
class	so ||| streaming query manager	count=2
class	predictions which gives the ||| linear regression summary	count=1
arg	[arg_1] labelcol="label", forceindexlabel=false) ||| [arg_2] [arg_1]	count=5
function	'full ||| full	count=1
module	representation of ||| ml linalg	count=2
function	the cluster ||| cluster	count=3
class	the rdd's elements ||| rdd	count=1
function	[function_1] string ||| [function_1] [function_2]	count=5
function	temp ||| temp	count=1
function	offsetranges ||| offset ranges	count=3
module	returns one of ||| sql	count=1
class	gaussianmixturemodel ||| gaussian mixture model	count=1
arg	datasets ||| dataseta datasetb	count=1
module	[module] a ||| [module]	count=8
class	on ||| streaming logistic regression with	count=1
class	all mixture ||| gaussian mixture model	count=1
function	the given join ||| join	count=1
function	stream [function_2] ||| [function_2] [function_1]	count=7
function	[function_1] or replaces ||| [function_1] [function_2]	count=1
function	from the ||| from	count=1
class	already partitioned data into ||| external group	count=1
function	the termination of this ||| termination	count=1
function_arg	[function_1] document ||| [function_1] [arg_2]	count=1
module	string representation ||| mllib linalg	count=1
function_arg	function [arg_2] ||| [arg_2] [function_1]	count=4
function	the model [function_2] ||| [function_2] on [function_1]	count=1
function	given data type ||| datatype	count=1
class	decision tree ||| decision tree model	count=1
arg	specified table ||| tablename	count=1
function	a labeledpoint [function_2] ||| [function_2] [function_1]	count=5
function	instance ||| has	count=1
function	[function_1] between ||| [function_1] [function_2]	count=4
module_class	[module_1] [class_2] adding a column or ||| [module_1] [class_2]	count=1
function	is [function_2] ||| [function_2] [function_1]	count=4
function_arg	[function_1] label ||| [function_1] [arg_2]	count=3
arg	observed [arg_2] ||| [arg_1] [arg_2]	count=1
arg	database table named table ||| table column lowerbound	count=1
module	return the column ||| mllib	count=1
function	[function_1] top ||| [function_1] [function_2]	count=1
module	param with a ||| ml	count=1
arg	by the optional key ||| key	count=1
class	the spark ||| spark context	count=1
function	key using an associative ||| by key	count=1
arg	[arg_1] [arg_2] 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 ||| [arg_1] [arg_2]	count=2
module_class	[module_1] broadcast on ||| [module_1] [class_2]	count=2
module	instance contains a ||| ml	count=1
class	broker ||| broker	count=1
class	awaitanytermination() can ||| manager	count=1
function	unique id of ||| id	count=1
class	singular vectors [class_2] ||| [class_2] [class_1]	count=1
arg	inputcol=none outputcol=none seed=none numhashtables=1) ||| inputcol outputcol seed numhashtables	count=1
function	cost sum of squared ||| compute cost	count=2
function	returns the [function] in ||| [function]	count=1
function	parammap into a ||| map to	count=1
class	:func awaitanytermination() can be ||| query manager	count=1
function	the file ||| file	count=1
arg	string column [arg_2] ||| [arg_1] [arg_2]	count=2
arg	master ||| master	count=1
function	numpartitions ||| num partitions	count=1
class	can ||| streaming	count=1
function	train the model on ||| train on	count=3
arg	mindocfreq=0 ||| mindocfreq	count=1
function	[function_1] of weights ||| [function_1] [function_2]	count=3
function	hadoop ||| hadoop	count=1
function	prefix ||| prefix	count=1
class	comprised of i ||| random rdds	count=3
function	the [function] in a ||| [function]	count=1
function_arg	[function_1] dstream ||| [arg_2] [function_1]	count=9
arg	another value ||| value subset	count=1
function	[function_1] the file ||| [function_2] [function_1]	count=1
module	new ||| sql	count=2
function	partition id ||| partition id	count=3
class	output a python rdd ||| rdd	count=4
function_arg	the maximum [arg_2] ||| [function_1] [arg_2]	count=1
function	in orc ||| orc	count=1
class	underlying :class [class_2] ||| [class_2] [class_1]	count=2
function	[function_1] rdd ||| [function_2] [function_1]	count=2
function_arg	[function_1] formula=none featurescol="features", ||| [arg_2] [function_1]	count=1
function	[function_1] each ||| [function_1] [function_2]	count=1
module	the [module] thresholds or ||| [module]	count=1
arg	condition ||| condition	count=2
arg	parses a column ||| col	count=1
function	stop() or by ||| await	count=1
arg	according to [arg_2] ||| [arg_2] [arg_1]	count=2
module_class	[module_1] binomial logistic ||| [module_1] [class_2]	count=4
function	create [function_2] ||| sql [function_1] [function_2]	count=2
function	offset ||| offset	count=1
class	return the ||| standard scaler	count=1
function	stream transform get ||| stream transform get	count=3
class	of the dataframe ||| data frame	count=1
function	[function] rdds ||| [function]	count=1
class	l1-norm loss ||| metrics	count=1
arg	input dataset [arg_2] ||| [arg_1] [arg_2]	count=2
class	as a :class ||| data frame	count=1
arg	the specified ||| tablename	count=1
function_arg	a param [arg_2] ||| [function_1] [arg_2]	count=1
arg	an :class rdd, ||| schema samplingratio verifyschema	count=1
function	note : experimental ||| count approx distinct	count=1
class	to wait ||| streaming query	count=1
module	be used ||| sql	count=1
class	which predictions are known ||| regression model	count=1
module	an javardd of object ||| ml	count=1
class	dump already ||| by	count=1
class	can be used to ||| frame	count=1
class	partitioned data into disks ||| group by	count=1
function	leaders ||| leaders	count=1
function_arg	[function_1] property ||| [arg_2] [function_1]	count=1
function	global ||| global	count=2
class	broadcast ||| broadcast	count=1
arg	inputcol=none outputcol=none) ||| inputcol outputcol	count=22
class	this model ||| mixture model	count=1
function	names ||| table names	count=1
function	initialweights ||| initial weights	count=1
arg	[arg] is ||| [arg]	count=2
function	left outer join ||| left outer join	count=3
module	[module] losstype or ||| [module]	count=1
class	for new ||| query manager	count=1
function	transforms ||| transform	count=1
function	return the [function] of ||| [function]	count=1
arg	multi-level ||| depth	count=2
function	right [function_2] ||| [function_2] [function_1]	count=4
class	a :class dataframe ||| data	count=4
arg	d samples drawn ||| numrows numcols	count=4
function	[function_1] on first ||| [function_1] [function_2]	count=4
function	[function_1] labeled points ||| [function_2] [function_1]	count=1
function	update ||| update	count=1
class	which is a ||| logistic regression	count=1
class	queries so ||| query manager	count=1
function	[function_1] columns in ||| [function_2] [function_1]	count=8
function	initmode ||| init mode	count=1
function	checkpoint data or create ||| get or create	count=1
function	binary mathfunction ||| binary mathfunction	count=1
arg	based on the ||| path	count=1
function	brackets pairs e g ||| brackets split	count=1
class	in "predictions" which gives ||| logistic regression summary	count=2
arg	or multiclass [arg_2] ||| [arg_2] [arg_1]	count=1
module_class	creates tuples of the [module_1] [class_2] ||| [module_1] [class_2]	count=2
function	a new ||| init	count=2
arg	param belongs to ||| param	count=1
class	queries ||| manager	count=1
function	intermediatestoragelevel ||| intermediate storage level	count=1
function	statistic functions ||| stat	count=1
module	[module] usercol ||| [module]	count=1
arg	applies [arg] to each ||| [arg]	count=1
module	gets the [module] metricname ||| [module]	count=2
function	[function] this ||| foreach [function]	count=1
function	the initial [function_2] ||| [function_1] [function_2]	count=1
module	be ||| sql	count=1
arg	[arg_1] outputcol=none handleinvalid="error") ||| [arg_1] [arg_2]	count=5
module	convert this ||| linalg	count=2
function	distance from a ||| distance	count=2
class	queries ||| query	count=1
function	the number ||| num	count=4
function	extract the year of ||| year	count=1
function	substring ||| substring index	count=1
function_arg	a parquet [arg_2] ||| [function_1] [arg_2]	count=1
class	[class_1] regression ||| [class_1] [class_2]	count=6
module	of nonzero elements ||| ml	count=1
function	the initial ||| initial	count=1
module_class	return the latest ||| mllib streaming kmeans	count=1
function_arg	[function_1] "num" number ||| [function_1] products for users [arg_2]	count=1
function	[function_1] based ||| [function_2] [function_1]	count=1
arg	key ||| key	count=3
class	for which predictions are ||| isotonic regression	count=1
function	the date ||| date	count=1
arg	statements ||| returntype	count=1
class	with a randomly generated ||| train validation split model	count=1
function	[function] could ||| get [function]	count=3
class	the accumulator's value ||| accumulator	count=1
function	generated unique long ids ||| unique id	count=1
class	:func awaitanytermination() ||| query manager	count=1
module	the [module] finalstoragelevel ||| [module]	count=1
module	terminations ||| sql	count=1
arg	[arg] to ||| [arg]	count=4
function	of rows of blocks ||| row blocks	count=1
function	numtrees ||| num trees	count=1
class	a new rdd ||| rdd	count=1
arg	dataframe representing the ||| sqlquery	count=2
class	transfer this instance's params ||| java params	count=1
class	mean ||| rdd	count=1
class	a [class] by ||| [class]	count=1
function_arg	save a [arg_2] ||| [arg_2] [function_1]	count=1
arg	from start [arg_2] ||| [arg_2] [arg_1]	count=2
class	parameters in ||| builder	count=1
module	computes an ||| mllib	count=1
function	value class from an ||| rdd	count=2
function	instance contains a param ||| has param	count=1
arg	port ||| port	count=1
class	ensemble ||| ensemble model	count=2
class	context ||| streaming context	count=1
function_arg	maximum item ||| max key	count=1
function_arg	fast [arg_2] ||| [function_1] [arg_2]	count=4
module	gets the [module] threshold or ||| [module]	count=2
class	in this model ||| bisecting kmeans model	count=1
class	wait ||| streaming query	count=1
function_arg	[function_1] [arg_2] [[structtype]] or [[arraytype]] of ||| [function_1] json [arg_2]	count=1
function	[function_1] of weights ||| [function_2] [function_1]	count=3
arg	formula=none featurescol="features", labelcol="label", ||| featurescol labelcol	count=2
class	[class_1] test ||| [class_1] [class_2]	count=2
function	the [function] ||| ui web [function]	count=2
module	of this instance with ||| ml param	count=1
arg	samples drawn ||| shape scale numrows	count=1
arg	pos in byte and ||| pos	count=1
module	gets the [module] min ||| [module]	count=1
class	wait for new terminations ||| streaming	count=1
module_class	value of [class_2] ||| [module_1] [class_2]	count=13
class	can be used again ||| streaming	count=1
arg	length len ||| len	count=1
module	and return its path ||| core	count=1
arg	forceindexlabel=false) ||| forceindexlabel	count=2
class	the spark fair ||| spark context	count=1
class	predictions [class_2] ||| [class_2] [class_1]	count=3
module	with a given ||| ml param	count=1
function	set to ||| set	count=1
function_arg	configuration [arg_2] ||| [function_1] [arg_2]	count=1
module	gets the [module] bucketlength or ||| [module]	count=1
function	recommends ||| recommend	count=2
function_arg	[function_1] database ||| [arg_2] [function_1]	count=2
class	squared error [class] ||| [class]	count=6
class	a given string ||| params	count=1
module	or compute the ||| mllib linalg	count=4
class	id [class] persists ||| [class]	count=1
class	number ||| sparse vector	count=1
class	regression model with l2-regularization ||| ridge regression with sgd	count=1
class	used again ||| manager	count=1
module	of parameters specified by ||| ml	count=1
function	of freedom [function_2] ||| [function_1] [function_2]	count=2
function	window size ||| window size	count=2
class	isotonicregressionmodel ||| isotonic regression model	count=1
function	param ||| param	count=2
function	params instances for the ||| params	count=1
function	list based [function_2] ||| [function_1] [function_2]	count=3
function	create an rdd for ||| create	count=2
function	sets window size ||| set window size	count=3
class	awaitanytermination() can ||| query	count=1
class	the column ||| scaler model	count=2
module	gets the [module] estimator ||| [module]	count=1
arg	a dictionary a list ||| size	count=4
arg	a dictionary of values ||| values	count=1
function	that all the ||| object size	count=1
module_class	gets the [module_1] [class_2] its default value ||| [module_1] [class_2]	count=1
function	intercept ||| intercept	count=2
function	[function_1] libsvm format ||| [function_2] [function_1]	count=3
class	queries so ||| query	count=1
function	approximate distinct count ||| approx count distinct	count=2
class	[class] remember ||| streaming [class]	count=1
function	for the termination ||| termination	count=1
module_class	and incrementing ||| core task context	count=1
module	gets the [module] scalingvec or ||| [module]	count=1
class	all mixture ||| mixture	count=1
function	number of rows in ||| count	count=1
arg	java_model to a python ||| java_model	count=1
module	[module] degree ||| [module]	count=1
class	sets ||| input col	count=1
arg	version of a ||| heap item	count=1
function	using an ||| by	count=1
class	stream query if ||| stream	count=1
class	attr lda ||| ldamodel	count=1
function	trigger for ||| trigger	count=1
function	[function_1] partitions ||| [function_2] [function_1]	count=4
function	a parquet ||| parquet	count=1
class	weightcol=none aggregationdepth=2): ||| linear svc	count=4
module	param ||| ml param	count=3
class	profiling on the ||| profiler	count=1
function	temporary table ||| table	count=1
function	returns the explained ||| explained	count=2
function	[function_1] converter ||| [function_1] [function_2]	count=1
arg	c{sparkcontext addfile()} ||| cls filename	count=1
class	comprised of vectors containing ||| random rdds	count=6
class	contains a param with ||| params	count=1
class	the model trained ||| tree ensemble model	count=2
function	can ||| reset	count=1
function_arg	in parquet [arg_2] ||| [function_1] path mode partitionby [arg_2]	count=3
module_class	for this class ||| ml java	count=1
function	receiver operating characteristic roc ||| roc	count=2
arg	[arg_1] predictioncol="prediction", k=2 ||| [arg_2] [arg_1]	count=2
class	this vector ||| vector	count=1
function	min ||| min	count=3
arg	[arg_1] [arg_2] maxiter=100 regparam=0 0 elasticnetparam=0 ||| [arg_1] [arg_2] maxiter	count=9
function	creates a model from ||| create	count=1
class	model ||| linear regression	count=1
function	embedded params [function_2] ||| [function_1] [function_2]	count=2
module_class	shortcut of write() ||| ml pipeline model	count=1
class	tree (e g ||| tree	count=1
arg	table via ||| url table mode properties	count=1
class	a paired rdd ||| factorization	count=1
class	a paired rdd ||| matrix factorization model	count=1
class	logistic [class_2] ||| [class_2] [class_1]	count=7
function	group the values ||| group	count=1
function	a py or ||| py	count=1
function_arg	substring [arg_2] ||| [arg_2] [function_1]	count=3
arg	array [arg] map ||| [arg]	count=2
module	defined ||| param	count=1
function	test [function_1] [function_2] ||| mllib streaming kmeans test [function_2] [function_1]	count=1
function	[function] 'u' ||| [function] unicode	count=1
class	each ||| summary	count=1
class	sample ||| data frame	count=1
class	does this configuration ||| conf	count=1
module	[module] solver ||| [module]	count=2
function_arg	transform the [arg_2] ||| [arg_2] [function_1]	count=2
class	create ||| hive context	count=1
arg	the elements in iterator ||| iterator	count=1
module	and return it ||| core	count=1
function	of top ||| top	count=1
function	[function_1] string ||| [function_2] [function_1]	count=5
arg	provide a new name ||| name	count=1
module_class	a [class_2] ||| [module_1] [class_2]	count=18
function	streamingcontext ||| context	count=1
module	the [module] maxcategories or ||| [module]	count=1
class	return the column ||| standard scaler model	count=1
arg	[arg_1] numfolds=3 ||| [arg_2] [arg_1]	count=2
function	prefix of ||| prefix	count=1
function	as a text ||| as text	count=3
class	to in this model ||| bisecting kmeans model	count=1
module_class	[module_1] vector to ||| [module_1] [class_2]	count=2
module_class	[module_1] a randomly ||| [module_1] [class_2] copy	count=3
arg	the observed ||| observed	count=1
function	a given string name ||| has	count=1
class	depth of tree ||| tree	count=1
module	the [module] losstype or ||| [module]	count=1
function	converts matrix columns ||| convert matrix columns to	count=1
class	again to ||| query	count=1
function_arg	[function_1] between ||| [function_1] [arg_2]	count=2
module_class	cluster assignments cluster sizes [module_1] [class_2] on the ||| [module_1] [class_2]	count=2
class	evaluator ||| evaluator	count=1
function	all the ||| size	count=1
class	of the rdd partitioned ||| rdd	count=1
class	'with [class] sc ||| [class]	count=1
module	separate arrays of indices ||| ml linalg	count=1
class	:class dataframereader ||| session	count=1
class	update ||| streaming kmeans model	count=1
function_arg	[function_1] id ||| [function_1] [arg_2]	count=5
class	return the ||| standard scaler model	count=1
arg	the master ||| master	count=1
class	rformula ||| rformula	count=1
function	the training set ||| training	count=1
arg	the centroids of ||| timeunit	count=1
function	left outer join of ||| left outer join	count=1
function	[function_1] rdd messagehandler ||| [function_2] [function_1]	count=1
class	bisecting k-means algorithm ||| bisecting kmeans	count=2
arg	of values ||| values	count=1
function	[function_1] freedom ||| [function_2] [function_1]	count=1
arg	the same param ||| m1 m2 param	count=1
module_class	[module_1] columns ||| [module_1] [class_2] writer	count=1
function	the explained ||| explained	count=2
class	rdd which is ||| rdd	count=1
module_class	gets [module_1] [class_2] or its default value ||| [module_1] [class_2] discretizer get handle invalid	count=1
function_arg	[function_1] given label ||| [function_1] [arg_2]	count=3
class	test ||| task context tests	count=1
arg	or multiclass classification ||| cls data numclasses categoricalfeaturesinfo	count=1
function	the min value ||| min	count=1
function	for each key ||| by key	count=1
arg	* [arg] + offset)) ||| [arg] npoints	count=1
class	this model ||| model	count=10
class	again to wait ||| streaming	count=1
module	[module] :py attr ||| [module]	count=2
class	with a function and ||| defined function	count=1
module	gets the [module] bucketlength ||| [module]	count=1
function	contains a ||| has param	count=1
arg	featurescol="features", ||| formula featurescol	count=2
function_arg	on the [arg_2] ||| [arg_2] [function_1]	count=3
function	distributed matrix on the ||| matrix	count=1
class	regression model on the ||| regression with	count=1
class	sqltransformer ||| sqltransformer	count=1
function	[function_1] distance ||| [function_2] [function_1]	count=2
function_arg	[function_1] inputcol=none outputcol=none) ||| [function_1] [arg_2]	count=1
function	instance contains a param ||| has	count=1
function	converter to drop ||| converter	count=1
class	[class] handleinvalid or ||| [class]	count=1
class	the model [class_2] ||| [class_2] [class_1]	count=4
function	for approximate distinct count ||| approx count distinct	count=1
arg	used in sql statements ||| returntype	count=1
class	'with sparkcontext as ||| spark context	count=1
arg	by its name ||| paramname	count=1
class	specifies the underlying output ||| data frame writer	count=1
class	input ||| frame reader	count=2
function	function types ||| function	count=1
arg	value ||| value subset	count=1
module_class	creates a copy [module_1] [class_2] ||| [module_1] [class_2] copy	count=10
function	week number ||| weekofyear	count=1
arg	[arg_1] k=2 ||| [arg_1] [arg_2]	count=4
function	the objects ||| size	count=1
class	use for saving ||| mlwriter	count=1
module	compute ||| mllib linalg	count=1
function	loads [function_2] ||| [function_2] [function_1]	count=3
module	gets the [module] vectorsize ||| [module]	count=1
function	old hadoop ||| hadoop	count=1
function	parammap into a ||| to	count=1
module	of indices and ||| ml linalg	count=1
function_arg	points [arg_2] ||| [arg_2] [function_1]	count=4
arg	property ||| key value	count=1
arg	substr ||| substr	count=1
function	boolean ||| boolean	count=1
arg	splits=none inputcol=none [arg_2] ||| [arg_2] [arg_1]	count=1
function	set a local ||| set local	count=3
function	create a converter to ||| create converter	count=1
function_arg	[function_1] [arg_2] ||| [function_1] start [arg_2]	count=1
function	set the selector type ||| selector type	count=1
module	pairs or two ||| linalg	count=2
module	gets the [module] fdr or ||| [module]	count=1
class	new ||| streaming	count=1
function	initial value ||| initial	count=1
module_class	in disks ||| core external	count=1
class	binomial logistic regression ||| logistic regression	count=4
module	instance contains a param ||| param	count=1
class	this instance contains ||| params	count=1
module	given value to ||| sql	count=1
class	sets ||| tree classifier params	count=1
module_class	[module_1] [class_2] new session that has ||| [module_1] [class_2]	count=1
arg	the spark sink deployed ||| ssc addresses storagelevel maxbatchsize	count=1
function	[function] instance ||| [function]	count=1
class	for new ||| streaming	count=1
function	into label indices ||| parse	count=1
function	wait ||| or timeout	count=1
class	for new terminations ||| query manager	count=1
function	[function_1] a rowmatrix ||| [function_2] [function_1]	count=2
class	the model trained ||| logistic regression model	count=2
arg	probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1 0 ||| probabilitycol	count=1
arg	rdd of document ||| document	count=1
class	this configuration ||| spark conf	count=1
function	a given ||| has	count=1
class	clusters ||| kmeans model	count=1
function	[function] instance based ||| [function]	count=1
arg	[arg_1] labelcol="label", predictioncol="prediction", ||| [arg_2] [arg_1]	count=16
function_arg	[function_1] default 5 ||| [function_1] [arg_2]	count=3
function	sets window [function_2] ||| [function_2] [function_1]	count=1
module	gets the [module] inputcol or ||| [module]	count=1
module	of each ||| ml	count=5
class	with a given string ||| params	count=1
class	estimated ||| linear regression summary	count=2
function	[function_1] field in ||| [function_1] [function_2]	count=1
function_arg	[function_1] [arg_2] solver="irls", linkpredictioncol=none variancepower=0 0 ||| [function_1] [arg_2]	count=2
function	rows of [function_2] ||| [function_1] [function_2]	count=2
module_class	[module_1] broker ||| [module_1] [class_2]	count=2
module	gets the [module] numfeatures or ||| [module]	count=1
class	dump ||| group by	count=2
arg	[arg] maxiter=100 regparam=0 ||| [arg]	count=3
function	convert a list of ||| to	count=1
class	model on ||| model	count=2
function	selector ||| set selector	count=1
function	[function_1] positive ||| [function_1] [function_2]	count=3
function	perform a left outer ||| left outer	count=1
arg	:class rdd, a ||| schema samplingratio verifyschema	count=1
function	each [function] ||| foreach [function]	count=1
function	[function_1] grid to ||| [function_1] [function_2]	count=1
function	can be used ||| reset	count=1
class	convert this matrix to ||| matrix	count=1
class	this bucketizer ||| bucketizer	count=1
function	set the selector ||| set selector	count=1
module	a list of ||| ml	count=1
function_arg	[function_1] centroids ||| [arg_2] [function_1]	count=1
function	matrix columns in ||| matrix columns to ml	count=1
class	sets ||| regressor params	count=1
function_arg	[function_1] data sampled ||| [function_1] [arg_2]	count=1
function_arg	synonyms [arg_2] ||| [function_1] [arg_2]	count=1
function	the month of ||| dayofmonth	count=1
module	mean variance and count ||| core	count=1
class	the returned ||| streaming test case	count=1
class	data or table already ||| data frame	count=1
class	values for each numeric ||| grouped	count=1
module_class	creates [module_1] [class_2] and some ||| [module_1] [class_2]	count=4
module	the [module] minconfidence ||| [module]	count=1
arg	path the model ||| path	count=1
function	soundex encoding for a ||| soundex	count=1
function	[function_1] of top ||| [function_2] [function_1]	count=1
function	type ||| type	count=3
function	the distinct ||| distinct	count=1
function	adds ||| add	count=1
arg	the spark sink ||| ssc addresses storagelevel maxbatchsize	count=1
module	gets the [module] subsamplingrate ||| [module]	count=1
function_arg	between [arg_2] ||| [function_1] [arg_2]	count=2
function	the global ||| global	count=1
function	or replaces ||| or replace	count=2
class	getorcreate() [class] app' syntax ||| [class]	count=1
function	perform a right ||| full	count=1
function	of ||| size	count=1
function_arg	[function_1] [arg_2] the ||| [function_1] [arg_2]	count=2
module_class	creates a [module_1] [class_2] some ||| [module_1] [class_2]	count=4
class	the ||| external	count=1
class	to wait for new ||| streaming query	count=1
function	a java [function_2] ||| [function_2] [function_1]	count=5
module	the [module] ||| [module]	count=197
function	of blocks [function_2] ||| [function_2] [function_1]	count=2
arg	the query as fast ||| processingtime once	count=1
class	function and ||| defined function	count=1
class	all mixture ||| mixture model	count=1
function	recovers all the partitions ||| recover partitions	count=1
module	[module] numbuckets ||| [module]	count=1
function	a python parammap into ||| map to	count=1
function_arg	[function_1] content and ||| [function_1] zip name [arg_2] ext dir	count=1
function	as the specified table ||| save as table	count=1
function	creates [function_2] ||| [function_1] [function_2]	count=5
function	the [function] ||| [function]	count=8
module	contains ||| param	count=1
function	the mean squared ||| mean squared	count=3
class	wait for new terminations ||| query manager	count=1
function	on the ||| on	count=4
function	[function_1] active stages ||| [function_2] [function_1]	count=4
function	sparkcontext ||| spark	count=1
arg	adds a term ||| term	count=2
arg	[arg_1] the expected ||| [arg_1] [arg_2]	count=2
function	array ||| array	count=3
module	norm [module] the given ||| [module]	count=1
function	of rows ||| row	count=1
arg	[arg] family="gaussian", link=none ||| labelcol [arg]	count=2
function	implicitprefs ||| implicit prefs	count=1
function	[function_1] all active ||| [function_2] [function_1]	count=6
class	new terminations ||| manager	count=1
module	[module] min ||| [module]	count=1
class	dump already partitioned data ||| group	count=1
function	a coordinatematrix ||| coordinate	count=2
function	by applying [function] rdds ||| [function]	count=1
module	return a javardd of ||| core	count=1
module_class	[module_1] [class_2] ||| [module_1] linalg [class_2]	count=2
function	a java system ||| system	count=1
arg	inputformat with arbitrary ||| inputformatclass keyclass valueclass	count=2
class	the unique id [class_1] [class_2] persists across restarts from ||| [class_1] [class_2] id	count=1
class	the accumulator's data type ||| accumulator param	count=1
function	window of time over ||| window	count=1
function	make predictions on ||| predict on	count=3
function	[function_1] a text ||| [function_2] [function_1]	count=1
class	disks ||| by	count=1
module_class	and group ||| core external group	count=1
arg	of another dstream with ||| other	count=1
arg	specific group matched by ||| str pattern idx	count=1
module	the underlying ||| mllib	count=1
function	partial objects do not ||| partial	count=1
function	[function_1] [function_2] archive containing ||| [function_1] file in [function_2] name	count=1
function	a new hivecontext for ||| for	count=1
class	returning the result as ||| reader	count=1
module_class	elements from ||| core spark context	count=1
module	given ||| ml param	count=2
class	the ||| merger	count=1
class	which is a ||| regression metrics	count=1
arg	formula=none featurescol="features", labelcol="label", forceindexlabel=false) ||| featurescol labelcol forceindexlabel	count=2
function	last ||| last	count=1
function	list of tables/views in ||| list tables	count=1
module_class	[module_1] this ||| [module_1] [class_2] copy	count=1
class	rdd's ||| rdd	count=2
class	lda ||| ldamodel	count=1
function	the global [function_2] ||| [function_1] [function_2]	count=1
module_class	creates [module_1] [class_2] uid and some ||| [module_1] [class_2] copy	count=4
arg	the chisqselector ||| selectortype	count=1
function	sets params ||| set params	count=8
function	[function_1] of blocks ||| [function_1] [function_2]	count=3
function	parammap into a java ||| map to java	count=1
function	each original column ||| original	count=1
module	singular ||| mllib linalg	count=3
function	for dataframe from ||| from	count=1
class	regression [class_2] ||| [class_1] [class_2]	count=4
module_class	creates tuples [module_1] [class_2] ||| [module_1] [class_2]	count=2
class	[class_1] context ||| [class_2] [class_1]	count=4
function_arg	selector type [arg_2] ||| [arg_2] [function_1]	count=1
arg	adds a term to ||| term	count=2
function	generates [function] ||| normal [function]	count=1
function	[function_1] or ||| [function_1] [function_2]	count=2
module	the [module] maxmemoryinmb or ||| [module]	count=1
function	the features ||| features	count=1
function	fit test ||| test	count=1
function	a java object ||| java	count=1
function	[function_1] size ||| [function_1] [function_2]	count=4
function_arg	type [arg_2] ||| [arg_2] [function_1]	count=1
arg	data sampled from a ||| data	count=1
function	how much of ||| size	count=1
class	tests a ||| tests	count=1
module_class	in this [class_2] ||| [module_1] [class_2]	count=4
function	on the log ||| log	count=1
class	adds a ||| accumulator	count=1
function_arg	function by ||| function name doc	count=1
module	the [module] percentile ||| [module]	count=1
class	much ||| merger	count=1
module	or compute ||| linalg	count=2
function_arg	__init__(self inputcol=none outputcol=none) ||| init inputcol outputcol	count=1
function_arg	the [function_1] [arg_2] before ||| [function_1] [arg_2]	count=2
function	[function_1] vector columns ||| [function_2] [function_1]	count=2
module	the [module] indices or ||| [module]	count=1
function	creates a :class dataframe ||| create	count=1
function	dstream by applying 'full ||| full	count=1
arg	the spark sink ||| maxbatchsize	count=1
function	ndcg value ||| ndcg	count=1
class	dump ||| by	count=1
arg	by applying a function ||| f	count=3
class	validates the block ||| block	count=1
function	columns in ||| columns from	count=2
module	rdd returns ||| sql	count=1
function_arg	[function_1] reduced into ||| [arg_2] [function_1]	count=1
arg	joins ||| on how	count=1
function	square root of ||| root	count=1
module	in json format (json ||| sql	count=1
class	number ||| indexed row matrix	count=1
arg	create a python topicandpartition ||| topic partition	count=1
class	this instance with ||| one vs	count=1
module	arrays [module] ||| [module]	count=2
module	return an ||| core	count=2
function	the threshold ||| threshold	count=2
arg	according to data ||| data decayfactor	count=3
function	estimator ||| estimator	count=1
function	trigger ||| trigger	count=1
module	gets the [module] maxcategories ||| [module]	count=1
class	tree ||| decision tree	count=1
function	set [function_2] ||| [function_1] [function_2]	count=2
module	return a copy of ||| core	count=1
module	elements in one ||| core	count=1
module_class	the latest ||| mllib streaming kmeans	count=1
function	of features ||| features	count=1
function	test that the ||| test	count=2
class	in the ensemble ||| tree ensemble	count=2
arg	withmean=false withstd=true [arg_2] ||| [arg_2] [arg_1]	count=2
class	[class_1] on the ||| [class_1] [class_2]	count=2
arg	representing the ||| sqlquery	count=1
function	file ||| file	count=3
function	that all ||| size	count=1
function	"zerovalue" which may ||| fold by	count=1
arg	creates ||| sparkcontext	count=1
function	into a java ||| to java	count=1
arg	dstream and other ||| other numpartitions	count=1
function	fwe ||| set fwe	count=1
class	attr predictions [class_2] ||| [class_1] [class_2]	count=3
class	:func ||| streaming	count=2
module_class	[module_1] gradient-boosted trees ||| [module_1] [class_2]	count=4
class	stages ||| status tracker	count=1
module	all pairs [module] ||| [module]	count=3
module	much of memory for ||| core	count=1
function	of a batch of ||| batch	count=2
arg	[arg_1] predictioncol="prediction", k=2 ||| [arg_1] [arg_2]	count=1
module	densematrix ||| linalg	count=1
function	length of a ||| length	count=1
function	dataframe of probabilities ||| probability	count=1
module	gets the [module] usercol or ||| [module]	count=1
module_class	return a default ||| core spark	count=1
class	as spark executor ||| spark	count=1
class	output ||| frame writer	count=1
arg	given user ||| user	count=1
function	true ||| true	count=2
arg	flatmap [arg] ||| [arg]	count=3
arg	inputcol=none outputcol=none ||| inputcol outputcol relativeerror	count=1
arg	column ||| col options	count=1
function	false ||| false	count=1
function	based [function_2] ||| [function_1] [function_2]	count=3
function	[function_1] matrix ||| [function_2] [function_1]	count=2
class	[class_1] classification ||| [class_2] [class_1]	count=1
class	[class_1] with weights ||| [class_1] [class_2]	count=3
class	stream ||| stream writer	count=1
class	the dataframe ||| data frame	count=1
function	checkpoint the ||| checkpoint	count=1
class	to use ||| context	count=1
class	used ||| query	count=1
module	[module] it ||| [module]	count=3
function	value in the key-value ||| map	count=1
arg	at pos in ||| pos	count=1
arg	a condition ||| condition	count=1
class	sparse [class] using ||| sparse [class]	count=2
arg	function with ||| f	count=1
module_class	[module_1] [class_2] ||| [module_1] linear regression [class_2]	count=2
function	the model predicts ||| predict on model	count=1
function	of weights is close ||| parameter accuracy	count=1
module_class	[module_1] [class_2] at least as extreme ||| [module_1] [class_2]	count=1
class	rdd into ||| rdd	count=1
function	probabilities ||| probability	count=1
class	columns on the ||| data frame	count=2
class	a function and ||| function	count=1
function	of each key ||| by key	count=1
class	as [class_2] ||| sql [class_1] [class_2] enter	count=1
arg	[arg] same code ||| [arg] name sinceversion	count=3
function	neutral [function] ||| [function]	count=3
class	to use for loading ||| mlreader	count=3
class	all ||| external	count=1
arg	with extra values from ||| extra	count=1
arg	[arg_1] and product ||| [arg_2] [arg_1]	count=4
class	ldamodel ||| ldamodel	count=1
function	get the [function_2] ||| [function_1] [function_2]	count=1
function	to ||| to	count=21
class	mean squared error which ||| regression	count=1
function	vectors ||| vectors	count=1
module_class	be included in [module_1] [class_2] ||| [module_1] [class_2]	count=8
function	for the termination of ||| termination	count=1
arg	saves the content ||| format mode partitionby	count=1
function	returns the number of ||| num	count=1
function	[function_1] [function_2] ||| sql [function_1] [function_2]	count=2
module_class	of this [class_2] ||| [module_1] train validation [class_2] copy	count=2
function_arg	[function_1] [arg_2] ||| [function_1] data [arg_2]	count=2
function	selectortype ||| selector type	count=1
function	of products [function_2] ||| [function_1] [function_2]	count=3
function	[function_1] original column ||| [function_2] [function_1]	count=2
function_arg	[function_1] [arg_2] maxbins=32 mininstancespernode=1 mininfogain=0 ||| [function_1] [arg_2] maxdepth	count=2
function	[function_1] null ||| [function_2] [function_1]	count=1
arg	to the specified table ||| tablename	count=1
function	embedded params to the ||| params to	count=1
arg	in matching ||| matching replace	count=1
class	word ||| word2vec	count=1
function	[function_1] the null ||| [function_2] [function_1]	count=5
arg	function [arg_2] ||| [arg_2] [arg_1]	count=2
module	two vectors we support ||| linalg	count=1
class	to ||| accumulator	count=1
class	training ||| logistic regression training summary	count=1
class	absolute error [class] ||| [class]	count=3
class	the ensemble ||| tree ensemble	count=2
class	which is a dataframe ||| logistic regression summary	count=1
arg	a function with ||| f	count=1
arg	parses the given ||| json_string	count=1
class	to all mixture ||| gaussian mixture model	count=1
arg	binary or multiclass ||| cls data numclasses	count=1
function	jvm seq of column ||| seq	count=1
class	__init__(self ||| abs scaler	count=1
function	of [function_2] ||| [function_2] [function_1]	count=1
class	[class] handleinvalid ||| [class]	count=1
function_arg	[function_1] [arg_2] 0 maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 ||| [function_1] [arg_2]	count=1
module_class	create a [module_1] [class_2] ||| [module_1] [class_2]	count=8
class	using rdd[vector] saveastextfile ||| mlutils	count=1
module	the [module] min or ||| [module]	count=1
module	[module_1] [module_2] ||| [module_2] [module_1] sparse	count=8
arg	optional parameters ||| params	count=1
module	the [module] numtrees ||| [module]	count=1
class	onevsrestmodel create ||| one vs rest	count=1
arg	from [arg] ||| [arg] delim	count=1
function	[function_1] trees in ||| [function_1] [function_2]	count=1
arg	return the ||| rdd	count=1
module	[module] are ||| [module]	count=1
class	how data ||| data	count=1
function	day of the month ||| dayofmonth	count=1
module	that all the objects ||| core	count=1
function	new mllib-local representation ||| as ml	count=4
function	keeplastcheckpoint ||| keep last checkpoint	count=2
function	the index of ||| with index	count=1
class	[class_1] singularvaluedecomposition ||| [class_1] [class_2]	count=2
class	[class] uid ||| [class]	count=6
function	[function_1] string format ||| [function_2] [function_1]	count=1
module_class	[module_1] matrix to ||| [module_1] [class_2]	count=2
class	so that ||| streaming query manager	count=1
function	for the null model ||| null	count=1
module	the [module] degree ||| [module]	count=1
class	get depth of tree ||| tree	count=1
arg	mincount the ||| mincount	count=1
class	broadcast variable ||| broadcast	count=1
class	convert this matrix ||| dense matrix	count=1
function	join ||| join	count=3
class	unit length normalization ||| normalizer	count=1
function	a param ||| has	count=1
class	into the returned ||| test case	count=1
function	intercept computed for ||| intercept	count=1
module	gets the [module] percentile ||| [module]	count=1
function	how ||| object size	count=1
class	this vector to the ||| vector	count=1
arg	end exclusive [arg_2] ||| [arg_2] [arg_1]	count=2
function	update the ||| update	count=1
function	contains ||| has	count=1
function	stream transform ||| stream transform	count=2
function	temporary table in the ||| table	count=1
module	the [module] numfeatures ||| [module]	count=1
function	computes ||| compute	count=2
class	for cross [class_2] ||| [class_1] [class_2]	count=1
function	approximate distinct count of ||| approx count distinct	count=1
arg	inverse=false ||| inverse	count=1
class	wait for new ||| manager	count=1
module	string name ||| param	count=1
function	false positive [function_2] ||| [function_2] [function_1]	count=1
class	[class_1] model with ||| [class_1] [class_2]	count=1
function	on first value ||| on	count=1
class	all the objects ||| external	count=1
function	the area [function_2] ||| [function_2] [function_1]	count=5
class	wait for ||| manager	count=1
class	of memory for ||| external	count=1
class	the accumulator's data type ||| accumulator	count=1
function	converts matrix columns ||| convert matrix columns from ml	count=1
module	gets the [module] numbuckets or ||| [module]	count=1
function	area ||| area	count=2
module	the list of column ||| sql	count=2
class	how much ||| external merger	count=1
function	with this streamingcontext ||| context	count=1
function	finding frequent ||| freq	count=1
module	gets the [module] isotonic ||| [module]	count=1
function	right [function_2] ||| [function_1] [function_2]	count=4
arg	each rdds ||| dstream n	count=1
arg	featurescol="features", predictioncol="prediction", [arg_2] ||| [arg_1] [arg_2]	count=3
function_arg	by applying c{f} ||| key by f	count=1
function	loads vectors saved ||| load vectors	count=2
class	sets ||| naive bayes	count=1
function	number of top features ||| set num top features	count=1
function	the stage [function_2] ||| [function_2] [function_1]	count=1
class	much of memory ||| merger	count=1
arg	data ||| data distname	count=1
class	and count ||| rdd	count=1
arg	the given name ||| name	count=1
module	[module] indices ||| [module]	count=1
arg	by codeblock [arg_2] ||| [arg_1] [arg_2]	count=1
class	external ||| external	count=1
function	returns the threshold ||| threshold	count=1
module	and return its ||| core	count=1
arg	format at the specified ||| compression	count=2
class	of the accumulator's ||| accumulator	count=1
module	containing union [module] frame ||| [module]	count=1
arg	table ||| table mode properties	count=1
function	[function_1] labeled ||| [function_1] [function_2]	count=1
class	:class dataframe ||| data frame	count=25
class	__init__(self ||| linear regression	count=2
function_arg	start [arg_2] ||| [function_1] start [arg_2]	count=1
function_arg	[function_1] reported an ||| [function_1] error [arg_2]	count=2
function_arg	text [arg_2] ||| [arg_2] [function_1]	count=1
function	root [function_2] ||| [function_2] [function_1]	count=1
arg	a given product ||| product	count=1
arg	of substr ||| substr str	count=1
arg	for ||| key	count=4
function	converts matrix columns ||| convert matrix columns	count=2
function	extract the minutes of ||| minute	count=1
function	ignore separators inside brackets ||| ignore brackets split	count=1
arg	[arg] maxiter=100 ||| featurescol [arg]	count=3
class	function ||| user defined function	count=1
function	storage type for ||| type	count=1
class	already ||| external group	count=1
arg	predictioncol="prediction", [arg_2] ||| [arg_1] [arg_2]	count=13
function	items as iterator ||| items	count=1
module_class	[module_1] [class_2] ||| [module_1] [class_2] get	count=8
module	awaitanytermination() can be used ||| sql	count=1
arg	[arg_1] product ||| [arg_2] [arg_1]	count=3
function	type for ||| type	count=1
class	this query that ||| streaming query	count=1
function	is ||| is	count=3
function	the log ||| log	count=1
arg	formula=none featurescol="features", [arg_2] ||| [arg_1] [arg_2]	count=4
class	partitioned ||| group by	count=2
function	a labeledpoint to ||| labeled point to	count=2
function	stream [function_2] ||| [function_1] [function_2]	count=7
class	instance's params ||| params	count=1
class	instance for this ||| one vs rest	count=1
function	aggregate [function_2] ||| [function_2] [function_1]	count=2
function	until any of the ||| any termination	count=1
class	in mixture ||| gaussian mixture model	count=1
function	[function_1] inside brackets ||| [function_2] [function_1]	count=1
module_class	[module_1] model on ||| [module_1] generalized linear regression [class_2]	count=1
module	[module] vectorsize or ||| [module]	count=1
arg	len with pad ||| len pad	count=1
module	the [module] selectortype ||| [module]	count=1
function	[function_1] string format ||| [function_1] [function_2]	count=1
class	randomly ||| cross validator model	count=1
class	with a randomly generated ||| train validation split	count=2
class	awaitanytermination() can be ||| manager	count=1
module	defaults closure [module] ||| [module]	count=3
class	linear regression ||| linear regression	count=1
arg	[arg_1] product ||| [arg_1] [arg_2]	count=3
function	of memory for ||| object size	count=1
function	[function_1] nodes summed ||| [function_1] [function_2]	count=1
function	of value ||| value	count=1
function	rdd's current storage level ||| storage level	count=1
module	the [module] :py ||| [module]	count=2
class	[class_1] [class_2] ||| [class_2] stream [class_1]	count=2
arg	labelcol="label", forceindexlabel=false) ||| labelcol forceindexlabel	count=4
module	the [module] mintf or ||| [module]	count=1
function	class inherit documentation ||| inherit	count=1
class	which [class_2] ||| [class_1] [class_2]	count=2
function	list of columns for ||| list columns	count=1
function	parses the expression string ||| expr	count=1
function	create [function_2] ||| [function_1] [function_2]	count=5
class	copy of the rdd ||| rdd	count=1
module	arrays of ||| ml linalg	count=1
function_arg	[function_1] observed ||| [arg_2] [function_1]	count=2
module_class	gets [module_1] [class_2] handleinvalid or its default ||| [module_1] [class_2] discretizer get handle invalid	count=1
function	returns a :class dataframe ||| sql	count=1
function_arg	the [function_1] [arg_2] ||| [function_1] [arg_2]	count=6
function	for ||| object	count=1
function	sql storage [function_2] ||| [function_2] [function_1]	count=2
function	is [function_2] ||| [function_1] [function_2]	count=4
function	[function_1] brackets pairs ||| [function_2] [function_1]	count=1
arg	99], quantilescol=none aggregationdepth=2): ||| fitintercept	count=1
class	stream query if this ||| data stream writer	count=1
function	foreachrdd [function_2] ||| [function_2] [function_1]	count=2
function	a converter ||| converter	count=1
function	the dot product of ||| dot	count=1
class	as a [class_2] ||| [class_1] [class_2]	count=1
module	or two [module_2] ||| [module_1] [module_2]	count=2
class	[class_1] :class dataframereader ||| [class_1] [class_2]	count=1
function	maxcategories ||| max categories	count=1
function_arg	the minimum [arg_2] ||| [arg_2] [function_1]	count=1
function	broadcast a read-only ||| broadcast	count=1
arg	in iterator ||| iterator	count=1
arg	observed data against ||| observed	count=1
module	the column ||| mllib	count=1
arg	an object is of ||| obj	count=1
module	the ||| sql	count=4
function	[function_1] rate ||| [function_1] [function_2]	count=6
function	log ||| log	count=3
function	compute the sum ||| sum	count=1
module_class	[module_1] [class_2] ||| [module_1] decision tree [class_2]	count=1
arg	class ||| cls	count=1
function	itemscol ||| items col	count=1
class	a paired rdd where ||| matrix factorization model	count=1
function	finding frequent items for ||| freq items	count=1
module	null ||| sql	count=2
class	underlying output ||| data stream writer	count=1
function	predict ||| predict	count=2
module_class	[module_1] [class_2] new session that has ||| [module_1] [class_2] new session	count=1
arg	a function [arg_2] ||| [arg_2] [arg_1]	count=2
class	adds [class_2] ||| [class_2] [class_1]	count=3
module_class	the [class_2] ||| [module_1] [class_2]	count=29
module	gets the [module] variancepower ||| [module]	count=1
class	stdout ||| profiler collector	count=1
arg	is later than ||| dayofweek	count=1
function	commutative reduce function ||| reduce by	count=1
function	default min [function_2] ||| [function_1] [function_2]	count=2
function	mean ||| mean	count=7
arg	multiclass classification ||| data numclasses categoricalfeaturesinfo	count=1
class	linear svm classifier ||| linear svcmodel	count=2
class	random [class_2] ||| [class_2] [class_1]	count=1
class	returned ||| py spark streaming test case	count=1
function	[function_1] rows of ||| [function_1] [function_2]	count=1
module	returns an active query ||| sql	count=1
class	function and ||| function	count=1
function_arg	transforms [arg_2] ||| [arg_2] [function_1]	count=2
function	stream foreachrdd get ||| stream foreach get	count=3
class	decision [class_2] ||| [class_1] [class_2]	count=2
module	[module] maxbins or ||| [module]	count=1
class	[class_1] model on ||| [class_1] [class_2]	count=2
arg	given a ||| cls	count=4
class	number ||| dense vector	count=2
class	vectors or ||| hashing tf	count=1
function	labeled ||| labeled	count=1
class	this dstream ||| kafka dstream	count=1
function_arg	[function_1] [arg_2] 0 elasticnetparam=0 0 tol=1e-6 ||| [function_1] [arg_2]	count=2
function	true label ||| label	count=2
function	a single script ||| single script	count=2
class	the column ||| model	count=1
class	which predictions ||| isotonic regression model	count=1
class	in one operation ||| rdd	count=1
function	a left ||| left	count=1
class	use for loading ||| mlreader	count=3
module	instance contains a ||| ml param	count=1
module	gets the [module] trainratio ||| [module]	count=1
function	from ||| range	count=1
arg	featurescol="features", predictioncol="prediction", [arg_2] ||| params [arg_1] [arg_2]	count=1
class	new spark [class_2] ||| [class_2] [class_1]	count=1
arg	the incoming dstream ||| dstream	count=3
class	be ||| streaming query manager	count=2
arg	of rdds ||| rdds	count=1
class	save ||| linear regression model	count=1
function_arg	[function_1] featurescol="features", labelcol="label", ||| [function_1] [arg_2]	count=14
arg	[arg] belongs to ||| [arg]	count=3
function	under the [function_2] ||| [function_2] [function_1]	count=1
function	for approximate [function_2] ||| [function_1] [function_2]	count=3
arg	saves the contents of ||| mode partitionby	count=1
function	recall ||| recall	count=3
class	[class] file ||| [class]	count=1
function	to a local representation ||| to local	count=1
function	[function_1] a rowmatrix ||| [function_1] [function_2]	count=2
arg	[arg_1] labelcol="label", metricname="f1") ||| [arg_1] [arg_2]	count=1
module	or two ||| mllib linalg	count=1
class	trees ||| trees	count=2
function	top [function_2] ||| [function_1] [function_2]	count=1
function	another module ||| module	count=1
arg	return the model ||| rdd	count=1
module	[module] maxiter or ||| [module]	count=1
function	add a py or ||| add py	count=1
module_class	[module_1] estimated coefficients ||| [module_1] [class_2]	count=4
class	accumulator's value ||| accumulator	count=2
function	into ||| parse	count=1
class	number ||| classification model	count=1
function	values for each key ||| key	count=2
module	[module] neutral ||| [module]	count=6
function_arg	return a new [function_1] [arg_2] dstream ||| [function_1] [arg_2]	count=2
class	the model ||| linear regression model	count=2
class	a dataframe ||| data frame	count=1
class	[class_1] query that ||| [class_1] [class_2] id	count=4
function	predictioncol ||| prediction col	count=1
function	receive accumulator updates in ||| update	count=1
function_arg	a new dstream by [function_1] [arg_2] dstream ||| streaming dstream [function_1] [arg_2]	count=2
module	every feature ||| mllib	count=1
class	of obtaining a test ||| test	count=1
module	the [module] layers or ||| [module]	count=1
function_arg	[function_1] [arg_2] maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 ||| [function_1] [arg_2] maxdepth	count=3
class	"predictions" which gives the ||| linear regression summary	count=3
module_class	[module_1] vectors ||| [module_1] [class_2]	count=2
module	[module] gaps or ||| [module]	count=1
function	the greatest value ||| greatest	count=1
arg	[arg_1] expected ||| [arg_2] [arg_1]	count=1
class	attr predictions which ||| generalized linear regression	count=2
function	min number of partitions ||| min partitions	count=1
module_class	[module_1] mean ||| [module_1] [class_2]	count=6
function	selector type of the ||| selector type	count=1
class	the model ||| generalized linear regression model	count=1
module	of memory ||| core	count=1
function	[function_1] predicts ||| [function_2] [function_1]	count=2
module_class	[module_1] this ||| [module_1] [class_2] vs rest copy	count=1
function	[function_1] external ||| [function_1] [function_2]	count=1
class	spark [class_2] ||| [class_1] [class_2]	count=1
module_class	creates [module_1] [class_2] ||| [module_1] [class_2]	count=24
class	a ||| rdd	count=1
function	rank ||| rank	count=1
module	the [module] initsteps ||| [module]	count=1
arg	the f function ||| f	count=2
function	freedom ||| freedom	count=1
module_class	of [class_2] ||| [module_1] decision tree [class_2]	count=1
function	[function_1] mllib vector ||| [function_1] [function_2]	count=3
class	in this ||| streaming	count=1
arg	correlation of two ||| col2 method	count=1
function	sets vector ||| set vector	count=2
arg	[arg_1] pad ||| sql rpad [arg_1] [arg_2]	count=2
class	[class] app' syntax ||| [class]	count=1
function	an exception [function_2] ||| sql install [function_1] [function_2]	count=1
class	for this udt ||| user defined type	count=1
function	nodes summed ||| nodes	count=2
module	[module] usercol or ||| [module]	count=1
function	into ||| param map to	count=1
function	inherit documentation from its ||| inherit	count=1
function	[function_1] the partitions ||| [function_1] [function_2]	count=1
function	the year of a ||| year	count=1
function	[function_1] or its ||| [function_2] [function_1]	count=1
class	[class] using stochastic ||| linear [class] with	count=1
arg	param from the param ||| param	count=1
function	load labeled [function_2] ||| [function_1] [function_2]	count=1
arg	saved ||| minpartitions	count=2
function	[function_1] of cols ||| [function_2] [function_1]	count=3
module_class	given [class_2] ||| [module_1] [class_2]	count=12
module	[module] regparam or ||| [module]	count=1
class	be found [class] was ||| [class]	count=1
function	that [function_2] ||| [function_2] [function_1]	count=3
class	with a ||| params	count=1
module	into disks ||| core	count=1
function	[function_1] grid ||| [function_2] [function_1]	count=1
arg	an object is ||| obj	count=1
function	aggregate the [function_2] ||| [function_2] [function_1]	count=3
class	this vector ||| sparse vector	count=1
function	a batch ||| batch	count=2
class	into the returned ||| streaming test case	count=1
class	left singular ||| singular	count=1
module	from this ||| linalg	count=1
function	a receiver ||| receiver	count=2
function	collection function ||| size	count=1
module_class	[module_1] terms ||| [module_1] [class_2]	count=1
module_class	creates a copy [module_1] [class_2] generated uid and some ||| [module_1] [class_2]	count=1
class	pipeline create and return ||| pipeline	count=1
class	can be used again ||| manager	count=1
function	python direct kafka ||| kafka	count=2
module_class	of [module_1] [class_2] ||| [module_1] [class_2]	count=10
function	[function] squared ||| [function] squared	count=4
function	to the ||| to	count=3
function	mean squared ||| mean squared	count=3
class	a :class ||| data frame	count=1
function	iterations default 1 which ||| iterations	count=1
class	elements in ||| rdd	count=1
function	[function_1] variance ||| [function_2] [function_1]	count=6
function	to get [function_2] ||| [function_1] [function_2]	count=1
arg	correlation of two columns ||| col1 col2 method	count=1
class	a ||| spark	count=5
module_class	[module_1] [class_2] - ||| [module_1] [class_2]	count=4
arg	a map [arg] ||| [arg]	count=3
arg	the [arg] ||| [arg]	count=2
class	as a ||| spark	count=1
class	for which predictions ||| regression model	count=1
class	__init__(self ||| regex tokenizer	count=1
class	content of ||| writer	count=1
class	that :func awaitanytermination() ||| manager	count=1
function	test that the ||| test training and	count=1
function	weights ||| weights	count=2
arg	top "num" number of ||| num	count=2
function	in which each ||| by	count=1
module	[module] modeltype or ||| [module]	count=1
module	this rdd's ||| core	count=1
arg	a dstream ||| dstream	count=3
module	and throws [module] ||| [module]	count=3
arg	labelcol="label", predictioncol="prediction", [arg_2] ||| [arg_2] [arg_1]	count=7
class	singular vectors ||| singular	count=2
function	subsamplingrate ||| subsampling rate	count=2
class	matrix to ||| matrix	count=2
function	embedded params ||| params	count=1
function	[function_1] [function_2] ||| mllib streaming kmeans test [function_2] [function_1]	count=1
arg	[arg_1] indices=none names=none) ||| [arg_1] [arg_2]	count=1
function	of the stage ||| stage	count=1
class	already partitioned ||| by	count=1
function	only create a ||| create	count=1
class	of the ||| rdd	count=1
function	changes the uid ||| uid	count=1
function	next memory limit if ||| next limit	count=1
arg	inverse=false [arg_2] ||| [arg_1] [arg_2]	count=1
arg	after [arg] >>> df ||| [arg]	count=1
class	curve which is ||| binary logistic regression	count=1
class	accumulator's data type ||| accumulator	count=1
function	smoothing ||| smoothing	count=1
function	of memory for ||| object	count=1
function	[function_1] leaders ||| [function_2] [function_1]	count=1
arg	n rows ||| n truncate vertical	count=1
function	this ||| has param	count=1
arg	[arg_1] forceindexlabel=false) ||| [arg_1] [arg_2]	count=7
module	[module] fitintercept ||| [module]	count=1
module_class	[module_1] [class_2] ||| [module_1] [class_2]	count=380
arg	len ||| len	count=2
class	rdd of key-value pairs ||| rdd	count=3
module	representation of a ||| ml linalg	count=1
class	already partitioned data into ||| by	count=1
function	[function_1] local ||| [function_1] [function_2]	count=2
module	gets the [module] fwe ||| [module]	count=1
function_arg	test of [arg_2] ||| [arg_2] [function_1]	count=5
function	the specified table ||| table	count=1
class	the rdd into ||| rdd	count=1
function	top [function_2] ||| [function_2] [function_1]	count=1
function	ordered in ascending ||| take ordered	count=1
function	under the [function_2] ||| [function_1] [function_2]	count=1
function	make predictions on a ||| predict on	count=1
module_class	the ldamodel ||| mllib ldamodel	count=1
function	[function_1] count of ||| [function_2] [function_1]	count=12
function	get number of ||| num	count=2
function	unifying ||| union	count=1
function	columns in an ||| columns from ml	count=2
class	a new spark ||| spark	count=1
module	that all the ||| core	count=1
module	multinomial ||| mllib	count=1
function_arg	[function_1] default 5 ||| [arg_2] [function_1]	count=3
function_arg	[function_1] been ||| [arg_2] [function_1]	count=2
class	a randomly ||| split model	count=1
class	[class_1] numtrees=20 ||| [class_2] [class_1]	count=16
function	sql storage type ||| sql type	count=3
function	for distinct count ||| count distinct	count=2
class	decision [class_2] ||| [class_2] [class_1]	count=2
function_arg	return a new dstream [function_1] [arg_2] ||| [function_1] [arg_2]	count=6
class	rdd[vector] saveastextfile ||| mlutils	count=1
module	gets the [module] censorcol ||| [module]	count=1
function	data ||| spill	count=1
arg	specified path ||| path mode	count=1
function	probability ||| probability col	count=1
class	[class_1] query ||| [class_1] [class_2]	count=2
arg	or multiclass ||| numclasses	count=1
arg	this instance ||| extra	count=1
arg	predictioncol="prediction", ||| predictioncol	count=23
class	specifies how data ||| data	count=1
module	[module] numitemblocks or ||| [module]	count=1
function	limits the result count ||| limit	count=1
class	:func awaitanytermination() can ||| manager	count=1
class	clusters in ||| clustering	count=1
class	again ||| streaming query manager	count=2
arg	formula=none featurescol="features", labelcol="label", ||| formula featurescol labelcol	count=4
module_class	[module_1] broker to ||| [module_1] [class_2]	count=2
arg	consist of ||| ascending numpartitions keyfunc	count=1
module	count of the ||| core	count=1
class	returns the root mean ||| metrics	count=1
function	given string ||| has	count=1
function	predict values for a ||| predict	count=2
function_arg	[function_1] a column ||| [arg_2] [function_1]	count=1
module	wait for new terminations ||| sql	count=1
class	:class statcounter members ||| stat counter	count=1
module	multiple copies [module] a ||| [module]	count=1
function	vector columns ||| vector columns	count=2
module_class	creates [module_1] [class_2] some ||| [module_1] [class_2] copy	count=4
module	:func awaitanytermination() can be ||| sql	count=1
function_arg	minimum [arg_2] ||| [function_1] [arg_2]	count=1
class	return a ||| rdd	count=1
arg	fixed values ||| param values	count=2
arg	with another value ||| value	count=1
arg	"num" number ||| num	count=2
module	[module] maxmemoryinmb or ||| [module]	count=1
function	java parammap into ||| java	count=1
arg	the correlation of ||| col2 method	count=1
class	:class windowspec ||| window	count=1
arg	generic function ||| createcombiner mergevalue mergecombiners numpartitions	count=1
function	register a java ||| register java	count=3
module	arbitrary key and ||| core	count=2
function_arg	[function_1] mincount ||| [arg_2] [function_1]	count=1
class	matrix to ||| dense matrix	count=1
function	finding frequent items ||| freq items	count=2
arg	with the spark sink ||| ssc addresses storagelevel maxbatchsize	count=1
function	number of ||| set num	count=1
class	"predictions" which [class_2] ||| [class_2] [class_1]	count=12
function_arg	unifying [arg_2] ||| [arg_2] [function_1]	count=2
function	predict values ||| predict	count=2
function	stopwords ||| stop words	count=1
function	the slideduration in seconds ||| slide duration	count=1
module	[module] windowsize or ||| [module]	count=1
module	partitioned data into ||| core	count=1
function	[function_1] local ||| [function_2] [function_1]	count=2
module	column names skipping null ||| sql	count=2
function	the count ||| count	count=1
class	queries so ||| streaming query manager	count=1
class	dataframe produced by ||| clustering	count=1
function	the initial value of ||| set initial	count=1
class	mean variance ||| rdd	count=1
function	deviance for the ||| deviance	count=1
module	gets the [module] percentile or ||| [module]	count=1
module	the [module] link or ||| [module]	count=1
module_class	[module_1] [class_2] by adding a column ||| [module_1] [class_2]	count=1
module	the [module] vectorsize ||| [module]	count=1
function	specified table ||| table	count=1
module	function but return ||| core	count=1
module_class	[module_1] ldamodel ||| [module_1] [class_2]	count=2
module	the ||| ml	count=1
function_arg	a json file ||| json path	count=1
arg	formula=none featurescol="features", [arg_2] ||| [arg_2] [arg_1]	count=4
class	each training data point ||| gaussian mixture summary	count=1
module_class	[module_1] from ||| [module_1] [class_2]	count=8
function	[function_1] an int ||| [function_2] [function_1]	count=4
function	py or ||| py file	count=1
function	values for each key ||| by key	count=1
function	[function_1] java object ||| [function_1] params to [function_2]	count=1
class	for params shared by ||| params	count=1
class	convert this vector to ||| vector	count=1
function	vector [function_2] ||| [function_2] [function_1]	count=9
class	accumulator's data type returning ||| accumulator	count=1
class	instance for this ||| pipeline model	count=1
module_class	[module_1] latest ||| [module_1] [class_2]	count=4
function	[function_1] java system ||| [function_1] [function_2]	count=1
class	group ||| external group	count=1
arg	the given key ||| key	count=1
function	find ||| find	count=3
arg	iterator ||| iterator key	count=1
module_class	[module_1] [class_2] or its default value ||| [module_1] [class_2]	count=1
function	converter to drop the ||| converter	count=1
module	the [module] fwe ||| [module]	count=1
function_arg	[function_1] value ||| [function_1] [arg_2]	count=1
class	sparkcontext at least the ||| spark context	count=1
function	[function_1] precision ||| [function_2] [function_1]	count=4
arg	spark sink ||| storagelevel maxbatchsize	count=1
arg	given [arg] ||| [arg]	count=1
function	columns in an ||| columns to ml	count=2
class	the rdd ||| rdd	count=3
arg	table and update ||| tablename	count=1
arg	in ||| col	count=1
arg	specified table ||| tablename overwrite	count=1
function	gets the ||| get	count=1
function	labeled points ||| labeled points	count=2
function	the [function] ||| run [function]	count=2
class	new ||| manager	count=1
arg	table ||| table column	count=1
class	this udt ||| user defined type	count=1
arg	each rdds into ||| dstream	count=1
class	set a [class_2] ||| [class_2] [class_1]	count=1
function	dump already partitioned ||| spill	count=1
function	instance contains a ||| param	count=1
class	sets ||| bisecting kmeans	count=2
function	applying [function] rdds ||| [function]	count=1
arg	featurescol="features", maxiter=20 seed=none checkpointinterval=10 ||| featurescol maxiter seed	count=2
function	learningoffset ||| learning offset	count=2
arg	input dataset for the ||| input	count=1
module	the [module] solver ||| [module]	count=2
class	pipeline ||| pipeline	count=4
function	underlying sql ||| sql	count=1
function	[function_1] driver ||| [function_1] [function_2]	count=1
function_arg	[function_1] an rdd ||| [function_1] [arg_2]	count=2
module	[module] separate ||| [module]	count=3
class	in this [class] ||| [class]	count=4
function_arg	[function_1] word ||| [function_1] [arg_2]	count=2
function	ignore separators [function_2] ||| [function_1] [function_2]	count=3
function	reducebykey to each ||| reduce	count=1
function	mean squared error ||| mean squared error	count=4
class	in "predictions" [class_2] ||| [class_2] [class_1]	count=18
class	for ||| streaming query	count=1
function	pretty printing of a ||| str	count=1
function	[function_1] has started ||| [function_2] [function_1]	count=2
arg	the expected distribution ||| expected	count=1
function_arg	java model [arg_2] ||| [function_1] cls sc [arg_2]	count=1
function	dot product of ||| dot	count=1
arg	assumed to consist ||| ascending numpartitions keyfunc	count=1
class	classification ||| classification	count=1
module	gets the [module] cachenodeids ||| [module]	count=1
arg	maxcategories=20 [arg_2] ||| [arg_1] [arg_2]	count=1
function	single script [function_2] ||| [function_1] [function_2]	count=3
function	partial objects do ||| partial	count=1
module_class	[module_1] model ||| [module_1] [class_2]	count=7
function	a left outer join ||| left outer join	count=1
class	vectors which this ||| vector	count=1
arg	[arg] of ||| [arg]	count=2
function	[function_1] window size ||| [function_1] [function_2]	count=1
function	matrix columns in an ||| matrix columns	count=2
function	false [function_2] ||| [function_2] [function_1]	count=2
function	:func awaitanytermination() can ||| reset	count=1
function	the index of ||| index	count=1
function	[function_1] with new ||| [function_2] [function_1]	count=1
arg	dstream ||| dstream	count=7
arg	splits=none [arg_2] ||| [arg_1] [arg_2]	count=2
class	linear [class_2] ||| [class_1] [class_2]	count=1
module	compute [module_2] ||| [module_1] [module_2]	count=2
function	set the trigger ||| trigger	count=1
function	instance is ||| is distributed	count=1
function	converts vector columns ||| convert vector columns from	count=1
class	input [class_2] ||| [class_2] [class_1]	count=4
arg	algorithm return ||| rdd	count=1
function	number of months between ||| months between	count=1
class	stream returning the result ||| stream reader	count=1
module	the [module] elasticnetparam ||| [module]	count=1
class	in "predictions" ||| summary	count=3
function	[function_1] of specific ||| [function_2] [function_1]	count=6
function	the list based ||| based	count=1
class	adds an input ||| stream	count=1
function	returns the value ||| get	count=1
function	instance to a ||| to	count=2
module	for ||| sql	count=1
arg	return ||| rdd	count=2
function_arg	window function [arg_2] ||| [function_1] [arg_2]	count=1
function	weights computed ||| weights	count=1
module	[module] fitintercept or ||| [module]	count=1
function	square root ||| root	count=2
arg	[arg] map if ||| [arg]	count=3
function_arg	function by name ||| function name doc	count=1
arg	data source ||| source	count=4
class	:py attr lda keeplastcheckpoint ||| ldamodel	count=1
arg	[arg_1] is received ||| [arg_1] [arg_2]	count=1
arg	the buckets ||| buckets	count=1
class	dump already partitioned data ||| external	count=1
module	of memory for ||| core	count=1
module_class	creates a copy [module_1] [class_2] and some ||| [module_1] [class_2]	count=4
function	array containing the ids ||| stage ids	count=1
arg	for data sampled ||| data	count=1
class	transfer this instance's params ||| params	count=1
arg	[arg_1] pad ||| sql rpad col [arg_1] [arg_2]	count=1
function	[function_1] keyed ||| [function_1] [function_2]	count=1
function	number ||| set num	count=1
function	a mllib vector ||| vector	count=1
function	[function_1] params are ||| [function_1] [function_2]	count=2
module	[module] threshold ||| [module]	count=2
arg	given product and ||| product	count=1
function	[function_1] [function_2] with ||| [function_2] [function_1]	count=2
function_arg	returns a [arg_2] ||| [function_1] [arg_2]	count=2
module	with ||| param	count=1
function_arg	[function_1] for ||| [arg_2] [function_1]	count=1
function	columns in ||| columns	count=4
function	wait ||| await termination	count=1
function	[function_1] distinct count ||| [function_2] [function_1]	count=4
class	for rformula ||| rformula	count=1
arg	scale < ||| scale	count=1
function_arg	on [arg_2] ||| [arg_2] [function_1]	count=3
function	libsvm format [function_2] ||| [function_2] [function_1]	count=8
function	list based ||| based	count=1
function	[function_1] messagehandler ||| [function_2] [function_1]	count=5
class	be used again to ||| streaming query	count=1
function	url ||| url	count=1
class	so ||| query manager	count=2
function	[function_1] functions registered ||| [function_2] [function_1]	count=2
class	based on ||| context	count=1
function	[function_1] columns for ||| [function_2] [function_1]	count=2
module	returns ||| sql	count=79
function	[function_1] outer ||| [function_2] [function_1]	count=6
function	again ||| reset	count=1
arg	in the specified database ||| tablename dbname	count=1
class	saved using rdd[vector] saveastextfile ||| mlutils	count=1
function	the l{sparkcontext} that ||| context	count=1
module_class	of [class_2] ||| [module_1] [class_2] rest copy	count=2
function	into label indices values ||| parse	count=1
module	a param with ||| param	count=1
function	in the training ||| training	count=1
arg	specified path ||| path	count=3
class	so that :func awaitanytermination() ||| manager	count=1
arg	i d samples drawn ||| shape scale numrows	count=1
module	representation ||| linalg	count=2
function_arg	[function_1] given product ||| [arg_2] [function_1]	count=2
function	[function_1] get ||| [function_2] [function_1]	count=10
function	functions registered in the ||| functions	count=1
class	loading ||| mlreader	count=3
function	dataframe from a ||| from	count=1
function	ordering columns in ||| order by	count=1
module_class	cluster assignments cluster [module_1] [class_2] trained on the ||| [module_1] [class_2]	count=1
module_class	return an rdd ||| core rdd	count=1
class	of the rdd's ||| rdd	count=1
function	compute the number ||| num	count=3
function	contains a ||| has	count=1
function	[function] create ||| [function]	count=3
function	initialized or not ||| ensure initialized	count=1
class	this model instance ||| regression model	count=2
class	[class] persists across ||| [class]	count=2
module	[module] binary or ||| [module]	count=2
function	right outer [function_2] ||| [function_2] [function_1]	count=3
function	value of weights ||| weights	count=1
module	contains the [module] ||| [module]	count=2
module	a given string ||| ml param	count=1
module_class	[module_1] :class dataframe ||| [module_1] [class_2] select	count=3
function	batch [function_2] ||| [function_2] [function_1]	count=2
arg	:class rdd, ||| schema samplingratio verifyschema	count=1
function	all values ||| all	count=1
arg	outputformat api ||| outputformatclass keyclass valueclass	count=1
arg	the spark sink ||| addresses storagelevel maxbatchsize	count=1
function	value ||| value	count=2
class	storagelevel based on ||| context	count=1
function	[function_1] cluster ||| [function_1] [function_2]	count=4
function	containing names of tables ||| tables	count=1
module	to ||| sql	count=1
arg	a [arg] ||| [arg] frombase	count=1
class	this [class] in a ||| [class]	count=2
function	parammap into [function_2] ||| [function_1] [function_2]	count=1
function	the ||| size	count=1
function	table ||| table	count=3
class	paired rdd ||| factorization model	count=1
class	returning the ||| reader	count=1
class	gives ||| linear	count=3
function	cost sum ||| compute cost	count=2
module	convert a number in ||| sql	count=1
function	as non-persistent and ||| unpersist	count=1
module	window function [module] the ||| [module]	count=2
class	that ||| external	count=1
class	that :func ||| streaming query	count=1
class	stringindexer ||| string indexer	count=1
class	to wait for ||| manager	count=1
function	get number [function_2] ||| [function_1] [function_2]	count=1
arg	saved using ||| path minpartitions	count=1
module	the list ||| sql	count=2
class	returns the root ||| metrics	count=1
class	return the ||| py spark streaming	count=1
class	set a configuration ||| spark conf	count=2
arg	marks the ||| blocking	count=1
function	each rdd contains the ||| by	count=1
function_arg	+= operator [arg_2] ||| [arg_2] [function_1]	count=2
function	[function_1] error ||| [function_2] [function_1]	count=10
class	used ||| streaming	count=1
arg	nodes or any hadoop-supported ||| path	count=2
class	which predictions are known ||| isotonic regression model	count=1
function_arg	a java [arg_2] ||| [function_1] [arg_2]	count=1
function	extract the week number ||| weekofyear	count=1
function	that with new ||| to df	count=2
function	a keyed ||| values	count=1
module	representation ||| mllib linalg	count=5
function	a python [function_2] ||| [function_2] [function_1]	count=2
class	this instance ||| pipeline	count=1
function	if the stage ||| stage	count=1
function	rdd of labeledpoint ||| load lib svmfile	count=1
class	return the column ||| scaler model	count=2
arg	the specified [arg_2] ||| [arg_1] [arg_2]	count=1
arg	an rdd of ||| rdd samplingratio	count=2
class	matrix ||| dense matrix	count=1
module	defined on ||| ml param	count=1
function	sigterm ||| sigterm	count=1
class	center for this model ||| model	count=2
function	sets the ||| set	count=2
function	min value ||| min	count=2
function	to a coordinatematrix ||| to coordinate	count=6
class	adds [class_2] ||| [class_1] [class_2]	count=1
function	containing a json string ||| json	count=1
arg	other ||| other	count=4
function	matrix on the ||| matrix	count=1
module	the [module] blocksize or ||| [module]	count=1
function	data into ||| spill	count=1
class	dataframe produced ||| clustering	count=1
function	a batch has completed ||| completed	count=1
function	outputcol=none [function] params for ||| [function]	count=1
function	sets the [function_2] ||| [function_2] [function_1]	count=2
module	the default implementation of ||| ml	count=1
module	gets the [module] numhashtables or ||| [module]	count=1
class	broker to ||| broker	count=1
function	the null ||| null	count=2
function	flatmapvalues ||| flat map values	count=1
function	[function_1] to the ||| [function_1] [function_2]	count=3
function	[function_1] predicts ||| [function_2] on [function_1]	count=2
function	contains a param ||| has	count=1
class	cachenodeids=false checkpointinterval=10 impurity="variance", ||| decision tree regressor	count=1
function	py ||| py	count=1
class	for ||| streaming	count=1
function	[function_1] threshold ||| [function_2] [function_1]	count=1
function	[function_1] prefix of ||| [function_2] [function_1]	count=1
function	:func awaitanytermination() ||| reset	count=1
function	model params are set ||| model params	count=1
arg	drawn ||| std numrows	count=1
arg	input dataset [arg_2] ||| [arg_2] [arg_1]	count=2
function	directory ||| path	count=1
module_class	and returns [class_2] ||| [module_1] [class_2]	count=1
function	jdbc ||| jdbc	count=1
function	rows of [function_2] ||| [function_2] [function_1]	count=2
function	returns a :class ||| sql	count=2
function	java ||| new java	count=1
class	the :class dataframe ||| data frame writer	count=5
class	return ||| scaler model	count=1
function	create an rdd ||| rdd	count=1
function_arg	unifying [arg_2] ||| streaming dstream [function_1] [arg_2]	count=2
arg	of ensuring all received ||| stopsparkcontext stopgracefully	count=1
class	layer ||| multilayer perceptron classification	count=1
arg	extra ||| extra	count=2
arg	string column [arg] times ||| col [arg]	count=1
class	'with [class] sc app ||| [class]	count=1
arg	is assumed to consist ||| ascending numpartitions keyfunc	count=1
arg	results as a ||| schema primitivesasstring prefersdecimal	count=2
arg	:py attr ||| value	count=101
function	none if the stage ||| stage	count=1
class	this context ||| streaming context	count=1
function	[function_1] functions registered ||| [function_1] [function_2]	count=2
arg	[arg] is null ||| [arg]	count=1
function	using ||| by	count=1
function	creates or [function_2] ||| [function_2] [function_1]	count=2
function_arg	[function_1] type datatype ||| [function_1] [arg_2]	count=1
module	matrix other from this ||| mllib linalg	count=1
arg	0 99], quantilescol=none aggregationdepth=2) ||| fitintercept	count=1
function	value ||| get	count=1
module	names ||| sql	count=2
module	all ||| core	count=1
arg	[arg_1] outputcol=none handleinvalid="error") ||| [arg_2] [arg_1]	count=5
arg	[arg_1] with pad ||| sql rpad col [arg_1] [arg_2]	count=1
class	recommends ||| matrix factorization model	count=2
function	[function_1] params instances ||| [function_1] [function_2]	count=1
class	the input ||| frame reader	count=1
module	that ||| sql	count=2
arg	wait a ||| timeout catch_assertions	count=1
arg	each element ||| preservespartitioning	count=2
function	that all the objects ||| object	count=1
module_class	gets the [module_1] [class_2] or its default value ||| [module_1] [class_2]	count=1
function	blocks in ||| col blocks	count=2
function	[function_1] accuracy ||| [function_1] [function_2]	count=1
function	partial ||| partial	count=1
function	a java udf so ||| java	count=1
module	the [module] mindf ||| [module]	count=1
arg	featurescol="features", labelcol="label", ||| formula featurescol labelcol	count=4
function	vector columns ||| vector columns to	count=1
function_arg	a text [arg_2] ||| [function_1] [arg_2]	count=1
function	params are set ||| params	count=1
function	[function] with ||| [function]	count=3
function_arg	[function_1] [arg_2] ||| [function_1] cls sc [arg_2]	count=3
function_arg	[function_1] given path ||| [arg_2] [function_1]	count=1
function	[function_1] tables/views in ||| [function_1] [function_2]	count=2
function	the minimum [function_2] ||| [function_1] [function_2]	count=4
function_arg	[function_1] another ||| [arg_2] [function_1]	count=7
function	the training ||| training	count=1
function	[function_1] java system ||| [function_2] [function_1]	count=1
function_arg	squared distance [arg_2] ||| [function_1] [arg_2]	count=1
module	gets the [module] itemscol ||| [module]	count=1
class	this dct ||| dct	count=1
class	regression model [class_2] ||| [class_1] [class_2]	count=3
class	that :func awaitanytermination() ||| query	count=1
arg	k=2 [arg_2] ||| [arg_1] [arg_2]	count=2
function_arg	return a new dstream [function_1] [arg_2] ||| streaming dstream [function_1] [arg_2]	count=6
function	by the given columns ||| by	count=1
function	left [function_2] ||| [function_1] [function_2]	count=4
function	as ||| as	count=5
arg	mincount ||| mincount	count=1
arg	[arg_1] inputcol=none outputcol=none) ||| params [arg_1] [arg_2]	count=1
class	this model ||| gaussian mixture model	count=1
module_class	of write() ||| ml pipeline	count=1
class	the underlying output ||| frame writer	count=1
class	[class_1] context to ||| [class_1] [class_2] remember duration	count=4
function	on [function] ||| package dependency on [function]	count=1
class	name ||| params	count=1
function	number of times ||| count	count=1
class	be used again ||| manager	count=1
function	convert a ||| to	count=2
function_arg	tables/views [arg_2] ||| [arg_2] [function_1]	count=1
function	the levenshtein distance of ||| levenshtein	count=1
function	system using the new ||| save as new	count=1
function_arg	[function_1] featurescol="features", ||| regressor [function_1] [arg_2]	count=1
function	a java [function_2] ||| [function_1] [function_2]	count=1
class	for multiclass [class_2] ||| [class_1] [class_2]	count=2
function_arg	[function_1] product and ||| [function_1] [arg_2]	count=1
function	call ||| call	count=1
arg	for data sampled from ||| data distname	count=1
arg	buckets the ||| numbuckets	count=1
arg	setparams(self [arg] seed=none numhashtables=1) ||| [arg] seed	count=1
class	with a randomly ||| split model	count=1
class	wait for new terminations ||| query	count=1
function	using the old hadoop ||| hadoop	count=1
module	[module] max ||| [module]	count=1
function	to [function_2] ||| [function_2] params [function_1]	count=1
module	[module_1] the ||| [module_2] [module_1]	count=6
function	finalstoragelevel ||| final storage level	count=1
function	[function_1] the companion ||| [function_2] params [function_1]	count=1
function	group the ||| group	count=1
class	wait for new ||| streaming	count=1
module	of [module_2] ||| [module_1] [module_2]	count=8
function	least value of ||| least	count=1
function_arg	mathfunction by name ||| mathfunction name doc	count=1
function_arg	__init__(self [arg_2] ||| [function_1] [arg_2]	count=33
class	dump already partitioned ||| by	count=1
function	initial [function_2] ||| [function_1] [function_2]	count=1
arg	or multiclass [arg_2] ||| [arg_1] [arg_2]	count=1
module_class	the given columns ||| sql data frame	count=1
arg	to end exclusive increased ||| end	count=1
module	gets the [module] losstype ||| [module]	count=1
class	be used again ||| streaming query manager	count=1
module	the [module] censorcol or ||| [module]	count=1
arg	joins with another ||| other on how	count=2
function_arg	[function_1] numpartitions partitions ||| [function_1] [arg_2]	count=1
class	[class_1] standard ||| [class_2] [class_1]	count=6
class	be used ||| query manager	count=1
function	single script file calling ||| script with local functions	count=1
function	impurity ||| impurity	count=2
function	as pandas pandas ||| to pandas	count=1
function_arg	size [arg_2] ||| [arg_2] [function_1]	count=2
function	[function_1] type for ||| [function_2] [function_1]	count=2
function	new hivecontext for ||| for	count=1
function	the minimum number of ||| min count	count=1
module	return a callsite representing ||| core	count=1
arg	that starts at pos ||| pos	count=1
function	until any of the ||| any	count=1
function	name for column ||| col	count=1
function	components ||| predict soft	count=2
class	:py attr predictions which ||| generalized linear regression	count=1
class	squared ||| sparse vector	count=2
class	degrees ||| linear regression summary	count=1
function	threshold if ||| threshold	count=1
class	for every feature ||| linear	count=1
arg	param ||| m2 param	count=1
function_arg	that is [arg_2] ||| [arg_2] [function_1]	count=2
class	[class] query that ||| [class]	count=1
function	[function_1] as ||| [function_1] [function_2]	count=1
function	centers represented as a ||| centers	count=3
function	an rdd ||| exponential vector rdd	count=1
module	param with ||| ml param	count=1
class	0 weightcol=none [class_2] ||| [class_1] [class_2]	count=2
function_arg	[function_1] saved using ||| [arg_2] [function_1]	count=3
function	abs vector ||| abs	count=1
class	by the param grid ||| param grid	count=1
function	weights is close to ||| parameter accuracy	count=1
module_class	value of [class_2] ||| [module_1] [class_2] get	count=8
function	the non-streaming ||| write	count=1
class	which [class_2] ||| [class_2] [class_1]	count=14
function_arg	[function_1] has been ||| [arg_2] [function_1]	count=2
class	how much of memory ||| external	count=1
function_arg	[function_1] centroids according ||| [arg_2] [function_1]	count=1
module	the [module] weightcol or ||| [module]	count=1
module	representation [module] the words ||| [module]	count=1
function	their vector representations ||| get vectors	count=1
class	the ||| standard scaler model	count=1
function	attach docstring from func ||| wrapped	count=1
module	for ||| core	count=1
arg	calculates the correlation ||| method	count=1
arg	the given path the ||| path	count=1
class	seed=none impurity="gini", numtrees=20 featuresubsetstrategy="auto", ||| random forest classifier	count=1
function_arg	of tables/views [arg_2] ||| [function_1] [arg_2]	count=1
class	[class_1] java ||| [class_2] [class_1]	count=8
module	[module_1] of ||| [module_2] [module_1] sparse matrix repr	count=1
function	[function_1] of each ||| [function_1] [function_2]	count=1
arg	by step ||| step	count=1
function	how much of ||| object size	count=1
class	[class_1] on the ||| [class_2] [class_1]	count=2
function	the deviance for the ||| deviance	count=1
function	memory ||| size	count=1
class	all ||| external merger	count=2
function	[function_1] the file ||| [function_1] checkpoint [function_2]	count=1
function	the driver as ||| to local	count=1
class	of tree (e ||| tree	count=1
function	deviance for the null ||| null deviance	count=1
class	[class_1] gives the ||| [class_2] [class_1]	count=29
arg	n rows to ||| n	count=1
function	to be placed into ||| modules to	count=1
function	the selector ||| set selector	count=1
class	a new [class] ||| [class]	count=2
class	new ||| query	count=1
class	this dstream ||| dstream	count=1
class	for new terminations ||| streaming query	count=1
arg	inputformat ||| inputformatclass	count=2
function	[function_1] null model ||| [function_1] [function_2]	count=1
function	[function_1] boolean ||| [function_1] [function_2]	count=3
function	weighted averaged ||| weighted fmeasure	count=1
module	returns an ||| sql	count=2
arg	column [arg] times ||| col [arg]	count=1
function_arg	[function_1] linearregressionmodel ||| [arg_2] [function_1]	count=1
class	[class_1] session ||| [class_2] [class_1]	count=1
arg	new item ||| item	count=1
function	labeledpoint to ||| labeled point to	count=2
function_arg	memory [arg_2] ||| [function_1] [arg_2]	count=4
class	sets ||| prediction col	count=2
function	[function_1] driver as ||| [function_2] [function_1]	count=4
function	[function_1] replaces a ||| [function_1] [function_2]	count=2
class	much of ||| external merger	count=1
class	for which predictions are ||| regression	count=1
function	[function_1] based on ||| [function_1] [function_2]	count=1
class	which predictions ||| regression model	count=1
function_arg	[function_1] item ||| [function_1] [arg_2]	count=6
function	right outer [function_2] ||| [function_1] [function_2]	count=3
function	that make up each ||| per	count=1
module_class	[module_1] this instance ||| [module_1] [class_2] copy	count=1
class	again ||| streaming query	count=1
function	orc ||| orc	count=1
function_arg	[function_1] number of ||| [arg_2] [function_1]	count=2
class	the rdd's elements in ||| rdd	count=1
module	this ||| linalg	count=1
class	name of the test ||| test	count=1
class	terminations ||| query	count=1
function_arg	[function_1] of document ||| [function_1] [arg_2]	count=1
module	[module] isotonic ||| [module]	count=1
module	param with a given ||| param	count=1
function	by the ||| by	count=1
class	model ||| streaming logistic regression	count=1
class	batches of ||| linear algorithm	count=1
arg	correlation ||| col2 method	count=1
function	list based on ||| based on	count=1
arg	or buffer ||| array_like dtype	count=2
arg	spark sink deployed ||| maxbatchsize	count=1
class	be used ||| streaming query manager	count=1
module	gets the [module] elasticnetparam ||| [module]	count=1
function	name ||| param	count=1
module	already partitioned data into ||| core	count=1
function	sets ||| set	count=133
class	returns ||| metrics	count=6
arg	data sampled from ||| data distname	count=1
function	stream foreachrdd ||| stream foreach	count=2
arg	the new item ||| item	count=1
function	in the key-value ||| map values	count=1
function	py or ||| py	count=1
function	[function_1] mean squared ||| [function_1] [function_2]	count=1
module	awaitanytermination() can ||| sql	count=1
function	[function_1] an external ||| [function_2] [function_1]	count=1
class	a [class] ||| [class] with	count=2
function_arg	return a new [function_1] [arg_2] ||| streaming dstream [function_1] [arg_2]	count=6
module	gets the [module] mininstancespernode or ||| [module]	count=1
function	[function_1] python parammap ||| [function_2] [function_1]	count=2
function	all ||| object size	count=1
class	on a model ||| linear regression	count=1
function	[function_1] variance ||| [function_1] [function_2]	count=6
function	dstream by applying 'left ||| left	count=1
class	convert this vector ||| vector	count=1
function	minimum number of times ||| min count	count=1
arg	rdd an rdd of ||| cls rdd	count=1
module_class	creates a copy [module_1] [class_2] uid and some ||| [module_1] [class_2] rest copy	count=1
module	a string representation ||| mllib linalg	count=1
function	get number [function_2] ||| [function_2] [function_1]	count=1
class	in tree ||| tree	count=1
arg	step ||| step	count=1
class	:py attr ||| lda	count=9
class	'x' has maximum membership ||| gaussian mixture	count=1
function	given a java object ||| java	count=1
arg	saved using rdd ||| path minpartitions	count=1
class	a paired ||| factorization model	count=1
class	new ||| streaming query	count=1
function	list of functions registered ||| list functions	count=1
arg	an rdd previously saved ||| minpartitions	count=1
function	jobs started ||| job	count=1
class	of model ||| model	count=2
function	[function_1] vector ||| [function_1] [function_2]	count=5
class	which is a risk ||| regression	count=2
class	that all the objects ||| external merger	count=1
arg	be downloaded ||| recursive	count=1
class	can ||| query	count=1
function	the minutes ||| minute	count=1
function	query underlying [function] ||| [function]	count=1
class	sets ||| tree params	count=3
function	the python module ||| module	count=1
function_arg	labeled points [arg_2] ||| [function_1] [arg_2]	count=4
module_class	[module_1] bisecting k-means ||| [module_1] [class_2]	count=2
function	[function_1] leaders ||| [function_1] [function_2]	count=1
class	the :class dataframe ||| frame writer	count=1
class	sets ||| aftsurvival regression	count=2
function	censorcol ||| censor col	count=1
function	all active ||| active	count=1
class	dataframe in ||| data frame	count=1
function	[function_1] temporary table ||| [function_2] [function_1]	count=2
class	much ||| external merger	count=2
arg	to fixed values ||| param values	count=2
module_class	convert this [class_2] ||| [module_1] [class_2]	count=12
module	[module] this ||| [module]	count=2
class	modeltype="multinomial", thresholds=none weightcol=none) ||| naive bayes	count=1
arg	given product ||| product	count=1
function	a new profiler using ||| new profiler	count=1
module	compute the ||| mllib linalg	count=4
function	vector columns in an ||| vector columns from ml	count=1
module	:func awaitanytermination() ||| sql	count=1
function	[function_1] checkpointed ||| [function_2] [function_1]	count=1
class	doc and [class] ||| [class]	count=3
arg	saves the content ||| name format mode partitionby	count=1
function	as the square root ||| root	count=2
class	this model ||| linear model	count=1
class	rdd[vector] ||| java vector	count=1
module	return ||| mllib	count=1
class	returns ||| spark session	count=1
function_arg	[function_1] applying c{f} ||| [arg_2] [function_1]	count=1
arg	wait a given amount ||| timeout catch_assertions	count=1
module	the [module] estimatorparammaps ||| [module]	count=1
function	of iterations default ||| iterations	count=1
module	combine functions and a ||| core	count=1
arg	time for a condition ||| condition	count=1
function	whether this instance is ||| is	count=1
module_class	[module_1] the rdd ||| [module_1] [class_2]	count=2
arg	formula=none featurescol="features", ||| formula featurescol	count=2
class	a dataframe that stores ||| alsmodel	count=2
class	from flume ||| flume utils	count=1
function	of the mean ||| mean	count=1
function_arg	a [function_1] [arg_2] ||| [function_1] [arg_2]	count=23
function	the stage info ||| stage info	count=1
function	rdd get ||| rdd get	count=2
module	vector ||| ml linalg	count=1
function	the log [function_2] ||| [function_2] [function_1]	count=1
function	partial objects do ||| save partial	count=1
class	feature ||| linear model	count=1
arg	numbits ||| numbits	count=1
function	that is ||| coalesce	count=1
function	nulltype ||| nulltype	count=1
function	new profiler using ||| new profiler	count=1
function	termination ||| termination	count=1
arg	object ||| obj	count=3
class	training ||| distributed ldamodel	count=1
function_arg	save path ||| save path	count=4
function	converter to ||| converter	count=1
arg	drawn ||| numrows numcols	count=4
function	[function_1] join ||| [function_2] [function_1]	count=8
module	gets the [module] featuresubsetstrategy ||| [module]	count=1
class	ensemble ||| tree ensemble	count=3
arg	given database ||| dbname	count=1
function	convert a dict into ||| to	count=1
arg	[arg_1] seed=none numhashtables=1) ||| [arg_1] [arg_2]	count=1
function	columns for the given ||| columns	count=1
function_arg	transform [arg_2] ||| [function_1] [arg_2]	count=1
function	that :func ||| reset	count=1
function	to a java ||| to java	count=9
module	[module] min or ||| [module]	count=1
module	of the [module] ||| [module]	count=3
module_class	creates [module_1] [class_2] and some ||| [module_1] [class_2] copy	count=4
function	infer the [function] from ||| infer [function]	count=1
module_class	shortcut of write() ||| ml mlwritable	count=1
function	in utc ||| utc	count=2
module	a param ||| ml	count=1
function	initialized or ||| ensure initialized	count=1
module	be used again ||| sql	count=1
function	to combine ||| combine	count=1
class	input ||| reader	count=2
class	sets ||| max iter	count=1
function	converts a ||| to	count=1
class	[class_1] with ||| [class_1] [class_2]	count=1
function	variant of _siftdown ||| siftdown	count=1
module	[module_1] [module_2] a sparsematrix >>> sm1 ||| [module_2] [module_1]	count=1
function	options ||| options	count=3
class	disks ||| group by	count=2
class	[class] remember rdds ||| [class]	count=2
function	list [function_2] ||| [function_2] [function_1]	count=8
class	new [class] containing ||| [class]	count=1
class	singularvaluedecomposition ||| value decomposition	count=2
function	new terminations ||| reset	count=1
function	cost sum ||| cost	count=2
arg	the input stream ||| stream	count=1
arg	the [arg] ||| path [arg]	count=4
function	get ||| get	count=7
module	[module] maxmemoryinmb ||| [module]	count=1
module	gets the [module] selectortype ||| [module]	count=1
arg	get the n elements ||| num	count=1
class	of :class ||| data frame	count=2
function	a sliding window ||| window	count=1
module	type or [module] the ||| [module]	count=1
class	so that :func awaitanytermination() ||| streaming query	count=1
module	gets the [module] intermediatestoragelevel ||| [module]	count=1
class	awaitanytermination() ||| streaming query	count=1
function_arg	[function_1] [arg_2] 0 tol=1e-6 fitintercept=true threshold=0 ||| [function_1] [arg_2]	count=2
function	[function_1] a cluster ||| [function_2] [function_1]	count=4
function	[function_1] global temporary ||| [function_1] [function_2]	count=1
arg	generic function to ||| createcombiner mergevalue mergecombiners numpartitions	count=1
function	to a boolean ||| to boolean	count=3
class	generated ||| validation	count=2
function	converts vector columns ||| convert vector columns to	count=1
function	of labeledpoint ||| load lib svmfile	count=1
class	0 weightcol=none ||| linear	count=2
function	heappop ||| heappop	count=1
function	features i e length ||| features	count=1
function	partial objects ||| partial	count=1
module	using an associative and ||| core	count=1
module	the [module] quantileprobabilities ||| [module]	count=1
function	the [function] from dict/namedtuple/object ||| infer [function]	count=1
class	wait for new ||| query manager	count=1
module	and a ||| core	count=2
function	fields word [function] ||| [function]	count=1
class	belongs to this params ||| params	count=1
class	returns a paired ||| matrix factorization	count=1
function	test that ||| test	count=2
function	of nodes summed ||| nodes	count=1
function	maxsentencelength ||| max sentence length	count=1
function	with the ||| range	count=1
function	labeled [function_2] ||| [function_1] [function_2]	count=1
function	items for columns possibly ||| items	count=1
class	a :class dataframe ||| grouped data	count=1
function	with ||| has	count=1
module	and profiles ||| core	count=1
function	two fields threshold ||| threshold	count=2
class	vectors ||| vector indexer model	count=1
module_class	this rdd which ||| core rdd	count=1
module_class	given [class_2] ||| [module_1] [class_2] writer	count=4
function	names of ||| table names	count=1
arg	maxcategories=20 [arg_2] ||| [arg_2] [arg_1]	count=1
function	extract ||| extract	count=1
arg	labelcol="label", featurescol="features", [arg_2] ||| [arg_2] [arg_1]	count=2
function_arg	[function_1] input stream ||| [arg_2] [function_1]	count=1
class	how much of ||| external merger	count=1
function	labeledpoint to a string ||| labeled point to	count=1
class	query either by :func ||| streaming	count=1
function	that all ||| object	count=1
module	gets the [module] evaluator or ||| [module]	count=1
class	sets ||| logistic regression	count=2
class	gaussians in mixture ||| mixture	count=1
function	a class inherit documentation ||| inherit doc	count=1
arg	between ||| v1 v2	count=1
function	a labeledpoint ||| labeled point	count=1
module	[module] cachenodeids or ||| [module]	count=1
function	batch of ||| batch	count=2
arg	returns a function ||| f	count=1
class	this [class] master ||| [class]	count=1
class	partitioned data into ||| group by	count=1
arg	iterator ||| iterator	count=1
arg	[arg_1] step ||| [arg_2] [arg_1]	count=2
function	whether this instance is ||| is distributed	count=1
class	which predictions are known ||| regression	count=1
class	regression [class_2] ||| [class_2] [class_1]	count=4
class	ranking ||| ranking metrics	count=2
function	if using checkpointing ||| checkpoint	count=1
module	given data ||| sql	count=2
function	the jobs started by ||| job	count=1
function	batch of jobs has ||| batch	count=2
module	create new ||| sql	count=1
class	model ||| decision tree model	count=1
arg	another value ||| value	count=1
function_arg	a keyed [arg_2] ||| [arg_2] [function_1]	count=1
module_class	[module_1] [class_2] ||| [module_1] external [class_2]	count=1
function	param with ||| param	count=1
function	[function_1] list based ||| [function_2] [function_1]	count=1
function	to this accumulator's value ||| add	count=1
class	sets ||| elastic net param	count=1
arg	joins [arg_2] ||| [arg_2] [arg_1]	count=2
module	again to wait ||| sql	count=1
class	the [class] trained on ||| gaussian mixture [class]	count=1
module	given data source ||| sql	count=1
function_arg	newline-delimited json [arg_2] ||| [function_1] [arg_2]	count=1
function	withmean ||| with mean	count=1
function	into [function_2] ||| [function_1] [function_2]	count=1
function	configuration ||| conf	count=2
function	given data type ||| parse datatype	count=1
class	this tests a ||| tests	count=1
function	[function_1] [function_2] ||| [function_1] count [function_2]	count=4
function	creates a :class ||| create	count=1
arg	generates ||| sc	count=2
function	stream api ||| stream from	count=1
function	single script file ||| single script	count=2
class	counts ||| grouped	count=1
function	to wait for new ||| reset	count=1
class	this :class dataframe ||| data frame	count=3
class	:func awaitanytermination() can ||| streaming query	count=1
class	that :func awaitanytermination() ||| streaming query manager	count=1
function	columns for the ||| columns	count=1
function	gets the [function_2] ||| [function_2] [function_1]	count=1
module	[module] itemscol ||| [module]	count=1
arg	column after position pos ||| str pos	count=1
function	regparam ||| reg param	count=1
function	and refresh all the ||| refresh	count=1
arg	the given [arg] ||| [arg]	count=1
arg	is received using ||| storagelevel	count=1
function	awaitanytermination() can be used ||| reset	count=1
arg	after position pos ||| pos	count=1
arg	validates that [arg] ||| [arg]	count=2
function	likelihood ||| likelihood	count=2
class	to ||| query	count=1
class	the context ||| context	count=1
class	cachenodeids=false checkpointinterval=10 impurity="gini", ||| classifier	count=1
function_arg	of tables/views [arg_2] ||| [arg_2] [function_1]	count=1
function	sort the list based ||| sort result based	count=1
module	gets the [module] layers ||| [module]	count=1
function	from a ||| from	count=1
module	the [module] maxbins or ||| [module]	count=1
class	in ||| param grid builder	count=1
class	broadcast on ||| broadcast	count=1
class	word2vec ||| word2vec	count=1
function	rdd get offsetranges ||| rdd get offset ranges	count=1
module_class	[module_1] terms or ||| [module_1] [class_2]	count=1
function_arg	type of [arg_2] ||| [function_1] [arg_2]	count=1
arg	parses ||| json_string	count=1
function	[function_1] top features ||| [function_1] [function_2]	count=1
class	already ||| by	count=1
function	[function] between ||| need [function]	count=1
module_class	[module_1] bisecting ||| [module_1] [class_2] kmeans	count=2
class	number ||| distributed matrix	count=1
function	data or ||| get or	count=1
function	synonyms ||| synonyms	count=1
function	tables/views in the ||| tables	count=1
function	start offset ||| offset	count=1
function	matrix columns in ||| matrix columns from	count=1
function	perform a left ||| left	count=1
function	or ||| get	count=2
function	as a temporary ||| as	count=1
class	already ||| external group by	count=2
function	for this ||| size	count=1
function_arg	window size [arg_2] ||| [function_1] [arg_2]	count=1
function	mintf ||| min tf	count=1
class	in this model ||| model	count=3
function	number of rows ||| num row	count=3
class	an rdd containing ||| rdd	count=1
function	approximate ||| approx	count=2
arg	each rdds into the ||| dstream n block	count=1
class	loss or [class] ||| regression [class]	count=1
arg	inverse=false [arg_2] ||| [arg_2] [arg_1]	count=1
function	on ||| transform	count=2
function	dataset for the test ||| test	count=1
function	minimum ||| min	count=2
class	checkpointinterval=10 impurity="gini", numtrees=20 featuresubsetstrategy="auto", ||| random forest classifier	count=1
function	returns a new ||| new	count=1
module	[module] scalingvec or ||| [module]	count=1
function	a single [function_2] ||| [function_1] [function_2]	count=5
module	gets the [module] names or ||| [module]	count=1
module_class	[module_1] block ||| [module_1] [class_2]	count=8
function	squared error ||| squared error	count=4
function_arg	[function_1] string in ||| [function_1] [arg_2]	count=1
function	checkpointed and materialized either ||| checkpointed	count=1
class	cachenodeids=false checkpointinterval=10 impurity="variance", seed=none ||| decision tree regressor	count=1
function	java udf ||| java	count=1
class	error which is ||| regression	count=1
function	fdr ||| set fdr	count=1
arg	key-value pairs ||| pairs	count=1
arg	saves the content of ||| mode partitionby	count=2
class	[class_1] sparkcontext ||| [class_2] [class_1]	count=4
module_class	of [class_2] ||| [module_1] generalized linear regression [class_2]	count=2
function	[function_1] likelihood ||| [function_2] [function_1]	count=5
class	saving ||| java mlwriter	count=1
arg	creates ||| sparkcontext sparksession jsqlcontext	count=1
arg	number n ||| n	count=2
function	partial ||| save partial	count=2
function_arg	a csv [arg_2] ||| [function_1] [arg_2]	count=3
function	[function_1] tables/views ||| [function_2] [function_1]	count=2
function	all the ||| object	count=1
function	start offset specified ||| from offset	count=1
arg	schema ||| schema options	count=1
arg	value or cleared ||| description interruptoncancel	count=1
class	the embedded ||| params	count=1
class	again to wait ||| manager	count=1
class	category if ||| multiclass	count=2
function	the list based on ||| based on	count=1
function	[function_1] items for ||| [function_2] [function_1]	count=1
arg	specified schema ||| schema options	count=1
module	the [module] usercol or ||| [module]	count=1
function	much of memory ||| object	count=1
function_arg	[function_1] mincount the ||| [arg_2] [function_1]	count=1
class	for each numeric ||| grouped	count=2
function	fit test of ||| test	count=1
arg	against the expected ||| expected	count=1
function	as a ||| as	count=2
arg	term ||| term	count=1
arg	for col1 ||| col1	count=1
class	the column ||| scaler	count=2
class	[class_1] tree ||| [class_2] [class_1]	count=2
arg	[arg_1] co ||| [arg_1] [arg_2]	count=3
function	[function_1] number ||| [function_2] [function_1]	count=3
arg	[arg_1] maxiter=100 ||| [arg_2] [arg_1]	count=3
arg	codeblock ||| cls	count=1
function	:func awaitanytermination() can be ||| reset	count=1
class	terms or ||| ldamodel	count=1
function	add a py or ||| add py file	count=1
function	number of months ||| months	count=1
arg	function func ||| func	count=1
arg	binary or multiclass classification ||| data numclasses categoricalfeaturesinfo	count=1
function	[function_1] param ||| [function_2] [function_1]	count=1
function_arg	maximum [arg_2] ||| [arg_2] [function_1]	count=1
function	with start offset specified ||| from offset	count=1
module	the [module] subsamplingrate ||| [module]	count=1
function	transpose ||| transpose	count=1
function_arg	[function_1] [arg_2] maxiter=100 regparam=0 0 elasticnetparam=0 ||| [function_1] [arg_2] maxiter	count=2
function	dump ||| spill	count=1
module	python wrapper of ||| ml	count=5
function	sliding window of time ||| window	count=1
function	the soundex encoding for ||| soundex	count=1
function	for ||| reset	count=1
function	columns in an input ||| columns from ml	count=2
function	gets ||| get	count=9
class	a [class] ||| [class] stream	count=3
function	and vector ||| get vectors	count=1
class	with a randomly ||| split	count=2
class	error which is defined ||| linear regression summary	count=1
function	to receive accumulator updates ||| update	count=1
module	all pairs [module] are ||| [module]	count=1
class	a model with weights ||| linear regression with	count=1
module_class	creates [module_1] [class_2] ||| [module_1] train validation [class_2] copy	count=6
module	a given ||| param	count=1
function	so ||| reset	count=1
function	rdd [function_2] ||| [function_1] [function_2]	count=3
function	[function_1] specific ||| [function_2] [function_1]	count=6
function	[function_1] using an ||| [function_2] [function_1]	count=8
class	the given parameters in ||| param grid builder	count=1
class	the mean variance and ||| rdd	count=1
class	into disks ||| group by	count=1
arg	observed data [arg_2] ||| [arg_1] [arg_2]	count=2
module	of this rdd's elements ||| core	count=1
class	memory ||| merger	count=1
function_arg	by [arg_2] ||| [arg_2] [function_1]	count=1
function	zips ||| zip	count=1
function	the [function] pairs in ||| collect as [function]	count=1
function	[function_1] keyed ||| [function_2] [function_1]	count=1
function_arg	[function_1] product ||| [arg_2] [function_1]	count=1
function	queries so that :func ||| reset	count=1
function	[function_1] cols ||| [function_2] [function_1]	count=5
function	window [function_2] ||| [function_1] [function_2]	count=2
arg	database table named table ||| table column	count=1
arg	top "num" number ||| num	count=2
function	[function_1] get offsetranges ||| [function_2] [function_1]	count=10
module	instance ||| ml param	count=2
arg	[arg_1] [arg_2] 0 tol=1e-6 fitintercept=true standardization=true ||| [arg_1] [arg_2]	count=4
module	[module] estimator ||| [module]	count=1
function	value to a ||| to	count=2
class	the content of ||| writer	count=1
module	of each cluster ||| ml	count=1
function	months ||| months	count=1
function	the driver returns none ||| driver	count=1
arg	or list ||| oneatatime	count=1
arg	right-pad the string column [arg_1] [arg_2] ||| [arg_1] [arg_2]	count=1
module	or compute [module_2] ||| [module_1] [module_2]	count=2
arg	have the same param ||| m2 param	count=1
class	configuration ||| spark conf	count=1
class	new [class] ||| [class]	count=2
function	stepsize ||| step size	count=2
class	already partitioned ||| external group by	count=1
arg	sql statements ||| returntype	count=1
arg	labelcol="label", predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", ||| labelcol predictioncol probabilitycol	count=1
function_arg	[function_1] [arg_2] ||| regressor [function_1] [arg_2]	count=2
arg	from one base ||| frombase tobase	count=1
function	as non-persistent ||| unpersist	count=1
class	[class_1] k-means algorithm ||| [class_1] [class_2]	count=1
module	"func" and a ||| core	count=1
class	which gives the ||| linear regression	count=6
function_arg	a text [arg_2] ||| [arg_2] [function_1]	count=1
class	[class_1] generated ||| [class_2] [class_1] copy	count=8
class	the context to ||| context	count=1
function	min [function_2] ||| [function_1] [function_2]	count=2
class	memory for this ||| merger	count=1
arg	exp(-x * [arg] + offset)) ||| [arg] npoints	count=1
module	for every feature ||| mllib	count=1
module_class	returns [class_2] ||| [module_1] [class_2] session	count=1
class	new rdd ||| rdd	count=2
class	much of ||| merger	count=1
function	iterations [function_2] ||| [function_2] [function_1]	count=4
module	[module] predictioncol or ||| [module]	count=1
function	labeledpoint [function_2] ||| [function_2] [function_1]	count=5
arg	an object into ||| obj	count=1
function	contains a param ||| param	count=1
function	columns ||| columns	count=5
class	a function ||| defined function	count=1
module_class	of write() ||| ml mlwritable	count=1
arg	observed data ||| observed	count=1
function	into ||| map to	count=1
class	[class_1] by key ||| [class_2] [class_1]	count=1
function	of freedom ||| of freedom	count=4
class	dump already ||| group	count=1
class	as session ||| spark session	count=1
module_class	[module_1] model on ||| [module_1] [class_2]	count=2
module	[module] max or ||| [module]	count=1
function	partitions ||| partitions	count=2
function	ignore separators inside ||| ignore	count=1
module	the [module] regparam ||| [module]	count=1
class	randomly ||| split	count=2
class	as a :class dataframe ||| data	count=3
module	implementation of ||| ml	count=1
module	[module] impurity or ||| [module]	count=1
function	[function_1] matrix columns ||| [function_1] [function_2]	count=2
function	of labeledpoint ||| lib svmfile	count=1
function	labeledpoint to a ||| labeled point to	count=2
module_class	of this instance ||| ml pipeline model	count=1
function	[function_1] boolean ||| [function_2] [function_1]	count=3
function	a labeledpoint to a ||| labeled point to	count=1
function_arg	sets [arg_2] ||| [function_1] min count [arg_2]	count=2
class	the param grid ||| param grid	count=2
class	which is defined as ||| regression	count=1
function	linkpower ||| link power	count=1
class	to this accumulator's ||| accumulator	count=1
function_arg	[function_1] [arg_2] maxbins=32 mininstancespernode=1 mininfogain=0 0 ||| [function_1] [arg_2] maxdepth	count=1
function	is of type ||| type	count=1
class	curve ||| binary logistic	count=2
function	index of ||| with index	count=1
function	transform the ||| transform	count=1
class	the underlying output ||| writer	count=1
function_arg	mathfunction [arg_2] ||| [arg_2] [function_1]	count=2
function	return the [function_1] [function_2] ||| [function_2] [function_1]	count=2
function	for all [function_2] ||| [function_2] [function_1]	count=2
function	:class dataframe [function_2] ||| [function_1] [function_2]	count=7
class	builder getorcreate() [class] ||| [class]	count=1
arg	iff [arg] ||| [arg]	count=2
function	script [function_2] ||| [function_2] [function_1]	count=3
class	specifies the input ||| frame reader	count=1
class	for each numeric columns ||| grouped data	count=2
class	to wait for ||| streaming	count=1
arg	the observed data against ||| observed	count=1
arg	the number ||| num	count=1
class	dump already partitioned ||| external	count=1
function	:class dataframe as ||| data frame as	count=2
class	string ||| params	count=1
arg	from [arg] before ||| [arg] delim	count=1
class	stream query if ||| data stream writer	count=1
function_arg	[function_1] [arg_2] maxbins=32 mininstancespernode=1 mininfogain=0 0 ||| regressor [function_1] [arg_2] maxdepth	count=1
class	wait for ||| streaming query manager	count=1
function	the square root ||| root	count=2
module	of ||| ml	count=95
class	for the stream query ||| stream writer	count=1
function	to the same ||| to	count=1
module	with a given string ||| ml param	count=1
module	[module] selectortype ||| [module]	count=1
function	matrix columns in ||| matrix columns	count=2
class	checkpointinterval=10 losstype="logistic", maxiter=20 ||| gbtclassifier	count=2
class	used ||| manager	count=1
class	defines the ||| spec	count=1
function	f1-measure ||| f1measure	count=1
module_class	from this block ||| mllib linalg block	count=1
function	the model [function_2] ||| [function_2] [function_1]	count=12
module	[module] the ||| [module]	count=10
class	:class dataframe ||| grouped data	count=1
function_arg	chain [arg_2] ||| [function_1] [arg_2]	count=1
class	generated ||| train validation	count=2
function	this ||| param	count=1
function	udf registration ||| udf	count=1
module	gets the [module] droplast ||| [module]	count=1
arg	the centroids according ||| decayfactor timeunit	count=2
class	a function and ||| defined function	count=1
function	[function_1] vector columns ||| [function_1] [function_2]	count=2
function	metricname ||| metric name	count=2
function	training set given the ||| training	count=1
function	python parammap ||| param map from	count=2
function	[function_1] started ||| [function_1] [function_2]	count=1
module_class	returns [class_2] ||| [module_1] [class_2] new session	count=2
function	initial ||| initial	count=1
class	sets ||| net param	count=1
arg	the specified partitioner ||| numpartitions partitionfunc	count=1
function_arg	__init__(self formula=none featurescol="features", ||| init formula featurescol	count=2
module	the rdd's ||| core	count=1
arg	get ||| num	count=1
function	ndcg value of all ||| ndcg	count=1
arg	other ||| other numpartitions	count=3
arg	[arg] is nan ||| [arg]	count=1
class	return ||| spark streaming	count=1
function	the features ||| features col	count=1
function_arg	[function_1] [arg_2] 0 elasticnetparam=0 0 tol=1e-6 ||| [function_1] [arg_2] maxiter	count=2
arg	string column ||| col	count=1
function_arg	[function_1] [arg_2] tol=1e-6 regparam=0 0 weightcol=none ||| [function_1] [arg_2]	count=2
class	partitioned data into disks ||| by	count=1
function	[function] params for ||| [function]	count=2
class	__init__(self ||| binary classification evaluator	count=1
module	contains ||| ml param	count=2
function	convert a value to ||| to	count=4
function	python direct kafka stream ||| kafka direct stream from	count=1
function	the new ||| as new	count=1
class	underlying output ||| frame writer	count=1
function_arg	[function_1] for data ||| [arg_2] [function_1]	count=1
arg	the spark sink deployed ||| storagelevel maxbatchsize	count=1
class	that ||| streaming	count=1
module	expressions and returns ||| sql	count=1
function	types inferred from ||| type	count=1
class	regression model on ||| regression with	count=3
class	[class_1] including ||| [class_2] [class_1]	count=4
arg	each rdds into the ||| dstream	count=1
module	the [module] trainratio or ||| [module]	count=1
function	[function_1] window ||| [function_1] [function_2]	count=1
function	maxdepth ||| max depth	count=1
function	"zerovalue" which ||| fold by	count=1
function	rdd ||| rdd	count=6
function	original column during fitting ||| original	count=1
arg	<http //jsonlines ||| mode compression dateformat	count=1
function	name ||| name	count=2
function	objects ||| object size	count=1
class	max ||| scaler model	count=1
function	[function_1] active ||| [function_2] [function_1]	count=4
arg	binary or multiclass ||| data numclasses	count=1
class	wait for new ||| streaming query	count=1
module	gets the [module] trainratio or ||| [module]	count=1
function	ids of ||| stage ids	count=1
class	for this dct ||| dct	count=1
function	[function_1] join ||| [function_1] [function_2]	count=8
function	index of the ||| partitions with index	count=1
class	data into ||| external group by	count=1
class	a [class] using ||| [class] with	count=1
module	the [module] maxmemoryinmb ||| [module]	count=1
function	transform the rdd of ||| transform	count=1
class	new [class] containing union ||| [class]	count=1
function	applies standardization transformation on ||| transform	count=1
class	the spark ||| spark	count=2
module	of rows whose distance ||| ml	count=1
class	objects ||| external	count=1
function	on first ||| on	count=1
module	dump already partitioned ||| core	count=1
arg	by name ||| name	count=1
function	brackets pairs e ||| brackets split	count=1
class	stream query if this ||| stream writer	count=1
function_arg	java parammap ||| java pyparammap	count=1
arg	and a neutral ||| zerovalue	count=1
function	functions registered ||| functions	count=1
function	value [function_2] ||| [function_2] by [function_1]	count=1
function	scalingvec ||| scaling vec	count=1
arg	each rdds into the ||| dstream n	count=1
class	using the model ||| model	count=1
arg	99], quantilescol=none aggregationdepth=2) ||| fitintercept	count=1
function	the index ||| with index	count=1
module_class	[module_1] rdd for ||| [module_1] [class_2]	count=4
arg	[arg_1] predictioncol="prediction", ||| [arg_1] [arg_2] maxdepth	count=8
function	as the specified ||| as	count=1
function	the selector type ||| selector type	count=1
module	and count ||| core	count=1
module	param with a given ||| ml	count=1
function	of the file to ||| file	count=1
class	a randomly ||| split	count=2
class	underlying output ||| stream writer	count=1
function	[function] could not ||| get [function]	count=3
class	mark the rdd ||| rdd	count=1
module	the [module] bucketlength or ||| [module]	count=1
class	a ||| spark submit tests	count=1
function	for this ||| object	count=1
function	of products [function_2] ||| [function_2] [function_1]	count=3
class	to all mixture ||| mixture model	count=1
function_arg	json [arg_2] ||| [arg_2] [function_1]	count=2
arg	[arg] dstream ||| [arg]	count=3
class	train a [class_1] [class_2] ||| [class_1] [class_2]	count=1
arg	col2 ||| col2	count=2
function	of rows [function_2] ||| [function_1] [function_2]	count=2
arg	user and [arg_2] ||| [arg_1] [arg_2]	count=1
module	[module_1] of ||| [module_1] [module_2]	count=2
function	creates a ||| create	count=4
function	into a ||| param map to	count=1
class	test ||| test	count=4
module	from the ||| mllib	count=1
function	ordered in ||| ordered	count=1
class	"predictions" which ||| regression summary	count=6
module	[module] binary ||| [module]	count=2
function	script ||| script	count=1
function	conversion ||| conversion	count=1
module_class	[module_1] rdd[vector] ||| [module_1] [class_2]	count=2
class	used again ||| query	count=1
arg	or multiclass ||| data numclasses	count=1
arg	[arg] to all ||| [arg]	count=2
function_arg	fast version ||| heappushpop heap item	count=1
module_class	value of [class_2] ||| [module_1] [class_2] discretizer get handle invalid	count=3
class	[class_1] [class_2] persists across restarts from ||| [class_1] [class_2] id	count=1
function	add a [function_2] ||| [function_1] [function_2]	count=4
class	adds ||| frame	count=1
function	instance ||| param	count=1
function	selector [function_2] ||| [function_2] [function_1]	count=3
function_arg	receiver [arg_2] ||| [function_1] error [arg_2]	count=1
class	batches of data ||| algorithm	count=1
function	this instance to a ||| to	count=2
module_class	creates [module_1] [class_2] uid and some ||| [module_1] [class_2]	count=4
class	onevsrestmodel ||| one vs rest	count=1
class	svm classifier ||| svcmodel	count=1
class	train a [class] using ||| [class] with	count=1
function	test [function_1] [function_2] with ||| [function_2] [function_1]	count=2
function	a given string ||| has param	count=1
function	[function] its ||| get [function]	count=1
function	register ||| register	count=1
class	curve which ||| binary logistic regression summary	count=2
class	instance contains a param ||| params	count=1
class	:class dataframe whose schema ||| data	count=1
class	residuals deviance pvalues ||| generalized linear regression	count=1
class	new :class dataframe ||| data frame	count=4
function	[function_1] a converter ||| [function_2] [function_1]	count=1
function	computes the area under ||| area under	count=2
function	with a dependency on ||| dependency	count=1
class	to wait ||| manager	count=1
function	worker ||| worker	count=1
function_arg	[function_1] featurescol="features", ||| [function_1] [arg_2]	count=16
function	given string ||| has param	count=1
module	to this accumulator's value ||| core	count=1
arg	wait ||| timeout catch_assertions	count=1
class	given parameters in ||| grid builder	count=1
module	[module] estimator or ||| [module]	count=1
arg	any hadoop-supported ||| path	count=2
function	k classes classification ||| classes	count=1
function	:py attr ||| value	count=1
function	count of ||| count	count=3
class	for multiclass [class_2] ||| [class_2] [class_1]	count=2
function	this ||| size	count=1
function	the test this ||| test	count=1
class	return the column ||| scaler	count=2
module_class	[module_1] the param ||| [module_1] [class_2]	count=2
function	+= operator ||| iadd	count=1
module	the [module] pattern or ||| [module]	count=1
class	dct ||| dct	count=1
module	gets the [module] windowsize ||| [module]	count=1
arg	date1 [arg_2] ||| [arg_2] [arg_1]	count=1
function	calculates the approximate quantiles ||| approx	count=1
module	given string ||| ml param	count=1
module_class	and incrementing as ||| core task context	count=1
function	[function_1] squared error ||| [function_1] [function_2]	count=7
function	minimum number ||| min count	count=2
class	:class dataframe to the ||| data frame	count=1
module	how ||| core	count=1
class	pca ||| pca	count=1
class	already ||| group by	count=2
class	[class_1] matrix this ||| [class_1] [class_2] subtract	count=1
class	matrix attributes which are ||| matrix	count=2
function	already partitioned data into ||| spill	count=1
arg	file ||| path	count=5
module	return [module] it ||| [module]	count=1
function_arg	into [arg_2] ||| [arg_2] [function_1]	count=1
arg	obj assume that ||| obj	count=1
module	the [module] minsupport ||| [module]	count=1
function	session to ||| session	count=1
module_class	returns [class_2] ||| [module_1] [class_2] select	count=1
function	brackets pairs ||| brackets split	count=1
function	rdd get [function_2] ||| [function_2] [function_1]	count=1
function_arg	[function_1] number ||| [function_1] users product [arg_2]	count=1
arg	i d samples drawn ||| numrows numcols	count=4
class	be used again to ||| manager	count=1
function	compare 2 [function_2] ||| [function_1] [function_2]	count=3
module	gets the [module] numhashtables ||| [module]	count=1
module_class	singular [class_2] ||| [module_1] singular [class_2]	count=1
arg	value of the given ||| col	count=1
class	a ||| spark submit	count=1
module	the [module] standardization or ||| [module]	count=1
function	java model from the ||| java	count=1
function	features [function_2] ||| [function_2] [function_1]	count=2
module	gets the [module] binary ||| [module]	count=2
function_arg	binary mathfunction [arg_2] ||| [function_1] [arg_2]	count=1
class	[class_1] and ||| [class_1] [class_2]	count=1
class	nodes in tree including ||| decision tree model	count=1
arg	fill the datatype ||| datatype	count=1
function	[function_1] rate for ||| [function_2] [function_1]	count=2
arg	[arg_1] [arg_2] maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 ||| [arg_1] [arg_2]	count=12
module	gets the [module] regparam ||| [module]	count=1
class	as spark executor memory ||| spark	count=1
class	impurity="gini", ||| classifier	count=2
function	list of names ||| names	count=1
arg	specified [arg_2] ||| [arg_1] [arg_2]	count=1
function	explained variance ||| explained variance	count=4
arg	datatype ||| datatype	count=1
function	register a [function_2] ||| [function_2] [function_1]	count=4
class	data into disks ||| external	count=1
function	matrix [function_2] ||| [function_2] [function_1]	count=8
module	underlying ||| sql	count=6
function	the minutes of a ||| minute	count=1
function	sets [function_2] ||| [function_1] [function_2]	count=15
class	data into ||| by	count=1
function	gets a param ||| get param	count=3
arg	featurescol="features", predictioncol="prediction", [arg_2] ||| [arg_2] [arg_1]	count=4
function	the java object ||| java	count=1
class	that ||| query	count=1
class	dump ||| external group	count=1
function	length of a string ||| length	count=1
class	linear regression model ||| linear regression	count=1
function	java ||| java	count=20
arg	some [arg] ||| [arg]	count=6
class	sets ||| classifier params	count=1
class	sets ||| validation split	count=1
function	[function_1] local representation ||| [function_2] [function_1]	count=1
class	max ||| abs scaler model	count=1
function	[function] this ||| [function]	count=1
class	leaf ||| decision	count=1
function	min value for each ||| min	count=1
function	the driver ||| driver	count=1
class	the [class_1] [class_2] persists across restarts from ||| [class_1] [class_2] id	count=1
class	registers ||| data frame	count=1
function	[function_1] group if ||| [function_1] ids for [function_2]	count=1
class	sqlcontext ||| sqlcontext	count=2
function	features ||| features col	count=1
class	each training ||| summary	count=1
function	using an [function_2] ||| [function_2] [function_1]	count=2
class	for this [class_2] ||| [class_1] [class_2]	count=2
function	[function_1] items ||| [function_2] [function_1]	count=1
class	sets ||| random forest params	count=2
class	already partitioned data into ||| external	count=1
function	java udf so it ||| java	count=1
function_arg	[function_1] file and ||| [function_1] [arg_2]	count=2
class	which is ||| logistic regression	count=1
module	of the ||| sql	count=2
function_arg	from [arg_2] ||| [function_1] without unbatching [arg_2]	count=1
module	of rows in this ||| sql	count=1
class	__init__(self ||| count vectorizer	count=1
class	linear ||| linear	count=2
module_class	[module_1] a randomly ||| [module_1] train validation [class_2] copy	count=6
class	underlying output ||| writer	count=1
arg	creates a ||| sparkcontext	count=1
module	that ||| core	count=1
arg	dictionary a ||| size	count=4
class	each training [class_2] ||| [class_2] [class_1]	count=2
arg	reference ||| java_model	count=1
module	[module_1] the ||| [module_1] [module_2]	count=6
class	the context to ||| streaming context	count=1
function	python module ||| module	count=1
class	partitioned data ||| group by	count=1
class	for each numeric columns ||| grouped	count=2
function	fields threshold [function_2] ||| [function_2] [function_1]	count=2
arg	table named table ||| table	count=1
arg	dataframe representing the result ||| sqlquery	count=2
function	[function_1] [function_2] using ||| [function_1] [function_2]	count=8
class	sets ||| output col	count=1
arg	predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", ||| predictioncol probabilitycol	count=2
function	[function_1] global temporary ||| [function_2] [function_1]	count=1
arg	using stochastic gradient descent ||| data iterations step	count=1
function	[function_1] key ||| [function_2] [function_1]	count=5
class	[class_1] classification evaluator ||| [class_2] [class_1]	count=1
arg	inputformat with ||| inputformatclass	count=2
class	set a ||| spark	count=1
function	class from an ||| rdd	count=2
function_arg	tables/views in [arg_2] ||| [function_1] [arg_2]	count=1
class	a paired rdd where ||| matrix factorization	count=1
function_arg	[function_1] string ||| [function_1] [arg_2]	count=1
function	data or create ||| get or create	count=2
function	soundex encoding for ||| soundex	count=1
arg	[arg_1] via ||| [arg_2] [arg_1]	count=8
class	disks ||| external group by	count=2
function_arg	[function_1] number of ||| [function_1] products for users [arg_2]	count=1
function	iterations ||| iterations	count=4
class	variance ||| rdd	count=1
module	the [module] binary or ||| [module]	count=2
module	adds two ||| mllib linalg	count=1
function_arg	[function_1] f-measure ||| [function_1] [arg_2]	count=1
class	this vector to ||| vector	count=1
function	stream transform [function_2] ||| [function_2] [function_1]	count=2
function	and c{other} ||| join	count=1
function	batch of [function_2] ||| [function_2] [function_1]	count=2
function	p ||| p	count=1
function	[function_1] features ||| [function_2] [function_1]	count=3
function_arg	[function_1] of another ||| [arg_2] [function_1]	count=3
function	instance contains a ||| has	count=1
function	the selector [function_2] ||| [function_1] [function_2]	count=3
function	[function_1] a boolean ||| [function_1] [function_2]	count=4
class	a new accumulator with ||| accumulator	count=1
function	stream transform get offsetranges ||| stream transform get offset ranges	count=1
class	predictions which [class_2] ||| [class_1] [class_2]	count=2
module	the [module] trainratio ||| [module]	count=1
function	users for a given ||| users	count=1
class	[class_1] query that ||| [class_2] [class_1]	count=4
function	the topics ||| topics	count=1
function	from ||| from	count=3
function	[function_1] the threshold ||| [function_2] [function_1]	count=1
module	so that :func awaitanytermination() ||| sql	count=1
class	return the ||| streaming	count=1
class	how much ||| merger	count=1
function_arg	[function_1] another dstream ||| [arg_2] [function_1]	count=3
class	that ||| query manager	count=2
arg	process after the fork() ||| sock	count=1
function_arg	[function_1] [arg_2] ||| [function_1] zip name [arg_2] ext dir	count=2
class	for ||| linear	count=1
function	global temporary ||| global temp	count=2
class	new terminations ||| streaming query	count=1
class	a function ||| function	count=1
arg	seed=none numhashtables=1) ||| seed numhashtables	count=1
module	vectors ||| mllib linalg	count=1
function	jobs has completed ||| completed	count=1
function	[function_1] text ||| [function_1] [function_2]	count=1
module	names of fields in ||| sql	count=1
function_arg	fit test [arg_2] ||| [function_1] [arg_2]	count=4
function	on first ||| on key	count=1
function	[function_1] temporary ||| [function_1] [function_2]	count=3
arg	spark sink deployed ||| addresses storagelevel maxbatchsize	count=1
class	batches of ||| streaming linear algorithm	count=1
function	create a java array ||| java array	count=1
module_class	[module_1] disks ||| [module_1] [class_2]	count=6
function	sets window [function_2] ||| [function_1] [function_2]	count=1
arg	withmean=false withstd=true ||| withmean withstd	count=2
class	0 weightcol=none [class_2] ||| [class_2] [class_1]	count=2
arg	setparams(self [arg] ||| [arg] seed	count=1
class	:class dataframe with an ||| data frame	count=1
module	elements ||| core	count=3
function_arg	[function_1] [arg_2] fitintercept=true maxiter=25 tol=1e-6 regparam=0 ||| [function_1] [arg_2]	count=1
module	[module] fwe or ||| [module]	count=1
function	json [function_2] ||| [function_2] [function_1]	count=1
function	calculates the norm ||| norm	count=3
class	the given parameters in ||| grid builder	count=1
function	each key ||| by key	count=2
arg	rawpredictioncol="rawprediction", labelcol="label", metricname="areaunderroc") ||| rawpredictioncol labelcol metricname	count=1
arg	python object ||| obj	count=2
arg	value of the ||| col	count=1
arg	[arg] inclusive) in ||| [arg]	count=1
class	train a [class] using stochastic ||| [class] with	count=1
function	returns the explained variance ||| explained variance	count=2
function	forget about past terminated ||| reset terminated	count=1
module	closure [module] ||| [module]	count=3
function	initial value of ||| set initial	count=1
function_arg	[function_1] fixed ||| [function_1] [arg_2]	count=2
function	linkpredictioncol ||| link prediction col	count=1
function	soundex encoding ||| soundex	count=1
function_arg	users for [arg_2] ||| [function_1] [arg_2]	count=2
arg	wait ||| timeout	count=2
function	features corresponding to ||| features	count=1
arg	to by codeblock ||| cls	count=1
arg	modlist ||| modlist	count=1
function_arg	param [arg_2] ||| [function_1] [arg_2]	count=1
class	gives ||| summary	count=1
class	that all the ||| merger	count=1
class	data or ||| data frame	count=1
function_arg	[function_1] a vector ||| [function_1] [arg_2]	count=1
class	block matrix this ||| block matrix	count=1
function_arg	tables/views [arg_2] ||| [function_1] [arg_2]	count=1
function	parammap into a python ||| from	count=1
function	[function_1] external ||| [function_2] [function_1]	count=1
class	:py ||| index	count=1
arg	a python topicandpartition ||| topic partition	count=1
arg	has reported an ||| receivererror	count=1
function	of value [function_2] ||| [function_2] by [function_1]	count=1
function	info ||| info	count=1
class	fitted model by ||| generalized linear regression	count=1
arg	[arg] to each ||| [arg]	count=2
class	elements in one operation ||| rdd	count=1
class	spark fair ||| spark context	count=1
arg	[arg_1] trainratio=0 75 ||| [arg_1] [arg_2]	count=1
class	much of memory for ||| merger	count=1
module	list of indices to ||| ml	count=1
module	the [module] evaluator or ||| [module]	count=1
function	the model [function_2] ||| [function_1] [function_2]	count=4
arg	[arg_1] count ||| [arg_2] [arg_1]	count=2
class	of the dataframe in ||| data frame	count=1
function	vector columns in ||| vector columns to ml	count=1
module_class	of words closest ||| ml word2vec	count=1
arg	[arg_1] labelcol="label", ||| [arg_2] [arg_1]	count=22
class	default ||| spark context	count=1
module	the [module] modeltype or ||| [module]	count=1
module	given string ||| ml	count=1
class	so that :func ||| manager	count=1
function	much ||| object	count=1
function	mean squared [function_2] ||| [function_2] [function_1]	count=2
function	blocks [function_2] ||| [function_2] [function_1]	count=2
class	vectors which this ||| vector indexer model	count=1
module	gets the [module] maxiter or ||| [module]	count=1
function	prediction on a ||| prediction	count=1
module	default implementation of ||| ml	count=1
arg	instance ||| extra	count=1
function	for k classes ||| classes	count=1
function	completed ||| completed	count=2
class	of this ||| rdd	count=1
module	gets the [module] weightcol or ||| [module]	count=1
class	this ||| rdd	count=1
function	the index of ||| partitions with index	count=1
function	summary (e ||| summary	count=1
class	udf with a function ||| function	count=1
class	data point ||| gaussian mixture	count=1
function	[function_1] an external ||| [function_1] [function_2]	count=1
module	the [module] aggregationdepth ||| [module]	count=1
function	instance to ||| to	count=2
class	a new [class] as ||| [class]	count=1
class	from ||| power iteration clustering	count=1
module	gets the [module] labelcol or ||| [module]	count=1
function	[function_1] stream foreachrdd ||| [function_1] [function_2]	count=2
class	impurity="gini", numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 ||| random forest classifier	count=1
arg	and [arg] dstream ||| [arg]	count=1
function	drops [function_2] ||| [function_2] [function_1]	count=2
class	this distributed [class_2] ||| [class_1] [class_2]	count=1
class	this instance ||| one vs	count=1
class	for this standardscaler ||| standard scaler	count=1
function	as the ||| as	count=1
arg	[arg_1] values ||| [arg_1] [arg_2]	count=2
arg	1 [arg] ||| [arg]	count=1
module	that :func ||| sql	count=1
class	feature ||| model	count=1
arg	exp(-x * [arg] + ||| [arg] npoints	count=1
function_arg	mathfunction by ||| mathfunction name doc	count=1
module	contains a ||| ml param	count=1
function	return the [function] ||| ui web [function]	count=2
class	the param [class_2] ||| [class_2] [class_1]	count=1
class	for multiclass classification evaluator ||| multiclass classification evaluator	count=1
class	spark ||| spark context	count=1
arg	of document to ||| document	count=1
class	as ||| reader	count=1
function	area under ||| area under	count=3
arg	[arg_1] labelcol="label", predictioncol="prediction", ||| [arg_1] [arg_2]	count=9
module	list of ||| sql	count=2
class	regression model with ||| regression with	count=3
function	dstream by [function] ||| [function]	count=2
class	partitioned ||| external	count=1
function	converts matrix [function_2] ||| [function_1] [function_2]	count=8
class	how ||| merger	count=1
class	as spark ||| spark	count=1
class	awaitanytermination() can be used ||| query	count=1
module_class	[module_1] columns on ||| [module_1] [class_2]	count=3
arg	featurescol="features", [arg_2] ||| params [arg_1] [arg_2]	count=2
module	to wait ||| sql	count=1
function	by [function] ||| [function]	count=2
class	carry over its keys ||| linear algorithm	count=1
arg	[arg_1] [arg_2] ||| [arg_2] [arg_1]	count=62
class	number ||| coordinate matrix	count=1
arg	expected distribution ||| expected	count=1
module	count of ||| core	count=1
class	numtrees=20 featuresubsetstrategy="auto", seed=none subsamplingrate=1 ||| random forest	count=1
module	gets the [module] maxbins or ||| [module]	count=1
function	vector conduct pearson's chi-squared ||| chi sq	count=1
class	a :class dataframe ||| data frame	count=5
arg	dataset and an ||| dataset key	count=1
class	this [class] within its ||| [class]	count=1
function	data type json string ||| parse datatype json string	count=1
function	[function_1] java udf ||| [function_2] [function_1]	count=1
module	[module] numtrees or ||| [module]	count=1
module_class	[module_1] [class_2] a column or replacing ||| [module_1] [class_2]	count=1
module	[module] frame ||| [module]	count=3
function	number [function_2] ||| [function_1] col [function_2]	count=1
class	[class] - ||| block [class]	count=1
function	min value [function_2] ||| [function_2] [function_1]	count=4
function_arg	[function_1] [arg_2] mininstancespernode=1 mininfogain=0 ||| [function_1] [arg_2] maxdepth	count=2
function	stage [function_2] ||| [function_2] [function_1]	count=1
arg	k=none [arg_2] ||| [arg_1] [arg_2]	count=1
class	model with weights ||| streaming linear regression with	count=1
class	a new [class] containing ||| [class]	count=1
function	save a ||| save	count=1
arg	condition to ||| condition	count=1
function	subset [function_2] ||| [function_1] [function_2]	count=1
arg	hostname port data ||| hostname port	count=1
class	from an rdd ||| rdd	count=1
module_class	[module_1] this instance ||| [module_1] [class_2] vs rest copy	count=1
function	params instances for ||| params	count=1
arg	table ||| table	count=2
class	__init__(self ||| classification evaluator	count=1
function	decode ||| decoder	count=1
class	gradient-boosted ||| gradient boosted	count=2
function	names of tables ||| names	count=1
function	a temporary table ||| table	count=1
arg	<http //jsonlines ||| compression dateformat	count=1
module_class	creates a [module_1] [class_2] and some ||| [module_1] [class_2] copy	count=4
class	[class] of ||| data frame [class]	count=1
module	[module] variancecol ||| [module]	count=1
module	used ||| sql	count=1
function	sort the [function_2] ||| [function_1] [function_2]	count=4
class	as [class_2] ||| [class_2] [class_1]	count=3
function	new map column ||| map	count=1
module	log of ||| ml	count=1
function	list based on first ||| based on key	count=1
module	or two ||| linalg	count=2
function	using an associative and ||| by	count=1
function	the given data type ||| datatype	count=1
function_arg	update the [arg_2] ||| [arg_2] [function_1]	count=4
class	'with sparksession builder getorcreate() [class_1] [class_2] app' syntax ||| [class_1] [class_2]	count=1
arg	api' hadoop [arg] ||| path [arg]	count=3
function	top ||| top	count=1
module	convert this ||| mllib linalg	count=2
class	model ||| linear	count=1
class	in this [class] master as ||| [class]	count=1
class	this params ||| params	count=1
arg	datasets to ||| dataseta datasetb	count=1
arg	to width [arg] ||| col [arg]	count=2
function	the threshold if ||| threshold	count=1
arg	[arg_1] predictioncol="prediction", maxiter=100 ||| [arg_2] [arg_1]	count=2
function	to array ||| to array	count=4
module	runs and profiles ||| core	count=1
function	[function_1] java object ||| [function_1] [function_2]	count=4
class	by :func ||| streaming	count=1
module	with ||| ml param	count=2
function	wait for ||| reset	count=1
class	"predictions" which gives ||| logistic regression summary	count=2
function	sparkui instance started by ||| ui web	count=1
class	external sort ||| external	count=1
function	how much ||| object size	count=1
function	foreachrdd get [function_2] ||| [function_1] [function_2]	count=1
class	test ||| stream tests	count=9
function	index ||| index	count=1
function	count of distinct ||| count	count=1
function	tokens in the training ||| training	count=1
class	batches of data ||| linear algorithm	count=1
function	[function_1] on the ||| [function_2] [function_1]	count=3
class	the ensemble ||| tree ensemble model	count=2
function	to [function_2] ||| [function_2] [function_1]	count=19
module	of parameters specified ||| ml	count=1
module	[module] itemcol ||| [module]	count=1
function_arg	[function_1] default 100 ||| [function_1] [arg_2]	count=3
class	that :func ||| query manager	count=1
function	list based on ||| based on key	count=2
function	an ||| rdd	count=2
class	return the column standard ||| standard scaler model	count=1
module	[module] finalstoragelevel ||| [module]	count=1
function	converts vector columns ||| convert vector columns from ml	count=1
class	params shared by them ||| params	count=1
class	the ||| scaler	count=1
function_arg	[function_1] [arg_2] ||| [function_1] started [arg_2]	count=4
function	transform [function_2] ||| [function_1] [function_2]	count=2
module_class	value [class_2] ||| [module_1] [class_2]	count=2
class	all the ||| external	count=1
arg	outputformat ||| outputformatclass	count=1
function	[function_1] script on ||| [function_1] [function_2]	count=2
function	[function_1] dataframe from ||| [function_1] [function_2]	count=1
module	an exception ||| mllib linalg	count=1
function	of specific ||| offset	count=1
class	rdd to ||| rdd	count=1
function	[function_1] get ||| [function_1] [function_2]	count=10
class	sparkcontext at ||| spark context	count=1
function	instance contains a param ||| param	count=1
class	this [class] master as ||| [class]	count=1
function	seconds [function] unix ||| [function]	count=1
function	for each original ||| original	count=1
arg	occurrence of substr ||| substr str	count=1
class	queries so that ||| manager	count=1
class	a :class dataframereader ||| spark session	count=2
module_class	gets [module_1] [class_2] handleinvalid or its default ||| [module_1] [class_2]	count=1
function	creates a global ||| create global	count=3
function	of all active stages ||| get active	count=1
class	for new terminations ||| streaming query manager	count=1
class	instance ||| params	count=1
function	underlying [function] ||| [function]	count=1
function	[function_1] on ||| [function_2] [function_1]	count=14
module	be used again to ||| sql	count=1
class	:class dataframe to a ||| frame writer	count=1
class	new ||| streaming query manager	count=2
module	vector ||| mllib linalg	count=1
function	ndcg value of ||| ndcg	count=1
function	the trigger for ||| trigger	count=1
class	returns a paired rdd ||| factorization	count=1
function	single [function_2] ||| [function_2] [function_1]	count=5
function	to a mllib vector ||| to vector	count=1
class	of memory for this ||| merger	count=1
function	number of ||| num	count=17
function	is ||| is distributed	count=1
function	features corresponding to that ||| features	count=1
function_arg	update the [arg_2] ||| [function_1] data [arg_2]	count=2
function	approximate [function_2] ||| [function_2] [function_1]	count=4
class	to this params ||| params	count=1
module_class	of words ||| ml word2vec	count=1
function	much of memory ||| size	count=1
module_class	creates [module_1] [class_2] ||| [module_1] [class_2] copy	count=6
class	a randomly generated ||| cross validator	count=1
arg	used ||| dataset	count=1
module	[module] labelcol ||| [module]	count=1
module	the [module] evaluator ||| [module]	count=1
class	scores into 0/1 predictions ||| linear classification model	count=1
function	key value pairs ||| key	count=1
module_class	[module_1] [class_2] handleinvalid or its default ||| [module_1] [class_2]	count=1
class	instance with a randomly ||| cross validator	count=1
function	withstd ||| with std	count=1
module_class	[module_1] [class_2] ||| [module_1] singular [class_2]	count=1
class	which is ||| linear regression summary	count=2
function	[function_1] py ||| [function_1] [function_2]	count=1
module	of the list ||| sql	count=2
module	param with a ||| param	count=1
function	summary [function_2] ||| [function_2] [function_1]	count=1
function	get all values ||| get all	count=1
class	estimated coefficients and ||| linear regression summary	count=1
class	[class_1] trained ||| [class_2] [class_1]	count=16
function	create a new ||| create	count=1
function	table in the catalog ||| table	count=1
function	by other, ||| multiply	count=1
class	right singular vectors ||| singular	count=1
arg	a data source ||| source	count=2
class	on the ||| with	count=1
function	index of ||| partitions with index	count=1
function	"zerovalue" which may be ||| fold by	count=1
class	in this [class_2] ||| [class_1] [class_2] remember duration	count=2
module_class	number [module_1] [class_2] ||| [module_1] [class_2] vocab	count=4
class	write() ||| pipeline	count=2
module	of indices and ||| ml	count=2
class	bisecting k-means ||| bisecting kmeans	count=2
arg	of length len ||| len	count=1
arg	is of length len ||| len	count=1
arg	[arg_1] inputcol=none ||| [arg_1] [arg_2]	count=1
function_arg	[function_1] the chisqselector ||| [arg_2] [function_1]	count=4
function	sql [function_2] ||| [function_2] [function_1]	count=2
function	by the given ||| by	count=1
module	as ||| sql	count=1
class	how ||| external merger	count=2
arg	in matching ||| matching	count=1
module	given value to scale ||| sql	count=1
arg	rdd with [arg] one ||| [arg]	count=1
function	[function_1] [function_2] ||| [function_1] params to [function_2]	count=1
function	[function_1] py or ||| [function_2] [function_1]	count=1
function	[function_1] text ||| [function_2] [function_1]	count=1
function	with [function_2] ||| [function_2] [function_1]	count=2
function_arg	the group [arg_2] ||| [arg_2] [function_1]	count=1
class	can be ||| streaming	count=1
class	[class_1] output ||| [class_2] [class_1]	count=1
function_arg	[function_1] input stream ||| [function_1] without unbatching [arg_2]	count=1
class	return a new rdd ||| rdd	count=1
function	python parammap into ||| to	count=1
function	note : ||| count approx	count=1
function	setparams(self ||| set params	count=19
arg	the first n elements ||| n	count=1
function	used again ||| reset	count=1
module	:func ||| sql	count=1
module	the [module] numhashtables ||| [module]	count=1
function_arg	[function_1] [arg_2] [[structtype]] or [[arraytype]] of ||| [function_1] [arg_2]	count=1
arg	to the specified table ||| tablename overwrite	count=1
function	[function_1] of partitions ||| [function_2] [function_1]	count=3
function	extract ||| regexp extract	count=1
module	gets the [module] droplast or ||| [module]	count=1
class	queue ||| streaming context	count=1
function	columns ||| columns from ml	count=4
function	already partitioned ||| spill	count=1
function	for all users the ||| for users	count=1
function	outer [function_2] ||| [function_1] [function_2]	count=6
module	contains a ||| ml	count=1
class	new accumulator ||| accumulator	count=1
function	coefs are predicted accurately ||| parameter accuracy	count=1
module	an exception ||| mllib	count=1
module	expected value of ||| ml	count=1
module_class	[module_1] estimated ||| [module_1] [class_2] coefficient standard errors	count=1
class	which is defined ||| regression	count=1
function	memory ||| object size	count=1
function	how much of memory ||| size	count=1
module	defined on the class ||| ml param	count=1
module	the [module] fwe or ||| [module]	count=1
class	seed=none impurity="variance") ||| gbtregressor	count=1
function	vector columns in ||| vector columns to	count=1
module	[module] vectorsize ||| [module]	count=1
function	by ||| by key	count=2
module	every ||| mllib	count=1
arg	featurescol="features", labelcol="label", predictioncol="prediction", maxiter=100 ||| featurescol labelcol predictioncol maxiter	count=1
function	[function_1] completed ||| [function_2] [function_1]	count=1
class	the singularvaluedecomposition ||| value decomposition	count=1
class	of this [class_2] ||| [class_1] [class_2] id	count=1
function_arg	[function_1] specified database ||| [function_1] [arg_2]	count=5
class	values ||| scaler model	count=1
function	for all ||| for	count=1
class	model trained on the ||| model	count=1
module	rows in ||| sql	count=2
arg	elements in iterator ||| iterator key	count=1
class	[class_1] test ||| [class_2] [class_1]	count=2
module	returns true if ||| sql	count=1
arg	[arg_1] numfolds=3 seed=none): ||| [arg_2] [arg_1]	count=2
class	"predictions" which gives the ||| logistic regression summary	count=2
class	again to wait for ||| query	count=1
module	with ||| ml	count=1
class	[class_1] score ||| [class_1] [class_2]	count=1
class	[class] stepsize=0 ||| [class]	count=6
class	:py attr lda ||| ldamodel	count=1
function	main ||| main	count=1
module	[module] itemcol or ||| [module]	count=1
function	list ||| list	count=4
function	[function_1] foreachrdd ||| [function_2] [function_1]	count=3
function	separators inside brackets pairs ||| brackets	count=1
class	wait ||| manager	count=1
module	and ||| core	count=16
class	threshold=0 0 weightcol=none ||| linear	count=2
function	transform [function_2] ||| [function_2] [function_1]	count=2
function	find [function_2] ||| [function_1] [function_2]	count=1
function	transforms the ||| transform	count=1
class	logistic regression ||| logistic regression	count=7
function	[function_1] of tables/views ||| [function_1] [function_2]	count=1
function	value to [function_2] ||| [function_2] [function_1]	count=4
function	[function_1] top ||| [function_2] [function_1]	count=1
arg	fixed ||| param	count=1
arg	the spark sink deployed ||| addresses storagelevel maxbatchsize	count=1
function	limits the ||| limit	count=1
class	group [class_2] ||| [class_1] [class_2]	count=1
arg	0 inputcol=none outputcol=none) ||| inputcol outputcol	count=1
function	a param ||| param	count=2
module	representation [module_2] ||| [module_2] [module_1] sparse matrix repr	count=1
arg	1 [arg] inclusive) ||| [arg]	count=1
module_class	of parameters [class_2] ||| [module_1] [class_2]	count=4
arg	featurescol="features", predictioncol="prediction", k=2 ||| featurescol predictioncol k	count=4
function_arg	[function_1] content and ||| [arg_2] [function_1]	count=1
function	driver returns none ||| driver	count=1
module	[module] scalingvec ||| [module]	count=1
arg	which is later than ||| dayofweek	count=1
function	based on first value ||| based on key	count=1
function	thread until the group ||| group	count=1
function	an rdd of labeledpoint ||| lib svmfile	count=1
class	right singular vectors of ||| singular	count=1
module	into a ||| core	count=1
arg	calculates the correlation ||| col2 method	count=1
arg	which the given date ||| date	count=1
class	queries so that :func ||| streaming query manager	count=1
function	from checkpoint data or ||| get or	count=1
arg	function to the ||| f	count=2
module	returns the documentation of ||| ml param	count=1
function	[function_1] json ||| [function_2] [function_1]	count=3
function	ids [function_2] ||| [function_2] [function_1]	count=4
function	queries so that ||| reset	count=1
function_arg	[function_1] by ||| [function_1] [arg_2]	count=2
function_arg	receiver has [arg_2] ||| [function_1] error [arg_2]	count=1
arg	multiclass ||| cls data numclasses	count=1
module_class	statistic [class_2] ||| [module_1] [class_2]	count=1
function	set ||| set	count=13
arg	applying a function to ||| f	count=3
function	value to a boolean ||| to boolean	count=1
arg	labelcol="label", featurescol="features", ||| labelcol featurescol	count=4
function	summary of ||| repr	count=2
function	how much of ||| object	count=1
arg	every module in modlist ||| modlist	count=1
function	how much ||| object	count=1
function	[function_1] rdd ||| [function_1] [function_2]	count=2
class	this [class_2] ||| [class_1] [class_2] id	count=1
module	the mean variance and ||| core	count=1
class	for ||| linear model	count=1
function	[function_1] main ||| [function_1] [function_2]	count=1
class	the vector ||| vector	count=1
function	of blocks ||| blocks	count=2
arg	representing the result of ||| sqlquery	count=1
function	stream transform [function_2] ||| [function_1] [function_2]	count=2
class	saves ||| writer	count=1
class	awaitanytermination() can ||| streaming query	count=1
module_class	and [class_2] ||| [module_1] [class_2]	count=10
function	[function_1] default value ||| [function_2] [function_1]	count=1
module	problem in multinomial ||| mllib	count=1
arg	the optional key function ||| key	count=1
function	drops the [function_2] ||| [function_1] [function_2]	count=2
arg	data sampled from ||| data	count=1
class	underlying output ||| data frame writer	count=1
function	the [function] ||| reduce by key [function]	count=3
function	add two ||| add	count=1
class	return ||| rdd	count=1
class	for params shared ||| params	count=1
class	bayes ||| bayes	count=1
module	[module] featurescol or ||| [module]	count=1
class	dump already partitioned data ||| by	count=1
function	an external ||| external	count=1
function	the given join expression ||| join	count=1
function	first ||| head	count=1
function_arg	csv [arg_2] ||| [function_1] [arg_2]	count=3
module	given data type string ||| sql	count=1
module	[module] names or ||| [module]	count=1
class	this accumulator's ||| accumulator	count=1
function_arg	synonyms of [arg_2] ||| [arg_2] [function_1]	count=1
function	block ||| block	count=1
function	2 ml params ||| params	count=1
class	[class_1] [class_2] uid and some ||| [class_2] [class_1] copy	count=8
function	the average [function] ||| [function]	count=6
class	of parallelism [class] ||| [class]	count=1
class	[class_1] :class dataframe ||| [class_2] [class_1]	count=7
class	specifies the input ||| reader	count=1
function	columns in an ||| columns from	count=2
arg	string str ||| str	count=1
class	data into disks ||| by	count=1
function	mllib vector ||| vector	count=1
function	this instance contains ||| has	count=1
class	again to wait for ||| streaming query manager	count=1
function	[function_1] nodes ||| [function_2] [function_1]	count=1
function	variance ||| variance	count=2
arg	an external database table ||| table mode properties	count=1
class	[class_1] score ||| [class_2] [class_1]	count=1
function	add a ||| add	count=2
function	aggregate the values ||| aggregate	count=1
function	default ||| default	count=2
class	the context ||| streaming context	count=1
function	python direct kafka ||| kafka direct	count=5
function	initialized or not ||| initialized	count=1
arg	[arg_1] [arg_2] 0 tol=1e-6 fitintercept=true threshold=0 ||| [arg_1] [arg_2]	count=2
class	dataframe in a ||| data frame	count=1
function	generates [function] ||| poisson [function]	count=1
function	recent ||| recent	count=1
module	the [module] labelcol ||| [module]	count=1
function	returns the schema of ||| schema	count=1
class	parameters in this grid ||| param grid builder	count=1
function	returns the [function] ||| [function]	count=1
class	queries so that :func ||| query	count=1
function_arg	[function_1] [arg_2] ||| [function_1] data frame data [arg_2]	count=1
function	[function_1] to a ||| [function_2] [function_1]	count=2
function	termination of this query ||| termination	count=1
function	matrix columns in an ||| matrix columns from	count=1
function	a param with a ||| param	count=1
class	[class] on the ||| bisecting kmeans [class]	count=3
function	script on a ||| script on	count=2
arg	while tracking the ||| preservespartitioning	count=1
class	in this model ||| kmeans model	count=2
arg	user [arg_2] ||| [arg_2] [arg_1]	count=1
module_class	[module_1] computation ||| [module_1] [class_2]	count=2
module	of this instance ||| ml	count=7
class	values for each numeric ||| grouped data	count=1
function	matrix [function_2] ||| [function_1] [function_2]	count=8
function_arg	save a [arg_2] ||| [function_1] [arg_2]	count=1
function_arg	setparams(self [arg_2] ||| [function_1] [arg_2] maxiter	count=6
arg	the format ||| format	count=1
function	'u' [function] ||| ignore unicode [function]	count=3
function	selector type of ||| selector type	count=1
function	of names ||| names	count=1
function	grid ||| grid	count=1
class	be ||| query	count=1
class	layers ||| multilayer perceptron classification	count=1
arg	calculates the correlation of ||| method	count=1
function	partitioned data into disks ||| spill	count=1
function	the explained variance ||| explained variance	count=4
arg	correlation of ||| method	count=1
function	transform get offsetranges ||| transform get offset ranges	count=1
arg	position pos ||| str pos	count=1
function	active ||| active	count=1
module	the documentation of ||| ml param	count=1
class	the stream ||| stream writer	count=1
class	be found [class] was garbage ||| [class]	count=1
function_arg	distance [arg_2] ||| [function_1] [arg_2]	count=1
function	arbitrary [function] ||| [function]	count=3
class	that :func awaitanytermination() ||| streaming query	count=1
function	labeledpoint [function_2] ||| [function_1] [function_2]	count=5
class	a list ||| spark	count=1
function	weighted averaged recall ||| weighted recall	count=2
function_arg	[function_1] parammap ||| [arg_2] [function_1]	count=6
function	area [function_2] ||| [function_1] [function_2]	count=2
arg	inputformat with arbitrary ||| inputformatclass	count=2
function	drops the global temporary ||| drop global temp	count=1
class	latest ||| streaming kmeans	count=1
function	of ||| of	count=1
function	converts matrix columns in ||| convert matrix columns to	count=1
class	0 modeltype="multinomial", thresholds=none weightcol=none) ||| naive bayes	count=1
module	much of memory ||| core	count=1
module	gets the [module] outputcol or ||| [module]	count=1
arg	name ||| name	count=6
function	total [function_2] ||| [function_1] [function_2]	count=1
function	minimum number of ||| min count	count=2
arg	[arg] maxdepth=5 maxbins=32 ||| [arg]	count=4
function	commutative reduce ||| reduce by	count=1
module	return a copy ||| core	count=1
function	to get [function_2] ||| [function_2] [function_1]	count=1
class	:func awaitanytermination() ||| streaming	count=1
class	comprised of ||| random rdds	count=9
function	sparsematrix ||| sparse	count=1
function	string name ||| param	count=1
function	precision of all the ||| precision	count=1
function	the index of the ||| with index	count=1
module	contains a param ||| ml param	count=1
function_arg	[function_1] incoming dstream ||| [function_1] [arg_2]	count=6
function	parses the ||| expr	count=1
class	means 1 leaf ||| decision	count=1
function_arg	the maximum item ||| max key	count=1
class	for this ||| other	count=1
function	script on a cluster ||| script on cluster	count=1
function	outer ||| outer	count=2
module	param ||| param	count=2
class	spark fair scheduler pool ||| spark context	count=1
function	:class column for approximate ||| approx	count=1
arg	col1 ||| col1	count=2
class	elements from an rdd ||| rdd	count=1
function_arg	months between [arg_2] ||| [arg_2] [function_1]	count=2
class	the model ||| streaming logistic regression	count=1
arg	the specified path ||| path	count=3
class	cachenodeids=false checkpointinterval=10 impurity="gini", numtrees=20 ||| random forest classifier	count=1
function	into a ||| map to	count=1
module_class	[module_1] :class dataframe ||| [module_1] [class_2]	count=3
function	number of [function_2] ||| [function_1] col [function_2]	count=1
class	accumulator's value only usable ||| accumulator	count=1
module	of numpy ||| ml	count=1
function	into a [function_2] ||| [function_1] [function_2]	count=1
arg	statements ||| f returntype	count=1
function	[function] on ||| test package [function] on	count=3
function_arg	of users [arg_2] ||| [arg_2] [function_1]	count=1
class	wait for ||| query manager	count=1
arg	splits=none [arg_2] ||| [arg_2] [arg_1]	count=2
module_class	number [module_1] [class_2] ||| [module_1] [class_2]	count=4
function	learningdecay ||| learning decay	count=1
module	is an ||| mllib	count=1
class	new spark configuration ||| spark conf	count=2
function	generates python code ||| code	count=1
arg	the given user ||| user	count=1
arg	in iterator ||| iterator key reverse	count=1
class	so ||| streaming query	count=1
function_arg	[function_1] "num" number ||| [function_1] users product [arg_2]	count=1
class	the left singular ||| singular	count=1
function	sparkcontext which is ||| spark	count=1
function	ids of all ||| stage ids	count=1
module	[module] mintf ||| [module]	count=1
arg	version of ||| heap item	count=1
class	:func awaitanytermination() can be ||| streaming query manager	count=1
function	the number of rows ||| num rows	count=3
arg	to [arg] ||| [arg]	count=2
function	as a [function_2] ||| [function_1] [function_2]	count=4
module	skipping ||| sql	count=2
class	a ||| accumulator	count=1
arg	[arg_1] [arg_2] 0 maxmemoryinmb=256 cachenodeids=false subsamplingrate=1 ||| [arg_1] [arg_2]	count=2
class	using the model trained ||| logistic regression model	count=1
class	params [class] ||| [class]	count=1
class	k-means ||| kmeans	count=1
module	the [module] metricname or ||| [module]	count=2
module_class	the underlying data ||| sql data	count=1
arg	the centroids ||| timeunit	count=2
module	runs and ||| core	count=1
class	for this pca ||| pca	count=1
class	optional ||| params	count=1
function	mininstancespernode ||| min instances per node	count=1
function	[function_1] number of ||| [function_1] [function_2]	count=2
arg	hadoop-supported ||| path	count=2
function_arg	[function_1] [arg_2] fitintercept=true maxiter=25 tol=1e-6 regparam=0 ||| [function_1] [arg_2] family	count=1
module_class	[module_1] :py attr ||| [module_1] [class_2]	count=31
arg	[arg_1] labelcol="label", ||| [arg_1] [arg_2]	count=22
class	much of ||| external	count=1
arg	format at the ||| compression	count=2
module	separate arrays of ||| ml linalg	count=1
function	receive accumulator updates ||| update	count=1
class	accumulator's value only ||| accumulator	count=1
class	[class_1] singularvaluedecomposition ||| [class_2] [class_1]	count=2
function	the uid ||| reset uid	count=1
function	set a [function_2] ||| [function_1] [function_2]	count=2
function	set the selector type ||| set selector type	count=1
class	data into ||| external group	count=1
class	instance with a randomly ||| split model	count=1
module	list of ||| ml	count=3
class	feature ||| linear	count=1
class	stream [class_2] ||| [class_1] [class_2]	count=5
function	predict the label ||| predict	count=1
arg	inputcol=none outputcol=none labels=none) ||| inputcol outputcol labels	count=4
function	the [function] pairs ||| collect as [function]	count=1
function_arg	java [arg_2] ||| [function_1] [arg_2]	count=1
function	month of a given ||| dayofmonth	count=1
function	precision of ||| precision	count=1
function	converts matrix columns in ||| convert matrix columns to ml	count=1
class	dump already partitioned ||| group	count=1
module	gets the [module] featureindex ||| [module]	count=1
class	[class] using stochastic ||| [class] with	count=1
class	the mean variance ||| rdd	count=1
class	accumulator's data type ||| accumulator param	count=1
class	impurity="gini", numtrees=20 ||| random forest classifier	count=4
class	[class] remember ||| [class]	count=2
function	columns ||| columns to ml	count=4
function	sql ||| sql	count=1
module	[module_1] of ||| [module_2] [module_1]	count=1
class	model ||| tree ensemble model	count=1
class	validator ||| validator	count=1
function	[function_1] java ||| [function_2] [function_1]	count=14
function	approximate quantiles ||| approx	count=1
module	two separate arrays of ||| ml	count=1
function	for new terminations ||| reset	count=1
function	of products for all ||| products for	count=1
arg	used in sql statements ||| f returntype	count=1
class	sparkcontext ||| streaming context	count=1
module	that all ||| core	count=1
function	the initial ||| set initial	count=1
class	that :func ||| query	count=1
function	[function_1] threshold ||| [function_1] [function_2]	count=1
function	[function_1] replaces a ||| [function_2] [function_1]	count=2
arg	python topicandpartition ||| topic partition	count=1
module_class	and returns [class_2] ||| [module_1] [class_2] select	count=1
arg	[arg] of this ||| f [arg]	count=2
function	mindocfreq ||| min doc freq	count=1
function	the model [function_2] ||| mllib streaming kmeans test [function_2] [function_1]	count=1
class	from the input ||| estimator	count=1
arg	for a given product ||| product	count=1
arg	string column to width [arg_1] [arg_2] ||| [arg_1] [arg_2]	count=2
function	in c{self} and c{other} ||| join	count=1
arg	the input dataset ||| dataset	count=4
function	that all the objects ||| size	count=1
function	selector type ||| selector type	count=1
class	randomly [class_2] ||| [class_2] [class_1]	count=2
function_arg	setparams(self [arg_2] ||| [function_1] [arg_2] checkpointinterval	count=1
function	replaces ||| replace	count=1
arg	[arg_1] predictioncol="prediction", ||| [arg_1] [arg_2] family	count=4
module_class	gets [module_1] [class_2] its default value ||| [module_1] [class_2]	count=1
function	threshold if any used ||| threshold	count=1
arg	[arg_1] labelcol="label", predictioncol="prediction", ||| [arg_1] [arg_2] maxdepth	count=4
function	distinct count of col ||| count distinct	count=2
function	brackets pairs e ||| brackets	count=1
arg	column ||| col	count=6
function	[function_1] place ||| [function_2] [function_1]	count=1
function	return the [function] pairs in ||| collect as [function]	count=1
arg	[arg_1] inputcol=none outputcol=none) ||| [arg_2] [arg_1]	count=9
class	recommends ||| factorization model	count=2
module	how much of memory ||| core	count=1
function	set the trigger for ||| trigger	count=1
function	module ||| module	count=1
class	a randomly [class_2] ||| [class_2] [class_1] copy	count=2
function	[function_1] recall ||| [function_2] by [function_1]	count=4
module	compute ||| core	count=1
module	instance contains a param ||| ml param	count=1
arg	the expected ||| expected	count=1
function	the ids ||| stage ids	count=1
arg	pos ||| pos	count=1
module	returns true ||| sql	count=1
module	this accumulator's ||| core	count=1
function	the global [function_2] ||| [function_2] [function_1]	count=1
arg	at pos ||| pos	count=1
function_arg	a receiver [arg_2] ||| [arg_2] [function_1]	count=2
class	[class_1] with weights ||| [class_2] [class_1]	count=3
module	in ||| streaming	count=1
function	handleinvalid ||| handle invalid	count=3
function	all the jobs ||| job	count=1
arg	based ||| path	count=1
class	randomly generated uid ||| cross validator model	count=1
arg	to data ||| data	count=1
function_arg	converts a [arg_2] ||| [arg_2] [function_1]	count=1
class	mean ||| standard scaler model	count=1
function_arg	[function_1] "num" number ||| [arg_2] [function_1]	count=2
class	predictions which [class_2] ||| [class_2] [class_1]	count=2
class	broadcast on the ||| broadcast	count=1
class	how data of a ||| data	count=1
function	relativeerror ||| relative error	count=1
function	the area under the ||| area under	count=2
function_arg	[function_1] [arg_2] ||| [function_1] error [arg_2]	count=4
arg	iterator ||| iterator key reverse	count=1
function	vector columns in an ||| vector columns	count=2
arg	n rows to ||| n truncate	count=1
function_arg	keyed [arg_2] ||| [function_1] [arg_2]	count=1
module_class	creates a [module_1] [class_2] some ||| [module_1] [class_2] copy	count=4
class	defines ||| window spec	count=2
function	:func ||| reset	count=1
class	python rdd ||| rdd	count=4
function	[function_1] into ||| [function_2] [function_1]	count=4
arg	[arg_1] inputcol=none ||| [arg_2] [arg_1]	count=1
class	used again to ||| query manager	count=1
arg	specified schema ||| schema	count=1
function	parquet files returning the ||| parquet	count=1
function	[function_1] for all ||| [function_1] [function_2]	count=2
class	write() ||| pipeline model	count=1
class	disks ||| external	count=2
class	defines ||| spec	count=1
class	[class_1] validator ||| [class_2] [class_1]	count=2
function	test of ||| test	count=1
function	given ||| has	count=1
class	this instance with ||| one	count=2
function	global [function_2] ||| [function_1] [function_2]	count=1
function	with the frame boundaries ||| range	count=1
function	again to ||| reset	count=1
function	predicted ||| prediction col	count=2
function_arg	[function_1] a word ||| [function_1] [arg_2]	count=2
class	logistic [class_2] ||| [class_1] [class_2]	count=7
function	this instance contains a ||| has	count=1
arg	property ||| defaultvalue	count=1
function	find synonyms of ||| find synonyms	count=1
class	queries ||| streaming query	count=1
class	null ||| generalized linear regression summary	count=1
function	each [function_2] ||| [function_2] [function_1]	count=3
function	'left ||| left	count=1
function	deviation ||| std	count=1
module	of the rdd's ||| core	count=1
function	sort [function_2] ||| [function_2] [function_1]	count=4
function	can be ||| reset	count=1
function_arg	a parquet file ||| parquet path	count=1
function	using the old hadoop ||| as hadoop	count=1
function	to their vector representations ||| get vectors	count=1
arg	value with another value ||| value	count=1
class	flume ||| flume	count=2
arg	inputformat ||| inputformatclass keyclass	count=2
class	how much of ||| merger	count=1
function	of columns of blocks ||| blocks	count=1
class	of the :class dataframe ||| frame writer	count=1
arg	expected ||| expected	count=1
arg	or list in ||| oneatatime	count=1
function	[function_1] rdd get ||| [function_1] [function_2]	count=1
function	vector columns in ||| vector columns from	count=1
function	training set given ||| training	count=1
arg	[arg_1] co ||| [arg_2] [arg_1]	count=3
arg	output with optional parameters ||| params	count=1
class	this distributed [class_2] ||| [class_2] [class_1]	count=1
arg	inputcol=none [arg_2] ||| [arg_2] [arg_1]	count=4
module_class	gets the [module_1] [class_2] handleinvalid or its default ||| [module_1] [class_2]	count=1
arg	sqltype() into class ||| cls	count=1
class	into the returned ||| py spark streaming test case	count=1
class	[class_1] returning ||| [class_2] [class_1]	count=1
arg	the specified path ||| path mode	count=1
module	of column names skipping ||| sql	count=2
class	params shared ||| params	count=1
module_class	of binomial logistic ||| ml logistic	count=4
class	impurity="variance", seed=none ||| regressor	count=1
class	this instance with a ||| one	count=2
function	features ||| features	count=3
class	this model instance ||| model	count=4
class	of ||| external	count=1
function_arg	parquet [arg_2] ||| [function_1] [arg_2]	count=2
class	field in "predictions" which ||| regression summary	count=3
function	that ||| size	count=1
function	[function_1] of top ||| [function_1] [function_2]	count=1
arg	sets the spark ||| sparksession	count=1
class	to wait for new ||| query	count=1
module	and return [module] it ||| [module]	count=1
function	including lambda function ||| function	count=2
class	the :class dataframe in ||| data frame writer	count=2
module	awaitanytermination() can be ||| sql	count=1
arg	value [arg_2] ||| [arg_1] [arg_2]	count=1
module_class	gets [module_1] [class_2] ||| [module_1] [class_2]	count=8
function	compare 2 ml types ||| compare	count=1
arg	[arg_1] specified path ||| [arg_2] mode partitionby [arg_1]	count=2
function	the termination ||| termination	count=1
function	compare 2 [function_2] ||| [function_2] [function_1]	count=3
arg	outputformat api mapred package ||| outputformatclass keyclass	count=1
module	[module] the label ||| [module]	count=1
function	test ||| test	count=4
function	with a ||| param	count=1
function	need [function] between ||| need [function]	count=1
class	onevsrest create and ||| one vs rest	count=1
function	that all the ||| object	count=1
class	every feature ||| model	count=1
module	the expected value of ||| ml	count=1
arg	stopwordremover ||| stopwords casesensitive	count=1
class	bisecting [class_2] ||| [class_1] [class_2]	count=2
function	that ||| reset	count=1
class	first ||| data frame	count=1
function	stream foreachrdd [function_2] ||| [function_1] [function_2]	count=2
module_class	creates a copy [module_1] [class_2] ||| [module_1] [class_2]	count=31
arg	equal to ||| numiterations	count=1
function	converts matrix ||| convert matrix	count=4
class	checkpointinterval=10 impurity="gini", ||| classifier	count=1
function	model intercept ||| intercept	count=1
class	data source ||| data frame	count=2
function	directory that contains ||| directory	count=1
arg	[arg_1] every element ||| [arg_1] [arg_2]	count=6
class	forest ||| forest	count=1
class	objective ||| regression training summary	count=2
function	cachenodeids ||| cache node ids	count=1
class	sets ||| validator params	count=1
module	accumulator's value ||| core	count=1
class	for the stream query ||| data stream	count=1
class	in ||| grid builder	count=1
function	or replaces a ||| or replace	count=2
class	write() ||| one vs rest	count=1
class	which is ||| regression summary	count=2
class	partitioned data into ||| external	count=1
arg	featurescol="features", predictioncol="prediction", ||| featurescol predictioncol	count=5
class	squared error which is ||| linear regression summary	count=1
class	input data ||| data stream reader	count=2
module	copy [module] ||| [module]	count=18
function	returns a java ||| java	count=1
arg	saves the contents ||| path format mode partitionby	count=1
class	in :py attr predictions ||| linear	count=1
arg	for binary or multiclass ||| data numclasses	count=1
module	[module] quantileprobabilities ||| [module]	count=1
function	the old hadoop ||| as hadoop	count=1
arg	inputformat with arbitrary ||| inputformatclass keyclass	count=2
function	[function_1] info ||| [function_1] [function_2] stageid	count=4
class	data or ||| data	count=1
arg	the correlation ||| col1 col2 method	count=1
module	of this ||| ml param	count=2
class	returned ||| streaming test case	count=1
class	for this [class] within its ||| [class]	count=1
arg	each ||| func	count=1
class	norm ||| dense vector	count=1
arg	mindocfreq=0 [arg_2] ||| [arg_1] [arg_2]	count=1
function_arg	group id ||| group groupid	count=1
function	of blocks in ||| col blocks	count=2
class	again to wait ||| streaming query	count=1
function	create a [function_2] ||| [function_1] file in [function_2] name	count=1
function	hook [function] ||| install [function]	count=2
arg	n rows to the ||| n truncate vertical	count=1
function_arg	group [arg_2] ||| [function_1] [arg_2]	count=1
class	the sparsevector ||| sparse vector	count=1
module	two separate arrays of ||| ml linalg	count=1
function	set to a ||| set	count=1
arg	at the ||| compression	count=1
function	by the given ||| bucket by	count=1
arg	external database table via ||| url table mode properties	count=1
arg	strategy="mean", missingvalue=float("nan"), inputcols=none outputcols=none): ||| strategy missingvalue inputcols outputcols	count=1
arg	[arg] number ||| [arg]	count=3
class	for the stream query ||| data stream writer	count=1
arg	java_class ||| java_class	count=1
class	specifies the input data ||| data stream reader	count=1
function	sparkcontext which ||| spark	count=1
arg	n ||| n	count=6
class	onevsrestmodel create and ||| one vs rest	count=1
class	an input ||| stream	count=1
function	forget about past terminated ||| terminated	count=1
arg	or cleared ||| description interruptoncancel	count=1
function	grid to ||| grid	count=1
class	the accumulator's value only ||| accumulator	count=1
function	which each rdd ||| by	count=1
class	convert this distributed model ||| distributed ldamodel	count=1
function_arg	note : [arg_2] ||| [function_1] [arg_2]	count=1
class	getorcreate() [class] ||| [class]	count=1
class	that all the objects ||| merger	count=1
class	the ensemble ||| ensemble	count=2
class	and count of the ||| rdd	count=1
arg	setparams(self featurescol="features", predictioncol="prediction", ||| params featurescol predictioncol	count=1
module	contains a param with ||| param	count=1
class	creates ||| spark session	count=1
module	gets the [module] numtrees ||| [module]	count=1
function	for approximate [function_2] ||| [function_1] count [function_2]	count=1
class	either by :func ||| streaming	count=1
function	[function_1] number ||| [function_1] [function_2]	count=3
function	[function_1] py or ||| [function_1] [function_2]	count=1
function_arg	[function_1] labelcol="label", ||| [arg_2] [function_1]	count=1
function	numfolds ||| num folds	count=1
function	a string column ||| string	count=1
class	memory ||| external merger	count=2
function	[function_1] weights ||| [function_1] [function_2]	count=2
module_class	a byte ||| core framed serializer	count=1
class	a spark ||| spark	count=2
function	for the test this ||| test	count=1
class	original ||| min max scaler model	count=1
function	number ||| num	count=21
function	save ||| save	count=9
class	builder getorcreate() [class] app' ||| [class]	count=1
module	gets the [module] fdr ||| [module]	count=1
function	outer join of c{self} ||| outer join	count=2
function	'new api' hadoop ||| new apihadoop	count=1
class	model by ||| linear	count=1
function	the spark_home ||| spark home	count=1
class	predictions which ||| linear regression	count=1
function_arg	a json [arg_2] ||| [arg_2] [function_1]	count=1
module_class	of each [class_2] ||| [module_1] gaussian mixture [class_2]	count=1
function	or gets ||| get	count=1
function	combinebykey ||| combine by key	count=1
class	wait for new ||| streaming query manager	count=1
arg	an ||| numpartitions	count=1
function	with a dependency ||| dependency	count=3
module	of rows whose ||| ml	count=1
function_arg	grid [arg_2] ||| [arg_2] [function_1]	count=2
function	rdd containing the distinct ||| distinct	count=1
arg	fixed [arg_2] ||| [arg_2] [arg_1]	count=1
arg	are array-like or buffer ||| array_like dtype	count=2
class	by key ||| by	count=1
function	of a batch ||| batch	count=2
class	again ||| query	count=1
module	gets the [module] k ||| [module]	count=1
arg	it ||| java_stage	count=4
class	used again ||| streaming query manager	count=1
class	that ||| manager	count=1
class	cachenodeids=false checkpointinterval=10 impurity="variance", ||| tree regressor	count=1
arg	item ||| key	count=2
module	gets the [module] numitemblocks or ||| [module]	count=1
class	this model ||| regression model	count=2
class	of this rdd's elements ||| rdd	count=1
module	[module] link ||| [module]	count=1
function	as the ||| save as	count=1
module	from this ||| mllib linalg	count=1
arg	[arg_1] the expected ||| [arg_2] [arg_1]	count=2
class	sets ||| generalized linear regression	count=3
function	the [function] from ||| infer [function]	count=1
function	call [function_2] ||| [function_1] [function_2]	count=1
module_class	creates a copy [module_1] [class_2] randomly generated uid and ||| [module_1] [class_2] vs rest copy	count=1
function	embedded params [function_2] ||| [function_2] [function_1]	count=2
function	into [function_2] ||| [function_2] [function_1]	count=1
class	such as the spark ||| spark	count=1
function_arg	[function_1] data ||| [function_1] [arg_2]	count=1
module_class	the [module_1] [class_2] ||| [module_1] [class_2] writer	count=3
function_arg	[function_1] path ||| [arg_2] [function_1]	count=4
function_arg	[function_1] document to ||| [function_1] [arg_2]	count=1
function	given string name ||| param	count=1
function_arg	keyed [arg_2] ||| [arg_2] [function_1]	count=1
class	in this [class] master ||| [class]	count=1
function_arg	size default 100 ||| size vectorsize	count=1
function	an rdd ||| normal vector rdd	count=2
function	for partition id ||| partition id	count=1
class	used again to wait ||| query manager	count=1
class	by the param ||| param	count=1
class	0 weightcol=none aggregationdepth=2): ||| linear svc	count=4
module	[module] this frame ||| [module]	count=2
arg	end exclusive ||| end	count=1
function	script on [function_2] ||| [function_1] [function_2]	count=1
function	for statistic functions ||| stat	count=1
arg	table via ||| url table mode	count=1
function	index of ||| index	count=1
class	[class_1] model ||| [class_2] [class_1]	count=3
function	value ||| add	count=1
module	gets the [module] metricname or ||| [module]	count=2
function	system using the new ||| as new	count=1
arg	the given path ||| sc path	count=6
class	model ||| ldamodel	count=1
function	selector type of the ||| set selector type	count=1
class	to wait for new ||| streaming	count=1
arg	[arg_1] labels=none) ||| [arg_2] [arg_1]	count=2
function	binary [function_2] ||| [function_1] [function_2]	count=1
module	[module] mintokenlength ||| [module]	count=1
function	[function] and ||| [function]	count=1
class	with ||| params	count=1
class	verify ||| tests	count=1
function	[function_1] type ||| [function_1] [function_2]	count=5
function	aggregate [function_2] ||| [function_1] [function_2]	count=2
module	two vectors we ||| linalg	count=1
function	on ||| on	count=8
module	[module] featurescol ||| [module]	count=1
function_arg	[function_1] [arg_2] ||| [function_1] min count [arg_2]	count=2
class	get depth of tree ||| tree model	count=1
function_arg	vector size [arg_2] ||| [arg_2] [function_1]	count=1
class	partitioned data ||| external group by	count=1
class	:func ||| query manager	count=2
class	queries so that :func ||| streaming query	count=1
class	for kmeans ||| kmeans	count=1
arg	table and ||| tablename	count=1
function	[function_1] based on ||| [function_2] [function_1]	count=1
class	of tree ||| tree	count=1
function	driver returns ||| driver	count=1
arg	the date ||| date	count=1
function	trees in ||| trees	count=1
class	adds an input ||| stream reader	count=1
module_class	in [class_2] ||| [module_1] [class_2]	count=4
function	generate ||| generate	count=1
function	of day in utc ||| utc	count=2
class	param grid ||| param grid	count=2
class	this distributed ||| distributed	count=1
class	null ||| regression summary	count=1
function	contains the count ||| count	count=1
module	the [module] pattern ||| [module]	count=1
class	queries ||| query manager	count=2
function	containing a json ||| json	count=1
function	with start offset ||| from offset	count=1
class	python rdd of ||| rdd	count=4
arg	to fixed ||| param	count=1
class	on ||| context	count=1
function	or compute the number ||| num	count=3
class	enable 'with [class] sc ||| [class]	count=1
module	used again to ||| sql	count=1
module	[module_1] two ||| [module_1] [module_2]	count=2
class	dataframe ||| data frame	count=11
arg	the table ||| tablename	count=1
function	contains ||| has param	count=1
arg	param ||| m1 m2 param	count=1
function	dstream in which each ||| by	count=1
function	squared [function_2] ||| [function_1] [function_2]	count=4
arg	:class dataframe representing the ||| sqlquery	count=1
class	data ||| by	count=1
function	the union ||| union	count=1
module	two vectors ||| linalg	count=1
function	iterations default ||| iterations	count=1
arg	labelcol="label", metricname="f1") ||| labelcol metricname	count=2
function_arg	[function_1] [arg_2] maxdepth=5 maxbins=32 mininstancespernode=1 mininfogain=0 ||| [function_1] [arg_2]	count=4
function_arg	orc [arg_2] ||| [arg_2] [function_1]	count=4
function	[function] arbitrary hadoop ||| new apihadoop [function]	count=3
function	creates an external ||| create external	count=3
module_class	of linear ||| ml linear	count=1
function	generate ||| generate logistic	count=1
class	dstream ||| dstream	count=2
function	map ||| map	count=2
class	:py attr predictions ||| generalized linear	count=1
function	rdd of labeledpoint ||| lib svmfile	count=1
function	to get ||| get	count=1
function	the training set given ||| training	count=1
arg	with [arg] one returning ||| [arg]	count=1
class	sets ||| classification evaluator	count=1
function	number ||| count	count=1
function	the test this should ||| test	count=1
function_arg	[function_1] [arg_2] ||| [function_1] products for users [arg_2]	count=6
function	selector ||| selector	count=1
class	[class] of the ||| data frame [class]	count=1
class	be used again ||| streaming	count=1
function	accuracy ||| accuracy	count=1
function	params for this ||| params	count=1
class	impurity="variance", seed=none variancecol=none) ||| regressor	count=1
class	the model ||| regression	count=1
function	left [function_2] ||| [function_2] [function_1]	count=4
function	each [function] this ||| foreach [function]	count=1
arg	given [arg_2] ||| [arg_2] [arg_1]	count=3
arg	an object ||| obj identifier	count=1
function	hivecontext for ||| for	count=1
function	given a java object ||| from java	count=1
class	every ||| linear model	count=1
function	the month ||| dayofmonth	count=1
function	as a [function_2] ||| [function_2] [function_1]	count=4
class	with weights already ||| with	count=1
function	threshold recall ||| recall by threshold	count=1
arg	before [arg] occurrences of ||| str delim [arg]	count=1
arg	format at ||| compression	count=2
class	for the spark ||| spark	count=1
arg	rdd with [arg] one returning ||| [arg]	count=1
function_arg	a new dstream by [function_1] [arg_2] dstream ||| [function_1] [arg_2]	count=2
class	right singular ||| singular	count=1
class	that all the ||| external	count=1
module	gets the [module] inputcol ||| [module]	count=1
module_class	creates a copy [module_1] [class_2] generated uid and some ||| [module_1] [class_2] rest copy	count=1
function	call [function_2] ||| [function_2] [function_1]	count=1
arg	table/view in the specified ||| tablename	count=1
function_arg	transforms the [arg_2] ||| [arg_2] [function_1]	count=2
class	used again to wait ||| manager	count=1
class	can be ||| manager	count=1
class	builder getorcreate() [class] app' syntax ||| [class]	count=1
class	on a model ||| streaming linear regression	count=1
arg	in modlist ||| modlist	count=1
arg	user ||| user	count=1
class	for which ||| regression model	count=1
module	number in ||| sql	count=1
function	name ||| has	count=1
arg	and other ||| other	count=1
function	this [function_2] ||| [function_2] [function_1]	count=2
module	gets the [module] maxcategories or ||| [module]	count=1
class	much of memory for ||| external	count=1
function_arg	option [arg_2] ||| [function_1] [arg_2]	count=1
function_arg	[function_1] format at ||| [function_1] path mode partitionby [arg_2]	count=4
arg	with the spark sink ||| addresses storagelevel maxbatchsize	count=1
function_arg	[function_1] [arg_2] family="gaussian", link=none fitintercept=true maxiter=25 ||| [function_1] [arg_2]	count=1
arg	substr ||| substr str	count=1
function	l{sparkcontext} that ||| context	count=1
function	much of memory for ||| object	count=1
class	awaitanytermination() can ||| streaming query manager	count=1
function_arg	extract a [arg_2] ||| [function_1] [arg_2]	count=1
module	an exception if ||| mllib	count=1
function	default min ||| default min	count=1
function	matrix on ||| matrix	count=1
module_class	[module_1] rdd which ||| [module_1] [class_2]	count=2
arg	database dbname ||| dbname	count=1
function	number of nonzero elements ||| num	count=2
function	objects ||| object	count=1
function	a rowmatrix ||| row	count=2
function	true positive rate ||| true positive rate	count=6
function	substring ||| substring	count=1
function	[function] of the ||| [function]	count=4
function	get [function_2] ||| [function_1] [function_2]	count=4
module	list of column names ||| sql	count=2
arg	of values ||| size values	count=1
function	with ||| range between	count=1
class	wait ||| query manager	count=2
class	the catalog ||| catalog	count=3
arg	setparams(self [arg_1] [arg_2] ||| params [arg_1] [arg_2]	count=4
arg	string ||| s	count=2
function	number of possible outcomes ||| num	count=1
function	on [function_2] ||| [function_2] [function_1]	count=4
arg	[arg_1] [arg_2] ||| [arg_1] [arg_2]	count=491
function	calculates the norm of ||| norm	count=1
module_class	shortcut of write() ||| ml pipeline	count=1
function	checkpointing using ||| checkpoint	count=1
arg	spark sink ||| addresses storagelevel maxbatchsize	count=1
function	the weights ||| weights	count=1
function	loads vectors ||| load vectors	count=2
class	model trained ||| logistic regression model	count=2
class	accumulator's data type returning ||| accumulator param	count=1
function_arg	__init__(self featurescol="features", ||| regressor init featurescol	count=1
arg	the [arg_1] [arg_2] ||| [arg_1] [arg_2]	count=2
class	[class_1] query ||| [class_2] [class_1]	count=2
arg	saved using rdd saveastextfile ||| minpartitions	count=1
module	vector representation [module] the ||| [module]	count=1
module	find norm [module] the given ||| [module]	count=1
module	[module] linkpower ||| [module]	count=1
module_class	[module_1] [class_2] checkpoint data ||| [module_1] [class_2]	count=2
function	rows of blocks ||| row blocks	count=3
function	the md5 digest and ||| md5	count=1
class	model ||| linear regression model	count=2
function	stream api with ||| stream	count=1
module	[module] isotonic or ||| [module]	count=1
class	checkpointinterval=10 impurity="variance", seed=none variancecol=none) ||| regressor	count=1
function	is checkpointed ||| is checkpointed	count=2
function	as a string column ||| string	count=1
class	contains a ||| params	count=1
class	batches of data ||| streaming linear algorithm	count=1
function	contains a ||| param	count=1
class	area ||| classification metrics	count=1
arg	based on ||| path	count=1
module	return ||| core	count=12
module	or compute [module_2] ||| [module_2] [module_1]	count=2
module_class	[module_1] model ||| [module_1] generalized linear regression [class_2]	count=1
function	a [function] instance ||| test java [function]	count=1
module	gets the [module] seed or ||| [module]	count=1
module	[module] tolowercase ||| [module]	count=1
function	[function] in ||| [function]	count=1
function_arg	[function_1] file ||| [function_1] [arg_2]	count=12
function	gets a [function_2] ||| [function_2] [function_1]	count=1
class	the [class_1] [class_2] ||| [class_1] [class_2]	count=2
function	[function_1] top features ||| [function_2] [function_1]	count=1
function	on the [function_2] ||| [function_2] [function_1]	count=3
module	representation [module_2] ||| [module_2] [module_1]	count=1
class	data into disks ||| external group by	count=1
class	depth of tree (e ||| tree model	count=1
function	[function_1] a global ||| [function_2] [function_1]	count=1
function	converts matrix columns in ||| convert matrix columns from ml	count=1
arg	than [arg] ||| dataseta [arg]	count=1
arg	outputformat api mapred package ||| outputformatclass keyclass valueclass	count=1
function	optimizedocconcentration ||| optimize doc concentration	count=2
class	of the dataframe ||| data frame writer	count=1
class	maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="gini", ||| classifier	count=1
module	[module] subsamplingrate ||| [module]	count=1
module	[module] maxiter ||| [module]	count=1
function	much ||| size	count=1
function	this instance contains a ||| param	count=1
function	directory for spill ||| path	count=1
class	parameters in this grid ||| grid builder	count=1
function	string column ||| string	count=1
function	foreachrdd get offsetranges ||| foreach get offset ranges	count=1
function	note : ||| approx	count=2
module_class	creates a [module_1] [class_2] ||| [module_1] [class_2]	count=24
class	[class_1] forest ||| [class_1] [class_2]	count=1
function	[function_1] likelihood ||| [function_1] [function_2]	count=5
arg	saves ||| mode partitionby	count=3
module	[module] weightcol or ||| [module]	count=1
function	[function_1] replaces ||| [function_2] [function_1]	count=2
function	list based [function_2] ||| [function_2] [function_1]	count=3
module_class	creates [module_1] [class_2] some ||| [module_1] [class_2]	count=4
arg	value numbits ||| col numbits	count=2
function	partial objects ||| save partial	count=1
function_arg	weighted averaged [arg_2] ||| [function_1] [arg_2]	count=1
arg	reduced into numpartitions partitions ||| numpartitions shuffle	count=1
function	the mean ||| mean	count=5
class	word2vec model's vocabulary default ||| word2vec	count=1
arg	withmean=false withstd=true [arg_2] ||| [arg_1] [arg_2]	count=2
arg	table named table ||| table column lowerbound	count=1
arg	multiclass classification ||| numclasses categoricalfeaturesinfo	count=1
arg	start [arg_2] ||| [arg_2] [arg_1]	count=5
function	[function_1] temporary ||| [function_2] [function_1]	count=3
class	lassomodel ||| lasso model	count=1
arg	inputcol=none outputcol=none [arg_2] ||| [arg_1] [arg_2]	count=4
arg	a term ||| term	count=1
arg	while tracking ||| preservespartitioning	count=1
class	of the :class dataframe ||| data frame writer	count=5
function	matrix columns in ||| matrix columns from ml	count=1
class	in this context to ||| streaming context	count=1
module_class	a gradient-boosted ||| mllib gradient boosted	count=2
function	classes values which the ||| classes	count=1
function	signed shift the ||| shift	count=1
class	that :func awaitanytermination() ||| query manager	count=1
class	"num" ||| model	count=1
function	[function_1] params ||| [function_2] [function_1]	count=8
class	onevsrestmodel create and ||| one vs rest model	count=1
function	array of features corresponding ||| features	count=1
function	the partition [function_2] ||| [function_1] [function_2]	count=1
arg	existing ||| samplingratio	count=1
class	accumulator's ||| accumulator param	count=1
class	the column standard ||| standard scaler model	count=2
function	to a string in ||| to	count=1
function	until any ||| await any	count=1
function	pretty printing of ||| str	count=1
function	convert a list ||| to	count=1
function	or ||| col	count=1
function	matrix columns ||| matrix columns from	count=1
class	[class] persists across ||| streaming [class]	count=1
class	obtaining a test ||| test	count=1
class	:class ||| data frame	count=6
class	which predictions ||| isotonic regression	count=1
function	synonyms of a ||| synonyms	count=1
function	matrix columns in an ||| matrix columns to ml	count=1
function	the embedded params ||| params	count=1
arg	of a word ||| word	count=1
function	on ||| on key	count=2
class	spark fair scheduler ||| spark	count=1
function	transform get [function_2] ||| [function_2] [function_1]	count=1
class	this ||| one vs	count=1
class	singular ||| singular	count=3
function	uid ||| uid	count=1
arg	the specified schema ||| schema	count=1
arg	[arg_1] and product ||| [arg_1] [arg_2]	count=4
class	queries so that :func ||| streaming	count=1
class	to wait for new ||| query manager	count=1
arg	saves the contents ||| mode partitionby	count=1
function	list [function_2] ||| [function_1] [function_2]	count=8
function_arg	[function_1] [arg_2] regparam=0 0 elasticnetparam=0 0 ||| [function_1] [arg_2] maxiter	count=2
class	stream ||| data stream writer	count=1
arg	database ||| dbname	count=4
class	the stream query if ||| stream	count=1
arg	is received ||| storagelevel	count=1
function	norm of ||| norm	count=1
module	gets the [module] quantileprobabilities ||| [module]	count=1
module_class	intermediate rdds used in [module_1] [class_2] ||| [module_1] [class_2]	count=2
class	frequency vectors or ||| hashing tf	count=1
class	all the ||| merger	count=1
function	week number of a ||| weekofyear	count=1
class	block [class_2] ||| [class_1] [class_2] subtract	count=2
function	type of ||| type	count=1
function	java udf so ||| java	count=1
arg	setparams(self formula=none featurescol="features", labelcol="label", ||| formula featurescol labelcol	count=1
function	months after [function] >>> df ||| [function]	count=1
module	of :class pyspark sql ||| sql	count=2
function	of columns [function_2] ||| [function_1] [function_2]	count=2
module_class	[module_1] training ||| [module_1] [class_2]	count=10
function	statistics ||| statistics	count=1
function	register a ||| register	count=1
function	a param ||| has param	count=1
function	return sparkcontext which ||| spark	count=1
function	but not in ||| subtract	count=1
module	gets the [module] n ||| [module]	count=1
function	two-sided p-value ||| p values	count=2
function	gets a [function_2] ||| [function_1] [function_2]	count=1
class	using the model ||| regression model	count=1
class	can be used ||| streaming	count=1
module	given string name ||| ml param	count=1
class	stream returning the ||| stream reader	count=2
arg	to d decimal places ||| d	count=1
class	sets ||| one hot encoder	count=2
class	partitioned data into disks ||| external	count=1
module	[module] stepsize or ||| [module]	count=2
module	instance contains ||| param	count=1
arg	featurescol="features", labelcol="label", forceindexlabel=false) ||| formula featurescol labelcol forceindexlabel	count=2
function	start [function_2] ||| core [function_1] [function_2]	count=2
module	compute [module_2] ||| [module_2] [module_1]	count=2
class	stream query if ||| data stream	count=1
arg	this dstream and other ||| other	count=1
class	:class dataframe ||| frame writer	count=1
arg	item ||| item	count=2
class	of this query that ||| streaming query	count=1
function	of memory for this ||| size	count=1
module	the underlying rdd with ||| mllib	count=1
function	param with ||| has param	count=1
module	return its path ||| core	count=1
function	datatype the data type ||| datatype	count=1
function	stream api ||| stream	count=2
function	false positive rate ||| false positive rate	count=3
function	[function_1] spark_home ||| [function_2] [function_1]	count=1
class	as a :class dataframe ||| spark session	count=1
function	on the log likelihood ||| log likelihood	count=1
function	distance ||| distance	count=2
function	numhashtables=1) [function] params ||| [function]	count=1
function	parse a [function_2] ||| [function_1] [function_2]	count=1
arg	correlation ||| method	count=1
function	the dot product ||| dot	count=2
function	driver as ||| to local	count=1
function	[function_1] zip ||| [function_1] file in [function_2] name	count=1
class	trained ||| logistic	count=1
function	sort order ||| sort	count=1
function	[function_1] a global ||| [function_1] [function_2]	count=1
function_arg	[function_1] dataframe representing ||| [arg_2] [function_1]	count=2
function	mean values ||| mean	count=1
function	on the [function_2] ||| [function_1] [function_2]	count=3
function	[function_1] completed ||| [function_1] [function_2]	count=1
function	on a ||| on	count=1
arg	[arg] maxiter=100 regparam=0 ||| featurescol [arg]	count=3
arg	each rdds into ||| dstream n block	count=1
function	[function_1] create ||| [function_2] [function_1]	count=4
function	[function_1] foreachrdd ||| [function_1] [function_2]	count=3
class	:func awaitanytermination() can ||| query manager	count=1
class	be used again ||| query manager	count=1
class	param ||| param	count=1
class	for this bucketizer ||| bucketizer	count=1
class	predictions [class_2] ||| [class_1] [class_2]	count=3
function	format ||| format	count=2
module	key and ||| core	count=2
class	[class_1] the singularvaluedecomposition ||| [class_1] [class_2]	count=2
function	the root [function_2] ||| [function_1] [function_2]	count=1
arg	datasets ||| dataseta	count=1
function	param with a given ||| has param	count=1
class	cross ||| cross	count=1
arg	format at [arg_2] ||| [arg_2] mode partitionby [arg_1]	count=2
function	of memory ||| object size	count=1
function_arg	receiver has been ||| receiver started receiverstarted	count=2
function	parse a field in ||| parse field	count=1
function_arg	model [arg_2] ||| [arg_2] [function_1]	count=1
class	column ||| column	count=1
class	this ||| external merger	count=2
class	ridgeregressionmode ||| ridge regression model	count=1
arg	file and ||| path schema sep	count=1
arg	in sql statements ||| name f returntype	count=1
module	gets the [module] tolowercase ||| [module]	count=1
module	embedded ||| ml param	count=1
function	[function] with ||| test train [function]	count=1
module	copies param ||| ml param	count=1
arg	the first n ||| n	count=1
function	value that match regexp ||| regexp	count=1
class	into ||| external	count=1
class	field in "predictions" ||| summary	count=3
arg	in a data source ||| source schema	count=2
function	be placed ||| modules	count=1
class	return the column ||| model	count=1
arg	the given user and ||| user	count=1
function	[function_1] json string ||| [function_2] [function_1]	count=3
class	this ||| one	count=2
function	number of rows ||| num rows	count=6
class	[class_1] generated ||| [class_2] [class_1]	count=8
arg	algorithm return the ||| rdd	count=1
function	matrix ||| matrix	count=3
function	predict the label of ||| predict	count=1
function	cached in-memory ||| cached	count=1
class	this rdd's elements ||| rdd	count=1
function_arg	[function_1] the observed ||| [arg_2] [function_1]	count=3
arg	multiclass ||| data numclasses	count=1
class	which ||| regression	count=11
function	inside brackets pairs ||| brackets	count=1
module_class	[module_1] [class_2] ||| [module_1] generalized linear regression [class_2]	count=2
class	for each numeric ||| grouped data	count=2
function_arg	weighted averaged [arg_2] ||| [arg_2] [function_1]	count=1
function	how much ||| size	count=1
arg	given user and ||| user	count=1
arg	correlation of two columns ||| col2 method	count=1
function	[function_1] prefix ||| [function_1] [function_2]	count=1
function	of [function_2] ||| [function_1] [function_2]	count=1
class	curve ||| binary classification metrics	count=1
class	objective ||| training summary	count=2
class	as a [class_2] ||| [class_2] [class_1]	count=3
function	deserialized batches lists of ||| stream without unbatching	count=1
module	shortcut of ||| ml	count=4
module_class	[module_1] matrix ||| [module_1] [class_2]	count=2
function	[function_1] param ||| [function_1] [function_2]	count=1
function_arg	[function_1] [arg_2] 0 checkpointinterval=10 losstype="squared", maxiter=20 ||| [function_1] [arg_2]	count=2
module_class	an rdd[vector] ||| mllib java vector	count=1
class	sets ||| tree ensemble params	count=1
function	rate for a ||| rate	count=1
class	values ||| scaler	count=1
module	python ||| streaming	count=1
arg	[arg_1] k=2 ||| [arg_2] [arg_1]	count=5
class	this vector to ||| sparse vector	count=1
function	inserts the ||| insert into	count=1
function	a binary mathfunction ||| binary mathfunction	count=1
module	rdd's elements in ||| core	count=1
arg	user [arg_2] ||| [arg_1] [arg_2]	count=1
function	code for a ||| code	count=1
class	which ||| isotonic regression model	count=1
arg	buffer ||| array_like dtype	count=2
class	for every ||| linear	count=1
function	size ||| size	count=4
arg	merges them with extra ||| extra	count=1
arg	dictionary ||| size	count=4
function_arg	[function_1] [arg_2] ||| [function_1] data decayfactor [arg_2]	count=1
function_arg	[function_1] featurescol="features", labelcol="label", ||| regressor [function_1] [arg_2]	count=1
function_arg	rate [arg_2] ||| [arg_2] [function_1]	count=1
function	instance contains ||| param	count=1
class	[class] containing union ||| [class]	count=1
function	the explained [function_2] ||| [function_1] [function_2]	count=2
module	[module] k ||| [module]	count=2
function	number of nonzero ||| num	count=2
function	this ||| has	count=1
class	for cross [class_2] ||| [class_2] [class_1]	count=1
function	:class dataframe as a ||| data frame as	count=1
class	tokenizer ||| tokenizer	count=1
arg	featurescol="features", labelcol="label", forceindexlabel=false) ||| featurescol labelcol forceindexlabel	count=2
function	compare 2 ml params ||| compare params	count=1
module_class	[module_1] terms or ||| [module_1] [class_2] vocab	count=1
module	the [module] k ||| [module]	count=2
function	[function_1] offsetranges ||| [function_2] [function_1]	count=13
function	calculates the length of ||| length	count=1
module_class	[module_1] vectors which ||| [module_1] [class_2]	count=2
function_arg	this grid [arg_2] ||| [function_1] [arg_2]	count=2
class	carry over its keys ||| streaming linear algorithm	count=1
arg	the same param ||| m2 param	count=1
function	is not contained ||| subtract	count=1
class	convert this matrix ||| matrix	count=1
function	quantileprobabilities ||| quantile probabilities	count=1
function	matrix columns in ||| matrix columns to	count=1
function_arg	[function_1] date1 ||| [arg_2] [function_1]	count=5
function	[function_1] default ||| [function_1] [function_2]	count=1
function	[function_1] statistics ||| [function_1] [function_2]	count=1
module	the [module] mininstancespernode or ||| [module]	count=1
function	columns of blocks in ||| col blocks	count=1
class	checkpointinterval=10 impurity="gini", [class_2] ||| [class_2] [class_1]	count=8
function	the value of spark ||| get	count=1
function	a batch [function_2] ||| [function_2] [function_1]	count=2
module	[module] mintf or ||| [module]	count=1
class	__init__(self ||| index to string	count=1
module	[module] mininstancespernode or ||| [module]	count=1
module	param with a given ||| ml param	count=1
module	again to wait for ||| sql	count=1
class	queries so ||| streaming	count=1
module	an exception if any ||| mllib linalg	count=1
class	__init__(self ||| string indexer	count=1
module	gets the [module] numtrees or ||| [module]	count=1
module	the [module] variancecol or ||| [module]	count=1
class	found [class] was garbage ||| [class]	count=1
function	of deserialized batches lists ||| stream without unbatching	count=1
class	the :class dataframe ||| frame	count=1
function	[function_1] java udf ||| [function_1] [function_2]	count=1
function	find the maximum ||| max	count=1
function	recommends the top ||| recommend	count=2
arg	content ||| content	count=1
class	so that :func ||| query manager	count=1
function	[function_1] positive rate ||| [function_2] [function_1]	count=3
function	set to a different ||| set	count=1
function	months [function_2] ||| [function_2] [function_1]	count=1
function_arg	save [arg_2] ||| [function_1] [arg_2]	count=4
class	returning the result ||| reader	count=1
arg	content and ||| content	count=1
module_class	[module_1] number ||| [module_1] [class_2]	count=14
function	bound on the log ||| log	count=1
module_class	a decision tree ||| mllib decision tree	count=1
class	training ||| linear regression training summary	count=1
function_arg	in orc [arg_2] ||| [function_1] path mode partitionby [arg_2]	count=3
class	dump already ||| external	count=1
function	inverse ||| inverse	count=1
class	which predictions are ||| isotonic regression	count=1
class	this standardscaler ||| standard scaler	count=1
class	:class dataframe ||| data	count=5
function	awaitanytermination() can ||| reset	count=1
module	[module] pattern ||| [module]	count=1
class	sgd ||| sgd	count=1
function	create a [function_2] ||| [function_1] [function_2]	count=6
class	logistic ||| logistic	count=4
arg	the database dbname ||| dbname	count=1
class	[class_1] [class_2] ||| [class_1] [class_2] remember duration	count=4
module_class	of [class_2] ||| [module_1] linear regression [class_2]	count=2
module	the [module] solver or ||| [module]	count=2
class	[class] bug ||| evaluator [class]	count=2
module	of numpy arrays ||| ml	count=1
function	[function_1] as a ||| [function_1] [function_2]	count=1
class	cachenodeids=false checkpointinterval=10 impurity="variance", seed=none ||| tree regressor	count=1
arg	the observed [arg_2] ||| [arg_2] [arg_1]	count=1
class	to in this model ||| kmeans model	count=2
function	recursive dependencies for debugging ||| to debug string	count=1
class	save ||| cloud pickler	count=1
function_arg	grid to [arg_2] ||| [arg_2] [function_1]	count=2
function	[function] missingvalue or ||| get missing [function]	count=1
module	[module] layers or ||| [module]	count=1
class	to ||| streaming query manager	count=2
function	[function_1] the mean ||| [function_1] [function_2]	count=2
function	the soundex encoding ||| soundex	count=1
function	distinct ||| distinct	count=3
function_arg	[function_1] by name ||| [arg_2] [function_1]	count=6
class	returned ||| spark streaming test case	count=1
function	of columns for ||| columns	count=1
function	batch [function_2] ||| [function_1] [function_2]	count=2
function	with start offset specified ||| offset	count=1
function	an exception ||| exception	count=1
arg	table ||| table column lowerbound	count=1
function	deserialized batches lists of ||| without unbatching	count=1
function	[function_1] [function_2] could not be found ||| [function_1] [function_2]	count=4
function	logic [function] ||| [function]	count=1
class	rdd is checkpointed ||| rdd	count=1
function	spark ||| spark	count=1
function	[function_1] driver returns ||| [function_2] [function_1]	count=1
function	each key using ||| by key	count=2
function	already partitioned data ||| spill	count=1
class	[class_1] forest ||| [class_2] [class_1]	count=1
class	for loading ||| java mlreader	count=1
function	a [function] ||| [function]	count=8
function_arg	test for [arg_2] ||| [function_1] [arg_2]	count=4
module_class	containing elements [class_2] ||| [module_1] [class_2]	count=2
module	gets the [module] minsupport or ||| [module]	count=1
function	current status of the ||| status	count=1
module	gets the [module] mintf ||| [module]	count=1
function	[function_1] the threshold ||| [function_1] [function_2]	count=1
class	sets ||| standard scaler	count=2
function	key value ||| key	count=1
function	[function_1] function ||| [function_1] [function_2]	count=1
function_arg	a [function_1] [arg_2] ||| [function_1] name [arg_2]	count=3
function	gets the name ||| get	count=1
class	this [class] need ||| [class]	count=1
function	[function_1] recall ||| [function_2] [function_1]	count=5
function	the minimum number ||| min count	count=2
function	topicdistributioncol [function_2] ||| [function_2] [function_1]	count=4
function	that ||| object size	count=1
function	stop the execution ||| stop	count=1
function	year ||| year	count=1
function	uid ||| reset uid	count=2
function	a py or ||| py file	count=1
function	[function_1] checkpointed and ||| [function_2] [function_1]	count=1
module_class	creates a copy [module_1] [class_2] ||| [module_1] train validation [class_2] copy	count=6
module	of expressions and returns ||| sql	count=1
function_arg	option [arg_2] ||| [arg_2] [function_1]	count=1
class	sets ||| multilayer perceptron classifier	count=4
module	[module] seed or ||| [module]	count=1
function	new java ||| new java	count=1
arg	spark sink ||| ssc addresses storagelevel maxbatchsize	count=1
class	already partitioned data ||| group	count=1
function	separators inside brackets pairs ||| brackets split	count=1
function	python [function_2] ||| [function_2] [function_1]	count=2
module	and name or ||| core	count=1
module	[module] stepsize ||| [module]	count=2
function	[function_1] field in ||| [function_2] [function_1]	count=1
function_arg	[function_1] labelcol="label", featurescol="features", ||| [function_1] [arg_2]	count=1
module	[module] numfeatures ||| [module]	count=1
function	positive [function_2] ||| [function_1] [function_2]	count=6
class	return ||| standard scaler model	count=1
function	[function_1] started ||| [function_2] [function_1]	count=1
function_arg	[function_1] chisqselector ||| [function_1] [arg_2]	count=4
function	one ||| add	count=1
module_class	[module_1] logistic regression ||| [module_1] [class_2]	count=12
function	n ||| n	count=1
function	deviance for ||| deviance	count=1
arg	version of a ||| heap	count=1
function	error ||| error	count=3
arg	database table [arg_2] ||| [arg_2] [arg_1]	count=2
arg	estimator=none estimatorparammaps=none evaluator=none ||| estimator estimatorparammaps evaluator	count=3
function	available ||| available	count=1
class	__init__(self ||| to string	count=1
module_class	singular [class_2] ||| [module_1] [class_2]	count=1
class	[class_1] matrix ||| [class_2] [class_1]	count=1
function	the year ||| year	count=1
class	can be used ||| query manager	count=1
function	instance contains ||| has	count=1
function	python parammap into ||| map to	count=1
function	and vector ||| vectors	count=1
arg	number of ||| num	count=2
function_arg	setparams(self [arg_2] ||| [function_1] [arg_2]	count=36
function	the companion [function_2] ||| [function_1] params to [function_2]	count=1
module	[module] bucketlength or ||| [module]	count=1
function	root ||| root	count=1
function	job ||| job	count=1
function	get the cluster ||| cluster	count=3
arg	with another ||| other	count=1
module	[module] evaluator or ||| [module]	count=1
class	returns a paired ||| factorization model	count=1
function	the += operator ||| iadd	count=1
module_class	[module_1] each training ||| [module_1] [class_2]	count=1
function	left outer ||| left outer	count=2
module	gets the [module] stepsize ||| [module]	count=2
arg	true iff [arg] is nan ||| [arg]	count=1
function_arg	by other, [arg_2] ||| [function_1] [arg_2]	count=1
module	of c{self} and ||| core	count=2
function	[function_1] columns in ||| [function_1] [function_2]	count=8
function	temporary table in ||| table	count=1
function	a local representation this ||| local	count=1
function	data or [function_2] ||| [function_1] [function_2]	count=1
function	[function_1] on first ||| [function_2] [function_1]	count=4
function	[function_1] to ||| [function_2] [function_1]	count=5
function_arg	functions registered [arg_2] ||| [arg_2] [function_1]	count=1
function	columns in an input ||| columns to ml	count=2
module_class	returns [class_2] ||| [module_1] [class_2]	count=33
arg	[arg] family="gaussian", ||| [arg]	count=2
class	data of a ||| data stream	count=1
module	returns the documentation of ||| ml	count=1
class	dstreams in this ||| streaming	count=1
function	the count of distinct ||| count	count=1
arg	around pattern pattern ||| pattern	count=1
module	norm [module] the ||| [module]	count=1
class	the content [class_2] ||| [class_2] [class_1]	count=2
function	nonnegative ||| nonnegative	count=1
function_arg	a java parammap ||| java pyparammap	count=1
class	test ||| kafka stream tests	count=9
function	in parquet ||| parquet	count=1
function_arg	[function_1] version ||| [function_1] [arg_2]	count=1
function_arg	return a new [function_1] [arg_2] this dstream ||| [function_1] [arg_2]	count=2
class	[class_1] output ||| [class_1] [class_2]	count=1
function	given ||| has param	count=1
arg	dataset and an ||| dataset key numnearestneighbors distcol	count=1
arg	product and ||| product	count=1
function	current status ||| status	count=1
function	batch has half ||| half	count=1
function	a local ||| local	count=3
class	use ||| context	count=2
function	unifying data ||| union	count=1
class	matrix this ||| matrix	count=1
module	dump already partitioned data ||| core	count=1
function	by applying 'full ||| full	count=1
function	waits for the termination ||| termination	count=1
function	option ||| option	count=1
class	:class dataframe to ||| frame writer	count=1
function	libsvm format ||| libsvm	count=2
function	that has exactly ||| coalesce	count=1
function	subset ||| subset	count=1
arg	the specified [arg_2] ||| [arg_2] [arg_1]	count=1
module_class	[module_1] [class_2] ||| [module_1] logistic regression [class_2]	count=1
module	the [module] numtrees or ||| [module]	count=1
class	onevsrestmodel ||| one vs rest model	count=2
class	a paired ||| factorization	count=1
class	of this [class_2] ||| [class_2] [class_1]	count=1
function	much of ||| size	count=1
module	[module] droplast ||| [module]	count=1
module	mean variance and ||| core	count=1
function	instance contains ||| has param	count=1
function	the new mllib-local representation ||| as	count=4
class	sets ||| handle invalid	count=1
function_arg	rate for [arg_2] ||| [function_1] [arg_2]	count=1
class	:func awaitanytermination() can be ||| query	count=1
arg	a python topicandpartition to ||| topic partition	count=1
function	initialized ||| ensure initialized	count=1
class	of memory ||| merger	count=1
class	parameters passed as ||| conf	count=1
function	on the driver returns ||| on driver	count=1
function	value to [function_2] ||| [function_1] [function_2]	count=4
function	returns a java storagelevel ||| java	count=1
class	into the returned ||| spark streaming test case	count=1
function	set the selector ||| selector	count=1
function	the index ||| partitions with index	count=1
function	a right [function_2] ||| [function_1] [function_2]	count=4
arg	for a condition to ||| condition	count=1
function	all values as ||| all	count=1
class	used ||| streaming query manager	count=2
arg	a new name ||| name	count=1
class	so ||| streaming	count=1
arg	[arg_1] trainratio=0 75 ||| [arg_2] [arg_1]	count=1
class	with weights ||| with	count=1
function_arg	[function_1] [arg_2] mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 ||| regressor [function_1] [arg_2] maxdepth	count=1
function	register a java udf ||| register java	count=1
function	into a java ||| param map to java	count=1
function	squared distance from a ||| squared distance	count=2
function	of ||| object	count=1
arg	buckets ||| numbuckets col	count=1
function	[function_1] stream api ||| [function_2] [function_1]	count=4
class	for input user ||| model	count=1
function	local ||| local	count=3
class	partitioned data into ||| by	count=1
arg	to consist of ||| ascending numpartitions keyfunc	count=1
module	[module] losstype ||| [module]	count=1
arg	saves the ||| mode partitionby	count=3
arg	database table named table ||| table	count=1
function_arg	worker [arg_2] ||| [function_1] [arg_2]	count=1
class	onevsrestmodel create and return ||| one vs rest	count=1
function_arg	[function_1] name ||| [function_1] [arg_2]	count=1
class	trained ||| tree ensemble	count=1
function	max abs vector ||| max abs	count=1
module	wait for ||| sql	count=1
arg	centroids according ||| decayfactor timeunit	count=2
function_arg	test for [arg_2] ||| [arg_2] [function_1]	count=4
class	in mixture ||| mixture	count=1
arg	url url and ||| url	count=1
class	degrees ||| generalized linear regression summary	count=1
function	a [function] to ||| [function]	count=1
arg	dictionary a list of ||| size	count=4
function	[function_1] [function_2] ||| [function_2] on [function_1]	count=2
module	window function [module] the value ||| [module]	count=2
function	dataframe from ||| from	count=1
class	of this rdd's ||| rdd	count=1
class	train a [class] ||| [class] with	count=1
module	the [module] statement or ||| [module]	count=1
arg	defined from start ||| start	count=1
arg	rdd, a list ||| schema samplingratio verifyschema	count=1
module_class	return all ||| core external merger	count=1
module	for new terminations ||| sql	count=1
class	the underlying output ||| stream writer	count=1
class	given ||| params	count=1
module	[module] a sparsematrix ||| [module]	count=1
class	left singular [class_2] ||| [class_1] [class_2]	count=1
arg	a ||| cls	count=1
function	synonyms of ||| synonyms	count=1
function_arg	__init__(self featurescol="features", labelcol="label", ||| init featurescol labelcol	count=7
function	deserialized batches lists ||| without unbatching	count=1
function	ordering columns in a ||| order by	count=1
class	be used ||| query	count=1
class	of the test ||| test	count=1
function	current status of ||| status	count=1
class	again to wait ||| query	count=1
function	[function_1] outer join ||| [function_2] [function_1]	count=6
class	already partitioned data ||| external	count=1
function	summary ||| summary	count=2
function	levenshtein distance of the ||| levenshtein	count=1
class	operation ||| rdd	count=1
function	queue of ||| queue	count=1
function_arg	[function_1] another dstream ||| streaming dstream [function_1] [arg_2]	count=3
class	"predictions" [class_2] ||| [class_2] [class_1]	count=18
function	prefix of string in ||| prefix	count=1
function	limits ||| limit	count=1
class	for ||| external merger	count=2
class	partitioned data ||| by	count=1
module	the [module] percentile or ||| [module]	count=1
arg	default 100 ||| vectorsize	count=1
function	with leaders ||| with leaders	count=2
function	binary ||| binary	count=2
class	[class_1] on ||| [class_2] [class_1]	count=2
module	in one operation ||| core	count=1
class	trees in the ensemble ||| ensemble	count=2
function	orc files returning ||| orc	count=1
function	java storagelevel based ||| java	count=1
function	log probability ||| log prior	count=1
class	mixture ||| mixture	count=2
function	rows [function_2] ||| [function_2] [function_1]	count=2
class	for multiclass classification ||| multiclass classification	count=2
function	union ||| union	count=1
module	value of each ||| ml	count=1
module	[module] thresholds or ||| [module]	count=1
function	the jobs started ||| job	count=1
arg	push item ||| item	count=1
function	'cogroup' between ||| cogroup	count=1
arg	days after [arg] >>> ||| [arg]	count=1
function	[function_1] of columns ||| [function_1] [function_2]	count=1
module	used again ||| sql	count=1
class	more examples ||| decision tree model	count=1
class	randomly generated uid and ||| cross validator	count=1
function	vector columns in an ||| vector columns to ml	count=1
function_arg	setparams(self labelcol="label", ||| set params labelcol	count=1
function	[function_1] prefix of ||| [function_1] [function_2]	count=1
function	sql storage type for ||| sql type	count=1
function	the ids [function_2] ||| [function_2] [function_1]	count=4
function	applying 'left ||| left	count=1
class	this context to ||| streaming context	count=1
class	such as the spark ||| spark context	count=1
function	single ||| single	count=2
class	for ||| model	count=6
module	the [module] itemscol or ||| [module]	count=1
class	attr lda ||| distributed ldamodel	count=1
class	produced by the ||| clustering	count=1
class	so that :func ||| streaming query manager	count=1
class	again ||| query manager	count=2
arg	[arg_1] metricname="f1") ||| [arg_2] [arg_1]	count=2
function	return the column mean ||| mean	count=1
module	[module] threshold or ||| [module]	count=2
function	day in utc ||| from utc	count=1
class	that all the objects ||| external	count=1
function	that :func awaitanytermination() can ||| reset	count=1
function_arg	[function_1] <http //jsonlines ||| [arg_2] [function_1]	count=4
function	[function_1] stream messagehandler ||| [function_2] [function_1]	count=2
function	the schema of ||| schema	count=1
function_arg	save a linearregressionmodel ||| save sc path	count=1
arg	predictioncol="prediction", maxiter=100 tol=1e-6 ||| predictioncol maxiter	count=2
module_class	[module_1] this ||| [module_1] [class_2]	count=2
function	py or zip dependency ||| py file	count=1
module_class	multinomial [class_2] ||| [module_1] [class_2]	count=4
class	that :func awaitanytermination() can ||| query	count=1
module	out into external storage ||| sql	count=1
class	data of ||| data stream	count=1
function	test this ||| test	count=1
function	in place ||| in place	count=2
function	return number of nodes ||| num nodes	count=1
arg	seed=none k=4 mindivisibleclustersize=1 ||| maxiter seed	count=2
function	column denoted by ||| getattr	count=1
arg	in the specified database ||| dbname	count=2
arg	d samples drawn ||| shape scale numrows	count=1
module	the [module] censorcol ||| [module]	count=1
function	can be used again ||| reset	count=1
function	as the specified table ||| as table	count=1
function	threshold if any ||| threshold	count=1
arg	in c{self} that ||| other numpartitions	count=1
function	[function_1] stream transform ||| [function_1] [function_2]	count=2
function	this instance to ||| to	count=2
class	the block ||| block	count=1
function	of value [function_2] ||| [function_2] [function_1]	count=1
class	impurity="variance", subsamplingrate=1 [class] ||| [class]	count=1
class	returns a paired ||| factorization	count=1
class	a param ||| params	count=1
class	pipelinemodel used for ml ||| pipeline model	count=1
module	:class pyspark sql ||| sql	count=2
function	a given string ||| param	count=1
function	[function_1] brackets ||| [function_2] [function_1]	count=1
class	[class_1] k-means algorithm ||| [class_2] [class_1]	count=1
class	new :class dataframe with ||| data frame	count=1
class	tree (e g depth ||| tree model	count=1
class	the ||| rdd	count=3
class	support vector machine on ||| svmwith sgd	count=1
class	__init__(self ||| cross validator	count=1
class	seed=none impurity="gini", ||| classifier	count=1
class	model ||| bayes model	count=1
function	outer join ||| outer join	count=4
class	a randomly generated ||| validation split	count=2
function	copy all ||| copy	count=1
function_arg	__init__(self formula=none [arg_2] ||| [function_1] [arg_2]	count=3
module	a ||| core	count=10
class	this stringindexer ||| string indexer	count=1
function	transfer this instance to ||| to	count=2
module_class	column [class_2] ||| [module_1] [class_2]	count=2
module_class	[module_1] [class_2] - ||| [module_1] [class_2] subtract	count=4
class	contains a param ||| params	count=1
class	stream [class_2] ||| [class_2] [class_1]	count=5
function	find synonyms ||| find synonyms	count=1
class	a paired rdd ||| factorization model	count=1
arg	named table ||| table	count=1
function	[function_1] vectors saved ||| [function_1] [function_2]	count=1
function	using the old hadoop ||| save as hadoop	count=1
module	of type ||| ml	count=1
class	[class_1] which ||| [class_2] [class_1]	count=22
function	gets summary (e ||| summary	count=1
module	the [module] subsamplingrate or ||| [module]	count=1
function	param with a ||| has param	count=1
module	gets the [module] ||| [module]	count=96
function	converts matrix [function_2] ||| [function_2] [function_1]	count=8
function	partition [function_2] ||| [function_2] [function_1]	count=2
function	[function_1] testing ||| [function_1] [function_2]	count=7
arg	an object ||| obj	count=2
module	the [module] numitemblocks or ||| [module]	count=1
function	gets summary ||| summary	count=1
function	of partitions ||| partitions	count=1
class	return the ||| scaler	count=1
function	initial [function_2] ||| [function_2] [function_1]	count=1
function	l{statcounter} object that captures ||| stats	count=1
class	"predictions" ||| summary	count=3
function	with the ordering defined ||| order by	count=1
class	columns ||| data frame	count=2
function	as the [function_2] ||| [function_1] [function_2]	count=1
function	intercept computed ||| intercept	count=1
function	add two values of ||| add	count=1
function	[function_1] into label ||| [function_2] [function_1]	count=4
arg	iff [arg] is ||| [arg]	count=2
module	and count of ||| core	count=1
arg	an rdd of ||| rdd	count=2
function	number of ||| count	count=1
module	returns true if the ||| sql	count=1
class	seed=none numtrees=20 ||| random forest	count=1
class	[class] as ||| [class]	count=2
arg	[arg] vectorsize ||| [arg]	count=1
function	a py ||| py	count=1
module	value of each instance ||| ml	count=1
function	total number ||| total num	count=2
module_class	[module_1] [class_2] :class datastreamreader ||| [module_1] [class_2] session	count=1
function_arg	setparams(self featurescol="features", ||| set params featurescol	count=8
function	for each original column ||| original	count=1
class	bisecting ||| bisecting	count=2
class	a model [class_2] ||| [class_2] [class_1]	count=3
function	window [function_2] ||| [function_2] [function_1]	count=2
arg	marks ||| blocking	count=1
function	string ||| param	count=1
class	sets ||| hot encoder	count=2
function	offsetrange ||| ranges	count=1
arg	srccol ||| srccol	count=1
function	a left outer ||| left outer	count=2
function	non-streaming ||| write	count=1
function	[function_1] specified table ||| [function_2] [function_1]	count=1
module	[module] tolowercase or ||| [module]	count=1
function	average [function] ||| [function]	count=6
function	subset [function_2] ||| [function_2] [function_1]	count=1
class	for input ||| model	count=1
function	[function_1] foreachrdd get ||| [function_2] [function_1]	count=3
arg	calculates the correlation ||| col1 col2 method	count=1
class	are the right singular ||| singular	count=1
function_arg	[function_1] content ||| [function_1] zip name [arg_2] ext dir	count=1
function	an exception [function_2] ||| [function_2] [function_1]	count=1
function	returns a ||| sql	count=1
module	gets the [module] finalstoragelevel ||| [module]	count=1
arg	saves the contents of ||| format mode partitionby	count=1
function_arg	[function_1] fixed values ||| [function_1] [arg_2]	count=2
class	random [class_2] ||| [class_1] [class_2]	count=1
arg	by name ||| name doc	count=4
function_arg	users [arg_2] ||| [arg_2] [function_1]	count=1
function	[function_1] array ||| [function_2] [function_1]	count=5
arg	table [arg_2] ||| [arg_2] [arg_1]	count=2
class	stream query ||| data stream	count=1
arg	another dstream ||| other	count=1
function	an rdd ||| poisson vector rdd	count=1
function	[function_1] matrix ||| [function_1] [function_2]	count=2
arg	[arg_1] according to ||| [arg_2] [arg_1]	count=4
function	minutes of a given ||| minute	count=1
function	number of [function_2] ||| [function_1] [function_2]	count=17
class	in the ensemble ||| ensemble	count=2
class	train a [class_1] [class_2] ||| mllib [class_1] [class_2] with	count=1
arg	for binary or multiclass ||| numclasses	count=1
module_class	a decision ||| mllib decision	count=1
class	in the ensemble ||| ensemble model	count=2
module_class	[module_1] [class_2] session that has separate ||| [module_1] [class_2] new session	count=1
class	queries so ||| streaming query	count=1
function	key [function_2] ||| [function_2] [function_1]	count=14
class	query that ||| query	count=1
function_arg	the minimum item ||| min key	count=1
function	into a python parammap ||| param map from	count=1
class	dump ||| external	count=1
module	values ||| sql	count=2
class	queries so that ||| query manager	count=1
function	the mean squared error ||| mean squared error	count=1
function	list of [function_2] ||| [function_1] [function_2]	count=11
function	cluster ||| cluster	count=7
function	key using an ||| by key	count=2
class	how much of ||| external	count=1
function	the initial [function_2] ||| [function_2] [function_1]	count=1
function	creates an ||| create	count=1
class	ml instance the default ||| mlreader	count=1
function	total ||| total	count=2
module	the [module] aggregationdepth or ||| [module]	count=1
function_arg	[function_1] saved using ||| [function_1] [arg_2]	count=3
module	expression that [module] true iff ||| [module]	count=2
class	awaitanytermination() can be used ||| streaming query manager	count=1
arg	map [arg] ||| [arg]	count=3
arg	[arg_1] and date2 ||| [arg_2] [arg_1]	count=1
function	[function_1] cluster ||| [function_2] [function_1]	count=4
function	the offsetrange of ||| ranges	count=1
arg	the correlation of two ||| method	count=1
class	and [class] ||| [class]	count=5
class	incrementing as ||| task context	count=1
function	the objects ||| object	count=1
arg	trainratio=0 75 seed=none): ||| trainratio	count=1
class	used again to wait ||| streaming query	count=1
module	the [module] numhashtables or ||| [module]	count=1
class	estimated coefficients and intercept ||| linear regression summary	count=1
function	awaitanytermination() can be ||| reset	count=1
function	recovers all the ||| recover	count=1
module	gets the [module] max ||| [module]	count=1
class	that all ||| merger	count=1
arg	defines an event time ||| eventtime delaythreshold	count=1
function	recommends the top "num" ||| recommend	count=2
module	returns ||| ml	count=1
function	count of distinct elements ||| count	count=1
function	much of memory for ||| size	count=1
function	right outer ||| full outer	count=2
module	of this instance this ||| ml	count=1
function	a window function ||| window function	count=1
arg	using string representations ||| compressioncodecclass	count=1
function	of rows in ||| count	count=1
arg	k=none [arg_2] ||| [arg_2] [arg_1]	count=1
class	:class dataframe, using ||| data frame	count=1
arg	centroids according to ||| decayfactor timeunit	count=2
class	content [class_2] ||| [class_2] [class_1]	count=2
class	:func awaitanytermination() ||| manager	count=1
function	to this ||| add	count=1
function	the driver returns ||| driver	count=1
class	the input [class_2] ||| [class_2] [class_1]	count=4
module_class	[module_1] [class_2] ||| [module_1] gaussian mixture [class_2]	count=3
class	find ||| word2vec model	count=1
function	restore an object ||| restore	count=1
arg	outputformat api mapred ||| outputformatclass keyclass valueclass	count=1
module	gets the [module] modeltype or ||| [module]	count=1
function	cost sum of ||| cost	count=2
class	data ||| group by	count=2
module	name ||| ml param	count=2
class	'x' to all mixture ||| gaussian mixture	count=1
function	foreachrdd ||| foreach	count=1
class	the underlying :class sparkcontext ||| spark session	count=1
class	used again to wait ||| streaming query manager	count=1
function	to an indexedrowmatrix ||| to indexed row	count=4
class	of the rdd ||| rdd	count=1
function	[function_1] columns ||| [function_2] [function_1]	count=10
arg	drawn ||| shape scale numrows	count=1
function_arg	the maximum [arg_2] ||| [arg_2] [function_1]	count=1
function	fwe ||| fwe	count=1
class	this model ||| linear regression model	count=2
function_arg	window function [arg_2] ||| [function_1] name [arg_2]	count=1
function	[function_1] blocks in ||| [function_1] [function_2]	count=4
class	with a randomly ||| cross validator	count=1
arg	cleared ||| description interruptoncancel	count=1
class	queries so that ||| query	count=1
arg	saves ||| path format mode partitionby	count=1
function	create an [function_2] ||| [function_1] [function_2]	count=1
class	'x' to all mixture ||| mixture	count=1
module	arrays of indices ||| ml	count=1
class	to use for loading ||| java mlreader	count=1
arg	by number n ||| n	count=1
class	for every feature ||| model	count=1
module	the [module] itemcol or ||| [module]	count=1
class	block matrix ||| block matrix	count=1
module	but return ||| core	count=1
function	[function_1] [function_2] into py4j which could ||| [function_1] [function_2]	count=1
arg	given key ||| key	count=1
function	finding frequent [function_2] ||| [function_2] [function_1]	count=4
class	the stream query if ||| data stream	count=1
class	using the model trained ||| tree ensemble model	count=1
function_arg	a param [arg_2] ||| [arg_2] [function_1]	count=1
function	output [function] the ||| partition [function]	count=1
class	for new ||| query	count=1
class	sets ||| min max scaler	count=2
class	clusters ||| power iteration clustering model	count=1
class	sets ||| count vectorizer	count=2
module	aggregate function [module] the ||| [module]	count=1
function	of tables/views in the ||| tables	count=1
class	'x' to all mixture ||| mixture model	count=1
function	parses ||| expr	count=1
module	of memory for this ||| core	count=1
function	weighted averaged ||| weighted	count=1
class	to wait ||| query	count=1
class	a new [class] containing union ||| [class]	count=1
function	lambda function ||| function	count=2
function	[function_1] info ||| [function_2] [function_1]	count=4
function	each key ||| key	count=2
arg	from the given path ||| path	count=4
module_class	value and parent ||| ml persistence test	count=1
function	[function_1] labeled ||| [function_2] [function_1]	count=1
class	for this stringindexer ||| string indexer	count=1
function	average precision map ||| average precision	count=1
arg	labelcol="label", predictioncol="prediction", [arg_2] ||| [arg_1] [arg_2]	count=7
module	[module] predictioncol ||| [module]	count=1
arg	classification ||| categoricalfeaturesinfo	count=1
function	the new ||| new	count=1
function	tables/views in the specified ||| tables	count=1
class	each dstreams in this ||| streaming	count=1
class	model on ||| with	count=1
class	[class_1] [class_2] remember rdds it generated ||| [class_1] [class_2] remember duration	count=4
class	which is ||| regression metrics	count=1
module	value pairs or two ||| linalg	count=2
module_class	[module_1] [class_2] ||| [module_1] tree ensemble [class_2]	count=1
class	this matrix to ||| dense matrix	count=1
module	and count of the ||| core	count=1
function	the root ||| get root	count=1
class	sets ||| quantile discretizer	count=3
function	to the companion ||| transfer params to	count=1
class	for new terminations ||| streaming	count=1
arg	convert python object ||| obj	count=2
function	given join ||| join	count=1
module	the given timezone returns ||| sql	count=1
function	this blockmatrix by other, ||| multiply	count=1
module	already ||| core	count=1
function_arg	recommends the [arg_2] ||| [function_1] users product [arg_2]	count=2
function	node depth 1 ||| depth	count=1
class	value ||| accumulator	count=1
class	class ||| collector	count=1
function_arg	a parquet [arg_2] ||| [arg_2] [function_1]	count=1
function	dependency on ||| dependency	count=1
arg	another dstream with ||| other	count=1
function	ml params instances ||| params	count=1
function	for dataframe from a ||| from	count=1
function_arg	[function_1] rep ||| [function_1] replace [arg_2]	count=3
arg	of document ||| document	count=1
arg	correlation of ||| col1 col2 method	count=1
module	partitioned ||| core	count=1
class	predictions which gives the ||| generalized linear regression summary	count=1
function	[function_1] script file ||| [function_2] [function_1]	count=2
module	param with ||| ml	count=1
function	external ||| external	count=1
module	two [module_2] ||| [module_2] [module_1] sparse	count=2
function	load [function_2] ||| [function_2] [function_1]	count=2
class	maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", ||| decision tree regressor	count=1
arg	value in c{self} that ||| other numpartitions	count=1
class	__init__(self ||| generalized linear regression	count=1
class	awaitanytermination() can ||| streaming	count=1
function	new ||| as new	count=1
function	month ||| dayofmonth	count=1
module	the [module] numfeatures or ||| [module]	count=1
class	a param with a ||| params	count=1
class	in ||| builder	count=2
function	optimizer ||| optimizer	count=1
module_class	python [class_2] ||| [module_1] [class_2]	count=8
class	this accumulator's value ||| accumulator	count=1
class	[class_1] regression model ||| [class_1] [class_2]	count=1
function	schema in the ||| schema	count=1
function	last value ||| last	count=1
function	date ||| next day	count=1
arg	a term to ||| term	count=1
class	awaitanytermination() can ||| query manager	count=1
module	[module] maxbins ||| [module]	count=1
function	[function_1] script ||| [function_2] [function_1]	count=2
function	[function_1] params are ||| [function_2] [function_1]	count=2
function	converts vector ||| convert vector	count=4
class	[class_1] including ||| [class_1] [class_2]	count=4
module	skipping null ||| sql	count=2
class	standard ||| standard	count=1
class	used again to wait ||| query	count=1
class	[class] by ||| [class]	count=2
arg	value [arg_2] ||| [arg_2] [arg_1]	count=1
function	the number of partitions ||| num partitions	count=1
class	parallelism [class] when ||| [class]	count=1
function	[function_1] under ||| [function_2] [function_1]	count=5
module	[module] mintokenlength or ||| [module]	count=1
arg	the correlation of ||| col1 col2 method	count=1
arg	the param ||| param	count=1
class	sets ||| chi sq selector	count=4
class	checkpointinterval=10 impurity="variance", seed=none variancecol=none) ||| tree regressor	count=1
function	the trigger ||| trigger	count=1
function	squared distance ||| squared distance	count=4
class	which is a dataframe ||| regression summary	count=1
function	jobs ||| job	count=1
arg	by ||| doc	count=2
function	find [function_2] ||| [function_2] [function_1]	count=1
class	with a randomly generated ||| validation split	count=2
class	column ||| model	count=1
module	[module] a new ||| [module]	count=3
arg	from start ||| start	count=2
module	count ||| core	count=1
function	seed=none): sets [function_2] ||| [function_2] [function_1]	count=2
function	ml params ||| params	count=1
module	so that ||| sql	count=1
class	which is a risk ||| linear regression summary	count=1
class	broadcast ||| spark context	count=1
arg	have the same param ||| m1 m2 param	count=1
function	:class dataframe [function_2] ||| [function_2] [function_1]	count=7
function	api with start offset ||| offset	count=1
function_arg	[function_1] input dataset ||| [function_1] [arg_2]	count=1
function	sliding window of ||| window	count=1
function	offsetrange of ||| ranges	count=1
class	count of the ||| rdd	count=1
arg	fixed [arg_2] ||| [arg_1] [arg_2]	count=1
function	of memory for ||| size	count=1
arg	a dictionary of values ||| size values	count=1
arg	than or equal ||| numiterations	count=1
function_arg	configuration [arg_2] ||| [arg_2] [function_1]	count=2
class	cluster for each training ||| summary	count=1
function_arg	__init__(self [arg_2] ||| [function_1] [arg_2] maxdepth	count=6
arg	using string representations of ||| compressioncodecclass	count=1
function	nodes summed over ||| nodes	count=2
class	multiple parameters passed as ||| conf	count=1
class	model ||| factorization model	count=1
class	and update the catalog ||| catalog	count=1
class	[class] on the ||| kmeans [class]	count=3
class	catalog ||| catalog	count=3
module	the [module] fitintercept or ||| [module]	count=1
class	the right singular vectors ||| singular	count=1
class	on a ||| with	count=1
module	gets the [module] variancecol ||| [module]	count=1
module	[module] variancepower or ||| [module]	count=1
function	the threshold that ||| threshold	count=1
function	new ||| new	count=2
arg	by codeblock ||| cls	count=1
class	checkpointinterval=10 losstype="logistic", ||| gbtclassifier	count=2
function	until any of ||| any termination	count=1
function	groups the ||| group by	count=1
function	this accumulator's value ||| add	count=1
function	[function_1] driver ||| [function_2] [function_1]	count=1
module	a shortcut of ||| ml	count=4
module	[module] regparam ||| [module]	count=1
class	impurity="variance", ||| regressor	count=1
module_class	[module_1] gradient-boosted ||| [module_1] [class_2]	count=4
class	clusters ||| clustering	count=1
function	[function_1] [function_2] a ||| [function_2] [function_1]	count=10
function	stats pstats stats ||| stats	count=1
arg	:param rdd an rdd ||| cls rdd	count=1
function	get number of nodes ||| num	count=1
function	need [function] between python ||| need [function]	count=1
class	words closest in ||| word2vec	count=1
function	session to use ||| session	count=1
arg	elements in iterator ||| iterator key reverse	count=1
arg	pos in byte ||| pos	count=1
function	computes hex ||| hex	count=1
function	job [function_2] ||| [function_1] ids for [function_2]	count=2
function	a new [function_2] ||| [function_1] [function_2]	count=1
arg	[arg] same ||| [arg] name sinceversion	count=3
function	using the new ||| save as new	count=1
class	variance and ||| rdd	count=1
class	params shared by ||| params	count=1
arg	an existing rdd ||| samplingratio	count=1
function_arg	[function_1] incoming dstream ||| [arg_2] [function_1]	count=6
arg	set numtopfeature for ||| numtopfeatures	count=1
function	of functions registered in ||| functions	count=1
function_arg	mathfunction [arg_2] ||| [function_1] name [arg_2]	count=1
class	attr predictions which ||| linear regression	count=1
function	number of partitions for ||| partitions	count=1
function	all the objects ||| size	count=1
module	[module] maxcategories or ||| [module]	count=1
class	sets ||| isotonic regression	count=1
function	columns in an ||| columns to	count=2
module	gets the [module] solver ||| [module]	count=2
function	a right outer ||| full outer	count=2
arg	representing the result ||| sqlquery	count=1
module_class	creates tuples of [module_1] [class_2] ||| [module_1] [class_2]	count=2
module_class	return a ||| core spark	count=1
module_class	gets the [module_1] [class_2] handleinvalid or its default ||| [module_1] [class_2] discretizer get handle invalid	count=1
function	names of tables in ||| table names	count=1
function	levenshtein distance ||| levenshtein	count=1
function	set a ||| set	count=2
function	__init__(self formula=none ||| init	count=1
class	can ||| frame	count=1
arg	function to [arg_2] ||| [arg_1] [arg_2]	count=2
module_class	[module_1] class ||| [module_1] [class_2]	count=2
arg	table named table ||| table column	count=1
module	gets the [module] numitemblocks ||| [module]	count=1
function	prefix of string ||| prefix	count=1
function	topicdistributioncol or its default ||| topic distribution col	count=1
module	and name ||| core	count=1
arg	1 [arg] inclusive) in ||| [arg]	count=1
function	[function_1] the partitions ||| [function_2] [function_1]	count=1
class	tree [class_2] ||| [class_1] [class_2]	count=1
function_arg	[function_1] another ||| streaming dstream [function_1] [arg_2]	count=3
class	for ||| query	count=1
function	already ||| spill	count=1
function_arg	substring [arg_2] ||| [function_1] [arg_2]	count=3
class	set a [class_2] ||| [class_1] [class_2]	count=1
class	[class] stepsize=0 1 ||| [class]	count=6
arg	rdd previously saved ||| minpartitions	count=1
module_class	[module_1] broadcast ||| [module_1] [class_2]	count=2
function_arg	transform the [arg_2] ||| [function_1] [arg_2]	count=2
module	elements in one operation ||| core	count=1
function	inherit documentation from its ||| inherit doc	count=1
function	session to use for ||| session	count=1
function	returns a java ||| get java	count=1
function	of users for ||| users	count=1
arg	to fixed [arg_2] ||| [arg_2] [arg_1]	count=1
function	with new specified ||| df	count=1
module	the [module] selectortype or ||| [module]	count=1
function	param with a ||| param	count=1
module_class	[module_1] [class_2] session that has separate ||| [module_1] [class_2]	count=1
module_class	elements [class_2] ||| [module_1] [class_2]	count=2
class	for ||| external	count=1
function_arg	[function_1] [arg_2] ||| [function_1] without unbatching [arg_2]	count=2
function	[function_1] users the ||| [function_1] [function_2]	count=3
class	impurity="variance") ||| gbtregressor	count=1
function	to a local ||| to local	count=3
class	of tree (e g ||| tree	count=1
function	sets vector [function_2] ||| [function_1] [function_2]	count=1
function	based [function_2] ||| [function_2] [function_1]	count=3
module	return an javardd of ||| ml	count=1
class	return ||| model	count=1
arg	a character in matching ||| matching	count=1
function_arg	[function_1] between ||| [arg_2] [function_1]	count=2
function	[function_1] as ||| [function_2] [function_1]	count=1
function	the distributed matrix on ||| matrix	count=1
class	computation ||| binary classification metrics	count=1
function	initial value of weights ||| initial weights	count=1
function	index ||| map partitions with index	count=1
arg	function [arg_2] ||| [arg_1] [arg_2]	count=2
arg	an event time ||| eventtime delaythreshold	count=1
function	use only create a ||| create	count=1
class	return the first ||| py spark streaming	count=1
function	[function_1] [function_2] ||| [function_1] checkpoint [function_2]	count=6
function	each key using ||| by key zerovalue	count=1
module_class	multinomial logistic ||| mllib logistic	count=1
arg	[arg_1] labelcol="label", metricname="f1") ||| [arg_2] [arg_1]	count=1
arg	[arg_1] database ||| [arg_1] [arg_2]	count=4
function	products for all ||| products for	count=2
function	the true label ||| label	count=2
arg	algorithm return the model ||| rdd	count=1
module	the [module] min ||| [module]	count=1
class	instance for params shared ||| params	count=1
function	return sparkcontext which is ||| spark	count=1
module	return the ||| mllib	count=3
class	dump already partitioned data ||| external group by	count=1
class	specifies the underlying output ||| data stream writer	count=1
class	a flume ||| flume	count=1
class	as the spark fair ||| spark context	count=1
function	vector [function_2] ||| [function_1] [function_2]	count=9
module	gets the [module] degree ||| [module]	count=1
arg	the given date ||| date	count=1
class	can be used again ||| query	count=1
function	accumulator's ||| add	count=1
module	gets the [module] numbuckets ||| [module]	count=1
class	block [class_2] ||| [class_2] [class_1]	count=2
module	returns an active ||| sql	count=1
class	[class_1] a data ||| [class_2] [class_1]	count=4
function_arg	[function_1] [arg_2] 0 maxmemoryinmb=256 cachenodeids=false subsamplingrate=1 ||| [function_1] [arg_2]	count=2
function	seed=none): sets [function_2] ||| [function_1] [function_2]	count=2
module_class	this rdd for ||| core rdd	count=1
function	creates or ||| create or	count=2
function_arg	on the [arg_2] ||| [function_1] [arg_2]	count=3
function	rate ||| rate	count=3
module	gets the [module] pattern or ||| [module]	count=1
function	window function ||| window function	count=1
function	the norm of ||| norm	count=1
function_arg	[function_1] inputcol=none outputcol=none) ||| [arg_2] [function_1]	count=1
arg	(from 1 [arg] inclusive) in ||| [arg]	count=1
class	rdd is ||| rdd	count=1
module_class	convert this matrix ||| mllib linalg matrix	count=1
class	data ||| external group	count=1
module_class	for [module_1] [class_2] ||| [module_1] [class_2]	count=2
class	this [class_2] ||| [class_1] [class_2] remember duration	count=2
class	partitioned data ||| external	count=1
function	script [function_2] ||| [function_1] [function_2]	count=3
function	predict values for ||| predict	count=2
function_arg	in parquet [arg_2] ||| [function_1] [arg_2]	count=1
module	memory for this ||| core	count=1
module	returns the ||| mllib	count=1
arg	end exclusive [arg_2] ||| [arg_1] [arg_2]	count=2
class	the [class] ||| spark [class]	count=1
module	[module] subsamplingrate or ||| [module]	count=1
function	under the ||| under	count=2
function	root directory that contains ||| root directory	count=1
function	index of the original ||| map partitions with index	count=1
class	a sparse [class] using either ||| sparse [class]	count=2
function	class inherit documentation from ||| inherit doc	count=1
class	in the rdd into ||| rdd	count=1
class	for this imputer ||| imputer	count=1
class	data into ||| group	count=1
module_class	of binomial [class_2] ||| [module_1] [class_2]	count=8
function	to a [function_2] ||| [function_2] [function_1]	count=14
class	the column standard ||| standard scaler	count=1
module	points from the ||| mllib	count=1
function	[function_1] vector ||| [function_2] [function_1]	count=5
function	of key value pairs ||| key	count=1
class	sets the context ||| streaming context	count=1
module	gets the [module] seed ||| [module]	count=1
class	be ||| streaming query	count=1
class	sets ||| items col	count=1
arg	[arg_1] data ||| [arg_2] [arg_1]	count=6
function	the [function_1] [function_2] ||| [function_1] [function_2]	count=8
class	an rdd ||| rdd	count=4
class	configuration ||| conf	count=4
function	squared distance from ||| squared distance	count=2
class	in this context ||| streaming context	count=1
class	on ||| logistic regression with	count=1
function_arg	[function_1] [arg_2] a [[structtype]] or [[arraytype]] ||| [function_1] [arg_2]	count=1
function	[function_1] a local ||| [function_1] [function_2]	count=2
function	reducebykey to ||| reduce	count=1
module	[module] solver or ||| [module]	count=2
module	number of ||| ml	count=3
class	a paired ||| matrix factorization model	count=1
arg	sql statements ||| f returntype	count=1
class	for cross ||| cross	count=1
function	java [function_2] ||| [function_1] [function_2]	count=2
function	ignore separators ||| ignore	count=1
function	thresholds ||| thresholds	count=2
function	for distinct [function_2] ||| [function_2] [function_1]	count=8
function	each rdd contains ||| by	count=1
function	summary statistics ||| summary statistics	count=2
module	[module] elasticnetparam ||| [module]	count=1
class	matrix to the ||| matrix	count=2
class	a randomly [class_2] ||| [class_2] [class_1]	count=2
class	the accumulator's data ||| accumulator	count=1
function	vectors saved ||| vectors	count=1
module	memory ||| core	count=1
class	the :class statcounter members ||| stat counter	count=1
module_class	[module_1] result ||| [module_1] test [class_2] p value	count=1
class	for which predictions ||| isotonic regression model	count=1
module	the [module] featurescol or ||| [module]	count=1
arg	the string column to [arg_1] [arg_2] ||| [arg_1] [arg_2]	count=1
class	wait for new terminations ||| streaming query manager	count=1
class	:class dataframe to a ||| frame	count=1
function	a given ||| param	count=1
module_class	creates a copy [module_1] [class_2] ||| [module_1] [class_2] vs rest copy	count=3
module	all the ||| core	count=1
arg	a given product and ||| product	count=1
class	model ||| matrix factorization model	count=1
class	as a :class dataframe ||| data frame reader	count=1
arg	scale ||| offset scale	count=1
class	to ||| streaming	count=1
arg	of the dstreams ||| dstreams transformfunc	count=1
module_class	creates a [module_1] [class_2] ||| [module_1] train validation [class_2] copy	count=6
function_arg	rate for [arg_2] ||| [arg_2] [function_1]	count=1
function	minimum [function_2] ||| [function_2] [function_1]	count=4
module	compute ||| linalg	count=2
module	gets the [module] selectortype or ||| [module]	count=1
arg	python topicandpartition to ||| topic partition	count=1
function	usercol ||| user col	count=1
class	[class_1] by key ||| [class_1] [class_2]	count=1
function	object by pyrolite whenever ||| object rdd	count=1
function	columns ||| columns to	count=2
class	:class dataframe ||| data frame writer	count=5
module	the [module] initmode ||| [module]	count=1
module	return a new ||| core	count=1
class	this matrix ||| matrix	count=2
module	of column ||| sql	count=2
function	to select filter ||| selected features	count=1
function	[function_1] java ||| [function_1] [function_2]	count=14
arg	get the ||| num	count=1
module_class	gets the [module_1] [class_2] or its default value ||| [module_1] [class_2] discretizer get handle invalid	count=1
arg	by codeblock co ||| cls co	count=2
function_arg	between [arg_2] ||| [arg_2] [function_1]	count=2
class	tests ||| tests	count=1
class	so that ||| query manager	count=1
function	convert ||| to	count=2
class	rdd of ||| rdd	count=4
function	the python direct kafka ||| kafka direct	count=5
function	in a text ||| text	count=1
function	single script ||| single script	count=2
module_class	creates a copy [module_1] [class_2] and some ||| [module_1] [class_2] copy	count=4
function	based ||| based	count=1
module	the [module] fdr or ||| [module]	count=1
function	a csv ||| csv	count=2
arg	into class ||| cls	count=1
module_class	[module_1] rdd ||| [module_1] [class_2]	count=36
arg	incoming dstream ||| dstream	count=3
arg	the correlation of two ||| col1 col2 method	count=1
class	original ||| scaler model	count=1
module	name ||| ml	count=1
class	depth of tree (e ||| tree	count=1
function	the area under ||| area under	count=4
class	train a [class_1] [class_2] using stochastic gradient descent ||| mllib [class_1] [class_2] with	count=1
function	string name ||| has param	count=1
function	join of ||| join	count=2
arg	outputcol=none handleinvalid="error") ||| handleinvalid	count=4
module	dump ||| core	count=1
class	or more examples ||| decision tree model	count=1
class	used again to ||| manager	count=1
function	build the union ||| union	count=1
function	returns ||| sql	count=1
class	so that :func awaitanytermination() ||| streaming	count=1
class	[class_1] model with ||| [class_2] [class_1]	count=1
function	add a py ||| add py file	count=2
arg	creates a ||| sparkcontext sparksession	count=1
class	which predictions are ||| regression model	count=1
class	vectors which ||| vector	count=1
function_arg	a text file ||| text path	count=1
function	[function_1] storage type ||| [function_1] [function_2]	count=2
function	[function_1] accuracy ||| [function_2] [function_1]	count=1
class	training ||| generalized linear regression training summary	count=1
module	a param with a ||| ml param	count=1
class	a model ||| linear regression	count=1
function	of features [function_2] ||| [function_2] [function_1]	count=2
function	create a java array ||| new java array	count=1
class	so ||| manager	count=1
function	only create ||| create	count=1
class	type ||| data type	count=1
function	and commutative reduce function ||| reduce	count=1
arg	saved using rdd ||| sc path minpartitions	count=1
class	dump ||| group	count=1
module	returns the ||| ml	count=1
class	param with a given ||| params	count=1
function	pearson's chi-squared ||| chi sq	count=1
function	all the objects ||| object	count=1
class	returned ||| test case	count=1
function	test for ||| test	count=1
arg	set numtopfeature ||| numtopfeatures	count=1
function	find the spark_home ||| find spark home	count=3
class	the input data ||| data stream reader	count=2
arg	[arg_1] every element ||| [arg_2] [arg_1]	count=6
arg	occurrence of substr ||| substr	count=1
module_class	[module_1] group ||| [module_1] [class_2]	count=2
function	positive rate ||| positive rate	count=6
class	[class_1] which gives ||| [class_1] [class_2]	count=4
class	of this [class] in a ||| [class]	count=2
function	containing a json ||| from json	count=1
module	[module] metricname or ||| [module]	count=2
class	the objects ||| merger	count=1
arg	featurescol="features", labelcol="label", [arg_2] ||| [arg_1] [arg_2] maxdepth	count=4
function	function ||| function	count=5
function	[function_1] [function_2] ||| core [function_1] [function_2]	count=2
function	find the [function_2] ||| [function_2] [function_1]	count=1
function	the sum ||| sum	count=1
function	the [function] from ||| [function]	count=1
function	py or zip dependency ||| py	count=1
arg	len with [arg_2] ||| [arg_2] [arg_1]	count=1
function	transform the rdd ||| transform	count=1
function	newline-delimited json ||| json	count=1
function	the selector ||| selector	count=1
function	rdd contains the count ||| count	count=1
function	get a local ||| local	count=1
module_class	containing elements from ||| core spark context	count=1
class	maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 losstype="logistic", ||| gbtclassifier	count=2
function	py or zip ||| py file	count=1
function	[function_1] of nodes ||| [function_1] [function_2]	count=1
module	[module_1] [module_2] a ||| [module_2] [module_1]	count=2
module	representation [module_2] ||| [module_1] [module_2]	count=2
module	gets the [module] itemcol ||| [module]	count=1
module	arrays of ||| ml	count=1
arg	co ||| co	count=1
function	a param with a ||| has param	count=1
function	wait ||| reset	count=1
function	memory for ||| object	count=1
class	specifies the input ||| stream reader	count=1
function	[function_1] has started ||| [function_1] [function_2]	count=2
module	for every ||| mllib	count=1
function	by applying [function] ||| [function]	count=1
function	pattern ||| pattern	count=1
function	[function_1] count ||| [function_2] by [function_1]	count=2
function	as the [function_2] ||| [function_2] [function_1]	count=1
class	this instance's params ||| params	count=1
module	wait for new ||| sql	count=1
arg	codeblock [arg_2] ||| [arg_2] [arg_1]	count=1
arg	multiclass ||| numclasses	count=1
function	ignore separators [function_2] ||| [function_2] [function_1]	count=3
arg	data ||| data iterations step	count=2
module	in one ||| core	count=1
class	stream ||| stream	count=5
class	to wait for ||| query manager	count=1
module	out into external ||| sql	count=1
class	random ||| word2vec	count=1
class	events from flume ||| flume utils	count=1
function	dataframe as pandas pandas ||| to pandas	count=1
class	grid ||| grid	count=1
module	adds a ||| core	count=1
class	for this ||| merger	count=1
function_arg	receiver has [arg_2] ||| [arg_2] [function_1]	count=2
function	spark_home ||| spark home	count=1
class	dump already partitioned data ||| group by	count=1
class	used again to wait ||| streaming	count=1
arg	[arg] this ||| [arg]	count=2
arg	named table ||| table column	count=1
function	all values as a ||| all	count=1
function	sets the threshold that ||| set threshold	count=1
function	[function_1] transform get ||| [function_2] [function_1]	count=3
module	[module] minconfidence ||| [module]	count=1
function	outputcol=none [function] ||| [function]	count=1
class	this ||| merger	count=1
arg	[arg_1] [arg_2] of ||| [arg_1] [arg_2]	count=14
arg	string in the ||| s	count=1
function	of jobs has started ||| started	count=1
function	dump already ||| spill	count=1
arg	the datatype ||| datatype	count=1
function	for ||| object size	count=1
arg	featurescol="features", ||| featurescol	count=24
function	a param with ||| param	count=1
function	checkpoint data or ||| get or	count=1
arg	rep ||| str pattern replacement	count=1
class	model ||| logistic regression	count=1
class	data into disks ||| group by	count=1
function	a given string name ||| has param	count=1
function	model predicts ||| predict on model	count=1
module	[module] k or ||| [module]	count=1
function	date ||| date	count=1
class	in tree including ||| tree model	count=1
function	test that the ||| test training	count=1
function	reducebykey to each rdd ||| reduce	count=1
class	category ||| multiclass	count=3
function	rows [function_2] ||| [function_1] [function_2]	count=2
class	tree including ||| decision tree model	count=2
function_arg	window function [arg_2] ||| [arg_2] [function_1]	count=2
module	gets the [module] mintokenlength or ||| [module]	count=1
module	[module] statement ||| [module]	count=1
module	gets the [module] minconfidence ||| [module]	count=1
function	extract the year ||| year	count=1
function	that user ||| user	count=1
arg	later than ||| dayofweek	count=1
arg	specified database ||| dbname	count=2
function	assign a [function] to this ||| [function]	count=1
module	two ||| mllib linalg	count=2
function	the ids of ||| stage ids	count=1
arg	table ||| tablename	count=2
function	of deserialized batches lists ||| without unbatching	count=1
function	functions registered in ||| functions	count=1
function	test this should ||| test	count=1
function_arg	option for the ||| option key value	count=1
function	params for ||| params	count=4
arg	[arg] belongs ||| [arg]	count=3
function	this instance contains ||| has param	count=1
class	which is ||| logistic regression summary	count=1
function_arg	[function_1] reference ||| [arg_2] [function_1]	count=1
function	the probability ||| probability	count=1
function	of columns for the ||| columns	count=1
class	the given parameters in ||| builder	count=1
function	schema in the tree ||| schema	count=1
function	until any of ||| any	count=1
function	creates or replaces ||| create or replace	count=3
arg	representing ||| sqlquery	count=1
class	vector to the ||| sparse vector	count=1
module	into ||| core	count=1
class	[class] bug fixed ||| evaluator [class]	count=2
function	minsupport ||| min support	count=1
class	this ||| model	count=2
function	[function_1] between ||| [function_2] [function_1]	count=4
arg	values ||| values	count=2
function	as [function_2] ||| [function_1] [function_2]	count=4
module	by a ||| core	count=2
function	max ||| max	count=2
module	spill by ||| core	count=1
class	[class_1] aggregationdepth=2): ||| [class_2] [class_1]	count=6
function	fields threshold [function_2] ||| [function_2] by [function_1]	count=2
class	sets ||| stop words remover	count=3
module	size of number of ||| ml	count=1
arg	featurescol="features", labelcol="label", [arg_2] ||| [arg_1] [arg_2] maxiter	count=3
function	featurescol ||| features col	count=1
module	[module] mindf or ||| [module]	count=1
function	[function_1] or replaces ||| [function_2] [function_1]	count=1
function	ids of [function_2] ||| [function_2] [function_1]	count=4
function	stream api with start ||| stream from	count=1
function	fast ||| heappushpop	count=1
function_arg	the [function_1] [arg_2] delim ||| [function_1] [arg_2]	count=2
module	a param with a ||| ml	count=1
function_arg	[function_1] of another ||| streaming dstream [function_1] [arg_2]	count=3
arg	each ||| func numpartitions partitionfunc	count=1
function_arg	configuration property ||| conf key defaultvalue	count=1
arg	them with extra ||| extra	count=1
module	the given ||| mllib	count=1
module	timezone returns ||| sql	count=1
class	gradient-boosted [class_2] ||| [class_2] [class_1]	count=2
function	top features ||| top features	count=2
class	all the objects ||| merger	count=1
class	create ||| submit tests	count=1
function	gets [function_2] ||| [function_2] [function_1]	count=1
class	this vector to the ||| sparse vector	count=1
class	new accumulator with a ||| accumulator	count=1
module	the [module] predictioncol ||| [module]	count=1
function_arg	unifying data [arg_2] ||| streaming dstream [function_1] [arg_2]	count=5
module_class	an [class_2] ||| [module_1] [class_2]	count=2
module	fields in ||| sql	count=1
function	or its ||| col	count=1
function	names of ||| names	count=1
class	compute ||| rdd	count=1
module	the [module] gaps or ||| [module]	count=1
module	gets the [module] blocksize or ||| [module]	count=1
function	converts vector columns ||| convert vector columns	count=2
function	outer join of ||| outer join	count=4
function	[function_1] rate ||| [function_2] [function_1]	count=6
arg	a dictionary a ||| size	count=4
class	enable 'with [class] ||| [class]	count=1
module_class	the column [class_2] ||| [module_1] [class_2]	count=2
function	[function_1] ml params ||| [function_1] [function_2]	count=1
class	for ||| streaming query manager	count=2
function	extract a ||| regexp extract	count=1
function	close to ||| parameter accuracy	count=1
function	to ||| add	count=1
class	already partitioned data ||| external group by	count=1
function	into a python ||| from	count=1
function	brackets ||| brackets	count=1
function	blocks in ||| blocks	count=1
class	vector to the ||| vector	count=2
arg	of a word ||| word num	count=1
arg	after which the centroids ||| timeunit	count=1
function	[function_1] a java ||| [function_2] [function_1]	count=14
function	the offsetrange ||| ranges	count=1
function	the driver as ||| local	count=1
arg	[arg_1] each element ||| [arg_2] [arg_1]	count=6
class	linear [class_2] ||| [class_2] [class_1]	count=3
class	to ||| streaming query	count=1
module_class	shortcut of [class_2] ||| [module_1] [class_2]	count=8
class	gradient-boosted [class_2] ||| [class_1] [class_2]	count=2
module	data into ||| core	count=1
function	[function_1] precision ||| [function_2] by [function_1]	count=4
arg	true iff [arg] is null ||| [arg]	count=1
class	max ||| max abs scaler model	count=1
function	partitioned data ||| spill	count=1
function	[function_1] a coordinatematrix ||| [function_1] [function_2]	count=2
function_arg	[function_1] value of ||| [arg_2] [function_1]	count=1
function	start offset specified ||| offset	count=1
class	a given ||| params	count=1
class	stream ||| data stream	count=1
class	[class_1] which gives ||| [class_2] [class_1]	count=22
function	partition of ||| partition	count=1
function	create a ||| create	count=3
function	libsvm format into label ||| parse libsvm	count=1
class	error [class] ||| [class]	count=9
function_arg	orc [arg_2] ||| [function_1] path mode partitionby [arg_2]	count=3
function	gets ||| get estimator param	count=1
class	data of a ||| data	count=1
module	list of numpy arrays ||| ml	count=1
arg	saved using rdd ||| minpartitions	count=1
class	[class] given initial ||| [class]	count=2
module	gets the [module] intermediatestoragelevel or ||| [module]	count=1
function	[function_1] global ||| [function_1] [function_2]	count=2
arg	column [arg_2] ||| [arg_2] [arg_1]	count=2
function	[function] into py4j ||| install [function]	count=1
function	the deviance for ||| deviance	count=1
arg	dstream and [arg] dstream ||| [arg]	count=1
module	already partitioned data ||| core	count=1
function_arg	[function_1] [arg_2] solver="auto", weightcol=none aggregationdepth=2) ||| [function_1] [arg_2]	count=2
arg	codeblock co ||| cls co	count=2
module	for this ||| core	count=1
function	the embedded params to ||| params to	count=1
function	[function_1] blocks ||| [function_1] [function_2]	count=3
function	half ||| half	count=1
class	[class_1] with ||| [class_2] [class_1]	count=1
function	a 'new api' hadoop ||| new apihadoop	count=1
module	the documentation of ||| ml	count=1
function	creates or [function_2] ||| [function_1] [function_2]	count=2
function	[function_1] parammap ||| [function_2] [function_1]	count=12
arg	reduced into ||| shuffle	count=1
function	start ||| range between	count=1
class	the unique [class_1] [class_2] persists across restarts from ||| [class_1] [class_2] id	count=1
function	file with ||| file	count=1
module	gets the [module] isotonic or ||| [module]	count=1
function	batch of jobs ||| batch	count=2
function_arg	[function_1] mincount the ||| [function_1] min count [arg_2]	count=1
class	so that ||| streaming	count=1
function	of rows ||| rows	count=3
function	to conversion ||| conversion	count=1
function	window over ||| window	count=1
class	rdd[vector] ||| java	count=1
function	[function_1] has completed ||| [function_1] [function_2]	count=2
function	the area [function_2] ||| [function_1] [function_2]	count=5
function	checkpoint the dstream ||| checkpoint	count=1
class	the underlying output ||| data frame writer	count=1
module	gets the [module] variancepower or ||| [module]	count=1
class	queries so that :func ||| manager	count=1
function	bias ||| bias	count=1
class	this [class_2] ||| [class_2] [class_1]	count=5
class	sets the context ||| context	count=1
class	__init__(self ||| bisecting kmeans	count=1
function	until termination ||| total	count=2
class	context to ||| context	count=2
module	defined on the class ||| param	count=1
function_arg	[function_1] representing the ||| [function_1] [arg_2]	count=1
function	single script on ||| single script on	count=3
class	result ||| result	count=1
function	the length of ||| length	count=1
class	unique id [class] ||| [class]	count=2
function	checkpointed and materialized ||| checkpointed	count=1
arg	the function func ||| func	count=1
function	the minimum [function_2] ||| [function_2] [function_1]	count=4
function_arg	text [arg_2] ||| [function_1] [arg_2]	count=1
function	the length ||| length	count=1
function	hook [function] into py4j ||| install [function]	count=1
class	so that :func awaitanytermination() ||| query manager	count=1
class	every feature ||| linear model	count=1
function	find the minimum ||| min	count=1
class	on a ||| regression with	count=1
function	[function_1] list ||| [function_2] [function_1]	count=4
arg	[arg_1] featurescol="features", ||| [arg_2] [arg_1]	count=2
function	the key-value ||| map values	count=1
arg	this obj assume that ||| obj	count=1
arg	given product and returns ||| product	count=1
class	logistic regression [class_2] ||| [class_1] [class_2]	count=2
class	squared error which is ||| regression	count=1
function	[function_1] the spark_home ||| [function_2] [function_1]	count=1
class	threshold=0 0 weightcol=none aggregationdepth=2): ||| linear svc	count=2
class	can be used ||| manager	count=1
class	column ||| standard scaler model	count=1
function	restore an object of ||| restore	count=1
function	the new ||| save as new	count=1
module	the [module] mininstancespernode ||| [module]	count=1
class	much of memory for ||| external merger	count=1
function	to this accumulator's ||| add	count=1
function_arg	[function_1] experimental ||| [function_1] [arg_2]	count=1
arg	later than the ||| dayofweek	count=1
function	mean squared [function_2] ||| [function_1] [function_2]	count=2
function	casesensitive ||| case sensitive	count=1
class	numtrees=20 featuresubsetstrategy="auto", ||| random forest	count=2
function	python parammap into a ||| map to	count=1
function	vocabulary [function] number of ||| vocab [function]	count=1
class	[class_1] returning the ||| [class_2] [class_1]	count=1
class	column standard ||| standard scaler	count=1
class	as the spark fair ||| spark	count=1
module	function "func" and a ||| core	count=1
class	on a ||| streaming logistic regression with	count=1
function	call java ||| call java	count=2
function	[function_1] converter to ||| [function_1] [function_2]	count=1
function	of names of tables ||| table names	count=1
class	that all the ||| external merger	count=1
module	the [module] maxiter or ||| [module]	count=1
class	for new ||| streaming query manager	count=1
class	variance and count of ||| rdd	count=1
class	this tokenizer ||| tokenizer	count=1
class	of the :class dataframe ||| frame	count=1
function	initial value of ||| initial	count=1
arg	database table via ||| url table mode	count=1
function	[function_1] a cluster ||| [function_1] [function_2]	count=4
function	a boolean ||| boolean	count=1
function	type of the ||| type	count=1
function	square root of the ||| root	count=1
function	returns the threshold if ||| threshold	count=1
arg	rdd, ||| schema samplingratio verifyschema	count=1
class	that :func ||| streaming query manager	count=1
function_arg	[function_1] observed ||| [function_1] [arg_2]	count=2
function	python direct kafka stream ||| kafka direct stream	count=5
module_class	return a [class_2] ||| [module_1] [class_2]	count=4
function	[function_1] driver returns ||| [function_1] [function_2]	count=1
function_arg	[function_1] for ||| [function_1] [arg_2]	count=1
function	[function_1] key ||| [function_1] [function_2] zerovalue	count=5
function	of value count ||| count by value	count=1
class	a randomly ||| cross validator model	count=1
function	the standard deviation ||| stdev	count=1
class	model with weights already ||| linear regression with	count=1
function_arg	[function_1] centroids according ||| [function_1] data [arg_2]	count=1
module	return a callsite ||| core	count=1
module	the [module] isotonic ||| [module]	count=1
function	items for columns ||| items	count=1
function	hook [function_1] [function_2] into py4j which could ||| sql install [function_1] [function_2]	count=1
arg	with the specified schema ||| schema options	count=1
function	with a given string ||| has param	count=1
class	matrix ||| matrix	count=6
function	[function_1] testing ||| [function_2] [function_1]	count=7
function_arg	the minimum [arg_2] ||| [function_1] [arg_2]	count=1
module	the [module] max or ||| [module]	count=1
class	'with sparksession builder getorcreate() [class_1] [class_2] app' syntax ||| sql [class_1] [class_2] enter	count=1
function	squared [function_2] ||| [function_2] [function_1]	count=4
class	for params ||| params	count=1
function_arg	[function_1] observed data ||| [arg_2] [function_1]	count=2
arg	which the centroids of ||| timeunit	count=1
function	given join expression ||| join	count=1
arg	column after position pos ||| pos	count=1
arg	the observed [arg_2] ||| [arg_1] [arg_2]	count=1
class	[class] key-value pairs ||| [class]	count=3
function	a dependency ||| dependency	count=3
function	an rdd ||| gamma vector rdd	count=1
class	the stream ||| data stream	count=1
function_arg	[function_1] saved ||| [function_1] [arg_2]	count=3
function_arg	[function_1] version of ||| [function_1] [arg_2]	count=1
function	id ||| id	count=2
arg	table and update the ||| tablename	count=1
class	specifies how data ||| data stream	count=1
class	the dataframe in a ||| data frame	count=1
function	[function] create and ||| [function]	count=3
function	separators inside brackets ||| brackets	count=1
arg	to each element ||| preservespartitioning	count=2
function	return the topics ||| topics	count=1
module	of each instance as ||| ml	count=1
arg	[arg_1] numfolds=3 seed=none) ||| [arg_2] [arg_1]	count=1
class	partitioned ||| group	count=1
module_class	gets [module_1] [class_2] its default value ||| [module_1] [class_2] discretizer get handle invalid	count=1
function	the column mean ||| mean	count=1
module	wait ||| sql	count=1
function_arg	find synonyms [arg_2] ||| [function_1] [arg_2]	count=1
class	queries so that :func ||| query manager	count=1
class	disks ||| external group	count=1
module	the [module] inputcol or ||| [module]	count=1
function	numfeatures ||| num features	count=1
class	randomly generated uid ||| cross validator	count=1
function	terminations ||| reset	count=1
module	classification problem in multinomial ||| mllib	count=1
module_class	[module_1] random ||| [module_1] [class_2]	count=2
arg	either recreate ||| checkpointpath setupfunc	count=1
function_arg	the group id ||| group groupid	count=1
class	this [class] using the ||| [class]	count=1
class	[class_1] k-means ||| [class_1] [class_2]	count=1
module	the mean variance ||| core	count=1
function	initial value of weights ||| set initial weights	count=1
class	data ||| external group by	count=2
function_arg	receiver [arg_2] ||| [function_1] started [arg_2]	count=1
class	name in [class] ||| [class]	count=1
arg	id ||| groupid	count=1
module	the [module] stepsize ||| [module]	count=2
function	a py ||| py file	count=1
module	[module] percentile ||| [module]	count=1
class	a ||| params	count=2
function	the profile stats ||| profiles	count=3
class	for new terminations ||| manager	count=1
function	[function] inputformat ||| [function]	count=2
class	cluster ||| bisecting kmeans model	count=2
function	length of ||| length	count=1
arg	[arg_1] maxiter=100 ||| [arg_1] [arg_2]	count=3
arg	database table via ||| url table mode properties	count=1
function_arg	[function_1] property ||| [function_1] key [arg_2]	count=1
class	vectors ||| vector indexer	count=1
module	the [module] seed ||| [module]	count=1
module	of type distributedldamodel ||| ml	count=1
arg	i d samples drawn ||| numrows numcols numpartitions	count=2
module	awaitanytermination() ||| sql	count=1
arg	or list in each ||| oneatatime default	count=1
class	is ||| basic profiler	count=1
arg	in the column ||| col	count=1
function	orc files returning the ||| orc	count=1
function	function including lambda function ||| function	count=2
class	every feature ||| linear	count=1
class	impurity="variance", seed=none ||| decision tree regressor	count=1
rep	of write() [function_arg_2] ||| [module_class_1] [function_arg_2]	count=8
class	[class_1] [class_2] - ||| [class_1] [class_2] subtract	count=1
function	to [function_2] ||| [function_1] [function_2]	count=18
module_class	[module_1] data ||| [module_1] [class_2]	count=12
class	already partitioned ||| group by	count=1
function	file to which ||| file	count=1
class	sets ||| forest params	count=2
module_class	convert this vector ||| mllib linalg vector	count=1
function	of months [function_2] ||| [function_1] [function_2]	count=1
class	:func awaitanytermination() can ||| streaming	count=1
arg	outputformat ||| outputformatclass keyclass valueclass	count=1
module	the [module] seed or ||| [module]	count=1
class	random ||| random	count=1
arg	tests whether this ||| paramname	count=1
class	original ||| max scaler model	count=1
function	how ||| object	count=1
class	model trained ||| model	count=1
class	in tree [class_2] ||| [class_2] [class_1]	count=1
function	the values of each ||| by	count=1
class	batches ||| linear algorithm	count=1
function	item ||| item	count=1
function	wait ||| termination or timeout	count=1
function	java storagelevel ||| java	count=1
class	paired ||| matrix factorization model	count=1
arg	correlation ||| col1 col2 method	count=1
function_arg	[function_1] reference ||| [function_1] [arg_2]	count=1
function	family ||| family	count=1
module	this instance ||| ml	count=1
function	to save ||| save	count=1
arg	ensuring all received data ||| stopsparkcontext stopgracefully	count=1
class	generate ||| regression with sgdtests	count=1
function	java array ||| java array	count=1
class	the ||| standard scaler	count=1
function	stream api with start ||| stream	count=1
arg	start to [arg_2] ||| [arg_2] [arg_1]	count=5
module	gets the [module] mintokenlength ||| [module]	count=1
class	return the ||| standard	count=1
arg	step [arg_2] ||| [arg_1] [arg_2]	count=1
function	foreachrdd get ||| foreach get	count=2
function_arg	synonyms [arg_2] ||| [arg_2] [function_1]	count=1
module	of ||| sql	count=4
arg	[arg_1] count ||| [arg_1] [arg_2]	count=2
function	a receiver has ||| receiver	count=2
arg	for this obj assume ||| obj	count=1
function	sample without ||| sample	count=1
class	this [class] ||| [class]	count=4
arg	format ||| format	count=1
function	the greatest value of ||| greatest	count=1
module_class	[module_1] new rdd ||| [module_1] [class_2]	count=2
function	scale ||| scale	count=1
module	a ||| mllib	count=4
module	[module] mindf ||| [module]	count=1
module	gets the [module] censorcol or ||| [module]	count=1
arg	given path ||| sc path	count=6
class	for each training ||| summary	count=1
function	model params are ||| model params	count=2
module	the [module] names or ||| [module]	count=1
module	other from this ||| linalg	count=1
function	a python parammap into ||| param map to	count=1
function_arg	weighted averaged f-measure ||| weighted fmeasure beta	count=1
class	external sort when the ||| external	count=1
module	to wait for ||| sql	count=1
arg	metricname="f1") ||| metricname	count=1
function	minutes of a ||| minute	count=1
arg	string column [arg] times and ||| col [arg]	count=1
function_arg	test [arg_2] ||| [function_1] [arg_2]	count=7
module	copies [module] a ||| [module]	count=1
class	batches ||| streaming linear algorithm	count=1
class	[class] matrix info ||| [class]	count=2
arg	external database table ||| table mode	count=1
arg	applies [arg] to ||| [arg]	count=2
class	this instance contains a ||| params	count=1
class	"predictions" which gives ||| linear regression summary	count=9
function	or [function_2] ||| [function_1] [function_2]	count=3
module_class	[module_1] a ||| [module_1] [class_2] session	count=1
function	under ||| under	count=2
class	data of ||| data	count=1
class	gaussians in mixture ||| gaussian mixture model	count=1
class	new terminations ||| query manager	count=1
arg	maxiter=100 ||| maxiter	count=1
function	key-value ||| map values	count=1
arg	the user-supplied param ||| param	count=1
module	separate arrays of indices ||| ml	count=1
class	optional default value and ||| params	count=1
class	python rdd of key-value ||| rdd	count=3
arg	specified partitioner ||| numpartitions partitionfunc	count=1
function	[function_1] commutative reduce ||| [function_2] [function_1]	count=10
class	estimated coefficients and ||| generalized linear regression training summary	count=1
class	that all ||| external merger	count=1
module	the [module] numbuckets ||| [module]	count=1
arg	key function ||| key	count=1
class	update ||| kmeans model	count=1
function_arg	into type datatype ||| cast datatype	count=1
class	can be used ||| query	count=1
function	the stage ||| stage	count=2
class	vector ||| sparse vector	count=1
function	java system ||| system	count=1
module_class	mark this rdd ||| core rdd	count=1
class	param ||| params	count=1
function	to wait ||| reset	count=1
module_class	creates a [module_1] [class_2] uid and some ||| [module_1] [class_2] copy	count=4
function	a new java ||| new java	count=2
class	sets the accumulator's ||| accumulator	count=1
function	parquet files returning ||| parquet	count=1
module	pairs or two ||| mllib linalg	count=1
class	with this dataframe ||| data frame	count=1
module	this instance contains a ||| ml param	count=1
class	memory for this ||| external merger	count=1
function	with a ||| has	count=1
function	until any ||| any	count=1
function	wait for new ||| reset	count=1
module	second is an array ||| mllib	count=1
class	values ||| standard scaler model	count=1
function	[function_1] each key ||| [function_1] [function_2] zerovalue	count=1
function	gaps ||| gaps	count=1
class	dependent variable given a ||| linear regression model base	count=1
module	wrapper of ||| ml	count=5
class	stream query ||| stream writer	count=1
class	which predictions are ||| isotonic regression model	count=1
class	distributed [class_2] ||| [class_1] [class_2]	count=1
arg	centroids according to data ||| data decayfactor timeunit	count=1
class	densevector with singular ||| singular	count=1
module	data into disks ||| core	count=1
arg	version ||| heap	count=1
class	[class_1] model on ||| [class_2] [class_1]	count=2
class	the underlying output ||| data stream writer	count=1
arg	scalingvec=none ||| scalingvec	count=1
class	the stream ||| stream	count=1
arg	given user [arg_2] ||| [arg_2] [arg_1]	count=1
function	index of the original ||| index	count=1
arg	count ||| count	count=1
class	returns a paired rdd ||| matrix factorization model	count=1
function	package ||| package	count=2
class	or ||| hashing tf	count=1
class	sets ||| regression evaluator	count=1
class	all the objects ||| external merger	count=1
class	numtrees=20 ||| random forest	count=3
class	__init__(self ||| hash lsh	count=1
class	determination ||| regression metrics	count=1
class	the [class_1] [class_2] persists across restarts from ||| [class_1] [class_2]	count=1
function	stage [function_2] ||| [function_1] [function_2] stageid	count=1
arg	spark sink deployed on ||| maxbatchsize	count=1
class	model ||| tree model	count=1
function	how ||| size	count=1
function	the file to ||| file	count=1
function	computes the levenshtein distance ||| levenshtein	count=1
arg	[arg_1] labelcol="label", predictioncol="prediction", ||| [arg_1] [arg_2] maxiter	count=3
function_arg	[function_1] data sampled ||| [arg_2] [function_1]	count=1
arg	or list ||| oneatatime default	count=1
function	get a local ||| get local	count=1
function	[function_1] group ||| [function_2] [function_1]	count=1
function_arg	test for data ||| test data distname	count=2
arg	each rdds into ||| dstream n	count=1
function	losstype ||| loss type	count=2
module	[module] variancecol or ||| [module]	count=1
class	session ||| session	count=1
class	data into disks ||| group	count=1
function	a converter to ||| converter	count=1
class	[class_1] k-means ||| [class_2] [class_1]	count=1
class	again to ||| query manager	count=1
function	[function_1] on a ||| [function_2] [function_1]	count=3
class	rdd for ||| rdd	count=1
function_arg	converts [arg_2] ||| [function_1] json [arg_2]	count=3
module	[module] estimatorparammaps ||| [module]	count=1
function	creates an [function_2] ||| [function_1] [function_2]	count=1
class	content of the dataframe ||| data frame writer	count=1
function	pretty printing ||| str	count=1
function	ignore separators inside brackets ||| ignore brackets	count=1
class	rating for ||| matrix factorization	count=1
module	the [module] tolowercase ||| [module]	count=1
function	offset specified ||| offset	count=1
function_arg	minimum [arg_2] ||| [arg_2] [function_1]	count=1
function	the termination of ||| termination	count=1
arg	url url and connection ||| url	count=1
function_arg	[function_1] centroids ||| [function_1] data decayfactor [arg_2]	count=1
class	feature selection by ||| chi sq selector	count=4
function	a [function_1] [function_2] ||| [function_2] [function_1]	count=6
class	linear [class_2] ||| mllib [class_1] [class_2] with	count=2
module	the [module] thresholds ||| [module]	count=1
function	sort the ||| sort result	count=1
arg	values ||| size values	count=1
function	inside brackets ||| brackets split	count=1
arg	n rows to the ||| n truncate	count=1
function	that ||| object	count=1
class	[class_1] which ||| [class_1] [class_2]	count=4
function	a new ||| new	count=1
arg	count to the number ||| num	count=1
function	of names ||| table names	count=1
class	for ||| query manager	count=2
arg	labelcol="label", predictioncol="prediction", maxiter=100 tol=1e-6 ||| labelcol predictioncol maxiter	count=1
function	ordered in ascending order ||| take ordered	count=1
module	of nonzero ||| ml	count=1
function	gets [function_2] ||| [function_1] [function_2]	count=1
function	params are set correctly ||| params	count=1
arg	the spark sink deployed ||| maxbatchsize	count=1
function	standard deviation ||| stdev	count=1
arg	[arg_1] path ||| [arg_1] [arg_2]	count=6
class	instance with a randomly ||| split	count=2
function	single script [function_2] ||| [function_2] [function_1]	count=3
class	awaitanytermination() ||| manager	count=1
function_arg	[function_1] been ||| [function_1] started [arg_2]	count=2
class	randomly generated ||| validation split model	count=1
function	return the [function] of this ||| [function]	count=1
module_class	in this rdd ||| core rdd	count=2
class	pipelinemodel ||| pipeline model	count=2
function	[function_1] items ||| [function_1] [function_2]	count=1
arg	add the new item ||| item	count=1
class	vectors which this ||| vector indexer	count=1
function	ignore ||| ignore	count=2
module	[module] statement or ||| [module]	count=1
function_arg	[function_1] to fixed ||| [arg_2] [function_1]	count=2
module	defined on the ||| ml param	count=1
function	map or ||| or	count=1
function	the old hadoop ||| hadoop	count=1
function	types ||| type	count=1
module	inputformat [module] ||| [module]	count=2
function	threshold precision ||| precision by threshold	count=1
module	list of indices ||| ml	count=1
class	[class_1] numtrees=20 featuresubsetstrategy="auto", ||| [class_2] [class_1]	count=16
function	standardization ||| standardization	count=1
class	the param ||| param	count=1
class	a [class] ||| [class]	count=6
class	the accumulator's data ||| accumulator param	count=1
class	and parent ||| persistence test	count=1
class	for which ||| isotonic regression model	count=1
class	sets ||| gaussian mixture	count=1
function	[function_1] ml params ||| [function_2] [function_1]	count=1
class	the stream query ||| data stream writer	count=1
class	of parallelism [class] when ||| [class]	count=1
function	of columns ||| cols	count=1
function_arg	[function_1] value ||| [arg_2] [function_1]	count=1
class	sets ||| aggregation depth	count=1
function	of trees in ||| trees	count=1
class	decision tree ||| decision tree	count=3
arg	table ||| table mode	count=1
function	returns the greatest value ||| greatest	count=1
module	[module] squared ||| [module]	count=3
function	a java udf ||| java	count=1
function	of tables/views in ||| tables	count=1
function	jvm seq of ||| seq	count=1
class	again to wait for ||| manager	count=1
arg	of col1 ||| col1	count=1
arg	database table ||| table	count=1
arg	param ||| param	count=4
arg	version of a heappush ||| heap	count=1
arg	k=2 ||| k	count=2
function	recovers all [function_2] ||| [function_1] [function_2]	count=1
arg	saves the content of ||| name format mode partitionby	count=1
arg	[arg_1] path ||| [arg_2] mode partitionby [arg_1]	count=2
module	the [module] variancepower or ||| [module]	count=1
arg	the sqltype() into class ||| cls	count=1
module_class	underlying [class_2] ||| [module_1] [class_2]	count=24
function	[function_1] outer join ||| [function_1] [function_2]	count=6
function	to a [function_2] ||| [function_1] [function_2]	count=14
function	area under the ||| area under	count=3
class	of tree (e ||| tree model	count=1
function	aggregate the [function_2] ||| [function_1] [function_2] zerovalue	count=1
function	the index of the ||| partitions with index	count=1
module	for new ||| sql	count=1
class	[class_1] [class_2] persists across restarts from ||| [class_1] [class_2]	count=1
arg	product and returns ||| product	count=1
function	value for each original ||| original	count=1
module	gets the [module] windowsize or ||| [module]	count=1
function	join expression ||| join	count=1
function	[function_1] type of ||| [function_2] [function_1]	count=3
class	or ||| tf	count=1
module	the [module] stepsize or ||| [module]	count=2
module	return [module] ||| [module]	count=1
module	objects ||| core	count=1
class	instance for params ||| params	count=1
arg	the rdd of document ||| document	count=1
arg	[arg_1] is received ||| [arg_2] [arg_1]	count=1
function	root ||| get root	count=1
function	all active stages ||| get active	count=1
module	gets the [module] maxmemoryinmb or ||| [module]	count=1
arg	[arg_1] maxiter=100 tol=1e-6 ||| [arg_1] [arg_2]	count=3
class	attr predictions which gives ||| linear regression summary	count=1
function	particular batch has half ||| half	count=1
function	the schema ||| schema	count=1
function	an int ||| int	count=1
module	ordered list of ||| ml	count=1
arg	rdd, a list or ||| schema samplingratio verifyschema	count=1
function_arg	a receiver [arg_2] ||| [function_1] started [arg_2]	count=1
class	the column ||| standard	count=1
module	of ||| ml linalg	count=7
function_arg	return a new dstream [function_1] [arg_2] this dstream ||| streaming dstream [function_1] [arg_2]	count=2
function	python code ||| code	count=1
arg	[arg_1] numbits ||| [arg_2] [arg_1]	count=1
function	in ||| in	count=1
arg	the sql ||| sqlcontext	count=4
module_class	[module_1] this instance ||| [module_1] [class_2]	count=4
class	specifies how data of ||| data stream	count=1
function	binary [function_2] ||| [function_2] [function_1]	count=1
arg	according ||| decayfactor	count=1
function	a [function] instance in ||| test java [function]	count=1
class	partitioned data into ||| external group by	count=1
function	[function_1] users the ||| [function_2] [function_1]	count=3
function	this instance contains a ||| has param	count=1
function	with start offset ||| offset	count=1
function	ids ||| stage ids	count=1
class	new spark [class_2] ||| [class_1] [class_2]	count=1
function	dict ||| dict	count=1
class	companion ||| java params	count=1
class	instance contains ||| params	count=1
function	given string ||| param	count=1
module_class	[module_1] all ||| [module_1] external [class_2]	count=1
function	parammap ||| param map	count=1
function	column mean values ||| mean	count=1
arg	this obj assume ||| obj	count=1
function	the sort order ||| sort	count=1
function	ignore the ||| ignore	count=1
class	which is a ||| linear regression summary	count=1
arg	dstreams ||| dstreams transformfunc	count=1
class	submit and ||| spark submit	count=4
module	value of ||| ml	count=186
function	sets params for this ||| set params	count=1
function	[function_1] features ||| [function_1] [function_2]	count=3
class	numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 0) ||| random forest	count=1
module	return an iterator of ||| core	count=2
function	min [function_2] ||| [function_2] [function_1]	count=4
arg	splits=none ||| splits	count=1
function_arg	returns a [arg_2] ||| [arg_2] [function_1]	count=2
function	[function_1] default ||| [function_2] [function_1]	count=1
class	which ||| regression summary	count=3
class	that ||| merger	count=1
class	tree (e g ||| tree model	count=1
module	[module] maxcategories ||| [module]	count=1
function	value [function_2] ||| [function_2] [function_1]	count=1
function	column for approximate ||| approx	count=1
function	[function_1] of cols ||| [function_1] [function_2]	count=3
function_arg	on [arg_2] ||| [function_1] [arg_2]	count=3
class	:py attr ||| quantile	count=1
function_arg	minimum item ||| min key	count=1
function	initial value [function_2] ||| [function_1] [function_2]	count=1
module	of each class ||| ml	count=1
class	sparse [class] using either ||| sparse [class]	count=2
class	terminations ||| query manager	count=2
function_arg	[function_1] document ||| [arg_2] [function_1]	count=1
function	data type ||| datatype	count=1
function_arg	function [arg_2] ||| [function_1] [arg_2]	count=2
module	vector representation [module] the words ||| [module]	count=1
module	a ||| param	count=2
function	of months ||| months	count=1
arg	external database table via ||| url table mode	count=1
function	with new ||| df	count=1
module_class	[module_1] linear ||| [module_1] [class_2]	count=2
class	in this :class dataframe ||| data frame	count=1
function	initial ||| set initial	count=1
class	a model ||| streaming linear regression	count=1
function	url of the ||| url	count=1
class	spark ||| spark	count=4
function_arg	[function_1] label ||| [arg_2] [function_1]	count=3
arg	inputcol=none outputcol=none ||| inputcol	count=1
function_arg	[function_1] given product ||| [function_1] [arg_2]	count=2
function	seed=none numhashtables=1) [function] params ||| [function]	count=1
arg	value ||| col	count=2
arg	in iterator ||| iterator key	count=1
function	creates a [function_2] ||| [function_1] [function_2]	count=1
class	variance and count ||| rdd	count=1
arg	a linearregressionmodel ||| sc path	count=1
function	rows ||| count	count=1
class	in the rdd ||| rdd	count=1
arg	[arg_1] labelcol="label", forceindexlabel=false) ||| [arg_1] [arg_2]	count=5
class	udfregistration for ||| sqlcontext	count=1
module	the [module] numbuckets or ||| [module]	count=1
arg	obj assume that all ||| obj	count=1
module	queries so that :func ||| sql	count=1
function	model [function_2] ||| [function_1] [function_2]	count=4
function	containing the distinct ||| distinct	count=1
class	so ||| query	count=1
class	curve ||| binary	count=4
function	a single ||| single	count=2
function	subset accuracy ||| subset accuracy	count=2
arg	withmean=false withstd=true inputcol=none outputcol=none) ||| withmean withstd inputcol outputcol	count=2
module	c{self} and c{other} ||| core	count=2
function	into ||| cast	count=1
class	be used again to ||| streaming query manager	count=1
function_arg	[function_1] dstream ||| [function_1] [arg_2]	count=9
module	the [module] binary ||| [module]	count=2
class	cluster ||| kmeans model	count=3
module	string ||| ml	count=1
function	weights is close ||| parameter accuracy	count=1
class	this ||| params	count=1
function	[function_1] vectors saved ||| [function_2] [function_1]	count=1
module	gets the [module] itemscol or ||| [module]	count=1
function_arg	[function_1] formula=none featurescol="features", ||| [function_1] [arg_2]	count=1
class	use for loading ||| java mlreader	count=1
class	feature ||| regression model	count=2
function	create an ||| create	count=2
arg	k=2 probabilitycol="probability", tol=0 01 ||| k probabilitycol	count=1
function	approximate [function_2] ||| [function_1] [function_2]	count=3
module_class	standard error [module_1] [class_2] coefficients and intercept ||| [module_1] [class_2] coefficient standard errors	count=1
class	can be ||| frame	count=1
arg	an rdd of row ||| rdd	count=2
function	set the initial value ||| initial	count=1
class	sparkcontext as ||| spark context	count=2
function	for k classes classification ||| classes	count=1
function	columns in an input ||| columns from	count=2
function	results immediately to ||| locally	count=1
arg	n elements in the ||| n	count=1
arg	two function together ||| f g	count=1
function	[function_1] a coordinatematrix ||| [function_2] [function_1]	count=2
function	testing ||| testing	count=1
module_class	to [module_1] [class_2] ||| [module_1] [class_2]	count=2
function	minimum [function_2] ||| [function_1] [function_2]	count=4
class	contains ||| params	count=1
function	stratified sample ||| sample	count=1
function_arg	unifying data [arg_2] ||| [arg_2] [function_1]	count=5
function	strategy ||| strategy	count=1
arg	the correlation ||| col2 method	count=1
arg	setparams(self formula=none featurescol="features", ||| featurescol	count=1
class	this configuration ||| conf	count=1
function	stream api with ||| stream from	count=1
function	[function_1] rdd get ||| [function_2] [function_1]	count=1
module	list of column ||| sql	count=2
function	[function_1] with new ||| [function_1] [function_2]	count=1
module	pairs [module] are smaller ||| [module]	count=1
class	java ||| java	count=1
class	currently ||| catalog	count=1
function	converter ||| converter	count=1
function	[function_1] handler ||| [function_2] [function_1]	count=2
class	again to ||| streaming	count=1
class	this streaming ||| streaming	count=1
function	brackets pairs e g ||| brackets	count=1
function_arg	computes hex [arg_2] ||| [arg_2] [function_1]	count=4
class	which ||| isotonic regression	count=1
function	on first value ||| on key	count=1
function	contains no ||| is empty	count=1
function	inherit documentation from ||| inherit doc	count=1
module_class	this rdd ||| core rdd	count=4
arg	centroids of ||| timeunit	count=1
arg	outputformat api mapred package ||| outputformatclass	count=1
function	returns the least value ||| least	count=1
function	[function_1] to a ||| [function_1] [function_2]	count=2
function	the norm of a ||| norm	count=1
function	an rdd ||| vector rdd	count=6
class	the mean ||| rdd	count=1
class	specifies the underlying output ||| writer	count=1
function	file system ||| file	count=1
function	all ||| object	count=1
function	index of the ||| index	count=1
arg	threshold ||| datasetb threshold	count=1
function	[function_1] params for ||| [function_2] [function_1]	count=5
function	[function_1] count ||| [function_2] [function_1]	count=14
module_class	[module_1] words closest ||| [module_1] [class_2]	count=2
function	summary ||| repr	count=2
function	explained [function_2] ||| [function_2] [function_1]	count=2
module_class	of [class_2] ||| [module_1] [class_2] vocab	count=4
arg	[arg_1] specified path ||| [arg_2] [arg_1]	count=2
class	this model instance ||| gaussian mixture model	count=1
function	classes values which ||| classes	count=1
module	gets the [module] fitintercept ||| [module]	count=1
function	accuracy equals to ||| accuracy	count=1
class	of this dstream ||| kafka dstream	count=1
function	json [function_2] ||| [function_1] [function_2]	count=1
class	the column ||| standard scaler	count=1
function	logic [function] instance based ||| [function]	count=1
class	of gaussians in mixture ||| gaussian mixture model	count=1
function	a local ||| get local	count=1
function	how much of memory ||| object	count=1
class	for multiclass ||| multiclass	count=1
class	returns a paired ||| matrix factorization model	count=1
class	vector ||| vector	count=5
module_class	return [class_2] ||| [module_1] [class_2]	count=9
class	paired rdd ||| matrix factorization	count=1
class	saves the ||| writer	count=1
function	user ||| user	count=1
arg	creates a ||| sparkcontext sparksession jsqlcontext	count=1
function	partial objects do not ||| save partial	count=1
function	inputcols ||| input cols	count=1
function	on [function_2] ||| [function_1] [function_2]	count=4
function	a labeledpoint [function_2] ||| [function_1] [function_2]	count=5
function	matrix columns in an ||| matrix columns to	count=1
class	the input ||| reader	count=1
class	which is a risk ||| regression metrics	count=1
function	names of tables ||| table names	count=1
function	instance to a java ||| to java	count=2
class	impurity="variance", seed=none ||| tree regressor	count=1
arg	[arg_1] by step ||| [arg_1] [arg_2]	count=3
module_class	[module_1] model ||| [module_1] decision tree [class_2]	count=1
class	queries so that ||| streaming query manager	count=1
function	sort ||| sort result	count=1
arg	[arg] of this ||| [arg]	count=1
class	the query ||| streaming query	count=1
function	[function_1] local representation ||| [function_1] [function_2]	count=1
arg	by step [arg_2] ||| [arg_1] [arg_2]	count=1
arg	featurescol="features", [arg_2] ||| [arg_2] [arg_1]	count=58
function	use only create ||| create	count=1
class	partitioned data into disks ||| external group	count=1
class	for which predictions ||| isotonic regression	count=1
module	the [module] minsupport or ||| [module]	count=1
class	imputer ||| imputer	count=1
class	for loading ||| mlreader	count=3
class	how ||| external	count=1
class	[class_1] grid ||| [class_2] [class_1]	count=4
function_arg	fast [arg_2] ||| [arg_2] [function_1]	count=4
class	the dataframe in ||| data frame	count=1
function	coldstartstrategy ||| cold start strategy	count=1
arg	featurescol="features", [arg_2] ||| [arg_1] [arg_2]	count=47
arg	from [arg] ||| rdd schema [arg]	count=2
module	that [module] true ||| [module]	count=2
class	used ||| streaming query	count=1
function	gets the [function_2] ||| [function_1] checkpoint [function_2]	count=1
function	seed=none): sets params ||| set params	count=2
arg	[arg_1] [arg_2] family="gaussian", link=none fitintercept=true maxiter=25 ||| [arg_1] [arg_2] family	count=6
function	parammap into [function_2] ||| [function_2] [function_1]	count=1
class	0 seed=none numtrees=20 ||| random forest	count=1
function_arg	a keyed dstream ||| values dstream	count=1
arg	[arg] modeltype ||| [arg]	count=1
function	limits the result ||| limit	count=1
class	impurity="gini", numtrees=20 featuresubsetstrategy="auto", ||| random forest classifier	count=4
module	instance contains a ||| param	count=1
arg	d decimal places ||| d	count=1
arg	step every element ||| step numslices	count=2
function	transforms the embedded params ||| params	count=1
function	[function] the ||| [function] by	count=1
function	foreachrdd [function_2] ||| [function_1] [function_2]	count=2
function	for debugging ||| to debug string	count=1
function_arg	converts a [arg_2] ||| [function_1] json [arg_2]	count=1
function	aggregate ||| aggregate	count=1
class	lda ||| distributed ldamodel	count=2
module	[module] n ||| [module]	count=1
class	one operation ||| rdd	count=1
function_arg	[function_1] database ||| [function_1] [arg_2]	count=2
function	the selector type of ||| selector type	count=1
function_arg	[function_1] a column ||| [function_1] json [arg_2]	count=1
function	this accumulator's ||| add	count=1
function	coordinatematrix ||| coordinate	count=2
function	into a [function_2] ||| [function_2] [function_1]	count=1
class	this matrix to the ||| dense matrix	count=1
function	script on ||| script on	count=2
function	queries ||| reset	count=1
class	:func awaitanytermination() ||| query	count=1
function_arg	[function_1] has been ||| [function_1] started [arg_2]	count=2
arg	the specified table ||| tablename overwrite	count=1
arg	predictioncol="prediction", probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1 ||| predictioncol probabilitycol	count=1
class	checkpointinterval=10 impurity="variance", seed=none variancecol=none) ||| decision tree regressor	count=1
class	dump already partitioned data ||| external group	count=1
function_arg	[function_1] product and ||| [arg_2] [function_1]	count=1
module	of object ||| ml	count=1
module	[module] metricname ||| [module]	count=2
function	[function_1] python ||| [function_2] [function_1]	count=2
function_arg	rate [arg_2] ||| [function_1] [arg_2]	count=1
function	a zip ||| zip	count=1
function	mindivisibleclustersize ||| min divisible cluster size	count=1
class	losstype="logistic", ||| gbtclassifier	count=2
function_arg	return a new dstream [function_1] [arg_2] dstream ||| streaming dstream [function_1] [arg_2]	count=2
function_arg	[function_1] [arg_2] ||| [function_1] name [arg_2]	count=6
class	string name ||| params	count=1
function	the model on toy ||| on model	count=1
function	returns the [function] in a ||| [function]	count=1
module	[module] fdr ||| [module]	count=1
module	gets the [module] losstype or ||| [module]	count=1
function	forceindexlabel ||| force index label	count=1
arg	a vector ||| vector	count=3
function	k classes ||| classes	count=1
module_class	[module_1] model ||| [module_1] tree ensemble [class_2]	count=1
module	the [module] blocksize ||| [module]	count=1
function	sets the threshold ||| set threshold	count=3
class	return ||| standard scaler	count=1
function	distinct count ||| count distinct	count=4
class	[class] query ||| [class]	count=1
arg	[arg_1] name ||| [arg_2] [arg_1]	count=6
function	associative and commutative reduce ||| reduce	count=1
class	to wait for ||| streaming query	count=1
class	estimated ||| generalized linear regression training summary	count=2
module_class	the bisecting k-means ||| mllib bisecting kmeans	count=2
module	gets the [module] featurescol ||| [module]	count=1
function	of each [function_2] ||| [function_1] [function_2] zerovalue	count=1
class	[class_1] input ||| [class_2] [class_1]	count=2
function	a java object ||| from java	count=1
function	accessible via jdbc ||| jdbc	count=1
class	1 leaf ||| decision	count=1
function	for distinct ||| distinct	count=1
class	for this test ||| other test	count=2
arg	or equal ||| numiterations	count=1
class	which is defined ||| linear regression summary	count=1
function	for all users ||| for users	count=2
function	rdd [function_2] ||| [function_2] [function_1]	count=3
module_class	[module_1] decision tree ||| [module_1] [class_2]	count=2
arg	wait a given amount ||| timeout	count=1
function	each key [function_2] ||| [function_2] [function_1]	count=14
arg	sql ||| sqlcontext	count=4
arg	later than the value ||| dayofweek	count=1
function	python code for a ||| code	count=1
arg	featurescol="features", [arg_2] ||| [arg_1] [arg_2] family	count=2
function	[function_1] [function_2] ||| [function_2] by [function_1]	count=10
function	an input stream that ||| stream	count=2
function	products ||| products	count=1
function	the approximate quantiles of ||| approx	count=1
function	by the given columns ||| bucket by	count=1
class	partitioned ||| by	count=1
arg	[arg_1] probabilitycol="probability", rawpredictioncol="rawprediction", ||| [arg_2] [arg_1]	count=3
function	a python ||| from	count=1
function	field in ||| field	count=1
function	[function_1] mean ||| [function_2] [function_1]	count=1
class	dump already ||| external group by	count=1
class	the param [class_2] ||| [class_1] [class_2]	count=1
class	into ||| group	count=1
function	deviance for [function_2] ||| [function_2] [function_1]	count=4
class	rdd as a ||| rdd	count=1
module	or two [module_2] ||| [module_2] [module_1] sparse	count=2
function	load labeled points ||| load labeled points	count=3
arg	a neutral ||| zerovalue	count=1
function	with [function_2] ||| [function_1] [function_2]	count=2
function	the offsetrange of specific ||| offset ranges	count=1
function	for each key using ||| by key	count=2
function	java storagelevel based ||| get java	count=1
module	gets the [module] scalingvec ||| [module]	count=1
arg	an event time ||| eventtime	count=1
function	the least value ||| least	count=1
function	a dependency on ||| dependency	count=1
function	a right outer join ||| full outer join	count=1
arg	observed [arg_2] ||| [arg_2] [arg_1]	count=1
class	invalidate ||| hive context	count=1
function	test that the final ||| test	count=1
function	[function_1] mllib vector ||| [function_2] [function_1]	count=3
module	how much ||| core	count=1
arg	[arg_1] predictioncol="prediction", ||| [arg_2] [arg_1]	count=38
arg	[arg_1] labels=none) ||| [arg_1] [arg_2]	count=2
function	started ||| started	count=3
function	the ordering columns ||| order by	count=1
function	data type [function_2] ||| [function_2] [function_1]	count=2
function	json string ||| json string	count=2
function	topicdistributioncol or ||| topic distribution col	count=2
function	outcomes for k classes ||| classes	count=1
class	[class] remember rdds ||| streaming [class]	count=1
module	aggregate function [module] the level ||| [module]	count=1
class	for every ||| model	count=1
arg	maintaining the heap ||| heap	count=1
function	2 ml params instances ||| params	count=1
function	columns of blocks ||| blocks	count=1
function	reducebykey ||| reduce	count=1
class	into ||| group by	count=2
function	context to ||| context	count=4
arg	previously saved ||| minpartitions	count=1
class	which is a ||| regression	count=3
class	already partitioned data into ||| group	count=1
class	content of the ||| writer	count=1
function_arg	memory [arg_2] ||| [arg_2] [function_1]	count=4
function_arg	[function_1] id ||| [arg_2] [function_1]	count=5
function	summary [function_2] ||| [function_1] [function_2]	count=1
module	gets the [module] threshold ||| [module]	count=2
module_class	the [module_1] [class_2] file ||| [module_1] [class_2]	count=1
function_arg	[function_1] mincount ||| [function_1] min count [arg_2]	count=1
module	one operation ||| core	count=1
module	with a given string ||| param	count=1
function	to the wrapped java ||| to	count=1
function_arg	[function_1] of document ||| [arg_2] [function_1]	count=1
function_arg	[function_1] given label ||| [arg_2] [function_1]	count=3
function	be used again to ||| reset	count=1
module	return a ||| core	count=7
class	as the spark ||| spark	count=1
module	gets the [module] minsupport ||| [module]	count=1
class	in mixture ||| mixture model	count=1
function	storage type ||| type	count=1
function	[function_1] tables/views ||| [function_1] [function_2]	count=2
arg	for a given label ||| label	count=1
class	adds ||| reader	count=1
function	of users for a ||| users	count=1
module	called by a ||| core	count=1
function	outputcol=none [function] params ||| [function]	count=1
class	for which ||| isotonic regression	count=1
function	a java ||| get java	count=1
function	[function_1] grid ||| [function_1] [function_2]	count=1
module_class	[module_1] rdd's elements ||| [module_1] [class_2]	count=2
function	partition ||| partition	count=3
function	shut down the ||| stop	count=1
module	a param with ||| ml	count=1
function	returns the value of ||| get	count=1
class	word2vec model's vocabulary ||| word2vec	count=1
class	return the first ||| streaming	count=1
class	of this matrix ||| matrix	count=1
class	group by key ||| group by	count=1
module_class	of write() ||| ml pipeline model	count=1
class	nodes in tree including ||| tree model	count=1
class	[class_1] grid ||| [class_1] [class_2]	count=4
class	kafkardd ||| kafka rdd	count=1
class	for saving ||| mlwriter	count=1
module	[module] windowsize ||| [module]	count=1
function	[function_1] distinct ||| [function_1] count [function_2]	count=4
function	as a temporary table ||| as table	count=1
class	which ||| linear regression summary	count=2
module	[module] separate arrays ||| [module]	count=3
function	[function_1] points ||| [function_1] [function_2]	count=2
class	be used again ||| query	count=1
arg	len with ||| len	count=1
function	predict the value ||| predict	count=1
arg	against the expected distribution ||| expected	count=1
arg	of another ||| other	count=1
class	partitioned data ||| external group	count=1
function	batch of [function_2] ||| [function_1] [function_2]	count=2
function_arg	points [arg_2] ||| [function_1] [arg_2]	count=4
arg	centroids of that particular ||| timeunit	count=1
arg	storage level ||| storagelevel	count=2
arg	inverse=false inputcol=none outputcol=none) ||| inverse inputcol outputcol	count=2
function	[function_1] items for ||| [function_1] [function_2]	count=1
arg	an existing ||| samplingratio	count=1
function	products for all users ||| products for users	count=1
function	tcp server to ||| server	count=1
class	:func ||| streaming query	count=1
arg	centroids [arg_2] ||| [arg_2] [arg_1]	count=6
function	into ||| spill	count=1
arg	labelcol="label", [arg_2] ||| [arg_1] [arg_2]	count=23
function	[function_1] a local ||| [function_2] [function_1]	count=2
class	column ||| scaler	count=2
class	kmeans ||| kmeans	count=1
function_arg	labeled points [arg_2] ||| [arg_2] [function_1]	count=4
class	the stream query ||| stream writer	count=1
arg	the spark ||| sparksession	count=1
class	optional default ||| params	count=1
function	a ||| add	count=1
function	inside brackets pairs e ||| brackets split	count=1
module	returns an array ||| sql	count=1
function	directory for ||| path	count=1
function_arg	[function_1] [arg_2] impurity="variance", subsamplingrate=1 ||| [function_1] [arg_2]	count=1
function	column for distinct count ||| count distinct	count=1
arg	file and ||| path schema	count=1
module	the [module] scalingvec or ||| [module]	count=1
module	a given string name ||| ml	count=1
arg	[arg_1] numpartitions partitions ||| [arg_2] [arg_1]	count=2
function	seed=none): sets params for ||| set params	count=1
class	sets ||| has reg param	count=1
module	returns one of multiple ||| sql	count=1
class	this model instance ||| mixture model	count=1
arg	n elements ||| n	count=1
function	training set ||| training	count=1
function	value to an int ||| to int	count=1
class	a randomly generated uid ||| cross validator model	count=1
module	union [module] this ||| [module]	count=1
module	the ||| core	count=4
arg	spark sink deployed ||| storagelevel maxbatchsize	count=1
arg	table via ||| url table	count=1
class	on a ||| logistic regression with	count=1
function	true label ||| label col	count=2
class	estimated coefficients and intercept ||| generalized linear regression training summary	count=1
arg	stored in the column ||| col	count=1
class	model with ||| with	count=1
function	stratified sample without replacement ||| sample	count=1
function	month of ||| dayofmonth	count=1
module	string name ||| ml param	count=1
function	each original ||| original	count=1
function	the number [function_2] ||| [function_2] [function_1]	count=4
function	compare [function_2] ||| [function_2] [function_1]	count=2
class	logistic regression model ||| logistic regression	count=2
module	gets the [module] gaps or ||| [module]	count=1
function	model ||| model	count=6
function	direct ||| direct	count=5
class	elements in one ||| rdd	count=1
arg	seed=none) ||| seed	count=1
class	the model ||| model	count=1
class	of this [class] using the ||| [class]	count=1
arg	an rdd of row ||| rdd samplingratio	count=2
arg	featurescol="features", labelcol="label", ||| featurescol labelcol	count=34
class	model should have been ||| loader	count=1
class	sets ||| hashing tf	count=2
class	passed as [class_2] ||| [class_2] [class_1]	count=4
module	returns one ||| sql	count=1
arg	different value or cleared ||| description interruptoncancel	count=1
module_class	[module_1] word2vec model's ||| [module_1] [class_2]	count=2
module	times and [module] it as ||| [module]	count=1
module	of indices ||| ml	count=3
class	partition ||| external merger	count=1
function	list of names of ||| names	count=1
function	using the new ||| as new	count=1
module	int containing elements ||| core	count=1
module	value of ||| ml param	count=28
function	[function_1] the global ||| [function_1] [function_2]	count=1
arg	data of another dstream ||| other	count=1
module	find norm [module] the ||| [module]	count=1
class	:func ||| query	count=1
arg	dataset ||| dataset featurescol	count=1
class	[class_1] classification ||| [class_1] [class_2]	count=1
class	for which predictions are ||| isotonic regression model	count=1
function	users the ||| users	count=1
function	jobs started by ||| job	count=1
class	awaitanytermination() ||| streaming query manager	count=2
arg	given label ||| label	count=1
function	[function_1] group if ||| [function_2] [function_1]	count=1
class	specifies how data of ||| data	count=1
class	at [class] position ||| [class]	count=2
function	[function] of the ||| recover [function]	count=1
function	[function_1] in ||| [function_2] [function_1]	count=8
function	[function_1] json ||| [function_1] [function_2]	count=3
arg	assumed to consist of ||| ascending numpartitions keyfunc	count=1
arg	[arg] map ||| [arg]	count=3
class	a data ||| data	count=1
module	gets the [module] statement ||| [module]	count=1
arg	database table ||| table mode	count=1
function_arg	[function_1] string str ||| [function_1] [arg_2] delim	count=1
function	count of col or ||| count	count=1
function	"zerovalue" ||| fold by	count=1
arg	n rows to the ||| n	count=1
function	threshold [function_2] ||| [function_2] [function_1]	count=2
module	get or compute ||| linalg	count=2
module	of each class as ||| ml	count=1
function	norm of a ||| norm	count=1
class	can be used again ||| query manager	count=1
arg	calculates the correlation of ||| col1 col2 method	count=1
class	which ||| logistic regression summary	count=2
class	number ||| java classification model	count=1
function	the ordering columns in ||| order by	count=1
function	levenshtein distance of ||| levenshtein	count=1
class	__init__(self ||| max abs scaler	count=1
module	[module] layers ||| [module]	count=1
module	the [module] linkpower or ||| [module]	count=1
class	gaussian ||| gaussian	count=2
module_class	of [class_2] ||| [module_1] logistic regression [class_2]	count=1
function	return the [function_1] [function_2] this ||| [function_2] [function_1]	count=2
function_arg	parquet [arg_2] ||| [function_1] path mode partitionby [arg_2]	count=3
module	the [module] outputcol ||| [module]	count=1
arg	consist ||| ascending numpartitions keyfunc	count=1
function_arg	a csv [arg_2] ||| [arg_2] [function_1]	count=3
arg	[arg_1] [arg_2] of this ||| [arg_1] [arg_2]	count=14
class	as a ||| reader	count=1
module	this ||| ml param	count=2
class	new spark ||| spark	count=1
class	vectors which ||| vector indexer model	count=1
module	gets the [module] binary or ||| [module]	count=2
function	java parammap [function_2] ||| [function_2] [function_1]	count=4
module	a ||| ml	count=2
module	again ||| sql	count=1
function	reduce ||| reduce	count=1
function	computes the area ||| area	count=2
class	[class_1] [class_2] uid and some ||| [class_2] [class_1]	count=8
arg	inputformat ||| inputformatclass keyclass valueclass	count=2
class	classification evaluator ||| classification evaluator	count=2
class	subsamplingrate=1 [class] featuresubsetstrategy="auto") ||| [class]	count=1
arg	[arg_1] step ||| [arg_1] [arg_2]	count=2
function	a right [function_2] ||| [function_2] [function_1]	count=4
class	byte ||| framed serializer	count=1
function_arg	[function_1] featurescol="features", predictioncol="prediction", ||| [function_1] [arg_2]	count=1
function	percentile ||| percentile	count=1
module	gets the [module] mininstancespernode ||| [module]	count=1
function	unifying data of ||| union	count=1
arg	c{self} that ||| other numpartitions	count=1
function	[function_1] size ||| [function_2] [function_1]	count=4
class	in ||| rdd	count=1
function	this ||| object	count=1
class	which is a ||| logistic regression summary	count=1
function	inputcol=none outputcol=none [function] ||| [function]	count=1
function	debugging ||| to debug string	count=1
class	objective ||| logistic regression training summary	count=1
function	[function_1] handler ||| sql install [function_1] [function_2]	count=1
function	columns ||| cols	count=1
function	single script on a ||| single script on	count=1
function_arg	selector type [arg_2] ||| [function_1] [arg_2]	count=1
function	of freedom [function_2] ||| [function_2] [function_1]	count=2
module	the [module] windowsize ||| [module]	count=1
function	group ||| group	count=3
class	the returned ||| py spark streaming test case	count=1
arg	the correlation ||| method	count=1
arg	have the same param ||| param	count=1
class	[class_1] regression model ||| [class_2] [class_1]	count=2
arg	[arg] maxiter=100 ||| [arg]	count=3
function	to each [function] this ||| foreach [function]	count=1
function	data type json string ||| datatype json string	count=1
function_arg	type [arg_2] ||| [function_1] [arg_2]	count=1
class	:func awaitanytermination() can be ||| manager	count=1
class	pulls events from flume ||| flume	count=1
class	the ensemble ||| ensemble model	count=2
arg	of the observed data ||| observed	count=1
arg	[arg_1] predictioncol="prediction", k=2 ||| params [arg_1] [arg_2]	count=1
class	specified by the param ||| param	count=1
module	[module] intermediatestoragelevel or ||| [module]	count=1
arg	dataset and an ||| dataset key numnearestneighbors	count=1
arg	a term to this ||| term	count=1
function	[function] missingvalue ||| get missing [function]	count=1
function	test that the ||| test training and prediction	count=1
arg	functions and a neutral ||| zerovalue	count=1
module	in multinomial ||| mllib	count=1
module	gets the [module] vectorsize or ||| [module]	count=1
function	an rdd ||| rdd	count=10
function	a java ||| new java	count=1
module	the [module] losstype ||| [module]	count=1
function	get or create global ||| get or create	count=1
arg	data ||| data iterations	count=2
arg	setparams(self formula=none featurescol="features", labelcol="label", ||| featurescol labelcol	count=1
class	frequent ||| prefix span model	count=1
module_class	[module_1] an rdd ||| [module_1] [class_2]	count=2
module	count of the rdd's ||| core	count=1
module	of rows ||| ml	count=1
class	sets ||| features col	count=1
function	test that ||| test training and	count=1
function	returns a new java ||| new java	count=1
class	the :class dataframe ||| data frame	count=8
class	[class_1] matrix ||| [class_1] [class_2] subtract	count=1
class	words ||| word2vec	count=1
function	a [function] ||| start update [function]	count=2
function	sets window ||| set window	count=2
class	so that ||| query	count=1
arg	inputcol=none outputcol=none ||| inputcol outputcol	count=6
function	deserialized batches lists ||| stream without unbatching	count=1
module	again to ||| sql	count=1
function	of blocks in ||| blocks	count=1
module	of nonzero elements this ||| ml linalg	count=1
arg	with extra ||| extra	count=1
arg	starts at pos in ||| pos	count=1
function_arg	[function_1] [arg_2] ||| [function_1] in zip [arg_2] dir	count=1
arg	iff [arg] is null ||| [arg]	count=1
function_arg	a memory string ||| memory s	count=1
module	a given string name ||| ml param	count=1
function	start offset ||| from offset	count=1
arg	[arg_1] and date2 ||| [arg_1] [arg_2]	count=1
class	onevsrest create ||| one vs rest	count=1
function	the index ||| index	count=1
arg	a function to each ||| f	count=1
function	this grid to ||| add grid	count=2
module	this instance contains ||| param	count=1
module	[module] impurity ||| [module]	count=1
function	to a mllib ||| to	count=1
function	memory ||| memory	count=1
function	each rdd [function_2] ||| [function_2] [function_1]	count=4
arg	version of a heappush ||| heap item	count=1
arg	sets the sql ||| sqlcontext	count=4
function	number of top ||| set num top	count=3
function_arg	[function_1] [arg_2] ||| [function_1] [arg_2]	count=793
class	this instance with a ||| one vs rest	count=1
class	new class dataframe ||| data frame	count=1
arg	value" ||| seqfunc combfunc numpartitions	count=1
class	paired rdd ||| matrix factorization model	count=1
function	vector ||| vector	count=4
function	year of a given ||| year	count=1
module	[module] n or ||| [module]	count=1
class	group by key ||| external group by	count=2
class	to a data ||| data	count=1
class	adds output ||| frame writer	count=2
arg	string column [arg_2] ||| [arg_2] [arg_1]	count=2
module	in this ||| core	count=2
function	with ||| range	count=1
function	[function_1] java object ||| [function_2] [function_1]	count=5
class	numtrees=20 featuresubsetstrategy="auto", subsamplingrate=1 ||| random forest	count=1
class	random forest ||| random forest	count=2
module	with a given ||| param	count=1
function	until any ||| await any termination	count=1
class	partitioned data into disks ||| group	count=1
arg	drawn ||| numrows numcols numpartitions	count=2
arg	by codeblock [arg_2] ||| [arg_2] [arg_1]	count=1
arg	an external database table ||| table mode	count=1
class	again to wait ||| query manager	count=1
function	products [function_2] ||| [function_1] [function_2]	count=3
module	gets the [module] variancecol or ||| [module]	count=1
function	sample without replacement ||| sample	count=1
module_class	creates a copy [module_1] [class_2] some ||| [module_1] [class_2] copy	count=4
arg	[arg_1] to ||| [arg_2] [arg_1]	count=4
module_class	[module_1] all ||| [module_1] [class_2]	count=1
class	in :py attr predictions ||| generalized linear	count=1
arg	based on the dataset ||| path	count=1
arg	first occurrence of substr ||| substr str	count=1
class	sets ||| words remover	count=3
class	for this ||| external	count=1
module	gets the [module] :py ||| [module]	count=2
class	error which ||| linear regression summary	count=1
function	results immediately ||| locally	count=1
class	create a new spark ||| spark	count=1
module	optional ||| ml param	count=1
function	by ||| by	count=4
function_arg	[function_1] [arg_2] ||| [function_1] job group groupid [arg_2]	count=2
class	maxmemoryinmb=256 cachenodeids=false checkpointinterval=10 impurity="variance", ||| tree regressor	count=1
arg	k=none ||| k	count=1
function	by applying 'left ||| left	count=1
module	the second is an ||| mllib	count=1
module	the [module] isotonic or ||| [module]	count=1
function	a java storagelevel based ||| java	count=1
class	error which is ||| linear regression summary	count=1
function	factor ||| factor	count=1
arg	saved using rdd saveastextfile ||| path minpartitions	count=1
function	[function_1] tables/views in ||| [function_2] [function_1]	count=2
class	enable 'with [class] sc app ||| [class]	count=1
function_arg	window size [arg_2] ||| [arg_2] [function_1]	count=1
function	to wait for ||| reset	count=1
function	[function_1] script on ||| [function_2] [function_1]	count=2
function	positive rate for a ||| positive rate	count=1
module	gets the [module] k or ||| [module]	count=1
function	or its default value ||| col	count=1
arg	the observed data ||| observed	count=1
class	deployed on a flume ||| flume	count=1
function	parse a ||| parse	count=1
function	create a java ||| new java	count=1
module	variance and count of ||| core	count=1
class	binomial logistic ||| logistic	count=2
arg	splits=none inputcol=none outputcol=none handleinvalid="error") ||| splits inputcol outputcol handleinvalid	count=1
class	:class dataframe using the ||| data frame	count=1
function	converts matrix columns in ||| convert matrix columns from	count=1
class	queries so that ||| streaming query	count=1
function	start [function_2] ||| [function_2] [function_1]	count=2
module	can be used again ||| sql	count=1
module	value ||| core	count=1
class	loads a ||| reader	count=2
arg	instance to [arg] ||| [arg]	count=2
function	[function_1] [function_2] could not be found ||| [function_1] [function_2] stageid	count=4
module	[module] quantileprobabilities or ||| [module]	count=1
class	impurity="gini", numtrees=20 featuresubsetstrategy="auto", seed=none ||| random forest classifier	count=1
class	given string name ||| params	count=1
module	column ||| sql	count=3
arg	given date ||| date	count=1
class	randomly generated ||| cross validator model	count=1
arg	with the specified schema ||| schema	count=1
class	:class dataframe whose ||| data	count=1
arg	date column ||| date	count=1
function	[function_1] get offsetranges ||| [function_1] [function_2]	count=10
class	this idf ||| idf	count=1
function	function ||| map values	count=1
module	in json format ||| sql	count=1
function	square root [function_2] ||| [function_2] [function_1]	count=3
function	a dense ||| dense	count=1
function	a local representation ||| local	count=1
arg	buckets the buckets ||| buckets	count=1
function	a class inherit documentation ||| inherit	count=1
function	for approximate [function_2] ||| [function_2] [function_1]	count=4
function	with the keys ||| keys	count=1
class	model trained on ||| model	count=1
arg	datasets to ||| dataseta	count=1
function	labeledpoint ||| load lib svmfile	count=2
function	months after [function] >>> ||| [function]	count=1
class	decision ||| decision	count=2
function	commutative reduce function ||| reduce	count=2
function	the companion [function_2] ||| [function_2] [function_1]	count=1
class	the accumulator's ||| accumulator	count=2
class	[class_1] data point ||| [class_2] [class_1]	count=6
function	load ||| load	count=1
function	compute the number of ||| num	count=3
module	[module] squared distances ||| [module]	count=3
class	that :func awaitanytermination() can ||| manager	count=1
arg	threshold ||| datasetb threshold distcol	count=1
arg	[arg_1] values ||| [arg_2] [arg_1]	count=2
function	all the jobs started ||| job	count=1
module	the [module] impurity ||| [module]	count=1
class	1 [class] ||| [class]	count=1
function_arg	[function_1] reduced into ||| [function_1] numpartitions [arg_2]	count=1
class	carry over its keys ||| algorithm	count=1
class	a given string name ||| params	count=1
arg	scale ||| scale	count=2
function	summary (e g ||| summary	count=1
class	stream query ||| stream	count=1
arg	with optional parameters ||| params	count=3
class	of model ||| tree ensemble model	count=1
module	[module] itemscol or ||| [module]	count=1
class	[class_1] configuration ||| [class_1] [class_2]	count=6
arg	schema ||| schema	count=2
arg	to the number ||| num	count=1
function_arg	[function_1] by name ||| [function_1] [arg_2]	count=10
function	checkpointed and ||| checkpointed	count=1
function	the number [function_2] ||| [function_1] [function_2]	count=4
function	converts [function_2] ||| [function_2] [function_1]	count=20
module	[module] modeltype ||| [module]	count=1
function	= sc ||| get	count=1
module	gets the [module] finalstoragelevel or ||| [module]	count=1
function	a batch has started ||| started	count=1
function	topics ||| topics	count=1
function	name ||| has param	count=1
class	block ||| block	count=2
function	need [function] ||| need [function]	count=1
class	data or table already ||| data	count=1
arg	splits=none inputcol=none [arg_2] ||| [arg_1] [arg_2]	count=1
arg	position pos ||| pos	count=1
class	that all ||| external	count=1
arg	the top "num" number ||| num	count=2
arg	function to ||| f	count=5
class	every ||| linear	count=1
class	depth of tree ||| tree model	count=1
function_arg	by [arg_2] ||| [function_1] [arg_2]	count=1
class	for this [class] within ||| [class]	count=1
arg	probabilitycol="probability", rawpredictioncol="rawprediction", smoothing=1 ||| probabilitycol	count=1
function_arg	[function_1] product ||| [function_1] [arg_2]	count=1
class	so that ||| streaming query	count=1
class	stream query if ||| stream writer	count=1
arg	[arg] blocksize ||| [arg]	count=1
function	[function_1] offsetranges ||| [function_1] [function_2]	count=13
class	new [class] as new ||| [class]	count=1
class	rdd's elements in ||| rdd	count=1
arg	[arg_1] predictioncol="prediction", ||| params [arg_1] [arg_2]	count=1
function	list of tables/views ||| list tables	count=1
function	is not contained in ||| subtract	count=1
function	directory for ||| dir	count=1
arg	[arg] family="gaussian", link=none ||| [arg]	count=2
function	into a jvm seq ||| seq	count=1
function	text ||| text	count=3
function	explained [function_2] ||| [function_1] [function_2]	count=2
arg	downloaded ||| recursive	count=1
arg	for ||| key value	count=4
module	with a ||| ml param	count=1
arg	<http //jsonlines ||| path mode compression dateformat	count=1
class	for the stream ||| data stream	count=1
class	for cross validator ||| cross validator	count=2
function	[function_1] zip ||| [function_2] [function_1]	count=1
function	locate the position of ||| locate	count=1
class	be used ||| manager	count=1
function	vector columns in ||| vector columns from ml	count=1
module	the [module] threshold ||| [module]	count=2
function	a java parammap ||| java	count=1
class	dstreams in this context ||| streaming context	count=1
class	adds input ||| stream reader	count=2
function	to the wrapped ||| to	count=1
arg	them with extra values ||| extra	count=1
function	python parammap into ||| param map to	count=1
arg	the given database ||| dbname	count=1
class	to this ||| accumulator	count=1
class	already partitioned data ||| by	count=1
module_class	[module_1] random forest ||| [module_1] [class_2]	count=2
function	users for ||| users	count=1
function	[function_1] [function_2] ||| [function_2] [function_1] inputformatclass keyclass valueclass keyconverter	count=12
class	weightcol=none ||| linear	count=2
arg	each ||| func numpartitions	count=1
function	key ||| key	count=4
function_arg	[function_1] column ||| [arg_2] [function_1]	count=1
function	create a converter ||| create converter	count=3
function	value to list ||| to list	count=2
function	elasticnetparam ||| elastic net param	count=1
arg	width [arg] ||| col [arg]	count=2
module_class	[module_1] [class_2] its default value ||| [module_1] [class_2] discretizer get handle invalid	count=1
function	again to wait ||| reset	count=1
class	[class] sc app ||| [class]	count=1
module	gets the [module] modeltype ||| [module]	count=1
class	the stream query if ||| stream writer	count=1
arg	by [arg_2] ||| [arg_2] [arg_1]	count=4
arg	[arg] outputcols ||| [arg]	count=1
arg	array-like or buffer ||| array_like dtype	count=2
function	a java array ||| java array	count=1
function	test that ||| test training and prediction	count=1
function	the partition id ||| partition id	count=2
module	matrix other from this ||| linalg	count=1
arg	the given path ||| path	count=6
function	value to ||| to	count=4
module	gets the [module] blocksize ||| [module]	count=1
class	[class_1] [class_2] remember rdds it generated ||| [class_1] [class_2]	count=4
module	javardd of ||| ml	count=1
function_arg	[function_1] number of ||| [function_1] users product [arg_2]	count=1
function	an rdd ||| log normal vector rdd	count=1
class	[class] on ||| kmeans [class]	count=3
class	sets ||| max scaler	count=2
function	select filter ||| selected features	count=1
class	data ||| data	count=11
function	of seconds [function] unix ||| [function]	count=1
function	vector columns ||| vector columns from	count=1
class	or [class] ||| regression [class]	count=1
function	accumulator's value ||| add	count=1
function	java ||| get java	count=1
class	0 means 1 leaf ||| decision	count=1
module	defined ||| ml param	count=1
function	java storagelevel ||| get java	count=1
module	of rows in ||| sql	count=2
arg	a character in matching ||| matching replace	count=1
class	values of the accumulator's ||| accumulator	count=1
function_arg	keyed dstream ||| values dstream	count=1
module	value pairs or two ||| mllib linalg	count=1
class	[class_1] returning the ||| [class_1] [class_2]	count=1
module	the [module] link ||| [module]	count=1
class	lda keeplastcheckpoint ||| ldamodel	count=1
class	that stores ||| alsmodel	count=2
class	idf ||| idf	count=1
function	convert a dict ||| to	count=1
module	an associative and ||| core	count=1
function	new mllib-local representation ||| as	count=4
function	a java array ||| new java array	count=2
function_arg	from [arg_2] ||| [arg_2] [function_1]	count=1
function	[function_1] on ||| [function_1] [function_2]	count=10
class	sets the context to ||| streaming context	count=1
module	of number of ||| ml	count=1
arg	the threshold ||| threshold	count=1
class	rdd partitioned ||| rdd	count=1
function	limit ||| limit	count=1
function	recovers all ||| recover	count=1
class	cross [class_2] ||| [class_1] [class_2]	count=1
module_class	[module_1] [class_2] a column or replacing ||| [module_1] [class_2] with column colname col	count=1
arg	n rows ||| n	count=2
class	singular vectors [class_2] ||| [class_1] [class_2]	count=1
arg	numfolds=3 seed=none) ||| numfolds	count=1
module	and [module] it as ||| [module]	count=1
class	sets ||| train validation split	count=1
function	columns for ||| columns	count=1
function_arg	recommends the [arg_2] ||| [arg_2] [function_1]	count=4
function	again to wait for ||| reset	count=1
function	[function] a ||| [function]	count=2
class	one ||| rdd	count=1
class	partitioned data into ||| group	count=1
class	vectors which ||| vector indexer	count=1
function	close to the desired ||| parameter accuracy	count=1
class	convert this matrix to ||| dense matrix	count=1
function	this grid ||| add grid	count=2
module	mean variance ||| core	count=1
module	the [module] mintf ||| [module]	count=1
arg	document to rdd of ||| document	count=1
class	sets ||| raw prediction col	count=1
class	cachenodeids=false checkpointinterval=10 seed=none impurity="gini", ||| classifier	count=1
function	to a rowmatrix ||| to row	count=6
function_arg	transforms [arg_2] ||| [function_1] [arg_2]	count=2
arg	[arg_1] end exclusive ||| [arg_2] [arg_1]	count=1
class	dataframe in ||| data frame writer	count=1
function	[function_1] of rows ||| [function_1] [function_2]	count=10
class	the test ||| test	count=1
function	zip ||| zip	count=1
class	udf with a function ||| user defined function	count=1
class	list of :class ||| data frame	count=2
function	sqlcontext [function] throws ||| [function]	count=1
arg	featurescol="features", labelcol="label", [arg_2] ||| [arg_1] [arg_2]	count=18
function	with two fields threshold ||| threshold	count=2
arg	threshold ||| threshold	count=1
function	used again to wait ||| reset	count=1
function	items ||| items	count=1
function	a [function] to this ||| [function]	count=1
class	dump already ||| group by	count=1
function	names of tables in ||| names	count=1
function_arg	a new dstream by [function_1] [arg_2] this dstream ||| streaming dstream [function_1] [arg_2]	count=2
function	code ||| code	count=1
module	[module] featureindex or ||| [module]	count=1
module	instance contains a param ||| ml	count=1
function	standard deviation of ||| stdev	count=1
function_arg	[function_1] featurescol="features", labelcol="label", ||| [arg_2] [function_1]	count=15
class	class ||| profiler collector	count=1
function	the test ||| test	count=1
module_class	[module_1] [class_2] its default value ||| [module_1] [class_2]	count=1
class	that :func awaitanytermination() can ||| streaming query manager	count=1
class	unique id [class] persists ||| [class]	count=1
function	the jobs ||| job	count=1
class	with a randomly ||| cross validator model	count=1
class	so that ||| manager	count=1
arg	at least the master ||| master	count=1
function_arg	[function_1] for data ||| [function_1] [arg_2]	count=1
class	predictions which gives ||| generalized linear regression summary	count=1
function	sort the [function_2] ||| [function_2] [function_1]	count=4
module	elements in this ||| core	count=3
class	for every ||| linear model	count=1
function	each ||| by	count=2
class	every ||| model	count=1
function	minutes of ||| minute	count=1
function_arg	[function_1] [arg_2] link=none fitintercept=true maxiter=25 tol=1e-6 ||| [function_1] [arg_2]	count=1
class	pipelinemodel used for ||| pipeline model	count=1
function	of months [function_2] ||| [function_2] [function_1]	count=1
class	the objects ||| external merger	count=1
function_arg	[function_1] saved ||| [arg_2] [function_1]	count=3
function	"zerovalue" ||| fold	count=1
function	number of columns ||| cols	count=1
module	can ||| sql	count=1
arg	get the n ||| num	count=1
class	data ||| data frame	count=3
module	the [module] names ||| [module]	count=1
module	separate arrays of ||| ml	count=1
arg	inputcol=none outputcol=none indices=none names=none) ||| inputcol outputcol indices names	count=1
class	broker to map ||| broker	count=1
class	awaitanytermination() can be ||| query manager	count=1
function	to each [function] ||| foreach [function]	count=1
module	[module] fdr or ||| [module]	count=1
class	training ||| training summary	count=1
module_class	[module_1] terms ||| [module_1] [class_2] vocab	count=1
module	documentation of ||| ml	count=1
class	instance contains a ||| params	count=1
function	which each rdd contains ||| by	count=1
function_arg	[function_1] content ||| [arg_2] [function_1]	count=1
function	as ||| save as	count=1
class	the column ||| standard scaler model	count=1
function	each [function_2] ||| [function_1] [function_2] zerovalue	count=1
class	comprised ||| random rdds	count=14
module	this instance ||| param	count=1
class	lda keeplastcheckpoint is set ||| ldamodel	count=1
class	randomly generated ||| validation split	count=2
function	if using checkpointing ||| get checkpoint	count=1
module_class	[module_1] decision ||| [module_1] [class_2]	count=2
function	[function_1] on toy ||| [function_2] [function_1]	count=4
module	the [module] degree or ||| [module]	count=1
module	gets the [module] solver or ||| [module]	count=2
module	deprecated in ||| sql	count=1
module	gets the [module] mindf or ||| [module]	count=1
module	names skipping null ||| sql	count=2
function	returns a java storagelevel ||| get java	count=1
function	wait for new terminations ||| reset	count=1
function	[function_1] the mean ||| [function_2] [function_1]	count=2
function	with ||| has param	count=1
class	an rdd containing all ||| rdd	count=1
arg	applies [arg] ||| [arg]	count=2
function_arg	positive rate [arg_2] ||| [arg_2] [function_1]	count=1
function	the partitions ||| partitions	count=1
function	k classes classification problem ||| classes	count=1
class	error which ||| regression	count=1
class	:func awaitanytermination() ||| streaming query manager	count=1
function	internal use only create ||| create	count=1
class	the column [class_2] ||| [class_2] [class_1]	count=2
class	randomly ||| split model	count=1
class	a randomly generated ||| validation split model	count=1
function	get total number of ||| total num	count=1
class	sets ||| has elastic net param	count=1
function	index of the ||| with index	count=1
arg	property [arg_2] ||| [arg_2] [arg_1]	count=2
arg	k=2 [arg_2] ||| [arg_2] [arg_1]	count=2
arg	labelcol="label", predictioncol="prediction", maxiter=100 ||| labelcol predictioncol maxiter	count=3
module	gets the [module] maxbins ||| [module]	count=1
function	ordered ||| take ordered	count=1
arg	* [arg] + ||| [arg] npoints	count=1
function_arg	[function_1] representing ||| [arg_2] [function_1]	count=1
module	of this ||| ml	count=7
function	approximate quantiles of ||| approx	count=1
module_class	[module_1] incrementing as ||| [module_1] [class_2]	count=2
class	logistic regression model on ||| logistic regression with	count=1
function	for all [function_2] ||| [function_1] [function_2]	count=2
function	[function_1] rowmatrix ||| [function_1] [function_2]	count=2
arg	inputformat with arbitrary key ||| inputformatclass keyclass valueclass	count=2
class	tree (e ||| tree	count=1
arg	a word ||| word	count=1
class	checkpointinterval=10 seed=none impurity="gini", ||| classifier	count=1
function	list based on first ||| based on	count=1
function	[function_1] of blocks ||| [function_2] [function_1]	count=4
class	the spark fair scheduler ||| spark context	count=1
class	in this [class_2] ||| [class_2] [class_1]	count=2
function	[function_1] freedom ||| [function_1] [function_2]	count=1
arg	for data sampled from ||| data	count=1
arg	elements in iterator ||| iterator	count=1
function	which each ||| by	count=1
arg	the n ||| n	count=2
class	squared error which ||| regression	count=1
class	memory for this ||| external	count=1
arg	a vector or ||| vector	count=1
function	get total number ||| total num	count=1
arg	[arg] of ||| f [arg]	count=4
function	the year of ||| year	count=1
function	a given string name ||| param	count=1
module	of object by unpickling ||| ml	count=1
arg	date1 [arg_2] ||| [arg_1] [arg_2]	count=1
class	a randomly generated ||| train validation split model	count=1
function	vector columns ||| vector columns from ml	count=1
function_arg	months between date1 ||| months between date1	count=1
function	loads a text ||| text	count=1
function	place ||| place	count=1
arg	correlation of two ||| method	count=1
function	java ||| from java	count=4
class	a new [class] as new ||| [class]	count=1
class	rdd ||| rdd	count=38
function	md5 digest and ||| md5	count=1
class	from flume ||| flume	count=1
class	the dataframe in ||| data frame writer	count=1
module	compute the ||| core	count=1
function_arg	[function_1] [arg_2] containing a [[structtype]] or ||| [function_1] json [arg_2]	count=2
class	curve which is ||| binary logistic regression summary	count=1
arg	the optional key ||| key	count=1
function_arg	[function_1] reported an ||| [arg_2] [function_1]	count=2
function_arg	return a new [function_1] [arg_2] this dstream ||| streaming dstream [function_1] [arg_2]	count=2
class	profiling ||| profiler	count=1
function	true positive ||| true positive	count=4
arg	for aggregator by name ||| name doc	count=1
arg	the elements in iterator ||| iterator key reverse	count=1
arg	same param ||| m1 m2 param	count=1
module	can be used ||| sql	count=1
class	checkpointinterval=10 impurity="variance", ||| regressor	count=1
arg	d samples drawn ||| std numrows	count=1
function	the k-means [function] ||| [function]	count=3
function	dependency ||| dependency	count=3
class	return an rdd containing ||| rdd	count=1
arg	each rdds ||| dstream	count=1
arg	an external database table ||| table	count=1
function	array of features ||| features	count=1
class	model ||| logistic regression model	count=1
function	number of top features ||| num top features	count=1
function	used ||| reset	count=1
function	[function_1] of rows ||| [function_2] [function_1]	count=10
module	[module] the squared ||| [module]	count=3
class	tree including ||| tree model	count=1
class	model with weights ||| linear regression with	count=1
module	dump already ||| core	count=1
class	for pipeline ||| pipeline	count=1
arg	the correlation of ||| method	count=1
arg	[arg_1] predictioncol="prediction", ||| [arg_1] [arg_2] maxiter	count=6
function	the output [function] the ||| partition [function]	count=1
module	the [module] estimatorparammaps or ||| [module]	count=1
function	that all the objects ||| object size	count=1
function_arg	[function_1] [arg_2] maxiter=100 regparam=0 0 elasticnetparam=0 ||| [function_1] [arg_2]	count=2
module	the [module] elasticnetparam or ||| [module]	count=1
function	from the ||| load	count=1
function	the partition ||| partition	count=1
function	param with ||| has	count=1
function	the stage [function_2] ||| [function_1] [function_2] stageid	count=1
function	column for distinct ||| distinct	count=1
function	that match regexp ||| regexp	count=1
arg	parses the ||| json_string	count=1
arg	module in modlist ||| modlist	count=1
arg	to ||| to	count=1
module	[module] trainratio or ||| [module]	count=1
function	model ||| create model	count=1
arg	[arg_1] predictioncol="prediction", maxiter=100 ||| [arg_1] [arg_2]	count=2
arg	[arg_1] each element ||| [arg_1] [arg_2]	count=6
function	data into disks ||| spill	count=1
function	create an [function_2] ||| [function_2] [function_1]	count=1
module	is an array ||| mllib	count=1
class	values ||| standard scaler	count=1
class	one or more examples ||| decision tree model	count=1
class	gives the ||| summary	count=1
module	defined on ||| param	count=1
function	generates python code for ||| code	count=1
class	of objects ||| serializer	count=1
function_arg	model reference ||| model java_model	count=1
function	tree pattern ||| tree	count=2
class	return ||| scaler	count=1
class	sets ||| reg param	count=1
function	of weights ||| weights	count=1
function	after [function] >>> ||| [function]	count=1
class	an rdd created by ||| rdd	count=1
function	count the ||| count	count=1
module	gets the [module] featureindex or ||| [module]	count=1
function	products [function_2] ||| [function_2] [function_1]	count=3
class	matrix attributes ||| matrix	count=2
function	function to get ||| get	count=1
module	documentation of ||| ml param	count=1
function	parse [function_2] ||| [function_2] [function_1]	count=1
module	gets the [module] thresholds or ||| [module]	count=1
function	topicdistributioncol [function_2] ||| [function_1] [function_2]	count=4
arg	from start [arg_2] ||| [arg_1] [arg_2]	count=2
function	[function_1] a python ||| [function_2] [function_1]	count=4
class	with a function ||| user defined function	count=1
function	a global ||| global	count=1
function	[function_1] the spark_home ||| [function_1] [function_2]	count=1
class	of a dataframe ||| data frame	count=1
class	instance's params ||| java params	count=1
function	default value ||| default	count=1
function	system ||| partition	count=1
function	session ||| session	count=1
class	:class dataframe with ||| data frame	count=1
class	rdd to the ||| rdd	count=1
function	[function_1] table in ||| [function_2] [function_1]	count=2
class	awaitanytermination() can be used ||| manager	count=1
class	specifies the input ||| data frame reader	count=1
function_arg	[function_1] format at ||| [arg_2] [function_1]	count=4
arg	given [arg] ||| name [arg] ext	count=1
class	the content ||| writer	count=1
function	window of ||| window	count=1
arg	property ||| key	count=1
function	rate for a given ||| rate	count=1
module_class	mark this [class_2] ||| [module_1] [class_2]	count=4
arg	saves the ||| name format mode partitionby	count=1
function	of each ||| by	count=1
module	of binomial ||| ml	count=2
function	types inferred ||| type	count=1
class	standardscaler ||| standard scaler	count=1
function_arg	[function_1] representing the ||| [arg_2] [function_1]	count=1
function	parse ||| parse	count=2
module	pyspark sql ||| sql	count=2
arg	data from a dstream ||| dstream	count=1
class	attr lda keeplastcheckpoint ||| distributed ldamodel	count=1
arg	[arg_1] database ||| [arg_2] [arg_1]	count=4
function	this ||| add	count=2
module	gets the [module] a ||| [module]	count=1
function_arg	[function_1] file and ||| [arg_2] [function_1]	count=2
arg	in sql statements ||| f returntype	count=1
module	of this instance this ||| ml param	count=1
module_class	return the [class_2] ||| [module_1] [class_2]	count=3
function	distance from a sparsevector ||| distance	count=2
class	already ||| group	count=1
module_class	[module_1] byte ||| [module_1] [class_2]	count=2
module	gets the [module] tolowercase or ||| [module]	count=1
function	stages ||| get	count=1
class	objects ||| merger	count=1
module	gets the [module] weightcol ||| [module]	count=1
function	of rows of ||| row	count=1
module	the [module] n ||| [module]	count=1
class	found [class] was ||| [class]	count=1
function	find the [function_2] ||| [function_1] [function_2]	count=1
function	[function] of ||| recover [function]	count=1
function	finding frequent [function_2] ||| [function_1] [function_2]	count=4
function	right ||| full	count=1
module	the [module] layers ||| [module]	count=1
class	train a [class_1] [class_2] using stochastic gradient descent ||| [class_1] [class_2]	count=1
arg	rdd, a ||| schema samplingratio verifyschema	count=1
arg	numpartitions partitions ||| numpartitions	count=2
class	checkpointinterval=10 impurity="variance", ||| decision tree regressor	count=1
function	profiler ||| profiler	count=1
module	and value class ||| core	count=2
function	sort ||| sort	count=1
function	the predicted ||| prediction	count=2
module_class	the [module_1] [class_2] file ||| [module_1] [class_2] writer	count=1
function	sum ||| sum	count=1
class	pipelinemodel create ||| pipeline model	count=1
module_class	array of [class_2] ||| [module_1] [class_2]	count=2
arg	for aggregator by ||| doc	count=1
arg	value of the date ||| date	count=1
function	with the frame boundaries ||| range between	count=1
class	set ||| option utils	count=1
function	the column mean values ||| mean	count=1
arg	dataframe ||| data	count=1
function	returns the mean ||| mean	count=3
arg	the specified database ||| tablename dbname	count=2
class	given parameters in ||| param grid builder	count=1
arg	according to ||| decayfactor	count=1
class	gives the ||| linear	count=3
function	group if ||| group	count=1
function	[function_1] the driver ||| [function_1] [function_2]	count=1
arg	binary or multiclass classification ||| numclasses categoricalfeaturesinfo	count=1
class	pipelinemodel used ||| pipeline model	count=1
class	spark fair ||| spark	count=1
function	[function_1] group ||| [function_1] ids for [function_2]	count=1
module	of nonzero elements ||| ml linalg	count=1
function	week number of ||| weekofyear	count=1
function_arg	[function_1] an rdd ||| [arg_2] [function_1]	count=2
class	a [class] by adding ||| [class]	count=1
arg	file ||| path schema sep encoding	count=2
arg	for data ||| data distname	count=1
function_arg	[function_1] word ||| [arg_2] [function_1]	count=2
module	files in ||| core	count=1
module	runs and profiles the ||| core	count=1
module	the [module] predictioncol or ||| [module]	count=1
function	[function_1] main ||| [function_2] [function_1]	count=1
class	for ||| merger	count=1
arg	len [arg_2] ||| [arg_2] [arg_1]	count=1
function	[function] of ||| [function]	count=12
function	[function_1] stream ||| [function_2] [function_1]	count=10
class	set ||| spark conf	count=1
function	the approximate ||| approx	count=1
class	be ||| query manager	count=2
function	containing a json string ||| from json	count=1
arg	start ||| start	count=5
function	degree ||| degree	count=1
class	accumulator's data ||| accumulator	count=1
class	write() ||| mlwritable	count=1
function	the minutes of ||| minute	count=1
class	external sort when ||| external	count=1
arg	[arg_1] expected ||| [arg_1] [arg_2]	count=1
module	an ||| mllib	count=4
arg	specified ||| tablename	count=1
arg	[arg_1] [arg_2] ||| substring str [arg_1] [arg_2]	count=1
function	the initial value ||| set initial	count=1
function	params are ||| params	count=1
function	java object ||| from java	count=1
class	:class dataframe in ||| data frame writer	count=2
function	file system uri ||| file	count=1
function	reduce ||| default reduce	count=1
function	of freedom for the ||| of freedom	count=1
module	this instance contains a ||| ml	count=1
function	on the driver ||| on driver	count=3
class	[class_1] matrix this ||| [class_2] [class_1]	count=1
module	gets the [module] gaps ||| [module]	count=1
class	dataframe in a ||| data frame writer	count=1
function	the cluster [function_2] ||| [function_1] [function_2]	count=6
class	of nodes in tree ||| tree	count=1
function	offsetrange of specific ||| offset ranges	count=3
function	a right ||| full	count=1
module	the [module] bucketlength ||| [module]	count=1
arg	[arg_1] featurescol="features", predictioncol="prediction", ||| [arg_2] [arg_1]	count=2
class	for this query ||| streaming query	count=2
function	partitioned ||| spill	count=1
function	matrix columns ||| matrix columns	count=2
class	mixture ||| gaussian mixture model	count=4
arg	[arg] maxdepth=5 maxbins=32 ||| featurescol [arg]	count=4
module	so that :func ||| sql	count=1
function	dot product ||| dot	count=2
module	of each cluster for ||| ml	count=1
module	param with ||| param	count=1
function	inside brackets pairs ||| brackets split	count=1
class	this udt ||| user defined	count=1
module	adds two ||| linalg	count=1
function_arg	[function_1] [arg_2] ||| [function_1] path mode partitionby [arg_2]	count=12
function	number of top ||| num top	count=1
module_class	[module_1] a randomly ||| [module_1] [class_2]	count=9
function	[function_1] script file ||| [function_1] [function_2]	count=2
function	sliding window ||| window	count=1
class	the rdd partitioned using ||| rdd	count=1
class	a test ||| test	count=1
class	a default otherwise ||| spark	count=1
arg	:class dataframe representing ||| sqlquery	count=1
function	an [function_2] ||| [function_2] [function_1] inputformatclass keyclass valueclass keyconverter	count=3
class	to wait ||| streaming	count=1
arg	function to each element ||| f preservespartitioning	count=2
arg	using the specified partitioner ||| numpartitions partitionfunc	count=1
class	awaitanytermination() ||| query manager	count=2
function	[function_1] place ||| [function_1] [function_2]	count=1
function	vector size ||| vector size	count=2
arg	rdd of document to ||| document	count=1
class	into ||| external group by	count=2
function	months [function_2] ||| [function_1] [function_2]	count=1
class	regression model [class_2] ||| [class_2] [class_1]	count=3
function	of months between ||| months between	count=2
module	pairs [module] are ||| [module]	count=1
class	tree ||| tree	count=4
arg	maxiter=100 tol=1e-6 seed=none ||| maxiter	count=1
class	again ||| manager	count=1
class	:func awaitanytermination() can ||| streaming query manager	count=1
arg	the centroids [arg_2] ||| [arg_2] [arg_1]	count=6
function	inserts ||| insert into	count=1
arg	first n ||| n	count=1
function	of the mean squared ||| mean squared	count=1
class	predictions which ||| generalized linear regression	count=2
function	four clusters ||| train	count=1
function	stream ||| stream from	count=1
function	match regexp ||| regexp	count=1
class	new terminations ||| streaming	count=1
module	operation ||| core	count=1
class	paired rdd where the ||| matrix factorization model	count=1
function	url of ||| url	count=1
module	sql ||| sql	count=2
function	local representation this discards ||| local	count=1
module_class	[module_1] [class_2] by adding a column ||| [module_1] [class_2] with column colname col	count=1
arg	labelcol="label", [arg_2] ||| [arg_2] [arg_1]	count=32
function	default param ||| param	count=1
class	this sqltransformer ||| sqltransformer	count=1
function	mean average precision map ||| mean average precision	count=1
module	gets the [module] linkpower ||| [module]	count=1
class	the input ||| data frame reader	count=1
module_class	statistic [class_2] ||| [module_1] test [class_2] p value	count=1
function	mean [function_2] ||| [function_2] [function_1]	count=6
arg	labelcol="label", [arg_2] ||| [arg_1] [arg_2] maxdepth	count=4
function	[function_1] the count ||| [function_2] [function_1]	count=4
arg	date ||| date	count=3
function	return the [function] pairs ||| collect as [function]	count=1
class	in tree [class_2] ||| [class_1] [class_2]	count=1
class	singular vectors of ||| singular	count=2
function	context to use for ||| context	count=4
function	converts matrix columns in ||| convert matrix columns	count=2
arg	in the user-supplied param ||| param	count=1
module	union [module] this frame ||| [module]	count=1
function	stratified sample without ||| sample	count=1
module	gets the [module] quantileprobabilities or ||| [module]	count=1
arg	saves ||| format mode partitionby	count=2
class	loading ||| java mlreader	count=1
class	'x' to all mixture ||| gaussian mixture model	count=1
function	model [function_2] ||| mllib streaming kmeans test [function_2] [function_1]	count=1
class	this instance with ||| one vs rest	count=1
function	null ||| null	count=2
class	into ||| external group	count=1
class	with a given ||| params	count=1
function	the approximate quantiles ||| approx	count=1
function	slideduration in seconds ||| slide duration	count=1
class	get ||| spark conf	count=1
arg	labelcol="label", featurescol="features", predictioncol="prediction", ||| labelcol featurescol predictioncol	count=2
module	an ||| mllib linalg	count=1
class	stop ||| streaming context	count=1
function	[function_1] weights ||| [function_2] [function_1]	count=2
function	model on toy data ||| on model	count=1
function	the mean [function_2] ||| [function_2] [function_1]	count=4
class	again to wait for ||| streaming	count=1
function	so that :func ||| reset	count=1
function	the companion ||| transfer	count=1
module_class	returns a ||| sql spark	count=1
class	[class] some ||| [class]	count=1
function	components ||| soft	count=1
module_class	gets [module_1] [class_2] or its default value ||| [module_1] [class_2]	count=1
function	the old hadoop ||| save as hadoop	count=1
class	mean ||| scaler model	count=1
arg	an object is of ||| obj identifier	count=1
class	can be used again ||| streaming query	count=1
class	model ||| naive bayes model	count=1
module	[module] aggregationdepth ||| [module]	count=1
function_arg	__init__(self featurescol="features", ||| init featurescol	count=7
function	py or zip ||| py	count=1
function_arg	+= operator [arg_2] ||| [function_1] [arg_2]	count=2
class	awaitanytermination() ||| query	count=1
class	randomly [class_2] ||| [class_2] [class_1] copy	count=2
module_class	[module_1] number ||| [module_1] linalg [class_2]	count=2
class	partitioned ||| external group	count=1
arg	a :class dataframe representing ||| sqlquery	count=1
function	[function_1] cols ||| [function_1] [function_2]	count=5
function_arg	a java [arg_2] ||| [arg_2] [function_1]	count=1
class	saves the content ||| writer	count=1
class	sets ||| regex tokenizer	count=2
function_arg	[function_1] [arg_2] impurity="variance", subsamplingrate=1 ||| regressor [function_1] [arg_2]	count=1
class	this instance with a ||| one vs rest model	count=1
function	percentile ||| set percentile	count=1
module	the [module] estimator ||| [module]	count=1
function	separators inside brackets ||| brackets split	count=1
function	adds a ||| add	count=1
function	inner logic [function] instance based ||| [function]	count=1
class	the :class dataframe to ||| frame writer	count=1
function	queries so ||| reset	count=1
function	the objects ||| object size	count=1
module	gets the [module] n or ||| [module]	count=1
class	given parameters in ||| builder	count=1
class	optional default value ||| params	count=1
arg	spark sink deployed on ||| addresses storagelevel maxbatchsize	count=1
function	the explained [function_2] ||| [function_2] [function_1]	count=2
function_arg	[function_1] number ||| [function_1] products for users [arg_2]	count=1
class	rdd which is assumed ||| rdd	count=1
module_class	[module_1] block matrix ||| [module_1] [class_2]	count=8
class	[class] containing ||| [class]	count=1
function	instance ||| has param	count=1
function	[function_1] transform ||| [function_2] [function_1]	count=3
class	can be used again ||| streaming query manager	count=1
function	inherit documentation ||| inherit	count=1
class	[class] instance ||| [class]	count=3
arg	[arg_1] [arg_2] maxiter=100 regparam=0 0 elasticnetparam=0 ||| [arg_1] [arg_2]	count=9
function	[function_1] squared ||| [function_1] [function_2]	count=7
function	awaitanytermination() ||| reset	count=1
function	predict the ||| predict	count=2
arg	tests whether this instance ||| paramname	count=1
module	and return [module] ||| [module]	count=1
function_arg	update the [arg_2] ||| [function_1] [arg_2]	count=1
class	the :class dataframe in ||| data frame	count=2
class	attr predictions [class_2] ||| [class_2] [class_1]	count=3
function	with ||| with	count=2
arg	n elements in ||| n	count=1
arg	the input param ||| param	count=1
arg	of substr ||| substr	count=1
function	list that contains all ||| collect	count=1
arg	datum ||| datum	count=1
class	for which ||| regression	count=1
module	[module] featuresubsetstrategy ||| [module]	count=1
arg	elements in seen in ||| windowduration slideduration	count=1
function	a batch of ||| batch	count=2
arg	input dataset this is ||| dataset	count=1
function	rows of ||| row	count=1
arg	number ||| num	count=4
class	paired rdd where the ||| factorization model	count=1
function	of each [function_2] ||| [function_2] [function_1]	count=1
arg	url url ||| url	count=1
module	[module] droplast or ||| [module]	count=1
function	docconcentration ||| doc concentration	count=2
function	brackets ||| brackets split	count=1
module	that :func awaitanytermination() ||| sql	count=1
function	for each key ||| key	count=2
function	[function_1] of tables/views ||| [function_2] [function_1]	count=1
function	python code for ||| code	count=1
arg	after position pos ||| str pos	count=1
class	with a randomly generated ||| cross validator model	count=1
module	a given ||| ml param	count=1
arg	from the param ||| param	count=1
class	this tests ||| tests	count=1
arg	end exclusive increased by ||| end	count=1
class	a randomly generated uid ||| cross validator	count=1
function	[function_1] under ||| [function_1] [function_2]	count=5
class	underlying :class ||| session	count=2
arg	spark sink ||| maxbatchsize	count=1
class	a param with ||| params	count=1
class	hadoop ||| spark context	count=1
module	union [module] frame ||| [module]	count=1
arg	samples drawn ||| numrows numcols	count=4
module	values ||| mllib	count=1
function_arg	update the [arg_2] ||| [function_1] data decayfactor [arg_2]	count=1
function	calculates the length ||| length	count=1
class	to wait for ||| query	count=1
function	accuracy equals ||| accuracy	count=1
class	[class_1] validator ||| [class_1] [class_2]	count=2
module	return a javardd ||| core	count=1
arg	given path the model ||| path	count=1
function	aggregate the ||| aggregate	count=1
module	containing elements ||| core	count=1
class	attr lda keeplastcheckpoint is ||| ldamodel	count=1
module	gets the [module] estimator or ||| [module]	count=1
function	the max value ||| max	count=1
function	[function_1] on the ||| [function_1] [function_2]	count=3
function	coefficients ||| coefficients	count=1
function	a given ||| has param	count=1
arg	experimental ||| timeout confidence	count=1
module	gets the [module] min or ||| [module]	count=1
function	[function_1] brackets ||| [function_1] [function_2]	count=1
function	[function_1] [function_2] ||| [function_1] col [function_2]	count=4
function	with [function] ||| test [function] on	count=1
function_arg	[function_1] [arg_2] family="gaussian", link=none fitintercept=true maxiter=25 ||| [function_1] [arg_2] family	count=1
module	one ||| core	count=1
arg	setparams(self [arg] seed=none ||| [arg] seed	count=1
function	internal function to get ||| get	count=1
module	the [module] numitemblocks ||| [module]	count=1
function	[function_1] nodes summed ||| [function_2] [function_1]	count=1
function	seed ||| seed	count=1
class	a function and ||| user defined function	count=1
module	the [module] fitintercept ||| [module]	count=1
class	:py attr lda ||| distributed ldamodel	count=1
arg	at the specified ||| compression	count=1
arg	string in ||| s	count=1
function	[function_1] [function_2] using ||| [function_1] [function_2] zerovalue	count=8
function	is close to ||| parameter accuracy	count=1
class	ranking ||| ranking	count=2
class	batches of data from ||| linear algorithm	count=1
class	queries ||| streaming query manager	count=2
class	__init__(self ||| tree classifier	count=1
module	in ||| sql	count=12
function_arg	squared distance [arg_2] ||| [arg_2] [function_1]	count=1
function	rows in ||| count	count=1
module	the files in ||| core	count=1
class	[class] as a ||| [class]	count=2
function	after [function] >>> df ||| [function]	count=1
arg	parammap ||| pyparammap	count=1
arg	multiclass classification ||| cls data numclasses categoricalfeaturesinfo	count=1
function	to list ||| to list	count=2
function	keyed ||| values	count=1
module	gets the [module] predictioncol ||| [module]	count=1
class	impurity="variance", seed=none variancecol=none) ||| decision tree regressor	count=1
function	dot product of two ||| dot	count=1
class	this ||| external	count=1
function	utc ||| utc	count=2
arg	function without changing ||| f	count=1
function	columns ||| columns from	count=2
function	until any of ||| await any	count=1
class	that :func awaitanytermination() can ||| streaming	count=1
arg	optional key function ||| key	count=1
arg	dataset and an ||| dataset	count=1
arg	mindocfreq=0 inputcol=none outputcol=none) ||| mindocfreq inputcol outputcol	count=2
class	return ||| streaming	count=1
function	name ||| set name	count=1
arg	predictioncol="prediction", labelcol="label", [arg_2] ||| [arg_1] [arg_2]	count=1
class	vector to ||| sparse vector	count=1
function	for udf registration ||| udf	count=1
arg	maxcategories=20 ||| maxcategories	count=1
module_class	of the [module_1] [class_2] ||| [module_1] [class_2]	count=4
class	:class dataframe ||| frame	count=1
function_arg	[function_1] labelcol="label", ||| [function_1] [arg_2]	count=1
module	[module] numbuckets or ||| [module]	count=1
module	[module] mininstancespernode ||| [module]	count=1
function	of names of ||| names	count=1
function_arg	[function_1] [arg_2] ||| [function_1] [arg_2] checkpointinterval	count=2
module	the [module] scalingvec ||| [module]	count=1
class	columns of a dataframe ||| data frame	count=1
module	of object by ||| ml	count=1
class	count ||| rdd	count=1
function	a temporary table in ||| table	count=1
function	memory for this ||| size	count=1
class	return ||| standard	count=1
function_arg	__init__(self [arg_2] ||| [arg_2] [function_1]	count=47
class	[class] on ||| bisecting kmeans [class]	count=3
class	this [class] in ||| [class]	count=2
class	already partitioned data ||| external group	count=1
class	the :class dataframe using ||| data frame	count=1
function	the norm ||| norm	count=3
arg	for a condition ||| condition	count=1
function	of functions registered ||| functions	count=1
function	from an [function_2] ||| [function_2] [function_1] inputformatclass keyclass valueclass keyconverter	count=3
function	matrix columns ||| matrix columns from ml	count=1
class	[class] given ||| [class]	count=2
function	ignore [function_2] ||| [function_2] [function_1]	count=2
function_arg	size default 5 ||| size windowsize	count=1
function_arg	[function_1] numpartitions partitions ||| [arg_2] [function_1]	count=1
function	associated with this streamingcontext ||| context	count=1
class	attr lda keeplastcheckpoint is ||| distributed ldamodel	count=1
arg	of the date ||| date	count=1
function_arg	size [arg_2] ||| [function_1] [arg_2]	count=2
function	the value of ||| get	count=1
class	wait for ||| streaming	count=1
arg	applies [arg] to all ||| [arg]	count=1
function	messagehandler ||| message handler	count=2
module	[module] weightcol ||| [module]	count=1
module_class	of this [class_2] ||| [module_1] [class_2] copy	count=1
arg	labelcol="label", featurescol="features", [arg_2] ||| [arg_1] [arg_2] family	count=2
module	gets the [module] indices ||| [module]	count=1
function_arg	that is [arg_2] ||| [function_1] [arg_2]	count=1
class	paired rdd where ||| matrix factorization	count=1
class	already partitioned ||| external group	count=1
module	the [module] quantileprobabilities or ||| [module]	count=1
module	given timezone returns ||| sql	count=1
class	that :func ||| manager	count=1
arg	outputformat api ||| outputformatclass keyclass	count=1
class	:func ||| manager	count=1
class	the returned ||| test case	count=1
class	[class_1] context ||| [class_1] [class_2] remember duration	count=4
function	table in the ||| table	count=1
module	javardd of object by ||| ml	count=1
function	features corresponding ||| features	count=1
function	[function_1] threshold that ||| [function_1] [function_2]	count=1
arg	the string column to [arg_1] [arg_2] ||| sql rpad [arg_1] [arg_2]	count=1
function	the true label ||| label col	count=2
class	return ||| py spark streaming	count=1
module	[module] finalstoragelevel or ||| [module]	count=1
class	for new ||| manager	count=1
function	companion ||| transfer	count=1
class	sets ||| input cols	count=1
class	mixture ||| mixture model	count=4
module	defined on the ||| param	count=1
module	c{self} and ||| core	count=2
function	labeledpoint ||| lib svmfile	count=2
arg	scale < 0 ||| scale	count=1
arg	[arg_1] inputcol=none outputcol=none) ||| [arg_1] [arg_2]	count=8
function	with the ||| range between	count=1
function	[function_1] stream messagehandler ||| [function_1] [function_2]	count=2
class	dataframe ||| data frame writer	count=1
module	gets the [module] usercol ||| [module]	count=1
class	already partitioned ||| group	count=1
function	original ||| original	count=1
function	offset specified ||| from offset	count=1
class	the left singular vectors ||| singular	count=1
function	day in utc ||| utc	count=2
class	for new terminations ||| query	count=1
class	the ||| external merger	count=2
class	that ||| streaming query	count=1
function	false [function_2] ||| [function_1] [function_2]	count=2
function	:class dataframe ||| data frame	count=1
module	of c{self} and c{other} ||| core	count=2
class	sets ||| multiclass classification evaluator	count=1
module	variance and ||| core	count=1
function	true [function_2] ||| [function_2] [function_1]	count=7
module_class	[module_1] bisecting ||| [module_1] [class_2]	count=4
module	the [module] variancecol ||| [module]	count=1
class	awaitanytermination() can be used ||| streaming	count=1
arg	observed ||| observed	count=1
function_arg	[function_1] linearregressionmodel ||| [function_1] [arg_2]	count=1
module	contains a param ||| ml	count=1
function	tcp server [function_2] ||| [function_2] [function_1]	count=2
module_class	compute the [class_2] ||| [module_1] [class_2]	count=4
function	rowmatrix ||| row	count=2
arg	[arg_1] numbits ||| [arg_1] [arg_2]	count=1
function	return sparkcontext ||| spark	count=1
function_arg	[function_1] [arg_2] ||| [function_1] users product [arg_2]	count=6
function_arg	computes hex value ||| hex col	count=1
arg	saves ||| name format mode partitionby	count=1
function_arg	[function_1] item ||| [arg_2] [function_1]	count=6
class	for feature selection by ||| chi sq selector	count=4
function	parammap into a java ||| to java	count=1
class	weightcol=none [class_2] ||| [class_1] [class_2]	count=2
function_arg	[function_1] [arg_2] ||| [function_1] half life halflife [arg_2]	count=1
function	greatest value of ||| greatest	count=1
arg	from an :class rdd, ||| schema samplingratio verifyschema	count=1
module_class	cluster assignments cluster [module_1] [class_2] trained on the ||| [module_1] gaussian mixture [class_2]	count=1
module	a param ||| param	count=1
function	the week number of ||| weekofyear	count=1
class	batches of data from ||| algorithm	count=1
class	partitioned data into disks ||| external group by	count=1
function_arg	a new dstream by [function_1] [arg_2] ||| [function_1] [arg_2]	count=8
class	for this idf ||| idf	count=1
function	much of memory for ||| object size	count=1
class	:class dataframe, ||| data frame	count=1
function	an [function_2] ||| [function_2] [function_1]	count=3
module	in obj ||| sql	count=1
function	for the null ||| null	count=1
class	:func ||| streaming query manager	count=2
arg	dictionary of values ||| size values	count=1
class	again to wait for ||| query manager	count=1
function	gets a ||| get	count=2
arg	[arg_1] forceindexlabel=false) ||| [arg_2] [arg_1]	count=7
class	this model ||| generalized linear regression model	count=1
module	the [module] vectorsize or ||| [module]	count=1
function	dump already partitioned data ||| spill	count=1
function	of nodes ||| nodes	count=1
arg	[arg_1] classification ||| [arg_2] [arg_1]	count=4
module	containing union [module] ||| [module]	count=3
arg	using either [arg] ||| [arg]	count=6
arg	numtopfeature ||| numtopfeatures	count=1
function	in ||| col	count=1
function_arg	[function_1] [arg_2] a [[structtype]] or [[arraytype]] ||| [function_1] json [arg_2]	count=1
function	count of col ||| count	count=2
function	with the frame ||| range	count=1
arg	in a data source ||| source	count=2
class	gaussians in mixture ||| gaussian mixture	count=1
function	until any of the ||| await any	count=1
function	find synonyms of a ||| find synonyms	count=1
class	function ||| function	count=1
arg	external database table ||| table mode properties	count=1
function	total [function_2] ||| [function_2] [function_1]	count=1
module_class	[module_1] [class_2] at least as extreme ||| [module_1] test [class_2] p value	count=1
arg	numfolds=3 ||| numfolds	count=1
function	dataframestatfunctions for statistic functions ||| stat	count=1
class	param with ||| params	count=1
function	[function_1] [function_2] into py4j which could ||| sql [function_1] [function_2]	count=1
module_class	[module_1] [class_2] or its default value ||| [module_1] [class_2] discretizer get handle invalid	count=1
function	list of names of ||| table names	count=1
function	each key using an ||| by key	count=1
class	to all mixture ||| mixture	count=1
arg	results as ||| schema primitivesasstring prefersdecimal	count=2
class	data ||| external	count=1
class	for input user and ||| model	count=1
module	hadoop inputformat [module] ||| [module]	count=1
class	much of memory ||| external	count=1
arg	an :class rdd, a ||| schema samplingratio verifyschema	count=1
class	the [class] ||| [class]	count=4
class	again to ||| streaming query manager	count=1
module	[module] estimatorparammaps or ||| [module]	count=1
class	a paired rdd where ||| factorization	count=1
class	which is a ||| regression summary	count=2
arg	an rdd ||| rdd	count=2
arg	threshold ||| threshold distcol	count=1
class	[class_1] [class_2] ||| [class_1] [class_2]	count=228
function	numuserblocks ||| num user blocks	count=1
class	losstype="logistic", maxiter=20 ||| gbtclassifier	count=2
class	data ||| streaming	count=1
function_arg	[function_1] [arg_2] mininstancespernode=1 mininfogain=0 ||| [function_1] [arg_2]	count=2
arg	to n ||| n	count=1
function	class inherit documentation ||| inherit doc	count=1
class	a randomly ||| cross validator	count=1
module	the objects ||| core	count=1
function	distinct [function_2] ||| [function_2] [function_1]	count=14
arg	which the centroids ||| timeunit	count=1
class	disks ||| group	count=1
class	which ||| regression model	count=1
class	used again ||| query manager	count=1
function	schema of ||| schema	count=1
function	each rdd ||| by	count=1
arg	given ||| sc	count=3
module	this rdd's elements ||| core	count=1
arg	in the database dbname ||| dbname	count=1
arg	with pad ||| pad	count=1
class	error which is defined ||| regression	count=1
module	column names ||| sql	count=2
class	pipeline create and ||| pipeline	count=1
function	maxiter ||| max iter	count=1
class	:func awaitanytermination() ||| streaming query	count=1
function	system using the new ||| new	count=1
function	vocabulary [function] number ||| vocab [function]	count=2
function	[function] in a ||| [function]	count=1
function	featuresubsetstrategy ||| feature subset strategy	count=1
class	a paired rdd where ||| factorization model	count=1
function	this [function_2] ||| [function_1] [function_2]	count=2
arg	n rows ||| n truncate	count=1
function	has half ||| half	count=1
class	[class] using ||| [class] with	count=1
arg	probabilitycol="probability", rawpredictioncol="rawprediction", ||| probabilitycol	count=1
function_arg	[function_1] applying c{f} ||| [function_1] [arg_2]	count=1
function	create a dense ||| dense	count=1
class	the input java ||| java estimator	count=2
function	start ||| start	count=1
module	all the objects ||| core	count=1
class	function and ||| user defined function	count=1
class	input java ||| java estimator	count=2
class	accumulator with a ||| accumulator	count=1
class	as a :class dataframe ||| data frame	count=3
function	checkpointed ||| checkpointed	count=1
module	all pairs [module] are smaller ||| [module]	count=1
class	in tree ||| decision tree	count=1
class	column ||| standard	count=1
arg	[arg_1] path ||| [arg_2] [arg_1]	count=8
arg	mindocfreq=0 [arg_2] ||| [arg_2] [arg_1]	count=1
arg	a flatmap [arg] ||| [arg]	count=3
function	[function_1] an indexedrowmatrix ||| [function_1] [function_2]	count=2
function	the threshold if any ||| threshold	count=1
class	much ||| external	count=1
function_arg	return a new dstream [function_1] [arg_2] this dstream ||| [function_1] [arg_2]	count=2
class	seed=none impurity="gini", [class_2] ||| [class_2] [class_1]	count=8
class	use ||| spark context	count=1
function	memory for ||| size	count=1
function_arg	[function_1] [arg_2] ||| [function_1] replace [arg_2]	count=3
function	compute the dot product ||| dot	count=2
function	broadcast a read-only variable ||| broadcast	count=1
class	this query ||| streaming query	count=2
function	memory for ||| object size	count=1
arg	a given label ||| label	count=1
function	converts vector [function_2] ||| [function_1] [function_2]	count=8
function	get the ||| get	count=1
function	be ||| reset	count=1
function	creates an [function_2] ||| [function_2] [function_1]	count=1
function	fit ||| fit	count=1
function	an input stream ||| stream	count=2
module	given string ||| param	count=1
class	which is a dataframe ||| logistic regression	count=1
module	in this ||| sql	count=1
function	[function_1] id ||| [function_1] [function_2]	count=3
module	a param with a ||| param	count=1
function	the mean [function_2] ||| [function_1] [function_2]	count=4
class	[class_1] classification evaluator ||| [class_1] [class_2]	count=1
function_arg	distance [arg_2] ||| [arg_2] [function_1]	count=1
class	loads ||| reader	count=3
class	wait for ||| query	count=1
function	number [function] ||| set num [function]	count=3
function_arg	setparams(self [arg_2] ||| [function_1] [arg_2] family	count=3
arg	date1 ||| date1	count=1
module	[module] aggregationdepth or ||| [module]	count=1
function	[function_1] a text ||| [function_1] [function_2]	count=1
class	wait for new ||| query	count=1
module	the [module] weightcol ||| [module]	count=1
function	stop ||| stop	count=3
arg	numfolds=3 seed=none): ||| numfolds	count=1
arg	n rows to ||| n truncate vertical	count=1
class	and count of ||| rdd	count=1
class	sets the accumulator's value ||| accumulator	count=1
function	param with a given ||| has	count=1
class	of memory for this ||| external merger	count=1
function	a param with ||| has	count=1
class	attr predictions which gives ||| generalized linear regression summary	count=1
class	memory for ||| external merger	count=1
class	passed as a list ||| spark conf	count=1
function	a value to ||| to	count=4
class	the model ||| logistic regression	count=1
class	instance with a randomly ||| cross validator model	count=1
class	again to ||| streaming query	count=1
function	the group ||| group	count=1
function	data points in ||| sizes	count=1
function	[function_1] vector size ||| [function_1] [function_2]	count=1
function	[function_1] table ||| [function_1] [function_2]	count=3
class	the sparkcontext ||| spark context	count=1
class	stepsize=0 1 [class] ||| [class]	count=1
arg	user-supplied param ||| param	count=1
function	true positive [function_2] ||| [function_1] [function_2]	count=5
module	a given string ||| param	count=1
function	test [function_1] [function_2] ||| [function_2] [function_1]	count=5
class	:class [class] ||| [class]	count=1
function	cache ||| cache	count=1
class	for this sqltransformer ||| sqltransformer	count=1
function	inputcol=none outputcol=none [function] params ||| [function]	count=1
class	this [class] within ||| [class]	count=1
class	to this accumulator's value ||| accumulator	count=1
class	memory ||| external	count=1
class	content of [class_2] ||| [class_2] [class_1]	count=4
class	all the ||| external merger	count=1
class	stream and ||| stream reader	count=2
function	a ||| has param	count=2
arg	specified database ||| tablename dbname	count=2
class	returns ||| multilabel metrics	count=1
function	word [function] ||| [function]	count=1
class	to in this model ||| model	count=2
class	the model ||| regression model	count=1
function	gets ||| get estimator	count=1
arg	of a dstream ||| dstream	count=1
module	the [module] itemcol ||| [module]	count=1
function	drops the [function_2] ||| [function_2] [function_1]	count=2
arg	centroids ||| timeunit	count=2
function	infer the [function] from dict/namedtuple/object ||| infer [function]	count=1
arg	[arg] containing ||| [arg]	count=1
arg	[arg_1] maxiter=100 tol=1e-6 ||| [arg_2] [arg_1]	count=3
module_class	[module_1] layer ||| [module_1] [class_2]	count=4
arg	term to this accumulator's ||| term	count=1
function	[function] that is ||| [function]	count=2
class	which predictions ||| regression	count=1
class	a default ||| spark	count=1
class	which ||| regression metrics	count=1
function	value to a mllib ||| to	count=1
arg	[arg] "zero ||| [arg] seqop	count=3
function_arg	of users [arg_2] ||| [function_1] [arg_2]	count=1
function	sets vector [function_2] ||| [function_2] [function_1]	count=1
function	for new ||| reset	count=1
function	[function_1] blocks in ||| [function_2] [function_1]	count=4
module	[module] numtrees ||| [module]	count=1
function	py ||| py file	count=2
class	transforms ||| java params	count=2
class	returning ||| reader	count=1
module	the [module] n or ||| [module]	count=1
function_arg	__init__(self [arg_2] ||| regressor [function_1] [arg_2]	count=4
class	[class_1] regression ||| [class_2] [class_1]	count=7
module	to ||| core	count=1
module	gets the [module] predictioncol or ||| [module]	count=1
function	of fit test of ||| test	count=1
arg	multiclass [arg_2] ||| [arg_1] [arg_2]	count=1
class	be used again ||| streaming query	count=1
function	the global temporary ||| global temp	count=2
class	wait ||| query	count=1
class	produced by ||| clustering	count=1
function	so that ||| reset	count=1
function	[function_1] for all ||| [function_2] [function_1]	count=2
function	[function_1] params ||| [function_1] [function_2]	count=8
module_class	gets the [module_1] [class_2] ||| [module_1] [class_2] get	count=8
class	columns on the file ||| data frame	count=1
module	[module] variancepower ||| [module]	count=1
function	[function_1] positive ||| [function_2] [function_1]	count=3
function	[function_1] [function_2] ||| [function_2] [function_1]	count=404
function	predicts ||| predict	count=1
class	sets ||| perceptron classifier	count=4
function	using the given join ||| join	count=1
function	in libsvm format into ||| parse libsvm	count=1
module	return whether this ||| core	count=1
function_arg	a keyed [arg_2] ||| [function_1] [arg_2]	count=1
module	separate arrays [module] ||| [module]	count=2
arg	character in matching ||| matching replace	count=1
module	the [module] standardization ||| [module]	count=1
function	file to which this ||| file	count=1
function	contains a param with ||| has	count=1
arg	(from 1 [arg] ||| [arg]	count=1
function	combine ||| combine	count=1
class	points using the model ||| regression model	count=1
arg	data ||| data	count=4
module	mark this ||| core	count=1
class	[class_1] data source ||| [class_2] [class_1]	count=2
module	and returns ||| sql	count=3
arg	join two [arg] ||| [arg] threshold	count=1
function	summary e g ||| summary	count=4
class	id [class] persists across ||| [class]	count=1
function_arg	newline-delimited json [arg_2] ||| [arg_2] [function_1]	count=1
function	list of names ||| table names	count=1
module	[module] censorcol ||| [module]	count=1
class	data into disks ||| external group	count=1
function	assign a [function] to ||| [function]	count=1
function	ids of all active ||| active stage ids	count=1
module	used again to wait ||| sql	count=1
class	the input ||| stream reader	count=1
class	on ||| with	count=2
arg	outputformat api ||| outputformatclass	count=1
arg	data sampled from a ||| data distname	count=1
arg	document ||| document	count=1
function	[function_1] join of ||| [function_2] [function_1]	count=8
function	view with ||| view	count=1
class	for this ||| external merger	count=1
module	that [module] true iff ||| [module]	count=2
class	new terminations ||| streaming query manager	count=1
function	its [function] ||| get or [function]	count=1
function	inherit documentation from ||| inherit	count=1
function_arg	java [arg_2] ||| [arg_2] [function_1]	count=1
module	memory for ||| core	count=1
function	an rdd of labeledpoint ||| load lib svmfile	count=1
function	stream foreachrdd [function_2] ||| [function_2] [function_1]	count=2
class	tree ||| tree model	count=2
class	used again to ||| streaming query manager	count=1
function	objective ||| objective	count=2
class	of memory ||| external merger	count=1
module	the mean ||| core	count=1
module	instance ||| param	count=1
function	a jvm seq of ||| seq	count=1
class	wait ||| streaming	count=1
function	is close to the ||| parameter accuracy	count=1
class	terms or words in ||| ldamodel	count=1
function	rawpredictioncol ||| raw prediction col	count=1
module	of this instance ||| ml param	count=2
function	items for ||| items	count=1
function_arg	[function_1] [arg_2] 0 tol=1e-6 fitintercept=true standardization=true ||| [function_1] [arg_2]	count=2
module	the [module] windowsize or ||| [module]	count=1
function	memory limit ||| limit	count=1
class	of this [class] using ||| [class]	count=1
function	get the [function_2] ||| [function_2] [function_1]	count=1
class	sparsevector ||| sparse vector	count=1
function	file to ||| file	count=1
function	[function_1] that user ||| [function_2] [function_1]	count=4
arg	or list in ||| oneatatime default	count=1
module	[module] pattern or ||| [module]	count=1
module	[module] featuresubsetstrategy or ||| [module]	count=1
module	by ||| core	count=1
function	only create a new ||| create	count=1
function	a ||| has	count=2
class	query ||| streaming query	count=1
function	[function] arbitrary ||| hadoop [function]	count=3
module_class	return [class_2] ||| [module_1] external [class_2]	count=1
function_arg	type of [arg_2] ||| [arg_2] [function_1]	count=1
class	[class_1] :class dataframereader ||| [class_2] [class_1]	count=1
function	[function] given ||| create [function] in	count=1
arg	reduced into [arg_2] ||| [arg_2] [arg_1]	count=2
function	[function_1] table in ||| [function_1] [function_2]	count=2
function	this instance contains ||| param	count=1
class	all mixture ||| gaussian mixture	count=1
arg	the elements in iterator ||| iterator key	count=1
arg	wait a given ||| timeout catch_assertions	count=1
arg	at pos in byte ||| pos	count=1
class	flume ||| flume utils	count=1
function	compute aggregates and ||| agg	count=1
function_arg	recommends the [arg_2] ||| [function_1] products for users [arg_2]	count=2
function	[function_1] until termination ||| [function_2] [function_1]	count=4
class	input ||| stream reader	count=2
function	groups ||| group by	count=1
arg	n ||| n truncate vertical	count=1
function	splits ||| split	count=1
module	[module] bucketlength ||| [module]	count=1
arg	value ||| aid value	count=1
module	gets the [module] itemcol or ||| [module]	count=1
arg	[arg_1] [arg_2] family="gaussian", link=none fitintercept=true maxiter=25 ||| [arg_1] [arg_2]	count=6
arg	scalingvec=none [arg_2] ||| [arg_2] [arg_1]	count=1
class	into disks ||| group	count=1
function	[function_1] stream ||| [function_1] [function_2]	count=10
class	return the column ||| standard	count=1
function	the levenshtein distance ||| levenshtein	count=1
class	paired rdd where the ||| matrix factorization	count=1
function	as pandas pandas ||| pandas	count=1
class	[class] key-value ||| [class]	count=3
function_arg	[function_1] string in ||| [arg_2] [function_1]	count=1
class	the rdd partitioned ||| rdd	count=1
class	[class_1] regression model ||| mllib [class_1] [class_2] with	count=1
class	tree (e ||| tree model	count=1
function	the offsetrange [function_2] ||| [function_2] [function_1]	count=2
module	gets the [module] featurescol or ||| [module]	count=1
function_arg	[function_1] [arg_2] ||| [function_1] numpartitions [arg_2]	count=1
class	column as a :class ||| data frame	count=1
class	can ||| query manager	count=2
module	[module] percentile or ||| [module]	count=1
function	json ||| json	count=4
arg	document to ||| document	count=1
class	the stream query if ||| data stream writer	count=1
module_class	[module_1] result ||| [module_1] [class_2]	count=1
function_arg	from the [arg_2] ||| [function_1] without unbatching [arg_2]	count=1
function	offsetrange of [function_2] ||| [function_2] [function_1]	count=2
arg	vector ||| vector	count=3
function	sqlcontext [function] throws exception ||| [function]	count=1
function	string format ||| string	count=1
function	[function] the ||| [function] on	count=1
module	string ||| param	count=1
function	a value to a ||| to	count=2
class	of this [class] in ||| [class]	count=2
function	extract the minutes ||| minute	count=1
class	checkpointinterval=10 impurity="variance", seed=none ||| tree regressor	count=1
module	gets the [module] maxmemoryinmb ||| [module]	count=1
function	register a [function_2] ||| [function_1] [function_2]	count=4
class	[class_1] gives ||| [class_2] [class_1]	count=29
arg	chisqselector ||| selectortype	count=1
function	create an rdd ||| create	count=2
arg	for data sampled ||| data distname	count=1
function	or [function_2] ||| [function_2] [function_1]	count=3
class	nodes in tree ||| decision tree	count=1
class	[class_1] [class_2] ||| [class_2] [class_1]	count=294
function_arg	[function_1] a word ||| [arg_2] [function_1]	count=2
class	save ||| regression model	count=1
class	estimated coefficients ||| generalized linear regression training summary	count=1
class	elements ||| rdd	count=2
class	be ||| manager	count=1
function_arg	maximum [arg_2] ||| [function_1] [arg_2]	count=1
function	api with start offset ||| from offset	count=1
class	column ||| standard scaler	count=1
arg	inputcol=none [arg_2] ||| [arg_1] [arg_2]	count=4
module	containing union [module] this frame ||| [module]	count=1
function	the selector type of ||| set selector type	count=1
class	:class list of :class ||| data frame	count=1
class	frequency vectors or ||| tf	count=1
module	queries so that ||| sql	count=1
class	used again to ||| query	count=1
module_class	of [class_2] ||| [module_1] [class_2] copy	count=4
arg	find the n ||| n	count=2
class	new terminations ||| query	count=1
function	this streamingcontext ||| context	count=1
function	or ||| or	count=1
arg	format at [arg_2] ||| [arg_2] [arg_1]	count=2
module	the list of ||| sql	count=2
function	[function_1] converter ||| [function_2] [function_1]	count=1
module	a given string name ||| param	count=1
function	string name ||| has	count=1
class	wait for ||| streaming query	count=1
class	pipeline create ||| pipeline	count=1
function	the new mllib-local representation ||| as ml	count=4
function	least value of the ||| least	count=1
function	create a new ||| init	count=1
class	[class] master ||| [class]	count=1
function	file with the ||| file	count=1
function	ordered ||| ordered	count=1
module	[module] standardization ||| [module]	count=1
class	objects ||| external merger	count=2
arg	the spark sink ||| storagelevel maxbatchsize	count=1
class	seed=none impurity="gini", numtrees=20 ||| random forest classifier	count=2
class	model ||| streaming linear regression	count=1
class	lda keeplastcheckpoint is set ||| distributed ldamodel	count=1
arg	statements ||| name f returntype	count=1
class	of memory for this ||| external	count=1
function	least value ||| least	count=1
class	mixture ||| gaussian mixture	count=2
class	for every feature ||| linear model	count=1
class	the ||| scaler model	count=1
function	explained ||| explained	count=2
arg	dictionary of values ||| values	count=1
function	handler ||| handler	count=1
class	a new spark configuration ||| spark conf	count=1
arg	weights ||| weights	count=1
function_arg	[function_1] [arg_2] containing a [[structtype]] or ||| [function_1] [arg_2]	count=2
class	curve which ||| binary logistic regression	count=1
module	the sum of ||| ml	count=1
function	year of a ||| year	count=1
class	of model ||| ensemble model	count=1
class	content ||| writer	count=1
class	column standard ||| standard scaler model	count=2
function	python parammap into a ||| to	count=1
function	[function_1] the companion ||| [function_2] [function_1]	count=7
function	has completed ||| completed	count=2
class	dump already ||| external group	count=1
class	paired rdd where ||| factorization	count=1
class	mean variance and ||| rdd	count=1
module	copies param ||| param	count=1
class	in "predictions" which gives ||| linear regression summary	count=3
function	of key value ||| key	count=1
function	the trigger for the ||| trigger	count=1
class	[class_1] svm classifier ||| [class_2] [class_1]	count=1
function	with ||| param	count=1
function	log likelihood ||| log likelihood	count=4
class	[class] matrix ||| [class]	count=2
function_arg	[function_1] [arg_2] regparam=0 0 elasticnetparam=0 0 ||| [function_1] [arg_2]	count=2
class	of this ||| streaming	count=2
class	this ||| streaming	count=4
module	[module] numhashtables or ||| [module]	count=1
function	the minimum ||| min	count=2
function	as [function_2] ||| [function_2] [function_1]	count=4
arg	user and product ||| user product	count=3
class	group [class_2] ||| [class_2] [class_1]	count=1
class	already partitioned ||| external	count=1
arg	[arg_1] metricname="f1") ||| [arg_1] [arg_2]	count=2
module	gets the [module] evaluator ||| [module]	count=1
function	solver ||| solver	count=1
function	parammap into ||| param map to	count=1
function	of day in utc ||| from utc	count=1
function	compute the standard deviation ||| stdev	count=1
function	instance to [function_2] ||| [function_1] [function_2]	count=2
function_arg	by other, another ||| multiply other	count=1
module	[module] numitemblocks ||| [module]	count=1
module	two [module_2] ||| [module_1] [module_2]	count=2
module	contains ||| ml	count=1
function	weights computed for every ||| weights	count=1
module	a copy [module] ||| [module]	count=18
class	the :class dataframe to ||| frame	count=1
class	parameters in ||| param grid builder	count=1
function	of top [function_2] ||| [function_1] [function_2]	count=1
arg	outputformat api mapred ||| outputformatclass keyclass	count=1
module	the [module] linkpower ||| [module]	count=1
arg	an object is ||| obj identifier	count=1
function	each tree ||| tree	count=1
function	calculates the approximate ||| approx	count=1
function	seed=none): sets ||| set	count=1
function_arg	group [arg_2] ||| [arg_2] [function_1]	count=1
function	[function_1] a keyed ||| [function_2] [function_1]	count=1
function	users ||| users	count=2
arg	first n elements in ||| n	count=1
function	of the month of ||| dayofmonth	count=1
class	memory for ||| external	count=1
arg	specified [arg_2] ||| [arg_2] [arg_1]	count=1
module	rows in this ||| sql	count=1
class	the decision tree ||| decision tree model	count=1
function	the [function] ||| print [function]	count=3
function	into a ||| to	count=1
class	use for saving ||| java mlwriter	count=1
arg	"num" number of ||| num	count=2
function_arg	[function_1] another ||| [function_1] [arg_2]	count=4
arg	step [arg_2] ||| [arg_2] [arg_1]	count=1
module	of the list of ||| sql	count=2
function	set the initial ||| set initial	count=1
arg	same param ||| param	count=1
class	set ||| kernel density	count=1
function_arg	mathfunction [arg_2] ||| [function_1] [arg_2]	count=1
function	the log likelihood ||| log likelihood	count=2
class	randomly generated ||| train validation split	count=2
class	estimated coefficients ||| linear regression summary	count=1
class	as ||| spark	count=1
module_class	returns [class_2] ||| [module_1] [class_2] with column colname col	count=3
function	probability ||| probability	count=1
module	gets the [module] link ||| [module]	count=1
module	[module] blocksize or ||| [module]	count=1
arg	instance [arg_2] ||| [arg_2] [arg_1]	count=2
class	rdd's elements in one ||| rdd	count=1
module	to this accumulator's ||| core	count=1
function	underlying sql [function_2] ||| [function_1] [function_2]	count=2
function	converts ||| convert	count=4
module	[module] fwe ||| [module]	count=1
function	param ||| has param	count=1
function	objects ||| size	count=1
function	a java storagelevel ||| java	count=1
module	[module] inputcol or ||| [module]	count=1
class	be used ||| streaming query	count=1
function	how much of memory ||| object size	count=1
function	until the group ||| group	count=1
class	vectors or ||| tf	count=1
class	for which predictions ||| regression	count=1
class	which is defined as ||| linear regression summary	count=1
class	underlying :class sparkcontext ||| spark session	count=2
function	topicdistributioncol or its ||| topic distribution col	count=2
arg	inclusive to end inclusive ||| end	count=2
class	[class] featuresubsetstrategy="auto") ||| [class]	count=2
function	to an int ||| to int	count=2
module	names skipping null values ||| sql	count=2
function	so that :func awaitanytermination() ||| reset	count=1
module	[module] trainratio ||| [module]	count=1
class	return the column ||| standard scaler	count=1
class	rdd's elements ||| rdd	count=3
function_arg	chain [arg_2] ||| [arg_2] [function_1]	count=1
module	gets the [module] regparam or ||| [module]	count=1
class	can be used ||| streaming query	count=1
arg	data sampled ||| data distname	count=1
module_class	this block ||| mllib linalg block	count=1
class	training ||| regression training summary	count=1
class	[class] when not ||| [class]	count=1
class	which predictions are ||| regression	count=1
function_arg	[function_1] number ||| [arg_2] [function_1]	count=2
class	the content of the ||| writer	count=1
function	old hadoop ||| save as hadoop	count=1
function_arg	a json [arg_2] ||| [function_1] [arg_2]	count=1
arg	spark sink deployed on ||| storagelevel maxbatchsize	count=1
function	is checkpointed and materialized ||| is checkpointed	count=1
function	aggregate the values of ||| aggregate	count=1
module	instance contains ||| ml	count=1
class	so that :func ||| query	count=1
arg	[arg_1] numfolds=3 ||| [arg_1] [arg_2]	count=2
function_arg	[function_1] [arg_2] maxbins=32 mininstancespernode=1 mininfogain=0 0 ||| [function_1] [arg_2]	count=2
class	this imputer ||| imputer	count=1
class	accumulator ||| accumulator	count=1
arg	given path ||| path	count=7
class	awaitanytermination() can be ||| streaming query	count=1
function_arg	[function_1] [arg_2] mininstancespernode=1 mininfogain=0 0 maxmemoryinmb=256 ||| [function_1] [arg_2] maxdepth	count=1
function	new map ||| map	count=1
function	java model ||| java	count=1
function	the [function] of ||| [function]	count=1
arg	used for ||| dataset	count=1
function	batch ||| batch	count=2
class	terms ||| ldamodel	count=1
function	[function_1] join of ||| [function_1] [function_2]	count=8
class	trees in the ensemble ||| tree ensemble model	count=2
module	[module] numfeatures or ||| [module]	count=1
class	regression ||| regression model	count=1
function	[function_1] under the ||| [function_2] [function_1]	count=5
class	model from ||| power iteration clustering model	count=1
module	of the ||| core	count=1
function	[function_1] positive rate ||| [function_1] [function_2]	count=3
function	number of [function_2] ||| [function_2] [function_1]	count=18
function	[function_1] mean ||| [function_1] [function_2]	count=1
arg	for key ||| key	count=1
class	block matrix ||| block matrix subtract	count=1
arg	droplast=true [arg] ||| droplast [arg]	count=1
function	sets the [function_2] ||| [function_1] [function_2]	count=2
function_arg	[function_1] data ||| [arg_2] [function_1]	count=1
function	outer [function_2] ||| [function_2] [function_1]	count=6
function_arg	[function_1] path ||| [function_1] [arg_2]	count=4
class	this model instance ||| linear regression model	count=2
function	all the ||| object size	count=1
class	[class_1] evaluator ||| [class_2] [class_1]	count=2
module	the [module] variancepower ||| [module]	count=1
function	of iterations default 1 ||| iterations	count=1
function	brackets pairs ||| brackets	count=1
function	[function_1] stream foreachrdd ||| [function_2] [function_1]	count=2
class	for ||| manager	count=1
function	key-value ||| map	count=2
class	resets ||| runtime config	count=1
function	[function_1] the global ||| [function_2] [function_1]	count=1
module	of the rdd's elements ||| core	count=1
class	:py attr predictions which ||| linear regression	count=1
module	gets the [module] aggregationdepth ||| [module]	count=1
module	gets the [module] fwe or ||| [module]	count=1
class	again to wait for ||| streaming query	count=1
function	fields threshold recall ||| recall by threshold	count=1
arg	generates ||| sc mean	count=3
class	used again ||| streaming query	count=1
function	[function_1] list ||| [function_1] [function_2]	count=4
arg	buckets ||| buckets	count=1
arg	[arg] before ||| [arg] delim	count=1
function	be used again ||| reset	count=1
arg	predictioncol="prediction", k=2 [arg_2] ||| [arg_1] [arg_2]	count=2
module	[module] minsupport or ||| [module]	count=1
arg	matching ||| matching	count=1
arg	start to end exclusive ||| start end	count=1
function	from the ||| from ml	count=1
function	a converter to drop ||| converter	count=1
class	trees in the ensemble ||| ensemble model	count=2
function	[function_1] type of ||| [function_1] [function_2]	count=3
module	an array of ||| mllib	count=1
function	indices to select filter ||| selected features	count=1
class	model [class_2] ||| [class_1] [class_2]	count=3
module	of nonzero elements this ||| ml	count=1
function_arg	vector size [arg_2] ||| [function_1] [arg_2]	count=1
class	for the stream ||| data stream writer	count=1
module	with a given ||| ml	count=1
function	script file ||| script	count=1
module	a python wrapper of ||| ml	count=5
function	perform a right outer ||| full outer	count=1
function	[function_1] spark_home ||| [function_1] [function_2]	count=1
module	the [module] k or ||| [module]	count=1
function	positive rate for ||| positive rate	count=2
arg	column [arg] times and ||| col [arg]	count=1
arg	a column ||| col	count=2
arg	input path ||| path	count=2
function	drops ||| drop	count=2
class	paired ||| factorization model	count=1
function	root directory that contains ||| get root directory	count=1
class	for the stream ||| stream	count=1
class	how much ||| external	count=1
class	regression score ||| regression metrics	count=2
arg	predictioncol="prediction", maxiter=100 ||| predictioncol maxiter	count=2
module	gets the [module] minconfidence or ||| [module]	count=1
class	dataframe produced by the ||| clustering	count=1
arg	frame boundaries from start ||| start	count=1
function	[function_1] dataframe from ||| [function_2] [function_1]	count=1
class	the spark fair scheduler ||| spark	count=1
arg	input param belongs to ||| param	count=1
function	of products ||| products	count=1
function_arg	json [arg_2] ||| [function_1] [arg_2]	count=2
class	parameters in ||| grid builder	count=1
module	array of ||| ml	count=1
class	rdd partitioned using the ||| rdd	count=1
class	cachenodeids=false checkpointinterval=10 impurity="variance", ||| regressor	count=1
arg	of another dstream ||| other	count=1
class	param with a ||| params	count=1
module_class	[module_1] [class_2] :class datastreamreader ||| [module_1] [class_2]	count=1
class	left singular vectors ||| singular	count=1
function	of memory for this ||| object size	count=1
class	that ||| streaming query manager	count=2
function	a java storagelevel ||| get java	count=1
function	a json ||| json	count=1
module_class	this [class_2] ||| [module_1] [class_2] subtract	count=1
class	[class_1] model ||| [class_1] [class_2]	count=3
class	a model [class_2] ||| [class_1] [class_2]	count=3
function	from ||| from ml	count=2
module_class	[module_1] estimated ||| [module_1] [class_2]	count=7
rep	[module_class_1] [function_arg_2] ||| [module_class_1] [function_arg_2]	count=40
function	number of cols ||| num cols	count=13
arg	a function ||| f	count=5
function	outputcol ||| output col	count=1
class	column ||| scaler model	count=2
function	sql storage [function_2] ||| [function_1] [function_2]	count=2
arg	formula=none featurescol="features", labelcol="label", forceindexlabel=false) ||| formula featurescol labelcol forceindexlabel	count=2
function	batch has started ||| started	count=1
class	[class_1] sgd ||| [class_1] [class_2] train cls data iterations	count=1
function	iterations until termination ||| total iterations	count=4
class	curve ||| binary classification	count=1
class	on a spark ||| spark	count=1
class	stream and ||| stream	count=1
class	retrieve gaussian ||| gaussian	count=1
class	lda keeplastcheckpoint is ||| ldamodel	count=1
function	replaces a ||| replace	count=1
arg	user and ||| user	count=1
arg	scalingvec=none [arg_2] ||| [arg_1] [arg_2]	count=1
module	the [module] inputcol ||| [module]	count=1
class	adds input ||| frame reader	count=1
function	cost sum of squared ||| cost	count=2
function	create ||| create	count=11
arg	setparams(self droplast=true [arg] ||| droplast [arg]	count=1
function	context ||| context	count=4
module	the [module] indices ||| [module]	count=1
function	or create global ||| or create	count=1
function	a value to list ||| to list	count=1
function	only create [function_2] ||| [function_2] [function_1]	count=2
module	the [module] minconfidence or ||| [module]	count=1
class	log ||| distributed ldamodel	count=1
module	note : deprecated in ||| sql	count=1
function_arg	[function_1] featurescol="features", ||| [arg_2] [function_1]	count=17
class	which is a risk ||| regression summary	count=1
arg	inputformat with ||| inputformatclass keyclass valueclass	count=2
arg	value of ||| col	count=1
class	this pca ||| pca	count=1
arg	neutral ||| zerovalue	count=1
class	truncated at [class] position ||| [class]	count=2
arg	function to the value ||| f	count=2
function	the partition [function_2] ||| [function_2] [function_1]	count=1
class	ensemble ||| tree ensemble model	count=3
function	[function_1] libsvm format ||| [function_1] [function_2]	count=3
module	[module] elasticnetparam or ||| [module]	count=1
function	drops the global ||| drop global	count=3
function	vector columns ||| vector columns to ml	count=1
module	functions and a ||| core	count=1
function	the model ||| model	count=3
class	udt ||| user defined type	count=1
class	to wait for ||| streaming query manager	count=1
function	local [function] view with ||| drop [function]	count=1
class	model ||| regression model	count=1
class	norm ||| sparse vector	count=2
arg	the centroids of that ||| timeunit	count=1
arg	saves the contents of ||| path format mode partitionby	count=1
class	[class] by adding ||| [class]	count=2
class	as a :class dataframe ||| grouped data	count=1
class	data into ||| external	count=1
function	number of rows ||| count	count=1
class	passed as ||| conf	count=1
function	[function] of ||| [function] by	count=1
class	[class_1] the singularvaluedecomposition ||| [class_2] [class_1]	count=2
module	[module_1] [module_2] ||| [module_1] [module_2]	count=8
arg	to consist ||| ascending numpartitions keyfunc	count=1
function	termination of ||| termination	count=1
module	statistic ||| stat	count=1
function	the root [function_2] ||| [function_2] [function_1]	count=1
function_arg	function [arg_2] ||| [function_1] name [arg_2]	count=2
class	of model ||| tree model	count=1
class	update the catalog ||| catalog	count=1
arg	a condition to ||| condition	count=1
class	partitioned data ||| group	count=1
class	trees in the ensemble ||| tree ensemble	count=2
function	print the profile stats ||| profiles	count=1
module	[module] evaluator ||| [module]	count=1
class	model ||| kmeans model	count=2
arg	days after [arg] >>> df ||| [arg]	count=1
function	[function_1] based ||| [function_1] [function_2]	count=1
function	of iterations ||| iterations	count=1
class	:func awaitanytermination() can be ||| streaming query	count=1
arg	with [arg] one ||| [arg]	count=1
class	in "predictions" which ||| regression summary	count=6
function	containing the ids of ||| stage ids	count=1
arg	num ||| num	count=1
class	sets ||| sq selector	count=4
function	that getting ||| test tc	count=1
class	[class_1] [class_2] - ||| [class_1] [class_2]	count=1
module_class	creates a copy [module_1] [class_2] uid and some ||| [module_1] [class_2] copy	count=4
function	be used ||| reset	count=1
function_arg	binary mathfunction [arg_2] ||| [arg_2] [function_1]	count=2
function	[function_1] type ||| [function_2] [function_1]	count=5
function	year of ||| year	count=1
class	cachenodeids=false checkpointinterval=10 impurity="variance", seed=none ||| regressor	count=1
arg	word ||| word num	count=1
arg	ensuring all received ||| stopsparkcontext stopgracefully	count=1
module	gets the [module] impurity ||| [module]	count=1
arg	smaller than [arg] ||| dataseta [arg]	count=1
function	the index of ||| map partitions with index	count=1
module	given string name ||| ml	count=1
class	that ||| external merger	count=2
module_class	in multinomial logistic ||| mllib logistic	count=1
class	singular vectors of the ||| singular	count=2
function	converts vector [function_2] ||| [function_2] [function_1]	count=8
arg	a word ||| word num	count=1
class	so that :func awaitanytermination() ||| streaming query manager	count=1
module_class	[module_1] [class_2] k-means ||| [module_1] [class_2]	count=2
function	data type json ||| datatype json	count=1
function	stop the ||| stop	count=2
module	in the ||| mllib	count=1
class	[class] using ||| linear [class] with	count=1
function	train the model ||| train	count=3
function	to ||| reset	count=1
arg	with extra values ||| extra	count=1
class	rdd is checkpointed locally ||| rdd	count=1
function_arg	test of [arg_2] ||| [function_1] [arg_2]	count=5
function	a single script on ||| single script on	count=1
function	a batch [function_2] ||| [function_1] [function_2]	count=2
class	of memory for ||| external merger	count=1
class	which predictions are known ||| isotonic regression	count=1
function_arg	json file ||| json path	count=1
function_arg	[function_1] column ||| [function_1] json [arg_2]	count=1
module	gets the [module] names ||| [module]	count=1
function	inputcol ||| input col	count=1
function	format or newline-delimited json ||| json	count=1
function	close ||| parameter accuracy	count=1
function	= ||| get	count=1
class	bisecting [class_2] ||| [class_2] [class_1]	count=2
class	vectors which this transforms ||| vector	count=1
module	the [module] mindf or ||| [module]	count=1
arg	predictioncol="prediction", labelcol="label", metricname="f1") ||| predictioncol labelcol metricname	count=3
class	model on the ||| with	count=1
class	rating for the given ||| matrix factorization	count=1
class	[class_1] context to ||| [class_2] [class_1]	count=4
class	with a function and ||| function	count=1
module	throws [module] ||| [module]	count=3
function	ordered in ||| take ordered	count=1
module	[module_1] [module_2] ||| [module_2] [module_1]	count=20
arg	term to ||| term	count=1
function	"zerovalue" which may ||| fold	count=1
module	with arbitrary key and ||| core	count=2
function	weighted averaged [function_2] ||| [function_1] [function_2]	count=1
module_class	[module_1] word2vec ||| [module_1] [class_2]	count=2
function	model on ||| on model	count=2
function	using the new ||| new	count=1
arg	observed data against the ||| observed	count=1
module	column names skipping ||| sql	count=2
function	shut down ||| stop	count=1
arg	centroids according [arg_2] ||| [arg_2] [arg_1]	count=2
arg	equal ||| numiterations	count=1
class	does this [class] need ||| [class]	count=1
class	vectors which this transforms ||| vector indexer	count=1
class	the underlying :class ||| session	count=1
module	adds ||| core	count=1
function	dependencies for debugging ||| to debug string	count=1
class	residuals mse r-squared ||| linear regression	count=1
module_class	gets [module_1] [class_2] ||| [module_1] [class_2] get	count=8
function	greatest value ||| greatest	count=1
function	test ||| test predictions	count=1
arg	the correlation of two ||| col2 method	count=1
arg	start to ||| start	count=1
class	tree (e g depth ||| tree	count=1
function	unique id ||| id	count=1
function	the probability ||| probability col	count=1
function	the count of ||| count	count=1
module	gets the [module] indices or ||| [module]	count=1
class	return an rdd created ||| rdd	count=1
function	gets the name of ||| get	count=1
function	greatest value of the ||| greatest	count=1
function	parammap into a ||| param map to	count=1
module	gets the [module] mintf or ||| [module]	count=1
function	creates [function_2] ||| utils [function_1] polling [function_2]	count=2
class	output ||| writer	count=5
module	arbitrary key and value ||| core	count=2
function	[function_1] using ||| [function_2] [function_1]	count=8
function	model on toy ||| on model	count=2
arg	inputcol=none outputcol=none [arg_2] ||| [arg_2] [arg_1]	count=4
function	gets summary (e g ||| summary	count=1
arg	inputcol=none outputcol=none handleinvalid="error") ||| inputcol outputcol handleinvalid	count=8
function	close to the ||| parameter accuracy	count=1
function	driver ||| driver	count=1
function	window of time ||| window	count=1
function	index ||| with index	count=1
arg	value ||| value	count=2
function	[function_1] a param ||| [function_2] [function_1]	count=1
module_class	files in [class_2] ||| [module_1] [class_2]	count=2
function	of rows ||| count	count=1
function	inherit documentation ||| inherit doc	count=1
function	get the root ||| get root	count=1
function_arg	[function_1] f-measure ||| [arg_2] [function_1]	count=1
class	this instance ||| one	count=2
arg	of the chisqselector ||| selectortype	count=1
class	with a randomly generated ||| validation split model	count=1
class	accumulator with ||| accumulator	count=1
module	[module] outputcol or ||| [module]	count=1
class	return the ||| model	count=1
function	approximate [function_2] ||| [function_1] count [function_2]	count=1
module_class	of this [class_2] ||| [module_1] [class_2]	count=7
class	which gives ||| linear regression	count=6
class	can be ||| streaming query manager	count=1
function_arg	[function_1] type datatype ||| [arg_2] [function_1]	count=1
module	associative and ||| core	count=1
class	wait for new terminations ||| streaming query	count=1
class	the dataframe in a ||| data frame writer	count=1
function	into label ||| parse	count=1
class	a python rdd of ||| rdd	count=4
function	an numpy ndarray ||| array	count=1
class	return the ||| scaler model	count=1
function	monotonically ||| monotonically	count=1
function	[function_1] py ||| [function_2] [function_1]	count=1
function	matrix columns ||| matrix columns to ml	count=1
function	ignore [function_2] ||| [function_1] [function_2]	count=2
arg	iff [arg] is nan ||| [arg]	count=1
function_arg	[function_1] [arg_2] before ||| [function_1] [arg_2] delim	count=1
function	number of columns of ||| num	count=1
function	a sliding window of ||| window	count=1
function	dataframe ||| data frame	count=1
class	used again to ||| streaming query	count=1
class	impurity="variance", ||| tree regressor	count=1
arg	k=none inputcol=none outputcol=none) ||| k inputcol outputcol	count=2
function_arg	[function_1] url url ||| [function_1] [arg_2]	count=1
module	of indices to ||| ml	count=1
module	of two vectors ||| ml linalg	count=2
arg	a column ||| col options	count=1
function	a py or zip ||| py file	count=1
function	converts [function_2] ||| [function_1] [function_2]	count=20
function	[function_1] a keyed ||| [function_1] [function_2]	count=1
function	locate ||| locate	count=1
function	of jobs has completed ||| completed	count=1
class	be used again to ||| streaming	count=1
class	cachenodeids=false checkpointinterval=10 losstype="logistic", ||| gbtclassifier	count=2
arg	featurescol="features", [arg_2] ||| [arg_1] [arg_2] maxiter	count=3
class	can ||| streaming query manager	count=2
function	much of ||| object size	count=1
function	stopwords=none casesensitive=false) sets ||| set	count=1
function	training ||| training	count=1
class	numtrees=20 featuresubsetstrategy="auto", seed=none ||| random forest	count=1
arg	saves the contents ||| format mode partitionby	count=1
arg	to end exclusive ||| end	count=1
module_class	number of [class_2] ||| [module_1] [class_2]	count=4
function	the uid ||| uid	count=1
module	data ||| core	count=1
class	params ||| java params	count=2
function	matrix on [function_2] ||| [function_2] [function_1]	count=4
function	memory for this ||| object	count=1
class	checkpointinterval=10 impurity="variance", seed=none ||| regressor	count=1
class	this instance ||| one vs rest model	count=1
arg	two [arg] ||| [arg] threshold	count=1
arg	labelcol="label", ||| labelcol	count=22
function	queue ||| queue	count=1
function_arg	[function_1] fixed ||| [arg_2] [function_1]	count=2
arg	labels=none) ||| labels	count=2
arg	centroids of that ||| timeunit	count=1
function_arg	users for [arg_2] ||| [arg_2] [function_1]	count=2
arg	for data ||| data	count=1
arg	d samples drawn ||| numrows numcols numpartitions	count=2
class	in a :class windowspec ||| window	count=1
module	[module] are smaller ||| [module]	count=1
arg	creates ||| sparkcontext sparksession	count=1
module_class	[module_1] layers ||| [module_1] [class_2]	count=2
function_arg	param [arg_2] ||| [arg_2] [function_1]	count=1
module_class	a random ||| mllib random	count=1
arg	buckets ||| numbuckets	count=1
arg	either [arg] ||| [arg]	count=6
function	active stages ||| get active	count=1
class	on ||| regression with	count=1
function	root [function_2] ||| [function_1] [function_2]	count=1
arg	reported an ||| receivererror	count=1
function_arg	[function_1] [arg_2] link=none fitintercept=true maxiter=25 tol=1e-6 ||| [function_1] [arg_2] family	count=1
class	this model instance ||| kmeans model	count=1
module	of parameters ||| ml	count=1
function	[function_1] to ||| [function_1] [function_2]	count=5
function	blockmatrix by other, ||| multiply	count=1
function	an indexedrowmatrix ||| indexed row	count=2
function	batch has half the ||| half	count=1
class	queries so ||| manager	count=1
arg	samples drawn ||| std numrows	count=1
function	rdd for dataframe from ||| from	count=1
function	with the frame ||| range between	count=1
arg	wait a given ||| timeout	count=1
arg	been ||| receiverstarted	count=1
module_class	[module_1] [class_2] k-means ||| [module_1] [class_2] kmeans	count=2
module	a ||| ml param	count=4
function	to that user ||| user	count=1
module_class	creates a copy [module_1] [class_2] uid and some ||| [module_1] [class_2]	count=5
module	key and value class ||| core	count=2
arg	checks whether a ||| cls instance gateway conf	count=1
function	a [function] ||| create [function]	count=2
function	from an ||| rdd	count=2
class	dump ||| external group by	count=2
function	with new specified column ||| df	count=1
function	columns in an ||| columns	count=4
function	[function] between python ||| need [function]	count=1
arg	string column to width [arg_1] [arg_2] ||| sql rpad col [arg_1] [arg_2]	count=2
module	in ||| core	count=4
module	cluster sizes [module] the ||| [module]	count=1
function	decode the ||| decoder	count=1
module	sum of ||| ml	count=1
class	ml instance ||| mlreader	count=1
class	fitted model by type ||| generalized linear regression	count=1
class	the [class] trained ||| gaussian mixture [class]	count=1
arg	optional key ||| key	count=1
arg	rdds ||| rdds	count=1
class	a python rdd ||| rdd	count=4
function	in which each rdd ||| by	count=1
class	to use for saving ||| java mlwriter	count=1
module	gets the [module] impurity or ||| [module]	count=1
function_arg	text file ||| text path	count=1
module	gets the [module] link or ||| [module]	count=1
function	[function_1] id ||| [function_2] [function_1]	count=3
class	embedded ||| params	count=1
arg	predictioncol="prediction", k=2 ||| predictioncol k	count=4
arg	to fixed [arg_2] ||| [arg_1] [arg_2]	count=1
class	to all mixture ||| gaussian mixture	count=1
function	hadoop configuration which ||| hadoop	count=1
class	the ||| standard	count=1
function	new hivecontext for testing ||| for testing	count=1
function	when stdin is closed ||| stdin	count=1
function	[function_1] matrix columns ||| [function_2] [function_1]	count=2
function	the number of ||| num	count=4
function	[function] into ||| install [function]	count=1
class	or ||| status tracker	count=1
function	a single script file ||| single script	count=1
arg	dataset ||| dataset featurescol labelcol	count=1
module	the [module] mintokenlength ||| [module]	count=1
module	function [module] the ||| [module]	count=3
class	convert this vector ||| sparse vector	count=1
function	add ||| add	count=2
module_class	[module_1] :py attr ||| [module_1] [class_2] get	count=24
function	given data type json ||| datatype json	count=1
arg	extra params ||| extra	count=6
class	fitted model ||| generalized linear regression	count=1
class	add ||| spark context	count=1
function_arg	[function_1] the observed ||| [function_1] [arg_2]	count=3
class	stored ||| matrix	count=1
arg	app name should be ||| appname sparkhome pyfiles	count=1
module	how much of ||| core	count=1
function	into disks ||| spill	count=1
module_class	[module_1] and parent ||| [module_1] [class_2]	count=2
arg	[arg_1] classification ||| [arg_1] [arg_2]	count=4
class	input ||| data frame reader	count=1
function	a python parammap ||| param map from	count=2
rep	[module_class_1] save path ||| [module_class_1] [function_arg_2]	count=32
function	schema in ||| schema	count=1
class	of parallelism [class] when not ||| [class]	count=1
function	[function_1] block ||| [function_2] [function_1]	count=3
function	of features corresponding to ||| features	count=1
class	the accumulator's ||| accumulator param	count=1
class	a new accumulator ||| accumulator	count=1
function	original column ||| original	count=1
class	of gaussians in mixture ||| mixture model	count=1
class	returns a paired rdd ||| factorization model	count=1
class	number ||| indexer model	count=1
