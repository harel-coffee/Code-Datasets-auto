core		debugprint	obj depth print_type file	print a computation graph as text to stdout or a file
core		pydotprint	fct outfile compact format	print to a file the graph of a compiled theano function's ops supports
core		min_informative_str	obj indent_level _prev_obs _tag_generator	returns a string specifying to the user what obj is the string will print out as much of the graph as is needed
core		var_descriptor	obj _prev_obs _tag_generator	returns a string with no endlines fully specifying how a variable is computed
core		hex_digest	x	returns a short mostly hexadecimal hash of a numpy ndarray
core	Raise	__init__	msg exc	msg - the argument to the exception exc - an exception class to raise in self
core		parse_config_string	config_string issue_warnings	parses a config string (comma-separated key=value components) into a dict
core		fetch_val_for_key	key delete_key	return the overriding config value for a key
core		get_config_md5		return a string md5 of the current config options it should be such that
core		AddConfigVar	name doc configparam root	add a new variable to theano config
core	ConfigParam	__init__	default filter allow_override	if allow_override is false we can't change the value after the import of theano
core		safe_no_dnn_workmem	workmem	make sure the user is not attempting to use dnn conv workmem
core		safe_no_dnn_workmem_bwd	workmem	make sure the user is not attempting to use dnn conv workmem_bwd
core		safe_no_dnn_algo_bwd	algo	make sure the user is not attempting to use dnn conv algo_bwd
core		warn_cxx	val	we only support clang++ as otherwise we hit strange g++/osx bugs
core		warn_default	version	return true iff we should warn about bugs fixed after a given version
core		local_bitwidth		return 32 for 32bit arch 64 for 64bit arch
core		python_int_bitwidth		return the bit width of python int c long int
core		short_platform	r p	return a safe shorter version of platform platform()
core		get_home_dir		return location of the user's home directory
core		ifelse	condition then_branch else_branch name	this function corresponds to an if statement returning and evaluating inputs in the then_branch if condition evaluates to true or
core		ifelse_lift_single_if_through_acceptable_ops	main_node	this optimization lifts up certain ifelse instances
core		dot	l r	return a symbolic matrix/dot product between l and r
core		get_scalar_constant_value	v	return the constant scalar 0-d value underlying variable v if v is the output of dimshuffles fills allocs rebroadcasts cast
core		sparse_grad	var	this function return a new variable whose gradient will be stored in a sparse format instead of dense
core		format_as	use_list use_tuple outputs	formats the outputs according to the flags use_list and use_tuple
core		grad_not_implemented	op x_pos x comment	return an un-computable symbolic variable of type x type
core		grad_undefined	op x_pos x comment	return an un-computable symbolic variable of type x type
core		Rop	f wrt eval_points	computes the r operation on f wrt to wrt evaluated at points given in eval_points
core		Lop	f wrt eval_points consider_constant	computes the l operation on f wrt to wrt evaluated at points given in eval_points
core		grad	cost wrt consider_constant disconnected_inputs	return symbolic gradients for one or more variables with respect to some cost
core		subgraph_grad	wrt end start cost	with respect to wrt, computes gradients of cost and/or from existing start gradients up to the end variables of a
core		_node_to_pattern	node	given an apply node obtain its connection pattern this is just a wrapper around op
core		_populate_var_to_app_to_idx	outputs wrt consider_constant	helper function for grad function
core		_populate_grad_dict	var_to_app_to_idx grad_dict wrt cost_name	helper function for grad function
core		_float_zeros_like	x	like zeros_like but forces the object to have a
core		_float_ones_like	x	like ones_like but forces the object to have a
core	numeric_grad	__init__	f pt eps out_type	return the gradient of f at pt
core	numeric_grad	abs_rel_err	a b	return absolute and relative error between a and b
core	numeric_grad	abs_rel_errors	g_pt	return the abs and rel error of gradient estimate g_pt g_pt must be a list of ndarrays of the same length as self
core	numeric_grad	max_err	g_pt abs_tol rel_tol	find the biggest error between g_pt and self gf
core		verify_grad	fun pt n_tests rng	test a gradient by finite difference method raise error on failure
core		jacobian	expression wrt consider_constant disconnected_inputs	:type expression vector 1-dimensional variable
core		hessian	cost wrt consider_constant disconnected_inputs	:type cost scalar 0-dimensional variable
core		_is_zero	x	returns 'yes', 'no', or 'maybe' indicating whether x is always 0
core		consider_constant	x	deprecated use zero_grad() or disconnected_grad() instead
core		zero_grad	x	consider an expression constant when computing gradients
core		undefined_grad	x	consider the gradient of this variable undefined and generate an error message if its gradient is taken
core		disconnected_grad	x	consider an expression constant when computing gradients while effectively not backpropagating through it
core		grad_clip	x lower_bound upper_bound	this op do a view in the forward but clip the gradient
core		grad_scale	x multiplier	this op scale or inverse the gradient in the backpropagation
misc		_asarray	a dtype order	convert the input to a numpy array
misc		post_gist	content description filename auth	post some text to a gist and return the url
misc		get_pull_request	project num github_api	get pull request info by number
misc		get_pulls_list	project github_api	get pull request list
misc		execute	execute verbose M N	:param execute if true execute a theano function that should call gemm
misc		subprocess_Popen	command	utility function to work around windows behavior that open windows
misc		call_subprocess_Popen	command	calls subprocess_popen and discards the output returning only the exit code
misc		output_subprocess_Popen	command	calls subprocess_popen returning the output error and exit code in a tuple
misc		render_string	string sub	string a string containing formatting instructions sub a dictionary containing keys and values to substitute for
misc	PersistentNdarrayID	_resolve_name	obj	determine the name the object should be saved under
misc		dump	obj file_handler protocol persistent_id	pickles an object to a zip file using external persistence
misc		load	f persistent_load	load a file that was dumped to a zip file
misc		zipadd	func zip_file name	calls a function with a file object saving it to a zip file
misc		cpuCount		returns the number of cpus in the system
misc.hooks		_rstrip	line JUNK	return line stripped of trailing spaces tabs newlines
misc.hooks		get_parse_error	code	checks code for ambiguous tabs or other basic parsing issues
misc.hooks		get_correct_indentation_diff	code filename	generate a diff to make code correctly indented
scalar		as_common_dtype		for for theano scalar scalar and tensorvariable
scalar		get_scalar_type	dtype	return a scalar dtype object
scalar		convert	x dtype	convert the input to a properly typed numpy value according to the current casting policy
scalar		upgrade_to_float		upgrade any int types to float32 or float64 to avoid losing precision
scalar		upgrade_to_float64		upgrade any int and float32 to float64 to do as scipy
scalar		upgrade_to_float_no_complex		don't accept complex otherwise call upgrade_to_float()
scalar	ScalarOp	c_code_contiguous	node name inp out	this function is called by elemwise when all inputs and outputs are c_contiguous
scalar		int_or_true_div	x_discrete y_discrete	return 'int' or 'true' depending on the type of division used for x / y
scalar		div_proxy	x y	proxy for either true_div or int_div depending on types of x y
scalar	Mod	c_code	node name inputs outputs	we want the result to have the same sign as python not the other implementation of mod
scalar	Cast	make_new_inplace	output_types_preference name	this op __init__ fct don't have the same parameter as other scalar op
scalar		cast	x dtype	symbolically cast x to a scalar of given dtype
scalar	Composite	make_new_inplace	output_types_preference name	this op __init__ fct don't have the same parameter as other scalar op
scalar	Composite	init_c_code		assemble the c code for this composite op
scalar	Composite	init_py_impls		return a list of functions that compute each output of self
scalar	Composite	init_name		return a readable string representation of self fgraph
scalar		shared	value name strict allow_downcast	sharedvariable constructor for scalar values default int64 or float64
compat		maybe_add_to_os_environ_pathlist	var newpath	unfortunately conda offers to make itself the default python and those who use it that way will probably not activate envs
compat		_add_doc	func doc	add documentation to a function
compat		_import_module	name	import module returning the module after the last dot
compat	_SixMetaPathImporter	is_package	fullname	return true if the named module is a package
compat		add_move	move	add an item to six moves
compat		remove_move	name	remove item from six moves
compat		with_metaclass	meta	create a base class with a metaclass
compat		add_metaclass	metaclass	class decorator for creating a class with a metaclass
compat		python_2_unicode_compatible	klass	a decorator that defines __unicode__ and __str__ methods under python 2
gpuarray		gpu_svd	a full_matrices compute_uv	this function performs the svd on gpu
gpuarray		gpu_matrix_inverse	a	this function performs the matrix inverse on gpu
gpuarray		version	raises	return the current cudnn version we link with
gpuarray	GpuDnnConv	get_out_shape	ishape kshape border_mode subsample	this function computes the output shape for a convolution with the specified parameters
gpuarray		dnn_conv	img kerns border_mode subsample	gpu convolution using cudnn from nvidia
gpuarray		dnn_conv3d	img kerns border_mode subsample	gpu convolution using cudnn from nvidia
gpuarray		dnn_gradweight	img topgrad kerns_shp border_mode	todo document this
gpuarray		dnn_gradweight3d	img topgrad kerns_shp border_mode	3d version of dnn_gradweight
gpuarray		dnn_gradinput	kerns topgrad img_shp border_mode	todo document this
gpuarray		dnn_gradinput3d	kerns topgrad img_shp border_mode	3d version of dnn_gradinput
gpuarray		dnn_pool	img ws stride mode	gpu pooling using cudnn from nvidia
gpuarray		dnn_batch_normalization_train	inputs gamma beta mode	performs batch normalization of the given inputs using the mean and variance of the inputs
gpuarray		dnn_batch_normalization_test	inputs gamma beta mean	performs batch normalization of the given inputs using the given mean and variance
gpuarray	NoCuDNNRaise	apply	fgraph	raise a error if cudnn can't be used
gpuarray		as_gpuarray_variable	x context_name	this will attempt to convert x into a variable on the gpu
gpuarray		infer_context_name		infer the context name to use from the inputs given
gpuarray	GpuKernelBase	gpu_kernels	node name	this is the method to override this should return an iterable
gpuarray	GpuKernelBase	kernel_version	node	if you override :meth c_code_cache_version_apply, call this method to have the version of the kernel support code and
gpuarray	BaseGpuCorrMM	flops	inp outp	useful with the hack in profilemode to print the mflops
gpuarray	BaseGpuCorrMM	c_code_helper	bottom weights top direction	this generates the c code for gpucorrmm (direction="forward"), gpucorrmm_gradweights (direction="backprop weights"), and
gpuarray	BaseGpuCorr3dMM	flops	inp outp	useful with the hack in profilemode to print the mflops
gpuarray	BaseGpuCorr3dMM	c_code_helper	bottom weights top direction	this generates the c code for gpucorr3dmm (direction="forward"), gpucorr3dmm_gradweights (direction="backprop weights"), and
gpuarray	GpuIncSubtensor	do_type_checking	node	should raise notimplementederror if c_code does not support the types involved in this node
gpuarray	GpuIncSubtensor	get_helper_c_code_args		return a dictionary of arguments to use with helper_c_code
gpuarray	GpuAdvancedIncSubtensor1_dev20	make_node	x y ilist	it differs from gpuadvancedincsubtensor1 in that it makes sure the indexes are of type long
gpuarray		curfft	inp norm	performs the fast fourier transform of a real-valued input on the gpu
gpuarray		cuirfft	inp norm is_odd	performs the inverse fast fourier transform with real-valued output on the gpu
gpuarray		work_dtype	dtype	return the data type for working memory
gpuarray		load_w	dtype	return the function name to load data
gpuarray		write_w	dtype	return the function name to write data
gpuarray		nvcc_kernel	name params body	return the c code of a kernel function
gpuarray		code_version	version	decorator to support version-based cache mechanism
gpuarray		inline_reduce	N buf pos count	return c++ code for a function that reduces a contiguous buffer
gpuarray		inline_softmax	N buf buf2 threadPos	generate code for a softmax
gpuarray		inline_reduce_fixed_shared	N buf x stride_x	return c++ code for a function that reduces a contiguous buffer
gpuarray		inline_softmax_fixed_shared	N buf x stride_x	generate code to perform softmax with a fixed amount of shared memory
gpuarray		gpu_supported	data	is the following data supported on the gpu? currently only complex aren't supported
gpuarray		move_to_gpu	data	do we want to move this computation to the gpu? currently we don't move complex and scalar
gpuarray		reg_context	name ctx	register a context by mapping it to a name
gpuarray		get_context	name	retrive the context associated with a name
gpuarray		list_contexts		return an iterable of all the registered context names
gpuarray	GpuArrayType	context		the context object mapped to the type's :attr context_name
gpuarray	GpuArrayType	dtype_specs		return a tuple python type c type numpy typenum that corresponds to self
gpuarray		gpuarray_shared_constructor	value name strict allow_downcast	sharedvariable constructor for gpuarraytype
gpuarray		grab_cpu_scalar	v nd	get a scalar variable value from the tree at v
gpuarray		find_node	v cls ignore_clients	find the node that has an op of of type cls in v
gpuarray		is_equal	var val	returns true if var is always equal to val
gpuarray		alpha_merge	cls alpha_in beta_in	decorator to merge multiplication by a scalar on the output
gpuarray		output_merge	cls alpha_in beta_in out_in	decorator to merge addition by a value on the output
gpuarray		inplace_allocempty	op idx	wrapper to make an inplace optimization that deals with allocempty this will duplicate the alloc input if it has more than one client
gpuarray		pad_dims	input leftdims rightdims	reshapes the input to a leftdims + rightdims tensor this helper function is used to convert pooling inputs with arbitrary
gpuarray		unpad_dims	output input leftdims rightdims	reshapes the output after pad_dims
gpuarray		use	device force default_to_move_computation_to_gpu move_shared_to_gpu	error and warning about cuda should be displayed only when this function is called
gpuarray		register_opt2	tracks	decorator for the new graphtogpu optimizer
gpuarray		op_lifter	OP cuda_only	op( host_from_gpu(), -> host_from_gpu(gpuop
gpuarray		local_gpua_alloc2	node	join(axis {alloc or hostfromgpu}, -> join axis gpualloc alloc
gpuarray		local_gpu_contiguous_gpu_contiguous	node	gpu_contiguous(gpu_contiguous x -> gpu_contiguous x
gpuarray		split_huge_add_or_mul	node	for add and mul it can happen that we have too much input that will make nvcc fail compilation of our current code
gpuarray		local_conv_gpu_conv	node	gpu_from_host abstractconv -> abstractconv(gpu_from_host)
gpuarray		local_gpu_elemwise_careduce	node	merge some gpucareducecuda and gpuelemwise
gpuarray		gpu_safe_new	x tag	internal function that constructs a new variable from x with the same type but with a different name old name + tag
gpuarray		gpu_reconstruct_graph	inputs outputs tag	different interface to clone that allows you to pass inputs
gpuarray		max_inputs_to_GpuElemwise	node_or_outputs	compute the maximum number of inputs that fit in a kernel call
gpuarray	GpuCAReduceCuda	supports_c_code	inputs	returns true if the current op and reduce pattern has functioning c code
gpuarray	GpuCAReduceCuda	_makecall	node name x z	return a string for making a kernel call
gpuarray	GpuCAReduceCuda	_k_decl	node nodename pattern ndim	return a string to declare a kernel function
gpuarray	GpuCAReduceCuda	_assign_init	first_item	this return the initial value for myresult
scan_module		safe_new	x tag dtype	internal function that constructs a new variable from x with the same type but with a different name old name + tag
scan_module		traverse	out x x_copy d	function used by scan to parse the tree and figure out which nodes it needs to replace
scan_module		clone	output replace strict share_inputs	function that allows replacing subgraphs of a computational graph
scan_module		map_variables	replacer graphs additional_inputs	construct new graphs based on 'graphs' with some variables replaced according to 'replacer'
scan_module		get_updates_and_outputs	ls	this function tries to recognize the updates ordereddict the list of outputs and the stopping condition returned by the
scan_module		expand_empty	tensor_var size	transforms the shape of a tensor from d1 d2 to d1+size d2
scan_module		equal_computations	xs ys in_xs in_ys	checks if theano graphs represent the same computations
scan_module		infer_shape	outs inputs input_shapes	compute the shape of the outputs given the shape of the inputs of a theano graph
scan_module	Validator	check	out	go backwards in the graph from out and check if out is valid
scan_module		scan_can_remove_outs	op out_idxs	looks at all outputs defined by indices out_idxs and see whom can be removed from the scan op without affecting the rest
scan_module		compress_outs	op not_required inputs	helpful function that gets a scan op a list of indices indicating which outputs are not required anymore and should be removed and
scan_module		reconstruct_graph	inputs outputs tag	different interface to clone that allows you to pass inputs
scan_module		forced_replace	out x y	check all internal values of the graph that compute the variable out for occurrences of values identical with x
scan_module		map	fn sequences non_sequences truncate_gradient	similar behaviour as python's map
scan_module		reduce	fn sequences outputs_info non_sequences	similar behaviour as python's reduce
scan_module		foldl	fn sequences outputs_info non_sequences	similar behaviour as haskell's foldl
scan_module		foldr	fn sequences outputs_info non_sequences	similar behaviour as haskell' foldr
scan_module		scan_checkpoints	fn sequences outputs_info non_sequences	scan function that uses less memory but is more restrictive
scan_module		remove_constants_and_unused_inputs_scan	node	move constants into the inner graph and remove unused inputs
scan_module	PushOutNonSeqScan	process_node	fgraph node	important note this function uses set and dictionary data structures
scan_module	PushOutSeqScan	process_node	fgraph node	important note this function uses set and dictionary data structure
scan_module	PushOutScanOutput	inner_sitsot_only_last_step_used	var scan_args	given a inner nit_sot output of scan return true iff the outer nit_sot output has only one client and that client is a subtensor
scan_module	ScanInplaceOptimizer	attempt_scan_inplace	fgraph node output_indices alloc_ops	attempts to replace a scan node by one which computes the specified outputs inplace
scan_module	ScanMerge	belongs_to_set	node set_nodes	this function checks if node node belongs to set_nodes, in the sense that it can be merged together with every other node in
scan_module		has_duplicates	l	returns true if l has any duplicates (according to __eq__)
scan_module		make_equiv	lo li	builds a dictionary of equivalences between inner inputs based on the equivalence of their corresponding outer inputs
scan_module	Scan	validate_inner_graph		perform some elementary validations on the inner graph to ensure that it is coherent
scan_module	Scan	make_node		conventions inner_x - the variable corresponding to x in the inner function
scan_module	Scan	execute	node args outs	the args are packed like this n_steps
scan_module	Scan	get_oinp_iinp_iout_oout_mappings		compute and return dictionary mappings between the inputs and outputs of the inner function and the inputs and outputs of the scan
scan_module		scan	fn sequences outputs_info non_sequences	this function constructs and applies a scan op to the provided arguments
gof	Unification	merge	new_best	links all the specified vars to a variable that represents their unification
gof	Unification	__getitem__	v	for a variable v returns a variable that represents the tightest set of possible values it can take
gof		unify_walk	a b U	unify_walk a b u returns an unification where a and b are unified given the unification that already exists in the unification u
gof		unify_walk	fv o U	freev is unified to boundvariable(other_object)
gof		unify_walk	bv o U	the unification succeed iff bv value == other_object
gof		unify_walk	ov o U	the unification succeeds iff other_object in orv options
gof		unify_walk	nv o U	the unification succeeds iff other_object not in nv not_options
gof		unify_walk	fv v U	both variables are unified
gof		unify_walk	bv v U	v is unified to bv value
gof		unify_walk	a b U	orv list1 == orv list2 == orv(intersection list1 list2
gof		unify_walk	a b U	nv list1 == nv list2 == nv(union list1 list2
gof		unify_walk	o n U	orv list1 == nv list2 == orv(list1 \ list2)
gof		unify_walk	vil l U	unifies vil's inner variable to orv list
gof		unify_walk	l1 l2 U	tries to unify each corresponding pair of elements from l1 and l2
gof		unify_walk	d1 d2 U	tries to unify values of corresponding keys
gof		unify_walk	a b U	checks for the existence of the __unify_walk__ method for one of the objects
gof		unify_walk	v o U	this simply checks if the var has an unification in u and uses it instead of the var
gof		unify_merge	v o U	this simply checks if the var has an unification in u and uses it instead of the var
gof	CLinkerObject	c_headers		optional return a list of header files required by code returned by this class
gof	CLinkerObject	c_header_dirs		optional return a list of header search paths required by code returned by this class
gof	CLinkerObject	c_libraries		optional return a list of libraries required by code returned by this class
gof	CLinkerObject	c_lib_dirs		optional return a list of library search paths required by code returned by this class
gof	CLinkerObject	c_support_code		optional return utility code for use by a variable or op to be included at global scope prior to the rest of the code for this class
gof	CLinkerObject	c_code_cache_version		return a tuple of integers indicating the version of this op
gof	CLinkerObject	c_compile_args		optional return a list of compile args recommended to compile the code returned by other methods in this class
gof	CLinkerObject	c_no_compile_args		optional return a list of incompatible gcc compiler arguments
gof	CLinkerObject	c_init_code		optional return a list of code snippets to be inserted in module initialization
gof	CLinkerOp	c_code	node name inputs outputs	required return the c implementation of an op
gof	CLinkerOp	c_code_cache_version_apply	node	return a tuple of integers indicating the version of this op
gof	CLinkerOp	c_code_cleanup	node name inputs outputs	optional return c code to run after c_code whether it failed or not
gof	CLinkerOp	c_support_code_apply	node name	optional return utility code for use by an op that will be inserted at global scope that can be specialized for the
gof	CLinkerOp	c_init_code_apply	node name	optional return a code string specific to the apply to be inserted in the module initialization code
gof	CLinkerOp	c_init_code_struct	node name sub	optional return a code string specific to the apply to be inserted in the struct initialization code
gof	CLinkerOp	c_support_code_struct	node name	optional return utility code for use by an op that will be inserted at struct scope that can be specialized for the
gof	CLinkerOp	c_cleanup_code_struct	node name	optional return a code string specific to the apply to be inserted in the struct cleanup code
gof	PureOp	make_node		required return an apply instance representing the application of this op to the provided inputs
gof	PureOp	_get_test_value	cls v	extract test value from variable v
gof	PureOp	__call__		optional return some or all output[s] of make_node
gof	PureOp	R_op	inputs eval_points	this method is primarily used by tensor rop
gof	PureOp	perform	node inputs output_storage params	required calculate the function on the inputs and put the variables in the output storage
gof	PureOp	do_constant_folding	node	this allows each op to determine if it wants to be constant folded when all its inputs are constant
gof	Op	prepare_node	node storage_map compute_map impl	make any special modifications that the op needs before doing make_thunk()
gof	Op	make_c_thunk	node storage_map compute_map no_recycling	like make_thunk but will only try to make a c thunk
gof	Op	make_py_thunk	node storage_map compute_map no_recycling	like make_thunk() but only makes python thunks
gof	Op	make_thunk	node storage_map compute_map no_recycling	this function must return a thunk that is a zero-arguments function that encapsulates the computation to be performed
gof	Op	make_node		create a "apply" nodes for the inputs in that order
gof		get_test_value	v	extract test value from v raises attributeerror if there is none
gof		missing_test_message	msg	displays msg a message saying that some test_value is missing in the appropriate form based on config
gof		debug_error_message	msg	displays a message saying that an error was found in some test_values
gof		debug_assert	condition msg	customized assert with options to ignore the assert
gof		get_debug_values		intended use for val_1
gof	OpenMPOp	c_compile_args		return the compilation arg "fopenmp" if openmp is supported
gof	OpenMPOp	c_headers		return the header file name "omp h" if openmp is supported
gof	OpenMPOp	test_gxx_support		check if openmp is supported
gof	OpenMPOp	update_self_openmp		make sure self openmp is not true if there is no support in gxx
gof	COp	get_path	cls f	convert a path relative to the location of the class file into an aboslute path
gof	COp	__init__	func_files func_name	sections are loaded from files in order with sections in later files overriding sections in previous files
gof	COp	load_c_code	func_files	loads the c code to perform the op
gof	COp	get_op_params		returns a list of name value pairs that will be turned into macros for use within the op code
gof	COp	c_init_code		get the code section for init_code
gof	COp	c_init_code_struct	node name sub	stitches all the macros and "init_code" together
gof	COp	c_code_cleanup	node name inputs outputs	stitches all the macros and "code_cleanup" together
gof		failure_code	sub use_goto	code contained in sub['fail'], usually substituted for % fail s
gof		failure_code_init	sub	code for failure in the struct init
gof		code_gen	blocks	from a list of l{codeblock} instances returns a string that executes them all in sequence
gof		struct_gen	args struct_builders blocks sub	generates a struct conforming to the following specifications parameters
gof		get_c_declare	r name sub	wrapper around c_declare that declares py_name
gof		get_c_init	r name sub	wrapper around c_init that initializes py_name to py_none
gof		get_c_extract	r name sub	wrapper around c_extract that initializes py_name from storage
gof		get_c_extract_out	r name sub	wrapper around c_extract_out that initializes py_name from storage
gof		get_c_cleanup	r name sub	wrapper around c_cleanup that decrefs py_name
gof		get_c_sync	r name sub	wrapper around c_sync that syncs py_name with storage
gof		apply_policy	policy r name sub	apply the list of policies to name r sub
gof		struct_variable_codeblocks	variable policies id symbol_table	update "sub" dict and create two codeblocks with different failure modes parameters
gof	CLinker	accept	fgraph no_recycling profile	associate linker with fgraph
gof	CLinker	fetch_variables		fills the inputs outputs variables orphans temps and node_order fields
gof	CLinker	code_gen		generates code for a struct that does the computation of the fgraph and stores it in the struct_code field of the instance
gof	CLinker	support_code		returns a list of support code strings that are needed by one or more variables or ops
gof	CLinker	compile_args		returns a list of compile args that are needed by one or more variables or ops
gof	CLinker	headers		returns a list of headers that are needed by one or more types or ops
gof	CLinker	init_code		return a list of code snippets that have to be inserted in the module initialization code
gof	CLinker	header_dirs		returns a list of lib directories that are needed by one or more types or ops
gof	CLinker	libraries		returns a list of libraries that are needed by one or more types or ops
gof	CLinker	lib_dirs		returns a list of lib directories that are needed by one or more types or ops
gof	CLinker	__compile__	input_storage output_storage storage_map keep_lock	compiles this linker's fgraph
gof	CLinker	make_thunk	input_storage output_storage storage_map keep_lock	compiles this linker's fgraph and returns a function to perform the computations as well as lists of storage cells for both the inputs
gof	CLinker	cmodule_key		return a complete hashable signature of the module we compiled
gof	CLinker	cmodule_key_	fgraph no_recycling compile_args libraries	do the actual computation of cmodule_key in a static method to allow it to be reused in scalar
gof	CLinker	compile_cmodule	location	this compiles the source code for this linker and returns a loaded module
gof	CLinker	get_dynamic_module		return a cmodule dynamicmodule instance full of the code for our fgraph
gof	CLinker	cthunk_factory	error_storage in_storage out_storage storage_map	returns a thunk that points to an instance of a c struct that can carry on the computation of this linker's fgraph
gof	_CThunk	find_task	failure_code	maps a failure code to the task that is associated to it
gof	OpWiseCLinker	accept	fgraph no_recycling profile	associate linker with fgraph
gof		_default_checker	x y	default checker for duallinker this checks that the
gof	DualLinker	__init__	checker schedule	initialize a duallinker
gof	DualLinker	accept	fgraph no_recycling profile	update/tie self with fgraph
gof	DualLinker	make_thunk		compiles this linker's fgraph and returns a function to perform the
gof		force_unlock		delete the compilation lock if someone else has it
gof		_get_lock	lock_dir	obtain lock on compilation directory
gof		release_lock		release lock on compilation directory
gof		set_lock_status	use_lock	enable or disable the lock on the compilation directory which is enabled by default
gof		lock	tmp_dir timeout min_wait max_wait	obtain lock access by creating a given temporary directory whose base will be created if needed but will not be deleted after the lock is removed
gof		refresh_lock	lock_file	'refresh' an existing lock by re-writing the file containing the owner's unique id using a new randomly generated id which is also returned
gof	Unlocker	unlock	force	remove current lock
gof	ParamsType	has_type	theano_type	return true if current paramstype contains the specified theano type
gof	ParamsType	get_field	theano_type	return the name string of the first field associated to the given theano type
gof	ParamsType	get_enum	key	look for a constant named key in the theano enumeration types wrapped into current paramstype
gof	ParamsType	enum_from_alias	alias	look for a constant that has alias alias in the theano enumeration types wrapped into current paramstype
gof	ParamsType	get_params		convenient method to extract fields values from a list of python objects and key-value args and wrap them into a :class params object compatible with current paramstype
gof	CLinkerType	c_element_type		optional return the name of the primitive c type of items into variables handled by this type
gof	CLinkerType	c_is_simple		optional return true for small or builtin c types
gof	CLinkerType	c_literal	data	optional writeme parameters
gof	CLinkerType	c_declare	name sub check_input	required return c code to declare variables that will be instantiated by c_extract
gof	CLinkerType	c_init	name sub	required return c code to initialize the variables that were declared by self
gof	CLinkerType	c_extract	name sub check_input	required return c code to extract a pyobject * instance
gof	CLinkerType	c_extract_out	name sub check_input	optional c code to extract a pyobject * instance
gof	CLinkerType	c_cleanup	name sub	return c code to clean up after c_extract
gof	CLinkerType	c_sync	name sub	required return c code to pack c types back into a pyobject
gof	CLinkerType	c_code_cache_version		return a tuple of integers indicating the version of this type
gof	PureType	filter	data strict allow_downcast	required return data or an appropriately wrapped/converted data
gof	PureType	filter_variable	other allow_convert	convert a symbolic variable into this type if compatible
gof	PureType	convert_variable	var	patch variable so that its type will match self if possible
gof	PureType	is_valid_value	a	required return true for any python object a that would be a legal value for a variable of this type
gof	PureType	value_validity_msg	a	optional return a message explaining the output of is_valid_value
gof	PureType	make_variable	name	return a new variable instance of type self
gof	PureType	__call__	name	return a new variable instance of type self
gof	PureType	values_eq	a b	return true if a and b can be considered exactly equal
gof	PureType	values_eq_approx	a b	return true if a and b can be considered approximately equal
gof	CDataType	_get_func		return a function that makes a value from an integer
gof	CDataType	make_value	ptr	make a value of this type
gof	EnumType	fromalias	alias	get a constant value by its alias
gof	EnumType	has_alias	alias	return true if and only if this enum has this alias
gof		cleanup		delete keys in old format from the compiledir
gof		print_compiledir_content		print list of %d compiled individual ops in the "theano config compiledir"
gof		basecompiledir_ls		print list of files in the "theano config base_compiledir"
gof	CallCache	persist	filename	cache "filename" as a pickle file
gof	CallCache	call	fn args key	retrieve item from the cache if available
gof	Node	get_parents		return a list of the parents of this node
gof	Apply	run_params		returns the params for the node or noparams if no params is set
gof	Apply	default_output		returns the default output for this node
gof	Apply	clone		duplicate this apply instance with inputs = self inputs
gof	Apply	clone_with_new_inputs	inputs strict	duplicate this apply instance in a new graph
gof	Variable	__str__		return a str representation of the variable
gof	Variable	__repr_test_value__		return a repr of the test value
gof	Variable	__repr__	firstPass	return a repr of the variable
gof	Variable	clone		return a new variable like self
gof	Variable	eval	inputs_to_values	evaluates this variable
gof	Constant	clone		we clone this object but we don't clone the data to lower memory requirement
gof		stack_search	start expand mode build_inv	search through a graph either breadth- or depth-first
gof		ancestors	variable_list blockers	return the variables that contribute to those in variable_list inclusive
gof		inputs	variable_list blockers	return the inputs required to compute the given variables
gof		variables_and_orphans	i o	extract list of variables between i and o nodes via dfs traversal and chooses the orphans among them
gof		ops	i o	set of ops contained within the subgraph between i and o parameters
gof		variables	i o	extracts list of variables within input and output nodes via dfs travesal parameters
gof		orphans	i o	extracts list of variables within input and output nodes via dfs travesal and returns the orphans among them
gof		clone	i o copy_inputs	copies the subgraph contained between i and o
gof		clone_get_equiv	inputs outputs copy_inputs_and_orphans memo	return a dictionary that maps from variable and apply nodes in the original graph to a new node a clone in a new graph
gof		io_toposort	inputs outputs orderings clients	perform topological sort from input and output nodes parameters
gof		io_connection_pattern	inputs outputs	returns the connection pattern of a subgraph defined by given inputs and outputs
gof		is_same_graph	var1 var2 givens debug	return true iff variables var1 and var2 perform the same computation
gof		op_as_string	i op leaf_formatter node_formatter	op to return a string representation of the subgraph
gof		as_string	i o leaf_formatter node_formatter	returns a string representation of the subgraph between i and o parameters
gof		view_roots	r	utility function that returns the leaves of a search through consecutive view_map()s
gof		list_of_nodes	inputs outputs	return the apply nodes of the graph between inputs and outputs
gof		is_in_ancestors	l_node f_node	goes up in the graph and returns true if the apply node f_node is found
gof		_contains_cycle	fgraph orderings	function to check if the given graph contains a cycle parameters
gof		fast_inplace_check	inputs	return the variables in inputs that are posible candidate for as inputs of inplace operation
gof	DestroyHandler	on_attach	fgraph	when attaching to a new fgraph check that 1) this destroyhandler wasn't already attached to some fgraph
gof	DestroyHandler	refresh_droot_impact		makes sure self droot self impact and self root_destroyer are up to
gof	DestroyHandler	on_import	fgraph app reason	add apply instance to set which must be computed
gof	DestroyHandler	on_prune	fgraph app reason	remove apply instance from set which must be computed
gof	DestroyHandler	on_change_input	fgraph app i old_r	app inputs[i] changed from old_r to new_r
gof	DestroyHandler	orderings	fgraph	return orderings induced by destructive operations
gof	BadOptimization	str_diagnostic		return a pretty multiline string representating the cause of the exception
gof	Feature	on_attach	function_graph	called by functiongraph attach_feature the method that attaches
gof	Feature	on_detach	function_graph	called by remove_feature feature should remove any dynamically-added
gof	Feature	on_import	function_graph node reason	called whenever a node is imported into function_graph which is just before the node is actually connected to the graph
gof	Feature	on_prune	function_graph node reason	called whenever a node is pruned removed from the function_graph after it is disconnected from the graph
gof	Feature	on_change_input	function_graph node i r	called whenever node inputs[i] is changed from r to new_r
gof	Feature	orderings	function_graph	called by toposort it should return a dictionary of
gof	Bookkeeper	on_attach	fgraph	called by functiongraph attach_feature the method that attaches
gof	Bookkeeper	on_detach	fgraph	should remove any dynamically added functionality
gof	History	on_detach	fgraph	should remove any dynamically added functionality
gof	History	revert	fgraph checkpoint	reverts the graph to whatever it was at the provided checkpoint undoes all replacements
gof	Validator	on_detach	fgraph	should remove any dynamically added functionality
gof	Validator	validate_	fgraph	if the caller is replace_all_validate just raise the exception
gof	ReplaceValidate	on_detach	fgraph	should remove any dynamically added functionality
gof	ReplaceValidate	replace_all_validate_remove	fgraph replacements remove reason	as replace_all_validate revert the replacement if the ops in the list remove are still in the graph
gof	NodeFinder	on_detach	fgraph	should remove any dynamically added functionality
gof	PrintListener	on_detach	fgraph	should remove any dynamically added functionality
gof		debug_counter	name every	debug counter to know how often we go through some piece of code
gof	ExtFunction	method_decl		returns the signature for this function
gof	DynamicModule	list_code	ofile	print out the code with line numbers to ofile
gof		dlimport	fullpath suffix	dynamically load a so pyd dll or py file
gof		dlimport_workdir	basedir	return a directory where you should put your so file for dlimport
gof		last_access_time	path	return the number of seconds since the epoch of the last access of a given file
gof		module_name_from_dir	dirname err files	scan the contents of a cache directory and return full path of the dynamic lib in it
gof		is_same_entry	entry_1 entry_2	return true iff both paths can be considered to point to the same module
gof		get_module_hash	src_code key	return an md5 hash that uniquely identifies a module
gof		get_safe_part	key	return a tuple containing a subset of key, to be used to find equal keys
gof	KeyData	add_key	key save_pkl	add a key to self keys and update pickled file if asked to
gof	KeyData	remove_key	key save_pkl	remove a key from self keys and update pickled file if asked to
gof	KeyData	save_pkl		dump this object into its key_pkl file
gof	KeyData	get_entry		return path to the module file
gof	KeyData	delete_keys_from	entry_from_key do_manual_check	delete from entry_from_key all keys associated to this keydata object
gof	ModuleCache	_get_module	name	fetch a compiled module from the loaded cache or the disk
gof	ModuleCache	refresh	age_thresh_use delete_if_problem cleanup	update cache data by walking the cache directory structure
gof	ModuleCache	_get_from_key	key key_data	returns a module if the passed-in key is found in the cache and none otherwise
gof	ModuleCache	_add_to_cache	module key module_hash	this function expects the compile lock to be held
gof	ModuleCache	module_from_key	key lnk keep_lock	return a module from the cache compiling it if necessary
gof	ModuleCache	check_key	key key_pkl	perform checks to detect broken __eq__ / __hash__ implementations
gof	ModuleCache	clear_old	age_thresh_del delete_if_problem	delete entries from the filesystem for cache entries that are too old
gof	ModuleCache	clear	unversioned_min_age clear_base_files delete_if_problem	clear all elements in the cache
gof	ModuleCache	clear_base_files		remove base directories 'cutils_ext', 'lazylinker_ext' and 'scan_perform' if present
gof	ModuleCache	clear_unversioned	min_age	delete unversioned dynamic modules
gof		_rmtree	parent ignore_nocleanup msg level	on nfs filesystems it is impossible to delete a directory with open files in it
gof		get_module_cache	dirname init_args	create a new module_cache with the k v pairs in this dictionary parameters
gof		get_lib_extension		return the platform-dependent extension for compiled modules
gof		get_gcc_shared_library_arg		return the platform-dependent gcc argument for shared libraries
gof		gcc_llvm		detect if the g++ version used is the llvm one or not
gof	Compiler	_try_compile_tmp	cls src_code tmp_prefix flags	try to compile and run a test program
gof	Compiler	_try_flags	cls flag_list preambule body	try to compile a dummy file with these flags
gof		try_march_flag	flags	try to compile and run a simple c snippet using current flags
gof		calculate_reallocate_info	order fgraph storage_map compute_map_re	writeme : explain the parameters
gof	VM	__call__		run the machine
gof	VM	clear_storage		free any internal references to temporary variables
gof	VM	update_profile	profile	accumulate into the profile object
gof	Stack	run_thunk_of_node	node	run the thunk corresponding to apply instance node
gof	VM_Linker	accept	fgraph no_recycling profile	check if fgraph is the first functiongraph that has ever been associated to self else create a new vm_linker
gof	VM_Linker	compute_gc_dependencies	variables	returns dict variable k -> list of variables [v1 v2 v3 ]
gof		log_thunk_trace	value f	log theano's diagnostic stack trace for an exception raised by raise_with_op
gof		thunk_hook	type value trace	this function is meant to replace excepthook and do some special work if the exception value has a __thunk_trace__
gof		raise_with_op	node thunk exc_info storage_map	re-raise an exception while annotating the exception object with debug info
gof	Linker	make_thunk		this function must return a triplet (function input_variables output_variables) where function is a thunk that operates on the
gof	Linker	make_function	unpack_single	returns a function that takes values corresponding to the inputs of the fgraph used by this l{linker} and returns values corresponding the the
gof		map_storage	fgraph order input_storage output_storage	ensure there is storage a length-1 list for inputs outputs and interior nodes
gof		gc_helper	node_list	return the set of variable instances which are computed by node_list
gof	PerformLinker	make_all	input_storage output_storage storage_map	returns function to run all nodes list of input containers list of outputs parameters
gof	WrapLinker	__copy__		shallow copy of a wraplinker
gof		WrapLinkerMany	linkers wrappers	variant on wraplinker that runs a series of wrapper functions instead of just one
gof	Optimizer	apply	fgraph	applies the optimization to the provided l{functiongraph} it may
gof	Optimizer	optimize	fgraph	this is meant as a shortcut to opt
gof	Optimizer	__call__	fgraph	same as self optimize fgraph
gof	Optimizer	add_requirements	fgraph	add features to the fgraph that are required to apply the optimization
gof		optimizer	f	decorator for fromfunctionoptimizer
gof		inplace_optimizer	f	decorator for fromfunctionoptimizer
gof	SeqOptimizer	warn	exc optimizer	default failure_callback for seqoptimizer
gof	SeqOptimizer	apply	fgraph	applies each l{optimizer} in self in turn
gof	SeqOptimizer	merge_profile	prof1 prof2	merge 2 profiles returned by this cass apply() fct
gof	MergeFeature	process_constant	fgraph c	check if a constant can be merged and queue that replacement
gof	MergeFeature	process_node	fgraph node	check if a node can be merged and queue that replacement
gof		is_same_graph_with_merge	var1 var2 givens	merge-based implementation of theano gof graph is_same_graph
gof		pre_constant_merge	vars	merge constants in the subgraph used to compute nodes in vars
gof	LocalOptimizer	tracks		return the list of op classes that this opt applies to
gof	LocalOptimizer	transform	node	transform a subgraph whose output is node
gof	LocalOptimizer	add_requirements	fgraph	if this local optimization wants to add some requirements to the fgraph this is the place to do it
gof	LocalMetaOptimizer	provide_inputs	node inputs	if implemented returns a dictionary mapping all symbolic variables in inputs to sharedvariable instances of suitable dummy values
gof	PatternSub	transform	node get_nodes	checks if the graph from node corresponds to in_pattern if it does
gof	NavigatorOptimizer	warn	exc nav repl_pairs local_opt	failure_callback for navigatoroptimizer print traceback
gof	NavigatorOptimizer	warn_inplace	exc nav repl_pairs local_opt	failure_callback for navigatoroptimizer
gof	NavigatorOptimizer	warn_ignore	exc nav repl_pairs local_opt	failure_callback for navigatoroptimizer ignore all errors
gof	NavigatorOptimizer	attach_updater	fgraph importer pruner chin	install some functiongraph listeners to help the navigator deal with the ignore_trees-related functionality
gof	NavigatorOptimizer	detach_updater	fgraph u	undo the work of attach_updater
gof	NavigatorOptimizer	process_node	fgraph node lopt	this function will use lopt to transform the node the
gof		out2in		uses the topooptimizer from the output nodes to input nodes of the graph
gof		in2out		uses the topooptimizer from the input nodes to output nodes of the graph
gof	OpKeyOptimizer	add_requirements	fgraph	requires the following features
gof		merge_dict	d1 d2	merge 2 dicts by adding the values
gof		pre_greedy_local_optimizer	list_optimizations out	this function traverses the computation graph described by all node in the graph before the variable out but that are not in the
gof		copy_stack_trace	from_var to_var	copies the stack trace from one or more tensor variables to one or more tensor variables
gof		check_stack_trace	f_or_fgraph ops_to_check bug_print	this function checks if the outputs of specific ops of a compiled graph have a stack
gof		simple_extract_stack	f limit skips	this is traceback extract_stack from python 2 7 with this change
gof		add_tag_trace	thing user_line	add tag trace to an node or variable
gof		memoize	f	cache the return value for each tuple of arguments which must be hashable
gof		deprecated	filename msg	decorator which will print a warning message on the first call
gof		uniq	seq	do not use set this must always return the same value at the same index
gof		difference	seq1 seq2	returns all elements in seq1 which are not in seq2 i e seq1\seq2
gof		toposort	prereqs_d	sorts prereqs_d keys() topologically
gof		flatten	a	recursively flatten tuple list and set in a list
gof		give_variables_names	variables	gives unique names to an iterable of variables modifies input
gof		remove	predicate coll	return those items of collection for which predicate item is true
gof		hash_from_file	file_path	return the md5 hash of a file
gof	FunctionGraph	__init__	inputs outputs features clone	create an functiongraph which operates on the subgraph bound by the inputs and outputs sets
gof	FunctionGraph	disown		cleans up all of this functiongraph's nodes and variables so they are not associated with this functiongraph anymore
gof	FunctionGraph	clients	r	set of all the node i pairs such that node inputs[i] is r
gof	FunctionGraph	__add_client__	r new_client	updates the list of clients of r with new_clients
gof	FunctionGraph	__remove_client__	r client_to_remove reason	removes all from the clients list of r
gof	FunctionGraph	__import_r__	variable reason	import variables to this functiongraph and also their apply_node if those nodes are not in this graph
gof	FunctionGraph	__import__	apply_node check reason	given an apply_node recursively search from this node to know graph and then add all unknown variables and apply_nodes to this graph
gof	FunctionGraph	change_input	node i new_r reason	changes node inputs[i] to new_r
gof	FunctionGraph	replace	r new_r reason verbose	this is the main interface to manipulate the subgraph in functiongraph
gof	FunctionGraph	replace_all	pairs reason	for every node that uses r as input makes it use new_r instead
gof	FunctionGraph	attach_feature	feature	adds a gof toolbox feature to this function_graph and triggers its
gof	FunctionGraph	remove_feature	feature	removes the feature from the graph
gof	FunctionGraph	execute_callbacks	name	execute callbacks calls getattr feature name (*args) for each feature which has
gof	FunctionGraph	collect_callbacks	name	collects callbacks returns a dictionary d such that
gof	FunctionGraph	toposort		toposort return an ordering of the graph's apply nodes such that
gof	FunctionGraph	orderings		return dict d s t d[node] is a list of nodes that must be evaluated
gof	FunctionGraph	check_integrity		call this for a diagnosis if things go awry
gof	FunctionGraph	clone	check_integrity	clone the graph and get a memo a dict that map old node to new node
gof	FunctionGraph	clone_get_equiv	check_integrity attach_feature	clone the graph and get a dict that maps old nodes to new ones parameters
gof	FunctionGraph	__getstate__		this is needed as some features introduce instance methods
gof		memodict	f	memoization decorator for a function taking a single argument
gof		make_dependence_cmp		create a comparator to represent the dependence of nodes in a graph
gof		reverse_dict	d	reverses direction of dependence dict
gof		_toposort	edges	topological sort algorithm by kahn [1] - o nodes + vertices
gof		posort	l	partially ordered sort with multiple comparators
gof		sort_apply_nodes	inputs outputs cmps	order a graph of apply nodes according to a list of comparators
gof		sort_schedule_fn		make a schedule function from comparators
gof		key_to_cmp	key	comparator function based on "key" function
gof		compile_cutils		do just the compilation of cutils_ext
compile		register_view_op_c_code	type code version	tell viewop how to generate c code for a theano type
compile		register_deep_copy_op_c_code	typ code version	tell deepcopyop how to generate c code for a theano type
compile		register_shape_c_code	type code version	tell shape op how to generate c code for a theano type
compile		shape_i	var i fgraph	equivalent of var shape[i], but apply if possible the shape feature
compile		register_shape_i_c_code	typ code check_input version	tell shape_i how to generate c code for a theano type
compile		as_op	itypes otypes infer_shape	decorator that converts a function into a basic theano op that will call the supplied function as its implementation
compile		register_rebroadcast_c_code	typ code version	tell rebroadcast how to generate c code for a theano type
compile		register_specify_shape_c_code	typ code version c_support_code_apply	tell specifyshape how to generate c code for a theano type
compile	SharedVariable	get_value	borrow return_internal_type	get the non-symbolic value associated with this sharedvariable
compile	SharedVariable	set_value	new_value borrow	set the non-symbolic value associated with this sharedvariable
compile	SharedVariable	zero	borrow	set the values of a shared variable to 0
compile		shared	value name strict allow_downcast	return a sharedvariable variable initialized with a copy or reference of value
compile		_atexit_print_fn		print profilestat objects in _atexit_print_list to _atexit_print_file
compile		print_global_stats		print the following stats
compile	ProfileStats	reset		ignore previous function call
compile	ProfileStats	class_time		dict op -> total time on thunks
compile	ProfileStats	class_callcount		dict op -> total number of thunk calls
compile	ProfileStats	class_nodes		dict op -> total number of nodes
compile	ProfileStats	class_impl		dict op -> total number of nodes
compile	ProfileStats	op_time		dict op -> total time on thunks
compile	ProfileStats	fill_node_total_time	node total_times	node -> fill total time icluding its parents returns nothing
compile	ProfileStats	compute_total_times		dict op -> total time icluding the time for parents
compile	ProfileStats	op_callcount		dict op -> total number of thunk calls
compile	ProfileStats	op_nodes		dict op -> total number of nodes
compile	ProfileStats	op_impl		dict op -> 'c' or 'py' depending how the op is implemented
compile		alias_root	v	return the variable to which v is aliased by view_maps and destroy_maps
compile		view_tree_set	v treeset	add to treeset all variables that are views of v given that v is not a view
compile		infer_reuse_pattern	fgraph outputs_to_disown	given an fgraph and a list of variables returns the list or set of all variables which may share the same underlying data storage
compile		fgraph_updated_vars	fgraph expanded_inputs	reconstruct the full "updates" dictionary mapping from functiongraph input variables to the fgraph outputs that will replace their values
compile		std_fgraph	input_specs output_specs accept_inplace	makes an functiongraph corresponding to the input specs and the output specs
compile	Function	__copy__		copy a function copied function have separate intermediate
compile	Function	copy	share_memory swap delete_updates name	copy this function copied function will have separated maker and
compile	Function	__call__		evaluates value of a function on given arguments
compile	Function	free		when allow_gc = false clear the variables in storage_map
compile	Function	get_shared		return the shared variable read or updated by by this function
compile		insert_deepcopy	fgraph wrapped_inputs wrapped_outputs	insert deepcopy in the fgraph to break aliasing of outputs
compile	FunctionMaker	create	input_storage trustme storage_map	create a function
compile		orig_function	inputs outputs mode accept_inplace	return a function that will calculate the outputs from the inputs
compile		convert_function_input	input	upgrade a input shortcut to an in instance
compile		get_info_on_inputs	named_inputs n_unnamed_inputs	return a human-readable description of named and un-named inputs
compile	MonitorMode	eval	i node fn	the method that calls the thunk fn
compile	MonitorMode	clone	link_kwargs optimizer	create a new instance of this mode
compile		rebuild_collect_shared	outputs inputs replace updates	function that allows replacing subgraphs of a computational graph
compile		pfunc	params outputs mode updates	function-constructor for graphs with shared variables
compile		iter_over_pairs	pairs	return an iterator over pairs present in the 'pairs' input
compile		_is_numeric_value	arr var	checks a variable against non-numeric types such as types slices empty arrays and none that need not be checked for nan and inf values
compile		flatten	l	turns a nested graph of lists/tuples/other objects into a list of objects
compile		contains_nan	arr node var	test whether a numpy ndarray contains any np nan values
compile		contains_inf	arr node var	test whether a numpy ndarray contains any np inf values
compile	BadThunkOutput	offending_op		return the op class whose c_code and perform implementations didn't match
compile	BadThunkOutput	str_diagnostic		return a pretty multiline string representing the cause of the exception
compile		char_from_number	number	converts number to string by rendering it in base 26 using capital letters as digits
compile		debugprint	r prefix depth done	print the graph leading to r to given depth
compile		_optcheck_fgraph	input_specs output_specs accept_inplace	create a functiongraph for debugging
compile		_check_inputs	node storage_map r_vals dr_vals	raise baddestroymap if necessary update dr_vals
compile		_check_viewmap	node storage_map	this functions raises a badviewmap exception when it detects the following
compile		_find_bad_optimizations0	order reasons r_vals	use a simple algorithm to find broken optimizations
compile		_find_bad_optimizations2	order reasons r_vals	use a simple algorithm to find broken optimizations
compile		_get_preallocated_maps	node thunk prealloc_modes def_val	preallocate outputs in different memory layouts
compile		_check_preallocated_output	node thunk prealloc_modes def_val	try to apply thunk() on different output storages
compile	_Maker	create	defaults trustme storage_map	create a function
compile	DebugMode	function_maker	i o m	return an instance of _maker which handles much of the debugging work
compile	DebugMode	__init__	optimizer stability_patience check_c_code check_py_code	if any of these arguments except optimizer is not none it overrides the class default
compile		register_linker	name linker	add a linker which can be referred to by name in mode
compile		register_optimizer	name opt	add a optimizer which can be referred to by name in mode
compile	Mode	register		adds new optimization instances to a mode
compile	Mode	clone	link_kwargs optimizer	create a new instance of this mode
compile		register_mode	name mode	add a mode which can be referred to by name in function
compile	OpFromGraph	_recompute_grad_op		converts self _grad_op from user supplied form to type self instance
compile	OpFromGraph	_recompute_rop_op		converts self _rop_op from user supplied form to type self instance
compile	OpFromGraph	get_grad_op		getter method for self _grad_op
compile	OpFromGraph	get_rop_op		getter method for self _rop_op
compile	OpFromGraph	set_grad_overrides	grad_overrides	set gradient overrides see help theano opfromgraph for syntax
compile	OpFromGraph	set_rop_overrides	rop_overrides	set r_op overrides see help theano opfromgraph for syntax
compile	OpFromGraph	connection_pattern	node	return connection pattern of subfgraph defined by inputs and outputs
compile		inline_ofg_expansion	node	this optimization expands internal graph of opfromgraph
compile		function_dump	filename inputs outputs mode	this is helpful to make a reproducible case for problems during theano compilation
compile		function	inputs outputs mode updates	return a :class callable object <theano compile function_module function>
tensor		check_equal_numpy	x y	return true iff x and y are equal
tensor		constructor	f	add f to :doc oplist
tensor		as_tensor_variable	x name ndim	return x, transformed into a tensortype
tensor		constant	x name ndim dtype	return a symbolic constant with value x
tensor		numpy_scalar	data	return a scalar stored in a numpy ndarray
tensor		get_scalar_constant_value	orig_v elemwise only_process_constants max_recur	return the constant scalar 0-d value underlying variable v
tensor		scalar	name dtype	return a symbolic scalar variable
tensor		vector	name dtype	return a symbolic vector variable
tensor		matrix	name dtype	return a symbolic matrix variable
tensor		row	name dtype	return a symbolic row variable (ndim=2 broadcastable=[true false])
tensor		col	name dtype	return a symbolic column variable (ndim=2 broadcastable=[false true])
tensor		tensor3	name dtype	return a symbolic 3-d variable
tensor		tensor4	name dtype	return a symbolic 4-d variable
tensor		tensor5	name dtype	return a symbolic 5-d variable
tensor		_scal_elemwise_with_nfunc	nfunc nin nout	replace a symbol definition with an elementwise version of the corresponding scalar op
tensor		_pack	x	convert x to a list if it is an iterable otherwise wrap it in a list
tensor		cast	x dtype	symbolically cast x to a tensor of type dtype
tensor		makeKeepDims	x y axis	reintroduces in y with length one the axes of x which have been left out in a prior reduction of x
tensor		max_and_argmax	a axis keepdims	returns maximum elements and their indices obtained by iterating over given axis
tensor		max	x axis keepdims	returns maximum elements obtained by iterating over given axis
tensor		argmax	x axis keepdims	returns indices of maximum elements obtained by iterating over given axis
tensor		min	x axis keepdims	returns minimum elements obtained by iterating over given axis
tensor		argmin	x axis keepdims	returns indices of minimum elements obtained by iterating over given axis
tensor		smallest		return the [elementwise] smallest of a variable number of arguments
tensor		largest		return the [elementwise] largest of a variable number of arguments
tensor		lt	a b	a < b
tensor		gt	a b	a > b
tensor		le	a b	a <= b
tensor		ge	a b	a >= b
tensor		eq	a b	a == b
tensor		neq	a b	a != b
tensor		allclose	a b rtol atol	implement numpy's allclose on tensors
tensor		isclose	a b rtol atol	implements numpy's isclose on tensors
tensor		switch	cond ift iff	if cond then ift else iff
tensor		and_	a b	bitwise a & b
tensor		or_	a b	bitwise a | b
tensor		xor	a b	bitwise a ^ b
tensor		abs_	a	|a| tensorvariable overloads the tensorvariable
tensor		expm1	a	e^a - 1
tensor		log	a	base e logarithm of a
tensor		log2	a	base 2 logarithm of a
tensor		log10	a	base 10 logarithm of a
tensor		sgn	a	sign of a
tensor		ceil	a	ceiling of a
tensor		floor	a	floor of a
tensor		trunc	a	trunc of a
tensor		iround	a mode	cast(round a mode ,'int64')
tensor		round	a mode	round_mode a with mode in [half_away_from_zero half_to_even]
tensor		sqr	a	square of a
tensor		sqrt	a	square root of a
tensor		deg2rad	a	convert degree a to radian
tensor		rad2deg	a	convert radian a to degree
tensor		cos	a	cosine of a
tensor		arccos	a	arccosine of a
tensor		sin	a	sine of a
tensor		arcsin	a	arcsine of a
tensor		tan	a	tangent of a
tensor		arctan	a	arctangent of a
tensor		arctan2	a b	arctangent of a / b
tensor		cosh	a	hyperbolic cosine of a
tensor		arccosh	a	hyperbolic arc cosine of a
tensor		sinh	a	hyperbolic sine of a
tensor		arcsinh	a	hyperbolic arc sine of a
tensor		tanh	a	hyperbolic tangent of a
tensor		arctanh	a	hyperbolic arc tangent of a
tensor		erfc	a	complementary error function
tensor		erfcx	a	scaled complementary error function
tensor		erfinv	a	inverse error function
tensor		erfcinv	a	inverse complementary error function
tensor		gammaln	a	log gamma function
tensor		psi	a	derivative of log gamma function
tensor		chi2sf	x k	chi squared survival function
tensor		j0	x	bessel function of the first kind of order 0
tensor		j1	x	bessel function of the first kind of order 1
tensor		jv	v x	bessel function of the first kind of order v real
tensor		i0	x	modified bessel function of the first kind of order 0
tensor		i1	x	modified bessel function of the first kind of order 1
tensor		iv	v x	modified bessel function of the first kind of order v real
tensor		real	z	return real component of complex-valued tensor z
tensor		imag	z	return imaginary component of complex-valued tensor z
tensor		angle	z	return polar-coordinate angle of complex-valued tensor z
tensor		complex	real imag	return complex-valued tensor with real and imag components
tensor		conj	z	return the complex conjugate of z
tensor		complex_from_polar	abs angle	return complex-valued tensor from polar coordinate specification
tensor		second	a b	create a matrix by filling the shape of a with b
tensor		ones_like	model dtype opt	equivalent of numpy ones_like
tensor		zeros_like	model dtype opt	equivalent of numpy zeros_like
tensor		zeros	shape dtype	create a tensor filled with zeros closer to numpy's syntax than alloc
tensor		ones	shape dtype	create a tensor filled with ones closer to numpy's syntax than alloc
tensor		nonzero	a return_matrix	returns one of the following if return_matrix is false default same as numpy :
tensor		flatnonzero	a	return a vector of indices that are non-zero in the flattened version of a
tensor		nonzero_values	a	return a vector of non-zero elements contained in the input array
tensor		tri	N M k dtype	an array with ones at and below the given diagonal and zeros elsewhere
tensor		tril	m k	lower triangle of an array
tensor		triu	m k	upper triangle of an array
tensor		eye	n m k dtype	return a 2-d array with ones on the diagonal and zeros elsewhere
tensor	Alloc	__call__	val	if the alloc would be useless this function returns val
tensor		transfer	var target	return a version of var transferred to target
tensor		register_transfer	fn	register a transfer function for alternative targets
tensor		sum	input axis dtype keepdims	computes the sum along the given axis es of a tensor input
tensor		prod	input axis dtype keepdims	computes the product along the given axis es of a tensor input
tensor		mean	input axis dtype op	computes the mean value along the given axis es of a tensor input
tensor		var	input axis ddof keepdims	computes the variance along the given axis es of a tensor input
tensor		std	input axis ddof keepdims	computes the standard deviation along the given axis es of a tensor input
tensor		maximum	x y	elemwise maximum see max for the maximum in one tensor
tensor		minimum	x y	elemwise minimum see min for the minimum in one tensor
tensor		div_proxy	x y	proxy for either true_div or int_div depending on types of x y
tensor		divmod	x y	elementvise divmod using floor_div and mod_check
tensor		true_div	a b	elementwise [true] division inverse of multiplication
tensor		int_div	a b	elementwise [floor] division inverse of multiplication
tensor		ceil_intdiv	a b	safely compute ceil(float_division a b
tensor		mod_check	x y	make sure we do not try to use complex numbers
tensor		clip	x min max	clip x to be between min and max
tensor		extract_constant	x elemwise only_process_constants	this function is basically a call to tensor get_scalar_constant_value
tensor		transpose	x axes	reorder the dimensions of x default reverse them
tensor		batched_dot	a b	compute the batched dot product of two variables batched_dot a b [i] = dot(a[i], b[i])
tensor		batched_tensordot	x y axes	compute a batched tensordot product
tensor	Split	grad	inputs g_outputs	join the gradients along the axis that was used to split x
tensor		addbroadcast	x	make the input broadcastable in the specified axes
tensor		unbroadcast	x	make the input impossible to broadcast in the specified axes
tensor		patternbroadcast	x broadcastable	make the input adopt a specific broadcasting pattern
tensor	Join	grad	axis_and_tensors grads	the gradient wrt a join op is a split, used to partition the gradient along the axis which was used for joining
tensor		join	axis	convenience function to concatenate tensortypes along the given axis
tensor		roll	x shift axis	convenience function to roll tensortypes along the given axis
tensor		shape_padleft	t n_ones	reshape t by left-padding the shape with n_ones 1s
tensor		shape_padright	t n_ones	reshape t by right-padding the shape with n_ones 1s
tensor		shape_padaxis	t axis	reshape t by inserting 1 at the dimension axis
tensor		stack		stack tensors in sequence on given axis default is 0
tensor		concatenate	tensor_list axis	alias for join(axis *tensor_list)
tensor		get_vector_length	v	return the run-time length of a symbolic vector
tensor		horizontal_stack		horizontally stack two l{tensortype}s
tensor		is_flat	var outdim	verifies the dimensionality of the var is equal to outdim
tensor		flatten	x ndim outdim	reshapes the variable x by keeping the first outdim-1 dimension size s of x the same
tensor		tile	x reps ndim	tile input array x according to reps
tensor	PermuteRowElements	_rec_perform	node x y inverse	perform the permutation by doing a recursion over the input dimensions
tensor		inverse_permutation	perm	computes the inverse of permutations
tensor		dot	a b	computes the dot product of two variables
tensor		_tensordot_as_dot	a b axes dot	reduces a tensor dot product to a matrix or vector dot product based
tensor		tensordot	a b axes	compute a generalized dot product over provided axes
tensor		outer	x y	return vector-vector outer product
tensor		diagonal	a offset axis1 axis2	a helper function for theano tensor extractdiag it accepts tensor with
tensor		diag	v k	a helper function for two ops theano tensor extractdiag and
tensor		stacklists	arg	recursively stack lists of tensors to maintain similar structure
tensor		ptp	a axis	range of values maximum - minimum along an axis
tensor		swapaxes	y axis1 axis2	swap axes of inputted tensor
tensor		choose	a choices out mode	construct an array from an index array and a set of arrays to choose from
tensor		detect_macos_sdot_bug		try to detect a bug in the default blas in macos
tensor		cblas_header_text		c header for the cblas interface
tensor		blas_header_text		c header for the fortran blas interface
tensor		mkl_threads_text		c header for mkl threads interface
tensor		openblas_threads_text		c header for openblas threads interface
tensor		ldflags	libs flags libs_dir include_dir	extract a list of compilation flags from config blas ldflags
tensor		_ldflags	ldflags_str libs flags libs_dir	extract list of compilation flags from a string
tensor		_as_scalar	res dtype	return none or a tensorvariable whose type is in t float_scalar_types
tensor		_gemm_from_factored_list	lst	returns none or a list to replace node outputs
tensor		_gemm_from_node2	node	:todo in many expressions there are many ways to turn it into a gemm
tensor		local_gemm_to_gemv	node	gemm acting on row or column matrices -> gemv
tensor		local_gemm_to_ger	node	gemm computing an outer-product -> ger
tensor		local_dot22_to_ger_or_gemv	node	dot22 computing an outer-product -> ger
tensor	_tensor_py_operators	reshape	shape ndim	return a reshaped view/copy of this variable
tensor	_tensor_py_operators	dimshuffle		reorder the dimensions of this variable optionally inserting broadcasted dimensions
tensor	_tensor_py_operators	transfer	target	if target is 'cpu' this will transfer to a tensortype if not already one
tensor	_tensor_py_operators	copy	name	return a symbolic copy and optionally assign a name
tensor	_tensor_py_operators	sum	axis dtype keepdims acc_dtype	see theano tensor sum
tensor	_tensor_py_operators	prod	axis dtype keepdims acc_dtype	see theano tensor prod
tensor	_tensor_py_operators	mean	axis dtype keepdims acc_dtype	see theano tensor mean
tensor	_tensor_py_operators	var	axis ddof keepdims corrected	see theano tensor var
tensor	_tensor_py_operators	std	axis ddof keepdims corrected	see theano tensor std
tensor	_tensor_py_operators	min	axis keepdims	see theano tensor min
tensor	_tensor_py_operators	max	axis keepdims	see theano tensor max
tensor	_tensor_py_operators	argmin	axis keepdims	see theano tensor argmin
tensor	_tensor_py_operators	argmax	axis keepdims	see theano tensor argmax
tensor	_tensor_py_operators	nonzero	return_matrix	see theano tensor nonzero
tensor	_tensor_py_operators	nonzero_values		see theano tensor nonzero_values
tensor	_tensor_py_operators	sort	axis kind order	see theano tensor sort
tensor	_tensor_py_operators	argsort	axis kind order	see theano tensor argsort
tensor	_tensor_py_operators	clip	a_min a_max	clip limit the values in an array
tensor	_tensor_py_operators	conj		see theano tensor conj
tensor	_tensor_py_operators	repeat	repeats axis	see theano tensor repeat
tensor	_tensor_py_operators	round	mode	see theano tensor round
tensor	_tensor_py_operators	ptp	axis	see 'theano tensor ptp'
tensor	_tensor_py_operators	swapaxes	axis1 axis2	return 'tensor swapaxes self axis1 axis2
tensor	_tensor_py_operators	fill	value	fill inputted tensor with the assigned value
tensor	_tensor_py_operators	choose	a choices out mode	construct an array from an index array and a set of arrays to choose from
tensor	_tensor_py_operators	squeeze		remove broadcastable dimensions from the shape of an array
tensor	_tensor_py_operators	compress	a axis	return selected slices only
tensor	TensorConstantSignature	_get_sum		compute sum of non nan / inf values in the array
tensor		make_declare	loop_orders dtypes sub	produce code to declare all necessary variables
tensor		make_alloc	loop_orders dtype sub fortran	generate c code to allocate outputs
tensor		make_loop	loop_orders dtypes loop_tasks sub	make a nested loop over several arrays and associate specific code to each level of nesting
tensor		make_reordered_loop	init_loop_orders olv_index dtypes inner_task	a bit like make_loop but when only the inner-most loop executes code
tensor		make_loop_careduce	loop_orders dtypes loop_tasks sub	make a nested loop over several arrays and associate specific code to each level of nesting
tensor		searchsorted	x v side sorter	find indices where elements should be inserted to maintain order
tensor		cumsum	x axis	return the cumulative sum of the elements along a given axis
tensor		cumprod	x axis	return the cumulative product of the elements along a given axis
tensor		diff	x n axis	calculate the n-th order discrete difference along given axis
tensor		bincount	x weights minlength assert_nonneg	count number of occurrences of each value in array of ints
tensor		squeeze	x	remove broadcastable dimensions from the shape of an array
tensor		compress	condition x axis	return selected slices of an array along given axis
tensor		repeat	x repeats axis	repeat elements of an array
tensor		bartlett	M	an instance of this class returns the bartlett spectral window in the time-domain
tensor		fill_diagonal	a val	returns a copy of an array with all elements of the main diagonal set to a specified scalar value
tensor		fill_diagonal_offset	a val offset	returns a copy of an array with all elements of the main diagonal set to a specified scalar value
tensor		to_one_hot	y nb_class dtype	return a matrix where each row correspond to the one hot encoding of each element in y
tensor	SortOp	__get_argsort_indices	a axis	calculates indices which can be used to reverse sorting operation of "a" tensor along "axis"
tensor		argsort	a axis kind order	returns the indices that would sort an array
tensor		make_constant	args	convert python litterals to theano constants in subtensor arguments
tensor		get_idx_list	inputs idx_list get_count	given a list of inputs to the subtensor and its idx_list reorders the inputs according to the idx list to get the right values
tensor		get_canonical_form_slice	theslice length	given a slice [start stop step] transform it into a canonical form that respects the conventions imposed by python and numpy
tensor	Subtensor	convert	entry slice_ok	change references to variables into references to types
tensor	Subtensor	get_constant_idx	inputs allow_partial only_process_constants elemwise	return the idx_list with constant inputs replaced by their python scalar equivalent
tensor	Subtensor	default_helper_c_code_args		returns a dictionary of default arguments to helper_c_code
tensor	Subtensor	helper_c_code	node name inputs outputs	the parameters c_prefix are there to allow reusing this function on pyarray and gpuarray object
tensor		set_subtensor	x y inplace tolerate_inplace_aliasing	return x with the given subtensor overwritten by y
tensor		inc_subtensor	x y inplace set_instead_of_inc	return x with the given subtensor incremented by y
tensor	IncSubtensor	do_type_checking	node	should raise notimplementederror if c_code does not support the types involved in this node
tensor	IncSubtensor	get_helper_c_code_args		return a dictionary of arguments to pass to helper_c_code
tensor	IncSubtensor	add_to_zview	name x fail	return c code to add x to zview should decref zview if the
tensor		_sum_grad_over_bcasted_dims	x gx	sum of gx over dimensions to reproduce x broadcastable
tensor		adv_index_broadcastable_pattern	a idx	this function is only used to determine the broadcast pattern for advancedsubtensor output variable
tensor		_scal_inplace	symbol	replace a symbol definition with an elementwise version of the corresponding scalar op
tensor		lt_inplace	a b	a < b inplace on a
tensor		gt_inplace	a b	a > b inplace on a
tensor		le_inplace	a b	a <= b inplace on a
tensor		ge_inplace	a b	a >= b inplace on a
tensor		eq_inplace	a b	a == b inplace on a
tensor		neq_inplace	a b	a != b inplace on a
tensor		and__inplace	a b	bitwise a & b inplace on a
tensor		or__inplace	a b	bitwise a | b inplace on a
tensor		xor_inplace	a b	bitwise a ^ b inplace on a
tensor		invert_inplace	a	bitwise ~a inplace on a
tensor		abs__inplace	a	|a| (inplace on a)
tensor		exp_inplace	a	e^a (inplace on a)
tensor		exp2_inplace	a	2^a (inplace on a)
tensor		expm1_inplace	a	e^a - 1 (inplace on a)
tensor		neg_inplace	a	-a inplace on a
tensor		inv_inplace	a	1 0/a inplace on a
tensor		log_inplace	a	base e logarithm of a inplace on a
tensor		log2_inplace	a	base 2 logarithm of a inplace on a
tensor		log10_inplace	a	base 10 logarithm of a inplace on a
tensor		sgn_inplace	a	sign of a (inplace on a)
tensor		ceil_inplace	a	ceil of a (inplace on a)
tensor		floor_inplace	a	floor of a (inplace on a)
tensor		trunc_inplace	a	trunc of a (inplace on a)
tensor		round_half_to_even_inplace	a	round_half_to_even_inplace a (inplace on a)
tensor		round_half_away_from_zero_inplace	a	round_half_away_from_zero_inplace a (inplace on a)
tensor		sqr_inplace	a	square of a (inplace on a)
tensor		sqrt_inplace	a	square root of a (inplace on a)
tensor		deg2rad_inplace	a	convert degree a to radian(inplace on a)
tensor		rad2deg_inplace	a	convert radian a to degree(inplace on a)
tensor		cos_inplace	a	cosine of a (inplace on a)
tensor		arccos_inplace	a	arccosine of a (inplace on a)
tensor		sin_inplace	a	sine of a (inplace on a)
tensor		arcsin_inplace	a	arcsine of a (inplace on a)
tensor		tan_inplace	a	tangent of a (inplace on a)
tensor		arctan_inplace	a	arctangent of a (inplace on a)
tensor		arctan2_inplace	a b	arctangent of a / b (inplace on a)
tensor		cosh_inplace	a	hyperbolic cosine of a (inplace on a)
tensor		arccosh_inplace	a	hyperbolic arc cosine of a (inplace on a)
tensor		sinh_inplace	a	hyperbolic sine of a (inplace on a)
tensor		arcsinh_inplace	a	hyperbolic arc sine of a (inplace on a)
tensor		tanh_inplace	a	hyperbolic tangent of a (inplace on a)
tensor		arctanh_inplace	a	hyperbolic arc tangent of a (inplace on a)
tensor		erfc_inplace	a	complementary error function
tensor		erfcx_inplace	a	scaled complementary error function
tensor		gammaln_inplace	a	log gamma function
tensor		psi_inplace	a	derivative of log gamma function
tensor		chi2sf_inplace	x k	chi squared survival function
tensor		j0_inplace	x	bessel function of the first kind of order 0
tensor		j1_inplace	x	bessel function of the first kind of order 1
tensor		jv_inplace	v x	bessel function of the first kind of order v real
tensor		i0_inplace	x	modified bessel function of the first kind of order 0
tensor		i1_inplace	x	modified bessel function of the first kind of order 1
tensor		iv_inplace	v x	modified bessel function of the first kind of order v real
tensor		second_inplace	a	fill a with b
tensor		maximum_inplace	a b	elementwise addition (inplace on a)
tensor		minimum_inplace	a b	elementwise addition (inplace on a)
tensor		add_inplace	a b	elementwise addition (inplace on a)
tensor		sub_inplace	a b	elementwise subtraction (inplace on a)
tensor		mul_inplace	a b	elementwise multiplication (inplace on a)
tensor		true_div_inplace	a b	elementwise division (inplace on a)
tensor		int_div_inplace	a b	elementwise division (inplace on a)
tensor		mod_inplace	a b	elementwise modulo (inplace on a)
tensor		pow_inplace	a b	elementwise power (inplace on a)
tensor		conj_inplace	a	elementwise conjugate (inplace on a)
tensor		transpose_inplace	x	perform a transpose on a tensor without copying the underlying storage
tensor		rfft	inp norm	performs the fast fourier transform of a real-valued input
tensor		irfft	inp norm is_odd	performs the inverse fast fourier transform with real-valued output
tensor	Cholesky	grad	inputs gradients	cholesky decomposition reverse-mode gradient update
tensor	CholeskyGrad	perform	node inputs outputs	implements the "reverse-mode" gradient [1]_ for the cholesky factorization of a positive-definite matrix
tensor	Solve	grad	inputs output_gradients	reverse-mode gradient updates for matrix solve operation c = a \ b
tensor		randomstate_constructor	value name strict allow_downcast	sharedvariable constructor for randomstate
tensor	RandomStreams	seed	seed	re-initialize each random stream
tensor	RandomStreams	__getitem__	item	retrieve the numpy randomstate instance associated with a particular stream
tensor	RandomStreams	__setitem__	item val	set the numpy randomstate instance associated with a particular stream
tensor	RandomStreams	gen	op	create a new random stream in this container
tensor		local_max_and_argmax	node	if we don't use the argmax change it to a max only
tensor		local_max_to_min	node	change -(max -x to min
tensor		local_alloc_dimshuffle	node	if a dimshuffle is inside an alloc and only adds dimension to the left remove it
tensor		local_reshape_dimshuffle	node	if a dimshuffle is inside a reshape and does not change the order of dimensions remove it
tensor		local_dimshuffle_alloc	node	if an alloc is inside a dimshuffle which only adds dimension to the left
tensor		local_dimshuffle_subtensor	node	if a subtensor is inside a dimshuffle which only drop broadcastable dimensions scrap the dimshuffle and index the
tensor	TensorType	clone	dtype broadcastable	return a copy of the type optionally with a new dtype or broadcastable pattern
tensor	TensorType	filter	data strict allow_downcast	convert data to something which can be associated to a tensorvariable
tensor	TensorType	filter_variable	other allow_convert	convert a symbolic variable into a tensortype if compatible
tensor	TensorType	dtype_specs		return a tuple python type c type numpy typenum that corresponds to self
tensor	TensorType	__eq__	other	compare true iff other is the same kind of tensortype
tensor	TensorType	__hash__		hash equal for same kinds of tensortype
tensor	TensorType	make_variable	name	return a tensorvariable of this type
tensor	TensorType	c_declare	name sub check_input	override clinkertype c_declare
tensor	TensorType	c_init	name sub	override clinkertype c_init
tensor	TensorType	c_extract	name sub check_input	override clinkertype c_extract
tensor	TensorType	c_cleanup	name sub	override clinkertype c_cleanup
tensor	TensorType	c_sync	name sub	override clinkertype c_sync
tensor	TensorType	c_headers	c_compiler	override clinkerobject c_headers
tensor	TensorType	c_support_code		override clinkerobject c_support_code
tensor	TensorType	value_zeros	shape	create an numpy ndarray full of 0 values
tensor	TensorType	get_shape_info	obj	return the information needed to compute the memory size of obj
tensor	TensorType	get_size	shape_info	number of bytes taken by the object represented by shape_info
tensor		gemv_c_code	y A x z	z <- beta * y + alpha * dot a x
tensor	Fourier	grad	inputs cost_grad	in defining the gradient the finite fourier transform is viewed as
tensor		scalarconsts_rest	inputs elemwise only_process_constants	partition a list of variables into two kinds scalar constants and the rest
tensor		broadcast_like	value template fgraph dtype	return a variable with the same shape and dtype as the template filled by broadcasting value through it
tensor	InplaceElemwiseOptimizer	apply	fgraph	usage inplaceelemwiseoptimizer op optimize fgraph
tensor		local_dimshuffle_lift	node	"lifts" dimshuffle through elemwise operations and merges consecutive dimshuffles
tensor		local_useless_dimshuffle_in_reshape	node	removes useless dimshuffle operation inside reshape reshape(vector
tensor		local_lift_transpose_through_dot	node	dot x y t -> dot y t x t
tensor		local_tensor_scalar_tensor	node	tensor_from_scalar(scalar_from_tensor x -> x
tensor		local_scalar_tensor_scalar	node	scalar_from_tensor(tensor_from_scalar x -> x
tensor	ShapeFeature	get_shape	var idx	optimization can call this to get the current shape_i it is better to call this then use directly shape_of[var][idx]
tensor	ShapeFeature	shape_ir	i r	return symbolic r shape[i] for tensor variable r int i
tensor	ShapeFeature	shape_tuple	r	return a tuple of symbolic shape vars for tensor variable r
tensor	ShapeFeature	default_infer_shape	node i_shapes	return a list of shape tuple or none for the outputs of node
tensor	ShapeFeature	unpack	s_i var	return a symbolic integer scalar for the shape element s_i
tensor	ShapeFeature	set_shape	r s override	assign the shape s to previously un-shaped variable r
tensor	ShapeFeature	update_shape	r other_r	replace shape of r by shape of other_r
tensor	ShapeFeature	set_shape_i	r i s_i	replace element i of shape_of[r] by s_i
tensor	ShapeFeature	init_r	r	register r's shape in the shape_of dictionary
tensor	ShapeFeature	same_shape	x y dim_x dim_y	return true if we are able to assert that x and y have the same shape
tensor		local_fill_sink	node	f(fill a b fill c d e) -> fill(c fill(a f b d e )) f need to be an elemwise that isn't a fill
tensor		local_fill_to_alloc	node	fill s v -> alloc(v shape s this is an important optimization because with the shape_to_shape_i
tensor		local_useless_fill	node	fill s v -> v this optimization is only needed in fast_compile to make the code
tensor		local_useless_alloc	node	if the input type is the same as the output type dtype and broadcast there is no change in the shape of the input
tensor		local_canonicalize_alloc	node	if the input type is the same as the output type dtype and broadcast there is no change in the shape of the input
tensor		local_alloc_empty_to_zeros	node	this convert allocempty to alloc of 0
tensor		local_subtensor_inc_subtensor	node	subtensor(setsubtensor x y idx idx) -> y
tensor		local_subtensor_remove_broadcastable_index	node	remove broadcastable dimension with index 0 or -1 a[:,:,:,0] -> a
tensor		local_subtensor_make_vector	node	replace all subtensor(make_vector) like [a b c][0] -> a
tensor		local_useless_elemwise	node	eq x x -> 1
tensor		local_alloc_unary	node	unary(alloc x shp -> alloc(unary x shp)
tensor		local_cast_cast	node	cast(cast x dtype1 dtype2) when those contrain
tensor		is_an_upcast	type1 type2	given two data types as strings check if converting to type2 from type1 constitutes an upcast
tensor		local_func_inv	node	check for two consecutive operations that are functional inverses and remove them from the function graph
tensor		is_inverse_pair	node_op prev_op inv_pair	given two consecutive operations check if they are the provided pair of inverse functions
tensor		local_remove_all_assert	node	an optimization disabled by default that removes all asserts from the graph
tensor		local_upcast_elemwise_constant_inputs	node	this explicitly upcasts constant inputs to elemwise ops when those ops do implicit upcasting anyway
tensor		local_useless_inc_subtensor	node	remove incsubtensor when we overwrite the full inputs with the new value
tensor		local_set_to_inc_subtensor	node	advancedincsubtensor1(x x[ilist]+other ilist set_instead_of_inc=true) ->
tensor		local_useless_slice	node	remove subtensor of the form x[0 :] -> x[0]
tensor		local_useless_subtensor	node	remove subtensor/advancedsubtensor1 if it takes the full input in the
tensor		local_subtensor_lift	node	unary x [idx] -> unary(x[idx])#any broadcast pattern
tensor		merge_two_slices	slice1 len1 slice2 len2	this function merges two slices into a single slice the code works on
tensor		local_subtensor_merge	node	refactored optimization to deal with all cases of tensor merging
tensor		local_subtensor_of_alloc	node	alloc val [x y] -> alloc(val[ ])
tensor		local_subtensor_of_dot	node	this optimization translates t dot a b [idxs] into t dot(a[idxs_a], b[idxs_b]),
tensor		local_IncSubtensor_serialize	node	when using subtensor gradient graphs can be ugly
tensor		local_inplace_setsubtensor	node	also work for gpuincsubtensor
tensor		local_inplace_incsubtensor1	node	also work for gpuadvancedincsubtensor1
tensor		local_incsubtensor_of_zeros	node	incsubtensor x zeros idx -> x
tensor		local_incsubtensor_of_zeros_to_setsubtensor	node	incsubtensor zeros x -> setsubtensor zeros x
tensor		local_setsubtensor_of_constants	node	setsubtensor(x x[idx], idx) -> x when x is constant or alloc
tensor		local_adv_sub1_adv_inc_sub1	node	optimize the possible advsub1(advincsub1
tensor		local_useless_inc_subtensor_alloc	node	replaces an [advanced]incsubtensor[1], whose increment is an alloc of a fully or partially broadcastable variable by one that skips the
tensor		local_useless_rebroadcast	node	remove rebroadcast if id does not actually change the broadcasting pattern
tensor		local_rebroadcast_lift	node	lifts rebroadcast through unary elemwise operations and merges consecutive rebroadcasts
tensor		apply_rebroadcast_opt	rval	apply as many times as required the optimization local_useless_rebroadcast and local_rebroadcast_lift
tensor		local_join_1	node	join i x => x remove join() when only one element is joined
tensor		local_join_empty	node	join i x y empty => join i x y remove empty inputs to joins
tensor		local_join_make_vector	node	join(0 make_vector1 make_vector2 => join(0 make_vector12
tensor		local_sumsqr2dot	node	this optimization detects t sqr( w dimshuffle('x',0 1) * g dimshuffle(0 'x',1) sum(axis= 1 2
tensor		local_expm1	node	this optimization detects exp a -1 and converts this to expm1 a
tensor		local_useless_switch	node	this optimization makes the following changes in the graph t
tensor		local_mul_switch_sink	node	this optimization makes the folowing changes in the graph t
tensor		local_div_switch_sink	node	this optimization makes the folowing changes in the graph t
tensor		local_useless_tile	node	tile(x 1 *n) -> x this is useless tile
tensor		local_useless_split	node	split{n_splits=1} x y -> x remove split with only 1 split
tensor		local_flatten_lift	node	flatten(unaryelemwise x -> unaryelemwise(flatten x this optimization is needed by optimization
tensor		local_useless_reshape	node	remove two kinds of useless reshape
tensor		local_reshape_to_dimshuffle	node	broadcastable dimensions in reshape are replaced with dimshuffle
tensor		local_reshape_lift	node	reshape(unaryelemwise x -> unaryelemwise(reshape x this optimization is needed by optimization
tensor	Canonizer	get_num_denum	input	this extract two lists num and denum such that the input is self
tensor	Canonizer	merge_num_denum	num denum	utility function which takes two lists num and denum and returns something which is equivalent to inverse(main(\*num),
tensor	Canonizer	simplify_factors	num denum	for any variable r which is both in num and denum removes it from both lists
tensor	Canonizer	simplify_constants	orig_num orig_denum out_type	find all constants and put them together into a single constant
tensor		local_sum_prod_mul_by_scalar	node	sum(scalar * smth) -> scalar * sum smth
tensor		local_elemwise_sub_zeros	node	elemwise{sub} x x -> zeros_like x
tensor		local_sum_prod_div_dimshuffle	node	sum(a / dimshuffle{ } b axis=l) -> sum(a axis={ }) / b
tensor		local_sum_prod_all_to_none	node	sum{0 1 n} -> sum{} or
tensor		local_op_of_op	node	prod(prod()) -> single prod()
tensor		local_reduce_join	node	reduce{scalar op}(join(axis=0 a b), axis=0) -> elemwise{scalar op} a b
tensor		local_useless_reduce	node	sum(a axis=[]) -> a
tensor		local_reduce_broadcastable	node	remove reduction over broadcastable dimensions
tensor		local_opt_alloc	node	sum(alloc constant shapes => constant*prod shapes
tensor		local_neg_div_neg	node	- (-a / b) -> a / b also performs - (c / b) -> -c / b) when c is a scalar constant
tensor		local_mul_zero	node	as part of canonicalization we replace multiplication by zero with zero
tensor		local_mul_to_sqr	node	x*x -> sqr x this is faster on the gpu when memory fetching is a big part of
tensor		local_intdiv_by_one	node	x // 1 -> x
tensor		local_zero_div	node	0 / x -> 0
tensor		local_pow_specialize_device	node	this optimization is not the same on all device we do it only on cpu here
tensor		local_mul_specialize	node	remove special-case constants from mul arguments and useless neg in inputs
tensor		check_for_x_over_absX	numerators denominators	convert x/abs x into sign x
tensor		local_abs_lift	node	move the abs toward the input
tensor		local_abs_merge	node	merge abs generated by local_abs_lift when the canonizer don't
tensor		local_greedy_distributor	node	optimize by reducing the number of multiplications and/or divisions
tensor		get_clients	node	used by erf/erfc opt to track less frequent op
tensor		get_clients2	node	used by erf/erfc opt to track less frequent op
tensor		local_elemwise_fusion_op	OP max_input_fct maker	we parametrize it to make it work for elemwise and gpuelemwise op
tensor		local_add_mul_fusion	node	fuse consecutive add or mul in one such node with more inputs
tensor		local_useless_composite	node	for elemwise composite that have multiple outputs remove the outputs that are not used
tensor		hash_from_ndarray	data	return a hash from an ndarray
tensor		shape_of_variables	fgraph input_shapes	compute the numeric shape of all intermediate variables given input shapes
tensor	Elemwise	get_output_info	dim_shuffle	return the outputs dtype and broadcastable pattern and the dimshuffled niputs
tensor	Elemwise	make_node		if the inputs have different number of dimensions their shape is left-completed to the greatest number of dimensions with 1s
tensor	Elemwise	python_constant_folding	node	return true if we do not want to compile c code when doing constant folding of this node
tensor	Prod	L_op	inp out grads	the grad of this op could be very easy if it is was not for the case where zeros are present in a given "group" (ie
tensor		load	path dtype broadcastable mmap_mode	load an array from an npy file
tensor		isend	var dest tag	non blocking send
tensor		mpi_send_wait_key	a	wait as long as possible on waits start send/recvs early
tensor		mpi_tag_key	a	break mpi ties by using the variable tag - prefer lower tags first
tensor		load_shared_variable	val	this function is only here to keep some pickles loading after a failed fix done in august 2011
tensor		tensor_constructor	value name strict allow_downcast	sharedvariable constructor for tensortype
tensor		scalar_constructor	value name strict allow_downcast	sharedvariable constructor for scalar values default int64 or float64
tensor	MatrixInverse	grad	inputs g_outputs	the gradient function should return
tensor	MatrixInverse	R_op	inputs eval_points	the gradient function should return
tensor		matrix_dot		shorthand for product between several dots
tensor	ExtractDiag	perform	node ins outs	for some reason numpy diag x is really slow so we
tensor		diag	x	numpy-compatibility method if x is a matrix return its diagonal
tensor		trace	X	returns the sum of diagonal elements of matrix x
tensor	Eigh	grad	inputs g_outputs	the gradient function should return
tensor	EighGrad	perform	node inputs outputs	implements the "reverse-mode" gradient for the eigensystem of a square matrix
tensor		qr	a mode	computes the qr decomposition of a matrix
tensor		svd	a full_matrices compute_uv	this function performs the svd on cpu
tensor		matrix_power	M n	raise a square matrix to the integer power n
tensor		tensorinv	a ind	does not run on gpu theano utilization of numpy
tensor		tensorsolve	a b axes	theano utilization of numpy linalg tensorsolve does not run on gpu!
tensor		_infer_ndim_bcast	ndim shape	infer the number of dimensions from the shape or the other arguments
tensor		_generate_broadcasting_indices	out_shape	return indices over each shape that broadcast them to match out_shape
tensor		uniform	random_state size low high	sample from a uniform distribution between low and high
tensor		normal	random_state size avg std	sample from a normal distribution centered on avg with the specified standard deviation std
tensor		binomial	random_state size n p	sample n times with probability of success prob for each trial return the number of successes
tensor		random_integers_helper	random_state low high size	helper function to draw random integers
tensor		random_integers	random_state size low high	sample a random integer between low and high both inclusive
tensor		choice_helper	random_state a replace p	helper function to draw random numbers using numpy's choice function
tensor		choice	random_state size a replace	choose values from a with or without replacement a can be a 1-d array
tensor		poisson	random_state size lam ndim	draw samples from a poisson distribution
tensor		permutation_helper	random_state n shape	helper function to generate permutations from integers
tensor		permutation	random_state size n ndim	return permutations of the integers between 0 and n-1
tensor		multinomial_helper	random_state n pvals size	helper function drawing from multinomial distributions
tensor		multinomial	random_state size n pvals	sample from one or more multinomial distributions defined by one-dimensional slices in pvals
tensor	RandomStreamsBase	binomial	size n p ndim	sample n times with probability of success p for each trial and return the number of successes
tensor	RandomStreamsBase	uniform	size low high ndim	sample a tensor of given size whose element from a uniform distribution between low and high
tensor	RandomStreamsBase	normal	size avg std ndim	sample from a normal distribution centered on avg with the specified standard deviation std
tensor	RandomStreamsBase	random_integers	size low high ndim	sample a random integer between low and high both inclusive
tensor	RandomStreamsBase	choice	size a replace p	choose values from a with or without replacement
tensor	RandomStreamsBase	poisson	size lam ndim dtype	draw samples from a poisson distribution
tensor	RandomStreamsBase	permutation	size n ndim dtype	return permutations of the integers between 0 and n-1
tensor	RandomStreamsBase	multinomial	size n pvals ndim	sample n times from a multinomial distribution defined by probabilities pvals as many times as required by size
tensor	RandomStreamsBase	shuffle_row_elements	input	return a variable with every row rightmost index shuffled
tensor.nnet	BaseCorr3dMM	as_common_dtype	in1 in2	upcast input variables if neccesary
tensor.nnet	BaseCorr3dMM	c_code_helper	bottom weights top direction	this generates the c code for corr3dmm (direction="forward"), corr3dmm_gradweights (direction="backprop weights"), and
tensor.nnet		get_diagonal_subtensor_view	x i0 i1	helper function for diagonalsubtensor and incdiagonalsubtensor
tensor.nnet		conv3d	signals filters signals_shape filters_shape	convolve spatio-temporal filters with a movie
tensor.nnet		local_inplace_DiagonalSubtensor	node	also work for incdiagonalsubtensor
tensor.nnet		get_conv_output_shape	image_shape kernel_shape border_mode subsample	this function compute the output shape of convolution operation
tensor.nnet		get_conv_shape_1axis	image_shape kernel_shape border_mode subsample	this function compute the output shape of convolution operation
tensor.nnet		get_conv_gradweights_shape	image_shape top_shape border_mode subsample	this function tries to compute the kernel shape of convolution gradweights
tensor.nnet		get_conv_gradweights_shape_1axis	image_shape top_shape border_mode subsample	this function tries to compute the image shape of convolution gradweights
tensor.nnet		get_conv_gradinputs_shape	kernel_shape top_shape border_mode subsample	this function tries to compute the image shape of convolution gradinputs
tensor.nnet		get_conv_gradinputs_shape_1axis	kernel_shape top_shape border_mode subsample	this function tries to compute the image shape of convolution gradinputs
tensor.nnet		check_conv_gradinputs_shape	image_shape kernel_shape output_shape border_mode	this function checks if the given image shapes are consistent
tensor.nnet		assert_conv_shape	shape	this function adds assert nodes that check if shape is a valid convolution shape
tensor.nnet		assert_shape	x expected_shape msg	wraps x in an assert to check its shape
tensor.nnet		conv2d	input filters input_shape filter_shape	this function will build the symbolic graph for convolving a mini-batch of a stack of 2d inputs with a set of 2d filters
tensor.nnet		conv3d	input filters input_shape filter_shape	this function will build the symbolic graph for convolving a mini-batch of a stack of 3d inputs with a set of 3d filters
tensor.nnet		conv2d_grad_wrt_inputs	output_grad filters input_shape filter_shape	compute conv output gradient w r t its inputs
tensor.nnet		conv3d_grad_wrt_inputs	output_grad filters input_shape filter_shape	compute conv output gradient w r t its inputs
tensor.nnet		conv2d_grad_wrt_weights	input output_grad filter_shape input_shape	compute conv output gradient w r t its weights
tensor.nnet		conv3d_grad_wrt_weights	input output_grad filter_shape input_shape	compute conv output gradient w r t its weights
tensor.nnet		bilinear_kernel_2D	ratio normalize	compute 2d kernel for bilinear upsampling this function builds the 2d kernel that can be used to upsample
tensor.nnet		bilinear_kernel_1D	ratio normalize	compute 1d kernel for bilinear upsampling this function builds the 1d kernel that can be used to upsample
tensor.nnet		bilinear_upsampling	input ratio batch_size num_input_channels	compute bilinear upsampling this function will build the symbolic graph for upsampling
tensor.nnet	BaseAbstractConv	flops	inp outp	useful with the hack in profiling to print the mflops
tensor.nnet	BaseAbstractConv	conv	img kern mode dilation	basic slow python 2d or 3d convolution for debugmode
tensor.nnet	SparseBlockGemv	make_node	o W h inputIdx	compute the dot product of the specified pieces of vectors and matrices
tensor.nnet	SparseBlockOuter	make_node	o x y xIdx	compute the dot product of the specified pieces of vectors and matrices
tensor.nnet		sparse_block_dot	W h inputIdx b	compute the dot product plus bias of the specified pieces of vectors and matrices
tensor.nnet		local_logsoftmax	node	detect log(softmax x and replace it with logsoftmax x
tensor.nnet		local_logsoftmax_grad	node	detect log(softmax x )'s grad and replace it with logsoftmax x 's grad
tensor.nnet		local_softmax_with_bias	node	try to turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias
tensor.nnet	CrossentropySoftmaxArgmax1HotWithBias	perform	node input_storage output_storage	the math where x is an input vector and t is a target index softmax x [i] = exp(x[i]) / sum_j(exp(x[j]))
tensor.nnet		crossentropy_to_crossentropy_with_softmax_with_bias	fgraph	this is a stabilization optimization
tensor.nnet		crossentropy_to_crossentropy_with_softmax	fgraph	this is a stabilization optimization that is more general than crossentropy_to_crossentropy_with_softmax_with_bias
tensor.nnet		_check_rows_is_arange_len_labels	rows labels	check that 'rows' is the same node as t arange(labels shape[0])
tensor.nnet		local_useless_crossentropy_softmax_1hot_with_bias_dx_alloc	node	replace a crossentropysoftmax1hotwithbiasdx op whose incoming gradient is an alloc of a scalar variable or one that has either broadcastable or
tensor.nnet		binary_crossentropy	output target	compute the crossentropy of binary random variables
tensor.nnet		sigmoid_binary_crossentropy	output target	compute the cross-entropy of binary random variables
tensor.nnet		categorical_crossentropy	coding_dist true_dist	return the cross-entropy between an approximating distribution and a true distribution
tensor.nnet		relu	x alpha	compute the element-wise rectified linear activation function
tensor.nnet		h_softmax	x batch_size n_outputs n_classes	two-level hierarchical softmax
tensor.nnet		elu	x alpha	compute the element-wise exponential linear activation function
tensor.nnet		confusion_matrix	actual pred	computes the confusion matrix of given vectors containing actual observations and predicted observations
tensor.nnet		conv3D	V W b d	3d "convolution" of multiple filters on a minibatch
tensor.nnet	BaseCorrMM	as_common_dtype	in1 in2	upcast input variables if neccesary
tensor.nnet	BaseCorrMM	c_code_helper	bottom weights top direction	this generates the c code for corrmm (direction="forward"), corrmm_gradweights (direction="backprop weights"), and
tensor.nnet	ScalarSigmoid	gen_graph		this method was used to generate the graph sigmoid_prec png in the doc
tensor.nnet		local_ultra_fast_sigmoid	node	when enabled change all sigmoid to ultra_fast_sigmoid
tensor.nnet		hard_sigmoid	x	an approximation of sigmoid
tensor.nnet		is_exp	var	match a variable with either of the exp x or -exp x patterns
tensor.nnet		is_mul	var	match a variable with x * y * z *
tensor.nnet		is_neg	var	match a variable with the -x pattern
tensor.nnet		local_exp_over_1_plus_exp	node	exp x /(1+exp x -> sigm x
tensor.nnet		parse_mul_tree	root	parse a tree of multiplications starting at the given root
tensor.nnet		replace_leaf	arg leaves new_leaves op	attempt to replace a leaf of a multiplication tree
tensor.nnet		simplify_mul	tree	simplify a multiplication tree
tensor.nnet		compute_mul	tree	compute the variable that is the output of a multiplication tree
tensor.nnet		perform_sigm_times_exp	tree exp_x exp_minus_x sigm_x	core processing of the local_sigm_times_exp optimization
tensor.nnet		local_sigm_times_exp	node	exp x * sigm -x -> sigm x
tensor.nnet		local_inv_1_plus_exp	node	1/(1+exp x -> sigm -x
tensor.nnet		local_1msigmoid	node	1-sigm x -> sigm -x
tensor.nnet		conv2d	input filters input_shape filter_shape	this function will build the symbolic graph for convolving a mini-batch of a stack of 2d inputs with a set of 2d filters
tensor.nnet		conv2d_transpose	input filters output_shape filter_shape	this function will build the symbolic graph for applying a transposed convolution over a mini-batch of a stack of 2d inputs with a set of 2d
tensor.nnet		local_inplace_sparse_block_gemv	node	sparseblockgemv(inplace=false) -> sparseblockgemv(inplace=true)
tensor.nnet		local_inplace_sparse_block_outer	node	sparseblockouter(inplace=false) -> sparseblockouter(inplace=true)
tensor.nnet		images2neibs	ten4 neib_shape neib_step mode	function :func images2neibs <theano tensor nnet neighbours images2neibs>
tensor.nnet		neibs2images	neibs neib_shape original_shape mode	function :func neibs2images <theano sandbox neighbours neibs2images>
tensor.nnet		conv2d	input filters image_shape filter_shape	deprecated old conv2d interface
tensor.nnet	ConvOp	getOutputShape	inshp kshp stride mode	computes the output dimensions of convolving an image of shape "inshp" with kernels of shape "kshp"
tensor.nnet	ConvOp	flops	inputs outputs	useful with the hack in profiling to print the mflops
tensor.nnet	ConvOp	perform	node inp out	by default if len img2d shape ==3 we todo
tensor.nnet	ConvOp	use_blas		return true if we will generate code that use gemm
tensor.nnet		gen_conv_code_unroll_batch_kern	d unroll_bsize unroll_ksize	c_code for convop that unroll the batch size loop
tensor.nnet		batch_normalization	inputs gamma beta mean	this function will build the symbolic graph for applying batch normalization to a set of activations
tensor.nnet		batch_normalization_train	inputs gamma beta axes	performs batch normalization of the given inputs using the mean and variance of the inputs
tensor.nnet		batch_normalization_test	inputs gamma beta mean	performs batch normalization of the given inputs using the given mean and variance
tensor.signal		max_pool_2d_same_size	input patch_size	takes as input a 4-d tensor it sets all non maximum values
tensor.signal		pool_2d	input ws ignore_border stride	downscale the input by a specified factor takes as input a n-d tensor where n >= 2
tensor.signal		pool_3d	input ws ignore_border stride	downscale the input by a specified factor takes as input a n-d tensor where n >= 3
tensor.signal	Pool	out_shape	imgshape ws ignore_border stride	return the shape of the output from this op for input of given shape and flags
tensor.signal	PoolGrad	out_shape	imgshape ws ignore_border stride	return the shape of the output from this op for input of given shape and flags
tensor.signal		conv2d	input filters image_shape filter_shape	signal conv conv2d performs a basic 2d convolution of the input with the
sparse		as_sparse_variable	x name	wrapper around sparsevariable constructor to construct a variable with a sparse matrix with the same dtype and
sparse		as_sparse_or_tensor_variable	x name	same as as_sparse_variable but if we can't make a sparse variable we try to make a tensor variable
sparse		sp_ones_like	x	construct a sparse matrix of ones with the same sparsity pattern
sparse		sp_zeros_like	x	construct a sparse matrix of zeros
sparse		csm_data	csm	return the data field of the sparse variable
sparse		csm_indices	csm	return the indices field of the sparse variable
sparse		csm_indptr	csm	return the indptr field of the sparse variable
sparse		csm_shape	csm	return the shape field of the sparse variable
sparse		cast	variable dtype	cast sparse variable to the desired dtype
sparse		col_scale	x s	scale each columns of a sparse matrix by the corresponding element of a dense vector
sparse		row_scale	x s	scale each row of a sparse matrix by the corresponding element of a dense vector
sparse		sp_sum	x axis sparse_grad	calculate the sum of a sparse matrix along the specified axis
sparse		clean	x	remove explicit zeros from a sparse matrix and re-sort indices
sparse		add	x y	add two matrices at least one of which is sparse
sparse		sub	x y	subtract two matrices at least one of which is sparse
sparse		mul	x y	multiply elementwise two matrices at least one of which is sparse
sparse		hstack	blocks format dtype	stack sparse matrices horizontally column wise
sparse		vstack	blocks format dtype	stack sparse matrices vertically row wise
sparse		structured_sigmoid	x	structured elemwise sigmoid
sparse		structured_exp	x	structured elemwise exponential
sparse		structured_log	x	structured elemwise logarithm
sparse		structured_pow	x y	structured elemwise power of sparse matrix x by scalar y
sparse		structured_minimum	x y	structured elemwise minimum of sparse matrix x by scalar y
sparse		structured_maximum	x y	structured elemwise maximum of sparse matrix x by scalar y
sparse		structured_add	x	structured addition of sparse matrix x and scalar y
sparse		sin	x	elemwise sinus of x
sparse		tan	x	elemwise tan of x
sparse		arcsin	x	elemwise arcsinus of x
sparse		arctan	x	elemwise arctan of x
sparse		sinh	x	elemwise sinh of x
sparse		arcsinh	x	elemwise arcsinh of x
sparse		tanh	x	elemwise tanh of x
sparse		arctanh	x	elemwise arctanh of x
sparse		rint	x	elemwise round half to even of x
sparse		sgn	x	elemwise signe of x
sparse		ceil	x	elemwise ceiling of x
sparse		floor	x	elemwise floor of x
sparse		log1p	x	elemwise log(1 + x)
sparse		expm1	x	elemwise e^x - 1
sparse		deg2rad	x	elemwise degree to radian
sparse		rad2deg	x	elemwise radian to degree
sparse		sqr	x	elemwise x * x
sparse		sqrt	x	elemwise square root of x
sparse		conj	x	elemwise complex conjugate of x
sparse		true_dot	x y grad_preserves_dense	operation for efficiently calculating the dot product when one or all operands are sparse
sparse		structured_dot	x y	structured dot is like dot except that only the gradient wrt non-zero elements of the sparse matrix
sparse		dot	x y	operation for efficiently calculating the dot product when one or all operands is sparse
sparse		local_csm_properties_csm	node	if we find csm_properties(csm(*args)), then we can replace that with the *args directly
sparse		local_inplace_remove0	node	optimization to insert inplace versions of remove0
sparse		local_inplace_addsd_ccode	node	optimization to insert inplace versions of addsd
sparse		local_addsd_ccode	node	convert addsd to faster addsd_ccode
sparse	StructuredDotCSR	c_code	node name inputs outputs	c-implementation of the dot product of the sparse matrix a and matrix b
sparse		local_usmm_csx	node	usmm -> usmm_csc_dense
sparse		local_csm_grad_c	node	csm_grad none -> csm_grad_c
sparse		sparse_constructor	value name strict allow_downcast	sharedvariable constructor for sparsetype
sparse.sandbox	ConvolutionIndices	evaluate	inshp kshp strides nkern	build a sparse matrix which can be used for performing
sparse.sandbox		convolve	kerns kshp nkern images	convolution implementation by sparse matrix multiplication
sparse.sandbox		max_pool	images imgshp maxpoolshp	implements a max pooling layer takes as input a 2d tensor of shape batch_size x img_size and
d3viz	PyDotFormatter	__init__	compact	construct pydotformatter object
d3viz	PyDotFormatter	__add_node	node	add new node to node list and return unique id
d3viz	PyDotFormatter	__node_id	node	return unique node id
d3viz	PyDotFormatter	__call__	fct graph	create pydot graph from function
d3viz		var_label	var precision	return label of variable node
d3viz		var_tag	var	parse tag attribute of variable node
d3viz		apply_label	node	return label of apply node
d3viz		apply_profile	node profile	return apply profiling informaton
d3viz		broadcastable_to_str	b	return string representation of broadcastable
d3viz		dtype_to_char	dtype	return character that represents data type
d3viz		type_to_str	t	return str of variable type
d3viz		dict_to_pdnode	d	create pydot node from dict
d3viz		replace_patterns	x replace	replace replace in string x
d3viz		safe_json	obj	encode obj to json so that it can be embedded safely inside html
d3viz		d3viz	fct outfile copy_deps	create html file with dynamic visualizing of a theano function graph
d3viz		d3write	fct path	convert theano graph to pydot graph and write to dot file
sandbox	FFT	make_node	frames n axis	compute an n-point fft of frames along given axis
sandbox		dct_matrix	rows cols unitary	return a rows x cols matrix implementing a discrete cosine transform
sandbox		multMatVect	v A m1 B	multiply the first half of v by a with a modulo of m1 and the second half by b with a modulo of m2
sandbox		guess_n_streams	size warn	return a guess at a good number of streams
sandbox	MRG_RandomStreams	seed	seed	re-initialize each random stream
sandbox	MRG_RandomStreams	inc_rstate		update self rstate to be skipped 2^134 steps forward to the next stream
sandbox	MRG_RandomStreams	get_substream_rstates	n_streams dtype inc_rstate	initialize a matrix in which each row is a mrg stream state and they are spaced by 2**72 samples
sandbox	MRG_RandomStreams	uniform	size low high ndim	sample a tensor of given size whose element from a uniform distribution between low and high
sandbox	MRG_RandomStreams	multinomial	size n pvals ndim	sample n (n needs to be >= 1 default 1) times from a multinomial distribution defined by probabilities pvals
sandbox	MRG_RandomStreams	choice	size a replace p	sample size times from a multinomial distribution defined by probabilities p, and returns the indices of the sampled elements
sandbox.linalg		psd	v	apply a hint that the variable v is positive semi-definite i e
sandbox.linalg		tag_solve_triangular	node	if a general solve() is applied to the output of a cholesky op then replace it with a triangular solve
sandbox.linalg		local_det_chol	node	if we have det x and there is already an l=cholesky x floating around then we can use prod(diag l to get the determinant
sandbox.linalg		spectral_radius_bound	X log2_exponent	returns upper bound on the largest eigenvalue of square symmetrix matrix x
typed_list	Index	perform	node inputs outputs	inelegant workaround for valueerror the truth value of an array with more than one element is ambiguous
typed_list	Count	perform	node inputs outputs	inelegant workaround for valueerror the truth value of an array with more than one element is ambiguous
typed_list	TypedListType	__eq__	other	two lists are equal if they contain the same type
typed_list	TypedListType	get_depth		utilitary function to get the 0 based level of the list
