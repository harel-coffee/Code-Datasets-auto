helper function for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x i0 i1	0.083333
addition	add	0.068966
to the type's	gpuarray gpu array	0.062500
that will be instantiated	gof clinker type	0.066667
class returns the bartlett spectral window in	bartlett	0.058824
true for small or builtin c	c is simple	0.200000
output dimensions of convolving an image	tensor nnet conv op get output	0.047619
fusion	fusion	1.000000
compiledir	compiledir	1.000000
symbolic integer scalar for the shape element s_i	tensor shape feature unpack s_i var	1.000000
removes	remove	0.035714
an input vector and	node input_storage	0.038462
reduces scan memory consumption	scan save mem	0.200000
the stopping condition returned by	ls	0.090909
a -1 and converts this to expm1	local expm1 node	0.066667
to get the 0 based level of the	get depth	0.050000
disabled by default that removes	local remove	0.166667
this return the initial value for myresult	gpuarray gpu careduce cuda assign init first_item	0.166667
offers to	to os	0.038462
navigator deal with	gof navigator	0.038462
the other implementation of mod	scalar mod c	0.125000
res	res	1.000000
structured addition of	structured add	0.142857
used to determine the	adv index broadcastable	0.050000
name to write	gpuarray write	0.200000
function from comparators	fn	0.083333
is	destroy	0.009709
computes the svd of a matrix	svd	0.034483
reorder the dimensions of this variable	tensor py operators	0.015625
true if and only if this enum has	gof enum type has	0.111111
of this type	pure type	0.142857
conv algo_bwd	algo bwd algo	1.000000
to the end variables	grad wrt end	0.050000
op whose perform() raises an exception	raise	0.076923
tell shape_i how to generate c code for	c code typ code check_input version	1.000000
exception some perform() or c_code() created	map	0.047619
variant on wraplinker that runs	gof wrap linker	0.083333
function	compile function	0.250000
operators to the basic constant class	constant	0.016667
bound on the	bound	0.043478
a symbolic row variable (ndim=2 broadcastable=[true false])	row	0.034483
required to compute the given variables	variable_list blockers	0.166667
this variable optionally inserting	tensor	0.006431
detect	gcc	0.023810
context object mapped to	type context	0.090909
incsubtensor when we overwrite the full inputs with	local useless inc subtensor node	0.066667
along a given axis	axis	0.025641
a comparator to represent the dependence of nodes	make dependence cmp	0.111111
return c code to declare variables that will	gof clinker type c declare name	0.333333
of mod	mod c code node	0.125000
|a|	abs	0.066667
a n-d	ws ignore_border stride	0.090909
compilation of cutils_ext	gof compile cutils	0.166667
output dimensions	conv op get output	0.047619
c-implementation of the dot product	dot csr c code	1.000000
c code to declare variables	type c declare	1.000000
should remove any dynamically added functionality	replace validate on detach fgraph	1.000000
into macros	cop	0.028571
gpu version of	gpu	0.011765
nodes in the original graph	outputs copy_inputs_and_orphans memo	0.029412
fill inputted tensor with the assigned value	tensor tensor py operators fill value	1.000000
not attempting to use dnn	core safe no dnn	0.125000
the dot product	true dot x	0.166667
help the navigator	navigator optimizer	0.037037
derivative of log gamma function	tensor psi a	1.000000
zeros_like but forces the object to	core float zeros	0.166667
the following	global	0.142857
see theano tensor argmax	tensor tensor py operators argmax axis keepdims	1.000000
triangular solve	linalg tag solve triangular node	0.142857
clone the	clone	0.041667
post some text to a	post	0.100000
some perform() or c_code() created a	view map	0.142857
convert addsd to faster addsd_ccode	local addsd	0.250000
to wrt, computes gradients of	subgraph grad wrt	0.062500
that operates on	gof linker make	0.250000
that are views of v given that v	v	0.011111
if fgraph is the first functiongraph	fgraph no_recycling	0.200000
input a 4-d tensor it sets all	pool 2d same size input patch_size	0.166667
n_classes	n_classes	1.000000
in the theano enumeration types wrapped	type enum from	0.333333
computes the output dimensions of convolving an	output	0.017241
of x default reverse them	transpose x axes	0.200000
transferred	transfer	0.058824
on cpu here	local pow specialize	0.250000
localoptgroup for	to gpulocal opt group	0.055556
of shape tuple or	tensor shape feature default infer shape	0.066667
shorter version of platform	platform	0.083333
the specified pieces of vectors and matrices	sparse block outer make node o x y	0.066667
arguments	args	0.051282
required return the c implementation of	c code node name inputs outputs	0.250000
matrix solve operation c	tensor solve	0.038462
all sigmoid to ultra_fast_sigmoid	ultra fast sigmoid	0.200000
module from the cache	module cache module from	0.333333
to get	get	0.020833
the orphans among them	orphans	0.090909
dict op -> total number of	compile profile stats op	0.166667
none or existing output storage see below	output_storage	0.166667
returns the signature for this function	ext function method decl	0.333333
3-d variable	tensor3 name	0.500000
of arguments to pass to helper_c_code	helper c code args	0.250000
replace it with logsoftmax x 's grad	tensor nnet local logsoftmax grad	0.200000
new variable instance of type self	gof pure type call	0.500000
return a symbolic row variable (ndim=2 broadcastable=[true	tensor row name	0.050000
of this variable optionally inserting	dimshuffle	0.014493
advancedsubrensor1	advanced subtensor1	0.200000
this method is primarily used	gof pure op r op inputs eval_points	0.250000
hack in profiling to print the mflops	nnet base abstract conv flops inp outp	0.125000
solve operation	solve	0.032258
creating a class with a metaclass	metaclass metaclass	0.125000
elemwise	sparse	0.057692
the platform-dependent extension for compiled modules	get lib extension	0.333333
for bilinear upsampling this	bilinear	0.038462
a graph of apply nodes	gof sort apply nodes	0.200000
vector	alloc	0.012500
a tensorvariable whose type	tensor as	0.066667
return a list of shape	infer shape	0.066667
grad and replace it with logsoftmax x 's	tensor nnet local logsoftmax	0.076923
for the minimum	minimum x	0.142857
the supplied function as its	compile as	0.050000
given an apply_node recursively search	apply_node check	0.066667
input a 4-d tensor	size input patch_size	0.166667
of suitable dummy values	provide	0.100000
the replacement if	gof replace validate replace	0.050000
walk	walk	1.000000
can be considered approximately equal	gof pure type values eq approx	1.000000
helper function	helper random_state low high size	0.500000
trace to an node or	trace	0.052632
insert deepcopy in the fgraph	insert deepcopy fgraph	0.500000
x and	linalg local	0.142857
detect if	gcc	0.023810
this op could be very	tensor prod l op	0.033333
in a new	with new	0.166667
raise	compile check inputs node	0.166667
<	tensor lt	1.000000
which each row is a mrg	sandbox mrg	0.125000
up to the end	grad wrt end	0.050000
set of ops contained	gof ops	0.083333
cusolver gpu solver op	gpu cusolver solve	1.000000
op could be very easy if it is	op	0.009174
a >= b inplace on a	tensor ge inplace a b	0.500000
unification in u and	o u	0.037037
lazy loading of moved objects in	moves urllib	0.076923
replacement if the ops in the	gof replace validate replace all	0.050000
of v given that v	tree set v	0.125000
also their apply_node if those nodes	gof	0.002381
gradient the finite fourier	tensor fourier grad	0.250000
hyperbolic tangent	tanh	0.250000
dense matrix	sd ccode	0.250000
operation to wait on a previously sent	wait	0.022727
return true if a and	a	0.008065
work for gpuincsubtensor	tensor local inplace setsubtensor	0.250000
sample from a normal distribution	streams base normal size	1.000000
gist and	gist	0.040000
number of	make	0.017857
pack c types back into	clinker type c sync	0.111111
simplify a	tensor nnet simplify	0.500000
draw random numbers using numpy's	replace p	0.500000
an array with more than one element	node inputs	0.086957
to turn softmax(sum_of_stuff) -> softmax_w_bias	tensor nnet local softmax with	0.200000
called by remove_feature feature should remove any dynamically-added	gof feature on detach function_graph	0.200000
to compute the image shape of convolution gradweights	nnet get conv gradweights shape 1axis image_shape	0.500000
logsoftmax x 's	tensor nnet local logsoftmax	0.076923
function :func neibs2images <theano sandbox neighbours neibs2images>	neibs2images neibs neib_shape original_shape mode	0.333333
dnn conv algo_bwd	dnn algo bwd algo	0.166667
with respect to wrt,	core subgraph	0.062500
an exception while annotating	op node thunk exc_info storage_map	0.250000
3-d	tensor tensor3 name	0.500000
replace_all_validate revert the replacement if the ops in	replace all validate remove fgraph	0.111111
scan return true iff	push out scan	0.050000
received array using	mpirecv	0.037037
check_input	check_input	0.555556
module	gof module	0.058824
and return full path of the dynamic lib	gof module name	0.076923
op	get op params	0.100000
the dimensions of this	tensor py operators dimshuffle	0.019231
headers that are needed by	headers	0.038462
outs	outs	0.250000
tensorvariable of this	make variable	0.166667
not try to use complex numbers	tensor mod check x y	0.166667
a graph is	destroy	0.009709
dot22	dot22	1.000000
c contiguous version of the	contiguous	0.058824
axis that was used	inputs g_outputs	0.090909
orv list1 == nv list2 ==	unify walk	0.500000
return a list of shape tuple	infer shape	0.066667
in a new graph	new inputs	0.166667
if cond	cond	0.142857
proxy for either	scalar div proxy	0.125000
is a	is	0.066667
compute sum of	tensor constant signature get sum	0.142857
the value after	param init default	0.040000
create pydot node from dict	dict to pdnode d	1.000000
by a specified factor takes as input	tensor signal pool 2d input	0.090909
:param execute if true execute	execute execute verbose	0.250000
helper function	helper random_state	0.571429
output shape	shape	0.010204
normalize	normalize	1.000000
1d kernel for bilinear	bilinear	0.019231
since the epoch of the last access	last access	0.040000
string specific to the apply	apply node	0.031250
data	w dtype	1.000000
a convolution with	dnn conv	0.090909
for any python object a that	gof pure	0.033333
string	d3viz replace patterns	1.000000
the [elementwise] largest of	tensor largest	0.333333
symbolic variables in inputs to	inputs	0.012658
dot22 computing an outer-product ->	tensor local dot22 to	1.000000
variables in inputs that	gof	0.002381
lock to be held	module cache add to cache module key module_hash	0.166667
function that allows replacing subgraphs	compile rebuild collect shared outputs inputs replace updates	0.500000
min for the minimum in one tensor	tensor minimum x y	0.090909
around c_extract that	gof	0.002381
to upsample	ratio normalize	0.200000
of this mode	compile mode	0.166667
o	o	0.384615
generate c	shape i c	0.250000
pydot graph and	d3viz d3write fct path	0.166667
idx list to get the right	tensor get idx list	0.076923
wraplinker that runs a	gof	0.002381
and destructive operations	destroy	0.009709
merge-based implementation of theano gof graph is_same_graph	is same graph with merge var1 var2 givens	0.166667
remove subtensor/advancedsubtensor1 if it takes	local useless subtensor node	0.200000
listeners to help the navigator deal with the	navigator optimizer attach	0.038462
array and a set of arrays to choose	tensor choose a choices out mode	0.200000
data structures	non seq	0.111111
navigator deal with	navigator optimizer	0.037037
see theano tensor std	py operators std	1.000000
the "reverse-mode" gradient for the eigensystem	eigh grad perform node inputs outputs	0.333333
of a real-valued input on the gpu	curfft inp norm	0.066667
the broadcast pattern	pattern a idx	0.066667
structured addition	structured add	0.142857
return a symbolic row variable (ndim=2 broadcastable=[true	row name	0.050000
optimization makes the folowing changes in the graph	tensor local mul switch sink node	0.045455
to make	to os environ pathlist	0.038462
it with logsoftmax x 's grad	tensor nnet local logsoftmax grad node	0.200000
c_code for convop that unroll the batch	conv code unroll batch kern d unroll_bsize unroll_ksize	0.125000
the stack trace	stack trace	0.055556
the bartlett spectral window in the	tensor bartlett	0.083333
fetch a compiled module from the loaded cache	module cache get module name	0.166667
an array with more than one element	node inputs outputs	0.125000
the output dimensions of convolving an image	conv op get output	0.047619
the axis that was used to split x	split grad inputs g_outputs	0.333333
return dict d s t d[node] is a	function graph orderings	0.200000
idx_list with constant	subtensor get constant idx	0.250000
numpy-compatibility method if x is a matrix return	tensor diag x	0.200000
and "init_code"	struct node name	0.500000
which computes the specified outputs inplace	inplace fgraph	0.142857
help the navigator	gof navigator optimizer attach	0.038462
data structures	out non seq scan	0.125000
register a transfer function for alternative	tensor register transfer fn	0.250000
for gpucorrmm	gpu corr mm	0.428571
more multinomial distributions defined by one-dimensional	tensor multinomial	0.037037
and/or from existing start	start	0.040000
return true if a and b	a b	0.066667
the variable v is positive	v	0.011111
task that is associated to it	gof cthunk find task failure_code	0.083333
wrt,	core subgraph	0.062500
computes the	ishape kshape	0.250000
inner graph of scan to outside of scan	scan	0.017241
sum of	sum	0.038462
the folowing changes in the graph	local mul switch sink	0.045455
hack in profiling to print the mflops	op flops inputs outputs	0.125000
listeners to help the navigator	navigator optimizer attach	0.038462
to raise in self	core raise init	0.100000
its substitute take different runtime values	bad optimization	0.333333
the same type	type	0.011905
the convolution gradient with respect to	dnn conv grad i	0.125000
a config string	parse config string	0.333333
is not attempting to use dnn conv workmem	safe no dnn workmem workmem	0.166667
decrefs	cleanup	0.125000
compiles the source code for this	compile cmodule location	0.038462
a search through consecutive view_map()s	gof view roots r	0.200000
c code when doing constant folding of	python constant folding	0.142857
a >= b inplace on a	ge inplace a b	0.500000
feature should remove any	feature on	0.200000
important note	process node fgraph node	0.142857
in pvals	pvals	0.071429
use within the op	get op	0.100000
inplace on	inplace	0.282051
load a	load	0.083333
functiongraph listeners to help the navigator	gof navigator optimizer attach updater	0.038462
a	a	0.612903
the type's	gpuarray gpu array type	0.062500
or more multinomial distributions defined by one-dimensional slices	multinomial random_state	0.040000
otypes	otypes	1.000000
scale	scale	0.380952
recognize the updates ordereddict the list of	scan_module get updates	0.034483
output dimensions of convolving an	output	0.017241
particular	getitem item	0.125000
this	tensor py operators	0.015625
in the view_map	bad	0.013158
elementwise [true] division inverse of	true div a b	0.333333
variable type	type	0.011905
perform	perform node x y inverse	0.166667
where function is a thunk	make thunk	0.125000
tries	top_shape border_mode	0.166667
the first half of v by	v	0.011111
formats the outputs	outputs	0.045455
to a tensor of type dtype	dtype	0.022727
inputs according to the idx list to get	tensor get idx list inputs	1.000000
of aliasing and	destroy	0.009709
some perform() or c_code() modified an	map	0.047619
join for gpu	gpu join	1.000000
by mapping it to	ctx	0.125000
the dimensions of this variable optionally inserting	py operators dimshuffle	0.019231
compiled individual ops in the "theano config compiledir"	compiledir content	0.166667
transfer to a tensortype	tensor tensor py operators transfer	0.125000
x and there is already an l=cholesky x	local	0.014085
this op could be	op	0.009174
op}(join(axis=0 a b), axis=0) -> elemwise{scalar op}	tensor local reduce join node	0.111111
c-implementation of the dot product	dot csr c code node	1.000000
copy of the type	tensor type	0.034483
-x pattern	nnet is neg	0.166667
compiled module from the loaded cache or the	gof module cache get module	0.166667
a !=	tensor neq a	1.000000
of the graph's apply nodes such that	gof function graph	0.031250
that gets a scan op a list of	op not_required inputs	0.071429
offers to make	to	0.017544
if those nodes are	gof	0.002381
function uses set and dictionary data structures	out non seq	0.125000
to print the mflops	corr3d mm flops inp outp	0.125000
sample from a uniform distribution	uniform random_state size	0.125000
connection pattern	from graph connection pattern node	0.076923
required return the	name	0.011111
helper function for diagonalsubtensor and	tensor nnet get diagonal subtensor view x i0	0.083333
wrapper to make an inplace	gpuarray inplace	0.200000
exception object with	raise with	0.333333
the tensor operators to the	tensor	0.006431
of localoptgroup for graphtogpu	group	0.047619
inner-most loop executes	tensor make reordered loop	0.111111
baddestroymap if	check inputs	0.125000
revert the replacement if the ops	replace all	0.050000
if l	l	0.111111
convolving a mini-batch of a stack of 2d	input_shape filter_shape	0.018519
the fgraph outputs that will replace their	fgraph	0.012195
and "code_cleanup"	cleanup node	0.142857
copy this function copied function will	compile function copy	0.333333
from the loaded cache or the disk	module cache get	0.250000
required to compute the given	variable_list blockers	0.166667
function for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor	0.083333
that allows replacing subgraphs	compile rebuild collect shared outputs inputs replace	0.500000
copies a vector to the diagonal	alloc diag	0.027027
to determine the broadcast pattern for advancedsubtensor output	tensor adv index broadcastable pattern a	0.066667
short	short	1.000000
replace a leaf	replace leaf	0.100000
the folowing changes in the	tensor local mul switch sink	0.045455
function on the inputs and	node inputs	0.043478
duplicate this apply instance in a new	gof apply clone with new inputs	0.250000
return a c contiguous version	contiguous	0.058824
return the idx_list with constant	constant idx	0.250000
is no change in the	node	0.014815
var shape[i], but apply if possible	i var i fgraph	0.200000
to the fgraph outputs that	fgraph	0.012195
2d or 3d convolution for	base abstract conv conv	0.125000
by	clinker	0.300000
variables [v1 v2 v3 ]	gof vm linker compute gc dependencies variables	0.250000
it work for elemwise and gpuelemwise	local elemwise fusion	0.166667
fgraph	fgraph no_recycling profile	0.200000
nodes to output nodes	gof	0.002381
op __init__ fct don't have the same parameter	cast make new inplace output_types_preference name	0.142857
pyobject * instance	sub check_input	0.500000
filter_shape	filter_shape	1.000000
transfer to a	tensor tensor py operators transfer	0.125000
sum	sum axis dtype	1.000000
along the specified axis	axis sparse_grad	0.333333
a symbolic integer scalar for the shape element	tensor shape feature unpack	0.500000
epoch of the last access of a given	last access	0.040000
folowing changes in the graph	local mul switch sink node	0.045455
special compound l{op} for the output of	softmax argmax1hot	0.083333
change the value after the import of	config param init default filter	0.040000
apply to	apply node	0.031250
of the form [0 0	crossentropy categorical1hot	0.166667
input a 4-d tensor it sets all	size input patch_size	0.166667
the connection pattern of a	gof io connection pattern	0.055556
minimum in one tensor	tensor minimum x y	0.090909
[true]	true	0.166667
see theano tensor prod	tensor py operators prod axis	1.000000
dimshuffle which only adds dimension	dimshuffle	0.014493
can be referred to	compile register linker	0.250000
if it takes the full	node	0.007407
lower triangle of an array	tril m k	0.250000
to use dnn	safe no dnn	0.125000
the cache of dynamically compiled modules on disk	module cache	0.071429
names to an	names	0.047619
evaluates	eval inputs_to_values	1.000000
the mflops	gpuarray base gpu corr3d mm flops inp outp	0.125000
row correspond to the one	one	0.076923
the fgraph outputs that	fgraph expanded_inputs	0.058824
to type2 from type1 constitutes an upcast	tensor is an upcast type1 type2	0.333333
in the destroy_map	destroy	0.009709
return a module	module	0.033333
compile lock to be held	module cache add to cache module key module_hash	0.166667
existence of the __unify_walk__ method for	gof unify walk a b u	0.037037
nodes in the original	inputs outputs copy_inputs_and_orphans memo	0.029412
for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x i0	0.083333
the constant scalar 0-d value underlying variable	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
the specified pieces of vectors	sparse block outer make	0.066667
this	tensor tensor py operators dimshuffle	0.019231
a slice [start stop step] transform	slice	0.038462
out_idxs and	out_idxs	0.050000
output	dims output input leftdims	0.333333
from	remove	0.035714
reproducible case for	compile function dump filename inputs outputs	0.166667
see theano tensor std	py operators std axis ddof keepdims corrected	1.000000
instance of type self	gof pure type make	1.000000
returns function to	gof	0.002381
_maker which handles much of the	compile debug mode function maker i o m	0.066667
list of variables within input	gof variables	0.125000
is basically a call to tensor get_scalar_constant_value	tensor extract constant x elemwise only_process_constants	0.058824
_maker which handles much of the debugging	mode function maker i o m	0.066667
a value	value ptr	0.500000
structured dot csr is like dot except	structured dot csr	0.500000
each row correspond to the one	one	0.076923
sort	sort	0.857143
overwrite the	useless inc subtensor	0.125000
variable out for	out	0.018519
bilinear upsampling this	bilinear	0.038462
returning the output	misc output	0.066667
outputs and the stopping condition returned by	and outputs ls	0.166667
a tensorvariable	tensor as	0.066667
out	out	0.111111
comparator to represent	cmp	0.058824
chooses the orphans among them	and orphans	0.166667
scalar constants and the rest	rest inputs elemwise only_process_constants	0.125000
a new graph	new	0.058824
code to the task that is associated to	gof cthunk find task failure_code	0.083333
the dimensions of this	tensor py	0.015873
data structures	scan_module push out non seq	0.125000
fill s v -> alloc(v shape	local fill to	0.250000
the stack trace from one or	copy stack trace	0.055556
in the forward but clip	clip x lower_bound upper_bound	0.090909
computations	computations	0.750000
returning the output error and exit code in	misc output subprocess popen command	0.100000
lib directories that are needed by one or	lib dirs	0.045455
the object should be saved under	misc persistent ndarray id resolve	0.333333
the equivalent of localoptgroup for graphtogpu	opt group	0.043478
of the __unify_walk__ method	unify walk a b u	0.037037
given an apply_node recursively search from this	import apply_node check	0.066667
to wait on a previously sent array using	mpisend wait	0.045455
convolution	border_mode subsample	0.500000
gradient [1]_ for the cholesky factorization	cholesky grad	1.000000
range of values maximum - minimum	tensor ptp a	0.333333
dimensions of this variable optionally inserting broadcasted dimensions	tensor tensor	0.014286
when one or all operands	x y	0.024390
c_extract_out	c extract out	1.000000
raise	compile check	0.166667
create	compile maker create defaults trustme storage_map	1.000000
type's :attr	gpuarray	0.023256
for diagonalsubtensor	nnet get diagonal subtensor view x i0 i1	0.083333
gradients up to the end variables	wrt end	0.050000
info	info	1.000000
warning message on the first	deprecated filename msg	0.041667
apply nodes according to a list	gof sort apply nodes inputs outputs cmps	0.050000
a new variable instance of type self	pure type call	0.500000
elemwise square root of x	sparse sqrt x	1.000000
replaced	allow_partial only_process_constants elemwise	0.166667
if a dimshuffle is inside an alloc and	local alloc dimshuffle node	0.166667
implementation of theano gof graph is_same_graph	is same graph with merge var1 var2 givens	0.166667
|	or	0.125000
with the *args directly	csm properties csm	0.142857
python object a that would be	gof pure	0.033333
op}(join(axis=0 a b), axis=0) -> elemwise{scalar op} a	tensor local reduce join node	0.111111
pattern has functioning c code	gpu careduce cuda supports c code	0.250000
listeners to help the navigator deal with the	navigator optimizer attach updater fgraph	0.038462
the c code for corr3dmm	tensor nnet base corr3d mm c code	0.090909
if	check inputs node storage_map	0.166667
use within the op code	get op	0.100000
lib directories that are needed by one	header dirs	0.045455
replace_all_validate revert the replacement if the ops in	validate replace all validate remove fgraph	0.111111
instance associated with a particular stream	tensor random streams setitem item val	0.142857
llvm one or not	llvm	0.100000
this variable optionally	operators	0.017241
of _maker which handles much of	function maker i o m	0.066667
reverse-mode gradient	grad inputs output_gradients	0.250000
sum or average over different patches	pool	0.066667
see theano tensor std	py operators std axis ddof	1.000000
the shape_of dictionary	feature init r r	0.333333
shape and	pool grad out shape	1.000000
ones with the same	ones like	0.333333
is not attempting to use dnn	core safe no dnn	0.125000
feature should remove any dynamically-added	feature on	0.200000
x*x -> sqr x this is faster on	tensor local mul to sqr node	0.166667
tuple of integers indicating the version	c code cache version apply	0.125000
of 3d inputs with a set of 3d	conv3d	0.076923
graph of scan to outside of scan	out scan output	0.125000
remove are still in the graph	replacements remove reason	0.055556
the dimensions of this variable optionally inserting broadcasted	tensor tensor py operators	0.015625
_maker which handles much of the	mode function maker i o m	0.066667
division	tensor int div	0.250000
as replace_all_validate revert the replacement	gof replace validate replace all validate remove	0.111111
if implemented	node	0.007407
-1 and converts this to expm1 a	expm1 node	0.066667
theano	tensor tensor py	0.174603
diagonalsubtensor and	get diagonal subtensor view x i0	0.083333
optimization to insert inplace versions of remove0	sparse local inplace remove0	0.333333
this	gpulocal	0.055556
the topooptimizer from the input nodes to	in2out	0.043478
variable (ndim=2 broadcastable=[false true])	tensor col name dtype	0.200000
hash equal for same kinds	type hash	0.166667
on all device we do it	device node	0.045455
to pack c types back into a pyobject	clinker type c sync	0.111111
the gradient	grad inputs g_outputs	0.076923
a variable representing a computation on a certain	array variable	0.500000
code	code	0.650000
round_half_to_even_inplace a (inplace on a)	round half to even inplace a	1.000000
then replace it with a triangular solve	solve triangular node	0.142857
for elemwise and gpuelemwise op	local elemwise fusion op op	0.200000
the diagonal	alloc diag	0.027027
inner graph to	inner graph	0.035714
infer the number of dimensions	infer	0.083333
into a basic theano op that will call	op itypes otypes infer_shape	0.047619
a set of arrays to choose from	choose a choices out mode	0.200000
we can replace that with the *args directly	csm properties csm	0.142857
help the navigator deal with the	gof navigator optimizer	0.038462
a dimshuffle which only adds dimension	tensor local dimshuffle	0.052632
useless dimshuffle	local useless dimshuffle	0.500000
the context object mapped to the type's	context	0.035714
without replacement from a	choice from	0.333333
nodes that must be evaluated	gof	0.002381
we can't change the value after the	init default filter	0.040000
a list to	list	0.066667
computes the svd of a matrix :math	svd	0.034483
replace_all_validate revert the replacement if the ops	validate replace all validate	0.111111
graph to ensure that it is coherent	graph	0.016393
replace_all_validate revert the replacement if	gof replace validate replace all validate remove fgraph	0.111111
is in t float_scalar_types	scalar res dtype	0.333333
set the values of a shared variable to	compile shared variable	0.083333
convenience function to concatenate tensortypes	tensor join	0.250000
performs batch	batch	0.111111
the inputs required to compute	gof inputs variable_list blockers	0.058824
of v given that v	v	0.011111
feature should remove	feature on detach	0.200000
input that	bad destroy	0.034483
deepcopyop how	deep copy op	0.250000
diagonal set to	diagonal	0.111111
will attempt to convert x into a	x context_name	0.100000
x with	x	0.008772
to make itself the default	to os	0.038462
localoptimizer and applies	local opt group	0.052632
specified axis	x axis sparse_grad	0.333333
function for diagonalsubtensor and	nnet get diagonal subtensor view	0.083333
convert python litterals to	tensor make	0.076923
that allows replacing subgraphs	clone output replace strict share_inputs	0.071429
makes a value from an integer	cdata type get func	1.000000
shape in the shape_of dictionary	tensor shape feature init r r	0.333333
list of nodes that	gof	0.002381
s to previously un-shaped variable r	r s override	1.000000
modulo (inplace on a)	mod inplace a	1.000000
kernel shape of convolution gradweights	get conv gradweights shape image_shape	0.500000
detect	tensor detect	0.166667
the image shape of convolution gradinputs	conv gradinputs shape kernel_shape	0.500000
shape tuple or	default infer shape	0.066667
functiongraph	gof function graph	0.031250
shp -> alloc(unary x shp)	local alloc unary	0.250000
associate specific code to each level of nesting	loop_orders dtypes loop_tasks sub	0.125000
op for input of given shape and flags	signal pool out shape imgshape ws ignore_border stride	0.200000
equivalent	gpulocal opt	0.055556
and	node name	0.066667
numpy randomstate instance associated with a particular stream	tensor random streams setitem item val	0.142857
of the last access	last access time	0.040000
replacement if the ops	replace all	0.050000
cast	cast	0.833333
specs and the output specs	fgraph input_specs output_specs accept_inplace	0.142857
min for the minimum in	tensor minimum	0.142857
that will be	gof clinker	0.033333
c-implementation of	csr c code	0.333333
erfinv	erfinv	0.833333
specified pieces of vectors and	sparse block gemv make node o w h	0.066667
python type c type numpy typenum that corresponds	tensor tensor type dtype specs	0.071429
an exception class to raise in	core raise	0.100000
unification in u and uses it instead of	o u	0.037037
self _grad_op from user supplied form to type	from graph recompute grad op	0.200000
important note this	seq scan process node fgraph node	0.142857
not support the types involved	tensor inc subtensor do type checking	0.142857
exception some perform() or c_code() created	bad view map	0.142857
operation on f wrt to wrt	rop f wrt	0.200000
the last access of a given file	last access time path	0.040000
x with the	x	0.008772
function name to write data	gpuarray write w dtype	0.200000
standard deviation std	std ndim	1.000000
navigator deal with the	navigator optimizer attach	0.038462
helper function to generate permutations from integers	tensor permutation helper random_state	0.333333
computes the	ishape kshape border_mode	0.250000
base class for operations that need to compile	gpu kernel base	0.333333
graph and get	graph	0.016393
return the idx_list with constant	subtensor get constant idx	0.250000
or a tensorvariable whose	as	0.024390
shape	default infer shape	0.066667
method to override this should return an	node name	0.033333
hash	hash	0.444444
spatio-temporal filters	signals filters	0.111111
of convolution	conv	0.148148
of suitable dummy values	local meta optimizer provide inputs	0.200000
wasn't	view	0.022727
the optimization to the provided l{functiongraph} it	optimizer apply	0.166667
the last access of a given	last access time path	0.040000
a hash from	tensor hash from	0.333333
it with a triangular solve	linalg tag solve triangular node	0.142857
return a version of var transferred to	tensor transfer var	0.100000
is unified to boundvariable(other_object)	walk fv o u	0.200000
shape	infer shape	0.066667
a b), axis=0) -> elemwise{scalar op} a b	local reduce join node	0.111111
input vector and t is a	node	0.007407
the idx list to get the right	get idx list	0.076923
scalar 0-dimensional	hessian	0.142857
deprecated old conv2d	tensor nnet conv2d input filters image_shape filter_shape	0.500000
useless reshape	useless reshape	0.200000
upcasts constant inputs	constant inputs node	0.125000
replaced by their python scalar equivalent	allow_partial only_process_constants elemwise	0.166667
hash from	hash from	0.333333
of 3d	tensor nnet conv3d	0.071429
mod	mod c code node	0.125000
dimensions	tensor	0.012862
unified to boundvariable(other_object)	gof unify walk fv o u	0.200000
fill s v -> alloc(v shape s this	tensor local fill to	0.250000
inserting broadcasted dimensions	tensor py operators	0.015625
the symbolic graph for convolving a mini-batch	input_shape filter_shape	0.027778
"reverse-mode" gradient for the	grad perform	0.083333
can't change the value after the	core config param init default	0.040000
does not support the types involved	inc subtensor do type checking	0.142857
generate a diff to	diff	0.071429
new instance of this mode	compile monitor mode clone link_kwargs optimizer	0.333333
a specified factor takes as	signal pool	0.142857
the type	tensor tensor type	0.041667
eye for	eye	1.000000
c code for corrmm	corr mm c code	0.090909
lib directories that are	clinker header dirs	0.055556
a vector to the diagonal	alloc diag	0.027027
ctx	ctx	0.625000
into a canonical	canonical	0.076923
sigmoid to ultra_fast_sigmoid	nnet local ultra fast sigmoid node	0.200000
to pack c types back into	gof clinker type c sync	0.111111
kernel for bilinear upsampling this	bilinear	0.038462
numpy random	mrg random	0.333333
in the list remove are still in	remove fgraph replacements remove reason	0.055556
functiongraph listeners to help the navigator	gof navigator optimizer attach updater fgraph	0.038462
compiles the source code for this linker	clinker compile cmodule location	0.038462
revert the replacement if the ops in the	replace	0.032258
for small or builtin c types	clinker type c is simple	0.250000
corr	corr	0.833333
self _grad_op from user supplied form to type	compile op from graph recompute grad	0.200000
the inner-most loop	make reordered loop	0.111111
if l has any duplicates (according	has duplicates l	0.111111
to use with	gpuarray	0.023256
the destroyhandler class detects when a	destroy handler	0.055556
apply instance in a	apply clone	0.333333
optimized for calculating the dot product	dot csr	0.111111
wasn't in	bad view	0.027027
to get the 0 based level	type get depth	0.050000
the ops in the list remove	remove	0.035714
of apply nodes according to a	sort apply nodes inputs outputs cmps	0.050000
that gets a scan op a list	op not_required	0.071429
choose from	choose	0.111111
gradient for	grad	0.010417
to make itself the default	to os environ pathlist var	0.038462
exception some perform() or c_code() created a memory	bad view map	0.142857
function for diagonalsubtensor and	nnet get diagonal subtensor view x	0.083333
and figure out which nodes it	scan_module traverse out x x_copy d	0.047619
the same rounding	round half	0.100000
vector	vector name	0.500000
raise baddestroymap	node storage_map r_vals	0.166667
0 based level of the list	list	0.066667
and replace it with logsoftmax x 's grad	nnet local logsoftmax grad	0.200000
wasn't in the destroy_map	bad destroy	0.034483
return the constant scalar 0-d value underlying variable	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
view	view	0.136364
output dimensions	nnet conv op get output	0.047619
diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x	0.083333
determine the name	name	0.011111
a numpy ndarray contains any np inf values	compile contains inf arr	0.500000
important note this	process node	0.142857
a sparse matrix and a dense vector	svcsr	0.090909
graph's state is invalid	inconsistency error	1.000000
of moved objects in	moves urllib	0.076923
for corr3dmm	nnet base corr3d mm	0.333333
operation to wait on a previously sent array	wait	0.022727
of cost and/or from existing start	start cost	0.100000
returns the connection pattern of a	io connection pattern	0.055556
cache	cache	0.344828
dot a	dot node	0.500000
a config	core parse config	0.333333
return a subtensor	subtensor	0.058824
has alias alias	alias alias	0.333333
in the list remove are still in	fgraph replacements remove reason	0.055556
change the value after	init default	0.040000
see min for the minimum in	tensor minimum	0.142857
a uniform distribution	tensor uniform random_state	0.125000
to help the navigator	navigator	0.032258
theano graphs represent the same computations	equal computations xs ys in_xs in_ys	0.333333
to make itself the default python and those	to os environ pathlist var	0.038462
op classes that this	gof local optimizer tracks	0.071429
is	is	0.533333
connection pattern of subfgraph defined	connection pattern	0.032258
the version	code cache version apply node	0.125000
outer product	block outer	0.250000
batch	batch	0.611111
sample from one or more multinomial distributions defined	tensor multinomial random_state size n	0.333333
given a slice [start	slice	0.038462
variant on wraplinker that runs a	gof	0.002381
return the idx_list with constant inputs	subtensor get constant idx inputs	0.250000
name the object should be saved under	misc persistent ndarray id resolve name obj	0.500000
return a	node name sub	0.111111
basic slow python 2d or 3d convolution for	abstract conv conv img kern mode dilation	1.000000
given a slice [start stop	slice	0.038462
number of dimensions from the shape or the	shape	0.010204
row variable (ndim=2 broadcastable=[true	row	0.034483
times from a multinomial distribution defined	streams multinomial	0.076923
a >=	ge	0.142857
of a real-valued input	rfft inp norm	0.142857
replace it with logsoftmax x 's grad	tensor nnet local logsoftmax grad node	0.200000
remove broadcastable dimensions from the shape of	tensor tensor py operators squeeze	0.200000
context_name	array	0.041667
persistent	persistent	0.833333
folowing changes in the	tensor local mul switch sink	0.045455
x -> sigm	nnet local	0.200000
convert addsd to	local addsd ccode node	0.250000
file that was dumped to a zip	f persistent_load	0.052632
c code to	op c code	0.333333
is the equivalent of localoptgroup	group	0.047619
add tag trace to	gof add tag trace thing user_line	0.166667
compute 2d kernel for bilinear upsampling this function	nnet bilinear	0.111111
conventions imposed	theslice length	0.052632
up to the end variables of	wrt end	0.050000
of the specified pieces of vectors	sparse block outer make	0.066667
top_shape	top_shape	0.172414
for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x	0.083333
the bartlett spectral window in	bartlett	0.058824
compute conv	tensor nnet conv2d	0.333333
numpy-compatibility method if x	tensor diag x	0.200000
destroyhandler class detects when a graph	destroy handler	0.055556
a tensorvariable whose type	as	0.024390
application of another op that takes the same	op sub	0.066667
replacement if	gof replace validate replace all	0.050000
the idx list to	idx list	0.250000
the named module	fullname	0.066667
function tries to	top_shape border_mode subsample	0.166667
need not be checked for nan	is numeric value arr var	0.166667
wasn't in the destroy_map	destroy	0.009709
some perform() or c_code() created	bad view map	0.142857
wrapper for numpy random randomstate	random state	1.000000
multiplication by a scalar on	gpuarray alpha	0.142857
the topooptimizer from the input	gof in2out	0.055556
an functiongraph	gof function graph	0.031250
baddestroymap if	check	0.083333
when enabled change all sigmoid to	sigmoid node	0.100000
the image shape of convolution gradweights	conv gradweights shape 1axis image_shape	0.500000
hack in profiling to print the mflops	flops inp outp	0.041667
for diagonalsubtensor and	diagonal subtensor view	0.083333
is false we can't change the value after	param init default filter	0.040000
__unify_walk__ method for one of	unify walk a b u	0.037037
of localoptgroup for graphtogpu	graph to gpulocal opt group	0.055556
a 4-d tensor	patch_size	0.050000
to the provided l{functiongraph} it may	optimizer apply fgraph	0.200000
for a function that reduces a contiguous buffer	gpuarray inline reduce n buf pos count	0.142857
exception class to raise in self	core raise	0.100000
vector and t is	node input_storage	0.038462
degree to radian	deg2rad	0.111111
complex otherwise call upgrade_to_float()	scalar upgrade to float no complex	0.333333
deepcopy in the fgraph	deepcopy fgraph	0.500000
c	c	0.321429
context	gpuarray reg context	0.333333
new graph	clone with new inputs	0.166667
value after	param init default filter	0.040000
calculate the n-th order discrete difference	tensor diff x n	0.333333
c code to initialize	c	0.017857
particular stream	random streams setitem item val	0.142857
kinds of	tensor type	0.034483
c contiguous version of the input	contiguous	0.058824
in which each row is a mrg	sandbox mrg	0.125000
computes the confusion matrix of	tensor nnet confusion matrix	0.166667
to roll	tensor roll	0.250000
of op classes that this opt applies to	gof local optimizer tracks	0.071429
variable v is positive semi-definite i	v	0.011111
flattened version of	tensor flatnonzero	0.166667
function to concatenate tensortypes along the given axis	tensor join axis	0.333333
the dimensions	tensor tensor py	0.015873
represent the dependence	dependence	0.035714
can't change the value after the import	init default filter	0.040000
should be inserted to maintain order	tensor searchsorted x v side sorter	0.142857
apply as many times as required	tensor apply	0.142857
some functiongraph listeners to help the navigator deal	gof navigator optimizer attach updater	0.038462
turned into macros for use within the	cop get	0.033333
the ignore_trees-related functionality	attach updater fgraph importer pruner chin	0.250000
replacement if the	replace	0.032258
a list of shape tuple or	default infer shape	0.066667
numpy-compatibility method if x is	tensor diag x	0.200000
filters with a	filters	0.032258
wasn't in the destroy_map	bad	0.013158
a list of l{codeblock} instances returns a	gof code gen blocks	0.050000
context object mapped to the	type context	0.090909
computes the sum along	tensor sum	0.111111
replace replace in string x	patterns x replace	1.000000
should remove any dynamically added functionality	gof print listener on detach fgraph	1.000000
matrix inverse on gpu	gpu matrix inverse a	0.200000
as required the optimization local_useless_rebroadcast and local_rebroadcast_lift	rebroadcast opt rval	0.200000
important note this function uses set and	seq scan process node fgraph node	0.142857
tuple of integers indicating the version	code cache version apply node	0.125000
the conventions imposed by python and numpy	theslice length	0.052632
the connection pattern	gof io connection pattern	0.055556
to py_none	get c init	0.500000
arctangent of a / b	tensor arctan2 a b	1.000000
shared	persistent shared	0.500000
required return c code	name sub	0.050000
default failure_callback for seqoptimizer	gof seq optimizer warn exc optimizer	1.000000
that with the *args directly	sparse local csm properties csm node	0.142857
runs a series of wrapper functions instead of	linker many linkers wrappers	0.047619
signature object for	tensor constant signature	0.100000
flatnonzero	flatnonzero	0.416667
inner-most loop executes code	reordered loop	0.111111
connection pattern of a subgraph defined	io connection pattern	0.055556
badviewmap exception when it detects	check viewmap node storage_map	0.111111
the folowing changes in the graph	mul switch sink node	0.045455
wasn't in	destroy	0.009709
decorator for fromfunctionoptimizer	gof inplace optimizer f	1.000000
graph optimizer that merges different cond ops	cond merge	1.000000
the subgraph bound by the inputs and outputs	inputs outputs	0.066667
diagonalsubtensor and	get diagonal subtensor view x	0.083333
wrt, computes gradients of	subgraph grad wrt	0.062500
randomstate instance associated with a particular stream	tensor random streams getitem item	0.142857
this explicitly upcasts constant inputs to elemwise	elemwise constant inputs	0.250000
tensor operators to the basic variable class	tensor variable	0.166667
we can't change the value after the	config param init default filter	0.040000
beta * y + alpha * dot	tensor gemv c code y	0.333333
pkl	pkl	1.000000
g++ version used is the	gcc	0.023810
compute conv	tensor nnet	0.052632
that allows replacing subgraphs of a computational	scan_module clone output replace strict share_inputs	0.071429
wasn't in the	bad	0.026316
wrapper around c_init that	gof get	0.100000
return	node name sub	0.111111
var shape[i], but apply if possible the shape	shape i var i fgraph	1.000000
can't change the value after the import	param init default	0.040000
a simple algorithm	order reasons r_vals	0.333333
if target is 'cpu' this will transfer	tensor tensor py operators transfer target	0.500000
return the platform-dependent gcc argument for shared libraries	gof get gcc shared library arg	0.333333
from a uniform distribution	uniform random_state	0.125000
the view_map	bad view	0.027027
compute sum of non	tensor constant signature get sum	0.142857
spatio-temporal filters	tensor nnet conv3d signals filters	0.111111
operation to wait on	mpisend wait	0.045455
sparse	sparse	0.230769
integers indicating the version	cache version apply	0.125000
into two kinds scalar constants and the rest	rest	0.076923
used to determine the broadcast pattern	tensor adv index broadcastable pattern a	0.066667
given an apply_node recursively search	import apply_node	0.066667
the convolution gradient with respect to the	dnn conv grad w	0.125000
broadcasted	py	0.014286
inner graph to ensure that	inner graph	0.035714
names to an iterable of variables modifies	gof give variables names variables	0.333333
subtensor is inside a dimshuffle	subtensor node	0.066667
node by one which computes the	node	0.007407
is an input vector and t is a	node input_storage	0.038462
transfer data to gpu	gpu from host	1.000000
round half to even of x	rint x	1.000000
compute conv output gradient w	nnet conv2d grad	0.333333
compat	compat	1.000000
that removes all asserts from the	tensor local remove all assert	0.055556
of moved objects in six moves urllib_error	six moves urllib error	0.142857
tuple python type c type numpy typenum that	type dtype specs	0.071429
same parameter as other scalar	scalar	0.035714
if the named module is a package	six meta path importer is package fullname	0.250000
important note this function uses	scan_module push out seq scan process node	0.142857
help the navigator deal with the ignore_trees-related functionality	navigator optimizer attach updater fgraph importer pruner chin	0.333333
the *args directly	sparse local csm properties csm node	0.142857
see theano tensor argsort	py operators argsort axis kind order	1.000000
raise	node	0.007407
type	pure type	0.285714
the conventions imposed by	theslice length	0.052632
to construct a variable	variable	0.022222
the svd of a matrix	svd	0.034483
create a function	compile function maker create input_storage trustme storage_map	1.000000
a mini-batch of a stack of 2d	input_shape filter_shape	0.018519
of each element in y	y	0.026316
permutation	permutation	0.454545
thunk that is a	thunk	0.021277
the replacement if the	replace all	0.050000
neibs2images	nnet neibs2images	0.333333
failure code to the task that is associated	gof cthunk find task failure_code	0.083333
return a hash from	tensor hash from	0.333333
theano expression whose gradient should be undefined	x	0.008772
pfunc	pfunc	1.000000
input that wasn't in the	bad destroy	0.034483
context associated with a name	gpuarray get context name	0.333333
of this	tensor tensor	0.028571
equivalent of localoptgroup for	group	0.047619
connection pattern of	connection pattern node	0.076923
of the	gof	0.002381
gradient updates for matrix solve operation c =	solve grad	0.250000
assert that x and y have	x y	0.024390
apply nodes according to	apply nodes inputs outputs cmps	0.050000
this method is primarily used by	gof pure op r op inputs eval_points	0.250000
of this op	l op	0.033333
dx	dx	1.000000
self _grad_op from user supplied form	op from graph recompute grad	0.200000
an functiongraph	function graph	0.040000
the navigator deal	gof navigator	0.038462
function to	random_state	0.250000
callbacks calls getattr feature name (*args) for	callbacks name	0.333333
to wait on	mpisend wait	0.045455
calls subprocess_popen returning the output error	misc output	0.066667
abstractconv(gpu_from_host)	gpuarray	0.023256
version of sparseblockouter see sparseblockouter's docstring	sparse block outer	0.047619
dot csr is like dot except that only	dot csr	0.111111
scrap the dimshuffle and index	local dimshuffle	0.052632
some elementary validations on the inner graph to	scan_module scan validate inner graph	0.035714
a file that was dumped to a	f persistent_load	0.052632
pieces of vectors and	sparse block gemv make node o w	0.066667
replaces an [advanced]incsubtensor[1], whose increment is an alloc	tensor local useless inc subtensor alloc node	0.166667
make a	tensor make	0.076923
the caller is replace_all_validate just raise	gof validator validate fgraph	0.125000
short mostly hexadecimal hash of a numpy ndarray	core hex digest x	0.083333
numpy randomstate instance associated with a particular stream	random streams getitem item	0.142857
uses the topooptimizer from the input	in2out	0.043478
specified pieces of vectors and matrices	sparse block outer make node o x y	0.066667
subgraph in functiongraph	gof function graph replace	0.500000
the broadcast pattern for advancedsubtensor	pattern a	0.066667
wrapper around c_init that initializes py_name to py_none	gof get c init r name sub	0.250000
variable optionally inserting broadcasted	tensor tensor py operators dimshuffle	0.019231
choose	tensor choose	0.250000
first outdim-1 dimension size s of x	x ndim outdim	0.333333
optional return	name inputs outputs	0.500000
replace_all_validate revert	all validate remove fgraph	0.166667
attempts to replace	scan_module scan inplace optimizer attempt	0.500000
views of v given that v	set v	0.125000
by transferring each of its outputs to	remove	0.035714
the given axis es of a tensor input	input axis dtype keepdims	0.500000
file	gof key data	0.333333
elemwise sinh of	sparse sinh	1.000000
version of var	var	0.035714
and	as	0.024390
we can replace that with the *args directly	local csm properties csm node	0.142857
offers to make itself the	to os environ pathlist	0.038462
around c_cleanup that	gof	0.002381
an apply_node recursively search from this node to	apply_node check	0.066667
to represent the dependence	make dependence	0.043478
a signature object for comparing	tensor constant signature	0.100000
conda offers to make itself	to os	0.038462
unfortunately conda offers to make itself the default	to os environ pathlist var	0.038462
replace it with logsoftmax x	nnet local logsoftmax node	0.142857
some functiongraph listeners to help the navigator	navigator optimizer attach updater	0.038462
that wasn't in	bad	0.026316
signature for this function	function method decl	0.333333
function is basically a call to	extract constant x elemwise only_process_constants	0.058824
x // 1 -> x	local intdiv by one node	1.000000
from a uniform distribution	uniform random_state size	0.125000
input a 4-d tensor	input patch_size	0.166667
if the passed-in key is found	get from key key key_data	0.111111
graphtogpu	to	0.017544
level of the list	list	0.066667
function is only used to determine the broadcast	tensor adv index broadcastable	0.050000
partially ordered sort with multiple comparators	gof posort l	1.000000
names to an iterable of variables modifies input	gof give variables names variables	0.333333
implementation of mod	scalar mod c code	0.125000
log base e	log	0.166667
current paramstype contains the specified theano type	has type theano_type	0.500000
computes gradients of cost and/or from existing	cost	0.045455
by one which computes the specified outputs inplace	inplace fgraph	0.142857
cache if available	gof call cache call fn args key	0.200000
partition a list	tensor scalarconsts	0.125000
apply_node recursively search from this node to know	apply_node check reason	0.066667
specified factor takes as	tensor signal pool	0.142857
the minimum in one tensor	tensor minimum x y	0.090909
updates	scan_module get updates	0.034483
tuple of symbolic shape vars for tensor variable	tensor shape feature shape tuple	1.000000
a dictionary of arguments to	args	0.025641
3d filters	conv3d input filters	0.142857
helper function for grad function	core populate grad dict var_to_app_to_idx	1.000000
of this variable optionally inserting	py operators	0.015625
see theano tensor argsort	tensor py operators argsort axis kind order	1.000000
each row is a mrg stream	sandbox mrg random streams	0.033333
3-d variable	tensor tensor3	0.500000
search through consecutive view_map()s	view roots r	0.200000
dimensions of this variable optionally	tensor tensor	0.014286
to help the navigator deal	gof navigator optimizer attach updater	0.038462
cross-entropy between an approximating distribution	nnet categorical crossentropy coding_dist true_dist	0.111111
the abs toward the input	tensor local abs lift node	0.333333
transfer function for alternative	transfer fn	0.125000
navigator deal with	gof navigator optimizer attach updater fgraph	0.038462
named	fullname	0.066667
this is	gpulocal	0.055556
convert python litterals	make constant	0.100000
make a schedule function from comparators	gof sort schedule fn	0.333333
one or more multinomial distributions defined by	tensor multinomial random_state	0.040000
module is	compat six meta path importer is	0.250000
c-implementation	csr c code	0.333333
a warning message on	deprecated filename msg	0.041667
insert deepcopy in the fgraph to break aliasing	compile insert deepcopy fgraph wrapped_inputs wrapped_outputs	1.000000
pvals	n pvals	0.125000
default failure_callback for seqoptimizer	warn exc optimizer	1.000000
as replace_all_validate revert the replacement if	replace all validate remove	0.111111
replace a	nnet replace	0.250000
transfer to a tensortype	operators transfer	0.125000
imgshape	imgshape	1.000000
still	fgraph replacements	0.250000
destroy_map	bad destroy	0.034483
inputs required to compute	inputs variable_list blockers	0.058824
-> softmax_w_bias matrix bias	tensor nnet local softmax with bias node	0.200000
wait on a previously received array	mpirecv wait	0.045455
symbolic row variable (ndim=2 broadcastable=[true	tensor row name	0.050000
helper function for diagonalsubtensor and	diagonal subtensor view x i0	0.083333
print the mflops	corr mm flops inp outp	0.125000
a value on the output	gpuarray output	0.200000
connection pattern of subfgraph defined by inputs and	op from graph connection pattern node	0.076923
int	int	0.833333
half to even	half to even	1.000000
floor of	tensor floor	1.000000
a symbolic	name ndim dtype	0.333333
for comparing tensorconstant	constant	0.016667
vector to the diagonal of an	alloc diag	0.027027
doing a recursion over	tensor permute row elements rec	0.047619
1	1	1.000000
instance of _maker which handles much	function maker i o m	0.066667
foldr	scan_module foldr	1.000000
retrive the context associated with a name	context name	0.333333
broadcast pattern	pattern a	0.066667
as the template	template	0.125000
try to turn softmax(sum_of_stuff) -> softmax_w_bias	nnet local softmax with	0.200000
any python object a that would	gof pure	0.033333
to make itself the default	to	0.017544
extract	extract	0.666667
function uses set and dictionary data structures	non	0.071429
the mflops	nnet conv op flops	0.125000
for scalar values default	scalar	0.017857
class with a metaclass	add metaclass metaclass	0.125000
validations on the inner	inner	0.041667
and converts this to expm1	local expm1 node	0.066667
helper function to generate permutations	permutation helper random_state n	0.333333
iff other is the same kind of tensortype	other	0.090909
graph	destroy	0.009709
changes node inputs[i] to	function graph change input node i	0.250000
by the inputs	inputs	0.012658
reorder the dimensions of this variable optionally inserting	tensor tensor py operators	0.015625
is an input vector and t	node	0.007407
be removed from	remove outs	0.500000
when we put in a functiongraph a constant	constant	0.016667
necessary update dr_vals	inputs node storage_map r_vals dr_vals	0.250000
functiongraph listeners to help the navigator	gof navigator	0.038462
with constant inputs replaced by their	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
filled with ones closer to numpy's	ones shape dtype	0.200000
border_mode	border_mode	1.000000
the gradient function should	matrix inverse r op inputs	0.500000
merge some gpucareducecuda and gpuelemwise	gpu elemwise careduce node	1.000000
multinomial distributions defined by one-dimensional slices in	multinomial random_state	0.040000
the name the	name obj	0.111111
finder	finder	1.000000
list of shape tuple	infer shape	0.066667
extract test value from v raises attributeerror	get test value v	0.250000
this is the	to	0.017544
that represents their unification	unification	0.076923
output gradient w r	conv3d grad	0.111111
r	debugprint r	0.250000
a symbolic row	row name	0.050000
this variable optionally	py operators	0.015625
convolution with	dnn conv get	0.100000
operation to	mpirecv	0.037037
reverse-mode gradient updates for	grad inputs output_gradients	0.250000
of the __unify_walk__ method for one	unify walk a b u	0.037037
number to string by rendering it	number number	0.125000
self _rop_op from user supplied form to	from graph recompute rop op	0.200000
a new graph	with new	0.166667
offers to make itself the default python	to os environ pathlist var	0.038462
wasn't in the view_map	view	0.022727
maps from variable and	get equiv inputs	0.142857
called whenever node inputs[i] is	feature on change input function_graph node i	0.333333
the specified pieces of vectors	sparse block outer make node	0.066667
important note this function	seq scan process node fgraph	0.142857
convert	make constant	0.100000
returns	gof local meta optimizer	0.500000
destroy_map	destroy	0.009709
the dimensions of this	py operators dimshuffle	0.019231
register a	register	0.100000
reorder the dimensions of this	tensor py	0.015873
proxy for either true_div or int_div depending	div proxy	0.125000
that uses r	gof function	0.043478
i and o	clone i o	1.000000
string representating the cause of	bad optimization str diagnostic	0.043478
to wrt, computes gradients of	core subgraph grad	0.062500
elemwise round half to even of x	sparse rint x	1.000000
return connection pattern of	connection pattern node	0.076923
outputs from	outputs	0.045455
argsort	argsort axis kind order	1.000000
on the inputs and put the variables	pure op perform node inputs output_storage params	0.047619
the navigator	navigator optimizer attach updater fgraph	0.038462
respect to wrt, computes gradients of	subgraph grad	0.062500
data field	csm data csm	0.333333
failure_callback for	exc	0.200000
the types involved in this node	tensor inc subtensor do type checking node	0.250000
simplify	simplify	0.857143
standard elements of an op or	object	0.083333
>	gt	0.250000
create a new instance of this mode	mode clone link_kwargs optimizer	0.333333
to help the navigator	gof navigator	0.038462
correspond to the one	tensor to one	0.125000
by transferring each of its	remove	0.035714
that x	x	0.008772
the caller is replace_all_validate just raise the exception	gof validator validate fgraph	0.125000
compute conv output gradient w r	tensor nnet conv3d grad	0.333333
global optimizer for pushing out the variables	push out	0.037037
complex-valued tensor from polar coordinate specification	from polar abs angle	0.250000
1/(1+exp x ->	nnet local inv 1 plus exp	0.333333
dimensions of this variable	tensor py operators dimshuffle	0.019231
where each row correspond to the one hot	one hot	0.142857
decorator to merge multiplication by	alpha merge cls alpha_in beta_in	0.200000
new random stream in this container	random streams gen op	0.250000
elemwise maximum see max for the maximum in	tensor maximum x	0.142857
and return full path of the dynamic	module name	0.062500
the dimensions of this variable optionally inserting	py	0.014286
special compound l{op} for	argmax1hot	0.058824
:param execute if true execute a theano	misc execute execute verbose m	0.250000
that allows replacing subgraphs of a computational	clone output replace strict share_inputs	0.071429
the navigator deal with	gof navigator optimizer attach updater	0.038462
some elementary validations on the inner	validate inner	0.142857
conda offers to make	to os environ pathlist var	0.038462
six moves urllib namespace that	module six	0.043478
path importer to import	path importer	0.333333
base class with a metaclass	with metaclass meta	0.333333
a signature object for comparing tensorconstant instances	constant signature	0.100000
:attr	gpu	0.011765
change the value after	param init default filter	0.040000
dimshuffle and index	tensor local dimshuffle	0.052632
replace a leaf of a multiplication tree	replace leaf	0.100000
to the end	end	0.040000
l{codeblock} instances returns	code gen blocks	0.050000
multiplication by a scalar	gpuarray alpha	0.142857
as replace_all_validate revert	validate	0.090909
important note this function uses	push out seq scan process node fgraph	0.142857
conv output gradient	conv3d grad	0.111111
from a multinomial	from	0.050000
clients list of r	r client_to_remove	0.200000
for corrmm	tensor nnet base corr mm	0.333333
merge abs generated by local_abs_lift when the canonizer	local abs merge node	0.333333
listeners to help the navigator deal with the	gof navigator optimizer attach updater	0.038462
subtensor of the form x[0 :] -> x[0]	tensor local useless slice node	0.250000
impossible to evaluate because	destroy	0.009709
function that allows replacing subgraphs of	clone output replace strict share_inputs	0.071429
see theano tensor sum	tensor py operators sum axis dtype keepdims	1.000000
destroyhandler class detects when a graph is	handler	0.071429
m1 and the second half	m1	0.027027
if allow_override is false we	allow_override	0.083333
a signature object for comparing tensorconstant instances	tensor constant signature	0.100000
norm	norm	1.000000
the subgraph bound by the inputs and	inputs	0.012658
free	free	0.833333
connection pattern of subfgraph	connection pattern	0.032258
an operation to wait on a previously	wait	0.045455
duplicate this apply instance in a new	gof apply clone with new inputs inputs strict	0.250000
max and argmax over a given	max and argmax	0.125000
makes the folowing changes in the graph	mul switch sink node	0.045455
return full path of the dynamic lib in	module name from	0.076923
to r to	debugprint r	0.250000
reduce	scan_module reduce	1.000000
decorator to merge multiplication	gpuarray alpha merge cls alpha_in beta_in	0.200000
1 0/a	tensor inv	1.000000
a copy of the type	tensor type	0.034483
represent the dependence of nodes in a graph	dependence	0.035714
around c_cleanup that decrefs py_name	gof get c cleanup r name sub	0.250000
into a canonical form that respects the	canonical form	0.045455
pattern has functioning	gpuarray gpu careduce cuda supports	0.166667
current op and reduce pattern has functioning	gpuarray gpu careduce cuda supports	0.166667
the output dimensions of	output	0.017241
create pydot node from dict	d3viz dict to pdnode d	1.000000
total time icluding the time	total times	0.200000
adding the values	dict d1 d2	0.333333
in u	u	0.100000
return a thunk that is a zero-arguments function	thunk	0.021277
replace_all_validate revert the replacement if the ops	gof replace validate replace all validate remove	0.111111
the outer	outer	0.083333
the idx_list with constant inputs replaced	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
mrg stream state and they are spaced by	sandbox mrg random streams	0.033333
proxy for either	proxy	0.095238
extract test value from v raises attributeerror	test value v	0.250000
module is a package	six meta path importer is package	0.500000
kernel shape of convolution gradweights	conv gradweights shape	0.333333
of theano's operations compilation optimization execution	profile stats	0.250000
a dimshuffle is inside	dimshuffle node	0.333333
this optimization makes the folowing changes in the	local mul switch sink	0.045455
es	dtype keepdims	0.250000
base	base	0.555556
param	param	0.833333
outputs from the inputs	inputs outputs mode accept_inplace	0.500000
its idx_list reorders the inputs according	inputs idx_list get_count	0.100000
wrapper around c_extract_out that initializes py_name from	gof get c extract out r name sub	0.333333
array and a set of arrays to choose	choose a choices out mode	0.200000
non-sequences	non	0.071429
output nodes	gof	0.002381
unroll	nnet gen conv code unroll	0.250000
-a inplace on a	neg inplace a	1.000000
the idx_list with constant inputs replaced by their	constant idx inputs allow_partial only_process_constants elemwise	0.071429
apply the list of policies to	gof apply policy policy	0.500000
the dimensions	tensor	0.006431
python litterals	make constant	0.100000
to recognize the updates ordereddict	get updates	0.034483
conda offers to make	to os environ	0.038462
and reduce pattern has functioning c code	careduce cuda supports c code inputs	0.250000
y with length one the axes of x	y	0.026316
given a inner nit_sot	inner sitsot	0.083333
to the end variables of	grad wrt end	0.050000
of a search through consecutive view_map()s	gof view roots r	0.200000
they are spaced by 2**72 samples	get substream rstates n_streams dtype inc_rstate	0.142857
return path to	get entry	0.333333
decorator which will print a warning message on	deprecated filename msg	0.041667
of useless	useless	0.076923
computes the l operation on f	f	0.052632
function is basically	tensor extract constant x elemwise only_process_constants	0.058824
between i and o nodes via	i o	0.041667
supplied function as its implementation	compile as	0.050000
with respect to wrt, computes gradients of	subgraph	0.047619
true iff x and y are equal	tensor check equal numpy x y	0.500000
to	array	0.041667
on cpu here	tensor local pow specialize	0.250000
optionally	tensor py operators dimshuffle	0.019231
it with logsoftmax x 's grad	local logsoftmax grad	0.200000
graph of apply nodes according	apply nodes inputs outputs cmps	0.050000
required anymore and should be removed	outs	0.050000
one or more multinomial distributions	multinomial random_state	0.040000
op and reduce pattern has functioning c	careduce cuda supports c	0.200000
swapaxes	swapaxes	1.000000
from the	remove	0.035714
or -exp	is	0.066667
sinh	sinh	0.857143
list of lib directories	header dirs	0.045455
change the value after the import of	default filter	0.040000
gemm	tensor gemm	0.166667
a comparator to represent	cmp	0.058824
along the given axis es of a tensor	axis dtype keepdims	0.083333
called by toposort it should return a dictionary	feature orderings function_graph	1.000000
gist	gist	0.240000
see theano tensor max	tensor tensor py operators max	1.000000
is r	r	0.028571
function-constructor for graphs with shared	compile pfunc params outputs mode updates	0.200000
apply instance from set which must be computed	on prune fgraph app reason	1.000000
if a subtensor is inside a dimshuffle which	subtensor node	0.066667
generates the c code for corr3dmm (direction="forward"),	corr3d mm c code	0.090909
the kernel shape of convolution gradweights	conv gradweights shape image_shape	0.500000
make a value	make value ptr	1.000000
elementwise	b	0.104478
associate linker with fgraph	gof op wise clinker accept fgraph	1.000000
of lib directories that are needed by	lib dirs	0.045455
removes all asserts	local remove all assert	0.055556
to turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	tensor nnet local softmax with bias node	0.200000
value after the import	config param init default filter	0.040000
node i pairs such	function graph clients	0.200000
wasn't in the	bad destroy	0.034483
implementation of mod	mod c code node name inputs outputs	0.125000
get a scalar	scalar	0.017857
decorator to merge multiplication	alpha merge cls alpha_in beta_in	0.200000
concatenate a number of scalars together into a	make	0.017857
for a constant that	gof params	0.200000
symbolic	name dtype	0.333333
used is the llvm one or not	gof gcc llvm	0.200000
true if l has any duplicates (according to	scan_module has duplicates l	0.111111
new variable	var name	0.250000
contents of	dirname err files	0.083333
failure_callback for navigatoroptimizer	exc nav repl_pairs local_opt	1.000000
uses set and dictionary data structures	push out non	0.125000
dimensions of this	tensor py operators	0.015625
shape in	tensor shape	0.058824
vector to the diagonal of	diag	0.023810
[v1 v2 v3 ]	gof vm linker compute gc dependencies	1.000000
shape	tensor shape feature shape	1.000000
upcast	upcast	1.000000
detect if the g++ version	gof gcc	0.027778
return connection pattern	graph connection pattern node	0.076923
connection pattern of subfgraph defined by inputs	compile op from graph connection pattern	0.076923
of v by a with a modulo	v	0.011111
if necessary update dr_vals	r_vals dr_vals	0.250000
also their apply_node if those nodes are	gof	0.002381
1/(1+exp x -> sigm	nnet local inv 1 plus exp node	0.333333
and/or from existing start gradients	start	0.040000
a nested loop over several arrays	loop	0.027778
to a	a	0.008065
dimensions of this variable optionally inserting	operators dimshuffle	0.019231
the inputs and put the	gof pure op perform node inputs output_storage params	0.047619
nesting	loop_orders dtypes loop_tasks sub	0.125000
a /	a	0.008065
new variable whose gradient will be stored in	grad var	0.333333
if the g++ version used is the	gof	0.002381
of this variable	tensor py operators dimshuffle	0.019231
equivalent of	gpulocal opt	0.055556
the list remove are still	replacements remove reason	0.055556
same op twice gives inconsistent outputs	bad	0.013158
broadcast pattern for advancedsubtensor output	pattern	0.028571
first functiongraph that	gof	0.002381
converts self _grad_op from user supplied form to	compile op from graph recompute grad	0.200000
drawing	pvals	0.071429
c code to allocate outputs	tensor make alloc loop_orders dtype sub fortran	0.200000
g++ version used	gof gcc	0.027778
patch variable so that its type will match	gof pure type convert variable var	1.000000
to convert x into	x context_name	0.100000
this generates the c code for corrmm (direction="forward"),	nnet base corr mm c code	0.090909
reproducible case for problems	compile function dump filename inputs outputs mode	0.166667
the confusion matrix of	confusion matrix	0.166667
the context object mapped to	context	0.035714
this variable	tensor	0.006431
nodes are not in this	function	0.052632
a previously received array using mpi	mpirecv	0.037037
merge multiplication by a scalar on the output	gpuarray alpha merge	0.076923
to the type's :attr	gpuarray gpu array	0.062500
wrapper around c_sync that syncs py_name with storage	gof get c sync r name sub	1.000000
a warning message on the first call	gof deprecated filename msg	0.041667
vector to the diagonal of an empty	diag	0.023810
and only adds dimension to the left	local	0.014085
apply to be	apply node	0.031250
inserting broadcasted	tensor tensor py	0.015873
output of neural-net classifiers	with bias	0.166667
see theano tensor argmin	tensor tensor py operators argmin axis	1.000000
gradient function should	matrix inverse r op	0.500000
the updates ordereddict the	get updates	0.034483
tag	tag	0.833333
[advanced]incsubtensor[1], whose increment is an alloc of	tensor local useless inc subtensor alloc node	0.166667
toposort return an	toposort	0.076923
the value after the import of	config param init default filter	0.040000
cosine of a	tensor cos a	1.000000
a graph either breadth- or depth-first	start expand mode build_inv	0.333333
that runs a series of wrapper functions	gof wrap linker many linkers wrappers	0.071429
that was used to	inputs g_outputs	0.090909
graph of apply nodes	apply nodes	0.200000
abs_tol	abs_tol	0.833333
reorder the dimensions of this	tensor tensor	0.014286
is not attempting to use dnn	no dnn	0.125000
in the destroy_map	bad	0.013158
new variable from	new	0.058824
crossentropysoftmax1hotwithbiasdx op whose incoming gradient is an alloc	crossentropy softmax 1hot with bias dx alloc node	1.000000
sample from a uniform distribution	tensor uniform random_state size	0.125000
the connection pattern of a subgraph defined	io connection pattern	0.055556
dot22 computing an outer-product -> ger	local dot22 to ger or	1.000000
bitwise a | b	tensor or a b	1.000000
with the same	like x	0.250000
poisson	poisson	0.857143
important note	process node fgraph	0.142857
a tuple of integers indicating the version	clinker object c code cache version	0.125000
in	in	0.461538
folowing changes in the	mul switch sink node	0.045455
none or existing input storage see below	input_storage	0.166667
this is	to gpulocal opt	0.055556
help the navigator	navigator	0.032258
expm1 a	tensor local expm1 node	0.066667
a particular stream	random streams setitem item val	0.142857
print a warning message on the first call	deprecated filename msg	0.041667
subprocess_popen returning the output	output	0.017241
initial value for	gpu	0.011765
to turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	tensor nnet local softmax with bias	0.200000
graph optimizer for scan makes it run inplace	push out dot1	1.000000
load a file that	misc load	0.250000
list of outputs and	and outputs	0.100000
the hack in profiling to print the mflops	flops	0.076923
replacement if the ops in the list	replace all	0.050000
use a simple algorithm	reasons r_vals	0.333333
replacement	replace validate replace all	0.050000
to recognize the updates ordereddict the list	updates	0.029412
computes the output dimensions of convolving an	conv op get output	0.047619
pruner	pruner	1.000000
of	tensor type	0.034483
print the mflops	flops inputs outputs	0.125000
equivalent of localoptgroup for	to gpulocal opt group	0.055556
list remove are	remove reason	0.142857
l{codeblock} instances returns a	code gen blocks	0.050000
the existence of the __unify_walk__ method	unify walk a b u	0.037037
generate c	register shape i c	0.250000
op without affecting the rest	op	0.009174
tensorconstant instances	constant	0.016667
c code for gpucorrmm (direction="forward"),	gpuarray base gpu corr mm c code	0.090909
the confusion	nnet confusion	0.333333
this function builds the 2d kernel that can	kernel 2d	0.050000
where function is a thunk that	gof linker make thunk	0.045455
to make code	code	0.050000
inner graph to ensure that	scan_module scan validate inner graph	0.035714
stack trace from one or more tensor	copy stack trace	0.055556
a number of scalars together into a vector	vector	0.066667
of	to gpulocal opt	0.055556
gpuincsubtensor	tensor local inplace setsubtensor	0.250000
baddestroymap	check inputs node storage_map r_vals	0.166667
of apply nodes according to a list	sort apply nodes inputs outputs cmps	0.050000
return c code to extract	type c extract name	0.250000
the last access of a given	last access time	0.040000
initializes py_name to py_none	init r name	1.000000
draw samples from a poisson distribution	streams base poisson size lam	1.000000
the variable v is positive semi-definite i e	v	0.011111
is a mrg	mrg	0.076923
minimum elements obtained by iterating over	keepdims	0.105263
unfortunately conda offers to make itself	to	0.017544
inserting 1 at the dimension	tensor shape padaxis	0.333333
diagonal of an empty matrix it does	alloc diag	0.027027
the confusion	tensor nnet confusion	0.333333
connection pattern	from graph connection pattern	0.076923
label	label node	0.250000
!= b inplace on a	tensor neq inplace a b	0.500000
baddestroymap if	r_vals	0.090909
converting to type2	type2	0.050000
replace_all_validate revert the	validate remove	0.166667
that wasn't in the view_map	view	0.022727
implementation of mod	scalar mod c code node name inputs outputs	0.125000
basic theano op that will call the	op itypes otypes infer_shape	0.047619
construct a variable with a	variable x	0.083333
see theano tensor prod	py operators prod axis dtype keepdims acc_dtype	1.000000
the 0 based level of the list	list type	0.100000
print a warning message	gof deprecated filename msg	0.041667
raises a badviewmap exception when it detects	check viewmap node storage_map	0.111111
sum(alloc constant shapes => constant*prod shapes	local opt alloc node	1.000000
a diff to make	diff	0.071429
ift	ift	1.000000
scalar values	scalar	0.017857
row variable	tensor row name dtype	0.050000
to the provided l{functiongraph} it	optimizer apply	0.166667
:attr context_name	type	0.011905
image shape of convolution gradweights	conv gradweights shape 1axis	0.500000
multiplication by	gpuarray alpha	0.142857
maximum see max for the maximum in one	tensor maximum x y	0.090909
of apply node	d3viz apply	0.333333
disabled by default that removes all asserts from	local remove all assert	0.055556
tuple of integers indicating the version	code cache version	0.125000
return those items of	gof	0.002381
converts self _grad_op from user supplied form	op from graph recompute grad op	0.200000
complex-valued tensor from	from	0.050000
method that attaches	gof bookkeeper on attach fgraph	0.142857
dimension axis	axis	0.025641
data type for working memory	gpuarray work dtype dtype	0.200000
output_variables) where function is a thunk	linker make thunk	0.125000
urllib namespace that resembles the python 3 namespace	moves urllib	0.038462
the graph leading to r to given depth	debugprint r prefix depth done	0.500000
kernel_shape	kernel_shape	0.714286
the context object mapped to the	gpuarray gpu array type context	0.090909
input vector and	node input_storage	0.038462
with logsoftmax x 's	nnet local logsoftmax	0.076923
try to use complex numbers	tensor mod check x y	0.166667
stabilization optimization that is more general than crossentropy_to_crossentropy_with_softmax_with_bias	tensor nnet crossentropy to crossentropy with	1.000000
traversal and chooses the orphans among them	orphans	0.090909
equivalent of localoptgroup	gpulocal opt group	0.055556
graph for convolving a mini-batch of a	input_shape filter_shape	0.027778
the stack trace from	copy stack trace	0.055556
thunk that	gof linker make thunk	0.045455
a c contiguous version of the	gpu contiguous	0.083333
that node	gof	0.002381
cache directory structure	module cache	0.071429
for diagonalsubtensor	tensor nnet get diagonal subtensor view x i0	0.083333
node by one which	node	0.007407
functions raises a badviewmap exception when it detects	compile check viewmap node	0.111111
the source code for	clinker compile cmodule location	0.038462
variable v if v	v	0.011111
list of lib	lib	0.125000
node	transform node	1.000000
getter method for self _rop_op	graph get rop op	1.000000
the c code for corr3dmm (direction="forward"),	tensor nnet base corr3d mm c code	0.090909
sum of non nan /	sum	0.038462
the subgraph contained between	copy_inputs	0.125000
a tuple of integers indicating the version	object c code cache version	0.125000
shape	shape image_shape	0.500000
to name r sub	r name sub	0.250000
new variable to theano config	config var name doc	0.500000
a given axis	axis	0.025641
return c code to extract	gof clinker type c extract name	0.250000
a uniform	uniform random_state size	0.125000
clone in	clone	0.020833
generates the c code for corrmm	nnet base corr mm c code	0.090909
to target	target	0.125000
upcasts constant inputs to	constant inputs	0.125000
symbolic row variable (ndim=2	row name	0.050000
the numeric shape of	tensor shape	0.058824
generic exception raised	error	0.025000
cache directory and return full	gof module name from dir	0.071429
attributes and tag during	variable attributes	0.333333
clone in a	clone	0.020833
c_code for convop	kern d unroll_bsize unroll_ksize	0.166667
a dictionary of arguments to pass	args	0.025641
the replacement	replace	0.032258
cache directory and return full path of the	module name from dir	0.071429
to generate permutations	permutation	0.090909
that uses r as	gof function	0.043478
swap	swap	1.000000
same rounding	round half	0.100000
the outputs of specific ops of a	trace f_or_fgraph ops_to_check bug_print	0.035714
same kinds of tensortype	tensor tensor type	0.041667
convolve spatio-temporal filters with a movie	nnet conv3d signals filters signals_shape filters_shape	0.333333
reduce pattern has functioning	careduce cuda supports	0.166667
functiongraph listeners to help the navigator deal	gof navigator optimizer	0.038462
a multinomial distribution	streams multinomial	0.076923
the first outdim-1 dimension size s of x	flatten x ndim outdim	0.333333
turn softmax(sum_of_stuff) ->	local	0.014085
grad_dict	grad_dict	1.000000
uses shared	persistent shared	0.500000
called by toposort it should return a dictionary	orderings function_graph	1.000000
python	python	1.000000
reorder	tensor tensor py operators dimshuffle	0.019231
respect to the	gpu dnn	0.133333
unroll	code unroll	0.250000
to this graph	gof function graph import	0.125000
the main diagonal set to a	fill diagonal offset a val offset	0.100000
-> list of variables [v1 v2 v3 ]	vm linker compute gc dependencies variables	0.250000
that represents their unification	gof unification	0.125000
to get the 0	get	0.020833
return the idx_list with constant inputs	tensor subtensor get constant idx inputs	0.250000
modulo	tensor mod	1.000000
minimum elements obtained by iterating over given axis	x axis keepdims	0.500000
scan return true	out scan	0.035714
replace_all_validate revert the replacement if the	validate replace all validate remove fgraph	0.111111
as strings check if converting to type2	type2	0.050000
diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x i0	0.083333
topooptimizer from	in2out	0.043478
es	ddof keepdims	0.250000
of this	operators dimshuffle	0.019231
compute conv output gradient w r	nnet conv2d grad	0.333333
the *args directly	local csm properties csm	0.142857
that reduces a contiguous buffer	gpuarray inline reduce n buf pos count	0.142857
x*x -> sqr x this	local mul to sqr node	0.166667
of mod	mod c code node name inputs	0.125000
to replace	nnet replace	0.250000
calculating the dot product	dot	0.071429
some functiongraph listeners to help the navigator	navigator	0.032258
llvm	llvm	0.500000
unification in u and uses	o u	0.037037
of scan return true iff the	push out scan	0.050000
pure	pure	0.833333
fetch a compiled module	module name	0.062500
six moves	six	0.175000
generic exception raised to indicate	error	0.025000
mflops	tensor nnet conv op flops inputs	0.125000
raised when grad is asked to	error	0.025000
c code for corr3dmm (direction="forward"),	nnet base corr3d mm c code	0.090909
to determine the broadcast pattern for	adv index broadcastable pattern a	0.066667
return a symbolic constant with value x	constant x name ndim dtype	0.333333
was dumped to a zip	f persistent_load	0.052632
this convert allocempty to alloc of	local alloc empty to zeros node	0.333333
is the equivalent of localoptgroup for graphtogpu	graph to gpulocal opt group	0.055556
on types of x	x	0.017544
baddestroymap	check inputs node storage_map	0.166667
a simple algorithm	bad optimizations2 order reasons r_vals	0.333333
output of scan return true iff	scan	0.017241
other implementation of mod	scalar mod c code node name inputs	0.125000
vector to the diagonal of an empty matrix	diag	0.023810
return true iff the outer nit_sot	last step used var scan_args	0.200000
navigator deal	gof navigator optimizer attach updater	0.038462
tensor_from_scalar(scalar_from_tensor x -> x	tensor local tensor scalar tensor node	1.000000
sum(a axis=[]) -> a	local useless reduce node	0.500000
x to a tensor	x	0.008772
view_map	bad	0.013158
the updates ordereddict the list of	scan_module get updates	0.034483
self _grad_op from user supplied form	compile op from graph recompute grad op	0.200000
for	nnet base abstract	0.500000
row correspond to the one hot	tensor to one hot	0.142857
optionally inserting	operators dimshuffle	0.019231
function tries	top_shape	0.137931
all variables which may share the same underlying	compile infer reuse pattern	0.100000
same kinds	tensor	0.006431
function computes the output shape	out shape ishape kshape border_mode subsample	0.500000
baddestroymap if	compile check inputs node	0.166667
compute the element-wise exponential linear activation function	tensor nnet elu x alpha	1.000000
function uses set and dictionary data structures	out non seq scan	0.125000
power (inplace on a)	pow inplace a	1.000000
a set of 3d	tensor nnet conv3d input	0.125000
in the original graph to a new node	outputs copy_inputs_and_orphans memo	0.029412
the value after the import	core config param init default	0.040000
indicating the version	cache version apply	0.125000
list of compilation flags	libs flags libs_dir include_dir	0.052632
output shape	get out shape	0.500000
apply nodes such that	gof function graph	0.031250
the matrix inverse on	matrix inverse a	0.200000
a subtensor	inc subtensor	0.250000
of the specified pieces of vectors and	sparse block gemv make node o w h	0.066667
m1 and	m1	0.027027
shape feature	shape	0.010204
sample from a normal distribution	random streams base normal size	1.000000
a basic theano op	op itypes otypes infer_shape	0.047619
test a	pt n_tests	0.500000
== b inplace on a	tensor eq inplace a b	0.500000
of cost and/or from existing	cost	0.045455
v given that v	tree set v	0.125000
detect if the g++	gof gcc	0.027778
view in the forward but clip the gradient	core grad clip x lower_bound upper_bound	0.250000
permutation by doing a recursion over the	tensor permute row elements rec	0.047619
a simple algorithm to find broken	compile find bad optimizations2 order reasons r_vals	0.111111
the permutation by doing a recursion over	permute row elements rec	0.047619
hack in profilemode to print the mflops	base gpu corr3d mm flops inp outp	0.125000
object into its key_pkl file	key data save pkl	1.000000
navigator	gof navigator	0.038462
the updates ordereddict the list of	get updates	0.034483
stack trace	gof copy stack trace	0.055556
tensor from d1 d2 to d1+size d2	scan_module expand empty tensor_var size	0.166667
2^a (inplace on a)	tensor exp2 inplace a	1.000000
f wrt to wrt	lop f wrt	0.200000
a <= b	tensor le a b	1.000000
of scan return	scan	0.017241
a >=	tensor ge	0.500000
release lock on compilation directory	gof release lock	1.000000
grab	grab	1.000000
fgraph to break aliasing of outputs	fgraph wrapped_inputs wrapped_outputs	0.111111
initializes py_name to py_none	get c init r name	1.000000
return full path of the dynamic lib in	gof module name	0.076923
the values of a shared	shared	0.062500
populate	populate	1.000000
compute 1d kernel for bilinear upsampling	nnet bilinear	0.111111
expects the compile lock to	cache add to	0.142857
topooptimizer from the input nodes to	gof in2out	0.055556
into a basic theano op that	op itypes otypes infer_shape	0.047619
computes	ishape kshape border_mode subsample	0.250000
alloc val [x y] -> alloc(val[ ])	local subtensor of alloc node	1.000000
times as required the optimization local_useless_rebroadcast and local_rebroadcast_lift	rebroadcast opt rval	0.200000
foldr	foldr	1.000000
merge multiplication by a	gpuarray alpha merge	0.076923
tensor where n >= 3	3d	0.100000
string (comma-separated key=value components) into	string config_string issue_warnings	0.333333
return complex-valued tensor with real and imag components	complex real imag	1.000000
transfer to a tensortype if not already one	operators transfer	0.125000
function-constructor for graphs with shared variables	compile pfunc params outputs mode updates	0.200000
platform-dependent	gof get	0.100000
determine the broadcast pattern for advancedsubtensor	adv index broadcastable pattern a	0.066667
connection pattern of a subgraph defined by	connection pattern	0.032258
for matrix solve	solve	0.032258
called whenever node inputs[i] is changed	feature on change input function_graph node i	0.333333
optionally	tensor	0.006431
a tensorvariable	make variable	0.166667
computes the svd of	svd	0.034483
the	array type	0.111111
multinomial distribution defined by probabilities	sandbox mrg random streams multinomial	0.250000
old_r	old_r	0.750000
of var transferred to target	tensor transfer var target	0.200000
always	gpuarray	0.023256
file into an aboslute path	cop get path cls f	0.166667
multiplication	mul	0.384615
wraplinker that	gof wrap	0.083333
^	tensor xor	0.333333
this is meant as a	gof optimizer optimize fgraph	0.200000
dimensions of this	py operators	0.015625
the apply to be	apply	0.016667
cond then ift else iff	tensor switch cond ift iff	0.500000
inputs according to the idx list to	idx list inputs	0.500000
all symbolic variables in inputs to sharedvariable instances	inputs	0.012658
destroyhandler class detects when a graph is impossible	handler	0.071429
returns the bartlett spectral window in the time-domain	tensor bartlett	0.083333
add	tensor inc subtensor add	1.000000
sample from a uniform distribution	tensor uniform random_state	0.125000
of nodes that must be evaluated	gof	0.002381
default that removes all asserts from the	remove all assert	0.055556
_maker which handles much of the debugging work	maker i o m	0.066667
symbolic row variable (ndim=2 broadcastable=[true	tensor row	0.050000
return label of apply node	d3viz apply label	0.500000
and y	y	0.026316
destroyhandler class detects when a graph is impossible	destroy handler	0.055556
elements and their	max and argmax a	0.250000
3-d	tensor tensor3	0.500000
make a nested loop over several arrays	make loop	0.200000
see theano tensor prod	py operators prod axis	1.000000
of headers that	clinker headers	0.047619
tries to	image_shape top_shape border_mode subsample	0.166667
convolution gradient with respect to the inputs	gpu dnn conv grad i	0.125000
see whom can be removed from	can remove outs	0.250000
a convolution with the specified	dnn conv get	0.100000
a -1 and converts this to expm1	tensor local expm1 node	0.066667
and a dense vector	svcsr	0.090909
_maker which handles much	mode function maker i o m	0.066667
is unified to boundvariable(other_object)	gof unify walk fv o u	0.200000
c code for gpucorrmm (direction="forward"), gpucorrmm_gradweights	gpuarray base gpu corr mm c code	0.090909
can be considered exactly	gof pure type values	1.000000
replace a leaf	tensor nnet replace leaf	0.100000
similar behaviour as haskell's foldl	foldl fn sequences outputs_info	1.000000
list of shape tuple or	feature default infer shape	0.066667
of outputs and the stopping condition returned	and outputs ls	0.166667
scan return true iff	scan_module push out scan	0.050000
return connection	from graph connection	0.500000
detect if the g++	gcc	0.023810
mean value	mean	0.062500
get	tensor get	0.250000
to evaluate because of	destroy	0.009709
from dict	dict	0.125000
version used is	gof gcc	0.027778
is only used to determine the broadcast pattern	tensor adv index broadcastable pattern	0.066667
the replacement if the ops in the	replace validate replace all	0.050000
to find broken	compile find bad optimizations2 order	0.333333
a __thunk_trace__	hook type	0.333333
tuple of integers indicating the version	code cache version apply	0.125000
multiplication by a scalar on the output	gpuarray alpha	0.142857
the value after the import of	default filter	0.040000
reorder the dimensions of this	tensor tensor py operators dimshuffle	0.019231
sample from one or more multinomial distributions	multinomial random_state size	0.333333
of ops contained within the subgraph	gof ops	0.083333
the dimensions of	py operators	0.015625
raise baddestroymap if	compile check	0.166667
symbolic scalar	tensor scalar name dtype	0.166667
the args	scan execute node args	1.000000
version	gof	0.002381
addition (inplace on a)	add inplace a	1.000000
perform the	perform node x	0.166667
prod	prod axis dtype	1.000000
as replace_all_validate revert the replacement	replace all validate remove fgraph	0.111111
by one-dimensional slices in pvals	n pvals	0.125000
if necessary update dr_vals	node storage_map r_vals dr_vals	0.250000
convolution gradinputs	get conv gradinputs	0.500000
function must return a thunk that is a	thunk	0.021277
return full path	module name from	0.076923
get the 0 based level of	get depth	0.050000
add the tensor operators	tensor	0.006431
print the mflops	gpu corr3d mm flops inp outp	0.125000
kernel that can be	kernel	0.133333
min for the minimum in	tensor minimum x	0.142857
important note this function uses set and dictionary	out seq scan process node fgraph node	0.142857
this is just speed opt	sigmoid	0.111111
cosine of a (inplace on a)	tensor cos inplace a	1.000000
symbolic row variable (ndim=2 broadcastable=[true false])	row	0.034483
of headers that are needed by one or	headers	0.038462
orig	orig	1.000000
merge	optimizer merge	0.200000
choices	choices	1.000000
constant	get constant	1.000000
exp	exp	0.833333
pattern of subfgraph	pattern node	0.125000
will print a warning message on the first	gof deprecated filename msg	0.041667
required return c code to declare variables that	gof clinker type c declare name sub	0.333333
basic slow python 2d or 3d	img kern mode dilation	0.250000
incomplete qr decomposition	qrincomplete	1.000000
this is the equivalent of	to gpulocal	0.055556
given axis	argmin x axis	0.500000
this object into its key_pkl file	gof key data save pkl	0.500000
callcount	callcount	0.714286
some elementary validations on the inner graph	inner graph	0.035714
this op	gof clinker op c code cache	1.000000
stack trace from one or	copy stack trace	0.055556
shape or the	shape	0.010204
the inner	scan validate inner	0.142857
an exception while annotating	exc_info storage_map	0.250000
maps from variable and apply nodes	get equiv	0.142857
this variable optionally inserting	py	0.014286
raise baddestroymap	check inputs	0.125000
fgraph and a	fgraph outputs_to_disown	0.047619
more multinomial distributions defined by one-dimensional slices in	tensor multinomial random_state	0.040000
sharedvariable constructor for scalar values default int64 or	scalar shared value name strict allow_downcast	0.200000
inverse error function for gpu	gpu erfinv	1.000000
only one client and that client is a	only	0.050000
a type that allows no values	null type	0.500000
complex-valued tensor from polar	from polar	0.250000
sine of	tensor sin	1.000000
of	py	0.014286
of	tensor py operators	0.015625
operation to wait on a previously received array	mpirecv wait	0.045455
would be a legal	is valid	0.250000
return	gof	0.023810
unknown variables and apply_nodes to this graph	graph	0.016393
unroll_bsize	unroll_bsize	1.000000
we can't change the value after the import	config param init default	0.040000
function performs the svd	svd	0.034483
reorder the dimensions of this variable	py	0.014286
code for our fgraph	gof clinker get dynamic module	0.200000
to help the navigator deal	navigator optimizer attach updater fgraph	0.038462
to compute the	nnet	0.064516
broadcasted	operators	0.017241
still in the	fgraph replacements	0.250000
must return a thunk	thunk	0.021277
of the last	last	0.076923
remove subtensor/advancedsubtensor1 if it takes	tensor local useless subtensor node	0.200000
help the navigator deal with	navigator optimizer	0.037037
of a tensor input	input	0.071429
pushing out the variables inside the	out	0.018519
function for diagonalsubtensor and	tensor nnet get diagonal subtensor	0.083333
necessary update dr_vals	compile check inputs node storage_map r_vals dr_vals	0.250000
that resembles the python 3 namespace	moves urllib	0.038462
find broken	find bad	0.333333
the	destroy	0.019417
track less frequent op	get clients2 node	0.200000
is a thunk that operates on the	gof linker make thunk	0.045455
inputs	inputs outputs	0.066667
a != b inplace on a	tensor neq inplace a b	0.500000
the stack trace from one	stack trace	0.055556
this op	gof clinker op	0.333333
-> list of variables [v1 v2 v3 ]	gof vm linker compute gc dependencies variables	0.250000
a variable with the -x pattern	tensor nnet is neg	0.166667
the replacement if the ops in the	replace all	0.050000
dimensions of this variable	tensor tensor py operators dimshuffle	0.019231
given axis es	axis dtype	0.083333
in the original graph	inputs outputs copy_inputs_and_orphans memo	0.029412
this to expm1	local expm1 node	0.066667
dense vector	x s	0.142857
conda offers to make	to os environ pathlist var newpath	0.038462
arctangent of	tensor arctan2	1.000000
only if this enum	gof enum type	0.166667
then replace it with a triangular solve	linalg tag solve triangular node	0.142857
fill inputted tensor with the assigned value	operators fill value	1.000000
the output error and exit code	output subprocess popen command	0.100000
to	to	0.140351
variables given input	variables	0.043478
avg	avg	0.833333
image shape of convolution gradinputs	conv gradinputs shape kernel_shape	0.500000
apply nodes according to a list	apply nodes inputs outputs cmps	0.050000
of	sparse	0.076923
that copies a vector to the diagonal of	alloc diag	0.027027
helper function for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor	0.083333
get_nodes	get_nodes	1.000000
offset	offset	1.000000
this apply instance	gof apply	0.181818
upper bound on the	bound	0.043478
hyperbolic sine of	tensor sinh	1.000000
total time icluding	total times	0.200000
to print the mflops	gpuarray base gpu corr3d mm flops inp outp	0.125000
use a simple algorithm	order reasons r_vals	0.333333
the type's	gpuarray gpu array	0.062500
return connection pattern of	from graph connection pattern	0.076923
directory and return full path of	module name from dir	0.071429
of lib directories that are needed by one	clinker header dirs	0.055556
function for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x	0.083333
the mflops	tensor nnet conv op flops inputs	0.125000
replace_all_validate revert the replacement if the ops	gof replace validate replace all validate	0.111111
this variable	gof variable	1.000000
with logsoftmax x 's grad	tensor nnet local logsoftmax grad node	0.200000
of this variable optionally inserting broadcasted	tensor tensor py	0.015873
params	params	1.000000
or 3d convolution for debugmode	base abstract conv conv	0.125000
used to determine the broadcast pattern for	tensor adv index broadcastable pattern a	0.066667
the	tensor tensor py	0.015873
apply nodes according to a list of comparators	sort apply nodes inputs outputs cmps	0.050000
wraplinker that runs	gof	0.002381
mini-batch of a stack of 2d inputs with	input_shape filter_shape	0.018519
baddestroymap if	inputs node	0.100000
compute a batched	tensor batched	0.333333
convert radian a	tensor rad2deg a	0.333333
output of scan return true iff the	scan	0.017241
gradients up to the end variables of a	wrt end	0.050000
[floor] division inverse of multiplication	int div	0.250000
viewmap	viewmap	1.000000
the dependence of nodes in a graph	gof make dependence	0.043478
compute the dot	nnet	0.032258
shape of all intermediate variables given input	tensor shape of variables fgraph input_shapes	0.100000
the svd of	svd	0.034483
the	alloc	0.025000
input nodes to output nodes of	gof	0.002381
a new variable to theano config	config var name doc	0.500000
in the default blas in	sdot	0.166667
the minimum in	tensor minimum	0.142857
followed by a modulo operation	modulo	0.250000
fill s v -> alloc(v shape s	tensor local fill to alloc node	0.250000
vector and t	node input_storage output_storage	0.038462
b), axis=0) -> elemwise{scalar op}	tensor local reduce join node	0.111111
the minimum	minimum	0.083333
the value after the import	init default filter	0.040000
inputs with a set of 2d filters	conv2d input filters	0.125000
the same	tensor shape feature same	0.333333
raise baddestroymap if	inputs node storage_map	0.166667
open windows	misc subprocess popen command	0.142857
when we overwrite the	tensor local useless inc subtensor node	0.066667
decorator to merge addition by a value on	merge cls alpha_in beta_in out_in	0.250000
of the specified pieces of vectors and	sparse block gemv make node o	0.066667
insert	compile insert	1.000000
set and dictionary data structures	push out non	0.125000
to represent the dependence of nodes in	gof make dependence	0.043478
the cache	call cache	0.200000
is	graph to gpulocal opt	0.055556
view	view tree set	0.500000
anymore and should be removed	outs	0.050000
hyperbolic cosine of a (inplace on a)	tensor cosh inplace a	1.000000
hash equal for	tensor tensor type hash	0.166667
for same kinds	tensor type	0.034483
the svd on	tensor svd a	0.200000
ops in the list remove are still in	remove fgraph replacements remove reason	0.055556
b), axis=0) -> elemwise{scalar op} a	tensor local reduce join	0.111111
[true] division inverse of multiplication	true div	0.250000
that will be turned into macros for use	cop get	0.033333
to allow it to be reused in scalar	gof clinker cmodule key fgraph no_recycling compile_args libraries	0.200000
uses the topooptimizer from the	gof in2out	0.055556
rounding than numpy round half	round half	0.100000
impls	impls	1.000000
complete hashable signature	clinker cmodule key	0.166667
determine the broadcast pattern for	tensor adv index broadcastable pattern a idx	0.066667
> b inplace on a	gt inplace a b	0.500000
for convop that unroll the batch	unroll batch kern d unroll_bsize unroll_ksize	0.125000
of arguments to pass to helper_c_code	get helper c code args	0.250000
add	subtensor add	1.000000
is the equivalent of	gpulocal	0.055556
iff x and y	numpy x y	1.000000
the shape feature	compile shape	0.250000
loop executes	loop	0.027778
clip x to be between min	tensor clip x min	0.500000
a matrix	matrix	0.055556
print a warning message on the first call	gof deprecated filename msg	0.041667
type	has type	1.000000
attempt to convert x into a	x context_name	0.100000
infer the	infer	0.083333
the folowing changes in the graph	local mul switch sink node	0.045455
dimshuffle is inside an alloc and only	tensor local alloc dimshuffle node	0.166667
update cache data by walking the cache	gof module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
sample n (n needs to	size n	0.090909
which each row is a mrg stream	sandbox mrg random streams	0.033333
comparing tensorconstant instances	constant	0.016667
transform a subgraph whose output is node	local optimizer transform node	1.000000
from	op from	1.000000
a	constant	0.016667
get the right	get	0.020833
raised by get_scalar_constant_value	error	0.025000
clip	clip	1.000000
writeme : explain	calculate reallocate info order fgraph storage_map compute_map_re	1.000000
!=	tensor neq	0.666667
failure_callback for	navigator optimizer warn inplace exc	0.500000
return unique node id	d3viz py dot formatter node id node	1.000000
elemwise degree to	sparse deg2rad x	0.333333
convolution gradient with respect to the inputs	dnn conv grad i	0.125000
an fgraph and a list of	fgraph outputs_to_disown	0.047619
a convolution	dnn conv	0.090909
output dimensions	get output	0.047619
up to the end variables of a	grad wrt end	0.050000
of multiplications and/or divisions	tensor local greedy distributor node	0.166667
listeners to help the navigator deal	gof navigator	0.038462
the constant scalar 0-d value underlying	core get scalar constant value	0.333333
as a decorator or context manager	flags	0.062500
computes the output dimensions	op get output	0.047619
a compiled module from the loaded	get module name	0.333333
of this op could	l op	0.033333
the compile lock to	add to	0.142857
functiongraph that has	gof	0.002381
input a n-d tensor where	input ws ignore_border stride	0.181818
elementwise [floor] division inverse of multiplication	tensor int div a b	0.333333
c code	c code	0.277778
python not the other implementation of mod	mod c	0.125000
indices obtained by iterating	keepdims	0.052632
profiling to print the mflops	tensor nnet conv op flops inputs	0.125000
asserts	assert	0.111111
and reduce pattern has functioning c code	gpuarray gpu careduce cuda supports c code inputs	0.250000
replace_all_validate revert the replacement if the ops in	gof replace validate replace all validate	0.111111
2d kernel for bilinear upsampling this	bilinear	0.019231
if allow_override is false we can't change the	filter allow_override	0.142857
in profiling to print the mflops	flops	0.076923
argmax	argmax	0.466667
output_specs	output_specs	1.000000
to find	compile find bad optimizations2 order	0.333333
a new variable from	new	0.058824
ignore_trees-related functionality	optimizer attach updater fgraph importer pruner chin	0.250000
dot product when one or all operands is	dot x y	1.000000
det x and there is already an	linalg local det	0.166667
we can't change the value after the	param init default filter	0.040000
equivalent of localoptgroup for	opt group	0.043478
base class for gpucorrmm, gpucorrmm_gradweights and gpucorrmm_gradinputs	base gpu corr mm	0.250000
e^a - 1	expm1 a	1.000000
x and there is already an l=cholesky	linalg local	0.142857
the 2d kernel that can be	kernel 2d	0.050000
leftdims +	leftdims	0.090909
this variable	py operators	0.015625
output dimensions of convolving an image of shape	nnet conv op get output	0.047619
shared variable to 0	shared variable zero borrow	0.200000
n (n needs to be	n	0.055556
apply instance in a new graph	apply clone with new inputs inputs	0.500000
|a| tensorvariable overloads the tensorvariable	abs a	0.333333
or more multinomial	tensor multinomial	0.037037
dnn conv algo_bwd	safe no dnn algo bwd algo	0.166667
it only on cpu here	tensor local pow specialize	0.250000
helper_c_code	gpu inc subtensor get helper c code	0.333333
to the fgraph this is the place to	fgraph	0.012195
operand optimized for calculating the dot product	dot	0.035714
one or more multinomial distributions defined by	tensor multinomial	0.037037
re-initialize each random stream	tensor random streams seed seed	1.000000
name in mode	name	0.011111
i of	i	0.076923
len img2d shape ==3 we todo	nnet conv op perform node inp out	0.166667
pattern for advancedsubtensor output	pattern	0.028571
specified pieces of vectors and matrices	sparse block gemv make node o	0.066667
of policies to name r sub	policy policy r name sub	0.250000
while annotating	op node thunk exc_info storage_map	0.250000
the connection pattern	io connection pattern	0.055556
into two kinds scalar constants and the rest	rest inputs elemwise	0.125000
wait on a previously	wait	0.045455
pieces of vectors and matrices	sparse block gemv make node o w	0.066667
filters with a	conv3d signals filters	0.111111
lib directories that are needed	clinker header dirs	0.055556
in profiling to print the mflops	conv op flops	0.125000
this function is basically	extract constant x elemwise only_process_constants	0.058824
this compiles the source code for	compile cmodule location	0.038462
makes the folowing changes in the graph	mul switch sink	0.045455
l has any duplicates (according to	has duplicates l	0.111111
the hack in profiling to print the mflops	abstract conv flops inp outp	0.125000
outputs and the stopping condition returned	and outputs ls	0.166667
with the *args directly	sparse local csm properties csm	0.142857
inserting broadcasted dimensions	tensor py operators dimshuffle	0.019231
help the navigator deal with the	navigator	0.032258
addition of a sparse matrix and a	add	0.034483
search	search	0.750000
of	py operators	0.015625
dictionary of arguments to pass to helper_c_code	tensor inc subtensor get helper c code args	0.250000
value after the import	init default filter	0.040000
n-d tensor where	ws ignore_border stride	0.090909
of this	py operators	0.015625
by doing a recursion over the input	permute row elements rec	0.047619
function :func neibs2images <theano sandbox neighbours neibs2images>	nnet neibs2images neibs neib_shape original_shape mode	0.333333
source code for this linker and returns	gof clinker compile cmodule location	0.038462
is only used to determine the	tensor adv index broadcastable	0.050000
method to override this should return an iterable	gpuarray gpu kernel base gpu kernels node name	0.166667
base 10 logarithm of a	tensor log10 a	0.500000
this optimization makes the folowing changes in	local mul switch sink	0.045455
sigmoid	sigmoid node	0.100000
detect log(softmax x and replace it with logsoftmax	logsoftmax	0.076923
an apply_node recursively search from this node to	import apply_node check reason	0.066667
if the given graph contains a cycle parameters	gof contains cycle	0.333333
values of a shared	shared	0.062500
detect if the g++ version	gof	0.002381
dependencies	dependencies	1.000000
destroy_map	bad	0.013158
dimshuffle and index the	local dimshuffle	0.052632
original	inputs outputs copy_inputs_and_orphans memo	0.029412
feature should remove	gof feature	0.125000
multinomial distributions defined by one-dimensional	tensor multinomial random_state	0.040000
type	type c	0.071429
slices in pvals	pvals	0.071429
this generates the c code for corrmm (direction="forward"),	corr mm c code	0.090909
loop executes code	make reordered loop	0.111111
mini-batch of	input_shape filter_shape	0.027778
every node that	gof function graph	0.031250
perform the permutation by doing a	perform node x y	0.166667
a	make	0.035714
hyperbolic tangent of a (inplace on a)	tensor tanh inplace a	1.000000
reproducible case for problems during	compile function dump filename inputs outputs mode	0.166667
an md5 hash that uniquely identifies a module	gof get module hash src_code key	1.000000
gpuincsubtensor	inplace setsubtensor node	0.250000
the hack in profiling to print the mflops	conv op flops inputs	0.125000
cache "filename" as a pickle	gof call cache persist filename	0.250000
same on all device we do it	device node	0.045455
baddestroymap if	compile	0.076923
tile input array x according to reps	tensor tile x reps	1.000000
with respect to wrt,	core subgraph grad wrt	0.062500
is	node	0.014815
that would be a	gof	0.002381
function-constructor for	compile pfunc params outputs mode updates	0.200000
the r operation on f wrt to wrt	rop f wrt	0.200000
after pad_dims	unpad	0.166667
not attempting to use dnn conv algo_bwd	core safe no dnn algo bwd algo	0.166667
structures	non	0.071429
inserting broadcasted	tensor py	0.015873
return a	name x	0.333333
to wrt, computes gradients	subgraph	0.047619
if the caller is replace_all_validate just raise the	gof validator validate fgraph	0.125000
maximum elements and their	tensor max and argmax a	0.250000
use dnn	no dnn	0.125000
sort	sort axis kind	1.000000
dictionary unary_out_lookup({int8 int32 float32 complex128})	unary out lookup	0.250000
function to concatenate tensortypes	join	0.090909
reorder the dimensions	tensor	0.006431
all symbolic variables in inputs to	inputs	0.012658
attach_feature the method that	gof	0.002381
b are unified	b	0.014925
output error and exit code in	output subprocess popen command	0.100000
replace_all_validate revert the replacement if the	validate replace all validate	0.111111
variable names when persisting	variable id	0.250000
important note this function	push out seq scan process node fgraph	0.142857
particular stream	tensor random streams setitem item val	0.142857
computes the confusion	tensor nnet confusion	0.333333
this function performs the svd	tensor svd	0.200000
add a	add	0.068966
apply	apply	0.183333
the graph and get a memo a dict	function graph	0.040000
dump this	gof	0.002381
python not the other implementation of mod	scalar mod c	0.125000
variable that represents their unification	gof unification	0.125000
readable string representation of self fgraph	scalar composite init	0.200000
strings check if converting to type2 from	type2	0.050000
toposort return an ordering of the	toposort	0.076923
stopping condition returned	ls	0.090909
of suitable dummy values	meta optimizer provide	0.200000
badviewmap exception when it detects	compile check viewmap node storage_map	0.111111
for comparing tensorconstant instances	tensor	0.003215
the image	1axis	0.142857
if true	verbose	0.166667
the platform-dependent gcc	gof get gcc	0.333333
variables var1 and	graph var1	1.000000
to the diagonal of an empty	diag	0.023810
implementation of mod	mod c code node	0.125000
first functiongraph that has ever	gof	0.002381
the inner graph to ensure that it is	scan validate inner graph	0.035714
start gradients up to the end	grad wrt end start	0.166667
an	node inputs	0.086957
matrix solve operation c =	solve	0.032258
distribution defined	streams	0.076923
inplace	gpuarray inplace	0.200000
disabled by default that removes	remove	0.035714
it with a triangular	triangular	0.076923
or more multinomial distributions defined by one-dimensional	tensor multinomial	0.037037
user is not attempting to use dnn	no dnn	0.125000
scan_module	scan_module	0.625000
the inner-most loop executes	tensor make reordered loop	0.111111
add	gof add	1.000000
filters	tensor nnet conv2d input filters	0.125000
of headers	clinker headers	0.047619
for	value f	1.000000
on the	gpu	0.011765
two kinds scalar constants and the rest	rest	0.076923
specific to the apply to be inserted in	apply	0.016667
with respect to wrt,	subgraph grad wrt	0.062500
this function uses set and dictionary data structures	out non seq scan	0.125000
of cost and/or from existing start gradients up	start cost	0.100000
inputs and put the variables	gof pure op perform node inputs output_storage params	0.047619
node inputs[i] to new_r	gof function graph change input node i new_r	0.500000
a to degree	a	0.008065
lib directories that	clinker lib dirs	0.055556
return a symbolic row variable	tensor row name dtype	0.050000
to generate c	shape i c	0.250000
litterals to	tensor	0.003215
the convolution gradient with respect to	dnn conv grad	0.125000
z <- beta *	z	0.111111
the folowing changes in the	mul switch sink node	0.045455
graphs	graphs additional_inputs	1.000000
a subtensor is inside a dimshuffle	subtensor node	0.066667
of numpy ones_like	ones like model dtype opt	0.333333
special compound	softmax argmax1hot	0.083333
the "reverse-mode"	perform node inputs outputs	0.333333
c type	type c	0.071429
in the list remove are still in	replacements remove reason	0.055556
string specifying to the user what obj is	core min informative str obj indent_level _prev_obs _tag_generator	0.333333
important note this function uses set	out seq scan process node	0.142857
helper function to generate permutations from integers	tensor permutation helper random_state n shape	0.333333
the given axis es of a	axis dtype	0.083333
construct a variable	variable x	0.083333
the method that attaches	gof bookkeeper on attach fgraph	0.142857
hack in profilemode to print the mflops	gpuarray base gpu corr mm flops inp outp	0.125000
cond	tensor switch cond	0.500000
profiling to print the mflops	tensor nnet base abstract conv flops inp outp	0.125000
cache or the	module cache	0.071429
kerns	kerns	1.000000
constant inputs to	constant inputs node	0.125000
not the same on all device	device	0.076923
defined by one-dimensional slices in pvals	n pvals	0.125000
one tensor	y	0.052632
of specific ops of	trace f_or_fgraph ops_to_check bug_print	0.035714
scale each columns	col scale x s	1.000000
makes the folowing changes	tensor local mul switch sink node	0.045455
for graphtogpu	gpulocal	0.055556
the image shape of	shape 1axis kernel_shape	0.250000
of another op that takes the	op sub	0.066667
this type	gof clinker type c	0.166667
profilemode to print the mflops	corr mm flops inp outp	0.125000
return a code	name	0.022222
parse the tree and figure out which nodes	scan_module traverse out x x_copy d	0.047619
stack trace from one or more tensor variables	gof copy stack trace	0.055556
of the destroyhandler	destroy handler	0.055556
full path of the	gof module	0.058824
of this variable optionally inserting broadcasted	tensor tensor py operators dimshuffle	0.019231
for for theano scalar scalar	scalar	0.017857
return a list of shape tuple or	infer shape	0.066667
module is a package	importer is package	0.500000
to help the navigator deal with	gof navigator optimizer attach updater	0.038462
is the equivalent	gpulocal opt	0.055556
min	min axis keepdims	1.000000
this function is only used to determine	adv index broadcastable	0.050000
this apply instance in	gof apply clone with	0.166667
tensortypes along the given axis	x shift axis	1.000000
localoptgroup	graph to gpulocal opt group	0.055556
replace_all_validate revert the replacement if	gof replace validate replace all validate remove	0.111111
of 3d	conv3d	0.076923
complex conjugate	conj	0.200000
optimization makes the folowing changes	tensor local mul switch sink	0.045455
uniform into sample from	from uniform	0.200000
a n-d tensor	ws ignore_border stride	0.090909
implements the "reverse-mode" gradient for the	grad perform node inputs	0.083333
list of headers that are	clinker headers	0.047619
wants to add some requirements to the fgraph	optimizer add requirements fgraph	0.333333
theano graphs represent the same computations	computations xs ys in_xs in_ys	0.333333
list of variables	gof variables	0.125000
diagonal of an	alloc diag	0.027027
builds the 1d kernel that can	kernel 1d	0.050000
exception some perform() or c_code() modified an input	map	0.047619
use with helper_c_code	helper c code	0.142857
and applies them	local	0.014085
name the object	name obj	0.111111
raise baddestroymap	check	0.083333
convert data to something which can be associated	tensor tensor type filter data	1.000000
tensor input	input	0.095238
this function compute the	nnet	0.032258
would be useless	call	0.166667
the dependence of	make dependence	0.043478
to use dnn	core safe no dnn	0.125000
respect to	gpu dnn	0.133333
the context object mapped	gpu array type context	0.090909
the dimensionality of the var is equal to	is flat var	0.200000
module is a package	path importer is package	0.500000
a badviewmap exception when it detects the following	compile check viewmap node storage_map	0.111111
and/or from existing start gradients up to the	start	0.040000
inner nit_sot	inner sitsot	0.083333
broadcast pattern for advancedsubtensor output variable	pattern a idx	0.066667
in the theano enumeration types wrapped into	type enum from	0.333333
module from	module from	0.166667
value after	init default	0.040000
indicating the version	c code cache version apply node	0.125000
search through	search	0.125000
validations on the inner graph to	validate inner graph	0.035714
match a variable with the -x pattern	nnet is neg	0.166667
function as its implementation	as	0.024390
that copies a vector	alloc	0.012500
broadcastable in the specified axes	addbroadcast x	0.142857
the struct initialization code	init code struct node	0.125000
of the last access of a given	last access time path	0.040000
the apply to	apply node	0.031250
x is an input vector and	node	0.007407
a new variable	new	0.058824
elemwise round half	sparse rint	0.250000
be removed and	compress outs	0.076923
apply_node recursively search from	apply_node check	0.066667
specifyshape how to generate	c_support_code_apply	0.111111
nodes	gof	0.004762
grad	grad dict var_to_app_to_idx grad_dict wrt	1.000000
for the existence of the __unify_walk__ method for	unify walk a b u	0.037037
the version	clinker object c code cache version	0.125000
list of lib directories	clinker lib dirs	0.055556
specified pieces of vectors	sparse block outer make node o x y	0.066667
gradient function should return	tensor matrix inverse r op	0.500000
the context object mapped	gpuarray gpu array type context	0.090909
return connection pattern of subfgraph defined by	graph connection pattern node	0.076923
support the types involved in this node	tensor inc subtensor do type checking node	0.250000
i	i r i	0.500000
a random	random	0.055556
to the one hot	to one hot	0.142857
of convolution gradinputs	conv gradinputs	0.500000
to a max	max	0.062500
nit_sot output of scan return true iff	scan	0.017241
optimizer which can be referred to by name	compile register optimizer name	1.000000
split x	tensor split	0.500000
the view_map	view	0.022727
see theano tensor std	tensor py operators std axis ddof	1.000000
represent the dependence of nodes	make dependence	0.043478
all sigmoid to	sigmoid	0.055556
from type1 constitutes an upcast	tensor is an upcast type1	0.500000
directory and return full	module name from dir	0.071429
validate	validate	0.454545
"reverse-mode"	perform node inputs outputs	0.333333
of apply nodes according to	apply nodes inputs outputs cmps	0.050000
dimshuffle and index	local dimshuffle	0.052632
axis=l) ->	tensor local	0.025641
of	graph to	0.055556
elementwise division (inplace on a)	tensor int div inplace a b	1.000000
converts self _rop_op from user supplied form to	compile op from graph recompute rop	0.200000
raise	raise	0.461538
reorder the dimensions of this variable	tensor py operators dimshuffle	0.019231
connection pattern of subfgraph defined by	graph connection pattern	0.076923
instance of this	link_kwargs optimizer	0.333333
the contents of a cache directory	dir dirname err files	0.166667
specific ops of a	trace f_or_fgraph ops_to_check bug_print	0.035714
that wasn't in the view_map	bad	0.013158
graph for convolving a mini-batch of	input_shape filter_shape	0.027778
overwrite	useless inc subtensor	0.125000
version of platform	platform	0.083333
return	gpu	0.011765
y with length one the axes	y axis	0.125000
1) times from a multinomial	multinomial	0.024390
and t is a	node input_storage output_storage	0.038462
support version-based cache mechanism	code version version	0.333333
the named module is a package	six meta path importer is package fullname	0.250000
return a c contiguous version of the	contiguous	0.058824
ls	ls	0.454545
the named module is a package	importer is package fullname	0.250000
row variable (ndim=2 broadcastable=[true false])	row	0.034483
return a c contiguous version of	contiguous	0.058824
dimensions of this variable optionally inserting broadcasted	tensor py operators dimshuffle	0.019231
from variable and apply nodes	get	0.020833
to	gpu array	0.055556
the initial value for	gpu	0.011765
lazy loading of moved objects in six moves	module six moves urllib error	0.142857
the template filled by broadcasting value	tensor broadcast like value template fgraph	0.125000
c-implementation of the dot product	dot csr c code node name	1.000000
dependence of nodes	gof make dependence	0.043478
occurrences of each value in array of ints	bincount x weights minlength assert_nonneg	0.125000
are	check	0.083333
the tensor	tensor	0.006431
batch normalization of the	nnet batch normalization	0.125000
function :func images2neibs <theano tensor	images2neibs	0.111111
dot product of the specified pieces of vectors	sparse block outer make node o	0.066667
type that represents an array	array type	0.055556
a dummy file with these flags	flags cls flag_list preambule body	0.250000
disabled by default that removes all asserts from	tensor local remove all assert	0.055556
add tag trace to an	add tag trace thing user_line	0.166667
given an apply_node recursively search from	apply_node	0.050000
scan_args	scan_args	1.000000
this method	gof	0.002381
for	graph to	0.055556
for the eigensystem	eigh	0.125000
boundvariable(other_object)	fv o u	0.200000
"init_code" together	code struct	0.500000
replace a leaf of	tensor nnet replace leaf	0.100000
type's :attr context_name	type	0.011905
representing a computation on a certain	array	0.041667
replace element i of shape_of[r] by s_i	shape feature set shape i r i s_i	1.000000
ultra_fast_sigmoid	local ultra fast	0.500000
the replacement if the	gof replace validate replace	0.050000
connection pattern	op from graph connection pattern	0.076923
return string representation of broadcastable	broadcastable to str b	0.500000
wrt, computes gradients of	core subgraph grad	0.062500
inplace optimization that deals with allocempty	gpuarray inplace allocempty op idx	0.166667
the diagonal of an	diag	0.023810
arctanh	arctanh	0.857143
a graph	destroy	0.009709
as python not the other implementation of mod	scalar mod c code node	0.125000
parse	parse mul	0.500000
n-th order discrete difference	diff x n	0.333333
helper function to generate permutations from integers	permutation helper random_state n shape	0.333333
type	gof params type has type	1.000000
value after the	core config param init default	0.040000
value for a	value a	0.250000
which will print a warning message	deprecated filename msg	0.041667
return	tensor	0.006431
the output dimensions of	op get output	0.047619
variable v if v is the output of	v	0.011111
of moved objects in six moves urllib_response	module six moves urllib response	0.333333
as its implementation	compile as	0.050000
apply to be inserted	apply node	0.031250
in august 2011	tensor load shared variable val	0.142857
implementation of mod	mod c code node name inputs	0.125000
helper function to draw random numbers using numpy's	helper random_state a replace p	1.000000
string specifying to the user what obj	min informative str obj indent_level _prev_obs _tag_generator	0.333333
is not attempting to use dnn conv workmem	core safe no dnn workmem workmem	0.166667
return a code string specific to the	node name	0.066667
the values of a shared	compile shared	0.166667
of the specified pieces of vectors and matrices	sparse block outer make	0.066667
only used to determine the broadcast pattern	adv index broadcastable pattern a	0.066667
if the g++	gof gcc	0.027778
the inputs and put the variables in the	pure op perform node inputs output_storage params	0.047619
a subgraph defined by given inputs and outputs	inputs outputs	0.066667
the mean value along the	mean	0.062500
is the equivalent of localoptgroup	opt group	0.043478
init	init	0.294118
failure_callback for	gof navigator optimizer warn inplace exc	0.500000
copies the stack trace from	copy stack trace	0.055556
that initializes py_name from storage	gof get c extract r name	0.250000
output dimensions of	nnet conv op get output	0.047619
f wrt to wrt	f wrt	0.200000
explaining the output of is_valid_value	pure type value validity	1.000000
x is an input vector and	node input_storage output_storage	0.038462
copy this function copied function	compile function copy	0.333333
y with	y	0.026316
for convolving a mini-batch of a	input_shape filter_shape	0.027778
frozendict subclass that maintains key order	frozen ordered dict	1.000000
alias that	bad view	0.027027
of l{codeblock} instances returns a string that	gof code gen blocks	0.050000
converting to type2 from type1 constitutes an upcast	is an upcast type1 type2	0.333333
hack in profiling to print the mflops	op flops inputs	0.125000
the compile lock to	to	0.017544
orv	unify walk	0.500000
to use	gpuarray gpu inc	0.333333
a nested loop over several arrays and	loop	0.027778
required return c code to extract	type c extract name	0.250000
this op __init__ fct don't have the same	composite make new inplace output_types_preference name	0.142857
factor takes as input	tensor signal pool 2d input	0.090909
for a convolution with the specified parameters	gpu dnn conv get	0.200000
r operation on f	core rop f	0.166667
performs batch normalization	nnet batch normalization	0.125000
compute conv output	tensor nnet conv2d	0.333333
expects the compile lock to be held	module cache add to cache module key module_hash	0.166667
this class	gof clinker object	0.250000
reorder the dimensions of this variable optionally	operators dimshuffle	0.019231
create a new instance of this	clone link_kwargs optimizer	0.111111
the dependence of nodes in a	make dependence	0.043478
list to	gemm from factored list	0.500000
if necessary update dr_vals	storage_map r_vals dr_vals	0.250000
optional	op c	0.500000
dtype as	dtype	0.022727
which will print a warning message on the	gof deprecated filename msg	0.041667
apply to be inserted	apply	0.016667
self _grad_op from user supplied form to type	compile op from graph recompute grad op	0.200000
:type cost scalar 0-dimensional variable	core hessian cost wrt consider_constant disconnected_inputs	0.500000
dot product of the specified pieces of vectors	sparse block gemv make	0.066667
without replacement from	choice from	0.333333
reordered	reordered	1.000000
b), axis=0) -> elemwise{scalar op} a b	local reduce join node	0.111111
this	py	0.014286
of the type	type	0.011905
each element in y	y	0.026316
duplicate this apply instance	gof apply	0.090909
of this op could	tensor prod l op	0.033333
tensor	tensor tensor py operators	0.015625
for same	type	0.011905
tries to recognize the updates ordereddict	get updates	0.034483
this convert allocempty to alloc	alloc empty to zeros	0.333333
reshapes the output after pad_dims	unpad dims output input leftdims	0.333333
false we can't change the value after the	default filter	0.040000
to find broken optimizations	compile find bad optimizations2	0.333333
return complex-valued tensor from polar coordinate specification	from polar abs angle	0.250000
one or more multinomial distributions defined	multinomial	0.024390
of this variable optionally	tensor py operators	0.015625
writeme	linker	0.142857
into a gemm	tensor gemm	0.166667
idx_list with constant inputs	constant idx inputs	0.250000
add some requirements to the fgraph this is	add requirements fgraph	0.333333
this function expects the compile lock to	module cache add to	0.142857
with logsoftmax x	local logsoftmax node	0.142857
generate permutations from integers	tensor permutation	0.166667
for corrmm (direction="forward"), corrmm_gradweights	corr mm	0.083333
a with	a replace	0.333333
elementwise division (inplace on a)	true div inplace a b	1.000000
failure_callback	gof navigator optimizer warn inplace exc	0.500000
fgraph and a list of	fgraph	0.012195
a list of shape	shape feature default infer shape	0.066667
output error and exit code in a	misc output subprocess popen command	0.100000
the dimensions of this variable optionally inserting broadcasted	operators dimshuffle	0.019231
of the specified pieces of vectors and matrices	sparse block outer make node o x	0.066667
output of scan return true iff	push out scan	0.050000
scalar scalar and tensorvariable	scalar as common dtype	0.250000
dimensions	tensor py	0.031746
as replace_all_validate revert the replacement if	validate replace all validate remove	0.111111
depend only on non-sequences	non seq	0.111111
optionally inserting broadcasted	tensor py operators	0.015625
output dimensions of	get output	0.047619
a basic theano op that will call	op itypes otypes infer_shape	0.047619
print list	gof print	1.000000
if the named module is a package	path importer is package fullname	0.250000
important note this function uses set and	process node fgraph	0.142857
calls subprocess_popen returning the output error	output	0.017241
partition a list of variables into two kinds	tensor scalarconsts	0.125000
this function tries	top_shape border_mode	0.166667
for	base abstract	0.500000
the ignore_trees-related functionality	updater fgraph importer pruner chin	0.250000
the	py operators dimshuffle	0.019231
same on all device we do it only	device node	0.045455
a symbolic row	row	0.034483
to a new fgraph check that 1) this	gof	0.002381
remove incsubtensor when we overwrite the full inputs	tensor local useless inc subtensor node	0.066667
output type dtype and broadcast there is no	tensor local useless alloc node	0.333333
profiling to print the mflops	conv op flops inputs outputs	0.125000
es of a	ddof keepdims	0.250000
attempt to replace a leaf of a	replace leaf arg leaves new_leaves op	0.250000
a -1 and converts this to expm1 a	expm1 node	0.066667
the	tensor tensor py operators dimshuffle	0.019231
broadcasted dense vector element wise	sdcsr	0.250000
x	nnet local	0.400000
to replace	tensor nnet replace	0.250000
create an functiongraph	gof function graph init	0.333333
should remove any dynamically added functionality	gof history on detach fgraph	1.000000
node that	gof function graph	0.031250
to be held	add to cache module key module_hash	0.166667
infer the number of dimensions	tensor infer	0.142857
value in array of ints	bincount x weights minlength assert_nonneg	0.125000
metaclass	metaclass	1.000000
c code when doing constant folding of	elemwise python constant folding	0.142857
and replace it with logsoftmax x	local logsoftmax node	0.142857
replace it with logsoftmax x 's grad	local logsoftmax grad node	0.200000
number to string by rendering it in base	number number	0.125000
1/(1+exp x ->	local inv 1 plus exp	0.333333
total time icluding the	compute total times	0.200000
value after the import of	default	0.030303
see theano tensor argmin	tensor py operators argmin axis keepdims	1.000000
override clinkerobject c_support_code	type c support code	1.000000
reduce pattern has functioning c code	gpu careduce cuda supports c code	0.250000
implement x[ilist] where ilist is	advanced subtensor1	0.200000
op around a	op	0.009174
op whose incoming gradient	softmax 1hot	0.333333
base class for corrmm, corrmm_gradweights and corrmm_gradinputs	base corr mm	0.250000
convert python	constant	0.016667
tile input array x according to	tensor tile x	0.500000
is the equivalent of localoptgroup for graphtogpu	gpulocal opt group	0.055556
unfortunately conda offers to make	to os	0.038462
the c code for this	init c code	0.333333
graph	gof	0.002381
the last access of a given	last access	0.040000
see min for the minimum in one	tensor minimum x y	0.090909
config blas ldflags	tensor ldflags	0.250000
exception some perform() or c_code() modified an input	bad destroy map	0.142857
the compile lock to	cache add to	0.142857
convert python	tensor make constant	0.100000
patch variable so that its type	gof pure type convert variable	1.000000
the gradients along the axis that was used	grad inputs g_outputs	0.076923
l has any duplicates (according to __eq__)	has duplicates l	0.111111
return a symbolic row	tensor row name	0.050000
unknown variables and apply_nodes to this graph	gof function graph import	0.125000
decorator to merge multiplication by a scalar on	gpuarray alpha merge cls alpha_in beta_in	0.200000
in which each row is a mrg	mrg	0.076923
should be removed	scan_module compress outs	0.076923
3d convolution for	nnet base abstract conv conv	0.125000
lock to be held	cache add to cache module key module_hash	0.166667
pieces of vectors	sparse block gemv make	0.066667
c header for openblas	openblas	0.111111
memory alias	bad	0.013158
with a triangular solve	linalg tag solve triangular node	0.142857
that attaches	gof bookkeeper on attach	0.142857
feature	feature on	0.200000
a value	value	0.043478
the folowing changes in the graph t	local mul switch sink node	0.045455
for	value	0.043478
litterals to	make constant	0.100000
as replace_all_validate revert the	all validate	0.166667
return a version of var transferred	transfer var	0.100000
alias that wasn't in the view_map	bad view	0.027027
i	set shape i r i	0.500000
can't change the value after	core config param init default	0.040000
by this	gof seq optimizer	0.200000
on the inputs and	node inputs	0.043478
log gamma function	tensor gammaln inplace a	1.000000
node a clone	clone	0.020833
module is a package	compat six meta path importer is package	0.500000
converts a function into a basic theano op	op itypes otypes infer_shape	0.047619
false we can't change the value after the	init default filter	0.040000
symbolic variables in inputs to sharedvariable instances of	inputs	0.012658
and return full path	module name from	0.076923
softmaxgrad	softmax grad	1.000000
this	tensor py operators dimshuffle	0.019231
create a comparator	cmp	0.058824
add tag trace to an	gof add tag trace thing user_line	0.166667
nested loop over	loop	0.027778
loading of moved objects in six moves urllib_robotparser	module six moves urllib robotparser	0.333333
work for gpuincsubtensor	inplace setsubtensor	0.250000
removes useless dimshuffle	tensor local useless dimshuffle	0.500000
the gradient function should	matrix inverse grad inputs	0.500000
adds new optimization instances to a mode	compile mode register	1.000000
dictionary data structures	non seq scan	0.090909
for debugmode	base abstract	0.500000
install some functiongraph listeners to help the navigator	gof navigator optimizer attach	0.038462
error and exit code in	subprocess popen command	0.083333
retrive the context associated	context	0.035714
reshaped view/copy of this	tensor py operators reshape shape ndim	0.111111
need not be checked for nan and inf	compile is numeric value arr var	0.166667
of v by a with	v	0.011111
images2neibs <theano tensor	images2neibs	0.111111
scalars together into a	make	0.017857
replace_all_validate revert the replacement if the ops	replace all validate remove fgraph	0.111111
extract list of variables	variables and	0.250000
maps a failure code to the task that	gof cthunk find task	0.142857
is meant as a shortcut to opt	optimizer optimize fgraph	0.200000
this apply instance in a new graph	gof apply clone with new inputs	0.250000
list to	tensor gemm from factored list	0.500000
replace it with a triangular solve	solve triangular node	0.142857
dimensions of this variable optionally inserting broadcasted dimensions	tensor py	0.015873
a function that	gof	0.002381
pattern of subfgraph defined by inputs and	pattern	0.028571
of this op	prod l op	0.033333
to wrt, computes gradients	core subgraph grad	0.062500
a !=	tensor neq	0.333333
tensorvariable	make variable	0.166667
matrix solve operation c = a	solve	0.032258
a memory alias that wasn't in the	bad view	0.027027
to get the	get	0.020833
be inserted at struct	struct node	0.062500
of the exp x or -exp x patterns	is exp	0.333333
apply instance in a	apply clone with	0.333333
elementary validations on the inner	scan_module scan validate inner	0.142857
to make itself the default python	to os environ pathlist var newpath	0.038462
is the equivalent of	graph to	0.055556
of 2d filters	filters	0.064516
raised when	tensor error	1.000000
checks if theano graphs represent the same	xs ys in_xs in_ys	0.111111
help the navigator deal with	navigator optimizer attach updater	0.038462
the inner graph	scan	0.017241
the node i pairs such that node inputs[i]	gof function graph clients	0.100000
convolution for debugmode	abstract conv conv	0.125000
tuple of integers indicating the version	cache version apply	0.125000
complex-valued tensor from polar	complex from polar	0.250000
the outputs from the inputs	inputs outputs	0.066667
sandbox	sandbox	1.000000
given version	version	0.031250
the value after the	config param init default	0.040000
filled with ones closer to	ones shape dtype	0.200000
n_ones	n_ones	1.000000
should remove any dynamically added functionality	history on detach fgraph	1.000000
the dimensions	tensor tensor py operators	0.015625
profiling to print the mflops	conv op flops	0.125000
inner graph to ensure that	validate inner graph	0.035714
graph to	graph	0.016393
graph to ensure that it is	graph	0.016393
duplicate this apply instance in	gof apply	0.090909
override clinkerobject c_support_code	tensor type c support code	1.000000
by given inputs and	inputs	0.012658
logsoftmax x 's grad	local logsoftmax grad node	0.200000
get the 0 based level of the	type get depth	0.050000
expression vector 1-dimensional variable	jacobian expression	0.500000
of apply nodes according to a list	gof sort apply nodes inputs outputs cmps	0.050000
add f to :doc oplist	constructor f	1.000000
maximum elements and their	max and argmax a	0.250000
variable to	variable	0.022222
maps old nodes to	equiv check_integrity attach_feature	0.200000
op( host_from_gpu(), -> host_from_gpu(gpuop	gpuarray op lifter op cuda_only	1.000000
checks if theano graphs represent	xs ys in_xs in_ys	0.111111
diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x i0	0.083333
convert radian a to degree	tensor rad2deg a	0.333333
safely compute ceil(float_division a b	tensor ceil intdiv a b	1.000000
returns upper bound on the largest eigenvalue	bound	0.043478
leaf of a multiplication tree	leaf	0.066667
calculate the function on the inputs and put	gof pure op perform node inputs output_storage params	0.047619
unfortunately conda offers to make itself the	to	0.017544
this to expm1	tensor local expm1	0.066667
signature for	method decl	0.333333
loading of moved objects in six moves urllib_error	six moves urllib	0.090909
transfer data to	from host	1.000000
replace_all_validate	all validate remove	0.166667
optionally inserting broadcasted	py	0.014286
list of lib directories that	lib dirs	0.045455
compiled module	module name	0.062500
broadcasted dimensions	tensor tensor py operators dimshuffle	0.019231
folowing changes	tensor local mul switch sink node	0.045455
respect to the inputs	dnn	0.030303
function tries to	image_shape top_shape border_mode	0.166667
execute callbacks calls getattr feature name (*args)	function graph execute callbacks name	0.500000
and apply nodes in the original graph to	inputs outputs copy_inputs_and_orphans memo	0.029412
from the shape	shape	0.010204
folowing changes in the	local mul switch sink	0.045455
print a warning message	deprecated filename msg	0.041667
arctan2	arctan2	0.833333
has a 'requirement' of the destroyhandler	add destroy handler	0.125000
load an array from an npy file	tensor load path dtype broadcastable mmap_mode	1.000000
increments a subtensor using advanced indexing	advanced inc subtensor	0.333333
apply nodes according	sort apply nodes inputs outputs cmps	0.050000
new	with new inputs inputs	0.166667
convert degree	tensor deg2rad	1.000000
between low and high	random_state size low high	0.500000
a number of scalars together into a vector	make vector	0.125000
hash of a	hash	0.055556
an array	node inputs	0.086957
gof	gof	0.014286
convert	constant	0.016667
exit code in a	subprocess popen command	0.083333
original graph	inputs outputs copy_inputs_and_orphans memo	0.029412
output	dims output input leftdims rightdims	0.333333
min for the minimum	minimum x	0.142857
try to compile a	compiler try	0.250000
output nodes of	gof	0.002381
return a dictionary of arguments to pass to	args	0.025641
sample from one	random_state size	0.250000
upgrade a input	input input	0.333333
conda offers to make itself	to os environ pathlist var newpath	0.038462
this generates the c	c	0.053571
fill s v -> alloc(v shape	tensor local fill to	0.250000
y t -> dot y t x t	tensor local lift transpose through dot node	0.333333
new variable	variable	0.022222
return a symbolic row	row name dtype	0.050000
the image shape	shape 1axis kernel_shape	0.250000
rest	rest inputs	0.125000
list remove are still in	replacements remove reason	0.055556
this function expects the compile lock to	cache add to	0.142857
a vector to	alloc	0.012500
with a name	name	0.011111
counter to	counter	0.142857
the computation to be performed	node	0.007407
apply_node recursively search from this node to know	import apply_node	0.066667
that will call the supplied function as	as	0.024390
on the inner graph to ensure that it	validate inner graph	0.035714
inputs required to compute the given	inputs variable_list blockers	0.058824
if theano graphs represent the	xs ys in_xs in_ys	0.111111
assemble the c code for this composite op	composite init c code	0.333333
navigator deal	navigator optimizer attach updater	0.038462
generates the c code for corrmm (direction="forward"),	base corr mm c code	0.090909
this graph	graph	0.016393
implement crossentropysoftmax1hotwithbiasdx	crossentropy softmax1hot with bias dx	0.500000
the g++ version	gof gcc	0.027778
of this op could	prod l op	0.033333
an apply_node recursively search	import apply_node check	0.066667
bound by the inputs and outputs	init inputs outputs	0.166667
wants to add some requirements to	optimizer add requirements	0.166667
function that allows replacing subgraphs of a computational	scan_module clone output replace strict share_inputs	0.071429
merge 2 profiles	optimizer merge	0.200000
grad_overrides	grad_overrides	1.000000
that respects the conventions imposed by python and	theslice length	0.052632
equivalent of	graph to	0.055556
round half to	rint	0.111111
like make_thunk() but only makes python thunks	op make py thunk node storage_map compute_map no_recycling	1.000000
with respect to wrt,	subgraph	0.047619
elements should be inserted to maintain order	tensor searchsorted x v side sorter	0.142857
use dnn conv workmem	core safe no dnn workmem workmem	0.166667
+ alpha * dot	gemv	0.100000
an input that wasn't	bad destroy	0.034483
symbolic row variable	tensor row name	0.050000
remove subtensor/advancedsubtensor1	local useless subtensor	1.000000
context object mapped to	gpuarray gpu array type context	0.090909
to add some requirements	optimizer add requirements	0.166667
a inner nit_sot	inner	0.041667
reduce pattern has functioning	gpu careduce cuda supports	0.166667
updates ordereddict the list of outputs and	scan_module get updates and outputs	0.333333
function performs the matrix	matrix	0.055556
seconds since the epoch of the last access	last access time path	0.040000
1-sigm x -> sigm -x	nnet local 1msigmoid node	1.000000
help the navigator deal	gof navigator optimizer attach updater	0.038462
is the	graph to gpulocal opt	0.055556
true for any python object a that	gof pure	0.033333
on wraplinker that runs a	gof wrap linker	0.083333
elementary validations on the inner graph to	scan_module scan validate inner graph	0.035714
return connection pattern of subfgraph defined by inputs	graph connection pattern node	0.076923
value has a __thunk_trace__	hook type value trace	1.000000
fill s v -> alloc(v shape	local fill to alloc	0.250000
self _rop_op from user supplied form to type	from graph recompute rop op	0.200000
object a that	gof	0.002381
variable v if v is	v	0.011111
c code	c	0.053571
first outdim-1 dimension size s of x	flatten x ndim outdim	0.333333
the caller is replace_all_validate just raise the	validator validate fgraph	0.125000
the grad of this op	tensor prod l op	0.033333
builds the 1d	1d	0.090909
from polar	complex from polar	0.250000
a dense n-dimensional 'meshgrid' with equally spaced points	nd grid	0.500000
scale each row	row scale x	1.000000
with kernels of shape	shape	0.010204
minimum elements obtained by iterating over given axis	min x axis keepdims	0.500000
by doing a recursion over the input dimensions	permute row elements rec	0.047619
the hack in profiling to print the mflops	tensor nnet base abstract conv flops inp outp	0.125000
and should be removed	compress outs	0.076923
decorator which will print a warning message	gof deprecated filename msg	0.041667
randomstate instance associated with a particular	item	0.076923
connection pattern of	graph connection pattern	0.076923
with	with op node	0.166667
cutils_ext	gof compile cutils	0.166667
symbolic scalar	scalar name dtype	0.166667
still in the graph	fgraph replacements	0.250000
raise baddestroymap	inputs node storage_map	0.166667
mul	mul	0.538462
reorder the dimensions of this variable optionally inserting	py operators	0.015625
dict op -> total	compile profile stats op	0.333333
decorator to merge multiplication by	gpuarray alpha merge cls alpha_in beta_in	0.200000
baddestroymap	compile	0.076923
a and b are unified given	a b	0.066667
float	float	1.000000
ints	weights minlength assert_nonneg	0.125000
self _rop_op from user supplied form to	from graph recompute rop	0.200000
argmin	argmin	1.000000
the tensor operators to the basic variable class	tensor variable	0.166667
class returns the bartlett spectral window in	bartlett m	0.083333
reshapes the output	dims output input leftdims rightdims	0.333333
c-implementation of	csr c code node name	0.333333
a compiled graph have a stack	stack	0.066667
the sum along the	sum	0.038462
represent the dependence of nodes in a graph	make dependence	0.043478
input nodes to	gof	0.002381
the navigator deal with the	gof navigator optimizer attach updater fgraph	0.038462
the subgraph bound by the inputs	inputs	0.012658
to	gpuarray gpu array type	0.062500
a modulo of m1 and	m1	0.027027
an array with more than	node inputs outputs	0.125000
to use dnn conv algo_bwd	safe no dnn algo bwd algo	0.166667
helper function for diagonalsubtensor and	get diagonal subtensor view x i0 i1	0.083333
neibs2images	neibs2images	0.750000
on f	lop f	0.166667
load a file that was dumped	load f persistent_load	0.333333
function performs the matrix inverse on	matrix inverse a	0.200000
comparing	tensor	0.003215
optionally inserting broadcasted dimensions	tensor tensor py operators	0.015625
conda offers to make itself the	to os environ pathlist var	0.038462
the source code for this linker	compile cmodule location	0.038462
the mflops	tensor nnet conv op flops	0.125000
implemented returns	gof local meta	0.500000
by default that removes all	tensor local remove all	0.166667
output error and exit code in a tuple	misc output subprocess popen command	0.100000
type's :attr context_name	gpu array type	0.062500
the inputs according	inputs	0.012658
elementwise conjugate (inplace on a)	tensor conj inplace a	1.000000
a with a modulo of m1 and	m1	0.027027
efficiently calculating the dot product	true dot x	0.166667
set and dictionary data structures	non seq scan	0.090909
return indices over	indices	0.076923
within the op	get op	0.100000
aliasing and	destroy	0.009709
return a symbolic vector variable	vector name dtype	0.166667
fourier	tensor fourier	0.333333
inserting	py operators	0.015625
from the shape or the other arguments	ndim bcast ndim shape	0.250000
list of libraries	libraries	0.111111
shorter version of platform platform()	core short platform r p	0.142857
associated with a particular	item	0.076923
return the inputs required to	gof inputs variable_list blockers	0.058824
this optimization makes the folowing changes	mul switch sink	0.045455
of this variable	py	0.014286
the output dimensions of	tensor nnet conv op get output	0.047619
default that removes all asserts from	tensor local remove all assert	0.055556
with the -x pattern	tensor nnet is neg	0.166667
is the equivalent	graph to gpulocal opt	0.055556
important note this function uses set and dictionary	process node fgraph	0.142857
converting to type2 from type1 constitutes an upcast	tensor is an upcast type1 type2	0.333333
x*x -> sqr x	local mul to sqr node	0.166667
a convolution with the specified	conv	0.037037
structures	non seq scan	0.090909
outer product of two	sparse block outer	0.047619
from dict	d3viz dict	0.333333
help the navigator deal with the	gof navigator optimizer attach	0.038462
shape and	signal pool grad out shape	1.000000
loop	make reordered loop	0.111111
x to be between	x	0.008772
a b), axis=0) -> elemwise{scalar op} a b	tensor local reduce join	0.111111
that x and	x	0.008772
type numpy typenum that corresponds to self	type dtype specs	0.071429
converts number to	char from number number	0.142857
r-operator	max pool rop	0.142857
number of multiplications and/or divisions	local greedy distributor node	0.166667
copy a function copied function have separate intermediate	compile function copy	0.333333
elemwise and gpuelemwise	tensor local elemwise fusion	0.166667
replacement	replace validate replace	0.050000
logsoftmax x	tensor nnet local logsoftmax node	0.142857
returns the connection	io connection	0.333333
helper function for diagonalsubtensor and	nnet get diagonal subtensor view x	0.083333
the specified pieces of vectors and	sparse block outer make node o x	0.066667
for product between several dots	tensor matrix dot	0.333333
python object a that would be a	gof pure	0.033333
a version of var transferred to	tensor transfer var	0.100000
wait on a previously sent	mpisend wait	0.045455
given inputs using the given mean and variance	test inputs gamma beta mean	0.500000
variable with the -x pattern	neg var	0.166667
thunk calls	callcount	0.142857
proxy	proxy	0.333333
partition a	tensor scalarconsts	0.125000
of gradient	grad	0.010417
same rounding algo as c round() fct	round half away from zero	0.500000
neural-net multiclass classifiers	softmax with bias	0.142857
for diagonalsubtensor	diagonal subtensor view x i0	0.083333
a	mpisend	0.037037
help the navigator deal with the	navigator optimizer attach	0.038462
of this node	node	0.007407
vector and t	node	0.007407
x	flatten x	0.166667
the flattened version of a	tensor flatnonzero a	0.200000
message explaining the output of is_valid_value	pure type value validity msg	1.000000
important note this function uses	process node fgraph	0.142857
if we are able to assert	dim_x dim_y	0.090909
prod(prod()) ->	tensor local	0.025641
if the named module is a package	compat six meta path importer is package fullname	0.250000
the hack in profiling to print the mflops	base abstract conv flops inp outp	0.125000
return the abs	abs	0.066667
gradient	grad x	0.333333
pieces of vectors	sparse block outer make node	0.066667
functiongraph listeners to help the navigator deal	navigator	0.032258
parse a	parse mul	0.500000
a poisson	base poisson	0.500000
of the form x[0 :] -> x[0]	local useless slice node	0.250000
optionally	operators dimshuffle	0.019231
the passed-in key is found in the	get from key key key_data	0.111111
scan return true iff the	scan	0.017241
or more multinomial distributions	tensor multinomial	0.037037
the end	wrt end	0.050000
updates ordereddict the list of	get updates	0.034483
performs batch normalization of the	tensor nnet batch normalization	0.125000
in array of ints	x weights minlength assert_nonneg	0.125000
the navigator deal with	gof navigator	0.038462
matrix and a dense vector	svcsr	0.090909
stopping condition returned by	ls	0.090909
fill s v -> alloc(v shape	tensor local fill	0.250000
in the graph and returns	in	0.076923
level of nesting	loop_orders dtypes loop_tasks sub	0.125000
equivalent of localoptgroup for	graph to gpulocal opt group	0.055556
sharedvariable constructor for scalar values default int64	scalar shared value name strict allow_downcast	0.200000
sample from	random_state size	0.250000
to sharedvariable instances of suitable dummy values	optimizer provide inputs	0.200000
out the variables inside the	out	0.018519
neibs	neibs	1.000000
inner graph to ensure that it	validate inner graph	0.035714
output	gpuarray output	0.200000
safely compute ceil(float_division a b	ceil intdiv a b	1.000000
compute_map	compute_map	1.000000
a new graph	with new inputs inputs	0.166667
ops contained	gof ops	0.083333
rebroadcast if id does not actually change	rebroadcast node	0.500000
attempt to replace a leaf of	tensor nnet replace leaf arg leaves new_leaves op	0.250000
fill s v -> alloc(v shape s	local fill to alloc node	0.250000
a six moves urllib namespace that resembles	six	0.025000
a modulo of m1	m1	0.027027
from a uniform	tensor uniform random_state	0.125000
by re-writing the	gof refresh	0.125000
register a transfer	tensor register transfer	1.000000
computes the inverse of	inverse	0.066667
l{codeblock} instances returns	gof code gen blocks	0.050000
attempt to convert x into a variable	variable x context_name	0.250000
*args directly	csm properties csm node	0.142857
load a so pyd dll or py file	dlimport fullpath suffix	0.333333
for gpuincsubtensor	inplace setsubtensor node	0.250000
initializes py_name	r	0.028571
source	compile cmodule location	0.038462
roll	roll	0.666667
create a new instance of this mode	compile mode clone link_kwargs optimizer	0.333333
that wasn't in the	bad destroy	0.034483
elements of each row inner-most dim	row elements	0.333333
dnn conv workmem	core safe no dnn workmem workmem	0.166667
without replacement	random streams base choice	1.000000
tile input array x according	tensor tile x	0.500000
the replacement if the ops in	replace validate replace all	0.050000
baddestroymap	storage_map r_vals	0.166667
softmax(sum_of_stuff) -> softmax_w_bias	tensor nnet local softmax with	0.200000
orv(list1 \ list2)	o n	1.000000
spatio-temporal filters with a movie	nnet conv3d signals filters signals_shape filters_shape	0.333333
a > b inplace on a	tensor gt inplace a b	0.500000
transfer to a tensortype	py operators transfer	0.125000
if	compile check inputs node storage_map	0.166667
makes the folowing changes in the graph	tensor local mul switch sink	0.045455
pass to helper_c_code	inc subtensor get helper c code	0.142857
to construct a variable with	variable x	0.083333
to get the 0 based level of	get	0.020833
method that	gof	0.004762
[true] division inverse of	true div	0.250000
unfortunately conda offers to make itself the	to os environ	0.038462
canonical form that respects	tensor get canonical form	0.045455
dimensions of this variable optionally inserting broadcasted dimensions	py operators dimshuffle	0.019231
a modulo of m1 and the second half	m1	0.027027
tensor filled with ones closer to numpy's	tensor ones shape dtype	0.250000
if theano graphs represent the same computations	equal computations xs ys in_xs in_ys	0.333333
string x	replace patterns x	1.000000
output dimensions of convolving an image of	tensor nnet conv op get output	0.047619
header for openblas threads	openblas threads	0.250000
img	img	1.000000
module file	key data	0.250000
this optimization makes the folowing changes	mul switch sink node	0.045455
the task	find task	0.142857
raise baddestroymap if necessary update dr_vals	r_vals dr_vals	0.250000
helper function for diagonalsubtensor and	nnet get diagonal subtensor	0.083333
operation on f	lop f	0.166667
this	gpuarray	0.046512
multinomial distribution	streams multinomial	0.076923
in the "theano config compiledir"	compiledir content	0.166667
the computation	node	0.007407
2d filters	tensor nnet conv2d input filters	0.125000
inputs and	node inputs	0.043478
b), axis=0) -> elemwise{scalar op}	local reduce join node	0.111111
alloc val [x y] -> alloc(val[ ])	local subtensor of alloc	1.000000
x and y have the same shape	tensor shape feature same shape x y	0.500000
search through consecutive view_map()s	gof view roots r	0.200000
c code to pack c types back into	gof clinker type c sync	0.111111
convolution for debugmode	base abstract conv conv	0.125000
of the specified pieces of vectors and	sparse block outer make node o x y	0.066667
default that removes all	remove all	0.166667
convert addsd to faster	sparse local addsd	0.250000
offers to	to os environ pathlist var	0.038462
compile a dummy file with these	cls flag_list preambule body	0.250000
multiplication by a scalar on the	gpuarray alpha	0.142857
this op	tensor prod l op	0.033333
function computes the output shape for a convolution	gpu dnn conv get out shape ishape kshape	0.333333
function tries to recognize the updates	updates	0.029412
required return c code to	name	0.011111
inserting	tensor	0.006431
get the 0 based level	get depth	0.050000
c	register specify shape c	0.250000
the equivalent of	gpulocal	0.055556
print the mflops	corr3d mm flops inp outp	0.125000
if	storage_map	0.090909
the dependence of nodes	make dependence	0.043478
leaf of	leaf	0.066667
elemwise signe of x	sparse sgn x	1.000000
the inverse of a matrix	matrix inverse	0.111111
python litterals to	make constant	0.100000
remove subtensor/advancedsubtensor1 if it	local useless subtensor node	0.200000
with respect to wrt, computes gradients	subgraph grad	0.062500
as python not the other implementation of mod	mod c code	0.125000
with debug info	with op node thunk	0.166667
subtensor(setsubtensor x y idx idx) -> y	tensor local subtensor inc subtensor	0.500000
this is just speed opt not for stability	scalar sigmoid	1.000000
the grad of downsample with max	downsample factor max grad grad	0.333333
dependence of nodes in a	gof make dependence	0.043478
that wasn't in the	destroy	0.009709
inputs required to compute the given	gof inputs variable_list blockers	0.058824
a symbolic	ndim dtype	0.333333
tensor	py operators	0.015625
resolve	resolve	1.000000
x and there is already an l=cholesky x	sandbox linalg local	0.142857
ones with the	ones like	0.333333
apply instance from set which must be computed	destroy handler on prune fgraph app reason	1.000000
c type numpy typenum that corresponds	tensor type dtype specs	0.071429
logsoftmax x 's grad	tensor nnet local logsoftmax grad	0.200000
as the output type dtype and broadcast there	local useless alloc	0.333333
of scan	scan_module push out scan	0.050000
the convolution gradient with respect to	gpu dnn conv grad i	0.125000
the clients list of r	r client_to_remove reason	0.200000
the input by a specified factor takes as	signal pool 2d	0.142857
out for occurrences of values identical with x	forced replace out x y	1.000000
tries to	image_shape top_shape border_mode	0.166667
the dimensions	dimshuffle	0.014493
loop	tensor make reordered loop	0.111111
a variable with the -x pattern	nnet is neg	0.166667
function performs the svd on	svd a	0.200000
function that allows replacing subgraphs of a	scan_module clone output replace strict share_inputs	0.071429
replace_all_validate revert the replacement	replace all validate remove	0.111111
if allow_override is false we can't change	allow_override	0.083333
kernel that can	kernel	0.133333
a badviewmap exception when it detects the following	check viewmap node storage_map	0.111111
dnn conv algo_bwd	no dnn algo bwd algo	0.166667
code	code filename	0.333333
type's :attr	gpuarray gpu array	0.062500
connection pattern of subfgraph defined by inputs	from graph connection pattern node	0.076923
in a	a	0.008065
same shape and dtype as the	dtype	0.022727
apply_node recursively search from this node to	import apply_node	0.066667
this class is a wrapper for	op	0.009174
set of ops contained within	gof ops	0.083333
print	compile print	0.500000
variables in inputs	inputs	0.012658
for same kinds of	tensor	0.006431
function computes the	ishape kshape	0.250000
this variable optionally	tensor py	0.015873
conda offers to make	to os environ pathlist	0.038462
with logsoftmax x 's	local logsoftmax	0.076923
sum of non nan / inf values in	sum	0.038462
like zeros_like but forces the object to	core float zeros like	0.200000
feature	feature feature	0.250000
raised by get_scalar_constant_value if called on something	error	0.025000
returning the output error and exit code in	output subprocess popen command	0.100000
should remove any dynamically added functionality	gof node finder on detach fgraph	1.000000
topooptimizer from the	gof in2out	0.055556
a compiled graph have a stack	check stack	0.142857
of x y	x y	0.048780
from the cache if available	cache call fn args key	0.200000
the replacement if the ops in the	gof replace validate replace	0.050000
a given "group" (ie	inp out grads	0.166667
conda offers to make itself the default	to os environ pathlist var newpath	0.038462
to get the 0 based level	type get	0.050000
the source code for this linker and	clinker compile cmodule location	0.038462
of a real-valued input	tensor rfft inp norm	0.142857
an apply_node recursively search	import apply_node	0.066667
instance associated with a particular stream	random streams getitem item	0.142857
input by a specified factor takes as	signal pool 2d	0.142857
kernels of shape "kshp"	shape inshp kshp stride mode	0.142857
dictionary of arguments to pass to helper_c_code	helper c code args	0.250000
is only used to determine the broadcast pattern	adv index broadcastable pattern a idx	0.066667
sample n (n needs to be	size n	0.090909
required return c code to extract a	clinker type c extract name	0.250000
toposort return an ordering of	toposort	0.076923
a variable with the -x pattern	neg	0.083333
error if cudnn can't be used	gpuarray no cu dnnraise apply fgraph	0.200000
multinomial distributions defined by one-dimensional slices in	tensor multinomial random_state	0.040000
if the g++ version used	gcc	0.023810
inputs with a set of 2d filters	nnet conv2d input filters	0.125000
construct a variable with a	variable	0.022222
x the	x	0.008772
shape tuple	infer shape	0.066667
theano config	config	0.100000
ignore_border	ignore_border	1.000000
detect if the g++ version	gcc	0.023810
the output dimensions of convolving an image of	op get output	0.047619
all asserts from	all assert	0.250000
return c	name	0.022222
returns	gof clinker	0.133333
to merge multiplication by a	alpha merge	0.076923
over pairs	over pairs pairs	1.000000
list of shape tuple	shape feature default infer shape	0.066667
by an op that will be inserted at	clinker op	0.071429
return a symbolic row variable (ndim=2 broadcastable=[true false])	tensor row name	0.050000
the context associated with	get context	0.111111
makes the folowing changes in the graph	local mul switch sink	0.045455
variable instance of type self	gof pure type make variable	0.333333
helper function to generate permutations	tensor permutation helper random_state n shape	0.333333
adding	dict d1 d2	0.333333
a variable with a sparse matrix	sparse variable x name	0.250000
revert the replacement if the ops in	gof replace validate replace	0.050000
the inner graph to ensure	scan_module scan validate inner graph	0.035714
support the types involved	inc subtensor do type checking	0.142857
instance of _maker which handles much	compile debug mode function maker i o m	0.066667
try to turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	tensor nnet local softmax with bias	0.200000
convert python	tensor	0.003215
function expects the compile lock to be held	module cache add to cache module key module_hash	0.166667
by erf/erfc opt to track less frequent op	tensor get clients2 node	0.200000
raise	r_vals	0.090909
pattern of a subgraph defined by	pattern	0.028571
that will call the supplied function as	compile as	0.050000
to the	array	0.041667
important note this function	seq scan process node fgraph node	0.142857
row variable (ndim=2 broadcastable=[true false])	tensor row	0.050000
module is	is	0.066667
for gpuincsubtensor	setsubtensor node	0.250000
op then replace it with a triangular solve	linalg tag solve triangular node	0.142857
machine	vm call	1.000000
tensorvariable whose	as	0.024390
var shape[i], but apply if possible the	i var i fgraph	0.200000
the navigator deal with	gof navigator optimizer attach	0.038462
canonical form that respects the	canonical form	0.045455
acc_dtype	acc_dtype	1.000000
alpha_in	alpha_in	1.000000
input that	destroy	0.009709
it with logsoftmax	logsoftmax	0.076923
and the rest	rest inputs elemwise	0.125000
trace	trace	0.368421
times from a multinomial	multinomial	0.024390
mpi	mpi	1.000000
the	tensor py	0.015873
shape	shape feature shape	1.000000
is only used to determine	tensor adv index broadcastable	0.050000
function for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view	0.083333
connection pattern of subfgraph	from graph connection pattern node	0.076923
any python object a that would be a	gof	0.002381
the c code for corr3dmm	nnet base corr3d mm c code	0.090909
node by one which computes the specified outputs	fgraph node output_indices alloc_ops	1.000000
deprecated old conv2d interface	tensor nnet conv2d input filters image_shape filter_shape	0.500000
in a new graph	with new	0.166667
g++ version used is	gcc	0.023810
char	char	1.000000
nodes such that	gof function	0.043478
destroyhandler	add destroy handler	0.125000
constant inputs	constant inputs	0.125000
sample from one or more multinomial distributions defined	multinomial random_state size n	0.333333
inner-most loop	reordered loop	0.111111
updates for matrix solve operation c = a	tensor solve	0.038462
<= b inplace on a	tensor le inplace a b	0.500000
get_scalar_constant_value if called on something that is not	not	0.076923
a real-valued input on the	curfft inp norm	0.066667
symbolically cast x to a	cast x	0.200000
apply nodes such that	gof	0.002381
upgrade a input shortcut to an in instance	compile convert function input input	0.333333
failed fix done in august 2011	tensor load shared variable val	0.142857
images2neibs <theano tensor nnet	nnet images2neibs	0.333333
set and dictionary data structures	scan_module push out non	0.125000
if the caller is replace_all_validate just raise	gof validator validate fgraph	0.125000
output type dtype and broadcast there	local canonicalize alloc	0.333333
in a new graph	new inputs inputs strict	0.166667
integers indicating the version	version apply	0.125000
the first functiongraph that	gof	0.002381
and t	node input_storage	0.038462
helper function for diagonalsubtensor and	tensor nnet get diagonal subtensor	0.083333
to make itself the	to os environ pathlist	0.038462
c code	clinker op c code	0.333333
with constant inputs replaced	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
important note	scan_module push out seq scan process node	0.142857
helper function for diagonalsubtensor	tensor nnet get diagonal subtensor view x	0.083333
an fgraph and a list of	fgraph	0.012195
test	pt n_tests rng	0.500000
important note this function uses set	push out seq scan process node	0.142857
return a subtensor copy using advanced indexing	advanced subtensor	0.250000
determine the broadcast pattern for advancedsubtensor output	tensor adv index broadcastable pattern a	0.066667
optionally with a new dtype or broadcastable pattern	clone dtype broadcastable	1.000000
multinomial distributions defined by one-dimensional	multinomial random_state	0.040000
the function name to load	gpuarray load	0.200000
only one client and that client is	sitsot only	0.066667
hex	hex	1.000000
replacement	validate replace all	0.050000
change the value after the import of	core config param init default	0.040000
extract test value from v	gof get test value v	0.250000
listeners to help the navigator deal with	gof navigator optimizer attach updater	0.038462
out_shape	out_shape	0.750000
the list of op classes	local optimizer tracks	0.200000
start gradients up to the end variables	wrt end start	0.166667
of m1 and the second	m1	0.027027
of l{codeblock} instances returns	gof code gen blocks	0.050000
return full path of the	module name	0.062500
a constant that is cached	cached constant error	0.200000
the signature for this function	gof ext function method decl	0.333333
multinomial distributions	tensor multinomial	0.074074
a x	a x	1.000000
cond	cond	0.857143
the eigensystem of	tensor eigh	0.333333
on f	f	0.105263
correctly indented	misc hooks get correct indentation	1.000000
computes the specified outputs inplace	inplace fgraph	0.142857
convert python litterals	tensor	0.003215
generates the c code for corr3dmm (direction="forward"),	base corr3d mm c code	0.090909
cache and none otherwise	cache	0.034483
of variables [v1 v2 v3 ]	gof vm linker compute gc dependencies variables	0.250000
the output type dtype and broadcast there is	tensor local useless alloc node	0.333333
help the navigator deal with	navigator optimizer attach updater fgraph	0.038462
local optimization wants to add some requirements to	local optimizer add requirements	0.500000
computes the l operation on f	core lop f	0.166667
it into a gemm	tensor gemm	0.166667
freev is unified to boundvariable(other_object)	fv o u	0.200000
the idx list to get	tensor get idx list	0.076923
pattern of a	pattern	0.028571
with the -x pattern	nnet is neg	0.166667
on	gpuarray as gpuarray	0.333333
unfortunately conda offers to make	to os environ	0.038462
set of all variables which may share	compile infer reuse pattern	0.100000
a vector to the diagonal of an empty	alloc diag	0.027027
sum{0 1 n} -> sum{} or	tensor local sum prod all to none node	1.000000
help the navigator deal with the	gof navigator optimizer attach updater	0.038462
scalar variable	tensor scalar name	0.500000
is basically a call to	extract constant x elemwise only_process_constants	0.058824
the dimensions of this variable	tensor py	0.015873
hack in profiling to print the mflops	conv op flops inputs outputs	0.125000
failure code to the task	task	0.083333
the replacement	gof replace validate replace all	0.050000
that decrefs py_name	gof get c cleanup r	0.250000
a comparator to represent the dependence	gof make dependence cmp	0.111111
replace_all_validate revert the replacement	validate replace all validate remove fgraph	0.111111
profilemode to print the mflops	gpu corr3d mm flops inp outp	0.125000
of nesting	loop_orders dtypes loop_tasks	0.125000
compiled graph have a stack	stack	0.066667
tree of multiplications starting at the given root	tree root	1.000000
if the outputs of specific ops	trace f_or_fgraph ops_to_check bug_print	0.035714
returns the connection pattern	connection pattern	0.032258
some elementary validations on the inner	inner	0.041667
inf	inf	1.000000
get the right values	tensor get	0.250000
to find broken optimizations	compile find bad	0.333333
replace_all_validate revert the replacement if the	gof replace validate replace all validate	0.111111
from a with or without replacement	random streams base choice size a	0.333333
see theano tensor min	py operators min axis keepdims	1.000000
are non-zero in the flattened version of	tensor flatnonzero	0.166667
return complex-valued tensor from	complex from	0.250000
the inner graph	scan validate inner graph	0.035714
of localoptimizer and applies them to the	local opt group	0.052632
for the output of neural-net multiclass classifiers	softmax with bias	0.142857
stride	stride	1.000000
computes the output dimensions of convolving an image	conv op get output	0.047619
gradient is an alloc of a scalar	alloc node	0.037037
this variable optionally inserting	tensor tensor py operators dimshuffle	0.019231
according to the idx list to get	tensor get idx list	0.076923
op could be very	prod l op	0.033333
dimensions of this variable optionally inserting	tensor	0.006431
to expm1	expm1	0.050000
destroyhandler class detects when	destroy handler	0.055556
the "reverse-mode" gradient for the eigensystem of	tensor eigh grad perform node inputs	0.333333
within	gof	0.002381
x to a tensor of type dtype	x dtype	1.000000
detect if the g++ version used is	gcc	0.023810
important note this function uses	out seq scan process node	0.142857
to one or more tensor variables	from_var to_var	0.250000
this convert allocempty to alloc	local alloc empty to zeros	0.333333
for convop that unroll the batch size loop	code unroll batch kern d unroll_bsize unroll_ksize	0.125000
class returns the bartlett spectral window in	tensor bartlett	0.083333
gets a scan op a list of	op not_required inputs	0.071429
return a hash from an ndarray	tensor hash from ndarray data	0.333333
for the maximum in	tensor maximum	0.142857
to theano constants in subtensor arguments	make constant args	0.250000
the last access of	last access	0.040000
path importer	path importer	0.333333
converts number to string by	compile char from number number	0.142857
the op code	get op	0.100000
test a	n_tests rng	0.500000
an op that copies a vector to	alloc	0.012500
the dimensions of x	x axes	0.200000
from	from	0.650000
of useless	local useless	0.111111
assign the shape	shape	0.010204
applies the optimization to the provided l{functiongraph} it	gof optimizer apply	0.166667
x	x name	1.000000
function for alternative targets	fn	0.083333
convolution shape	conv shape shape	1.000000
conv output gradient w r t its weights	grad wrt weights input output_grad filter_shape input_shape	0.333333
input nodes to output	gof	0.002381
elementary validations on the inner graph to ensure	validate inner graph	0.035714
diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x i0 i1	0.083333
directory and return	name from dir	0.250000
diagonal	alloc diag	0.027027
return connection pattern of subfgraph defined	connection pattern	0.032258
attributes and tag	variable attributes	0.333333
vector and t	node input_storage	0.038462
for convolving a mini-batch	input_shape filter_shape	0.027778
object but we don't clone the	clone	0.020833
equivalent of localoptgroup for	gpulocal opt group	0.055556
the exp x or -exp x patterns	nnet is exp	0.333333
around c_extract that initializes py_name from storage	gof get c extract r	0.250000
average pooling	average pool grad	0.200000
to the	gpuarray gpu array type	0.062500
undefined	undefined	1.000000
gradient	conv3d grad	0.111111
dimensions of this variable optionally inserting broadcasted dimensions	dimshuffle	0.014493
list remove are still	fgraph replacements remove reason	0.055556
same shape	shape feature same shape	0.333333
extract test value from v raises attributeerror	gof get test value v	0.250000
how to generate c	specify shape c	0.250000
navigator deal	navigator	0.032258
det x and there is already	local det	0.166667
computes the output dimensions of convolving an	nnet conv op get output	0.047619
uniform into sample without replacement from	choice from uniform	0.166667
of scan return	push out scan	0.050000
split x	tensor split grad	0.500000
toposort return	function graph toposort	0.125000
cutils_ext	compile cutils	0.166667
base class with a metaclass	compat with metaclass meta	0.333333
to wrt,	subgraph grad	0.062500
important note	scan process node	0.142857
dimensions of this variable optionally inserting broadcasted	py operators	0.015625
a class with a metaclass	compat add metaclass metaclass	0.125000
-1 and converts this to expm1	tensor local expm1 node	0.066667
same shape	tensor shape feature same shape	0.333333
solve	linalg tag solve	1.000000
the shape	set shape	0.333333
source	gof clinker compile cmodule location	0.038462
the version	version apply node	0.125000
cross-entropy between an approximating distribution	tensor nnet categorical crossentropy coding_dist true_dist	0.111111
svd on cpu	svd a full_matrices compute_uv	0.200000
six moves and its	six	0.025000
the c code for corrmm (direction="forward"),	base corr mm c code	0.090909
run a simple c snippet using current flags	march flag flags	0.333333
c code for gpucorrmm (direction="forward"), gpucorrmm_gradweights	base gpu corr mm c code	0.090909
maximum see max for the maximum in	tensor maximum x	0.142857
return a symbolic	name dtype	0.333333
optimization makes the folowing changes in	tensor local mul switch sink node	0.045455
compute	nnet conv3d	0.142857
c_code for convop that unroll the batch size	conv code unroll batch kern d unroll_bsize unroll_ksize	0.125000
make code	code filename	0.333333
extract a list of compilation flags	libs flags libs_dir include_dir	0.052632
a variable with	variable x	0.083333
[elementwise] smallest of	tensor smallest	0.333333
pieces of vectors	sparse block outer make node o x	0.066667
of the specified pieces of vectors	sparse block outer make node o x y	0.066667
or 3d convolution for	abstract conv conv	0.125000
->	node	0.007407
this variable optionally inserting broadcasted dimensions	dimshuffle	0.014493
view	compile view tree	0.500000
c	compile register specify shape c	0.250000
uniform	tensor uniform	0.125000
true for any python object a that would	gof	0.002381
the hack in profilemode to print the mflops	mm flops inp outp	0.125000
as replace_all_validate	validate remove	0.166667
incsubtensor x zeros idx -> x	local incsubtensor of zeros	1.000000
helper function to	helper random_state n shape	0.500000
the abs toward the input	local abs lift node	0.333333
of apply nodes according to	gof sort apply nodes inputs outputs cmps	0.050000
parametrize it to	max_input_fct maker	0.083333
graph optimizer that merges different scan ops	scan merge	1.000000
optional return	gof	0.002381
and run a simple c snippet using current	march flag	0.250000
toposort return an ordering	toposort	0.076923
new	new inputs	0.166667
an apply_node recursively search from	import apply_node check reason	0.066667
context object mapped to	array type context	0.090909
history	history	1.000000
input and	input	0.023810
is false we can't change the value after	default	0.030303
fourier transform of a real-valued input on	curfft inp norm	0.066667
msg	msg	0.500000
op classes that this opt applies	gof local optimizer tracks	0.071429
return connection pattern	op from graph connection pattern	0.076923
gof graph is_same_graph	is same graph with merge var1 var2 givens	0.166667
sharedvariable constructor for randomstate	tensor randomstate constructor value name strict allow_downcast	1.000000
outside of scan	out scan	0.035714
a signature object for	tensor constant signature	0.100000
the version	c code cache version apply node	0.125000
output specs	input_specs output_specs accept_inplace	0.142857
base class for abstractconv parameters	base abstract conv	1.000000
be a 1-d	random_state size a replace	0.333333
broadcast pattern for advancedsubtensor	pattern	0.028571
dict op ->	compile profile stats compute	1.000000
names when persisting to	id	0.100000
functiongraph listeners to help the navigator deal	gof navigator optimizer attach updater fgraph	0.038462
convolution implementation by sparse matrix multiplication	sparse sandbox convolve kerns kshp nkern images	0.500000
of the __unify_walk__ method for one	gof unify walk a b u	0.037037
slice [start stop step] transform	slice	0.038462
concatenate multiple tensorvariables along some axis	join	0.090909
and return full path of	module name	0.062500
all sigmoid	sigmoid	0.055556
cudnn batch	dnn batch	0.333333
on the inner	scan validate inner	0.142857
inplace optimization that deals with allocempty this	inplace allocempty op idx	0.166667
from existing start gradients	start	0.040000
indicating the version	version apply node	0.125000
or more multinomial distributions defined by	multinomial random_state	0.040000
wrapper around c_extract_out that initializes py_name from	gof get c extract out r name	0.333333
op whose incoming gradient is	softmax 1hot	0.333333
a dummy file with these	cls flag_list preambule body	0.250000
solve operation c = a \	tensor solve	0.038462
floor	floor	1.000000
return the data type for working memory	gpuarray work dtype dtype	0.200000
fill s v -> alloc(v shape	local fill	0.250000
filled with ones closer to numpy's syntax than	ones shape dtype	0.200000
diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x	0.083333
forward but clip	clip x lower_bound upper_bound	0.090909
(direction="forward"), gpucorrmm_gradweights (direction="backprop weights"),	helper bottom weights top direction	0.055556
the provided l{functiongraph} it may	optimizer apply fgraph	0.200000
operation on f wrt to wrt	core rop f wrt	0.200000
given "group" (ie	inp out grads	0.166667
that x and y	x y	0.024390
to draw random numbers using numpy's	replace p	0.500000
this	gof clinker object c code	0.500000
extension	extension	1.000000
toposort return an	function graph toposort	0.125000
bitwise ~a inplace on a	invert inplace a	1.000000
input vector and t is	node input_storage	0.038462
a c contiguous version	gpu contiguous	0.083333
prod	prod axis dtype keepdims	1.000000
an apply_node recursively search from this	import apply_node	0.066667
lib directories that are needed by one or	header dirs	0.045455
then replace it with a triangular solve	linalg tag solve triangular	0.142857
that reduces scan memory consumption	scan save mem	0.200000
symbolically cast x to	cast x	0.200000
openblas threads	tensor openblas threads	0.250000
loop_orders	loop_orders	1.000000
c_code for convop that unroll the batch	code unroll batch kern d unroll_bsize unroll_ksize	0.125000
runs a series of wrapper functions instead	many linkers wrappers	0.047619
optimization to the provided l{functiongraph} it may	optimizer apply fgraph	0.200000
replacement	replace	0.032258
the broadcast pattern for advancedsubtensor output variable	pattern	0.028571
done	done	1.000000
given axis es of a tensor	axis dtype op	0.083333
multiplication	tensor mul	1.000000
c header for openblas threads	tensor openblas threads	0.250000
replace it with logsoftmax	logsoftmax node	0.125000
the	execute node	1.000000
the args are packed like this n_steps	node args outs	1.000000
in the list remove are still in the	remove fgraph replacements remove reason	0.055556
the "reverse-mode" gradient	grad perform node inputs	0.166667
this generates the c code for gpucorrmm (direction="forward"),	gpuarray base gpu corr mm c code	0.090909
addsd	addsd	0.555556
to pack c types back into	clinker type c sync	0.111111
list of shape tuple or	tensor shape feature default infer shape	0.066667
assemble the c code for this	init c code	0.333333
a memory alias that wasn't in	bad	0.013158
updates for matrix solve operation	tensor solve	0.038462
r to	r	0.057143
modifies input	gof give	0.500000
has only one client and that	only	0.050000
crossentropysoftmax1hotwithbiasdx op whose incoming gradient is	local useless crossentropy softmax 1hot with bias dx	0.111111
givens	givens	1.000000
trace to an node	trace	0.052632
additional_inputs	additional_inputs	1.000000
expm1	tensor local expm1 node	0.066667
in the list remove are still in the	fgraph replacements remove reason	0.055556
add tag trace	gof add tag trace thing user_line	0.166667
batch normalization	batch norm inference	1.000000
the template filled by broadcasting value through	tensor broadcast like value template fgraph	0.125000
optimizer	optimizer	0.437500
to compute the	tensor nnet	0.070175
code to pack c types back into a	gof clinker type c sync	0.111111
python object a that would	gof pure	0.033333
product between several dots	tensor matrix dot	0.333333
dict op -> total number of	profile stats class	0.500000
for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor	0.083333
of the specified pieces of vectors and matrices	sparse block outer make node	0.066667
func	func	1.000000
partition a list of variables into two kinds	scalarconsts	0.076923
function uses set and dictionary data structures	scan_module push out non seq scan	0.125000
:param execute if true execute	misc execute execute verbose m n	0.250000
the replacement if the ops in the list	gof replace validate replace	0.050000
standard elements	object	0.083333
the hack in profiling to print the mflops	conv op flops	0.125000
filters with	signals filters	0.111111
bitwise ~a inplace on a	tensor invert inplace a	1.000000
numeric shape of all intermediate variables given input	tensor shape of variables fgraph input_shapes	0.100000
see theano tensor std	tensor py operators std axis ddof keepdims	1.000000
exp a -1 and converts this to expm1	expm1 node	0.066667
unfortunately conda offers to make itself the	to os environ pathlist	0.038462
connection pattern of a subgraph defined	connection pattern	0.032258
from polar coordinate specification	from polar abs angle	0.250000
the last access of a given file	last access time	0.040000
scan	out scan	0.071429
merge 2 dicts by adding the	gof merge dict d1 d2	0.333333
dimensions from the shape or the	shape	0.010204
each shape that broadcast them to	tensor generate broadcasting	0.066667
of shape tuple or	infer shape	0.066667
image shape	shape kernel_shape	0.250000
values of a shared variable to 0	shared variable zero borrow	0.200000
the output	nnet conv op get output	0.047619
nodes to output nodes of	gof	0.002381
same computations	equal computations	0.333333
a crossentropysoftmax1hotwithbiasdx op whose incoming gradient is	crossentropy softmax 1hot with bias dx	0.111111
removes all asserts	remove all assert	0.055556
folowing changes in the graph	tensor local mul switch sink	0.045455
a clone in a	clone	0.020833
the list remove are still in	remove fgraph replacements remove reason	0.055556
inputs using the given mean and variance	test inputs gamma beta mean	0.500000
fgraph outputs that will replace their	fgraph expanded_inputs	0.058824
this function tries	image_shape top_shape border_mode subsample	0.166667
1/(1+exp x -> sigm	nnet local inv 1 plus exp	0.333333
fill inputted tensor with the assigned	py operators fill	1.000000
a new variable from x	new x	0.500000
op a list	op	0.009174
this should return an iterable	gpuarray gpu kernel base gpu kernels node name	0.166667
main diagonal set to a specified scalar	tensor fill diagonal offset a val offset	0.100000
type's	gpu array	0.055556
__unify_walk__ method for	gof unify walk a b u	0.037037
dimensions of this variable optionally inserting	tensor py operators	0.015625
it with a triangular solve	tag solve triangular node	0.142857
dimensions of this variable optionally	tensor	0.006431
context object mapped to	context	0.035714
inserting	tensor tensor py	0.015873
the llvm one or not	gof gcc llvm	0.200000
x and replace it with logsoftmax x	nnet local logsoftmax	0.076923
sum of non	sum	0.038462
arguments to pass to helper_c_code	helper c code args	0.250000
name the	name	0.011111
* y + alpha * dot a x	gemv c code y a x	0.500000
pydot graph and write to dot	d3viz d3write fct path	0.166667
more multinomial distributions	tensor multinomial random_state	0.040000
offers to make itself the	to	0.017544
upper bound on the largest	bound	0.043478
to this graph	gof function graph	0.031250
a config string (comma-separated key=value components) into a	config string config_string issue_warnings	0.166667
outputs	fgraph	0.012195
and "init_code"	init code struct node name sub	0.500000
on the inner graph to ensure that	scan_module scan validate inner graph	0.035714
optional	c	0.071429
this variable optionally inserting broadcasted	dimshuffle	0.014493
dot product	dot csr	0.111111
determine the name the object	name obj	0.111111
gradient is an alloc	alloc node	0.037037
1) it has a 'requirement' of the destroyhandler	destroy handler	0.055556
return connection pattern of subfgraph defined	op from graph connection pattern	0.076923
the connection	io connection	0.333333
to this graph	graph	0.016393
numpy's isclose on	isclose a	0.500000
mflops	base gpu corr mm flops inp outp	0.125000
return the cross-entropy between an approximating distribution	nnet categorical crossentropy coding_dist true_dist	0.111111
validations on the inner graph	scan validate inner graph	0.035714
step] transform it into a canonical form	tensor get canonical form	0.045455
a function that will calculate	compile orig function	0.166667
value has a __thunk_trace__	hook type value	1.000000
the convolution gradient with respect to the weights	dnn conv grad w	0.125000
of x the same	x	0.008772
functions that compute	scalar composite init py impls	0.166667
of the main diagonal set to a	tensor fill diagonal offset a val offset	0.100000
openblas threads interface	openblas threads text	0.250000
from the loaded cache or the disk	cache get	0.250000
new	new inputs inputs	0.166667
convenience function to roll tensortypes	tensor roll x shift	0.250000
raise baddestroymap if	node	0.007407
some requirements to the fgraph this is the	requirements fgraph	0.250000
mflops	flops inputs outputs	0.125000
the platform-dependent gcc	get gcc	0.333333
copies a vector to the diagonal of	diag	0.023810
variable optionally inserting broadcasted	py operators	0.015625
allocempty	allocempty	1.000000
of this variable optionally inserting broadcasted dimensions	tensor py operators	0.015625
function that	gof	0.004762
only on cpu here	tensor local pow specialize	0.250000
and	destroy	0.009709
epoch of the last access of a given	last access time	0.040000
tensor from polar	complex from polar	0.250000
return a tuple of integers indicating the version	version	0.093750
the method to override this should return an	node name	0.033333
can be a 1-d	random_state size a	0.333333
_maker which handles much of the	maker i o m	0.066667
the one hot	tensor to one hot	0.142857
stack	gof check stack	0.142857
perform the permutation	perform node	0.083333
fetch a compiled module from the loaded cache	cache get module name	0.166667
localoptgroup instead	group	0.047619
return a string	name	0.011111
order v real	v x	0.100000
reorder the dimensions	operators	0.017241
product of the specified pieces of vectors	sparse block gemv make	0.066667
builds the 2d kernel that can be	kernel 2d	0.050000
the g++ version used	gcc	0.023810
not in this	function	0.052632
how to generate c	compile register shape i c	0.250000
that unroll the batch	conv code unroll batch	0.166667
upper bound	bound	0.043478
t	t	0.714286
change the value after the import of	core config param init default filter	0.040000
mini-batch of a stack of	input_shape filter_shape	0.027778
list of l{codeblock} instances returns a	code gen blocks	0.050000
still	remove fgraph replacements	0.250000
tensor_from_scalar(scalar_from_tensor x -> x	tensor local tensor scalar	1.000000
of a	tensor	0.009646
replace_all_validate revert the replacement	replace validate replace all validate remove	0.111111
base class for functiongraph extensions	feature	0.083333
set of 3d	conv3d	0.076923
fill s v -> alloc(v	local fill to alloc node	0.250000
gradient is taken	grad x	0.333333
apply_node	apply_node	0.250000
compute sum of non	constant signature get sum	0.142857
a numpy ndarray contains any np inf values	compile contains inf arr node	0.500000
a reshaped view/copy of this variable	tensor py operators reshape shape ndim	0.111111
polar	polar	1.000000
this apply instance in a new graph	gof apply clone with new inputs inputs	0.250000
print the mflops	nnet conv op flops inputs outputs	0.125000
called whenever node inputs[i] is changed from	change input function_graph node i	0.333333
implementation of mod	scalar mod c	0.125000
important note this	out seq scan process node fgraph	0.142857
to help the navigator	gof navigator optimizer attach updater	0.038462
return c code to declare variables that	gof clinker type c declare name sub check_input	0.333333
in the forward but clip the gradient	core grad clip x lower_bound upper_bound	0.250000
to compile a dummy file with these	cls flag_list preambule body	0.250000
raise baddestroymap if necessary update dr_vals	dr_vals	0.111111
scalar values default int64	scalar shared	0.083333
change the value after the	default	0.030303
a 4-d tensor it sets all non maximum	patch_size	0.050000
n-d tensor	ws ignore_border stride	0.090909
a c contiguous version of the	contiguous	0.058824
replace a crossentropysoftmax1hotwithbiasdx op whose incoming gradient	crossentropy softmax 1hot with bias dx	0.111111
backpropagation	x multiplier	1.000000
memory alias	bad view	0.027027
es of	ddof keepdims	0.250000
return true if we are able	dim_x dim_y	0.090909
kernel shape of convolution gradweights	get conv gradweights shape	0.333333
broadcasted dimensions	py	0.014286
sparse matrix multiplication	sparse	0.019231
x default reverse them	transpose x	0.200000
bessel function of	tensor jv inplace	1.000000
if the ops in the list remove are	remove reason	0.142857
mflops	nnet conv op flops inputs	0.125000
move the abs toward the input	local abs lift node	0.333333
__unify_walk__ method for one of the objects	gof unify walk a b u	0.037037
as replace_all_validate revert the replacement if the ops	replace validate replace all validate remove fgraph	0.111111
parametrize it	max_input_fct maker	0.083333
subgraph between i and o parameters	i o	0.083333
type1 constitutes an upcast	tensor is an upcast type1	0.500000
zeros_like but forces the	core float zeros	0.166667
this is the equivalent	to gpulocal	0.055556
is the	gpulocal	0.055556
import six moves and	six	0.025000
work for gpuincsubtensor	tensor local inplace setsubtensor node	0.250000
this linker's fgraph	input_storage output_storage storage_map keep_lock	0.250000
i	shape i r i	0.500000
to print the mflops	conv op flops	0.125000
nodes	nodes	0.700000
normalization of	normalization	0.153846
work for gpuincsubtensor	setsubtensor	0.111111
simple algorithm	reasons r_vals	0.333333
solve	tag solve	1.000000
the complex conjugate of z	tensor conj z	0.333333
a legal value for a variable of this	is valid value a	0.076923
hack in profiling to print the mflops	tensor nnet conv op flops inputs	0.125000
named module	fullname	0.066667
takes a list of localoptimizer	opt group	0.043478
returns a short mostly hexadecimal hash of	core hex digest x	0.083333
the dimensionality of the var is equal to	tensor is flat var	0.200000
convert addsd to faster	addsd	0.111111
for scalar	scalar	0.017857
decorator to merge multiplication by a scalar	gpuarray alpha merge cls alpha_in beta_in	0.200000
would be a legal value for	is valid value	0.250000
returned by this	gof	0.002381
to	array type	0.055556
remove are still in the	fgraph replacements remove reason	0.055556
list of lib	clinker lib	0.333333
to wait on a previously received array	mpirecv wait	0.045455
return c code to declare variables that	gof clinker type c declare name	0.333333
the contents of a	dirname err files	0.083333
of a subgraph defined by given inputs and	inputs	0.012658
an apply_node recursively search from this node	import apply_node check reason	0.066667
helper function for diagonalsubtensor	nnet get diagonal subtensor	0.083333
computes the output shape	get out shape ishape kshape border_mode	0.500000
compiles the source code for this linker	cmodule location	0.038462
sample from one or more multinomial distributions defined	multinomial random_state size	0.333333
sample n (n needs to be >= 1	size n	0.090909
seq	seq	0.833333
helper function for diagonalsubtensor and	tensor nnet get diagonal subtensor view	0.083333
to py_none	init	0.058824
a new instance of this	clone link_kwargs optimizer	0.111111
and only	local	0.014085
that runs a series of	gof wrap linker many linkers wrappers	0.071429
pattern of subfgraph defined by inputs	pattern	0.028571
existing start gradients up	start	0.040000
3d inputs with a set of 3d	nnet conv3d	0.071429
a dictionary that	gof	0.002381
like zeros_like but forces the object to have	core float zeros like	0.200000
device we do it only on cpu here	local pow specialize device node	1.000000
arctangent	arctan	0.142857
a n-d tensor where	ws ignore_border stride	0.090909
with the ignore_trees-related functionality	updater fgraph importer pruner chin	0.250000
this variable optionally	tensor tensor py operators dimshuffle	0.019231
changed from r to	r	0.028571
:type	wrt consider_constant disconnected_inputs	1.000000
step] transform it into a canonical	get canonical	0.125000
gives unique names to an iterable of variables	variables names variables	0.333333
minimum elements obtained by iterating	keepdims	0.052632
according to a list	inputs outputs cmps	0.166667
where each row correspond to the one hot	tensor to one hot	0.142857
takes as	tensor signal pool 2d	0.142857
the inner graph to ensure	inner graph	0.035714
this optimization makes the folowing changes in	tensor local mul switch sink node	0.045455
det x and there is already an l=cholesky	linalg local det	0.166667
is inside a dimshuffle which	node	0.007407
diagonalsubtensor and	diagonal subtensor	0.083333
incsubtensor	incsubtensor	1.000000
of this mode	monitor mode	0.333333
"reverse-mode"	perform node inputs	0.333333
input	gpu	0.011765
last access of a	last access time path	0.040000
at all outputs defined by indices out_idxs	out_idxs	0.050000
increment	inc	0.166667
an apply_node recursively search from this node	import apply_node	0.066667
replace_all_validate revert the replacement if the ops in	replace all validate remove	0.111111
stack trace from	stack trace	0.055556
str	str	1.000000
images2neibs <theano tensor nnet neighbours images2neibs>	tensor nnet images2neibs ten4 neib_shape neib_step mode	0.333333
transform a subgraph whose output is node	gof local optimizer transform node	1.000000
file the graph of a compiled theano	fct	0.083333
elementary validations on the inner graph to	inner graph	0.035714
device we do it	device node	0.045455
and do some special work if	gof	0.002381
2d kernel that	kernel 2d	0.050000
navigator deal with the	gof navigator optimizer attach updater	0.038462
this is the	gpulocal	0.055556
tuple of integers indicating the version	version	0.093750
the main diagonal set to a specified scalar	tensor fill diagonal offset a val offset	0.100000
it with a triangular solve	solve triangular node	0.142857
with respect to wrt, computes gradients	core subgraph grad	0.062500
this	gof clinker object	0.500000
a variable with a	variable	0.022222
return connection pattern of subfgraph	graph connection pattern node	0.076923
the fgraph outputs that will replace their values	fgraph	0.012195
the op code	op	0.009174
twice gives inconsistent outputs	bad thunk output	0.200000
tensorvariable whose type is in t float_scalar_types	as scalar res dtype	0.500000
reshape t by inserting 1 at	shape padaxis t	0.333333
on f wrt to wrt	lop f wrt	0.200000
respects the conventions imposed	theslice length	0.052632
the basic variable class	variable	0.044444
of this op could be very easy if	tensor prod l op	0.033333
that removes all asserts	local remove all assert	0.055556
performs batch normalization of the	dnn batch normalization	0.125000
c code to declare variables	c declare	1.000000
localoptgroup for graphtogpu	to gpulocal opt group	0.055556
diagnosis if things go awry	gof function graph check integrity	0.250000
calling the method query	graph to gpudb	0.142857
[elementwise] smallest	smallest	0.125000
offers to make itself the default python	to os environ pathlist	0.038462
the gradient function should return	tensor matrix inverse r op	0.500000
gpu_from_host abstractconv	conv gpu conv	1.000000
unpack	unpack	0.625000
original graph to a new node	outputs copy_inputs_and_orphans memo	0.029412
the fgraph this is the place	fgraph	0.012195
canonical	get canonical	0.125000
inserting broadcasted	operators	0.017241
expression vector 1-dimensional	core jacobian expression	0.500000
-1 and converts this to expm1 a	expm1	0.050000
raised by get_scalar_const_value if	error	0.025000
of all variables which may share the same	infer reuse pattern	0.100000
change the value after the	core config param init default filter	0.040000
input that wasn't in	destroy	0.009709
return connection pattern of subfgraph defined by inputs	op from graph connection pattern	0.076923
a number of scalars	make	0.017857
tensor_from_scalar(scalar_from_tensor x -> x	tensor local tensor scalar tensor	1.000000
svd of a matrix :math	svd	0.034483
a diff to make code	diff code	0.333333
source code for this linker	clinker compile cmodule location	0.038462
for graphtogpu	graph to	0.055556
clone the graph and get	function graph clone get	0.333333
by a with a modulo of m1	m1	0.027027
the mean value along	tensor mean	0.111111
e^a	tensor exp	1.000000
required return c	name sub	0.050000
explicitly upcasts constant inputs to	constant inputs node	0.125000
in profiling to print the mflops	flops inputs	0.125000
the list remove are still in the graph	remove fgraph replacements remove reason	0.055556
to the fgraph outputs that will replace their	fgraph expanded_inputs	0.058824
the template filled by broadcasting value	broadcast like value template fgraph	0.125000
the dimensions of this variable optionally	tensor tensor	0.014286
the navigator deal with the	gof navigator optimizer attach updater	0.038462
input a	size input	0.500000
listeners to help the navigator deal with	gof navigator optimizer attach updater fgraph	0.038462
by doing a recursion over the input dimensions	tensor permute row elements rec	0.047619
fgraph and a list of variables returns the	fgraph	0.012195
optimize	optimize	1.000000
that unroll the batch size	unroll batch	0.166667
elements	elements	1.000000
of the specified pieces of vectors and	sparse block gemv make node	0.066667
of 2d filters	tensor nnet conv2d input filters	0.125000
implemented	implemented	1.000000
be turned into macros for use within	cop	0.028571
in	view	0.022727
return connection pattern of	compile op from graph connection pattern	0.076923
compute sum	tensor constant signature get sum	0.142857
helper function drawing from multinomial distributions	tensor multinomial helper random_state n pvals size	0.500000
a convolution with the	conv	0.037037
_maker which handles much of the	debug mode function maker i o m	0.066667
lower triangle of an array	tensor tril m k	0.250000
gpuincsubtensor	local inplace setsubtensor	0.250000
match a variable with the -x pattern	tensor nnet is neg	0.166667
dimensions of this variable optionally inserting broadcasted dimensions	tensor py operators dimshuffle	0.019231
respect to wrt, computes	core subgraph grad	0.062500
to theano config	config	0.100000
revert the replacement	replace validate replace	0.050000
update self rstate to	rstate	0.090909
revert the replacement if the	replace all	0.050000
of another op that takes the same inputs	op sub	0.066667
the idx_list with	idx	0.076923
new variable to theano config	config var name	0.500000
return the c implementation of	c code node name inputs outputs	0.250000
python litterals	tensor make	0.076923
computes the output dimensions of convolving an image	output	0.017241
an fgraph and a list	fgraph	0.012195
to	gpu	0.011765
inputs required to compute	gof inputs variable_list blockers	0.058824
inputs with a set of 3d	nnet conv3d input	0.125000
different	outputs tag	1.000000
input makes it use new_r instead	replace all pairs reason	0.333333
extract test value from v raises attributeerror if	test value v	0.250000
if l has any duplicates (according to	scan_module has duplicates l	0.111111
es of a	dtype op	0.250000
a warning message on the first	deprecated filename msg	0.041667
import	import r	1.000000
the dimensions of this variable optionally inserting	tensor tensor py	0.015873
to be inserted in the module initialization code	init code	0.142857
node by one which computes the specified outputs	fgraph node	1.000000
only if this enum has this	gof enum type has	0.111111
input of given shape and flags	pool grad out shape imgshape ws ignore_border stride	0.200000
the type optionally with a new dtype	tensor tensor type clone dtype	0.333333
bitwise ~a	invert	0.166667
variable optionally inserting	tensor py operators dimshuffle	0.019231
c_extract_out that	gof get c extract out	0.500000
modified bessel function of	tensor i1	1.000000
broadcastable	broadcastable	0.777778
feature should remove	gof feature on detach	0.200000
type's	gpuarray gpu array type	0.062500
temporary directory whose base will be created if	gof	0.002381
computes gradients of cost and/or	cost	0.045455
constant	elemwise python constant	1.000000
unfortunately conda offers to make itself the	to os environ pathlist var newpath	0.038462
if	gof	0.011905
a sparse	core sparse	0.066667
updates ordereddict the	scan_module get updates	0.034483
a transfer function for alternative targets	transfer fn	0.125000
* y + alpha * dot	gemv c code y	0.333333
it with logsoftmax x 's	tensor nnet local logsoftmax	0.076923
is inside	node	0.014815
vm	vm	1.000000
list of l{codeblock} instances returns	gof code gen blocks	0.050000
to determine the broadcast pattern for	adv index broadcastable pattern	0.066667
gradients up to the end variables of	grad wrt end	0.050000
and return full path of the dynamic	gof module name	0.076923
function for diagonalsubtensor	get diagonal subtensor view x i0 i1	0.083333
to get the	get depth	0.050000
helper function for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x i0 i1	0.083333
function into a basic theano op	op itypes otypes infer_shape	0.047619
the dimensions of this variable optionally inserting broadcasted	tensor tensor py operators dimshuffle	0.019231
that wasn't in the destroy_map	bad	0.013158
a &	and a	1.000000
output dimensions of convolving	tensor nnet conv op get output	0.047619
for	gpulocal	0.055556
offers to	to os environ pathlist	0.038462
op could be very	tensor prod l op	0.033333
fast fourier transform of a real-valued input	tensor rfft inp norm	0.142857
with a particular	setitem item val	0.125000
freev is unified to boundvariable(other_object)	walk fv o u	0.200000
signature object for comparing tensorconstant instances	tensor constant signature	0.100000
combine gpukernelbase and	cgpu kernel base	0.500000
scrap the dimshuffle and index the	local dimshuffle	0.052632
called whenever node inputs[i] is changed from	on change input function_graph node i	0.333333
a canonical form that	tensor get canonical form	0.045455
compute the	nnet	0.096774
other arguments	ndim bcast ndim	0.333333
subtensor(setsubtensor x y idx idx) ->	local subtensor inc subtensor	0.500000
generates the c code for gpucorrmm (direction="forward"),	gpu corr mm c code	0.090909
d2 to d1+size d2	scan_module expand empty tensor_var size	0.166667
scalar values default int64 or	scalar shared	0.083333
search through a	stack search	0.333333
sparseblockouter see sparseblockouter's	outer	0.083333
this is the main interface to manipulate the	r new_r reason verbose	0.071429
merge multiplication by a scalar	alpha merge	0.076923
data by walking the cache directory structure	module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
outer product of two sets of pieces	sparse block outer	0.047619
the bartlett spectral window in the	bartlett	0.058824
for openblas threads	openblas threads	0.250000
and replace it with logsoftmax x	tensor nnet local logsoftmax	0.076923
if the	gcc	0.023810
upper triangle of an array	triu m k	0.250000
since the epoch of the last access	last access time path	0.040000
decorator or context manager	flags	0.062500
maximum in	tensor maximum x	0.142857
the dependence of	gof make dependence	0.043478
of x the	flatten x	0.166667
change the value after the import	init default	0.040000
functiongraph and also their apply_node if those	gof	0.002381
a uniform into sample without replacement from a	choice from uniform	0.166667
upcasts constant inputs to elemwise	elemwise constant inputs	0.250000
named module is a package	meta path importer is package fullname	0.250000
unroll_ksize	unroll_ksize	1.000000
the passed-in key is found in	get from key key key_data	0.111111
and replace it with logsoftmax x 's grad	tensor nnet local logsoftmax grad node	0.200000
outputs_info	outputs_info	0.714286
row is a mrg stream state and they	mrg random streams	0.033333
module	get module	0.200000
a dimshuffle which only adds dimension to	dimshuffle	0.014493
"lifts" dimshuffle through elemwise operations and merges consecutive	dimshuffle lift	0.250000
is false we can't change the value after	config param init default filter	0.040000
variables in fgraph that cannot be merged together	merge feature	1.000000
elemwise complex conjugate of	sparse conj	1.000000
as the template filled by broadcasting value through	tensor broadcast like value template fgraph	0.125000
attempting to use dnn conv algo_bwd	core safe no dnn algo bwd algo	0.166667
an array with	node	0.014815
this is just speed opt not for stability	ultra fast scalar sigmoid	1.000000
modified bessel function of	tensor i1 inplace	1.000000
the equivalent of localoptgroup	opt group	0.043478
raise baddestroymap if	compile check inputs	0.166667
be inserted at struct	struct	0.047619
g_outputs	g_outputs	1.000000
apply nodes in the original graph to	inputs outputs copy_inputs_and_orphans memo	0.029412
fill s v -> alloc(v shape s this	local fill	0.250000
generate a diff to make	diff	0.071429
upcasts constant inputs to elemwise	elemwise constant inputs node	0.250000
value after the import of	init default	0.040000
optionally inserting broadcasted	tensor tensor py operators	0.015625
apply the list	apply	0.016667
we can replace that with the *args directly	sparse local csm properties csm	0.142857
inserted in the struct	struct	0.047619
stream	streams	0.230769
wasn't	bad	0.026316
last access of	last access	0.040000
the stack trace from one	gof copy stack trace	0.055556
jv	jv	1.000000
pushing out the	out	0.018519
c	register shape i c	0.250000
exception raised	error	0.025000
a mrg stream state and they are spaced	mrg random streams	0.033333
new_r	new_r reason	0.500000
variable v if v is the	v	0.011111
and matrices returning pieces of vectors :	sparse block gemv	0.166667
absolute tolerance used as threshold for gradient	abs_tol	0.166667
the types involved in this node	inc subtensor do type checking node	0.250000
to wait on a previously sent array using	wait	0.022727
for	tensor tensor type	0.041667
return c code to declare variables	clinker type c declare name sub check_input	0.500000
return a reshaped view/copy of this variable	tensor py operators reshape shape ndim	0.111111
takes as input a n-d tensor	tensor signal pool 2d input ws ignore_border stride	0.100000
returns true if var is always equal to	gpuarray is equal var	0.250000
copies the stack trace from one or	copy stack trace	0.055556
a signature object	tensor constant signature	0.100000
do	do	1.000000
exception object with	raise with op node thunk	0.333333
to get the	type get	0.050000
theano constants in subtensor arguments	make constant args	0.250000
function uses set and dictionary data structures	non seq	0.111111
the hack in profilemode to print the mflops	gpuarray base gpu corr3d mm flops inp outp	0.125000
and chooses the orphans among them	orphans	0.090909
transfer	transfer	0.411765
the method to override this should return	name	0.011111
to determine the	tensor adv index broadcastable	0.050000
the output dimensions of	conv op get output	0.047619
i	i	0.846154
to support version-based cache mechanism	code version version	0.333333
c code to	c	0.053571
string representation of broadcastable	broadcastable to str b	0.500000
this is meant as a shortcut to	gof optimizer optimize fgraph	0.200000
updates ordereddict the list of	scan_module get updates	0.034483
reorder	tensor py	0.015873
the fast fourier transform of a real-valued input	tensor rfft inp norm	0.142857
connection pattern of subfgraph defined	op from graph connection pattern	0.076923
the first outdim-1 dimension size s of x	tensor flatten x ndim outdim	0.333333
the l operation on f wrt to wrt	lop f wrt	0.200000
along the given axis es of a	axis dtype keepdims	0.083333
an array from an index array and a	a	0.008065
list remove are still in the	replacements remove reason	0.055556
transfer to a tensortype	transfer	0.058824
inner nit_sot output of	inner sitsot	0.083333
view	compile view	0.500000
compute the dot product of the specified	tensor nnet	0.035088
graph and get a	function graph	0.040000
after pad_dims	gpuarray unpad dims	1.000000
ignore	ignore	0.833333
graphtogpu	gpulocal opt	0.055556
context object mapped to the	gpu array type context	0.090909
fill	second inplace	1.000000
pushing out	push out	0.037037
function performs the svd on	tensor svd a	0.200000
that allows no values	null	1.000000
convolve spatio-temporal filters with	signals filters	0.111111
by the inputs and outputs	inputs outputs	0.066667
diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view	0.083333
partition a list of variables into	tensor scalarconsts	0.125000
return a hash	hash	0.055556
complex otherwise call upgrade_to_float()	upgrade to float no complex	0.333333
post some text to a gist and	post gist	0.333333
a list of header	header	0.100000
functiongraph a constant	constant	0.016667
real-valued input	tensor rfft inp norm	0.142857
can take any	free	0.166667
the convolution gradient with respect	dnn conv grad i	0.125000
get the 0 based level of	type get	0.050000
spatio-temporal filters with a	tensor nnet conv3d signals filters	0.111111
dimensions of this variable optionally inserting broadcasted	py	0.014286
symbolic row variable (ndim=2 broadcastable=[true	row name	0.050000
true for small or builtin c types	type c is simple	0.250000
isclose	isclose	1.000000
with a triangular solve	solve triangular	0.142857
to this graph	function graph import	0.125000
some requirements to the fgraph	requirements fgraph	0.250000
takes as input	tensor signal pool 2d input	0.090909
ceil	ceil	1.000000
can replace that with the *args directly	sparse local csm properties csm	0.142857
the end	end	0.040000
if	gof gcc	0.027778
supplied function as its	as	0.024390
the constant scalar	get scalar constant	0.333333
bitwise a | b	or a b	1.000000
change the value after the import of theano	config param init default	0.040000
return the platform-dependent gcc argument for shared libraries	get gcc shared library arg	0.333333
apply as many times as	apply	0.016667
function uses set and dictionary data structures	push out non	0.125000
of 3d	conv3d input	0.125000
of this op could be very easy if	prod l op	0.033333
of headers that are	headers	0.038462
the type optionally with a new dtype	type clone dtype	0.333333
the cache	gof call cache	0.200000
inserting	tensor py operators	0.015625
c types	type c	0.071429
compute the image shape of convolution gradinputs	tensor nnet get conv gradinputs shape kernel_shape	0.500000
or 3d convolution for	tensor nnet base abstract conv conv	0.125000
scan to outside of scan	scan output	0.125000
with constant inputs replaced by their	constant idx inputs allow_partial only_process_constants elemwise	0.071429
revert the replacement if the ops in	replace	0.032258
the method that attaches	gof bookkeeper on attach	0.142857
and y have the	y	0.026316
the updates	get updates	0.034483
compare true	type eq	1.000000
the input to	input	0.023810
gradient for the eigensystem	eigh grad	1.000000
the output	tensor nnet conv op get output	0.047619
a schedule function from comparators	schedule fn	0.333333
to name	name	0.011111
revert the replacement if the ops	validate replace	0.050000
to compile c code when doing constant folding	tensor elemwise python constant folding	0.142857
a tuple of integers indicating the version	c code cache version apply node	0.125000
call the supplied function as its	compile as	0.050000
revert the replacement	replace validate replace all	0.050000
perform the	perform node x y inverse	0.166667
and dictionary data structures	out non seq scan	0.125000
that runs a series	gof wrap linker many linkers wrappers	0.071429
python not the other implementation of mod	mod c code node	0.125000
nodes according to a list of comparators	nodes inputs outputs cmps	0.166667
unversioned_min_age	unversioned_min_age	1.000000
loading of moved objects in six moves urllib_response	module six moves urllib response	0.333333
fill s v -> alloc(v shape s	tensor local fill	0.250000
an internal theano problem	debug mode	0.200000
scalar op	scalar	0.035714
context object mapped to the type's :attr	array type context	0.090909
the fgraph outputs that will replace	fgraph	0.012195
computes the mean value	mean	0.062500
important note this function uses set and dictionary	scan_module push out seq scan process node fgraph	0.142857
for abstractconv	abstract conv	0.666667
node inputs[i] to new_r	change input node i new_r	0.500000
that was dumped to	f persistent_load	0.052632
the subgraph contained between i and o	clone i o copy_inputs	0.333333
global optimizer for pushing out the	push out	0.037037
the svd on cpu	tensor svd a full_matrices compute_uv	0.200000
sparse format instead of dense	core sparse	0.066667
a convolution with the	dnn conv	0.090909
variable	tensor tensor	0.014286
warning message	deprecated filename msg	0.041667
optimization makes the folowing changes in the graph	mul switch sink node	0.045455
elements should be inserted to maintain order	searchsorted x v side sorter	0.142857
to merge multiplication by a scalar	alpha merge	0.076923
optionally	tensor tensor py operators	0.015625
angle	angle	1.000000
abstract op for	abstract	0.222222
diag	diag	0.119048
of variables	gof variables	0.125000
[floor] division inverse	tensor int div	0.250000
hash equal for same kinds of tensortype	tensor type hash	0.166667
the folowing changes	local mul switch sink node	0.045455
on the inputs and put the variables	gof pure op perform node inputs output_storage params	0.047619
gradient the finite	grad	0.010417
in a functiongraph a constant that is cached	cached constant error	0.200000
trace from one or	trace	0.052632
dimshuffle which only adds dimension to the	local dimshuffle	0.052632
indentation	indentation	1.000000
"reverse-mode" gradient for	grad perform node inputs outputs	0.083333
the first functiongraph that has	gof	0.002381
value after the	param init default filter	0.040000
gets a scan op a list	op not_required	0.071429
modulo of m1 and the second half	m1	0.027027
in the list remove are still	fgraph replacements remove reason	0.055556
given an apply_node recursively search from this	apply_node check reason	0.066667
compute 2d kernel for	nnet	0.016129
decorator to merge multiplication by a	gpuarray alpha merge cls alpha_in beta_in	0.200000
upcasts constant	constant	0.016667
baddestroymap	check inputs	0.125000
apply nodes according to a list of comparators	gof sort apply nodes inputs outputs cmps	0.050000
to pass to helper_c_code	inc subtensor get helper c code	0.142857
pieces of vectors and	sparse block outer make node o x y	0.066667
should be removed	compress outs	0.076923
and figure out which nodes it needs to	scan_module traverse out x x_copy d	0.047619
optimizer remove shapefeature as an fgraph feature	un shape optimizer	1.000000
c code for gpucorrmm (direction="forward"), gpucorrmm_gradweights	gpu corr mm c code	0.090909
division (inplace on a)	tensor true div inplace a	1.000000
convolve spatio-temporal filters with a	signals filters	0.111111
wasn't in	view	0.022727
from a uniform distribution	tensor uniform random_state size	0.125000
elemwise and gpuelemwise op	tensor local elemwise fusion op op	0.200000
assert	assert	0.555556
dimshuffle which only adds dimension to	tensor local dimshuffle	0.052632
a dimshuffle which only adds dimension to the	local dimshuffle	0.052632
(direction="forward"), corr3dmm_gradweights (direction="backprop weights"),	helper bottom weights top direction	0.055556
print the mflops	gpu corr mm flops inp outp	0.125000
lib directories	clinker header dirs	0.055556
for graphtogpu	to gpulocal	0.055556
to the type's :attr	gpu array	0.055556
bartlett spectral window in	tensor bartlett m	0.083333
eigenvalue of square symmetrix matrix	sandbox linalg spectral radius	0.166667
connection pattern of	op from graph connection pattern node	0.076923
replacement if the	gof replace validate replace all	0.050000
to make	to os environ pathlist var	0.038462
an object for getting and setting this configuration	configparam	0.166667
and apply nodes in the original graph to	outputs copy_inputs_and_orphans memo	0.029412
input to a leftdims + rightdims	input leftdims rightdims	0.333333
the mflops	tensor nnet conv op flops inputs outputs	0.125000
the inner-most loop executes	loop	0.027778
impossible to evaluate	destroy	0.009709
optimization that deals with allocempty this will	allocempty op idx	0.250000
pieces of vectors and matrices	sparse block outer make node	0.066667
diagonalsubtensor	nnet get diagonal subtensor	0.083333
this optimization	tensor local	0.025641
-> single prod()	tensor local op of op node	0.500000
by a specified factor takes as	signal pool	0.142857
variable on the	as gpuarray variable	0.166667
this variable optionally inserting broadcasted dimensions	tensor tensor py	0.015873
runs a series of wrapper functions instead	wrap linker many linkers wrappers	0.047619
the output dimensions of convolving	nnet conv op get output	0.047619
cosine of	tensor cos	1.000000
represent the dependence of nodes in a	dependence	0.035714
from existing start gradients up to the end	end start	0.166667
if	storage_map r_vals	0.166667
compiled module from the loaded cache or the	gof module cache get module name	0.166667
of mod	scalar mod c code node name inputs	0.125000
reorder the dimensions	tensor tensor py operators dimshuffle	0.019231
the	gpuarray	0.046512
to type2	type2	0.050000
all unknown variables and apply_nodes to this graph	function graph	0.040000
the given axis	axis	0.102564
loop over several arrays	loop	0.027778
decorator which will print a warning message	deprecated filename msg	0.041667
3d filters	tensor nnet conv3d input filters	0.142857
and t is a	node input_storage	0.038462
to the fgraph outputs that will replace	fgraph expanded_inputs	0.058824
converts number to string by rendering it	char from number number	0.142857
tensor from polar coordinate specification	from polar abs angle	0.250000
add some requirements	add requirements	0.333333
reorder the dimensions of this variable	tensor tensor py operators dimshuffle	0.019231
tell rebroadcast how to generate c code for	compile register rebroadcast c code typ code version	1.000000
and max	max	0.062500
how to generate c	compile register specify shape c	0.250000
of the type optionally with a new dtype	tensor tensor type clone dtype	0.333333
with the *args directly	local csm properties csm	0.142857
imag	imag	1.000000
a memory alias	view	0.022727
folowing changes in	mul switch sink node	0.045455
change all sigmoid to ultra_fast_sigmoid	local ultra fast sigmoid	0.200000
if and only if this	gof	0.002381
function for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor	0.083333
keeping the first outdim-1 dimension size s	ndim outdim	0.142857
data to	data strict allow_downcast	0.500000
a memory alias	bad view	0.027027
baddestroymap if	storage_map r_vals	0.166667
function on the inputs and put the variables	gof pure op perform node inputs output_storage params	0.047619
default that removes all asserts from the graph	local remove all assert	0.055556
support the types involved	tensor inc subtensor do type checking	0.142857
to assert that x and y have the	x y	0.024390
makes the folowing changes	tensor local mul switch sink	0.045455
inputs with a set of 3d	tensor nnet conv3d	0.071429
inverse error function	tensor erfinv a	1.000000
for efficiently calculating the dot product	dot	0.035714
all device we	device node	0.045455
proxy for either true_div or int_div depending	tensor div proxy	0.125000
to the type's	gpu	0.011765
warning message on	deprecated filename msg	0.041667
a six moves urllib namespace that resembles the	six	0.025000
it with logsoftmax x	tensor nnet local logsoftmax node	0.142857
a schedule function from comparators	gof sort schedule fn	0.333333
-a	neg	0.083333
around c_init that	gof get c	0.166667
vil	vil	1.000000
type	type has type	1.000000
apply instance from set which must be computed	gof destroy handler on prune fgraph app reason	1.000000
whose base will be created if	gof	0.002381
search through a graph either breadth- or depth-first	stack search start expand mode build_inv	1.000000
default that removes all asserts from the graph	tensor local remove all assert	0.055556
a nested loop over	loop	0.027778
generates the c code for corrmm (direction="forward"), corrmm_gradweights	nnet base corr mm c code	0.090909
elementwise [floor] division inverse of	tensor int div a b	0.333333
kinds of useless	tensor local useless	0.111111
replacement if the	validate replace all	0.050000
profiles returned by this	gof seq optimizer	0.200000
to a name	name	0.011111
memory	view	0.022727
apply instance in a new	apply clone with new inputs	0.500000
the matrix inverse	matrix inverse	0.111111
memory is aliased that should not be	aliased memory error	1.000000
scale	scale x s	0.500000
|	tensor or	0.333333
smallest	smallest	0.625000
compute conv output	tensor nnet	0.052632
to use with	gpuarray gpu inc subtensor get	0.333333
and replace it with logsoftmax x	tensor nnet local logsoftmax node	0.142857
isclose	tensor isclose	1.000000
the abs and rel error of gradient	grad abs rel	0.333333
remove	local subtensor remove	1.000000
a thunk that operates on	gof linker make thunk	0.045455
constant	tensor subtensor get constant	1.000000
baddestroymap	node	0.007407
new node a clone in a	clone	0.020833
an alloc and only adds dimension to	local alloc	0.111111
an apply_node recursively search	apply_node check reason	0.066667
then replace it with a triangular	triangular	0.076923
the inner graph to ensure that it	scan validate inner graph	0.035714
loop executes	tensor make reordered loop	0.111111
compute 2d kernel	nnet	0.016129
is the equivalent	graph to	0.055556
leaf	leaf	0.400000
return connection pattern of subfgraph defined by	connection pattern node	0.076923
time icluding	times	0.100000
as python not the other implementation of mod	scalar mod c code node name inputs outputs	0.125000
decrefs py_name	get c cleanup r	1.000000
row variable	tensor row name	0.050000
of	gof	0.002381
of a compiled graph have a stack	gof check stack	0.142857
name to write data	gpuarray write w dtype	0.200000
call	call	0.833333
fgraph this is the place	fgraph	0.012195
allocates	alloc diag	0.027027
to merge multiplication by a scalar	gpuarray alpha merge	0.076923
:attr context_name	gpuarray gpu array	0.062500
return	node name	0.166667
apply_node if those nodes are not in this	gof function	0.043478
for matrix solve operation	tensor solve	0.038462
elemwise complex conjugate of x	sparse conj x	1.000000
anymore and should be removed and	scan_module compress outs	0.076923
x is an input vector and t is	node	0.007407
by default that removes	remove	0.035714
install some functiongraph listeners to help the navigator	gof navigator optimizer	0.038462
then replace it with a triangular solve	solve triangular	0.142857
constitutes an	is an	1.000000
and reduce pattern has functioning c code	careduce cuda supports c code	0.250000
dtype as the template filled by broadcasting value	tensor broadcast like value template fgraph dtype	1.000000
context object mapped to the	array type context	0.090909
remove are still in	remove fgraph replacements remove reason	0.055556
the output dimensions of convolving an image of	tensor nnet conv op get output	0.047619
this function	function	0.052632
value for a variable	value a	0.250000
two kinds of useless	useless	0.076923
the type's :attr	gpuarray gpu	0.045455
v by	v	0.011111
factor takes as input a n-d tensor	signal pool 2d input ws ignore_border stride	0.100000
source code for this linker	compile cmodule location	0.038462
the input by a specified factor takes as	signal pool	0.142857
gets a scan op a list of indices	op not_required inputs	0.071429
navigator deal	navigator optimizer attach	0.038462
diagonalsubtensor and	nnet get diagonal subtensor view x i0	0.083333
y + alpha * dot	tensor gemv c code y	0.333333
on wraplinker that runs a series	gof wrap linker many linkers wrappers	0.071429
convert addsd to faster	addsd ccode node	0.250000
the inputs required to compute the	gof inputs variable_list blockers	0.058824
into a canonical	tensor get canonical	0.125000
inserting broadcasted	tensor	0.006431
to expm1	tensor local expm1 node	0.066667
the "reverse-mode" gradient for the eigensystem	eigh grad perform node	0.333333
of trailing spaces tabs	misc hooks	0.250000
graph and	graph	0.032787
create an functiongraph	function graph init	0.333333
advancedincsubtensor1(x x[ilist]+other ilist set_instead_of_inc=true) ->	local set to inc subtensor node	1.000000
inserting broadcasted	py operators dimshuffle	0.019231
a graph of apply nodes according to a	apply nodes inputs outputs cmps	0.050000
tries to	top_shape border_mode	0.166667
c code to initialize	type c	0.071429
arguments to	args	0.025641
a compiled module from the loaded cache or	gof module cache get module	0.166667
value	value ptr	0.500000
raise baddestroymap if	storage_map r_vals	0.166667
failure_callback for navigatoroptimizer	warn inplace exc nav repl_pairs local_opt	1.000000
*args directly	csm properties csm	0.142857
a string specifying to the user what obj	core min informative str obj indent_level _prev_obs _tag_generator	0.333333
localoptimizer and applies them to	local opt group	0.052632
y -> x	tensor local	0.025641
converts self _rop_op from user supplied form to	op from graph recompute rop op	0.200000
in the original graph	outputs copy_inputs_and_orphans memo	0.029412
each row is a mrg stream state and	mrg random streams	0.033333
r-operator for the	max pool rop	0.142857
corresponding element of a dense vector	s	0.071429
with logsoftmax x	nnet local logsoftmax	0.076923
the scan that depend only on	seq scan	0.250000
given an fgraph and a list of variables	fgraph	0.012195
svd of a matrix :math a	svd	0.034483
to a max	local max	0.250000
for openblas	openblas	0.111111
one	tensor to one	0.125000
this apply instance in a new graph	gof apply clone with new inputs inputs strict	0.250000
exception while annotating	exc_info storage_map	0.250000
filters with a movie	conv3d signals filters signals_shape filters_shape	0.333333
badviewmap exception when it detects the	compile check viewmap node storage_map	0.111111
a new	clone with new inputs inputs	0.166667
->	tensor local	0.153846
conv output gradient w r	conv2d grad	0.111111
the mflops	flops inputs outputs	0.125000
check if converting to type2 from type1	type1 type2	0.166667
object into its key_pkl	save pkl	1.000000
sample from one or	size n	0.090909
to load	gpuarray load	0.200000
to the user what obj is the string	obj	0.083333
es of a tensor	dtype keepdims	0.250000
to merge multiplication	gpuarray alpha merge	0.076923
try	try	0.666667
the convolution gradient with respect to	gpu dnn conv grad	0.125000
nodes in the original graph	inputs outputs copy_inputs_and_orphans memo	0.029412
specified pieces of vectors and matrices	sparse block outer make node o	0.066667
value after the	config param init default filter	0.040000
to wrt, computes	subgraph grad wrt	0.062500
force	force	1.000000
into a canonical form	get canonical form	0.045455
input vector and t is	node	0.007407
[advanced]incsubtensor[1], whose increment is an alloc of a	tensor local useless inc subtensor alloc node	0.166667
the grad of this op	l op	0.033333
over given axis	axis	0.051282
to get	type get	0.050000
a special compound l{op} for	softmax argmax1hot	0.083333
of lib directories that are	clinker lib dirs	0.055556
convolution with the	conv get	0.100000
maintain order	tensor searchsorted x v side sorter	0.142857
diagonalsubtensor	diagonal subtensor	0.083333
function tries to recognize the updates ordereddict the	get updates	0.034483
a mini-batch of a stack of	input_shape filter_shape	0.027778
make the input broadcastable in the specified axes	addbroadcast x	0.142857
fill s v -> alloc(v shape s this	tensor local fill to alloc node	0.250000
compile lock to	module cache add to	0.142857
the outputs	outputs	0.090909
module is	meta path importer is	0.250000
primitive c type of items into variables handled	type c element	1.000000
only one client and that client is	only	0.050000
enum has this	enum type has	0.500000
as replace_all_validate revert	all validate remove	0.166667
loop over several arrays and	loop	0.027778
make a nested loop over several arrays and	tensor make loop	0.200000
of _maker which handles much of	maker i o m	0.066667
this functiongraph and also their apply_node if those	gof	0.002381
apply to be	apply	0.016667
for theano scalar scalar	scalar	0.017857
has any duplicates (according to __eq__)	scan_module has duplicates	0.333333
with the	like	0.111111
the value after	default	0.030303
gpuincsubtensor	local inplace setsubtensor node	0.250000
not try to use complex numbers	mod check x y	0.166667
see theano tensor prod	tensor tensor py operators prod	1.000000
helper function for diagonalsubtensor and	diagonal subtensor view x i0 i1	0.083333
the outputs of specific ops	trace f_or_fgraph ops_to_check bug_print	0.035714
in the list remove are still	remove fgraph replacements remove reason	0.055556
the inputs according to the	inputs	0.012658
helper function for diagonalsubtensor	diagonal subtensor	0.083333
x to be between min and max	x min max	1.000000
this is	gpulocal opt	0.055556
the list remove are still in	fgraph replacements remove reason	0.055556
reps	reps	0.857143
a < b	tensor lt a b	1.000000
the same shape	shape feature same shape	0.333333
in a new	clone with new inputs inputs	0.166667
replace_all_validate revert the replacement if	gof replace validate replace all validate	0.111111
given an fgraph and	fgraph	0.012195
reorder	dimshuffle	0.014493
the navigator deal with the	gof navigator optimizer	0.038462
insert deepcopy in	insert deepcopy	0.333333
replacement	gof replace validate replace	0.050000
return a symbolic row variable (ndim=2 broadcastable=[true false])	tensor row name dtype	0.050000
important note this function	push out seq scan process node fgraph node	0.142857
that use	nnet conv op use	1.000000
inner	validate inner	0.142857
and	format as	1.000000
-> alloc(unary x shp)	tensor local alloc unary	0.250000
the 1d kernel that can be	kernel 1d	0.050000
return full path of the dynamic lib	gof module name	0.076923
and apply nodes in the original	inputs outputs copy_inputs_and_orphans memo	0.029412
tensorvariable of	make variable	0.166667
fgraph and a	fgraph	0.012195
this graph	gof function graph	0.031250
connection pattern of subfgraph defined by inputs	graph connection pattern	0.076923
rval	rval	1.000000
the maximum in	tensor maximum x	0.142857
the existence of the __unify_walk__ method for one	unify walk a b u	0.037037
the epoch of the last access	last access time path	0.040000
that operates on	gof linker	0.250000
if the alloc	alloc	0.012500
the connection	connection	0.100000
disabled by default that removes all asserts	tensor local remove all assert	0.055556
variable optionally	tensor tensor py operators dimshuffle	0.019231
optional return	clinker object c	1.000000
new graph	new inputs inputs strict	0.166667
apply instance from set which must be computed	prune fgraph app reason	1.000000
return the idx_list with constant inputs replaced	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
return a list of	get	0.020833
on the inputs and put the	gof pure op perform node inputs output_storage params	0.047619
create a comparator to represent the dependence of	gof make dependence cmp	0.111111
upper	triu	0.166667
see theano tensor std	tensor py operators std axis ddof keepdims corrected	1.000000
unified to boundvariable(other_object)	fv o u	0.200000
perform the permutation by	perform node x	0.166667
x and there is already an	linalg local	0.142857
tensor_from_scalar(scalar_from_tensor	tensor scalar tensor	1.000000
python litterals	tensor	0.003215
a list of library search paths	lib dirs	0.045455
the dimshuffle and index the	local dimshuffle	0.052632
reorder	dim shuffle	0.500000
into a vector	vector	0.066667
only avail on compute capability 2	dev20	0.166667
dense vector	s	0.071429
conda offers to make itself the	to os environ pathlist var newpath	0.038462
draw samples from a poisson distribution	base poisson size lam ndim dtype	1.000000
an op	op	0.045872
sqr	sqr	0.625000
from the shape or	shape	0.010204
py_none	c init	0.500000
to wait on	mpirecv wait	0.045455
multiplication by a scalar on the	alpha	0.083333
1 0/a inplace on	inv inplace	1.000000
to replace	replace	0.032258
of inplace	inplace	0.025641
for the minimum in one tensor	tensor minimum x y	0.090909
is the	no_recycling profile	0.250000
symbolic input	input	0.047619
given axis es of a tensor	axis dtype keepdims	0.083333
new graphtogpu optimizer	register opt2 tracks	0.250000
cache directory and return	name from dir	0.250000
stop step] transform it into a canonical	tensor get canonical	0.125000
a symbolic row variable (ndim=2 broadcastable=[true false])	row name dtype	0.050000
if the	gof	0.002381
required return	name sub check_input	0.500000
the subgraph bound by the inputs	init inputs	0.083333
clip	tensor clip	1.000000
a signature object for comparing	constant signature	0.100000
sparseblockouter(inplace=false) -> sparseblockouter(inplace=true)	nnet local inplace sparse block outer node	1.000000
the dimensions	tensor py	0.015873
by default that removes all asserts from	tensor local remove all assert	0.055556
the permutation by doing a recursion over the	tensor permute row elements rec	0.047619
of arguments to	args	0.025641
mflops	base abstract conv flops inp outp	0.125000
some functiongraph listeners to help the navigator deal	gof navigator optimizer attach	0.038462
an unification in u and	o u	0.037037
the output dimensions of convolving an image of	get output	0.047619
compute	tensor nnet get	0.250000
diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor	0.083333
a dimshuffle which only adds dimension to the	dimshuffle	0.014493
to the apply to	apply node	0.031250
as its implementation	as	0.024390
performs the svd on	tensor svd a	0.200000
w r t its weights	wrt weights input output_grad filter_shape input_shape	0.333333
ignore all errors	gof navigator optimizer warn ignore	1.000000
v	v	0.122222
to the provided l{functiongraph} it may	gof optimizer apply fgraph	0.200000
return the inputs	inputs	0.012658
gradient wrt inputs for gpucorrmm	gpu corr mm grad inputs	1.000000
see theano tensor sort	py operators sort	1.000000
abstract op for batch normalization	abstract batch norm train	1.000000
of this variable optionally inserting	tensor tensor py operators	0.015625
replacement if the ops in the	gof replace validate replace	0.050000
one or more multinomial distributions defined by	multinomial random_state	0.040000
patterns	patterns	0.833333
removes all from the clients list of r	function graph remove client r client_to_remove	1.000000
a dimshuffle is inside an alloc and only	local alloc dimshuffle node	0.166667
output shape	out shape	0.250000
make the input broadcastable in the specified axes	tensor addbroadcast x	0.142857
the form [0 0	crossentropy categorical1hot	0.166667
output after pad_dims	unpad dims output input leftdims rightdims	0.333333
with a metaclass	add metaclass metaclass	0.125000
a new variable instance of type self	gof pure type make variable	0.333333
reducing the number of multiplications and/or divisions	local greedy distributor node	0.166667
to recognize the updates ordereddict the list	get updates	0.034483
connection pattern of	connection pattern	0.064516
of	of	1.000000
list of headers that	headers	0.038462
localoptimizer and applies them to the	local opt group	0.052632
chooses the orphans among them	orphans	0.090909
constant	subtensor get constant	1.000000
of which is sparse	sparse	0.038462
the sum	sum x	0.333333
class to raise in	core raise	0.100000
a graph of apply	sort apply	0.200000
localoptgroup instead of	group	0.047619
tell shape_i how to generate c code for	i c code typ code check_input version	1.000000
to recognize the updates ordereddict the	updates	0.029412
grad of this op could be very	l op	0.033333
a diff to	diff	0.071429
output error and exit code in a tuple	output subprocess popen command	0.100000
complete hashable signature of the	clinker cmodule key	0.166667
multinomial distribution defined	streams multinomial	0.076923
wrapper around c_cleanup that decrefs py_name	gof get c cleanup r name sub	0.250000
and exit code	subprocess popen command	0.083333
meta path	meta path	0.500000
important note this function	process node	0.142857
the broadcast pattern for	pattern a	0.066667
a set of 3d	tensor nnet conv3d	0.071429
dimensions of this variable optionally	tensor tensor py operators dimshuffle	0.019231
dict op -> total time icluding the time	compile profile stats compute total times	0.333333
denum removes it	denum	0.125000
compare true iff other is the same	type eq other	0.250000
is impossible to evaluate because of aliasing and	destroy	0.009709
previously sent array	mpisend	0.037037
execute	execute	1.000000
add a new variable to theano config	add config var name doc configparam	1.000000
respect to wrt, computes gradients of	subgraph	0.047619
a comparator to represent the dependence of	dependence cmp	0.111111
-1 and converts this to expm1 a	tensor local expm1 node	0.066667
base will be created if	gof	0.002381
return a safe shorter version of platform platform()	core short platform r p	0.142857
also their apply_node if those	gof	0.002381
a variable with the -x pattern	nnet is neg var	0.166667
for use within the op	get op params	0.100000
clients list of r	r	0.028571
nodes according to a list of	nodes inputs outputs cmps	0.166667
for	gpu dnn	0.066667
return a symbolic row	row name	0.050000
the output dimensions	get output	0.047619
numpy ndarray contains any np nan values	compile contains nan arr	0.500000
padaxis	padaxis	1.000000
this is the equivalent of	graph	0.016393
end variables	grad wrt end	0.050000
index array and a	a	0.008065
the compile lock to	module cache add to	0.142857
to the basic variable class	tensor variable	0.166667
the dimensions of this	tensor	0.006431
string x	patterns x	1.000000
is	gof is	0.250000
of arguments to pass	args	0.025641
specific to the apply to be	apply node	0.031250
convolution gradient with respect to the inputs	dnn conv grad	0.062500
gpucorrmm_gradweights (direction="backprop weights"), and	helper bottom weights top direction	0.055556
a mode	compile mode	0.166667
kind of order v real	v x	0.100000
output shape of convolution operation	get conv output shape image_shape kernel_shape border_mode subsample	0.500000
multinomial distributions defined by one-dimensional slices	tensor multinomial	0.037037
convert x into	x context_name	0.100000
important note this function uses set and dictionary	process node	0.142857
in the specified axes	addbroadcast x	0.142857
stack trace from	gof copy stack trace	0.055556
seq2	seq2	1.000000
a previously received	mpirecv	0.037037
the gradients along the axis that was used	inputs g_outputs	0.090909
kern	kern	1.000000
dot product of two	dot	0.035714
a previously sent array using	mpisend	0.037037
dimensions of x default reverse them	transpose x axes	0.200000
broadcasted	tensor py operators dimshuffle	0.019231
compilation flags from	libs flags libs_dir include_dir	0.052632
persist	persist	1.000000
list of r	r client_to_remove reason	0.200000
compute	tensor nnet conv3d	0.142857
svd on	tensor svd a	0.200000
list of headers that are needed by one	clinker headers	0.047619
the one	to one	0.125000
takes a list of localoptimizer and applies them	local opt group	0.052632
function uses set and dictionary data structures	scan_module push out non	0.125000
inputs with a set of 2d filters	tensor nnet conv2d input filters	0.125000
_prev_obs	_prev_obs	1.000000
type's :attr	array	0.041667
by mapping it to a	ctx	0.125000
exception some perform() or c_code() created	view map	0.142857
function must return a thunk that is	thunk	0.021277
see max for the maximum	maximum x	0.142857
x and there	sandbox linalg local	0.142857
fgraph outputs that will	fgraph	0.012195
"lifts" dimshuffle through elemwise operations and merges	tensor local dimshuffle lift node	0.250000
node by	node output_indices	0.142857
perform the permutation	perform node x y inverse	0.166667
it does	alloc	0.012500
grad of this op could be	l op	0.033333
argsort	argsort	1.000000
convert addsd	local addsd	0.250000
associate linker with	op wise clinker accept	1.000000
proxy for either true_div or int_div	proxy	0.095238
series of	many linkers wrappers	0.047619
original graph to a	outputs copy_inputs_and_orphans memo	0.029412
register r's shape in the	shape	0.010204
the stack trace from	stack trace	0.055556
makes the folowing changes	mul switch sink node	0.045455
r's shape in	tensor shape	0.058824
apply to be inserted in the struct	struct	0.047619
numpy-compatibility method if x is a	diag x	0.200000
type2 from type1	type1 type2	0.166667
makes the folowing changes in	tensor local mul switch sink node	0.045455
apply	sort apply	0.200000
self	self	1.000000
this function performs the matrix inverse on	matrix inverse a	0.200000
the value after the import of theano	param init default	0.040000
like make_thunk() but only makes python thunks	node storage_map compute_map no_recycling	1.000000
the corresponding element of a dense vector	x s	0.142857
math where x is an input vector and	node input_storage	0.038462
exception	bad view	0.027027
convolution for	nnet base abstract conv conv	0.125000
wasn't in the view_map	bad	0.013158
given an apply_node recursively search from this node	apply_node check reason	0.066667
sample n	size n	0.090909
broadcastable	d3viz broadcastable	0.500000
x is a matrix return its diagonal	x	0.008772
python int c long int	core python int bitwidth	0.250000
inputs	inputs	0.291139
numpy ndarray contains any np nan values	compile contains nan arr node	0.500000
the "reverse-mode" gradient for the	grad perform node inputs outputs	0.083333
'requirement' of the destroyhandler	add destroy handler	0.125000
the replacement if the ops in the list	validate replace all	0.050000
series	many linkers wrappers	0.047619
axis=l) -> sum(a axis={ }) / b	tensor local sum prod div dimshuffle node	0.333333
inputs	gof inputs	0.333333
the input to a leftdims + rightdims	input leftdims rightdims	0.333333
sorter	sorter	1.000000
that will be instantiated by	gof clinker type	0.066667
optimization that deals with allocempty this will duplicate	allocempty op idx	0.250000
used to determine the	tensor adv index broadcastable	0.050000
try to	gof compiler try	0.250000
help the navigator deal with the	navigator optimizer	0.037037
convert addsd to faster addsd_ccode	addsd ccode	0.250000
for input of given shape and flags	signal pool out shape imgshape ws ignore_border stride	0.200000
wait on	mpirecv wait	0.045455
shape s to previously un-shaped variable r	set shape r s override	0.500000
the replacement if the ops in the	replace validate replace	0.050000
logsoftmax x	tensor nnet local logsoftmax	0.076923
each row	row	0.068966
a uniform distribution between low and high	tensor uniform random_state size low high	0.333333
"theano config compiledir"	compiledir content	0.166667
attempt to replace a leaf of	replace leaf arg leaves new_leaves op	0.250000
one	y	0.078947
of this variable optionally	py operators	0.015625
for gpucorr3dmm	gpu corr3d mm	0.500000
that allows replacing subgraphs of a	scan_module clone output replace strict share_inputs	0.071429
implements numpy's isclose on tensors	tensor isclose a b rtol atol	0.500000
consider an expression constant when computing gradients	core zero grad x	0.333333
construct a variable	variable	0.022222
issue_warnings	issue_warnings	1.000000
ignore all errors	navigator optimizer warn ignore	1.000000
topooptimizer from the input nodes to output	in2out	0.043478
compilation flags from config blas	libs flags libs_dir include_dir	0.052632
along the given axis es	axis dtype op	0.083333
random_state	random_state	0.416667
calculate the sum	sum	0.038462
to represent the dependence of	make dependence	0.043478
ops in the list remove are still	fgraph replacements remove reason	0.055556
of integers indicating the version	version	0.093750
pattern for	pattern a	0.066667
img2d shape ==3 we todo	nnet conv op perform node inp out	0.166667
and return	name from	0.500000
the destroy_map	bad	0.013158
enum	enum	1.000000
item from the cache if available	gof call cache call fn args key	0.200000
to get the 0 based	type get	0.050000
type dtype	dtype	0.022727
fun	fun	0.833333
1/(1+exp x -> sigm -x	local inv 1 plus exp	0.333333
that gets a scan op a list	op not_required inputs	0.071429
warning about cuda should be displayed only	gpuarray use device force default_to_move_computation_to_gpu move_shared_to_gpu	0.333333
a string for	gpu	0.011765
value underlying variable	value	0.043478
uniform	uniform random_state size	0.125000
return c code to extract a	gof clinker type c extract name	0.250000
by a specified factor takes as input	signal pool 2d input	0.090909
draw samples from a poisson distribution	random streams base poisson size lam ndim dtype	1.000000
message on the first	msg	0.083333
the optimization to the provided l{functiongraph} it may	optimizer apply fgraph	0.200000
gemm computing an outer-product -> ger	tensor local gemm to ger node	1.000000
this function performs the svd	svd	0.034483
will be turned into macros for use	cop	0.028571
to the basic variable class	variable	0.022222
max for the maximum in	tensor maximum	0.142857
return none or a tensorvariable	as	0.024390
value after the import of	default filter	0.040000
six	six	0.125000
debug counter to	gof debug counter	0.500000
set and dictionary data structures	scan_module push out non seq scan	0.125000
[floor] division inverse of multiplication	tensor int div	0.250000
if a	a	0.008065
required return the c implementation	c code node name inputs outputs	0.250000
a symbolic vector variable	tensor vector name dtype	0.166667
start gradients up to the end	end start	0.166667
converts this to expm1 a	tensor local expm1 node	0.066667
broadcast pattern for advancedsubtensor	pattern a	0.066667
this is the equivalent of	to	0.017544
if necessary update dr_vals	dr_vals	0.111111
print profilestat objects in _atexit_print_list to _atexit_print_file	compile atexit print	1.000000
unconditional start-to-finish program execution in python	loop gc	1.000000
return true if and only if this	gof	0.002381
listeners to help the navigator deal with	navigator optimizer attach	0.038462
this variable	tensor py operators	0.015625
series of wrapper functions instead of	many linkers wrappers	0.047619
dimensions of this variable optionally inserting	tensor tensor	0.014286
fourier transform of a real-valued input	rfft inp norm	0.142857
max	max axis	1.000000
grad of this op could be	prod l op	0.033333
dnn conv algo_bwd	core safe no dnn algo bwd algo	0.166667
max for the maximum	maximum x	0.142857
use a simple algorithm	bad optimizations2 order reasons r_vals	0.333333
default failure_callback	optimizer warn exc	1.000000
shape tuple	shape	0.010204
arccosine	arccos	0.285714
obj to	obj	0.083333
a list of l{codeblock} instances returns a	code gen blocks	0.050000
unroll the	tensor nnet gen conv code unroll	0.250000
and the rest	rest inputs	0.125000
dimshuffle	dimshuffle	0.115942
this op could be very easy	op	0.009174
in	destroy	0.009709
bound by the inputs	init inputs	0.083333
diagonalsubtensor	get diagonal subtensor view	0.083333
and remove	local	0.014085
rel	rel	0.555556
reorder the dimensions of this variable optionally	tensor py	0.015873
the same computations	equal computations	0.333333
outputs and	and outputs	0.100000
of l{codeblock} instances returns	code gen blocks	0.050000
raise	check inputs node storage_map	0.166667
x and replace it with logsoftmax	logsoftmax node	0.125000
important note this function uses set	seq scan process node fgraph	0.142857
according to a	inputs outputs cmps	0.166667
cost scalar 0-dimensional	hessian cost	0.500000
streams	streams	0.384615
original graph to a new node a	inputs outputs copy_inputs_and_orphans memo	0.029412
to numpy random	mrg random	0.333333
return connection pattern of subfgraph defined	graph connection pattern	0.076923
a particular stream	tensor random streams setitem item val	0.142857
conv output gradient w r t its inputs	conv3d grad wrt inputs output_grad filters input_shape filter_shape	0.333333
replacement if the ops in the	validate replace all	0.050000
libs	libs	1.000000
transfer to a tensortype if not	tensor tensor py operators transfer	0.125000
to the type's :attr	gpuarray gpu array type	0.062500
dimensions of this variable optionally inserting	py operators dimshuffle	0.019231
add two	add	0.034483
return specified diagonals	extract diag	1.000000
sample from a uniform	uniform	0.043478
see theano tensor max	py operators max axis keepdims	1.000000
helper function for grad function	core populate grad dict	1.000000
alloc would be useless this function returns val	alloc call val	1.000000
is an alloc of a	alloc node	0.074074
op could be very	op	0.009174
symbolic graph for convolving a mini-batch of a	input_shape filter_shape	0.027778
implements the "reverse-mode" gradient for the eigensystem of	tensor eigh grad perform node inputs	0.333333
output for	output	0.017241
the replacement	replace validate replace	0.050000
see theano tensor sort	tensor py operators sort axis	1.000000
in the original graph to	outputs copy_inputs_and_orphans memo	0.029412
cause of the	bad optimization str diagnostic	0.043478
makes the folowing changes in the graph t	mul switch sink node	0.045455
graph	graph	0.131148
replace it with a triangular solve	tag solve triangular	0.142857
lib directories	clinker lib dirs	0.055556
diagonal set	diagonal	0.111111
of lib directories	clinker header dirs	0.055556
version used is the	gof	0.002381
replace replace in string x	d3viz replace patterns x replace	1.000000
transform it into a canonical form that respects	tensor get canonical form	0.045455
this variable optionally inserting broadcasted	tensor tensor py operators	0.015625
tensor filled with ones closer to	tensor ones shape dtype	0.250000
output type dtype and broadcast there is no	tensor local canonicalize alloc node	0.333333
gradient of conv3d with respect	conv grad3d	0.333333
tensors	b rtol atol	1.000000
same shape and dtype as	dtype	0.022727
to determine the broadcast pattern for advancedsubtensor output	adv index broadcastable pattern a	0.066667
same shape and dtype	dtype	0.022727
es of a	dtype keepdims	0.250000
the original graph to a new	inputs outputs copy_inputs_and_orphans memo	0.029412
post	misc post	0.200000
a b u returns	gof	0.002381
main diagonal set to a specified scalar	fill diagonal offset a val offset	0.100000
c	op c	0.250000
class to raise	raise init	0.200000
that unroll the	conv code unroll	0.250000
combine gpukernelbase and cop	cgpu kernel base	0.500000
-> sum(a axis={ }) / b	local sum prod div dimshuffle	0.333333
for product between several dots	matrix dot	0.333333
from a uniform	tensor uniform random_state size	0.125000
print the mflops	op flops inputs	0.125000
like	like	0.666667
up to the end variables of a	end	0.040000
of nodes	nodes	0.100000
type	params type has type	1.000000
for calculating the dot product	dot	0.035714
takes as	signal max pool 2d	1.000000
function for diagonalsubtensor and	nnet get diagonal subtensor view x i0	0.083333
that unroll the batch size	tensor nnet gen conv code unroll batch	0.166667
tensor from polar	from polar	0.250000
unique names to an iterable of variables modifies	gof give variables names variables	0.333333
that broadcast them to	generate broadcasting	0.066667
given shape and flags	grad out shape imgshape ws ignore_border stride	0.200000
bug in the default blas in	sdot bug	0.500000
find broken optimizations	find bad optimizations2	0.333333
this optimization makes the folowing changes	tensor local mul switch sink	0.045455
equivalent of localoptgroup	graph to gpulocal opt group	0.055556
to new_r	new_r	0.142857
leaf of a	leaf	0.066667
required return c code to declare variables	type c declare name sub check_input	0.333333
unique names	names	0.047619
the dimensions of x default reverse them	tensor transpose x	0.200000
use dnn	safe no dnn	0.125000
two lists are equal if they contain the	eq other	0.166667
important note this function uses set	scan process node fgraph node	0.142857
convenience function to concatenate tensortypes	join	0.090909
up to the end	end	0.040000
_maker which handles much of the debugging work	debug mode function maker i o m	0.066667
has only one client and that client is	only	0.050000
to r	r	0.028571
loading of moved objects in	moves urllib	0.076923
value on the output	output	0.017241
context_name	context_name	1.000000
defining the gradient the	grad	0.010417
a new instance of this mode	compile monitor mode clone link_kwargs optimizer	0.333333
the main interface to manipulate	r new_r reason verbose	0.071429
the version	object c code cache version	0.125000
calling the same op twice gives inconsistent outputs	bad thunk	0.200000
a symbolic 3-d variable	tensor3 name dtype	0.166667
default that removes all asserts from the	tensor local remove all assert	0.055556
if this	gof	0.004762
wrapper around c_extract that initializes py_name from storage	gof get c extract r name sub	0.250000
overwrite the full	useless inc subtensor	0.125000
into macros for use within the	cop	0.028571
variable optionally inserting	py operators	0.015625
length	length	1.000000
an [advanced]incsubtensor[1], whose increment	tensor local useless inc subtensor	0.333333
a crossentropysoftmax1hotwithbiasdx op whose incoming	useless crossentropy softmax 1hot with bias dx	0.111111
test a	n_tests	0.142857
converts number	from number number	0.142857
remove are still in the	remove fgraph replacements remove reason	0.055556
op that will call the supplied function	op	0.009174
the convolution gradient with respect	dnn conv grad	0.125000
gradient updates for matrix solve operation c	tensor solve grad	0.250000
the inner graph to	validate inner graph	0.035714
this convert allocempty to alloc of	local alloc empty to zeros	0.333333
a new instance	clone link_kwargs optimizer	0.111111
return c code to declare variables that	gof clinker type c declare name sub	0.333333
the same	shape feature same	0.333333
with the ignore_trees-related functionality	attach updater fgraph importer pruner chin	0.250000
constant scalar 0-d value underlying	core get scalar constant value	0.333333
return label of	label node	0.250000
3d convolution for	abstract conv conv	0.125000
simplify	nnet simplify	0.500000
where each row correspond to the one	tensor to one	0.125000
to compute the image shape of convolution gradweights	tensor nnet get conv gradweights shape 1axis	0.500000
together into a vector	vector	0.066667
list of localoptimizer and applies	local opt group	0.052632
replace it with logsoftmax x 's	nnet local logsoftmax	0.076923
turned into macros for use within the op	cop get op	0.200000
failure_callback	warn inplace exc	0.500000
compiles	clinker compile	1.000000
y with length one the axes	y	0.026316
on wraplinker that runs a	gof wrap	0.083333
shape_i how to generate	check_input	0.111111
for diagonalsubtensor	nnet get diagonal subtensor view x	0.083333
getter method for self _rop_op	compile op from graph get rop op	1.000000
important note this function uses set and dictionary	push out seq scan process node	0.142857
given axis	tensor argmin x axis	0.500000
specific to the apply to	apply	0.016667
implement the grad of max pooling	max pool grad	0.333333
list of outputs and the stopping condition returned	and outputs ls	0.166667
x -> x	local	0.014085
variable optionally	operators dimshuffle	0.019231
given shape and flags	out shape imgshape ws ignore_border stride	0.200000
revert the replacement if the	validate replace all	0.050000
config_string	config_string	1.000000
this function tries to recognize the updates	get updates	0.034483
bartlett	bartlett	0.294118
the subgraph between i and o parameters	i o	0.083333
numpy round	round	0.076923
the 1d kernel that can	kernel 1d	0.050000
fetch a compiled module from the loaded	get module	0.200000
reproducible case for problems during theano compilation	compile function dump filename	0.166667
for same kinds of tensortype	tensor type	0.034483
this simply checks if	gof unify walk v	1.000000
required return	node name	0.033333
see theano tensor sort	tensor py operators sort axis kind order	1.000000
revert the replacement if	gof replace validate replace	0.050000
an input that	destroy	0.009709
the svd of a	svd	0.034483
of this variable optionally inserting broadcasted dimensions	dimshuffle	0.014493
of a cache directory and return full	gof module name from dir	0.071429
that unroll the batch size	code unroll batch	0.166667
the source	gof clinker compile cmodule location	0.038462
individual ops in the "theano config compiledir"	compiledir content	0.166667
to help the navigator deal with the	gof navigator optimizer attach updater fgraph	0.038462
value after	core config param init default filter	0.040000
the 2d kernel that	kernel 2d	0.050000
x ->	tensor nnet local	0.200000
matrix bias	bias	0.166667
return connection pattern of subfgraph defined	from graph connection pattern node	0.076923
type	typed_list typed list type	0.333333
tangent of	tensor tan	1.000000
tries to	top_shape border_mode subsample	0.166667
return	name inputs outputs	0.500000
if c_code does not support the types involved	inc subtensor do type checking	0.142857
on a)	inplace	0.025641
list of shape	default infer shape	0.066667
pooling	pool	0.200000
optimizations2	optimizations2	1.000000
input a 4-d	pool 2d same size input patch_size	0.166667
of ints	weights minlength assert_nonneg	0.125000
degree	deg2rad	0.111111
is a	path importer is	0.250000
fill inputted tensor with the assigned	tensor py operators fill	1.000000
should remove any dynamically added functionality	validator on detach fgraph	1.000000
output gradient	conv3d grad	0.111111
of this variable	py operators	0.015625
with the -x pattern	neg	0.083333
op and reduce pattern has functioning c code	gpuarray gpu careduce cuda supports c code	0.250000
function uses set and dictionary data structures	push out non seq	0.125000
total	compute total	1.000000
or -exp	nnet is	1.000000
"reverse-mode" gradient for the eigensystem of	tensor eigh grad perform	0.333333
series of wrapper	many linkers wrappers	0.047619
conda offers to make itself the default python	to os environ pathlist	0.038462
the specified pieces of vectors and matrices	sparse block gemv make node o w	0.066667
op a	op	0.009174
it into a canonical form	tensor get canonical form	0.045455
perform some elementary validations on the inner	inner	0.041667
the diagonal of an empty matrix it does	alloc diag	0.027027
output of scan return	scan	0.017241
inputs of inplace	inplace check inputs	1.000000
data to	data	0.090909
:func neibs2images	nnet neibs2images	0.333333
important note this function uses set and	seq scan process node fgraph	0.142857
div	div	0.416667
a uniform	uniform random_state	0.125000
node that uses	gof function graph	0.031250
inverse fast fourier transform with real-valued output on	cuirfft inp norm is_odd	0.333333
sample from one or more multinomial	tensor multinomial random_state size n	0.333333
numpy-compatibility method if x is a matrix return	diag x	0.200000
c_init that initializes py_name to py_none	gof get c init r	0.250000
for the outputs of node	node	0.007407
input a 4-d	2d same size input patch_size	0.166667
return complex-valued tensor from polar coordinate specification	complex from polar abs angle	0.250000
detect	gof gcc	0.027778
false we can't change the value after	config param init default filter	0.040000
see theano tensor sum	tensor py operators sum axis	1.000000
:attr	array type	0.055556
reorder the dimensions of	tensor py operators	0.015625
other implementation of mod	scalar mod c code node name inputs outputs	0.125000
zeros_like but forces the object to have	core float zeros	0.166667
header for openblas threads interface	openblas threads text	0.250000
we can't change the value after	default	0.030303
square root	sqrt	0.285714
return a reshaped view/copy	py operators reshape shape ndim	0.333333
a rows	rows	0.125000
operation on f wrt to wrt	core lop f wrt	0.200000
specified factor takes as	signal pool	0.142857
all unknown variables and apply_nodes to this graph	graph	0.016393
listeners to help the navigator deal with	gof navigator optimizer	0.038462
convert python litterals	constant	0.016667
tensor operators to the basic constant class	constant	0.016667
that x and y have the same shape	shape feature same shape x y	0.500000
together	make	0.017857
shape	shape	0.285714
the type's	type	0.011905
scan return true iff the	out scan	0.035714
of the specified pieces of vectors and	sparse block outer make	0.066667
returns the connection pattern of	gof io connection pattern	0.055556
in an easy to manipulate	args	0.025641
debug	debug	1.000000
to maintain order	tensor searchsorted x v side sorter	0.142857
generate permutations from integers	permutation	0.090909
create pydot graph object from	dot formatter	1.000000
is not attempting to use dnn	safe no dnn	0.125000
is only used to determine the broadcast	tensor adv index broadcastable	0.050000
output of scan return true	push out scan	0.050000
is impossible to	destroy	0.009709
array and a set of arrays	a choices out mode	0.111111
inner-most loop executes code	tensor make reordered loop	0.111111
dimensions of this variable optionally	tensor py operators	0.015625
the platform-dependent gcc argument for shared libraries	gof get gcc shared library arg	0.333333
maps old nodes to new ones	equiv check_integrity attach_feature	0.200000
[start stop step] transform it into a canonical	canonical	0.076923
execute callbacks calls getattr feature name (*args) for	function graph execute callbacks name	0.500000
exception some perform() or c_code() modified	map	0.047619
hack in profiling to print the mflops	tensor nnet conv op flops	0.125000
the output error and exit code in	misc output subprocess popen command	0.100000
of the specified pieces of vectors and matrices	sparse block gemv make node	0.066667
compute sum of	signature get sum	0.142857
helper function for diagonalsubtensor	diagonal subtensor view x i0	0.083333
replacer	replacer	1.000000
the updates ordereddict the list of outputs and	get updates and outputs	0.333333
dictionary of arguments to pass to helper_c_code	subtensor get helper c code args	0.250000
into macros for	cop get	0.033333
inp	inp	1.000000
on wraplinker that runs a	gof	0.002381
x*x -> sqr x this is	tensor local mul to sqr node	0.166667
this op __init__ fct don't have the same	cast make new inplace output_types_preference name	0.142857
diagonal of an empty matrix	alloc diag	0.027027
into macros for	cop	0.028571
the output error and exit code in a	output subprocess popen command	0.100000
true if var is always equal to val	gpuarray is equal var val	1.000000
correct	correct	1.000000
the dot product of two variables	tensor dot a b	0.333333
some perform() or c_code() created a memory	view map	0.142857
apply_node recursively search from	import apply_node check reason	0.066667
and return full	module name from	0.076923
inserting	tensor tensor py operators dimshuffle	0.019231
code to the task that	gof cthunk find task	0.142857
search	stack search	0.333333
this graph	graph import	0.125000
gradient is an alloc of a scalar variable	alloc node	0.037037
0-d value underlying	value	0.043478
raise baddestroymap if necessary update dr_vals	check inputs node storage_map r_vals dr_vals	0.250000
moved objects in six moves urllib_request	module six moves urllib request	0.333333
set of all variables which may share the	compile infer reuse pattern	0.100000
listeners to help the navigator deal with the	gof navigator optimizer attach updater fgraph	0.038462
function computes the output shape	get out shape ishape kshape	0.500000
into	make	0.017857
3-d	tensor3 name	0.500000
platform-dependent extension for compiled modules	gof get lib extension	0.333333
sample from one or	size	0.076923
of useless reshape	local useless reshape node	0.200000
runs a series of	wrap linker many linkers wrappers	0.047619
a multiplication tree	mul tree	0.250000
required return c code to extract	c extract name	0.250000
the dimensions of this variable optionally	operators dimshuffle	0.019231
the -x pattern	is neg var	0.166667
since the epoch of the last	last	0.076923
more multinomial distributions defined	tensor multinomial	0.037037
type	typed list type	0.250000
a symbolic row variable (ndim=2	tensor row name	0.050000
to help the navigator deal	gof navigator optimizer attach	0.038462
broadcasted	tensor tensor	0.014286
return connection	connection	0.100000
remove broadcastable dimensions from the shape	tensor tensor py operators squeeze	0.200000
the dimensions	operators dimshuffle	0.019231
listeners to help the navigator	gof navigator optimizer	0.038462
pattern has functioning	gpu careduce cuda supports	0.166667
version used is the llvm one or not	gof gcc llvm	0.200000
one or more multinomial	multinomial random_state	0.040000
of variables	variables	0.086957
takes as	tensor signal max pool 2d	1.000000
function-constructor for graphs	compile pfunc params outputs mode updates	0.200000
this object but we don't clone	gof constant clone	0.166667
that removes all	local remove all	0.166667
wait on a previously sent array using mpi	mpisend wait	0.045455
f	core lop f	0.166667
symbolically cast x to a	tensor cast x	0.200000
inserted in the module initialization code	init code	0.142857
a ^	xor	0.125000
compilation lock if someone else has it	gof force unlock	0.500000
determine the broadcast pattern	tensor adv index broadcastable pattern a idx	0.066667
one hot	tensor to one hot	0.142857
spatio-temporal filters	filters	0.032258
to the fgraph this	fgraph	0.012195
source code for this	clinker compile cmodule location	0.038462
the indices	indices	0.076923
method that attaches	gof bookkeeper on attach	0.142857
grad and replace it with logsoftmax x 's	nnet local logsoftmax	0.076923
full "updates" dictionary mapping from functiongraph input variables	updated vars	1.000000
transform of a real-valued input on the gpu	gpuarray curfft inp norm	0.066667
modified bessel function of	tensor i1 inplace x	1.000000
the replacement if the ops in the	validate replace all	0.050000
array and a set of arrays to	a choices out mode	0.111111
in an easy	args	0.025641
operation on f	core lop f	0.166667
input a 4-d tensor it	input patch_size	0.166667
output gradient	grad	0.031250
the same shape and dtype as the	dtype	0.022727
an fgraph and a	fgraph outputs_to_disown	0.047619
subclass to add the tensor operators	tensor	0.006431
to print the mflops	flops	0.076923
input a 4-d tensor it sets all non	size input patch_size	0.166667
the replacement	validate replace all	0.050000
the gradient in	core grad	0.166667
the value after the import of	core config param init default filter	0.040000
step] transform it into a canonical form that	get canonical form	0.045455
of 2d filters	input filters	0.117647
topooptimizer from the	in2out	0.043478
base 2 logarithm of	tensor log2 a	0.500000
this variable	tensor tensor	0.014286
a n-d tensor where n >= 2	ws ignore_border stride	0.045455
for bilinear upsampling	bilinear	0.038462
required return c code to declare variables	type c declare name sub	0.500000
helper function drawing	helper random_state n pvals	1.000000
diagonalsubtensor	nnet get diagonal subtensor view x i0 i1	0.083333
pushing out the variables inside the	push out	0.037037
this optimization makes the folowing changes in	mul switch sink node	0.045455
returns the connection	gof io connection	0.333333
change all sigmoid to ultra_fast_sigmoid	nnet local ultra fast sigmoid	0.200000
test	pt n_tests	0.500000
instance of type self	pure type make	1.000000
eigh	eigh	0.625000
then replace it with a triangular	triangular node	0.125000
an	bad destroy	0.034483
with the -x pattern	is neg	0.166667
[floor]	tensor int	1.000000
if l has any duplicates (according to	has duplicates l	0.111111
variable	tensor tensor py	0.015873
was dumped to a	f persistent_load	0.052632
implements numpy's isclose on tensors	isclose a b rtol atol	0.500000
diff to make code correctly indented	misc hooks get correct indentation diff code	0.333333
insert deepcopy in the	compile insert deepcopy	0.333333
given an fgraph and a list	fgraph	0.012195
the inputs required to compute the	inputs variable_list blockers	0.058824
dimensions	tensor tensor py operators	0.031250
csm_grad none -> csm_grad_c	sparse local csm grad c node	1.000000
also their apply_node if	gof	0.002381
existing start gradients	start	0.040000
equal	equal	1.000000
replacement	gof replace validate replace all	0.050000
the output	output	0.086207
of	tensor tensor	0.042857
of moved objects	moves urllib	0.115385
need not be checked for nan	compile is numeric value arr var	0.166667
it to a max	tensor local max	0.250000
if l has any duplicates (according	scan_module has duplicates l	0.111111
as replace_all_validate revert the replacement if the ops	replace all validate remove fgraph	0.111111
or a tensorvariable	as	0.024390
-> alloc(unary x	local	0.014085
the topooptimizer from the input nodes	gof in2out	0.055556
the "reverse-mode" gradient [1]_ for the cholesky factorization	cholesky grad perform node inputs	0.500000
alloc of	tensor local alloc	0.111111
determine the broadcast pattern	tensor adv index broadcastable pattern a	0.066667
this optimization makes the folowing changes	tensor local mul switch sink node	0.045455
an input that wasn't in	destroy	0.009709
and	core format as	1.000000
of dimensions from the shape or	shape	0.010204
are spaced by 2**72 samples	get substream rstates n_streams dtype inc_rstate	0.142857
given an fgraph and a list of variables	fgraph outputs_to_disown	0.047619
4-d	patch_size	0.050000
output dimensions of convolving an image	get output	0.047619
:param execute if true execute a theano function	execute execute verbose m	0.250000
some perform() or c_code() created	view map	0.142857
a c contiguous version of	contiguous	0.058824
used to determine the broadcast pattern for	adv index broadcastable pattern	0.066667
unfortunately conda offers to	to os environ	0.038462
given axis es	axis ddof keepdims	0.083333
unify values of	unify	0.125000
the output type dtype and broadcast there	tensor local canonicalize alloc	0.333333
gradient	grad fun	0.500000
the context object mapped to the	array type context	0.090909
connection pattern of a	io connection pattern	0.055556
product of the specified pieces of vectors	sparse block outer make node o x y	0.066667
"reverse-mode" gradient for the eigensystem of	tensor eigh grad perform node inputs	0.333333
for	gpuarray base gpu	1.000000
dict d s t d[node] is a list	function graph orderings	0.200000
existence of the __unify_walk__ method	gof unify walk a b u	0.037037
returns function to run	gof	0.002381
fast	fast	0.833333
a special compound l{op}	crossentropy softmax argmax1hot	0.083333
compute 1d kernel for bilinear upsampling this	tensor nnet bilinear	0.111111
this compiles the source code	clinker compile cmodule location	0.038462
replace_all_validate revert	validate remove fgraph	0.166667
the stack trace from one or	stack trace	0.055556
with constant inputs	constant idx inputs	0.250000
arguments to pass to helper_c_code	get helper c code args	0.250000
variable from x	x	0.008772
orv list1 == nv list2 ==	gof unify walk	1.000000
number of multiplications and/or divisions	tensor local greedy distributor node	0.166667
lower triangle of	tril m k	0.250000
given a inner nit_sot	inner	0.041667
an	destroy	0.009709
used to determine the broadcast pattern for advancedsubtensor	tensor adv index broadcastable pattern a idx	0.066667
a clone in a new graph	clone	0.020833
return connection pattern of subfgraph defined by inputs	connection pattern	0.032258
copies the subgraph contained between i and o	i o copy_inputs	0.333333
map	scan_module map	0.500000
raise	compile	0.076923
the given axis es of a tensor	axis dtype	0.083333
can't change the value after the	init default	0.040000
inputs	init inputs	0.083333
[advanced]incsubtensor[1], whose increment is an alloc	local useless inc subtensor alloc node	0.166667
[true] division inverse of multiplication	tensor true div	0.250000
add an item	compat add	0.250000
a symbolic row	tensor row	0.050000
dimensions of x	x	0.008772
we can't change the value after the	default	0.030303
diagonalsubtensor and	get diagonal subtensor	0.083333
value for	value	0.043478
returns a module if the	module	0.033333
revert the replacement if the ops in the	gof replace validate replace all	0.050000
connection pattern of a	gof io connection pattern	0.055556
between low and high	low high ndim	1.000000
idx_list	idx_list	1.000000
fetch a compiled module	module	0.033333
of all variables which may share	compile infer reuse pattern	0.100000
inner graph to ensure that it	scan_module scan validate inner graph	0.035714
connection pattern of subfgraph	graph connection pattern	0.076923
on the inner	scan_module scan validate inner	0.142857
folowing changes in	tensor local mul switch sink node	0.045455
c-implementation of the	csr c code node name inputs outputs	0.333333
filename	filename	0.833333
in profilemode to print the mflops	gpu corr mm flops inp outp	0.125000
add	compat add	0.250000
this generates the c code for corr3dmm (direction="forward"),	tensor nnet base corr3d mm c code	0.090909
element in y	y	0.026316
float64	float64	1.000000
return the idx_list with constant	get constant idx	0.250000
compute the image shape of convolution gradinputs	nnet get conv gradinputs shape kernel_shape	0.500000
for the minimum	minimum	0.083333
compute conv output gradient	nnet conv2d grad	0.333333
and	node name inputs	1.000000
false we can't change the value after the	core config param init default	0.040000
sharedvariable constructor for sparsetype	sparse sparse constructor value name strict allow_downcast	1.000000
replace_all_validate revert	all validate	0.166667
none for the	i_shapes	0.050000
op for cudnn batch normalization	gpu dnn batch norm inference	0.333333
the updates ordereddict the list of	updates	0.029412
output	dims output input	0.333333
type	clinker type c code	1.000000
use within the op	op	0.009174
the hack in profiling to print the mflops	conv op flops inputs outputs	0.125000
global optimizer of type topooptimizer	topo db	0.166667
mode	compile mode	0.333333
to use	gpuarray gpu inc subtensor	0.333333
object but we don't clone	constant clone	0.250000
maxandargmax	max and argmax	0.125000
even	even	1.000000
signature object for comparing tensorconstant instances	signature	0.066667
return a rows x cols matrix implementing	sandbox dct matrix rows cols unitary	0.333333
out	push out	0.037037
the original graph to	outputs copy_inputs_and_orphans memo	0.029412
the context associated with	context	0.035714
addition of a	add	0.034483
for bilinear	bilinear	0.038462
baddestroymap if	compile check inputs	0.166667
a tensorvariable	as	0.024390
subtensor(setsubtensor x y idx idx) -> y	tensor local subtensor inc subtensor node	0.500000
raised when grad is	error	0.025000
ccode	ccode	1.000000
this is the	gpulocal opt	0.055556
fortran	fortran	1.000000
gradient wrt inputs for corr3dmm	corr3d mm grad inputs	1.000000
elements of each row inner-most dim of	row elements	0.333333
of	destroy	0.009709
unary(alloc x shp -> alloc(unary x shp)	local alloc unary	0.250000
computes the mean value along the	tensor mean	0.111111
hint that the variable v	sandbox linalg psd v	0.250000
x and there is already	sandbox linalg local	0.142857
lock to be held	to cache module key module_hash	0.166667
attempts to replace a scan	scan_module scan inplace optimizer attempt scan	1.000000
version of var transferred	transfer var	0.100000
dot(x, y t) = z	sampling	1.000000
work for elemwise and gpuelemwise	tensor local elemwise fusion	0.166667
runs a series	many linkers wrappers	0.047619
the idx_list with constant inputs replaced by	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
defined by given inputs and outputs	inputs outputs	0.066667
variable optionally inserting broadcasted	tensor	0.006431
in a new graph	with new inputs	0.166667
not the other implementation of mod	scalar mod c code node name inputs	0.125000
by given inputs and outputs	inputs outputs	0.066667
arccos	arccos	0.714286
with respect to wrt, computes	core subgraph	0.062500
iff x and	numpy x	1.000000
the other implementation of mod	mod c	0.125000
csm	csm	1.000000
zip	misc	0.142857
the convolution gradient with	conv grad i	0.500000
leaves	leaves	1.000000
-> single prod()	local op of op node	0.500000
to the diagonal of an	diag	0.023810
int_div depending on types of x y	x y	0.048780
required return	name sub	0.150000
the convolution gradient with	conv grad w	0.500000
apply instance	apply clone	0.333333
convop that unroll the batch size loop	unroll batch kern d unroll_bsize unroll_ksize	0.125000
destroy	destroy	0.048544
loop	loop	0.194444
is	graph to	0.055556
raise baddestroymap if	inputs	0.012658
and "init_code" together	init code struct node name	0.500000
that was used	grad inputs g_outputs	0.076923
determine the broadcast pattern for advancedsubtensor output	adv index broadcastable pattern	0.066667
unroll the	conv code unroll	0.250000
a special compound l{op} for the output	argmax1hot	0.058824
checks if theano graphs represent the same computations	scan_module equal computations xs ys in_xs in_ys	0.333333
input a 4-d tensor it sets	max pool 2d same size input patch_size	0.166667
lib directories that are needed by one or	clinker lib dirs	0.055556
of tensortype	type	0.011905
help the navigator deal	navigator optimizer	0.037037
collect	collect	1.000000
compare true	eq	0.111111
changes node inputs[i] to new_r	change input node i new_r	0.500000
important note this function	process node fgraph node	0.142857
to use with	gpuarray gpu inc	0.333333
to helper_c_code	subtensor get helper c code	0.142857
we do	node	0.007407
remove split with only 1 split	useless split	1.000000
elemwise arctanh of x	sparse arctanh x	1.000000
removes all from the	function graph remove client	0.200000
the output type dtype and broadcast there is	local canonicalize alloc node	0.333333
the leaves of a search through consecutive view_map()s	view roots r	0.200000
is impossible to evaluate because of	destroy	0.009709
from polar	tensor complex from polar	0.250000
list of op classes that this opt applies	gof local optimizer tracks	0.071429
only if this enum has	gof enum type has	0.111111
apply_nodes to this graph	gof function graph	0.031250
op twice gives inconsistent outputs	bad thunk	0.200000
nodes such that	gof function graph	0.031250
wraplinker that runs a	gof wrap	0.083333
to compute the image shape of convolution gradinputs	nnet get conv gradinputs shape 1axis kernel_shape	0.500000
a numpy ndarray	tensor	0.003215
alloc	tensor alloc	0.333333
load an array from an npy file	load path dtype broadcastable mmap_mode	1.000000
the grad of this op could be very	prod l op	0.033333
dimshuffle which only adds dimension to	local dimshuffle	0.052632
by default that removes all asserts from	local remove all assert	0.055556
cond then ift else iff	cond ift iff	0.500000
pattern of subfgraph defined by inputs and outputs	pattern	0.028571
hyperbolic arc tangent of a	tensor arctanh a	1.000000
x and replace it with logsoftmax x	nnet local logsoftmax node	0.142857
minimal type used for passing contexts	gpu context type	0.333333
output dimensions of convolving an image of	output	0.017241
between i and o nodes	i o	0.041667
returns a string	string	0.111111
use within the op code	get op params	0.100000
pattern has functioning c	gpuarray gpu careduce cuda supports c	0.200000
op could be	prod l op	0.033333
raise	storage_map r_vals	0.166667
to add some requirements to the	optimizer add requirements	0.166667
computes the output dimensions	get output	0.047619
the hack in profiling to print the mflops	tensor nnet conv op flops	0.125000
the folowing changes in the graph	mul switch sink	0.045455
compute	nnet conv2d	0.333333
and output nodes via dfs travesal parameters	i o	0.041667
to match out_shape	out_shape	0.125000
inc	inc	0.833333
this op could	tensor prod l op	0.033333
wasn't in	bad	0.026316
the "reverse-mode" gradient for the	grad perform node inputs	0.083333
row variable (ndim=2 broadcastable=[true	row name dtype	0.050000
the op	compile profile stats op	0.166667
profilemode to print the mflops	base gpu corr3d mm flops inp outp	0.125000
the variable v is	v	0.011111
value after the import	default	0.030303
if fgraph is the first	fgraph no_recycling profile	0.200000
implement the grad of average pooling	average pool grad	0.200000
value after the import of	param init default filter	0.040000
to find broken	compile find bad optimizations2	0.333333
the output dimensions of convolving an	output	0.017241
or none for the outputs of	i_shapes	0.050000
attempt to	arg leaves new_leaves op	0.500000
return complex-valued tensor from	tensor complex from	0.250000
uses	gof	0.002381
3d	3d	0.500000
to compile a dummy file with these flags	flags cls flag_list preambule body	0.250000
attempting to use dnn conv workmem	no dnn workmem workmem	0.166667
implements the "reverse-mode" gradient for the	grad perform node	0.083333
prod(prod()) ->	local	0.014085
try to turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	tensor nnet local softmax with bias node	0.200000
inputs with a set of 3d	nnet conv3d	0.071429
the convolution gradient with respect to the	dnn conv grad	0.125000
instances of suitable dummy values	provide inputs	0.200000
zeros	of zeros	1.000000
computes the output shape	out shape ishape kshape	0.500000
important note this	seq scan process node	0.142857
a thunk	make thunk	0.125000
py_none	get c init	0.500000
validations on the inner graph	inner graph	0.035714
the dependence of nodes in	dependence	0.035714
builds the 1d kernel that can be	kernel 1d	0.050000
compile lock to be held	add to cache module key module_hash	0.166667
if the caller is replace_all_validate just raise	validator validate fgraph	0.125000
inner graph	validate inner graph	0.035714
bug	bug	0.857143
to wait on a previously received array using	mpirecv wait	0.045455
apply_node recursively search from this node to know	apply_node check	0.066667
subgraph contained between i and o	clone i o copy_inputs	0.333333
src_code	src_code	1.000000
orig_v	orig_v	1.000000
this is the equivalent of localoptgroup for graphtogpu	gpulocal opt group	0.055556
of specific ops of a compiled graph have	trace f_or_fgraph ops_to_check bug_print	0.035714
print a warning message on the first	deprecated filename msg	0.041667
input a	max pool 2d same size input	0.500000
attempts to replace a scan	inplace optimizer attempt scan	1.000000
find broken	find bad optimizations2 order	0.333333
given axis	min x axis	1.000000
output gradient	conv2d grad	0.111111
a decorator or context manager	flags	0.062500
lock to	gof module cache add to	0.142857
a badviewmap exception when it detects	check viewmap node storage_map	0.111111
output of neural-net multiclass classifiers	softmax with bias	0.142857
legal value for a variable of	is valid value a	0.076923
cpu correlation implementation using matrix multiplication	corr3d mm	0.111111
variable optionally inserting	dimshuffle	0.014493
from a uniform into sample without replacement from	choice from uniform	0.166667
should be removed	outs	0.050000
generates the c code for corrmm	base corr mm c code	0.090909
meta	meta	0.857143
module is a	six meta path importer is	0.250000
subtensor(setsubtensor x y idx idx) ->	local subtensor inc subtensor node	0.500000
as replace_all_validate revert the	validate remove	0.166667
the	scan_module scan execute node	1.000000
gemm	gemm	0.600000
whether it	node	0.007407
reshaped view/copy of this	tensor tensor py operators reshape shape ndim	0.111111
called whenever node inputs[i] is	on change input function_graph node i	0.333333
transfer to a tensortype if not	tensor py operators transfer	0.125000
if	node	0.029630
row variable (ndim=2	row name	0.050000
to extract a	extract	0.111111
config string (comma-separated key=value components) into a dict	core parse config string config_string issue_warnings	0.166667
compute a generalized dot product over provided	tensor tensordot a b	1.000000
the dimensions of this variable optionally	tensor py operators	0.015625
of tensortype	tensor tensor	0.014286
:param execute if true execute a theano	execute execute verbose m n	0.250000
series of wrapper functions	many linkers wrappers	0.047619
how to generate c	register specify shape c	0.250000
has only one client	sitsot only	0.066667
output_storage	output_storage	0.833333
provided	add	0.034483
to recognize the updates ordereddict the list	scan_module get updates	0.034483
detect	detect	0.545455
y	y	0.394737
a shared variable to 0	shared variable zero borrow	0.200000
2d inputs with a set of 2d filters	input filters	0.117647
return connection pattern of subfgraph defined by inputs	compile op from graph connection pattern	0.076923
a module	gof module	0.058824
to make	to	0.017544
symbolically cast x to a tensor of	tensor cast x	0.200000
infer the number of	infer	0.083333
convert addsd to	addsd	0.111111
return selected slices only	py operators compress a axis	1.000000
of all variables which may share the	infer reuse pattern	0.100000
this function uses set and dictionary data structures	non seq	0.111111
gc	gc	1.000000
matrix where each row correspond to the one	to one	0.125000
the given axis es of a	axis dtype keepdims	0.083333
optionally inserting	tensor py operators	0.015625
function to get the 0 based level of	type get	0.050000
default that removes all	tensor local remove all	0.166667
used to determine the broadcast pattern for advancedsubtensor	tensor adv index broadcastable pattern a	0.066667
matrix solve operation c = a \ b	tensor solve	0.038462
listeners to help the navigator deal with	navigator optimizer attach updater	0.038462
the sum along	sum	0.038462
the user is not attempting to use dnn	dnn	0.060606
reorder the dimensions of this variable optionally	tensor tensor py operators	0.015625
the hack in profiling to print the mflops	nnet base abstract conv flops inp outp	0.125000
fill a with	second inplace a	0.333333
the value after the import	config param init default filter	0.040000
in a given "group" (ie	inp out grads	0.166667
return a symbolic row variable (ndim=2 broadcastable=[true false])	tensor row	0.050000
this function is basically a	extract constant x elemwise only_process_constants	0.058824
the inner-most loop	tensor make reordered loop	0.111111
will be turned into macros	cop get	0.033333
1) times from a multinomial distribution	streams multinomial	0.076923
the value after the import	init default	0.040000
a canonical	canonical	0.076923
gemm computing an outer-product -> ger	local gemm to ger node	1.000000
a module if the	gof module	0.058824
of this op could be	prod l op	0.033333
from the shape or the	shape	0.010204
idx list to get the	get idx list	0.076923
the subgraph bound by the inputs and outputs	init inputs outputs	0.166667
the dimensions of this variable	tensor tensor py operators	0.015625
incsubtensor x zeros idx -> x	tensor local incsubtensor of zeros	1.000000
other implementation of mod	scalar mod c code node	0.125000
called by remove_feature feature should remove any	feature on detach function_graph	0.200000
a series	linker many linkers wrappers	0.047619
string representating the cause of the	bad optimization str diagnostic	0.043478
assert that x	x	0.008772
output has	output	0.017241
the variables inside the scan that	scan	0.034483
:attr	gpuarray gpu	0.045455
dictionary data structures	push out non seq scan	0.125000
mul	local mul	1.000000
spaced by 2**72 samples	get substream rstates n_streams dtype inc_rstate	0.142857
interface to manipulate the subgraph in functiongraph	gof function graph replace r new_r reason verbose	0.250000
for	graph to gpulocal opt	0.055556
should remove any dynamically added functionality	print listener on detach fgraph	1.000000
shape of	tensor shape	0.058824
user is not attempting to use dnn	dnn	0.060606
in profiling to print the mflops	tensor nnet conv op flops inputs outputs	0.125000
fourier transform of a real-valued input on the	gpuarray curfft inp norm	0.066667
for myresult	gpuarray gpu careduce cuda assign init first_item	0.166667
into a canonical form that respects	tensor get canonical form	0.045455
their apply_node if those	gof	0.002381
unroll the batch	tensor nnet gen conv code unroll batch	0.166667
a	gpu	0.011765
as replace_all_validate revert the replacement if	gof replace validate replace all validate	0.111111
a variable that represents their unification	unification	0.076923
a clone in a new	clone	0.020833
each shape that broadcast them to match	generate broadcasting	0.066667
return a tuple of integers indicating the version	cache version apply	0.125000
one or more multinomial distributions defined by one-dimensional	multinomial random_state	0.040000
the	tensor	0.019293
cpu	cpu	1.000000
replacement if the ops in the list	validate replace all	0.050000
default output for	apply default output	0.250000
copies the stack trace from one or	gof copy stack trace	0.055556
crossentropysoftmax1hotwithbiasdx op whose incoming gradient	local useless crossentropy softmax 1hot with bias dx	0.111111
this variable optionally inserting broadcasted dimensions	py operators	0.015625
replacement if	replace validate replace all	0.050000
optionally inserting broadcasted dimensions	operators dimshuffle	0.019231
of scan	push out scan	0.050000
reshaped view/copy of this variable	tensor tensor py operators reshape shape ndim	0.111111
inner graph of scan to outside of scan	scan output	0.125000
new variable	var name doc	0.250000
into macros for use within the	cop get	0.033333
computes the outer product of	sparse block outer	0.047619
reshapes the input to	input	0.023810
a	destroy	0.009709
stack trace from one or more tensor	stack trace	0.055556
used	gcc	0.023810
that was used	inputs g_outputs	0.090909
wrt, computes	subgraph	0.047619
constants and the rest	rest inputs elemwise	0.125000
execute callbacks calls getattr feature name (*args)	execute callbacks name	0.500000
the first half of v by a	v	0.011111
log	log	0.833333
return full	gof module name from	0.076923
that will be	gof clinker type	0.066667
macros for use within	cop	0.028571
a code string specific to the apply to	apply node	0.031250
the destroyhandler class detects when	handler	0.071429
drawing	n pvals	0.125000
dimshuffle which only adds dimension to the left	dimshuffle	0.014493
sample a	size	0.076923
to sharedvariable instances of suitable dummy values	meta optimizer provide inputs	0.200000
of a real-valued input on the	gpuarray curfft inp norm	0.066667
proxy	scalar div proxy	0.125000
-> sum(a axis={ }) / b	local sum prod div dimshuffle node	0.333333
return a code string specific to the	name	0.022222
the hack in profiling to print the mflops	op flops inputs	0.125000
macros for use within the op code	cop get op	0.200000
an op that copies a vector	alloc	0.012500
revert the replacement if	validate replace	0.050000
graph of apply nodes according to a	sort apply nodes inputs outputs cmps	0.050000
the var	var	0.035714
uses shared variable names when persisting to zip	shared variable id	0.142857
that v	v	0.011111
will call the supplied function as its	compile as	0.050000
removes all from the clients list of r	function graph remove client r	1.000000
return a symbolic 3-d variable	tensor tensor3 name dtype	0.166667
op __init__ fct don't have	cast make new inplace output_types_preference name	0.142857
expm1 a	expm1	0.050000
for same kinds of tensortype	tensor tensor	0.014286
that can be	gof	0.004762
given axis es of a	axis ddof keepdims	0.083333
x y idx idx) -> y	tensor local	0.025641
output gradient w r t its inputs	conv2d grad wrt inputs output_grad filters input_shape filter_shape	0.333333
with some	scan_module map	0.500000
a	a replace p	0.250000
dimshuffle is inside an alloc and	tensor local alloc dimshuffle node	0.166667
padleft	padleft	1.000000
op that copies a vector to the	alloc	0.012500
as the output type dtype and broadcast there	local canonicalize alloc	0.333333
matrix solve	tensor solve	0.038462
the confusion	confusion	0.125000
a signature object for comparing tensorconstant instances	signature	0.066667
x is an input vector and t is	node input_storage	0.038462
shape "inshp" with kernels of shape "kshp"	shape inshp kshp stride mode	0.142857
generates the c code for gpucorrmm (direction="forward"), gpucorrmm_gradweights	gpuarray base gpu corr mm c code	0.090909
computes the	ishape kshape border_mode subsample	0.250000
vector to the diagonal of	alloc diag	0.027027
returns the connection pattern of a subgraph defined	io connection pattern	0.055556
and rel error of	rel	0.111111
this variable optionally inserting broadcasted dimensions	operators	0.017241
to evaluate because of aliasing and destructive operations	destroy	0.009709
that operates on	gof	0.002381
macros for use within the	cop	0.028571
the equivalent	to gpulocal	0.055556
this compiles the source code for this linker	gof clinker compile cmodule location	0.038462
output after pad_dims	unpad dims output input	0.333333
symbolic row	tensor row name	0.050000
data structures	scan_module push out non	0.125000
on the gpu	gpuarray	0.023256
[floor] division inverse	int div	0.250000
a compiled module from the loaded	get module	0.200000
the cache if available	gof call cache call fn args key	0.200000
solve operation	tensor solve	0.038462
determine the name	name obj	0.111111
a short mostly hexadecimal hash of a numpy	core hex digest x	0.083333
a config string	core parse config string	0.333333
a real-valued input on	gpuarray curfft inp norm	0.066667
this compiles the source	clinker compile cmodule location	0.038462
es of	dtype keepdims	0.250000
axis that was used to	inputs g_outputs	0.090909
variable optionally inserting broadcasted dimensions	tensor	0.006431
in the theano enumeration types wrapped into current	params type enum from	0.333333
mod	scalar mod c code node name inputs	0.125000
re-raise an exception while annotating	thunk exc_info storage_map	0.250000
compiled theano	fct	0.083333
function builds the 1d kernel that can be	kernel 1d	0.050000
the existence of the __unify_walk__ method for	unify walk a b u	0.037037
update self rstate to be skipped 2^134	rstate	0.090909
row variable	tensor row	0.050000
into a vector	make vector	0.125000
initializes py_name from storage	get c extract r name sub	1.000000
same	shape feature same	0.333333
i and	i	0.076923
x and there is already	linalg local	0.142857
the idx_list with constant inputs	tensor subtensor get constant idx inputs	0.250000
a shared variable to	compile shared variable	0.083333
sharedvariable constructor for scalar values default	scalar shared value name strict allow_downcast	0.200000
with	with op	0.166667
a view	compile view	0.500000
a n-d tensor where n	ws ignore_border stride	0.090909
of the specified pieces of vectors and	sparse block outer make node o x	0.066667
replace a leaf of a multiplication tree	tensor nnet replace leaf	0.100000
fgraph this is the place to do it	fgraph	0.012195
and converts this to expm1 a	tensor local expm1	0.066667
the module file	key data	0.250000
the replacement if the ops in the list	validate replace	0.050000
true for small or builtin	is simple	0.200000
full path of the dynamic lib in it	gof module	0.058824
epoch of the last access of	last access	0.040000
functiongraph listeners to help the navigator deal with	gof navigator optimizer	0.038462
upcasts constant inputs to	constant inputs node	0.125000
the folowing changes in the graph t	mul switch sink	0.045455
error and exit code in a	subprocess popen command	0.083333
b), axis=0) -> elemwise{scalar op}	tensor local reduce join	0.111111
returns maximum elements and their	max and argmax a	0.250000
not the other implementation of mod	mod c	0.125000
helper function for diagonalsubtensor	diagonal subtensor view x i0 i1	0.083333
the contents of a cache	dirname err files	0.083333
pattern for advancedsubtensor output variable	pattern a idx	0.066667
a new	clone	0.041667
for every node that uses r	gof function graph	0.031250
apply_node if those	gof	0.002381
point	entry_1 entry_2	1.000000
exception some perform() or c_code() modified an	bad destroy map	0.142857
sample from a normal distribution	normal size	1.000000
reorder	operators dimshuffle	0.019231
this type	gof clinker type c code cache	1.000000
dnn	no dnn	0.125000
a <= b	le a b	1.000000
a failure code to the task	task	0.083333
of the specified pieces of vectors and matrices	sparse block outer make node o x y	0.066667
this	to gpulocal opt	0.055556
side	side	1.000000
r's shape in the shape_of dictionary	shape feature init r r	0.333333
concatenate tensortypes	tensor join	0.250000
pattern	pattern a	0.066667
the dimensions of this variable	py operators dimshuffle	0.019231
return a thunk that is a zero-arguments	thunk	0.021277
by transferring each of its outputs to the	remove	0.035714
new	clone	0.041667
series	linker many linkers wrappers	0.047619
the passed-in key is found	get from key key key_data	0.111111
self _grad_op from user supplied form to	from graph recompute grad op	0.200000
is a mrg stream state and	sandbox mrg random streams	0.033333
like zeros_like but forces	core float zeros like x	0.200000
gradient w r t its inputs	grad wrt inputs output_grad filters input_shape filter_shape	0.333333
op without affecting the	op	0.009174
of policies to	policy policy	0.125000
this apply instance in a	gof apply clone with	0.166667
if current paramstype contains the specified theano type	params type has type theano_type	0.500000
return full path of the dynamic lib	module name from	0.076923
with logsoftmax	logsoftmax node	0.125000
addition of a sparse	add	0.034483
and converts this to expm1	local expm1	0.066667
abs and	abs	0.066667
the	graph to gpulocal	0.055556
the context object mapped	type context	0.090909
would be useless this function returns val	call val	1.000000
dimensions of this	dimshuffle	0.014493
numeric shape of all intermediate variables	tensor shape of variables fgraph input_shapes	0.100000
or more multinomial distributions defined by one-dimensional slices	multinomial	0.024390
shape of	shape	0.061224
computation unit composed	code block	0.333333
the optimization to the provided l{functiongraph} it	gof optimizer apply	0.166667
a cache directory and	dir	0.076923
topooptimizer from the input nodes to output nodes	in2out	0.043478
exception raised to indicate	error	0.025000
the *args directly	csm properties csm node	0.142857
uses the topooptimizer from the input nodes to	in2out	0.043478
to print the mflops	mm flops inp outp	0.125000
the diagonal of an	alloc diag	0.027027
attempts to replace a	optimizer attempt	0.500000
function tries	image_shape top_shape	0.166667
symbolic type representing a	type	0.011905
convert addsd to faster	sparse local addsd ccode node	0.250000
navigator deal	gof navigator optimizer	0.038462
to represent the dependence of nodes	gof make dependence	0.043478
that x and y have the	x y	0.024390
the function name to write	gpuarray write	0.200000
that will be turned into macros for	cop get	0.033333
set of 2d filters	tensor nnet conv2d input filters	0.125000
to represent the dependence of nodes	make dependence	0.043478
callbacks calls getattr feature name (*args)	callbacks name	0.333333
output type dtype and broadcast there	tensor local canonicalize alloc	0.333333
arctangent of a / b (inplace on a)	tensor arctan2 inplace a b	1.000000
to construct a variable with a	variable x name	0.083333
like zeros_like but forces	core float zeros like	0.200000
links all the specified vars	merge new_best	0.142857
this function tries to recognize the updates ordereddict	updates	0.029412
be inserted in the struct initialization code	init code struct	0.125000
a list of header search paths	header dirs	0.045455
raises a badviewmap exception when it detects	check viewmap node	0.111111
parse a	nnet parse mul	0.500000
a reshaped view/copy	py operators reshape shape ndim	0.333333
draw samples from a poisson distribution	poisson size lam ndim	1.000000
max	local max	0.250000
function computes	ishape kshape border_mode subsample	0.250000
will be turned into macros for use within	cop	0.028571
label of apply	apply label	0.500000
x ->	nnet local	0.200000
six moves urllib namespace	six	0.025000
reorder the dimensions of this variable optionally inserting	tensor py	0.015873
apply_node recursively search	import apply_node	0.066667
for diagonalsubtensor	get diagonal subtensor view x i0 i1	0.083333
for diagonalsubtensor and	nnet get diagonal subtensor	0.083333
2d or 3d convolution for	abstract conv conv	0.125000
1 0/a inplace on a	tensor inv inplace a	1.000000
represent the dependence of nodes in a	gof make dependence	0.043478
this generates the c code for corrmm (direction="forward"),	tensor nnet base corr mm c code	0.090909
replace_all_validate revert the replacement	replace all validate remove fgraph	0.111111
variable optionally inserting broadcasted	py operators dimshuffle	0.019231
a new variable to theano config	config var name doc configparam root	0.500000
with a set of 2d filters	input filters	0.117647
version used is the llvm one or not	gcc llvm	0.200000
remove subtensor/advancedsubtensor1 if it takes the	useless subtensor node	0.200000
compute sum of non nan /	constant signature get sum	0.142857
the topooptimizer from the	in2out	0.043478
hint that the variable v is positive	sandbox linalg psd v	0.250000
rebroadcast if id does not actually change the	rebroadcast node	0.500000
given a slice [start stop step] transform it	slice	0.038462
insert deepcopy in the	insert deepcopy	0.333333
of this type	tensor type	0.034483
on the inner graph to	inner graph	0.035714
to	to os environ pathlist	0.038462
a new random stream in this container	random streams gen op	0.250000
base op for cudnn batch normalization	gpu dnn batch norm inference	0.333333
that removes all asserts from	remove all assert	0.055556
expects the compile lock to	gof module cache add to	0.142857
to wait on a	mpirecv wait	0.045455
an array with more than one element is	node inputs	0.086957
pattern of subfgraph defined by	pattern node	0.125000
can't change the value after the	config param init default filter	0.040000
remove reduction over broadcastable dimensions	tensor local reduce broadcastable node	1.000000
an op by transferring each of its	op remove	0.250000
this	tensor tensor	0.014286
or a tensorvariable	tensor as	0.066667
rop	rop	1.000000
the subgraph in functiongraph	function graph replace	0.500000
theano_type	theano_type	0.625000
represents their unification	unification	0.076923
outer product of two sets of pieces of	sparse block outer	0.047619
this optimization	local	0.014085
location	location	1.000000
str	to str t	0.166667
equivalent of	gpulocal	0.055556
a code string specific to the apply	apply	0.016667
new node a clone in a new graph	clone	0.020833
must return a thunk that	thunk	0.021277
dimshuffle is inside an alloc and	local alloc dimshuffle node	0.166667
expects the compile lock to	to	0.017544
compute sum of non nan / inf	tensor tensor constant signature get sum	0.142857
variant on wraplinker that runs	gof	0.002381
"lifts" dimshuffle through elemwise operations and merges consecutive	tensor local dimshuffle lift	0.250000
context associated	gpuarray get context	0.111111
given graph contains a cycle parameters	contains cycle	0.333333
true if the named module is a package	is package fullname	0.250000
replacement if	replace	0.032258
output error and exit code	output subprocess popen command	0.100000
function computes	ishape kshape	0.250000
specified factor takes as input a n-d	tensor signal pool 2d input ws ignore_border stride	0.100000
determine the broadcast	tensor adv index broadcastable	0.050000
return the indices	indices	0.076923
an operation to wait on a previously sent	wait	0.022727
find broken optimizations	find bad optimizations2 order	0.333333
and	grad out	1.000000
this is the equivalent of localoptgroup for graphtogpu	graph to gpulocal opt group	0.055556
in the flattened version of a	tensor flatnonzero a	0.200000
scan in an easy to manipulate	scan args	0.250000
print a warning message on the first	gof deprecated filename msg	0.041667
comparator to represent the dependence	make dependence cmp	0.111111
theano	tensor py	0.174603
this function builds the 1d kernel that can	kernel 1d	0.050000
all sigmoid to ultra_fast_sigmoid	nnet local ultra fast sigmoid node	0.200000
triangular solve	linalg tag solve triangular	0.142857
variables in inputs to sharedvariable instances of	inputs	0.012658
dependence	make dependence	0.043478
see theano tensor repeat	tensor tensor py operators repeat repeats axis	1.000000
return an	node name	0.033333
to type2 from	type2	0.050000
the fgraph this	fgraph	0.012195
has this alias	has alias alias	1.000000
outfile	outfile	1.000000
important note this function uses	process node fgraph node	0.142857
one or more multinomial	tensor multinomial random_state	0.040000
list of lib directories that are	lib dirs	0.045455
downsample with	downsample factor	0.500000
suitable dummy values	local meta optimizer provide inputs	0.200000
as the template filled by broadcasting value through	broadcast like value template fgraph	0.125000
diagonalsubtensor	nnet get diagonal subtensor view x	0.083333
this	tensor	0.006431
see theano tensor repeat	py operators repeat repeats axis	1.000000
bitwise a |	tensor or a	1.000000
previously received array using mpi	mpirecv	0.037037
add tag trace to an node	add tag trace thing user_line	0.166667
a <	lt a	1.000000
value in array of ints	x weights minlength assert_nonneg	0.125000
tag trace	tag trace thing user_line	0.166667
conda offers to make itself the default python	to os environ	0.038462
a	tensor constant	0.055556
be turned into macros	cop	0.028571
duplicate this apply instance in a new	gof apply clone with new inputs inputs	0.250000
op could	l op	0.033333
row is a mrg stream state and	mrg random streams	0.033333
based level of the list	list	0.066667
the new graphtogpu optimizer	gpuarray register opt2 tracks	0.250000
updates for matrix solve operation c	solve	0.032258
with the *args directly	sparse local csm properties csm node	0.142857
a == b inplace on a	tensor eq inplace a b	0.500000
output gradient w r	conv2d grad	0.111111
see theano tensor nonzero_values	tensor py operators nonzero values	1.000000
to be between min and	min	0.071429
transfer to a tensortype if not already one	tensor tensor py operators transfer	0.125000
diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view	0.083333
filters with	tensor nnet conv3d signals filters	0.111111
computes the output shape	out shape ishape kshape border_mode	0.500000
c_init that	gof	0.002381
called by toposort it should return a dictionary	gof feature orderings function_graph	1.000000
eval_points	eval_points	0.833333
signature object for comparing tensorconstant instances	constant signature	0.100000
op	op params	0.100000
if this enum has this	gof enum type has	0.111111
minimum elements obtained by iterating over given axis	argmin x axis keepdims	0.500000
conda offers to make itself	to os environ	0.038462
on f	core lop f	0.166667
c code for corrmm (direction="forward"), corrmm_gradweights	base corr mm c code	0.090909
a new	new inputs	0.166667
the cache and	cache	0.034483
function as	compile as	0.050000
this function expects the compile lock to	add to	0.142857
apply nodes according to a list of comparators	apply nodes inputs outputs cmps	0.050000
python litterals to theano constants in subtensor arguments	tensor make constant args	0.250000
functiongraph listeners to help the navigator	navigator optimizer attach updater fgraph	0.038462
for convop that unroll the batch size loop	unroll batch kern d unroll_bsize unroll_ksize	0.125000
elementwise modulo (inplace on a)	mod inplace a b	1.000000
connection pattern of subfgraph	op from graph connection pattern	0.076923
the maximum in one tensor	tensor maximum x y	0.090909
fgraph outputs that will replace their values	fgraph expanded_inputs	0.058824
can't change the value after the import of	init default filter	0.040000
object with debug	with	0.076923
represent the dependence	gof make dependence	0.043478
x and y have the	x y	0.024390
use dnn conv workmem	no dnn workmem workmem	0.166667
series of wrapper functions	wrap linker many linkers wrappers	0.047619
diagonalsubtensor and	nnet get diagonal subtensor view x	0.083333
mrg stream state and they are spaced by	mrg random streams	0.033333
substream	substream	1.000000
name	name obj	0.111111
parses a config string (comma-separated key=value components) into	parse config string config_string issue_warnings	0.166667
optimization makes the folowing changes in	local mul switch sink node	0.045455
graph for convolving a mini-batch of a stack	input_shape filter_shape	0.027778
reorder the dimensions	dimshuffle	0.014493
feature should	feature on	0.200000
to concatenate tensortypes along the given axis	tensor join axis	0.333333
new variable instance of type self	gof pure type	0.500000
create pydot node from	to pdnode d	0.333333
listeners to help the navigator deal	gof navigator optimizer attach updater fgraph	0.038462
the node i pairs such that node	gof function graph clients	0.100000
simplify a	simplify	0.142857
algo as c round() fct	away from zero	1.000000
an array with more than one	node inputs outputs	0.125000
wrt, computes gradients	core subgraph grad	0.062500
named module is a package	is package fullname	0.250000
on wraplinker that runs a series of wrapper	gof wrap linker many linkers wrappers	0.071429
apply the	apply	0.016667
cause of the exception	bad optimization str diagnostic	0.043478
returns a module if	module	0.033333
object but we don't clone the data to	clone	0.020833
some functiongraph listeners to help the navigator deal	gof navigator optimizer attach updater fgraph	0.038462
context object mapped to the type's :attr	context	0.035714
for efficiently calculating the dot product	true dot	0.166667
value after the import of theano	core config param init default filter	0.040000
using advanced indexing	advanced inc	1.000000
replace_all_validate revert the replacement if	validate replace all validate remove fgraph	0.111111
the	graph to gpulocal opt	0.055556
1) it has a 'requirement' of the destroyhandler	add destroy handler	0.125000
given axis es of a tensor input	input axis dtype	0.500000
round half	round half	0.100000
triu	triu	0.833333
special compound	argmax1hot	0.058824
the replacement if	validate replace all	0.050000
gpu version of sparseblockgemv check sparseblockgemv's docstring for	gpu sparse block gemv	1.000000
more multinomial distributions defined by one-dimensional	multinomial random_state	0.040000
an	node	0.029630
function must return a thunk that	thunk	0.021277
linker's fgraph	thunk input_storage output_storage storage_map keep_lock	0.333333
some text to a gist	gist	0.040000
for cudnn	gpu dnn	0.066667
remove subtensor/advancedsubtensor1	useless subtensor	1.000000
the output dimensions of convolving an image of	output	0.017241
symbolic row variable (ndim=2 broadcastable=[true false])	tensor row	0.050000
input vector and t is	node input_storage output_storage	0.038462
output dimensions of convolving an image of shape	output	0.017241
expects the compile lock to	module cache add to	0.142857
helper function to	helper random_state a	0.500000
helper function for grad function	core populate grad dict var_to_app_to_idx grad_dict	1.000000
multiply the first half of v by a	v	0.011111
a number of scalars together into a	make	0.017857
swap axes of inputted tensor	tensor swapaxes y axis1 axis2	1.000000
m1 and the	m1	0.027027
r-operator for	gpu max pool rop	0.333333
to construct a variable with	variable	0.022222
node a clone in	clone	0.020833
changes node inputs[i] to new_r	function graph change input node i new_r	0.500000
type	tensor type	0.068966
a pyobject * instance	sub check_input	0.500000
of scan return true	scan	0.017241
scalar constants and the rest	rest inputs	0.125000
return true if we are able to	dim_x dim_y	0.090909
leaf_formatter	leaf_formatter	1.000000
set	set	1.000000
a input shortcut to an in instance	compile convert function input input	0.333333
a and b can be considered approximately equal	type values eq approx a b	1.000000
impossible to evaluate because of aliasing	destroy	0.009709
an fgraph and a list of variables	fgraph	0.012195
is an input vector and t	node input_storage output_storage	0.038462
the destroy_map	bad destroy	0.034483
a leftdims +	leftdims	0.090909
over each shape that broadcast them to	tensor generate broadcasting	0.066667
is false we can't change the value after	init default	0.040000
feature should	feature	0.083333
for bilinear upsampling this function builds	bilinear	0.038462
the graph and get a memo a dict	graph	0.016393
for the eigensystem of	tensor eigh	0.333333
optionally inserting broadcasted dimensions	py operators	0.015625
alias that wasn't in the view_map	bad	0.013158
transfer data between gpus	gpu to gpu	1.000000
batch normalization of	dnn batch normalization	0.125000
scalar variable	scalar name	0.500000
while annotating	thunk exc_info storage_map	0.250000
:attr	gpuarray gpu array	0.062500
g++ version used is the	gof gcc	0.027778
of mod	scalar mod c	0.125000
gradient wrt filters for gpucorrmm	gpu corr mm grad weights	1.000000
on the output	gpuarray output	0.200000
v real	v x	0.100000
get the 0 based	type get depth	0.050000
to merge multiplication by a	gpuarray alpha merge	0.076923
to add the tensor operators to the	tensor	0.006431
and chooses the orphans among them	and orphans	0.166667
called whenever node inputs[i] is	change input function_graph node i	0.333333
a normal	tensor random streams base normal	0.500000
theano expression whose gradient should be truncated	x	0.008772
shape and	grad out shape	1.000000
is	gpulocal	0.055556
reproducible case for problems	compile function dump filename inputs	0.166667
base class for operations that need to	gpu kernel base	0.333333
remove are still	replacements remove reason	0.055556
mm	mm	1.000000
if allow_override is false	allow_override	0.083333
insert deepcopy	compile insert deepcopy	0.333333
to outdim	outdim	0.142857
indent_level	indent_level	1.000000
the broadcast pattern for advancedsubtensor output	pattern a idx	0.066667
of the specified pieces of vectors and	sparse block gemv make node o w	0.066667
symbolic type representing a numpy	tensor type	0.034483
of scalars together into a vector	vector	0.066667
returns true if var is always equal	gpuarray is equal var	0.250000
the inputs and	init inputs	0.083333
of the specified pieces of vectors and matrices	sparse block gemv make node o w h	0.066667
offers to make itself the default python and	to os	0.038462
by doing a recursion over the input	tensor permute row elements rec	0.047619
a simple algorithm	optimizations2 order reasons r_vals	0.333333
values from a with or without replacement	tensor random streams base choice size a replace	0.333333
canonical form that respects	get canonical form	0.045455
the -x pattern	tensor nnet is neg var	0.166667
tuple of symbolic shape vars for tensor variable	shape tuple	1.000000
help the navigator deal	navigator optimizer attach	0.038462
lst	lst	1.000000
along the given axis es of a	axis dtype	0.083333
a tree of multiplications starting at the given	tree	0.125000
the replacement if the	validate replace all	0.050000
to get the 0 based level of the	get	0.020833
c-implementation	csr c code node name inputs outputs	0.333333
get a	get	0.020833
instance of _maker which handles much of	function maker i o m	0.066667
the inputs and put the variables	pure op perform node inputs output_storage params	0.047619
specified pieces of vectors and	sparse block gemv make	0.066667
it does the	alloc	0.012500
c contiguous version of	gpu contiguous	0.083333
e^a (inplace on a)	tensor exp inplace a	1.000000
text to a gist and	gist	0.040000
msg	init msg	1.000000
around c_cleanup that decrefs py_name	gof get c cleanup r	0.250000
replaces an [advanced]incsubtensor[1], whose increment	local useless inc subtensor	0.333333
folowing changes in the graph t	mul switch sink node	0.045455
division inverse of multiplication	div	0.166667
the idx list to get the right	tensor get idx list	0.076923
input shortcut to an in instance	compile convert function input input	0.333333
elements obtained by iterating over given axis	argmin x axis keepdims	0.500000
the "reverse-mode" gradient for the	grad perform	0.083333
it to a max	max	0.062500
a memory alias that wasn't	bad	0.013158
dimensions of this variable	operators dimshuffle	0.019231
function for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view	0.083333
for same kinds	type	0.011905
meta path importer to	meta path importer	0.166667
i of	shape i r i	0.500000
initialization	init	0.058824
code to pack c types back into	clinker type c sync	0.111111
the inputs required to compute the given	inputs variable_list blockers	0.058824
this preserve some variables attributes and tag during	preserve variable attributes	0.500000
the equivalent of	to gpulocal	0.055556
find	find bad optimizations2 order	0.333333
on the inner	validate inner	0.142857
compare true iff other is the same	eq other	0.166667
replace	tensor nnet replace	0.250000
variable out for occurrences	out	0.018519
will call the supplied function as	compile as	0.050000
return a tuple of integers indicating the version	cache version	0.125000
readable string representation of self	scalar composite init	0.200000
create a new instance of this mode	monitor mode clone link_kwargs optimizer	0.333333
change the value after the	config param init default filter	0.040000
optimizer of type topooptimizer	topo db	0.166667
reverse-mode gradient updates	grad inputs output_gradients	0.250000
expects the compile lock to be held	add to cache module key module_hash	0.166667
trace to	trace	0.052632
pooling	gpu dnn pool	0.500000
kind	kind	1.000000
optimization makes the folowing changes in the	mul switch sink node	0.045455
modified bessel function of	tensor i1 x	1.000000
scan that depend only on	seq scan	0.250000
dimensions of x	x axes	0.200000
shape s to previously un-shaped variable r	feature set shape r s override	0.500000
shape and dtype	dtype	0.022727
perform the permutation by doing	perform node x y	0.166667
is not the same on all device	device	0.076923
to add the tensor operators	tensor	0.006431
have the same shape	shape feature same shape	0.333333
transfer to a tensortype if not	operators transfer	0.125000
the source code for	cmodule location	0.038462
current op and reduce pattern has functioning	gpu careduce cuda supports	0.166667
optimization makes the folowing changes	mul switch sink node	0.045455
a dimshuffle is inside an alloc	alloc dimshuffle node	0.333333
feature should remove any dynamically-added	feature on detach	0.200000
each row is a mrg stream state	sandbox mrg random streams	0.033333
add an	compat add	0.250000
(direction="forward"), corrmm_gradweights (direction="backprop weights"), and	helper bottom weights top direction	0.055556
inserting broadcasted dimensions	py operators dimshuffle	0.019231
a symbolic 3-d variable	tensor tensor3 name dtype	0.166667
load a file that	load	0.083333
an array from disk	from disk	1.000000
compiles the source code	compile cmodule location	0.038462
computes gradients of cost and/or from existing start	start cost	0.100000
tell rebroadcast how to generate c code for	register rebroadcast c code typ code	1.000000
kernel	kernel	0.333333
the dimensions of this	tensor py operators	0.015625
inner-most loop executes code	loop	0.027778
leftdims	leftdims	0.545455
to print the mflops	tensor nnet conv op flops inputs	0.125000
a symbolic integer scalar for the shape element	shape feature unpack	0.500000
symbolic integer scalar for the shape element s_i	shape feature unpack s_i	1.000000
from a uniform distribution	tensor uniform	0.125000
the diagonal of an empty matrix	diag	0.023810
the folowing changes in the	local mul switch sink node	0.045455
to wait on	wait	0.045455
attempts to replace a	scan_module scan inplace optimizer attempt	0.500000
remove subtensor/advancedsubtensor1 if it takes the	tensor local useless subtensor node	0.200000
of m1 and the second half by	m1	0.027027
the r operation on f wrt to wrt	core rop f wrt	0.200000
value after the	init default	0.040000
import	import r variable reason	1.000000
expression is alpha * x y + z	usmm csc dense	1.000000
tensortype	tensor tensor	0.014286
the standard deviation along	std	0.058824
is the equivalent	opt	0.043478
diagonal of an empty matrix it does the	alloc diag	0.027027
the computation to	node	0.007407
macos	macos	1.000000
input that wasn't in the destroy_map	bad	0.013158
similar behaviour as python's map	map fn sequences non_sequences	1.000000
some perform() or c_code() modified an input	bad destroy map	0.142857
should remove any dynamically added functionality	validate on detach fgraph	1.000000
initializes py_name	r name	0.250000
op could be very easy if it	l op	0.033333
to a gist and	gist	0.040000
lib directories that	lib dirs	0.045455
make a nested loop over	make loop	0.200000
given an fgraph and a	fgraph	0.012195
of lib directories that are needed	lib dirs	0.045455
kinds of useless reshape	tensor local useless reshape node	0.200000
clone this	gof	0.002381
pieces of vectors and	sparse block outer make node o	0.066667
operation to wait on	wait	0.045455
required return c code to declare variables	c declare name sub	0.500000
cthunk	cthunk	1.000000
the values of a shared variable	compile shared variable	0.083333
implements the "reverse-mode" gradient for the eigensystem	eigh grad perform node	0.333333
convolving a mini-batch of a	input_shape filter_shape	0.027778
false we can't change the value after the	param init default filter	0.040000
scale each row	row scale	1.000000
function on the inputs and put the	gof pure op perform node inputs output_storage params	0.047619
see theano tensor prod	py operators prod	1.000000
conventions imposed by python	theslice length	0.052632
evaluate because of aliasing and destructive	destroy	0.009709
the l operation on f	f	0.052632
warn	warn	1.000000
alloc would be useless	tensor alloc call	0.333333
vector	make vector	0.125000
inserting	py	0.014286
inserted to maintain order	tensor searchsorted x v side sorter	0.142857
the equivalent of	gpulocal opt	0.055556
a symbolic column variable (ndim=2 broadcastable=[false true])	col name dtype	0.200000
is impossible	destroy	0.009709
install some functiongraph listeners to help the navigator	navigator optimizer attach updater	0.038462
node	sub transform node	1.000000
which will print a warning message on the	deprecated filename msg	0.041667
difference	difference	1.000000
work for elemwise and gpuelemwise	tensor local elemwise	0.166667
url	content description filename auth	0.250000
base 2 logarithm	log2 a	1.000000
division (inplace on a)	int div inplace a	1.000000
series of	wrap linker many linkers wrappers	0.047619
takes as input a n-d	signal pool 2d input ws ignore_border stride	0.100000
v raises	v	0.011111
replace it with a triangular	triangular node	0.125000
baddestroymap	node storage_map	0.166667
op	alloc	0.012500
profilemode to print the mflops	flops inp outp	0.083333
connection pattern	graph connection pattern node	0.076923
to find	compile find bad optimizations2	0.333333
by functiongraph attach_feature the method that attaches	gof bookkeeper on attach fgraph	0.142857
function that reduces a contiguous buffer	gpuarray inline reduce n buf pos count	0.142857
set of 3d	nnet conv3d input	0.125000
in_ys	in_ys	1.000000
total time icluding the time for	compute total times	0.200000
and replace it with logsoftmax x 's	local logsoftmax	0.076923
of lib directories that are needed by	header dirs	0.045455
existence of the __unify_walk__ method for one of	gof unify walk a b u	0.037037
the connection pattern of	connection pattern	0.032258
with a set of 3d filters	nnet conv3d input filters	0.142857
l{linker}s which keep all nodes in the	local linker	0.333333
compute 2d kernel for bilinear upsampling this function	tensor nnet bilinear	0.111111
a thunk that is a zero-arguments	thunk	0.021277
ops when those ops do implicit upcasting anyway	local upcast	1.000000
is the	gof gcc	0.027778
converts this to expm1	tensor local expm1 node	0.066667
should be removed and	compress outs	0.076923
cos	cos	0.714286
return a symbolic row	tensor row	0.050000
to compute the image shape of convolution gradweights	tensor nnet get conv gradweights shape 1axis image_shape	0.500000
folowing changes in the graph	mul switch sink node	0.045455
evaluate because of aliasing	destroy	0.009709
structured addition of a sparse matrix and a	structured add	0.142857
ordering of the graph's apply nodes such that	gof function	0.043478
a new variable instance of type self	gof pure type	0.500000
kshp	kshp	1.000000
template filled by broadcasting value through	broadcast like value template fgraph	0.125000
multinomial distributions defined by	multinomial	0.024390
the specified outputs	fgraph	0.012195
c_code does not support the types involved	tensor inc subtensor do type checking	0.142857
by re-writing the file containing	gof refresh	0.125000
t -> dot y t x t	tensor local lift transpose through dot node	0.333333
cache "filename" as	gof call cache persist filename	0.250000
with respect to wrt, computes gradients of	core subgraph grad wrt	0.062500
in self	core	0.111111
this is the equivalent of	gpulocal	0.055556
input a	2d same size input	0.500000
reorder the dimensions of this	tensor py operators	0.015625
converts this to expm1 a	tensor local expm1	0.066667
current op and reduce pattern has functioning	careduce cuda supports	0.166667
are non-zero in the flattened version	flatnonzero	0.083333
kinds of tensortype	tensor type	0.034483
the graph and get	function graph	0.040000
split	split	0.750000
object with debug	with op	0.166667
to the apply to be inserted in the	apply node	0.031250
correspond to the one	to one	0.125000
variables given	variables	0.043478
the first functiongraph that has ever	gof	0.002381
types	params type	0.250000
cached	cached	1.000000
an input that wasn't	destroy	0.009709
the dimensions of this variable optionally inserting broadcasted	py	0.014286
cost and/or	cost	0.045455
a true distribution of the form [0 0	crossentropy categorical1hot	0.166667
first functiongraph that has	gof	0.002381
try to detect	tensor detect	0.166667
baddestroymap if	storage_map	0.090909
reverse-mode gradient updates for matrix solve operation	tensor solve grad inputs output_gradients	0.333333
use_list and	as use_list	1.000000
maker	maker	0.833333
replace it with logsoftmax x 's	tensor nnet local logsoftmax	0.076923
remove two kinds of useless reshape	local useless reshape	0.200000
make a nested	tensor make	0.076923
an [advanced]incsubtensor[1], whose increment is an alloc of	tensor local useless inc subtensor alloc node	0.166667
function that uses	scan_module	0.125000
this class returns the bartlett spectral window in	bartlett	0.058824
gen	gen	1.000000
conv output gradient w r t its inputs	grad wrt inputs output_grad filters input_shape filter_shape	0.333333
remove are still in	fgraph replacements remove reason	0.055556
xidx	xidx	1.000000
to a specified scalar value	a	0.008065
to pass to helper_c_code	subtensor get helper c code	0.142857
listeners to help the navigator deal with the	navigator optimizer attach updater	0.038462
signature object for	constant signature	0.100000
this is the equivalent of localoptgroup for	opt group	0.043478
product of the specified pieces of vectors	sparse block gemv make node o	0.066667
stack sparse matrices vertically row wise	sparse vstack blocks format dtype	1.000000
assign the shape	shape feature set shape	0.333333
a diff to make code correctly indented	misc hooks get correct indentation diff code	0.333333
that allows replacing subgraphs	scan_module clone output replace strict share_inputs	0.071429
fortran blas	blas	0.125000
this optimization makes the folowing changes in the	mul switch sink	0.045455
symbolic row variable (ndim=2 broadcastable=[true false])	tensor row name	0.050000
of outputs and the	and outputs	0.100000
incsubtensor when we overwrite the	tensor local useless inc subtensor node	0.066667
litterals to theano constants in subtensor arguments	constant args	0.250000
convolution gradient with respect to	gpu dnn conv grad w	0.125000
by passing a dictionary unary_out_lookup({int8 int32 float32 complex128})	unary out lookup	0.250000
the cross-entropy between an approximating distribution and	tensor nnet categorical crossentropy coding_dist true_dist	0.111111
outputs and the stopping condition returned by the	and outputs ls	0.166667
softmax(sum_of_stuff) -> softmax_w_bias matrix bias	nnet local softmax with bias node	0.200000
the type	type	0.011905
an instance of _maker which handles much of	debug mode function maker i o m	0.066667
type c type numpy typenum that	tensor tensor type dtype specs	0.071429
post some text to a gist and	misc post gist	0.333333
y with length one the	y axis	0.125000
updates ordereddict the list of outputs and	updates and outputs	0.333333
a new node a clone in	clone	0.020833
work for gpuincsubtensor	inplace setsubtensor node	0.250000
to 0	zero borrow	0.500000
occurrences of each value in array of ints	weights minlength assert_nonneg	0.125000
lower triangle	tensor tril m k	0.250000
the updates	scan_module get updates	0.034483
tree of multiplications starting at the given root	mul tree root	1.000000
of ints	x weights minlength assert_nonneg	0.125000
cache or the disk	cache	0.034483
unify values of corresponding keys	unify walk d1 d2 u	1.000000
readable string representation of	scalar composite init	0.200000
the replacement	gof replace validate replace	0.050000
of this variable optionally	tensor tensor py	0.015873
y with length	y axis	0.125000
that runs	gof wrap linker	0.083333
pickler that strips unnecessary attributes from theano objects	strip pickler	1.000000
in pvals	n pvals	0.125000
the output shape of convolution operation	get conv shape 1axis image_shape kernel_shape border_mode subsample	0.333333
reset	reset	1.000000
as the output type dtype and broadcast there	tensor local useless alloc	0.333333
helper function for diagonalsubtensor and	diagonal subtensor view x	0.083333
cond	switch cond	0.500000
function computes the output shape	out shape ishape kshape	0.500000
largest eigenvalue of square symmetrix matrix	sandbox linalg spectral radius	0.166667
inserting	tensor tensor py operators	0.015625
some perform() or c_code() modified an	bad destroy map	0.142857
a badviewmap exception when it detects	compile check viewmap node storage_map	0.111111
create pydot graph object from theano function	py dot formatter	0.500000
with a modulo of m1 and	m1	0.027027
loop over	loop	0.027778
dimensions	operators	0.034483
default_to_move_computation_to_gpu	default_to_move_computation_to_gpu	1.000000
in the original graph to	inputs outputs copy_inputs_and_orphans memo	0.029412
empty	empty	0.833333
resembles the python 3 namespace	moves urllib	0.038462
connection pattern of subfgraph defined by inputs	connection pattern	0.032258
dct	dct	1.000000
replace a leaf of	replace leaf	0.100000
pydot graph and write to dot file	d3viz d3write fct path	0.166667
on the inner graph to ensure that it	scan_module scan validate inner graph	0.035714
the output dimensions of convolving an	tensor nnet conv op get output	0.047619
return a string	name x z	0.333333
return a symbolic integer scalar	unpack	0.125000
apply_node if those nodes are	gof	0.002381
sample from one or more multinomial distributions defined	tensor multinomial random_state size	0.333333
update self rstate to be skipped 2^134 steps	rstate	0.090909
localoptimizer and applies them to the node	local opt group	0.052632
conda offers to make itself the default python	to os	0.038462
seconds since the epoch of the last access	last access	0.040000
broadcast them	tensor generate broadcasting	0.066667
inner graph to ensure that it is coherent	inner graph	0.035714
row correspond to the one hot	one hot	0.142857
can't change the value after	default filter	0.040000
operators to	tensor	0.006431
op then replace it with a triangular	triangular node	0.125000
max for the maximum in one tensor	tensor maximum x y	0.090909
of localoptgroup for	gpulocal opt group	0.055556
the maximum in	tensor maximum	0.142857
a six moves urllib	module six	0.043478
headers that	headers	0.038462
navigator deal	gof navigator optimizer attach	0.038462
the outputs	outputs mode accept_inplace	0.166667
on	as gpuarray	0.333333
return a symbolic row variable	row name dtype	0.050000
calculating the dot product	true dot x	0.166667
is the equivalent of	opt	0.043478
the variables that	gof clinker	0.033333
return the platform-dependent	gof get	0.100000
==	tensor eq	0.666667
metaclass	compat add metaclass metaclass	0.125000
the fgraph to break aliasing of outputs	fgraph wrapped_inputs wrapped_outputs	0.111111
2d inputs with a set of 2d filters	nnet conv2d input filters	0.125000
cholesky	cholesky	0.833333
the outputs of specific ops of	trace f_or_fgraph ops_to_check bug_print	0.035714
integers indicating the version	object c code cache version	0.125000
feature	feature on detach	0.200000
the task that is associated to it	gof cthunk find task failure_code	0.083333
given axis	tensor min x axis	1.000000
setsubtensor(x x[idx], idx) -> x when x is	local setsubtensor of constants node	0.250000
function to roll	tensor roll	0.250000
instance node	node node	0.333333
module is a	meta path importer is	0.250000
return complex-valued tensor from polar	tensor complex from polar	0.250000
basic slow python 2d	img kern mode dilation	0.250000
blas ldflags	ldflags	0.111111
still in the	replacements	0.111111
is no change in the shape	node	0.014815
connection pattern	connection pattern node	0.076923
the	gpuarray gpu array type	0.125000
core	core	0.555556
config string (comma-separated key=value components) into a	parse config string config_string issue_warnings	0.166667
the diagonal of an empty	diag	0.023810
dimensions of this variable optionally inserting broadcasted dimensions	py operators	0.015625
theano gof graph is_same_graph	is same graph with merge var1 var2 givens	0.166667
set to a specified scalar value	a	0.008065
on f	rop f	0.166667
and "code_cleanup" together	cleanup node name inputs outputs	0.500000
output nodes of the	gof	0.002381
given axis es	axis dtype keepdims	0.083333
total time icluding the time for	total times	0.200000
pack c types back into a pyobject	gof clinker type c sync	0.111111
of this op could be very	l op	0.033333
sparse format instead of	sparse	0.019231
function only avail on compute capability 2	dev20	0.166667
for a convolution	gpu dnn conv	0.200000
given axis es of a	axis dtype op	0.083333
this function uses set and dictionary data structures	scan_module push out non seq scan	0.125000
function to concatenate tensortypes	tensor join	0.250000
a dimshuffle	dimshuffle	0.014493
remove incsubtensor when we overwrite the full inputs	local useless inc subtensor node	0.066667
return none or a tensorvariable	tensor as	0.066667
v real	v	0.022222
contains	gof contains	0.500000
convolve spatio-temporal filters with a	nnet conv3d signals filters	0.111111
c_extract that initializes py_name from storage	gof get c extract r	0.250000
to the diagonal of an empty	alloc diag	0.027027
a basic theano op that will	op itypes otypes infer_shape	0.047619
var transferred to target	tensor transfer var target	0.200000
|a| (inplace on a)	abs inplace a	1.000000
mode that	mode	0.062500
to the idx list to get the	get idx list	0.076923
some perform() or c_code() created a memory alias	map	0.047619
reorder the	py operators dimshuffle	0.019231
a b	a b	0.066667
product of the specified pieces of vectors and	sparse block outer make node o x	0.066667
shape	tensor shape	0.176471
return connection pattern of subfgraph defined by inputs	graph connection pattern	0.076923
to merge multiplication by a scalar on	gpuarray alpha merge	0.076923
instance of	link_kwargs optimizer	0.333333
randomstate instance associated with a particular	setitem item val	0.125000
value after the	core config param init default filter	0.040000
a == b	eq a b	1.000000
the replacement if the	validate replace	0.050000
compare true	tensor type eq	1.000000
apply nodes according to a	sort apply nodes inputs outputs cmps	0.050000
cache and none	cache	0.034483
the "reverse-mode" gradient for	grad perform	0.083333
the mflops	tensor nnet base abstract conv flops inp outp	0.125000
the equivalent of localoptgroup	to gpulocal opt group	0.055556
of an empty matrix it	alloc	0.012500
theano graphs represent	xs ys in_xs in_ys	0.111111
computes the mean value	tensor mean	0.111111
deep	deep	1.000000
in profiling to print the mflops	tensor nnet base abstract conv flops inp outp	0.125000
a list of libraries	libraries	0.111111
to help the navigator deal with the	navigator optimizer attach updater	0.038462
name r	r name	0.250000
and return full path of the dynamic	gof module name from	0.076923
kinds	tensor type	0.034483
a list of shape	shape	0.010204
a gradient	grad fun pt	0.500000
mflops	flops inp outp	0.125000
broadcasted	operators dimshuffle	0.019231
base 2 logarithm of a inplace on a	tensor log2 inplace a	1.000000
gpuelemwise	gpuarray local gpu elemwise	1.000000
to the end variables of a	wrt end	0.050000
c type numpy typenum that corresponds to	tensor tensor type dtype specs	0.071429
return path to the	get entry	0.333333
convert addsd	sparse local addsd	0.250000
row variable	row	0.034483
object but we don't clone the data to	constant clone	0.250000
remove subtensor/advancedsubtensor1 if it takes the full input	useless subtensor node	0.200000
of arguments to pass to helper_c_code	inc subtensor get helper c code args	0.250000
false we can't change the value after the	default	0.030303
the contents of a cache directory	from dir dirname err files	0.166667
computes the output dimensions of convolving an image	nnet conv op get output	0.047619
wasn't in the view_map	bad view	0.027027
"lifts" dimshuffle through elemwise operations and merges	dimshuffle lift	0.250000
apply_node recursively search	apply_node	0.050000
set of ops contained within the	gof ops	0.083333
the constant scalar 0-d value underlying	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
of	make	0.017857
this op could be very easy if it	op	0.009174
bartlett spectral window in the time-domain	bartlett m	0.083333
parses a config string (comma-separated key=value components) into	config string config_string issue_warnings	0.166667
bwd	bwd	1.000000
within the op	get op params	0.100000
the output after pad_dims	gpuarray unpad dims output input	0.333333
the idx_list with constant inputs replaced by	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
of moved objects in six moves urllib_error	six moves urllib	0.090909
scalar variable	tensor scalar	0.333333
variable	tensor tensor py operators dimshuffle	0.019231
can replace that with the *args directly	local csm properties csm node	0.142857
random	streams base random	0.500000
the name the object should be saved under	misc persistent ndarray id resolve name obj	0.500000
standard elements of an op or type used	object	0.083333
a mini-batch of a	input_shape filter_shape	0.027778
a symbolic row variable	tensor row name dtype	0.050000
an	bad	0.013158
value in array of ints	weights minlength assert_nonneg	0.125000
some functiongraph listeners to help the navigator deal	gof navigator	0.038462
dimensions of this variable optionally inserting broadcasted	dimshuffle	0.014493
required return the c implementation of an op	clinker op c code node name inputs outputs	0.500000
function to roll tensortypes	tensor roll x shift	0.250000
that can be specialized for	gof	0.004762
variables that	gof clinker type	0.066667
see theano tensor min	tensor tensor py operators min axis	1.000000
matrix solve operation c	solve	0.032258
return a copy of the type	type	0.011905
this function tries to recognize the updates ordereddict	scan_module get updates	0.034483
where elements should be inserted to maintain order	tensor searchsorted x v side sorter	0.142857
this generates the c code for corrmm	tensor nnet base corr mm c code	0.090909
to the diagonal of an empty matrix it	alloc diag	0.027027
anymore and should be removed and	compress outs	0.076923
in a new	new inputs	0.166667
merge 2	optimizer merge	0.200000
this function builds the 2d kernel that	kernel 2d	0.050000
and the output specs	fgraph input_specs output_specs accept_inplace	0.142857
wait on a previously received	wait	0.022727
label of apply node	d3viz apply label	0.500000
transform it into a canonical form that	tensor get canonical form	0.045455
matrix solve operation c = a \	tensor solve	0.038462
reshapes the output	dims output	0.333333
the inner	inner	0.041667
is needed as some features introduce instance methods	gof function graph getstate	0.500000
if implemented	inputs node	0.100000
linalg	linalg	1.000000
return none or a tensorvariable whose	tensor as	0.066667
inner-most loop	tensor make reordered loop	0.111111
the constant scalar 0-d value underlying	get scalar constant value	0.333333
checks if theano graphs represent the same computations	equal computations xs ys in_xs in_ys	0.333333
localoptgroup for graphtogpu	group	0.047619
is the first	no_recycling	0.111111
an operation to wait on a previously received	wait	0.022727
return permutations of	tensor permutation random_state size	0.500000
is the first	no_recycling profile	0.250000
setsubtensor(x x[idx], idx) -> x	tensor local setsubtensor of constants node	0.250000
feature should remove any	gof feature on	0.200000
the cause of the exception	bad optimization str diagnostic	0.043478
we parametrize it to make it work	max_input_fct maker	0.083333
function is	constant	0.016667
helper function for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view	0.083333
it with logsoftmax x 's	local logsoftmax	0.076923
outputs	outputs mode accept_inplace	0.166667
the hack in profilemode to print the mflops	corr mm flops inp outp	0.125000
parametrize it to make it	max_input_fct maker	0.083333
full path of the dynamic lib in it	module	0.033333
apply_node if those nodes are not in	gof	0.002381
graph for convolving a mini-batch	input_shape filter_shape	0.027778
an operation to	mpisend	0.037037
optimization disabled by default that removes all asserts	tensor local remove all assert	0.055556
r operation on f wrt to wrt	core rop f wrt	0.200000
gradient w r t its inputs	conv3d grad wrt inputs output_grad filters input_shape filter_shape	0.333333
the batch size	batch	0.055556
update cache data by walking the cache directory	gof module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
of integers indicating the version	clinker object c code cache version	0.125000
loading of moved objects in	moves urllib error	0.250000
updates for matrix solve operation c =	tensor solve	0.038462
|a| (inplace on a)	tensor abs inplace a	1.000000
important note this function uses set and dictionary	process node fgraph node	0.142857
an apply_node recursively search from this node to	apply_node check reason	0.066667
a symbolic column variable (ndim=2 broadcastable=[false true])	tensor col name dtype	0.200000
reorder the dimensions of this variable optionally inserting	operators	0.017241
functiongraph and also their apply_node if those nodes	gof	0.002381
a sparse	sparse	0.019231
that are non-zero in the flattened version of	tensor flatnonzero	0.166667
j0	j0	0.833333
of this op could be very easy if	op	0.009174
the output	dims output input leftdims rightdims	0.333333
integers indicating the version	cache version	0.125000
where each row correspond to the one hot	to one hot	0.142857
turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	nnet local softmax with bias node	0.200000
the context object mapped to the	gpu array type context	0.090909
the class file into an aboslute path	cop get path cls f	0.166667
return a list of the	get	0.020833
along the given axis es	axis dtype	0.083333
x and replace it with logsoftmax x	local logsoftmax	0.076923
return full path	module name	0.062500
the source code for this	gof clinker compile cmodule location	0.038462
of headers that are needed by	clinker headers	0.047619
:param execute if true execute	execute execute verbose m n	0.250000
instances	constant	0.016667
to make itself the default python and those	to os	0.038462
cudnn batch normalization	dnn batch norm inference	1.000000
outside of scan	scan	0.017241
the dimensions of this variable optionally	tensor tensor py	0.015873
computes the outer product of two	sparse block outer	0.047619
tensor_from_scalar(scalar_from_tensor	tensor scalar	0.333333
break aliasing	wrapped_inputs wrapped_outputs	0.166667
an input	bad	0.013158
from the loaded cache or	cache get	0.250000
each shape that broadcast them	generate broadcasting	0.066667
cu	cu	1.000000
of x	x	0.052632
the original graph to a new node a	outputs copy_inputs_and_orphans memo	0.029412
this variable optionally	py	0.014286
retrive the context associated with a	gpuarray get context	0.111111
remove two kinds of useless reshape	tensor local useless reshape	0.200000
for navigatoroptimizer	nav repl_pairs local_opt	0.500000
randomstate	randomstate	1.000000
for scalar values default int64	scalar shared	0.083333
lib directories that are needed by	lib dirs	0.045455
reorder the dimensions of	py operators dimshuffle	0.019231
variable	tensor py	0.015873
the fgraph outputs that will	fgraph expanded_inputs	0.058824
-x pattern	is neg	0.166667
half of v by a with	v	0.011111
method that calls	compile monitor mode eval i	0.500000
dict op -> total	compile profile stats compute total	1.000000
a convolution	conv get	0.100000
signature object for comparing	tensor constant signature	0.100000
with a particular stream	tensor random streams setitem item val	0.142857
user is not attempting to use dnn	core safe no dnn	0.125000
set and dictionary data structures	non seq	0.111111
canonical form	tensor get canonical form	0.045455
r	r	0.371429
raise	check inputs node storage_map r_vals	0.166667
arctangent of	tensor arctan	1.000000
and reduce pattern has functioning	gpu careduce cuda supports	0.166667
a list of localoptimizer and applies them	local opt group	0.052632
kinds of	type	0.011905
diagonalsubtensor and	get diagonal subtensor view	0.083333
transfer to a tensortype if not already one	transfer	0.058824
dictionary data structures	out non seq	0.125000
important note this function uses set	push out seq scan process node fgraph	0.142857
in y	y	0.026316
be turned into macros for use within	cop get	0.033333
sinus	sin	0.142857
and return full	gof module name from	0.076923
cudnn	dnn	0.121212
computes the outer	outer	0.083333
pushing out the variables inside	out	0.018519
inner graph to ensure that it	inner graph	0.035714
subprocess	subprocess	1.000000
the navigator deal	navigator optimizer attach updater	0.038462
stream	random streams getitem	1.000000
fourier transform of a real-valued input on the	curfft inp norm	0.066667
and exit code in a tuple	subprocess popen command	0.083333
over given axis	argmin x axis	0.500000
inserting broadcasted	tensor tensor py operators dimshuffle	0.019231
scale each	scale x s	0.500000
convert addsd to faster addsd_ccode	addsd	0.111111
in august 2011	load shared variable val	0.142857
raise	inputs	0.012658
on the inner graph to ensure that it	scan validate inner graph	0.035714
return the abs and rel error	abs rel	0.166667
canonical form that respects	canonical form	0.045455
input a 4-d tensor it sets	2d same size input patch_size	0.166667
change all sigmoid to	sigmoid	0.055556
use with helper_c_code	get helper c code	0.142857
not the other implementation of mod	mod c code	0.125000
getitem	getitem	1.000000
of this variable	tensor tensor	0.028571
op for	gpu	0.011765
implementation of theano gof graph	graph	0.016393
is a mrg stream state	sandbox mrg random streams	0.033333
return permutations of	tensor permutation random_state size n ndim	0.500000
for efficiently calculating the dot product	true dot x	0.166667
neg	neg	0.416667
values of a shared	compile shared	0.166667
of this op could	op	0.009174
variable with a sparse matrix	sparse variable	0.250000
function for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view x	0.083333
version	code cache version	0.125000
stream	random streams	0.235294
return the platform-dependent gcc	get gcc	0.333333
without replacement a can be a 1-d array	choice random_state size a	1.000000
the specified pieces of vectors	sparse block outer make node o x y	0.066667
raise baddestroymap if necessary update dr_vals	compile check inputs node storage_map r_vals dr_vals	0.250000
the replacement if	replace	0.032258
theslice	theslice	1.000000
return indices over each shape that broadcast them	tensor generate broadcasting indices	1.000000
this convert allocempty to alloc of	tensor local alloc empty to zeros node	0.333333
[advanced]incsubtensor[1], whose increment	tensor local useless inc subtensor	0.333333
specified pieces of vectors and matrices	sparse block gemv make node o w	0.066667
see theano tensor argsort	tensor py operators argsort axis kind	1.000000
the output dimensions of convolving an image	op get output	0.047619
shape tuple	feature default infer shape	0.066667
the current op and reduce pattern has functioning	gpu careduce cuda supports	0.166667
pattern has functioning c code	gpuarray gpu careduce cuda supports c code inputs	0.250000
elemwise sinus of	sparse sin	1.000000
computes the inverse of a matrix	matrix inverse	0.111111
one which computes the specified outputs	fgraph	0.012195
filters with a	tensor nnet conv3d signals filters	0.111111
that operates	gof	0.002381
tensor_from_scalar(scalar_from_tensor x -> x	local tensor scalar	1.000000
crossentropysoftmax1hotwithbiasdx op whose incoming gradient is	useless crossentropy softmax 1hot with bias dx	0.111111
print the mflops	tensor nnet base abstract conv flops inp outp	0.125000
of specific ops	trace f_or_fgraph ops_to_check bug_print	0.035714
function for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x i0 i1	0.083333
compiles the source code for this	gof clinker compile cmodule location	0.038462
with the *args directly	csm properties csm node	0.142857
determine the broadcast pattern for advancedsubtensor output	tensor adv index broadcastable pattern	0.066667
graph of apply	sort apply	0.200000
usage inplaceelemwiseoptimizer op optimize	tensor inplace elemwise optimizer apply	1.000000
function for diagonalsubtensor	nnet get diagonal subtensor view x	0.083333
directories that are needed	dirs	0.071429
pieces of vectors and	sparse block gemv make node o w h	0.066667
stabilization optimization	crossentropy to crossentropy with softmax with bias fgraph	0.333333
the inputs and outputs	init inputs outputs	0.166667
to add the tensor operators to	tensor	0.006431
formatter	formatter	1.000000
and the rest	rest	0.076923
text to a gist and return the url	gist content description filename auth	0.250000
c code for corrmm	nnet base corr mm c code	0.090909
return a version of var	var	0.035714
hot encoding of each element in y	hot y nb_class dtype	1.000000
functiongraph listeners to help the navigator deal with	gof navigator	0.038462
function to draw random numbers using numpy's	random_state a replace p	1.000000
symbolic graph for convolving a mini-batch of	input_shape filter_shape	0.027778
task that is associated to	gof cthunk find task failure_code	0.083333
the inner graph	inner graph	0.035714
true for small or builtin c types	clinker type c is simple	0.250000
make a nested loop	make loop	0.200000
cosh	cosh	0.714286
represent the dependence of nodes	gof make dependence	0.043478
register r's shape in	tensor shape	0.058824
returns the connection pattern of a subgraph defined	gof io connection pattern	0.055556
compile_args	compile_args	1.000000
exception object with debug info	raise with	0.333333
see theano tensor	py operators	0.171875
this graph	gof function graph import	0.125000
base e logarithm of a	tensor log a	0.500000
memory alias that wasn't in the view_map	bad	0.013158
the c code for corr3dmm (direction="forward"), corr3dmm_gradweights	base corr3d mm c code	0.090909
each value in array of ints	x weights minlength assert_nonneg	0.125000
use complex numbers	tensor mod check x y	0.166667
order a graph of apply nodes according	sort apply nodes inputs outputs cmps	0.050000
used to determine the broadcast pattern	tensor adv index broadcastable pattern	0.066667
dimensions of this variable optionally	tensor py	0.015873
the epoch of the last access of a	last access time path	0.040000
an alloc and only adds dimension to the	tensor local alloc	0.111111
that unroll the	nnet gen conv code unroll	0.250000
returning the output error and exit code	output subprocess popen command	0.100000
removes useless	tensor local useless	0.111111
loading of moved objects in six moves	six moves urllib	0.181818
a function	compile function	0.250000
connection	op from graph connection	0.500000
:attr context_name	array	0.041667
a series of wrapper functions instead of	many linkers wrappers	0.047619
constant scalar 0-d value underlying	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
safely compute ceil(float_division a	tensor ceil intdiv a	1.000000
this op	gof clinker object c code	0.500000
adds new optimization instances to a	register	0.100000
proxy for either true_div	tensor div proxy	0.125000
reorder the	py operators	0.015625
try to compile	compiler try	0.250000
set	compile op from graph set	1.000000
constant when computing gradients	core zero grad	1.000000
similar behaviour as python's	fn sequences non_sequences	1.000000
baddestroymap if necessary update dr_vals	compile check inputs node storage_map r_vals dr_vals	0.250000
the equivalent of	graph	0.016393
diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x	0.083333
replacement if the ops in the	replace validate replace	0.050000
x to be between min	x min	0.500000
that	gof wrap linker	0.083333
the specified outputs inplace	inplace fgraph	0.142857
c_code does not support the types involved	inc subtensor do type checking	0.142857
removes all asserts from	local remove all assert	0.055556
op scale or inverse the gradient	grad scale	0.333333
how to generate c	c	0.071429
baddestroymap	inputs node storage_map r_vals	0.166667
is a mrg	sandbox mrg	0.125000
a tuple of integers indicating the version	c code cache version apply	0.125000
this function tries to	image_shape top_shape border_mode	0.166667
exception value has a __thunk_trace__	thunk hook type value	1.000000
code when doing constant folding of	python constant folding	0.142857
that will be instantiated	gof clinker	0.033333
respects the conventions imposed by python	theslice length	0.052632
calculating the dot product	dot csr	0.111111
retrieve item from the cache	gof call cache	0.200000
the grad of this op could be	l op	0.033333
compute the dot product of the	tensor nnet	0.035088
node	gof pattern sub transform node	1.000000
gradient [1]_ for the cholesky factorization of	tensor cholesky grad	1.000000
the dimensionality of the var is equal	is flat var	0.200000
new_leaves	new_leaves	1.000000
hack in profilemode to print the mflops	corr3d mm flops inp outp	0.125000
argmax over a given	argmax	0.066667
version used is the	gof gcc	0.027778
enabled change all sigmoid to ultra_fast_sigmoid	ultra fast sigmoid node	0.200000
an operation to wait on a previously received	mpirecv wait	0.045455
det x and	linalg local det	0.166667
reorder the	dimshuffle	0.014493
the inner-most loop executes code	loop	0.027778
memo a dict that	gof function	0.043478
perform the permutation	perform node x	0.166667
implements the "reverse-mode" gradient for	grad perform node inputs outputs	0.083333
to help the navigator deal with	gof navigator optimizer attach	0.038462
reduces scan	scan	0.017241
unfortunately conda offers to make itself the	to os	0.038462
the c code for gpucorrmm	gpu corr mm c code	0.090909
[true]	tensor true	1.000000
fast fourier transform of a real-valued input	rfft inp norm	0.142857
the tree and figure out which nodes it	scan_module traverse out x x_copy d	0.047619
to convert x into a variable on the	gpuarray as gpuarray variable x context_name	0.166667
same type	typed_list typed list type	0.333333
the subgraph contained between i and o	gof clone i o copy_inputs	0.333333
a uniform distribution between low and high	uniform random_state size low high	0.333333
output	output input leftdims	0.333333
subtensor is inside a	subtensor node	0.066667
load a	misc load	0.250000
performs batch normalization of the given	dnn batch normalization	0.125000
r operation on f	rop f	0.166667
not the other implementation of mod	mod c code node name inputs outputs	0.125000
some text to a gist and return the	gist	0.040000
a memory alias that wasn't in the view_map	bad view	0.027027
the dimensions	py operators dimshuffle	0.019231
replacement if the ops in the list	replace	0.032258
not the other implementation of mod	scalar mod c code node name	0.125000
or the other arguments	ndim bcast ndim	0.333333
the gradient function should return	tensor matrix inverse r op inputs	0.500000
batch size loop	batch	0.055556
create a six moves urllib namespace	module six	0.043478
variable on	gpuarray as gpuarray variable	0.166667
gives unique names to an iterable of	names	0.047619
cdata	cdata	1.000000
from polar	from polar	0.250000
to the task	cthunk find task	0.142857
failure code to the task	find task	0.142857
sigmoid to	sigmoid node	0.100000
subtensor	subtensor	0.647059
this apply instance	gof apply clone with	0.166667
and reduce pattern has functioning c code	gpu careduce cuda supports c code inputs	0.250000
respect to wrt,	subgraph	0.047619
for gpucorrmm (direction="forward"),	gpuarray base gpu corr mm	0.333333
convert addsd	local addsd ccode	0.250000
the output of neural-net classifiers	with bias	0.166667
and return full path of	gof module name from	0.076923
a graph of apply nodes according	sort apply nodes inputs outputs cmps	0.050000
hyperbolic arc sine	arcsinh	0.142857
dimensions of this	tensor py	0.015873
returns upper bound	bound	0.043478
functiongraph and also their apply_node if	gof	0.002381
return connection pattern of subfgraph defined by	graph connection pattern	0.076923
bug in the default blas in macos	macos sdot bug	0.500000
helper function for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view x i0	0.083333
dimensions of this variable optionally inserting broadcasted	operators	0.017241
a set of 2d filters	tensor nnet conv2d input filters	0.125000
customized assert with options to ignore the assert	debug assert condition msg	1.000000
uses the topooptimizer from the	in2out	0.043478
consider	consider	1.000000
the given graph contains a cycle parameters	contains cycle fgraph	0.333333
broadcast pattern for advancedsubtensor output variable	pattern	0.028571
directories that are needed by one	dirs	0.071429
a reshaped view/copy of this	tensor py operators reshape shape ndim	0.111111
hint that the	sandbox linalg psd	0.500000
in the original graph to a new	inputs outputs copy_inputs_and_orphans memo	0.029412
global optimizer for pushing out the variables	out	0.018519
n_outputs	n_outputs	1.000000
inplace versions of	sparse local inplace	1.000000
shared variable to	shared variable	0.071429
dimensions of this variable optionally	tensor tensor py operators	0.015625
op	clinker op c code	0.333333
variable with the -x pattern	nnet is neg	0.166667
sum	sum x	0.333333
search through	gof stack search	0.333333
broadcasted	tensor tensor py operators	0.015625
nit_sot output of scan return true iff the	scan	0.017241
op twice gives inconsistent outputs	bad thunk output	0.200000
diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor	0.083333
raised by get_scalar_constant_value if called on	error	0.025000
print a warning	deprecated filename	0.166667
add the	tensor	0.006431
with a set of 3d	conv3d	0.076923
structured addition of a	structured add	0.142857
return the complex conjugate of z	tensor conj z	0.333333
the thunk	thunk	0.021277
all of theano's operations compilation optimization execution	profile stats	0.250000
list of lib directories that are needed	clinker header dirs	0.055556
compare true iff other is	type eq other	0.250000
can be considered exactly	pure type values	1.000000
for the new graphtogpu optimizer	gpuarray register opt2 tracks	0.250000
random	random streams base random	0.500000
the hack in profiling to print the mflops	nnet conv op flops	0.125000
batch normalization	tensor nnet batch normalization	0.125000
bound by the inputs and	inputs	0.012658
of this variable optionally inserting broadcasted	tensor py operators	0.015625
of this	operators	0.017241
register a transfer function for	register transfer fn	0.250000
to determine the broadcast pattern for advancedsubtensor output	tensor adv index broadcastable pattern a idx	0.066667
that unroll	tensor nnet gen conv code unroll	0.250000
baddestroymap if necessary update dr_vals	node storage_map r_vals dr_vals	0.250000
convert x into a variable on	gpuarray as gpuarray variable x context_name	0.166667
is an input vector and t	node input_storage	0.038462
change the value after the import of	init default filter	0.040000
md5 hash of a file	hash from file file_path	1.000000
this compiles the source code for this	cmodule location	0.038462
sample from a uniform	tensor uniform random_state	0.125000
type's	gpu	0.011765
arguments to pass to helper_c_code	inc subtensor get helper c code args	0.250000
rec	rec	1.000000
this op could	prod l op	0.033333
tell rebroadcast how to generate c code for	register rebroadcast c code typ code version	1.000000
the replacement	replace validate replace all	0.050000
the broadcast pattern for advancedsubtensor output variable	pattern a idx	0.066667
a series of wrapper functions instead	many linkers wrappers	0.047619
string specific to the apply to be	apply node	0.031250
a symbolic row	tensor row name dtype	0.050000
links all	merge new_best	0.142857
runs a series	wrap linker many linkers wrappers	0.047619
all variables which may share the same	infer reuse pattern	0.100000
a that	gof	0.002381
already attached to some	on attach	1.000000
macros for use within the op code	cop get op params	0.200000
the updates ordereddict the list of outputs and	updates and outputs	0.333333
fill s v -> alloc(v shape s this	local fill to	0.250000
in the list remove	remove	0.035714
convert radian a to	tensor rad2deg a	0.333333
the args are packed like this n_steps	scan execute node args outs	1.000000
a |	tensor or	0.333333
0 / x -> 0	tensor local zero div	1.000000
if	r_vals	0.090909
moved objects in	moves urllib	0.076923
for convop that	kern d unroll_bsize unroll_ksize	0.166667
by indices out_idxs and	out_idxs	0.050000
converts self _rop_op from user supplied form	op from graph recompute rop	0.200000
of	tensor tensor py operators	0.015625
trustme	trustme	1.000000
x to a	x	0.008772
of the specified pieces of vectors and	sparse block gemv make	0.066667
overwrite the full inputs	useless inc subtensor	0.125000
element-wise exponential linear activation	elu x alpha	0.500000
of this variable optionally	py	0.014286
should return an iterable	gpuarray gpu kernel base gpu kernels node name	0.166667
list remove are still in the	fgraph replacements remove reason	0.055556
symbolic	ndim dtype	0.333333
a transfer function	transfer fn	0.125000
and applies them to the	local	0.014085
diagnosis if things go awry	function graph check integrity	0.250000
that has ever been	gof	0.002381
looks at all outputs defined by indices out_idxs	out_idxs	0.050000
inner graph to ensure that it is coherent	validate inner graph	0.035714
sharedvariable constructor	value name strict allow_downcast	0.500000
none for	i_shapes	0.050000
generates the c code for corrmm (direction="forward"), corrmm_gradweights	corr mm c code	0.090909
of the last access of a given file	last access time	0.040000
return the data field	csm data csm	0.333333
required anymore and should be removed and	scan_module compress outs	0.076923
that copies a vector to the diagonal	alloc diag	0.027027
_maker which handles much of the debugging	maker i o m	0.066667
true if a	a	0.008065
view_map	bad view	0.027027
the biggest error between g_pt and self gf	core numeric grad max err g_pt abs_tol rel_tol	0.500000
the other implementation of mod	mod	0.071429
c_cleanup that decrefs py_name	gof get c cleanup r name	0.250000
the "reverse-mode" gradient for the eigensystem of	tensor eigh grad perform node	0.333333
|a| tensorvariable overloads the	abs a	0.333333
convert x into a variable	variable x context_name	0.250000
assert that x and y	x y	0.024390
between i and o nodes via dfs	i o	0.041667
and t is	node input_storage output_storage	0.038462
with respect to wrt,	subgraph grad	0.062500
hack in profilemode to print the mflops	corr mm flops inp outp	0.125000
this is the	graph	0.016393
protocol	protocol	1.000000
input a 4-d tensor it sets	input patch_size	0.166667
the image shape of convolution gradinputs	get conv gradinputs shape 1axis kernel_shape	0.500000
to choose from	choose	0.111111
a comparator to	cmp	0.058824
corresponding keys	walk d1 d2 u	0.333333
fast fourier transform of a real-valued input on	gpuarray curfft inp norm	0.066667
v	set v	0.125000
round half to	round half to	0.166667
optionally inserting broadcasted dimensions	tensor py	0.015873
cache and	cache	0.034483
and only adds dimension to the left remove	tensor local	0.025641
subtensor(setsubtensor	subtensor inc subtensor	1.000000
will probably not activate envs	compat maybe add	0.200000
broadcasted dimensions	tensor tensor py operators	0.015625
x and there is already an l=cholesky x	linalg local	0.142857
base 10 logarithm	log10 a	1.000000
functiongraph that has ever been	gof	0.002381
convert	make	0.017857
for scalar values	scalar shared	0.083333
ceil of	tensor ceil	1.000000
variable representing a computation on a certain gpu	gpu array variable	1.000000
variable r	r	0.028571
of multiplications and/or divisions	local greedy distributor node	0.166667
return a	gpu	0.011765
a previously	mpisend	0.037037
dot product dot(x, y t) = z	sampling dot csr	0.500000
important note this function	push out seq scan process node	0.142857
and argmax over a given axis	and argmax	0.166667
and return full path	gof module name	0.076923
new variable instance of type self	pure type make variable	0.333333
of mod	scalar mod c code node	0.125000
gpu convolution using cudnn from nvidia	gpuarray dnn conv img kerns border_mode	1.000000
the output shape of	output shape	0.250000
l{codeblock} instances returns a string that	code gen blocks	0.050000
raise baddestroymap if	compile	0.076923
reorder the	tensor	0.006431
as replace_all_validate revert the replacement if the ops	replace all validate remove	0.111111
wrapper for numpy sort function	sort op	0.250000
headers that are	headers	0.038462
gemv	gemv	0.600000
customized assert with options to ignore the assert	gof debug assert condition msg	1.000000
wrt,	subgraph grad wrt	0.062500
replace_all_validate revert the replacement if	validate replace all validate	0.111111
helper function to generate permutations	permutation helper random_state n shape	0.333333
vector and t is	node input_storage output_storage	0.038462
config	parse config	0.333333
reorder	tensor tensor py	0.015873
allocate uninitialized memory	alloc empty	1.000000
the "reverse-mode" gradient	grad perform node inputs outputs	0.166667
output dimensions of convolving an	op get output	0.047619
sum of non nan / inf values	sum	0.038462
new variable from x	new x	0.500000
original graph to a new node a	outputs copy_inputs_and_orphans memo	0.029412
this convert allocempty to alloc of 0	tensor local alloc empty to zeros node	0.333333
approximately	approx	0.166667
the __unify_walk__ method for one of the	gof unify walk a b u	0.037037
create a comparator to represent the	cmp	0.058824
reshapes the output	dims output input	0.333333
to help the navigator deal with the	gof navigator	0.038462
generate a diff	diff	0.071429
the c code for corrmm (direction="forward"), corrmm_gradweights	nnet base corr mm c code	0.090909
a memo a dict that	gof	0.002381
opt	opt	0.217391
over each shape that broadcast them to match	tensor generate broadcasting	0.066667
new instance of this	clone link_kwargs optimizer	0.111111
be a legal value for a variable	is valid value a	0.076923
the navigator deal with	gof navigator optimizer	0.038462
of shape	feature default infer shape	0.066667
op}(join(axis=0 a b), axis=0) -> elemwise{scalar op}	local reduce join node	0.111111
structured dot csr is like dot except that	structured dot csr	0.500000
for gpuincsubtensor	setsubtensor	0.111111
function to get the 0 based level	get	0.020833
unroll the batch size	tensor nnet gen conv code unroll batch	0.166667
:type cost scalar 0-dimensional	core hessian cost wrt consider_constant disconnected_inputs	0.500000
compute the image shape of convolution gradweights	tensor nnet get conv gradweights shape 1axis image_shape	0.500000
fill s v -> alloc(v shape s this	local fill to alloc	0.250000
unary(alloc x shp -> alloc(unary x	tensor local	0.025641
the cache	cache	0.103448
reshapes the output after pad_dims	gpuarray unpad dims output input leftdims rightdims	0.333333
reallocate	reallocate	1.000000
of x	flatten x	0.166667
set of 2d filters	filters	0.064516
to determine the broadcast pattern	adv index broadcastable pattern a	0.066667
l operation on f	lop f	0.166667
or more multinomial distributions defined by	tensor multinomial	0.037037
of variables	gof variables and	0.250000
that copies a vector to the	alloc	0.012500
to make it work for elemwise and gpuelemwise	local elemwise fusion	0.166667
a -1 and converts this to expm1 a	local expm1 node	0.066667
the struct	struct node	0.062500
of this variable	py operators dimshuffle	0.019231
a sparse matrix	sparse	0.019231
tuple of integers indicating the version	cache version apply node	0.125000
x -> sigm -x	local	0.014085
remove	local useless	0.111111
biggest error between g_pt and self gf	core numeric grad max err g_pt abs_tol rel_tol	0.500000
start gradients up to the end variables of	end start	0.166667
log 1+x	log1p	0.166667
warning message on the	deprecated filename msg	0.041667
search	gof stack search	0.333333
the context associated with a	context	0.035714
loading of moved objects in six moves	six moves urllib error	0.142857
code when doing constant folding	tensor elemwise python constant folding	0.142857
auth	auth	1.000000
shape and dtype as the	dtype	0.022727
mflops	nnet base abstract conv flops inp outp	0.125000
compare true iff other is	tensor tensor type eq other	0.250000
function for diagonalsubtensor and	diagonal subtensor view	0.083333
within the op	op params	0.100000
self _grad_op from user supplied form	compile op from graph recompute grad	0.200000
log base 2	log2	0.142857
feature should remove any dynamically-added	gof feature on detach	0.200000
setsubtensor(x x[idx], idx)	setsubtensor of constants	1.000000
to turn it into a gemm	gemm	0.066667
dictionary of arguments to pass to helper_c_code	get helper c code args	0.250000
contents	dirname err files	0.083333
type's :attr context_name	gpuarray	0.023256
single prod()	op of op	1.000000
row variable (ndim=2 broadcastable=[true	tensor row name	0.050000
replacement if the	replace validate replace all	0.050000
the dimensions of x default reverse them	transpose x	0.200000
the same shape and dtype as	dtype	0.022727
outputs from the	outputs mode	0.166667
one or more multinomial distributions defined	tensor multinomial	0.037037
and replace it with logsoftmax x	nnet local logsoftmax	0.076923
str of variable type	d3viz type to str t	0.500000
to the user what obj	obj	0.083333
canonical form	canonical form	0.045455
remove	subtensor remove	1.000000
factor takes as input a	tensor signal pool 2d input	0.090909
that runs a series of wrapper	gof wrap linker many linkers wrappers	0.071429
impossible to evaluate because of aliasing and	destroy	0.009709
an functiongraph	gof function graph init	0.333333
log(softmax x and replace it with logsoftmax x	local logsoftmax node	0.142857
return connection pattern of subfgraph	connection pattern	0.032258
see theano tensor prod	tensor tensor py operators prod axis	1.000000
to make itself the	to os environ pathlist var	0.038462
optimizer for pushing out the	push out	0.037037
and dictionary data structures	non	0.071429
contents of a cache directory	dir dirname err files	0.166667
the ignore_trees-related functionality	importer pruner chin	0.250000
be turned into macros	cop get	0.033333
idx list to get the right	get idx list	0.076923
random stream	tensor random streams	0.285714
respect to the inputs	gpu dnn	0.066667
stack trace from one or more	gof copy stack trace	0.055556
of 3d	tensor nnet conv3d input	0.125000
supplied function as	as	0.024390
apply instance in a new graph	apply clone with new	0.500000
tuple or none for the	i_shapes	0.050000
called by	function_graph	0.125000
doing a recursion over the input	permute row elements rec	0.047619
compare true iff other is the	eq other	0.166667
for convop that unroll the batch size loop	conv code unroll batch kern d unroll_bsize unroll_ksize	0.125000
the graph and get a	function graph	0.040000
a convolution with the specified	dnn conv	0.090909
the graph and get a memo	function graph	0.040000
type numpy typenum that corresponds	tensor type dtype specs	0.071429
subprocess_popen returning the output error and exit code	output subprocess popen command	0.100000
replace_all_validate revert the replacement if the ops	replace all validate remove	0.111111
argmax over a given axis or over	argmax	0.066667
computes the output shape	out shape ishape kshape border_mode subsample	0.500000
pieces of vectors	sparse block gemv make node o	0.066667
for	opt	0.043478
flag_list	flag_list	1.000000
c header for openblas threads interface	tensor openblas threads text	0.250000
a context by mapping it to a name	context name ctx	0.500000
tree and figure out which nodes it needs	scan_module traverse out x x_copy d	0.047619
for bilinear upsampling this function builds the	bilinear	0.038462
a module if	module	0.033333
dot22 computing an outer-product -> ger	local dot22 to ger	1.000000
the g++ version used is	gof	0.002381
one or more multinomial	multinomial	0.024390
return a tensorvariable of this type	type make variable name	1.000000
print the mflops	gpuarray base gpu corr3d mm flops inp outp	0.125000
calculate the n-th order discrete difference	diff x n	0.333333
inserting	operators	0.017241
override this should return	name	0.011111
an alloc of	alloc node	0.037037
that removes all asserts from	tensor local remove all assert	0.055556
remove are still	remove fgraph replacements remove reason	0.055556
the apply to be inserted in	apply	0.016667
input that wasn't	bad destroy	0.034483
and "init_code" together	struct node name sub	0.500000
replace_all_validate	all validate	0.166667
profiling to print the mflops	base abstract conv flops inp outp	0.125000
get_count	get_count	1.000000
return the function name to load data	gpuarray load w dtype	0.200000
localoptgroup for graphtogpu	opt group	0.043478
match a variable with either of	var	0.035714
cuda_only	cuda_only	1.000000
try to turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	nnet local softmax with bias	0.200000
with kernels of shape "kshp"	shape inshp kshp stride mode	0.142857
on wraplinker that	gof wrap linker	0.083333
by	by	1.000000
of localoptgroup for graphtogpu	to gpulocal opt group	0.055556
an apply_node recursively search from this node to	apply_node	0.050000
e^a (inplace on a)	exp inplace a	1.000000
disown	disown	1.000000
reproducible case for problems during theano	compile function dump filename inputs outputs mode	0.166667
that operates	gof linker	0.250000
object a that would be a	gof pure	0.033333
reorder the dimensions of this variable optionally	tensor	0.006431
i and o	i o	0.041667
standard deviation along the	std	0.058824
variable with the same shape and dtype as	dtype	0.022727
variables and apply_nodes to this graph	function graph import	0.125000
dnnraise	dnnraise	1.000000
a max	local max	0.250000
to use with	gpuarray gpu	0.045455
fgraph outputs that will replace	fgraph	0.012195
cause	bad optimization str diagnostic	0.043478
helper function drawing from	helper random_state n pvals size	1.000000
the first functiongraph that has ever been associated	gof	0.002381
numpy-compatibility method if x is a matrix	tensor diag x	0.200000
values from a with or	size a replace p	0.333333
is a mrg stream	sandbox mrg random streams	0.033333
and only adds dimension to the	local	0.014085
to determine the broadcast pattern for advancedsubtensor	adv index broadcastable pattern a idx	0.066667
previously sent array using mpi	mpisend	0.037037
-a	tensor neg	1.000000
of _maker which handles much of the	compile debug mode function maker i o m	0.066667
an apply_node recursively search from this node to	import apply_node	0.066667
outputs according to the flags use_list and use_tuple	format as use_list use_tuple outputs	1.000000
called by remove_feature feature should remove any dynamically-added	feature on detach function_graph	0.200000
bound by the inputs and outputs	inputs outputs	0.066667
variable optionally	operators	0.017241
config string	config string	0.333333
to merge multiplication by a scalar on the	gpuarray alpha merge	0.076923
the replacement if the ops	replace all	0.050000
of downsample with max	downsample factor max	0.500000
if theano graphs represent the same computations	computations xs ys in_xs in_ys	0.333333
input that wasn't in the destroy_map	bad destroy	0.034483
apply nodes in the original graph	outputs copy_inputs_and_orphans memo	0.029412
implemented returns	gof local meta optimizer	0.500000
function that	scan_module	0.125000
all unknown variables and apply_nodes to this graph	gof function graph import	0.125000
the kernel shape of convolution gradweights	conv gradweights shape	0.333333
return a symbolic 3-d variable	tensor3 name dtype	0.166667
value after the	default	0.030303
signature object	tensor constant signature	0.100000
recognize the updates ordereddict the	get updates	0.034483
distribution between low and high	low high	0.333333
the value after the import of	default	0.030303
scale each columns	col scale	1.000000
return a dictionary of arguments to	args	0.025641
a to	a	0.024194
reshapes the output after pad_dims	gpuarray unpad dims output input	0.333333
gradient	grad inputs g_outputs	0.076923
generates the c code for corrmm (direction="forward"),	corr mm c code	0.090909
the mflops	conv op flops inputs outputs	0.125000
will be turned into macros for use	cop get	0.033333
y with	y axis	0.125000
of scan	out scan	0.071429
and the output specs	input_specs output_specs accept_inplace	0.142857
module if the	module	0.033333
of the input	gpu	0.011765
ishape	ishape	1.000000
function on the inputs and put	pure op perform node inputs output_storage params	0.047619
the argmax	argmax node	0.500000
the epoch of the last access of	last access	0.040000
convolution with	dnn conv	0.090909
if the named module is a package	is package fullname	0.250000
num_input_channels	num_input_channels	1.000000
out the	push out	0.037037
a badviewmap exception when it detects the following	compile check viewmap node	0.111111
round half	rint	0.111111
allocate outputs	make alloc loop_orders dtype sub fortran	0.200000
an array with more than	node inputs	0.086957
upper triangle of	triu m k	0.250000
an optimization	node	0.007407
optionally	py operators	0.015625
a multinomial distribution defined by probabilities	sandbox mrg random streams multinomial	0.250000
a nested loop over several	loop	0.027778
source code for	cmodule location	0.038462
a module	module	0.100000
that	gof linker	0.250000
generates the c code for corrmm	corr mm c code	0.090909
if a subtensor is inside	subtensor node	0.066667
of headers that are	clinker headers	0.047619
similar behaviour as	fn sequences	1.000000
change the value after the import of	param init default filter	0.040000
compile c code when doing constant folding	constant folding	0.142857
called whenever node inputs[i] is changed from r	feature on change input function_graph node i r	1.000000
i_shapes	i_shapes	0.250000
a series of wrapper functions instead	wrap linker many linkers wrappers	0.047619
given an apply_node recursively search from	apply_node check reason	0.066667
:func images2neibs <theano tensor nnet	tensor nnet images2neibs	0.333333
le	le	0.625000
is meant as	optimizer optimize fgraph	0.200000
compute sum of	constant signature get sum	0.142857
nan	nan	1.000000
loading of moved objects in six moves urllib_request	six moves urllib request	0.333333
this variable optionally inserting broadcasted dimensions	tensor tensor	0.014286
validations on the inner	scan validate inner	0.142857
convert addsd to faster addsd_ccode	addsd ccode node	0.250000
a dictionary of arguments to pass to helper_c_code	inc subtensor get helper c code args	0.250000
wrapper around c_init that	gof	0.002381
nonzero_values	nonzero values	1.000000
an array with more than one element is	node inputs outputs	0.125000
self _rop_op from user supplied form	from graph recompute rop op	0.200000
a global optimizer for pushing out the	push out	0.037037
a != b	neq a b	1.000000
the dimensions of	py	0.014286
try to compile	try	0.111111
the equivalent of	graph to	0.055556
to determine the broadcast pattern for advancedsubtensor output	tensor adv index broadcastable pattern	0.066667
the version	code cache version	0.125000
node inputs[i] to new_r	change input node i new_r reason	0.500000
the caller is replace_all_validate just raise the exception	validator validate fgraph	0.125000
for debugmode	abstract	0.111111
x	x y	0.024390
this alias	alias alias	0.333333
normalization	norm inference	1.000000
hyperbolic cosine	cosh	0.285714
exp x or -exp x patterns	nnet is exp	0.333333
a subtensor copy using advanced indexing	advanced subtensor	0.250000
to help the navigator	navigator optimizer attach updater	0.038462
x the	tensor flatten x	0.166667
sum(a axis=[]) ->	local useless reduce node	0.500000
scaled complementary error function	erfcx a	1.000000
shp -> alloc(unary x shp)	local alloc unary node	0.250000
a new variable to	var name doc configparam	0.250000
2d or 3d convolution for	tensor nnet base abstract conv conv	0.125000
as replace_all_validate revert the replacement if the ops	gof replace validate replace all validate remove	0.111111
replace a crossentropysoftmax1hotwithbiasdx op whose incoming gradient	local useless crossentropy softmax 1hot with bias dx	0.111111
unfortunately conda offers to	to os environ pathlist	0.038462
have separated maker	share_memory swap delete_updates name	0.250000
dot22 computing an outer-product -> ger	tensor local dot22 to ger or	1.000000
proxy for	tensor div proxy	0.125000
navigator deal with the	gof navigator optimizer attach	0.038462
error	error	0.125000
an op	gof clinker op	0.333333
input that wasn't in	bad destroy	0.034483
local optimizer of	local	0.014085
0	zero borrow	0.500000
optimization makes the folowing changes	tensor local mul switch sink node	0.045455
two kinds scalar constants and the rest	rest inputs elemwise	0.125000
raise	inputs node storage_map r_vals	0.166667
graph and	function graph	0.080000
convolve spatio-temporal filters with	tensor nnet conv3d signals filters	0.111111
sample from one or more	size n	0.090909
apply the list of policies	gof apply policy policy	0.500000
the r operation on f	rop f	0.166667
get the 0 based level of	type get depth	0.050000
of _maker which handles much	function maker i o m	0.066667
graph of apply nodes according to a	apply nodes inputs outputs cmps	0.050000
v by a with	v	0.011111
type1	type1	0.857143
ignore previous function call	compile profile stats reset	1.000000
ops in the "theano config compiledir"	compiledir content	0.166667
revert the replacement if the ops in	replace validate replace	0.050000
see theano tensor sum	tensor tensor py operators sum axis dtype	1.000000
raise in	core raise init	0.100000
this class	gof	0.011905
the forward but clip the gradient	grad clip x lower_bound upper_bound	0.250000
the inverse	inverse	0.066667
that will be instantiated by	gof clinker	0.033333
attempt to replace a leaf of	nnet replace leaf arg leaves new_leaves op	0.250000
[elementwise] largest of	tensor largest	0.333333
module component with similar interface to numpy random	mrg random streams	0.033333
base class for corr3dmm, corr3dmm_gradweights and corr3dmm_gradinputs	base corr3d mm	0.250000
name in mode	name opt	0.333333
true_div or int_div depending on types of x	x	0.017544
split{n_splits=1} x y -> x	local	0.014085
to the type's	gpu array type	0.062500
the number of multiplications and/or divisions	tensor local greedy distributor node	0.166667
tensor_from_scalar(scalar_from_tensor x -> x	local tensor scalar tensor	1.000000
x*x -> sqr x this is faster	tensor local mul to sqr node	0.166667
kernel shape of convolution gradweights	conv gradweights shape image_shape	0.500000
thing	thing	1.000000
deg2rad	deg2rad	0.555556
a can be a 1-d	random_state size a	0.333333
on the	gpuarray	0.023256
access	access time	0.200000
of the last access of a	last access time path	0.040000
of this	tensor tensor py operators dimshuffle	0.019231
doing a recursion over the input dimensions	permute row elements rec	0.047619
source code for	gof clinker compile cmodule location	0.038462
set	op from graph set	1.000000
the input	input	0.023810
and its idx_list reorders the inputs	inputs idx_list get_count	0.100000
elemwise minimum see min for the minimum	minimum	0.083333
matrix a and matrix b	structured	0.071429
that this opt	gof	0.002381
sum along the	sum	0.038462
along the axis that was used to	inputs g_outputs	0.090909
perform some elementary validations on the inner graph	scan_module scan validate inner graph	0.035714
to a specified	a	0.008065
of this variable optionally inserting broadcasted dimensions	tensor py	0.015873
compute the image shape of convolution gradweights	nnet get conv gradweights shape 1axis image_shape	0.500000
given an fgraph and a list of	fgraph	0.012195
helper function to generate permutations from integers	permutation helper random_state	0.333333
a vector to the diagonal of an	alloc diag	0.027027
of	dimshuffle	0.014493
reorder the dimensions of this variable optionally	tensor tensor	0.014286
this function performs the svd on	svd a	0.200000
arguments to pass to helper_c_code	tensor inc subtensor get helper c code args	0.250000
return a code string specific to the apply	name sub	0.050000
equivalent of	graph	0.016393
no_recycling	no_recycling	0.555556
so pyd dll or py file	dlimport fullpath suffix	0.333333
in the struct	struct node	0.062500
y with length one the axes of	y	0.026316
have	name	0.011111
computes the sum along the	tensor sum	0.111111
uses shared variable	persistent shared variable	0.500000
computes the sum along the	sum	0.038462
defined by one-dimensional slices in pvals	pvals	0.071429
to insert inplace versions of remove0	sparse local inplace remove0 node	0.333333
a diagnosis if things go awry	function graph check integrity	0.250000
an input that wasn't in the destroy_map	bad destroy	0.034483
schedule	sort schedule	0.333333
returns	gof clinker make	1.000000
shape tuple	shape feature default infer shape	0.066667
to get the 0 based level of	type get depth	0.050000
function builds the 2d kernel that	kernel 2d	0.050000
type is in t float_scalar_types	scalar res dtype	0.333333
a legal	is valid	0.250000
a canonical form that	canonical form	0.045455
str of variable type	type to str t	0.500000
a dict that	gof function	0.086957
by calling the method query	graph to gpudb	0.142857
uniform into sample	uniform	0.086957
dictionary of arguments to pass to helper_c_code	inc subtensor get helper c code args	0.250000
a memory alias that wasn't in the	bad	0.013158
expm1 a	tensor local expm1	0.066667
list of header	header	0.100000
a graph is impossible to evaluate	destroy	0.009709
if it takes the full input	node	0.007407
/ dimshuffle{ } b axis=l) ->	local	0.014085
global optimizer for pushing out the variables inside	out	0.018519
module is a	importer is	0.250000
get a constant value by its alias	gof enum type fromalias alias	1.000000
compiled module from the loaded cache	module cache get module name	0.166667
memory alias that wasn't in the view_map	view	0.022727
copies a vector to the diagonal of	alloc diag	0.027027
filters with a	nnet conv3d signals filters	0.111111
profiling to print the mflops	abstract conv flops inp outp	0.125000
optimization that deals with allocempty this	allocempty op idx	0.250000
slice [start stop step] transform it	slice	0.038462
with respect to wrt, computes gradients of	core subgraph	0.062500
that would	gof	0.002381
to use dnn conv algo_bwd	dnn algo bwd algo	0.166667
self _rop_op from user supplied form	compile op from graph recompute rop	0.200000
passed-in key is found in the	get from key key key_data	0.111111
normalization of the given	normalization	0.153846
diagonal of an empty matrix it	diag	0.023810
function is basically a	extract constant x elemwise only_process_constants	0.058824
real-valued input	rfft inp norm	0.142857
return those items of	gof remove	0.250000
round_half_to_even_inplace a (inplace on a)	tensor round half to even inplace a	1.000000
elemwise minimum see min for the minimum	minimum x	0.142857
change all sigmoid to ultra_fast_sigmoid	tensor nnet local ultra fast sigmoid	0.200000
for pushing out the variables inside the	push out	0.037037
r as input makes it use new_r instead	replace all pairs reason	0.333333
graph optimizer that reduces scan memory consumption	scan save mem	0.200000
functiongraph	function graph	0.040000
is	to	0.017544
of library search paths	lib dirs	0.045455
square	sqr	0.250000
the gradient	grad	0.031250
construct a variable with a sparse matrix	sparse variable x name	0.250000
for	fast	0.166667
if the outputs of specific ops of	trace f_or_fgraph ops_to_check bug_print	0.035714
to compute the image shape of convolution gradinputs	tensor nnet get conv gradinputs shape kernel_shape	0.500000
scale or inverse the	scale	0.047619
remove split with only 1 split	useless split node	1.000000
std	std axis ddof keepdims	1.000000
openblas threads	openblas threads	0.250000
not the other implementation of mod	scalar mod c code	0.125000
perform some elementary validations on the inner	validate inner	0.142857
retrieve item from the cache	call cache	0.200000
to track less frequent op	get clients2 node	0.200000
replacement if the ops	replace	0.032258
data or an appropriately wrapped/converted data	type filter data strict allow_downcast	0.250000
return true for small or builtin c types	type c is simple	0.250000
scalars together into a vector	make vector	0.125000
is an alloc of a fully or	alloc node	0.037037
x and there	linalg local	0.142857
mean value	tensor mean	0.111111
uses set and dictionary data structures	non seq	0.111111
required to compute	variable_list blockers	0.166667
by a specified factor takes as input a	tensor signal pool 2d input	0.090909
gets a scan	not_required inputs	0.250000
complex	complex	0.750000
variable	dimshuffle	0.014493
sigmoid to ultra_fast_sigmoid	tensor nnet local ultra fast sigmoid	0.200000
remove incsubtensor when we overwrite	tensor local useless inc subtensor node	0.066667
takes as	signal max pool 2d same	1.000000
an operation to wait on a	mpisend wait	0.045455
the list of op classes that this opt	gof local optimizer tracks	0.071429
array	mpisend	0.037037
det x and there	sandbox linalg local det	0.166667
turn softmax(sum_of_stuff) -> softmax_w_bias	tensor nnet local softmax with	0.200000
return the indices field	csm indices csm	0.333333
clone the graph and	function graph clone	0.333333
sample from one or more multinomial	multinomial random_state size	0.333333
override clinkertype c_declare	tensor type c declare name sub check_input	1.000000
an appropriately wrapped/converted data	type filter data strict allow_downcast	0.250000
reorder the dimensions of	py operators	0.015625
directory and	from dir	0.125000
an input that wasn't in the destroy_map	destroy	0.009709
function to	random_state low high	0.500000
dictionary data structures	push out non seq	0.125000
used to determine the broadcast pattern for advancedsubtensor	adv index broadcastable pattern a	0.066667
compute the kernel shape of convolution gradweights	tensor nnet get conv gradweights shape	0.500000
march	march	1.000000
predicate	remove predicate	1.000000
compile c code when doing constant folding of	elemwise python constant folding	0.142857
x and there is already	local	0.014085
the dimensions of	tensor py operators dimshuffle	0.019231
and "init_code"	code struct node name sub	0.500000
important note this function uses set	scan process node	0.142857
replace_all_validate revert the replacement if the ops in	replace validate replace all validate remove	0.111111
of a real-valued input on	curfft inp norm	0.066667
explicitly upcasts constant inputs	constant inputs node	0.125000
base e logarithm	log a	1.000000
return connection pattern of subfgraph defined by	connection pattern	0.032258
output shape	output shape	0.250000
text to a gist and return	gist	0.040000
function uses set and dictionary data structures	scan_module push out non seq	0.125000
of this type	tensor tensor type	0.041667
see theano tensor sort	py operators sort axis	1.000000
variant on wraplinker that runs a	gof wrap linker	0.083333
we parametrize it to make it work for	max_input_fct maker	0.083333
with a set of 3d filters	conv3d input filters	0.142857
broadcast them to	generate broadcasting	0.066667
the idx_list with constant inputs	subtensor get constant idx inputs	0.250000
remove broadcastable dimensions from the shape of an	py operators squeeze	0.200000
c code when doing constant folding of	constant folding	0.142857
safe shorter version of platform	platform	0.083333
shape_i how	check_input	0.111111
python litterals to theano constants in subtensor arguments	make constant args	0.250000
the given axis es	axis dtype keepdims	0.083333
uses shared variable names when persisting to zip	persistent shared variable id	0.142857
to sharedvariable instances of suitable dummy values	provide inputs	0.200000
ops in the list remove are still in	fgraph replacements remove reason	0.055556
offers to make itself the default	to os environ	0.038462
transfer to	transfer	0.058824
"reverse-mode" gradient	grad perform node inputs outputs	0.166667
apply	gof sort apply	0.200000
this functiongraph and also their apply_node if	gof	0.002381
factor takes as input a n-d	tensor signal pool 2d input ws ignore_border stride	0.100000
add two matrices at least one	add x y	0.333333
with respect to wrt, computes gradients	subgraph	0.047619
the hack in profiling to print the mflops	tensor nnet conv op flops inputs outputs	0.125000
convert addsd	local addsd ccode node	0.250000
this	py operators	0.015625
an operation to wait on	wait	0.045455
of this variable optionally inserting	tensor tensor	0.014286
other scalar	scalar	0.035714
from existing start gradients up to the	start	0.040000
of a cache directory and return full	module name from dir	0.071429
dictionary that	gof	0.002381
when allow_gc = false clear the variables	compile function free	0.250000
value after the import of theano	param init default	0.040000
is sparse	sparse	0.038462
merge 2 profiles returned by this	gof seq optimizer merge	1.000000
implementation of mod	scalar mod c code node	0.125000
this variable optionally inserting	operators dimshuffle	0.019231
outputs defined by indices out_idxs and	out_idxs	0.050000
computes the standard deviation along the	tensor std	0.111111
unification u	u	0.050000
unfortunately conda offers to make	to os environ pathlist var newpath	0.038462
the shape s to previously un-shaped variable r	tensor shape feature set shape r s override	0.500000
return a tuple of integers indicating the version	code cache version apply	0.125000
decorator for creating a class with a metaclass	compat add metaclass metaclass	0.125000
this variable optionally inserting broadcasted	tensor tensor py operators dimshuffle	0.019231
help the navigator deal with	gof navigator optimizer attach updater fgraph	0.038462
bartlett spectral window in	bartlett	0.058824
-> softmax_w_bias matrix bias	tensor nnet local softmax with bias	0.200000
the subgraph in functiongraph	gof function graph replace	0.500000
updated	updated	1.000000
a convolution	dnn conv get	0.100000
l operation on f wrt to wrt	core lop f wrt	0.200000
important note this function uses	scan process node	0.142857
instance of _maker which handles much	mode function maker i o m	0.066667
iv	iv	1.000000
around c_init that	gof get	0.100000
set and dictionary data structures	push out non seq	0.125000
from a uniform into sample	uniform	0.086957
raise in	core raise	0.100000
bartlett spectral window in the	tensor bartlett m	0.083333
try to detect a	tensor detect	0.166667
in a new graph	with new inputs inputs strict	0.166667
this variable optionally inserting broadcasted	py operators dimshuffle	0.019231
the graph's apply nodes such that	gof function	0.043478
reorder the dimensions of this	operators dimshuffle	0.019231
r operation on f	f	0.052632
a triangular solve	solve triangular node	0.142857
gpu convolution using cudnn from nvidia	gpuarray dnn conv img kerns	1.000000
for the maximum in one	tensor maximum x y	0.090909
each row is a mrg	sandbox mrg	0.125000
op scale or inverse	scale	0.047619
if	check inputs node storage_map r_vals	0.166667
directories that	dirs	0.071429
return the data	data	0.090909
list to	factored list	0.500000
the tree and figure out which nodes	scan_module traverse out x x_copy d	0.047619
output dimensions	tensor nnet conv op get output	0.047619
to indicate an internal theano problem	debug mode	0.200000
the outputs	outputs mode	0.166667
it with a triangular solve	tag solve triangular	0.142857
elementwise division (inplace on a)	tensor true div inplace a b	1.000000
that would	gof pure	0.033333
profiling to print the mflops	nnet conv op flops inputs	0.125000
beta	beta	1.000000
for same kinds of tensortype	tensor tensor type	0.041667
symbolic 3-d variable	tensor tensor3 name dtype	0.166667
expects the compile lock to be held	to cache module key module_hash	0.166667
respect to	dnn	0.060606
kernel that can be used	kernel	0.133333
inputs to the subtensor and its idx_list reorders	idx_list get_count	0.090909
compute	tensor nnet	0.210526
existing start gradients up to	start	0.040000
input a 4-d	size input patch_size	0.166667
neibs2images <theano sandbox neighbours neibs2images>	neibs2images neibs neib_shape original_shape mode	0.333333
to roll tensortypes	tensor roll x shift	0.250000
a comparator to represent the	cmp	0.058824
in the theano enumeration types wrapped	params type enum from	0.333333
conda offers to make itself the default python	to os environ pathlist var	0.038462
optimizer for pushing out the variables inside the	out	0.018519
default that removes	tensor local remove	0.166667
revert the replacement if the	gof replace validate replace	0.050000
an array with more than one	node	0.014815
see theano tensor repeat	tensor py operators repeat repeats axis	1.000000
the equivalent of localoptgroup	gpulocal opt group	0.055556
subtraction	tensor sub	1.000000
replace_all_validate revert the replacement if the	replace validate replace all validate remove	0.111111
the inner graph to ensure that	validate inner graph	0.035714
reorder	tensor py operators	0.015625
eq x x -> 1	local useless elemwise node	1.000000
to choose	tensor choose	0.250000
helper function to draw random integers	random integers helper random_state low	1.000000
it with a triangular	triangular node	0.125000
of conv3d with respect	conv grad3d	0.333333
map old node	check_integrity	0.090909
det x and there is already an l=cholesky	sandbox linalg local det	0.166667
beta * y + alpha * dot a	gemv c code y a	0.333333
a list of localoptimizer and applies	local opt group	0.052632
the source code	clinker compile cmodule location	0.038462
gradient wrt inputs	grad inputs	0.777778
the replacement if the ops in the list	replace	0.032258
a symbolic row variable (ndim=2 broadcastable=[true	row name	0.050000
only used to determine the broadcast pattern	tensor adv index broadcastable pattern a	0.066667
to compile c code when doing constant folding	constant folding	0.142857
replace_all_validate	validate	0.090909
convert addsd	sparse local addsd ccode node	0.250000
will print a warning	deprecated filename	0.166667
that will call the supplied function as its	compile as	0.050000
given an apply_node recursively search from	import apply_node	0.066667
:func neibs2images	neibs2images	0.125000
the gradient	grad inputs	0.111111
is false we can't change the value after	param init default	0.040000
important note	out seq scan process node	0.142857
raise baddestroymap if necessary update dr_vals	storage_map r_vals dr_vals	0.250000
see theano tensor min	py operators min	1.000000
value after the import of theano	default	0.030303
create a comparator to represent the dependence	dependence cmp	0.111111
a dimshuffle is inside an alloc and only	tensor local alloc dimshuffle node	0.166667
temporary variables	vm clear storage	1.000000
perform some elementary validations on the inner graph	validate inner graph	0.035714
feature should remove any dynamically-added	gof feature	0.125000
connection pattern of subfgraph	compile op from graph connection pattern node	0.076923
the source code	compile cmodule location	0.038462
use a simple algorithm to find broken	compile find bad optimizations2 order reasons r_vals	0.111111
max for the maximum in	tensor maximum x	0.142857
headers	clinker headers	0.047619
model	model	1.000000
shp -> alloc(unary x	local	0.014085
a triangular solve	tag solve triangular node	0.142857
copies a vector to the diagonal of an	diag	0.023810
directory and return full path of the	module name from dir	0.071429
the end variables of a	end	0.040000
way will probably not activate envs	compat maybe add	0.200000
symbolic integer scalar for the shape element s_i	tensor shape feature unpack s_i	1.000000
see theano tensor max	py operators max	1.000000
doing a recursion over the	tensor permute row elements rec	0.047619
other implementation of mod	mod c code node name inputs outputs	0.125000
the platform-dependent	get	0.020833
and apply_nodes to this graph	gof function graph import	0.125000
makes the folowing changes in	local mul switch sink	0.045455
the c code for gpucorrmm	base gpu corr mm c code	0.090909
return true if and only if this enum	gof enum type	0.166667
optionally inserting broadcasted	operators dimshuffle	0.019231
[advanced]incsubtensor[1], whose increment is an alloc of a	local useless inc subtensor alloc node	0.166667
to wrt, computes gradients of	subgraph grad	0.062500
to print the mflops	gpu corr3d mm flops inp outp	0.125000
attempting to use dnn	core safe no dnn	0.125000
each value in array of ints	weights minlength assert_nonneg	0.125000
in the fgraph to	fgraph	0.012195
dimensions	py	0.028571
the context object mapped to the type's	type context	0.090909
mflops	flops	0.076923
for the new graphtogpu optimizer	register opt2 tracks	0.250000
connection pattern of subfgraph defined by inputs and	compile op from graph connection pattern node	0.076923
operators to the basic constant class	tensor constant	0.055556
execute callbacks calls getattr feature name (*args) for	graph execute callbacks name	0.500000
function as	as	0.024390
replace a crossentropysoftmax1hotwithbiasdx op whose incoming	local useless crossentropy softmax 1hot with bias dx	0.111111
to the input specs and the output specs	fgraph input_specs output_specs accept_inplace	0.142857
specified pieces of vectors and	sparse block outer make	0.066667
initialization code	init code	0.142857
grad of downsample with max	downsample factor max grad grad	0.333333
mrg stream state and	mrg random streams	0.033333
to helper_c_code	helper c code	0.142857
the version	cache version apply	0.125000
alias that	bad	0.013158
prior reduction of x	x	0.008772
special compound l{op} for the output	softmax argmax1hot	0.083333
mapping all symbolic variables in inputs to sharedvariable	inputs	0.012658
the c code for gpucorrmm (direction="forward"),	gpuarray base gpu corr mm c code	0.090909
stride_x	stride_x	1.000000
gpu	gpu	0.200000
l operation on f wrt to wrt	f wrt	0.100000
raise baddestroymap if	node storage_map	0.166667
to the type's	gpuarray gpu	0.045455
return full path of the dynamic	module name from	0.076923
x x ->	local	0.014085
maximum see max for the maximum in	tensor maximum	0.142857
value after the import	param init default	0.040000
if	compile check inputs node	0.166667
implement the grad	grad grad	0.166667
return the inputs	gof inputs	0.333333
a diff to make code correctly indented	misc hooks get correct indentation diff code filename	0.333333
scalars together	make	0.017857
inserted in the struct	struct node	0.062500
square of	tensor sqr	1.000000
the navigator deal with the	navigator optimizer attach	0.038462
a symbolic constant with value x	constant x name ndim dtype	0.333333
replacement if the ops in the list	replace validate replace all	0.050000
to compile c code when doing constant folding	elemwise python constant folding	0.142857
implements the "reverse-mode" gradient for the eigensystem of	tensor eigh grad perform	0.333333
replacement if the ops	replace validate replace all	0.050000
return permutations of	tensor permutation random_state	0.500000
irfft	irfft	1.000000
g_pt g_pt	g_pt	0.166667
convert x into a variable on the gpu	gpuarray variable x context_name	0.166667
to_var	to_var	1.000000
optimizer which can be referred to by	compile register optimizer	0.500000
for	to gpulocal opt	0.055556
retrive the context associated with a name	get context name	0.333333
graph of apply nodes according to a list	sort apply nodes inputs outputs cmps	0.050000
that	bad destroy	0.034483
unification in u and uses it	o u	0.037037
debugprint	debugprint	1.000000
for scalar values default int64 or float64	scalar	0.017857
that initializes py_name from storage	gof get c extract r	0.250000
op and reduce pattern has functioning c	gpuarray gpu careduce cuda supports c	0.200000
with a	with	0.076923
l operation on f	core lop f	0.166667
apply_node recursively search from this	import apply_node	0.066667
true	true	0.833333
numpy ndarray value	tensor	0.003215
removes all from the	gof function graph remove client	0.200000
opt to track less frequent op	get clients2 node	0.200000
a matrix :math a using magma library	gpu magma matrix	0.333333
compute a batched tensordot product	tensor batched tensordot x y axes	0.333333
same op twice gives inconsistent outputs	bad thunk	0.200000
in the	destroy	0.009709
the indices field	csm indices csm	0.333333
x is a matrix return	x	0.008772
return a string	name x	0.333333
create pydot node	to pdnode d	0.333333
a copy of the type	type	0.011905
product of the specified pieces of vectors and	sparse block outer make node	0.066667
a convolution with the specified	conv get	0.100000
a new variable to	var name doc configparam root	0.250000
a vector	alloc	0.012500
node that uses r as	gof function graph	0.031250
replace_all_validate revert the replacement if the ops	replace validate replace all validate remove fgraph	0.111111
on the inputs and put the	pure op perform node inputs output_storage params	0.047619
repl_pairs	repl_pairs	1.000000
the connection pattern of a	io connection pattern	0.055556
convert addsd to faster addsd_ccode	sparse local addsd ccode node	0.250000
updates for matrix solve	solve	0.032258
a dictionary of arguments to pass to helper_c_code	tensor inc subtensor get helper c code args	0.250000
the list of outputs and the	and outputs	0.100000
to print the mflops	gpuarray base gpu corr mm flops inp outp	0.125000
of the exp x or -exp x patterns	nnet is exp	0.333333
a new	new inputs inputs strict	0.166667
computes the output dimensions of	output	0.017241
on wraplinker that	gof wrap	0.083333
dimensions from the shape or the other arguments	ndim bcast ndim shape	0.250000
diagonalsubtensor	tensor nnet get diagonal subtensor view	0.083333
copy_inputs	copy_inputs	0.625000
hint that the variable v is	sandbox linalg psd v	0.250000
function to roll	roll	0.111111
return true if the named module	fullname	0.066667
of apply nodes according to a	apply nodes inputs outputs cmps	0.050000
where each row correspond to the one	to one	0.125000
method is primarily used by	pure op r op inputs eval_points	0.125000
another op that takes the same inputs as	op sub	0.066667
modifies	gof give	0.500000
for graphtogpu	to gpulocal opt	0.055556
performs batch	tensor nnet batch	0.500000
module from the	module from	0.166667
for all of theano's operations compilation optimization execution	profile stats	0.250000
that was dumped to a	f persistent_load	0.052632
3d inputs with a set of 3d	conv3d input	0.125000
signature for this function	gof ext function method decl	0.333333
the replacement if the ops	replace validate replace all	0.050000
special compound	crossentropy softmax argmax1hot	0.083333
in a new	clone with new inputs	0.166667
diagonal of an empty	alloc diag	0.027027
to previously un-shaped variable	override	0.142857
upsample	ratio normalize	0.200000
should remove any dynamically added functionality	gof replace validate on detach fgraph	1.000000
:type expression vector 1-dimensional variable	jacobian expression wrt consider_constant disconnected_inputs	0.500000
to print the mflops	op flops inputs outputs	0.125000
r-operator for	max pool rop	0.142857
current paramstype contains the specified theano type	type theano_type	0.500000
another op that takes	op sub	0.066667
input	same size input	0.500000
fgraph and a list of variables returns	fgraph	0.012195
clone the graph and get a dict that	gof function graph clone get	1.000000
this is the	graph to gpulocal	0.055556
to wait on a previously	wait	0.045455
kernel for bilinear upsampling	bilinear	0.038462
ldflags	tensor ldflags	0.250000
variables that	gof	0.002381
computes the output dimensions of	tensor nnet conv op get output	0.047619
generates the c code for gpucorrmm	base gpu corr mm c code	0.090909
output_types_preference	output_types_preference	1.000000
an input that wasn't in the	bad destroy	0.034483
the topooptimizer from the input nodes to	gof in2out	0.055556
makes the folowing changes in the graph t	local mul switch sink	0.045455
copy_inputs_and_orphans	copy_inputs_and_orphans	1.000000
sample from a normal distribution	base normal size	1.000000
takes as input a n-d tensor	signal pool 2d input ws ignore_border stride	0.100000
a graph is impossible to evaluate because	destroy	0.009709
exception	destroy	0.009709
implements the "reverse-mode" gradient for the eigensystem	eigh grad perform	0.333333
connection pattern of	gof io connection pattern	0.055556
represent the dependence of nodes in a	make dependence	0.043478
reproducible case for problems during theano compilation	compile function dump filename inputs outputs	0.166667
to make	to os environ	0.038462
were declared by	init	0.058824
function for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x i0 i1	0.083333
comparator to represent the dependence of nodes in	gof make dependence cmp	0.111111
dot product followed by a modulo	dot modulo	0.250000
a graph of apply nodes according	apply nodes inputs outputs cmps	0.050000
to	gpuarray gpu inc	0.333333
inputs and put the variables in the	pure op perform node inputs output_storage params	0.047619
is the equivalent of localoptgroup for	to gpulocal opt group	0.055556
compute the dot product of the specified	nnet	0.032258
code for our fgraph	clinker get dynamic module	0.200000
class file into an aboslute path	cop get path cls f	0.166667
return a list of shape	feature default infer shape	0.066667
profiling to print the mflops	nnet base abstract conv flops inp outp	0.125000
'requirement' of the destroyhandler	destroy handler	0.055556
as python not the other implementation of mod	scalar mod	0.125000
construct a variable with a sparse matrix	sparse variable x	0.250000
return a c contiguous version of the	gpu contiguous	0.083333
apply instance	apply clone with	0.333333
conda offers to make itself	to os environ pathlist	0.038462
loading of moved objects in six moves urllib_robotparser	six moves urllib robotparser	0.333333
subgraph bound by the inputs	inputs	0.012658
and replace it with logsoftmax	logsoftmax node	0.125000
with constant inputs replaced	constant idx inputs allow_partial only_process_constants elemwise	0.071429
types involved	inc subtensor do type checking	0.142857
the cache and none	cache	0.034483
x and replace it with logsoftmax x	tensor nnet local logsoftmax node	0.142857
alias that wasn't	view	0.022727
make a schedule	gof sort schedule	0.333333
the list of op classes that this	gof local optimizer tracks	0.071429
used to determine	adv index broadcastable	0.050000
new variable to	var name doc configparam root	0.250000
integers indicating the version	code cache version apply node	0.125000
toolbox feature to this function_graph and triggers its	function graph attach feature feature	1.000000
an input vector and t	node input_storage output_storage	0.038462
numpy ones_like	ones like model dtype opt	0.333333
apply_node recursively search	apply_node check reason	0.066667
the values of a shared variable to	compile shared variable	0.083333
replaces an [advanced]incsubtensor[1], whose increment	tensor local useless inc subtensor	0.333333
function for diagonalsubtensor and	tensor nnet get diagonal subtensor view x	0.083333
of theano gof graph is_same_graph	is same graph with merge var1 var2 givens	0.166667
of a shared variable	shared variable	0.071429
new_r	new_r	0.857143
__unify_walk__ method for one of the objects	unify walk a b u	0.037037
of numpy ones_like	ones like model dtype	0.333333
stack trace from	copy stack trace	0.055556
a new	new	0.117647
platform-dependent gcc argument for shared libraries	gof get gcc shared library arg	0.333333
baddestroymap if	check inputs node storage_map r_vals	0.166667
declare variables	declare	0.142857
offers to make itself the	to os	0.038462
return connection pattern of subfgraph defined by inputs	compile op from graph connection pattern node	0.076923
revert the replacement if the ops	replace	0.032258
output type dtype and broadcast there	local useless alloc	0.333333
apply nodes	sort apply nodes	0.200000
return full path of the dynamic	module name	0.062500
__unify_walk__ method	gof unify walk a b u	0.037037
takes as	tensor signal	1.000000
converts number to string by rendering it	compile char from number number	0.142857
representing a value on a certain	array	0.041667
is the equivalent of localoptgroup for	graph to gpulocal opt group	0.055556
one or more multinomial distributions	multinomial	0.024390
the source code for this linker and	gof clinker compile cmodule location	0.038462
y -> x	local	0.014085
the input to a leftdims +	input leftdims	0.166667
in the fgraph to break aliasing of outputs	fgraph wrapped_inputs wrapped_outputs	0.111111
replace_all_validate revert the replacement	validate replace all validate	0.111111
maps from variable and apply	get equiv inputs	0.142857
context	context	0.214286
if this enum	gof enum	0.166667
default failure_callback for seqoptimizer	optimizer warn exc optimizer	1.000000
indicating the version	c code cache version apply	0.125000
specific ops of	trace f_or_fgraph ops_to_check bug_print	0.035714
is an alloc of a scalar	alloc node	0.037037
a triangular solve	linalg tag solve triangular node	0.142857
numpy's isclose on tensors	tensor isclose a b rtol atol	0.500000
removes all from the clients list of r	gof function graph remove client r client_to_remove reason	1.000000
input	bad	0.013158
modified bessel function	i0 inplace	1.000000
operation to wait on a previously received	wait	0.022727
this function is basically a call to	extract constant x elemwise only_process_constants	0.058824
code when doing constant folding of	tensor elemwise python constant folding	0.142857
for diagonalsubtensor	diagonal subtensor view	0.083333
// 1	intdiv by one node	1.000000
of _maker which handles much of the	mode function maker i o m	0.066667
a special compound l{op} for the	crossentropy softmax argmax1hot	0.083333
the equivalent	gpulocal opt	0.055556
inputs outputs variables orphans temps and node_order fields	clinker fetch variables	1.000000
return a list of shape tuple	default infer shape	0.066667
current paramstype contains the specified theano type	type has type theano_type	0.500000
an alloc and only adds dimension to	tensor local alloc	0.111111
special compound l{op}	crossentropy softmax argmax1hot	0.083333
sort	sort axis	1.000000
graph leading to r to given depth	compile debugprint r prefix depth done	0.500000
solve operation c =	tensor solve	0.038462
multinomial distributions	multinomial	0.048780
or updated by by this function	function	0.052632
hash equal for same kinds of tensortype	type hash	0.166667
compile c code when doing constant folding of	tensor elemwise python constant folding	0.142857
to pack c types back into a pyobject	gof clinker type c sync	0.111111
complex-valued tensor from polar coordinate specification	complex from polar abs angle	0.250000
graph have a stack	stack	0.066667
replace_all_validate revert the replacement if the ops in	gof replace validate replace all validate remove	0.111111
folowing changes in the graph t	mul switch sink	0.045455
a new variable whose gradient will be stored	grad var	0.333333
runs a series of	linker many linkers wrappers	0.047619
wait on	wait	0.045455
optionally inserting broadcasted dimensions	operators	0.017241
view_map	view	0.022727
infer the number of dimensions from	tensor infer	0.142857
is false we can't change the value after	default filter	0.040000
none or a tensorvariable	as	0.024390
the apply to be inserted	apply	0.016667
output dimensions of convolving	output	0.017241
function for diagonalsubtensor	tensor nnet get diagonal subtensor	0.083333
x and there is	linalg local	0.142857
op could be very easy if	prod l op	0.033333
be inserted in the struct initialization code	init code struct node	0.125000
modified bessel function	i1	0.166667
matrix solve operation	tensor solve	0.038462
the theano enumeration types	type	0.011905
can be considered approximately equal	pure type values eq approx	1.000000
the diagonal of an empty	alloc diag	0.027027
the connection pattern of a subgraph defined	gof io connection pattern	0.055556
shape s to previously un-shaped variable r	shape r s override	0.500000
this helps numerical stability	scalar softplus	1.000000
the graph of a compiled theano	fct	0.083333
convolution gradient with respect to the	gpu dnn conv grad w	0.125000
such that	gof function	0.043478
the input broadcastable in the specified axes	addbroadcast x	0.142857
the navigator deal with the	gof navigator	0.038462
2^a	tensor exp2	1.000000
updates ordereddict the list of outputs and	get updates and outputs	0.333333
tuple of integers indicating the version	version apply	0.125000
the specified axis	axis sparse_grad	0.333333
of scan to outside of scan	scan output	0.125000
if target is 'cpu' this will transfer	py operators transfer target	0.500000
of apply	gof sort apply	0.200000
loop executes code	tensor make reordered loop	0.111111
operation to wait on a previously	mpirecv wait	0.045455
variable we want its gradient inputs scale	x	0.008772
return the idx_list with constant inputs	constant idx inputs	0.250000
a !=	neq	0.125000
see theano tensor argmin	tensor py operators argmin axis	1.000000
with	with	0.538462
the image shape of	shape 1axis image_shape	0.250000
abstract op for batch	abstract batch	1.000000
perform the permutation by doing a	perform node	0.083333
that allows replacing subgraphs of a	clone output replace strict share_inputs	0.071429
without replacement	tensor random streams base choice	1.000000
with debug info	with	0.076923
algo	algo	1.000000
inserting broadcasted	tensor tensor py operators	0.015625
to convert x into a variable	variable x context_name	0.250000
dot	dot node	0.500000
shared variable names when persisting to zip file	shared variable id	0.142857
be a legal	is valid	0.250000
return full path of	gof module name	0.076923
indicating the version	code cache version apply	0.125000
given an apply_node recursively search from this node	import apply_node check reason	0.066667
to	module cache add to	0.142857
dot product of the specified pieces of vectors	sparse block gemv make node o	0.066667
complete hashable signature of	clinker cmodule key	0.166667
shape that broadcast them to	tensor generate broadcasting	0.066667
a list of shape tuple or	feature default infer shape	0.066667
fetch a compiled module from the loaded cache	module cache get module	0.166667
diagnostic	diagnostic	1.000000
inputs	check inputs	0.125000
platform-dependent	get	0.020833
takes as	tensor signal pool	0.142857
takes as input	tensor signal max pool 2d same size input	0.500000
inserted in the struct initialization code	init code struct node	0.125000
integers indicating the version	version	0.093750
a simple algorithm to find	compile find bad optimizations2 order reasons r_vals	0.111111
this generates the c code for corr3dmm (direction="forward"),	base corr3d mm c code	0.090909
as replace_all_validate	all validate remove	0.166667
new variable to	var name doc configparam	0.250000
x is an input vector and t is	node input_storage output_storage	0.038462
compiles the source code for	clinker compile cmodule location	0.038462
for gpucorrmm	gpuarray base gpu corr mm	0.333333
has only one client and that	sitsot only	0.066667
of this op could be very easy if	l op	0.033333
function is basically a call to	tensor extract constant x elemwise only_process_constants	0.058824
return a symbolic vector	vector name dtype	0.166667
apply_node recursively search from	apply_node	0.050000
main diagonal set to a	fill diagonal offset a val offset	0.100000
op could be	op	0.009174
connection pattern of	io connection pattern	0.055556
an operation to wait on	mpisend wait	0.045455
compile lock to be held	to cache module key module_hash	0.166667
alias that wasn't in the	bad view	0.027027
allows replacing subgraphs of a computational	scan_module clone output replace strict share_inputs	0.071429
a convolution with	conv get	0.100000
diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x i0 i1	0.083333
retrive the context associated with	context	0.035714
filters	input filters	0.176471
in inputs that	gof	0.002381
the image shape of convolution gradweights	get conv gradweights shape 1axis	0.500000
as replace_all_validate revert	all validate	0.166667
scalar	tensor scalar name	0.500000
be turned into macros for use within the	cop get	0.033333
the optimization local_useless_rebroadcast and local_rebroadcast_lift	rebroadcast opt rval	0.200000
value after the import	core config param init default filter	0.040000
this variable	tensor py operators dimshuffle	0.019231
hash from an ndarray	hash from ndarray data	0.333333
input a 4-d tensor it sets	same size input patch_size	0.166667
to the diagonal of an empty matrix	alloc diag	0.027027
checks code for	code	0.050000
an [advanced]incsubtensor[1], whose increment	local useless inc subtensor	0.333333
version of sparseblockgemv check sparseblockgemv's docstring	sparse block gemv	0.166667
raise	inputs node storage_map	0.166667
inplace optimization that deals with allocempty	inplace allocempty op idx	0.166667
the specified axes	addbroadcast x	0.142857
converts this to expm1	local expm1	0.066667
some elementary validations on the inner graph	scan validate inner graph	0.035714
encoding of	nb_class dtype	0.200000
with a triangular solve	sandbox linalg tag solve triangular	0.142857
compute the numeric shape	shape	0.010204
of this variable optionally inserting broadcasted dimensions	tensor tensor py operators dimshuffle	0.019231
"lifts" dimshuffle through elemwise operations	tensor local dimshuffle lift	0.250000
input a n-d	input ws ignore_border stride	0.181818
return a code string	node name sub	0.111111
true if we are able to	dim_x dim_y	0.090909
that must	gof	0.002381
in defining the gradient the finite fourier	tensor fourier grad	0.250000
pattern for advancedsubtensor output	pattern a idx	0.066667
convert x into a variable on the	gpuarray variable x context_name	0.166667
numpy	numpy	1.000000
return true iff x and y are equal	check equal numpy x y	0.500000
true if var is always equal	gpuarray is equal var	0.250000
for matrix solve operation	solve	0.032258
roll tensortypes along the given axis	tensor roll x shift axis	0.333333
op that will call the	op	0.009174
the sum along	tensor sum	0.111111
this function uses set and dictionary data structures	non	0.071429
this op could	l op	0.033333
a sparse format instead of	sparse	0.019231
dnn conv workmem	no dnn workmem workmem	0.166667
epoch of the last access of a	last access time	0.040000
of this variable optionally inserting broadcasted dimensions	py operators dimshuffle	0.019231
in a new	with new inputs inputs strict	0.166667
idx_list with constant inputs replaced by	constant idx inputs allow_partial only_process_constants elemwise	0.071429
hot	hot	1.000000
for corrmm (direction="forward"), corrmm_gradweights	base corr mm	0.250000
function for diagonalsubtensor	diagonal subtensor view x i0 i1	0.083333
hack in profilemode to print the mflops	gpu corr3d mm flops inp outp	0.125000
replace	replace	0.193548
context_name	type	0.011905
the version	version apply	0.125000
f_node	f_node	1.000000
memory alias that wasn't	bad	0.013158
a normal	streams base normal	0.500000
dimensions of this	tensor tensor py	0.015873
a graph of apply nodes	sort apply nodes	0.200000
mflops	op flops inputs	0.125000
replace_all_validate revert the replacement if	replace validate replace all validate remove fgraph	0.111111
if and only if this enum	gof enum type	0.166667
install some functiongraph listeners to help the navigator	gof navigator	0.038462
failure	failure	1.000000
batch normalization of	tensor nnet batch normalization	0.125000
with a particular stream	random streams setitem item val	0.142857
generate c code to allocate outputs	make alloc loop_orders dtype sub fortran	0.200000
default reverse them	tensor transpose	1.000000
return label	d3viz var label var precision	1.000000
axis1	axis1	1.000000
same	tensor tensor	0.014286
for scalar values default	scalar shared	0.083333
op and reduce pattern has functioning c code	gpu careduce cuda supports c code inputs	0.250000
number of scalars	make	0.017857
shape in the shape_of dictionary	shape feature init r r	0.333333
the subtensor and its idx_list reorders the	idx_list get_count	0.090909
turned into macros for use	cop	0.028571
of headers that are needed by one	clinker headers	0.047619
is the equivalent	to gpulocal opt	0.055556
called by remove_feature feature should remove any	gof feature on detach function_graph	0.200000
headers that are	clinker headers	0.047619
this convert allocempty to alloc of 0	local alloc empty to zeros	0.333333
symbolic	dtype	0.068182
exp x or -exp x patterns	is exp	0.333333
cache directory structure	gof module cache	0.083333
main subclasses - :class enumlist	enum type	0.500000
we can't change the value after the import	config param init default filter	0.040000
device we do	device node	0.045455
removes	local remove	0.166667
to roll	roll	0.111111
idx list to get	tensor get idx list	0.076923
:math a using magma library	gpu magma	0.285714
computes the output dimensions of	conv op get output	0.047619
to the idx list to get the right	tensor get idx list	0.076923
makes the folowing changes in	mul switch sink node	0.045455
to outside of scan	out scan	0.035714
changed from r	r	0.028571
failure_callback	inplace exc	0.500000
apply_node recursively search from this node to know	apply_node	0.050000
subgraph bound by the inputs and outputs	init inputs outputs	0.166667
nesting	loop_orders dtypes loop_tasks	0.125000
compute sum of non nan /	signature get sum	0.142857
compilation flags	libs flags libs_dir include_dir	0.052632
dot product of the specified pieces of vectors	sparse block outer make node	0.066667
round half to even	round half to even	0.166667
revert the replacement if the ops in	validate replace all	0.050000
convolution for debugmode	nnet base abstract conv conv	0.125000
hash equal for same	hash	0.055556
(comma-separated key=value components) into a	config_string issue_warnings	0.333333
changes node inputs[i] to new_r	graph change input node i new_r	0.500000
try to detect	detect	0.090909
dimensions of this variable optionally inserting broadcasted dimensions	py	0.014286
atol	atol	1.000000
dict op -> total number of thunk calls	profile stats class callcount	1.000000
by a specified factor takes as	tensor signal pool 2d	0.142857
usmm -> usmm_csc_dense	sparse local usmm csx node	1.000000
that will be inserted at struct	struct	0.047619
return the [elementwise] largest	largest	0.125000
tan	tan	0.750000
used to determine the broadcast pattern for	adv index broadcastable pattern a	0.066667
helper function	tensor diagonal	1.000000
equivalent of localoptgroup	opt group	0.043478
clip x to be between min and max	tensor clip x min max	1.000000
that	bad	0.026316
helper	helper	1.000000
:attr context_name	gpuarray	0.023256
pt	pt	1.000000
return the cross-entropy between an approximating distribution	tensor nnet categorical crossentropy coding_dist true_dist	0.111111
to six moves	move move	0.250000
inputs_to_values	inputs_to_values	1.000000
the equivalent of localoptgroup	group	0.047619
a set of arrays to	a choices out mode	0.111111
stabilization optimization that is more general than crossentropy_to_crossentropy_with_softmax_with_bias	nnet crossentropy to crossentropy with softmax	1.000000
to the	alloc	0.012500
of l{codeblock} instances returns a string that executes	code gen blocks	0.050000
function that gets a scan	not_required inputs	0.250000
of	to gpulocal	0.055556
to help the navigator deal with	navigator optimizer	0.037037
the input specs and the output specs	compile std fgraph input_specs output_specs accept_inplace	0.142857
try to turn softmax(sum_of_stuff) -> softmax_w_bias	tensor nnet local softmax with	0.200000
c_extract_out	extract out	1.000000
incsubtensor when we overwrite	local useless inc subtensor node	0.066667
normalization	normalization	0.538462
connection pattern of subfgraph	from graph connection pattern	0.076923
travesal and	i o	0.041667
fill s v -> alloc(v	tensor local fill to alloc	0.250000
as replace_all_validate revert the replacement if the	gof replace validate replace all validate remove fgraph	0.111111
fgraph and a list of variables returns	fgraph outputs_to_disown	0.047619
-> x remove split with only 1 split	tensor local useless split node	1.000000
is primarily used by tensor rop	pure op r op inputs eval_points	0.125000
install some functiongraph listeners to help the navigator	gof navigator optimizer attach updater	0.038462
the navigator deal with the	gof navigator optimizer attach	0.038462
a new graph	with new inputs inputs strict	0.166667
conv output gradient	grad	0.031250
help the navigator deal	gof navigator optimizer attach	0.038462
of moved objects in six moves	module six moves urllib error	0.142857
the hack in profiling to print the mflops	nnet conv op flops inputs outputs	0.125000
this	graph to	0.055556
deepcopyop how to generate c	deep copy op c	1.000000
the stack trace from one or more tensor	stack trace	0.055556
the __unify_walk__ method for one of the objects	gof unify walk a b u	0.037037
local optimization wants to add	local optimizer add	1.000000
output_variables) where function is a thunk	thunk	0.021277
important note this function	scan_module push out seq scan process node fgraph	0.142857
enumeration types	type	0.011905
canonical	tensor get canonical	0.125000
convert data to something which can be associated	tensor type filter data strict	1.000000
baddestroymap if necessary update dr_vals	dr_vals	0.111111
for matrix solve	tensor solve	0.038462
required	variable_list blockers	0.166667
the op	get op	0.100000
elemwise round half to	sparse rint	0.250000
apply nodes according to a list	sort apply nodes inputs outputs cmps	0.050000
connection pattern of subfgraph defined by inputs and	compile op from graph connection pattern	0.076923
should be inserted to maintain order	searchsorted x v side sorter	0.142857
replace it with a triangular solve	sandbox linalg tag solve triangular node	0.142857
run a simple c snippet using current	march flag	0.250000
convolution gradient with respect to the weights	gpu dnn conv grad	0.062500
the updates ordereddict the list	updates	0.029412
function to roll tensortypes along the given axis	roll x shift axis	0.333333
convolution gradient with	conv grad	0.500000
to determine the broadcast pattern	tensor adv index broadcastable pattern a idx	0.066667
object with debug info	with op node thunk	0.166667
signature object for	signature	0.066667
or set of all variables which may share	compile infer reuse pattern	0.100000
by default that removes all asserts	remove all assert	0.055556
offers to make itself	to os environ pathlist	0.038462
type	type call	0.500000
see theano tensor std	py operators std axis ddof keepdims	1.000000
retrieve item from the cache	cache	0.034483
graph's apply nodes such that	gof	0.002381
of	gpu	0.011765
loop over several	loop	0.027778
e^a	exp	0.166667
pattern has functioning c code	gpu careduce cuda supports c code inputs	0.250000
decorator which will print a warning	deprecated filename	0.166667
op and reduce pattern has functioning c code	gpu careduce cuda supports c code	0.250000
variable optionally inserting broadcasted	tensor py operators	0.015625
compiles the source	cmodule location	0.038462
triangular solve	sandbox linalg tag solve triangular node	0.142857
the specified pieces of vectors and matrices	sparse block outer make	0.066667
:attr context_name	gpu array type	0.062500
if allow_override is false we can't	allow_override	0.083333
to raise in self	core raise	0.100000
det x and there is already an	local det	0.166667
the connection pattern of a subgraph	io connection pattern	0.055556
makes the folowing changes in	local mul switch sink node	0.045455
return an	op	0.018349
a meta path	meta path	0.500000
application of another op that takes the	op sub	0.066667
that can be specialized	gof	0.004762
retrive the context associated	get context	0.111111
alias	alias	1.000000
from polar coordinate specification	complex from polar abs angle	0.250000
enum	enum type	0.500000
pieces of vectors	sparse block outer make node o	0.066667
a variable with	variable	0.022222
n_streams	n_streams	1.000000
uniform	uniform random_state	0.125000
convop	kern d unroll_bsize unroll_ksize	0.166667
[advanced]incsubtensor[1], whose increment	local useless inc subtensor	0.333333
get the 0 based level of the	get depth	0.050000
look for a constant that	gof	0.002381
the "reverse-mode"	perform	0.117647
a specified factor takes as	tensor signal pool	0.142857
symbolic constant with value x	constant x name ndim dtype	0.333333
other implementation of mod	mod c	0.125000
reorder the dimensions	tensor py	0.015873
inv	inv	0.714286
links all the specified	merge new_best	0.142857
last access	last access time path	0.040000
a config string (comma-separated key=value components) into	core parse config string config_string issue_warnings	0.166667
the output	output input leftdims	0.333333
reorder the dimensions of this variable optionally inserting	tensor tensor py	0.015873
version	gcc	0.023810
function for diagonalsubtensor and	get diagonal subtensor	0.083333
if the g++ version	gof	0.002381
and its idx_list reorders the	idx_list get_count	0.090909
out the variables inside the	push out	0.037037
product of two sets of pieces of	sparse block	0.111111
input_specs	input_specs	1.000000
the 2d kernel that can	kernel 2d	0.050000
list of nodes that must be	gof	0.002381
symbolic row	tensor row name dtype	0.050000
list of policies to	policy policy	0.125000
the image shape	shape 1axis image_shape	0.250000
destroyhandler class detects when a	handler	0.071429
the inner	validate inner	0.142857
wait on a previously sent	wait	0.022727
as replace_all_validate revert the replacement if the ops	validate replace all validate	0.111111
have the same	tensor shape feature same	0.333333
[floor] division inverse of	tensor int div	0.250000
contents of a cache directory and return full	module name from dir dirname err files	1.000000
the idx_list with constant inputs replaced by their	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x i0	0.083333
all asserts	all assert	0.250000
inputs replaced by their python scalar equivalent	inputs allow_partial only_process_constants elemwise	0.166667
softmax(sum_of_stuff) -> softmax_w_bias matrix bias	tensor nnet local softmax with bias node	0.200000
compute the	tensor nnet	0.105263
variable optionally inserting	tensor tensor py operators dimshuffle	0.019231
can replace that with the *args directly	csm properties csm node	0.142857
to make itself the default python	to os environ	0.038462
wrapper around c_extract that initializes py_name from storage	gof get c extract r	0.250000
in profilemode to print the mflops	mm flops inp outp	0.125000
comparator to represent the dependence of nodes	dependence cmp	0.111111
if current paramstype contains the specified theano type	type theano_type	0.500000
with a metaclass	metaclass metaclass	0.125000
lists/tuples/other objects into a	compile flatten l	0.200000
the llvm one or not	gcc llvm	0.200000
the subgraph bound by the inputs and	init inputs	0.083333
scale or inverse the gradient	grad scale	0.333333
>= 3	3d	0.100000
a schedule function from comparators	sort schedule fn	0.333333
pydot graph	d3viz d3write fct path	0.166667
is an input vector and t is	node input_storage	0.038462
cls	cls	1.000000
runs a series of wrapper functions	linker many linkers wrappers	0.047619
sample from one or more multinomial	tensor multinomial random_state size	0.333333
are not required anymore and should be removed	outs	0.050000
uses the topooptimizer from the input nodes	gof in2out	0.055556
in	tensor	0.009646
optimizer for pushing out the variables inside	out	0.018519
a 4-d	patch_size	0.050000
variant on wraplinker that	gof wrap linker	0.083333
a tuple of integers indicating the version	code cache version apply node	0.125000
baddestroymap if	compile check inputs node storage_map	0.166667
unary(alloc x shp -> alloc(unary x shp)	local alloc unary node	0.250000
product of the specified pieces of vectors and	sparse block outer make	0.066667
toposort	toposort	0.461538
beta * y + alpha * dot a	tensor gemv c code y a	0.333333
variable of this type	type	0.011905
a triangular	triangular node	0.125000
uses set and dictionary data structures	out non seq scan	0.125000
remove are still in	replacements remove reason	0.055556
returns the bartlett spectral window in	bartlett	0.058824
perform	perform node	0.083333
dimensions of this variable optionally inserting broadcasted dimensions	tensor tensor py operators dimshuffle	0.019231
test a gradient	grad fun pt n_tests rng	1.000000
return str	to str t	0.166667
to allocate outputs	make alloc loop_orders dtype sub fortran	0.200000
baddestroymap	inputs node	0.100000
symbolic row variable (ndim=2 broadcastable=[true false])	row name dtype	0.050000
the constant scalar 0-d value underlying	scalar constant value	0.333333
vector	tensor vector name	0.500000
string specifying to the user what obj is	min informative str obj indent_level _prev_obs _tag_generator	0.333333
equivalent of localoptgroup	group	0.047619
compare true iff other is the same kind	tensor tensor type eq other	0.250000
as replace_all_validate	validate	0.090909
the -x pattern	tensor nnet is neg	0.166667
graph of apply nodes according to a list	apply nodes inputs outputs cmps	0.050000
matrix it does the	alloc	0.012500
removes all asserts from the graph	tensor local remove all assert	0.055556
which will print a warning message	gof deprecated filename msg	0.041667
turn softmax(sum_of_stuff) -> softmax_w_bias	nnet local softmax with	0.200000
the value after the	core config param init default filter	0.040000
node that uses r	gof function graph	0.031250
outputs_to_disown	outputs_to_disown	1.000000
copies	alloc	0.012500
retrieve item from the cache if available	call cache call fn args key	0.200000
a specified	a	0.008065
in the view_map	bad view	0.027027
var transferred	transfer var	0.100000
maximum see max for the maximum	maximum	0.083333
that removes all	remove all	0.166667
modified bessel function	i0	0.166667
graph of apply	apply	0.016667
truncate_gradient	truncate_gradient	1.000000
real-valued input on the gpu	curfft inp norm	0.066667
specified pieces of vectors and	sparse block gemv make node o	0.066667
an exception class to raise in self	core raise	0.100000
create an functiongraph	gof function graph	0.031250
reshape t by inserting 1 at the dimension	tensor shape padaxis t	0.333333
we	node	0.007407
shape of convolution operation	get conv shape 1axis image_shape kernel_shape border_mode subsample	0.333333
order	order	1.000000
iff other is the	other	0.090909
make it work for elemwise and gpuelemwise	local elemwise fusion	0.166667
standard deviation	tensor std	0.111111
indicating the version	clinker object c code cache version	0.125000
reduce pattern has functioning c code	gpu careduce cuda supports c code inputs	0.250000
the dimensions of this variable optionally	tensor py	0.015873
navigator	navigator optimizer attach	0.038462
pos	pos	1.000000
**inherit from**: - :class enumtype	enum list	1.000000
for diagonalsubtensor and	tensor nnet get diagonal subtensor view x	0.083333
the inner graph to	scan validate inner graph	0.035714
return connection pattern	connection pattern node	0.076923
gets a scan op a list of indices	op not_required	0.071429
listeners to help the navigator	gof navigator optimizer attach	0.038462
function is basically a call to tensor	extract constant x elemwise only_process_constants	0.058824
followed by a	modulo	0.250000
return true iff x and y are equal	tensor check equal numpy x y	0.500000
the epoch of the last	last	0.076923
localoptgroup	opt group	0.043478
of this variable optionally inserting	tensor py operators	0.015625
value	value	0.521739
reorder the dimensions of this variable	py operators dimshuffle	0.019231
the "reverse-mode" gradient [1]_ for the cholesky factorization	cholesky grad perform node inputs outputs	0.500000
fill s v -> alloc(v shape s	local fill to alloc	0.250000
posort	posort	1.000000
c code to initialize the variables that	gof clinker type c	0.166667
apply_node recursively search	import apply_node check reason	0.066667
to expm1	expm1 node	0.066667
the specified pieces of vectors and matrices	sparse block gemv make node o	0.066667
sqrt	sqrt	0.714286
the grad	grad	0.010417
merge 2 profiles returned by	seq optimizer merge	0.200000
that removes all asserts from the graph	remove all assert	0.055556
matrix solve operation c = a	tensor solve	0.038462
c_support_code_apply	c_support_code_apply	0.555556
a list to	factored list	0.500000
to val	val	0.125000
merge abs generated by local_abs_lift when the canonizer	tensor local abs merge node	0.333333
a vector to the diagonal of an	diag	0.023810
kinds of useless reshape	tensor local useless reshape	0.200000
the source	clinker compile cmodule location	0.038462
function for diagonalsubtensor and	diagonal subtensor view x i0 i1	0.083333
version used is the llvm one or not	llvm	0.100000
it with logsoftmax x 's	nnet local logsoftmax	0.076923
access of a given	access	0.100000
for	to gpulocal	0.055556
wait on	mpisend wait	0.045455
parse the tree and figure out which	scan_module traverse out x x_copy d	0.047619
has only one client and that client is	sitsot only	0.066667
scan in an easy to manipulate format	scan args	0.250000
wraplinker that	gof wrap linker	0.083333
called by functiongraph attach_feature the method that attaches	gof feature on attach function_graph	1.000000
for diagonalsubtensor and	nnet get diagonal subtensor view	0.083333
we can't change the value after the import	default	0.030303
unlock	unlock	1.000000
for scalar values default int64	scalar	0.017857
output of scan return	scan_module push out scan	0.050000
makes the folowing changes in the	mul switch sink	0.045455
if a dimshuffle	dimshuffle	0.014493
their unification	unification	0.076923
idx_list with constant	constant idx	0.250000
poisson	base poisson	0.500000
mrg stream state and they are spaced	sandbox mrg random streams	0.033333
l{codeblock} instances returns a string that executes	code gen blocks	0.050000
the destroyhandler class detects when	destroy handler	0.055556
sub1	sub1	1.000000
converts self _grad_op from user supplied form	from graph recompute grad op	0.200000
an operation to	mpirecv	0.037037
apply_node recursively search from	apply_node check reason	0.066667
elementwise subtraction (inplace on a)	tensor sub inplace a b	1.000000
apply nodes	apply nodes	0.200000
"reverse-mode" gradient for the	grad perform node	0.083333
apply as many	tensor apply	0.142857
important note this function uses set and	scan_module push out seq scan process node	0.142857
feature should	feature on detach	0.200000
of variables	variables and	0.250000
filters with a movie	tensor nnet conv3d signals filters signals_shape filters_shape	0.333333
the "reverse-mode"	perform node inputs	0.333333
y with length one the axes of x	y axis	0.125000
v given that v	set v	0.125000
the context object mapped to the	type context	0.090909
sample from one	size	0.076923
return a symbolic	dtype	0.068182
the value after the import of theano	default filter	0.040000
reduce pattern has functioning c	gpu careduce cuda supports c	0.200000
contents of a cache directory and	dir dirname err files	0.166667
broadcast pattern for advancedsubtensor	pattern a idx	0.066667
the fgraph outputs that will replace their values	fgraph expanded_inputs	0.058824
new variable like self	variable clone	1.000000
the type's :attr context_name	gpuarray gpu	0.045455
image_shape	image_shape	1.000000
of an op	clinker op	0.071429
value after the import of theano	core config param init default	0.040000
with a modulo	a	0.008065
transform of a real-valued input	tensor rfft inp norm	0.142857
cost scalar 0-dimensional variable	hessian cost	0.500000
symbolic type representing a numpy ndarray	tensor type	0.034483
by	gof	0.002381
in a sparse format instead	core sparse	0.066667
temporarily adjust autocasting behavior	autocast float as	1.000000
careduce that reuse the python code from gpuarray	careduce cpy	1.000000
important note this function uses set and dictionary	out seq scan process node fgraph	0.142857
change the value after	config param init default	0.040000
feature	feature	0.500000
nit_sot output of scan return true iff	push out scan	0.050000
a tensorvariable of this	make variable	0.166667
reasons	reasons	1.000000
output after pad_dims	unpad dims output input leftdims	0.333333
-1 and converts this to expm1	expm1	0.050000
string specific to the apply to be inserted	apply node	0.031250
the replacement if the ops in the	gof replace validate replace all	0.050000
link_kwargs	link_kwargs	1.000000
dimensions	tensor tensor py	0.031746
scan that depend only on non-sequences	non seq scan	0.090909
of this op	op	0.009174
type's :attr context_name	gpuarray gpu array	0.062500
replace it with a triangular solve	linalg tag solve triangular node	0.142857
to merge	merge	0.142857
add a	key data add	0.500000
a variable on the	as gpuarray variable	0.166667
important note this function uses set	seq scan process node fgraph node	0.142857
pattern of subfgraph defined	pattern node	0.125000
a vector to the	alloc	0.012500
revert the replacement if the ops	validate replace all	0.050000
folowing changes in	mul switch sink	0.045455
all symbolic variables in inputs to sharedvariable	inputs	0.012658
requirements	requirements	0.625000
represent the dependence	make dependence	0.043478
a warning message	deprecated filename msg	0.041667
computes the confusion matrix of	nnet confusion matrix	0.166667
new random stream in this container	tensor random streams gen op	0.250000
create a new random stream in this container	tensor random streams gen op	0.250000
logsoftmax x 's	local logsoftmax	0.076923
of an op	gof clinker op	0.333333
the first outdim-1 dimension size s	ndim outdim	0.142857
along the given axis es of	axis dtype	0.083333
of this variable optionally	operators	0.017241
between low and high	size low high	0.500000
node a clone in a new graph	clone	0.020833
a transfer	transfer	0.058824
to r to	r	0.028571
a recursion over the	permute row elements rec	0.047619
row	tensor row name	0.050000
checks if the outputs of specific ops	trace f_or_fgraph ops_to_check bug_print	0.035714
useless dimshuffle	tensor local useless dimshuffle	0.500000
input of given shape and flags	grad out shape imgshape ws ignore_border stride	0.200000
warning message	gof deprecated filename msg	0.041667
same	tensor tensor type	0.041667
with constant inputs replaced by their python scalar	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
dfs traversal and chooses the orphans among them	orphans	0.090909
u and uses it	o u	0.037037
to make itself the default python and	to os environ pathlist var newpath	0.038462
that broadcast them	generate broadcasting	0.066667
a metaclass	metaclass metaclass	0.125000
updates	updates	0.176471
tuple of integers indicating the version	clinker object c code cache version	0.125000
the axis that was used to split x	tensor split grad inputs g_outputs	0.333333
output of scan return true	out scan	0.035714
det x and there is	sandbox linalg local det	0.166667
a subtensor is inside a dimshuffle which	subtensor node	0.066667
of all variables which may share	infer reuse pattern	0.100000
is the first functiongraph	no_recycling profile	0.250000
we can't change the value after	init default filter	0.040000
moved objects in six moves urllib_robotparser	module six moves urllib robotparser	0.333333
also work for gpuincsubtensor	setsubtensor node	0.250000
all	all	1.000000
match	var	0.035714
required calculate the function on the inputs and	node inputs	0.043478
the finite fourier	tensor fourier	0.333333
the context associated with a	gpuarray get context	0.111111
helpful function that gets a scan	not_required inputs	0.250000
delete keys in old format from the	gof cleanup	0.333333
compute a batched tensordot product	batched tensordot x y axes	0.333333
the end variables of	grad wrt end	0.050000
duplicate this apply instance in	gof apply clone	0.166667
post some text to a gist	post gist	0.333333
return connection pattern of subfgraph	from graph connection pattern node	0.076923
symbolic vector	tensor vector name dtype	0.166667
form x[0 :] -> x[0]	local useless slice node	0.250000
their apply_node if those nodes are not	gof	0.002381
change the value after the import of	param init default	0.040000
dimensions	py operators	0.031250
equivalent of	to gpulocal opt	0.055556
to the one	one	0.076923
will have separated maker and	share_memory swap delete_updates name	0.250000
names to an iterable of variables modifies input	give variables names variables	0.333333
specs and the output specs	compile std fgraph input_specs output_specs accept_inplace	0.142857
helper function to draw random integers	random integers helper random_state	1.000000
is no change	node	0.014815
i	clone i	1.000000
mflops	mm flops inp outp	0.125000
spatio-temporal filters with a movie	conv3d signals filters signals_shape filters_shape	0.333333
using mpi	mpirecv	0.037037
the batch size loop	batch	0.055556
of broadcastable	broadcastable	0.111111
of lib directories that are needed by	clinker header dirs	0.055556
tensortypes	x shift	1.000000
for use within the op	get op	0.100000
cost and/or from existing start gradients up to	start cost	0.100000
pydot graph and write	d3viz d3write fct path	0.166667
input of given shape and flags	signal pool out shape imgshape ws ignore_border stride	0.200000
implements numpy's isclose on	tensor isclose a	0.500000
the specified pieces of vectors	sparse block outer make node o	0.066667
-> sum(a axis={ }) / b	tensor local sum prod div dimshuffle node	0.333333
ultra	ultra	1.000000
load data	load w dtype	1.000000
exception class to raise	raise init	0.200000
with debug	with op node	0.166667
deprecated old conv2d	nnet conv2d input filters image_shape filter_shape	0.500000
of integers indicating the version	code cache version apply	0.125000
an instance of _maker which handles much	debug mode function maker i o m	0.066667
the epoch of the last access	last access	0.040000
gpua	gpua	1.000000
from a uniform	uniform	0.043478
represent the dependence of nodes in a graph	gof make dependence	0.043478
replace it with logsoftmax x 's grad	nnet local logsoftmax grad	0.200000
all intermediate variables given input shapes	of variables fgraph input_shapes	0.250000
be a legal value for	is valid value	0.250000
get the 0	type get depth	0.050000
c code for corr3dmm (direction="forward"), corr3dmm_gradweights	nnet base corr3d mm c code	0.090909
only one client and that client	sitsot only	0.066667
enabled change all sigmoid to	sigmoid node	0.100000
the convolution gradient with respect to the weights	gpu dnn conv grad w	0.125000
and "code_cleanup" together	cleanup node name inputs	0.500000
helper function drawing from multinomial	multinomial helper random_state n pvals size	0.500000
tensor_from_scalar(scalar_from_tensor	scalar	0.017857
setitem	setitem	1.000000
this generates the c code for corr3dmm (direction="forward"),	corr3d mm c code	0.090909
folowing changes	tensor local mul switch sink	0.045455
called whenever node inputs[i] is changed from	feature on change input function_graph node i	0.333333
sent array using	mpisend	0.037037
output after pad_dims	gpuarray unpad dims output input leftdims	0.333333
code string specific to the apply	apply node	0.031250
in the struct initialization code	init code struct node	0.125000
of corresponding keys	walk d1 d2 u	0.333333
get a constant value by its alias	enum type fromalias alias	1.000000
return a code string specific to	node name	0.066667
return a code	name sub	0.050000
with a triangular solve	tag solve triangular	0.142857
expm1	local expm1	0.066667
the image shape of	shape kernel_shape	0.250000
variable optionally inserting broadcasted dimensions	tensor py operators	0.015625
an index array and a set of arrays	a choices out mode	0.111111
special debugging linker	linker	0.142857
are packed like this n_steps	outs	0.050000
multiline string representating the cause of	bad optimization str diagnostic	0.043478
also work for gpuincsubtensor	tensor local inplace setsubtensor node	0.250000
modified bessel function	i1 inplace	1.000000
max	max	0.687500
that allows replacing subgraphs of	scan_module clone output replace strict share_inputs	0.071429
returns maximum elements and their	tensor max and argmax a	0.250000
the c code for corr3dmm	base corr3d mm c code	0.090909
dot product of two variables	tensor dot a b	0.333333
raises a badviewmap exception when it detects the	check viewmap node	0.111111
a multinomial distribution defined	streams multinomial	0.076923
the cross-entropy between an approximating distribution and a	tensor nnet categorical crossentropy coding_dist true_dist	0.111111
diagonalsubtensor	get diagonal subtensor view x	0.083333
call the supplied function as	compile as	0.050000
in the original graph to a	inputs outputs copy_inputs_and_orphans memo	0.029412
merge abs generated by local_abs_lift when the	local abs merge node	0.333333
have the same	feature same	0.333333
axis that was used to split x	split grad inputs g_outputs	0.333333
axis	x axis	0.200000
a -1 and converts this to expm1	expm1	0.050000
grad	grad node	1.000000
matrix where each row correspond to the one	tensor to one	0.125000
context object mapped to the type's	array type context	0.090909
optionally inserting	operators	0.017241
important note	seq scan process node	0.142857
the r operation on f	core rop f	0.166667
if cond	tensor switch cond	0.500000
method	method	1.000000
ignore_trees-related functionality	attach updater fgraph importer pruner chin	0.250000
output dimensions of convolving	op get output	0.047619
revert the replacement if the	gof replace validate replace all	0.050000
setsubtensor(x x[idx], idx) -> x when x	local setsubtensor of constants node	0.250000
type that allows no values	null type	0.500000
remove broadcastable dimensions from the shape of an	tensor tensor py operators squeeze	0.200000
"lifts" dimshuffle through elemwise operations and	tensor local dimshuffle lift	0.250000
for operations that need to compile	gpu kernel	0.250000
to wrt, computes gradients of	core subgraph grad wrt	0.062500
the source code for this linker and returns	gof clinker compile cmodule location	0.038462
convert data to something which can be associated	tensor tensor type filter data strict	1.000000
function to roll tensortypes	roll x shift	0.250000
c-implementation of the	csr c code node name inputs	0.333333
self _rop_op from user supplied form to type	compile op from graph recompute rop	0.200000
in profiling to print the mflops	nnet conv op flops inputs	0.125000
wasn't in the	bad view	0.027027
standard elements of an op or type	object	0.083333
fourier	fourier	0.750000
function should return	tensor matrix inverse	1.000000
persistent_load	persistent_load	1.000000
make a nested loop	tensor make loop	0.200000
value underlying	value	0.043478
broadcastable dimensions scrap the dimshuffle and index	local dimshuffle	0.052632
a schedule	schedule	0.125000
[start stop step] transform it into a canonical	tensor get canonical	0.125000
(function input_variables output_variables) where function is a thunk	make thunk	0.125000
2**72 samples	get substream rstates n_streams dtype inc_rstate	0.142857
variable out for occurrences of values identical with	scan_module forced replace out	0.500000
of this variable optionally inserting broadcasted dimensions	tensor tensor py	0.015873
takes as	signal pool	0.142857
input a n-d tensor	input ws ignore_border stride	0.181818
true if l	l	0.111111
see theano tensor sum	py operators sum axis dtype keepdims acc_dtype	1.000000
the subgraph contained between i and o	i o copy_inputs	0.333333
to theano constants in subtensor arguments	tensor make constant args	0.250000
idx_list with constant	tensor subtensor get constant idx	0.250000
an op by transferring each	op remove	0.250000
inserting broadcasted dimensions	tensor tensor	0.014286
a <=	le a	1.000000
softmax(sum_of_stuff) ->	local	0.014085
to be inserted in the struct	struct	0.047619
by transferring each of its outputs	remove	0.035714
a convolution with the	dnn conv get	0.100000
the equivalent of localoptgroup for	group	0.047619
y with length one	y	0.026316
dtype as the template filled by broadcasting value	broadcast like value template fgraph dtype	1.000000
to compute the kernel shape of convolution gradweights	nnet get conv gradweights shape image_shape	0.500000
inputs replaced by their python scalar	inputs allow_partial only_process_constants elemwise	0.166667
data	data strict	0.500000
that will be turned into macros for use	cop	0.028571
string specific to the apply to	apply node	0.031250
broadcast them to match	generate broadcasting	0.066667
offers to make itself the default	to os environ pathlist var	0.038462
return full path of the dynamic	gof module name	0.076923
in the	view	0.022727
list of lib directories that	clinker lib dirs	0.055556
variable class	variable	0.022222
replace_all_validate revert the replacement if the ops in	replace validate replace all validate remove fgraph	0.111111
the leaves of a search through consecutive view_map()s	gof view roots r	0.200000
for use within the op code	op	0.009174
for comparing	constant	0.016667
tensortype	tensor tensor type	0.041667
context object mapped to the type's :attr	type context	0.090909
elements and their	tensor max and argmax a	0.250000
_maker which handles much	maker i o m	0.066667
name in	name	0.011111
on	gpuarray	0.023256
in the	bad	0.026316
stop step] transform it into a canonical form	get canonical form	0.045455
unfortunately conda offers to	to os	0.038462
reshape t by inserting 1 at the	shape padaxis t	0.333333
on types of x y	x y	0.048780
a signature object	signature	0.066667
:func neibs2images	tensor nnet neibs2images	0.333333
code string specific to the apply to	apply node	0.031250
gpu_contiguous(gpu_contiguous x -> gpu_contiguous x	gpuarray local gpu contiguous gpu contiguous node	0.500000
a canonical	get canonical	0.125000
two kinds scalar constants and the rest	rest inputs elemwise only_process_constants	0.125000
some requirements to the fgraph this	requirements fgraph	0.250000
an instance of _maker which handles much	mode function maker i o m	0.066667
and return full path of	module name from	0.076923
returns the bartlett spectral window in	tensor bartlett m	0.083333
exception raised to	error	0.025000
the constant scalar 0-d value underlying variable v	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
input specs and the output specs	input_specs output_specs accept_inplace	0.142857
performs batch normalization of the	nnet batch normalization	0.125000
correspond to the one hot	one hot	0.142857
order v	v x	0.100000
this explicitly upcasts constant inputs to	constant inputs	0.125000
and see whom can	can	0.142857
to boundvariable(other_object)	walk fv o u	0.200000
to be inserted in the module initialization	init	0.058824
simple algorithm	bad optimizations2 order reasons r_vals	0.333333
product of the specified pieces of vectors	sparse block outer make	0.066667
replace a leaf of a multiplication	tensor nnet replace leaf	0.100000
full_matrices	full_matrices	1.000000
python function that takes theano variables as inputs	fun	0.166667
[floor]	int	0.166667
six moves urllib namespace	module six	0.043478
profiling informaton	profile node profile	1.000000
install some functiongraph listeners to help the navigator	navigator	0.032258
this variable optionally inserting broadcasted dimensions	tensor	0.006431
the other implementation of mod	scalar mod c code node name inputs outputs	0.125000
it	alloc	0.012500
this is the equivalent of	graph to gpulocal	0.055556
wrt, computes gradients	core subgraph	0.062500
first kind of order v real	v x	0.100000
the output type dtype and broadcast there	tensor local useless alloc	0.333333
function for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x i0 i1	0.083333
inserting	py operators dimshuffle	0.019231
a code string specific to the apply	apply node	0.031250
on something that is not	not	0.076923
1/(1+exp x -> sigm -x	nnet local inv 1 plus exp node	0.333333
dummy file with these flags	flags cls flag_list preambule body	0.250000
structures	push out non seq scan	0.125000
to recognize the updates ordereddict the	scan_module get updates	0.034483
associated with a particular stream	random streams getitem item	0.142857
source code	clinker compile cmodule location	0.038462
z <- beta	z	0.111111
output specs	fgraph input_specs output_specs accept_inplace	0.142857
op __init__ fct don't have the same parameter	composite make new inplace output_types_preference name	0.142857
each shape that broadcast them to match	tensor generate broadcasting	0.066667
a symbolic row variable (ndim=2	row name	0.050000
log10	log10	0.833333
test a	fun pt n_tests rng	0.500000
wait on a previously received	mpirecv wait	0.045455
debug counter	debug counter	0.500000
2d kernel for bilinear	bilinear	0.019231
pieces of vectors	sparse block gemv make node o w	0.066667
if allow_override is false we can't change the	allow_override	0.083333
help the navigator deal	navigator optimizer attach updater	0.038462
value after the import	param init default filter	0.040000
inner-most loop executes	make reordered loop	0.111111
compact	compact	1.000000
ptp	ptp	1.000000
that	gof function graph	0.031250
the axis that was used	grad inputs g_outputs	0.076923
convert python	make constant	0.100000
tensor operators	tensor	0.006431
raised by get_scalar_const_value if called on something	error	0.025000
unary(alloc x shp -> alloc(unary x shp)	tensor local alloc unary node	0.250000
of the main diagonal set to a	fill diagonal offset a val offset	0.100000
tensor of type dtype	dtype	0.022727
takes as	signal max	1.000000
generic exception raised to indicate an	error	0.025000
as python not the other implementation of mod	mod c code node name inputs outputs	0.125000
an expression	x	0.008772
along the given axis es of a tensor	axis dtype op	0.083333
a symbolic 3-d	tensor tensor3 name dtype	0.166667
the output	conv op get output	0.047619
is no change in	node	0.014815
minimum in one	tensor minimum x y	0.090909
respect to wrt, computes gradients of	core subgraph	0.062500
1/(1+exp	inv 1 plus exp	1.000000
in defining the gradient the	grad	0.010417
find broken	find	0.125000
enabled change all sigmoid	sigmoid	0.055556
and apply_nodes to this graph	gof function graph	0.031250
decorator to support version-based cache mechanism	code version version	0.333333
assert that x and	x	0.008772
extract list of variables	variables	0.043478
is not attempting to use dnn conv workmem	no dnn workmem workmem	0.166667
class with a metaclass	metaclass metaclass	0.125000
the outputs from	outputs	0.045455
baddestroymap if	node storage_map	0.166667
a signature object for	constant signature	0.100000
c code	clinker type c	0.250000
hash equal for same kinds of	hash	0.055556
the context associated with a	get context	0.111111
a sparse format instead of dense	core sparse	0.066667
the other implementation of mod	mod c code node name inputs outputs	0.125000
important note this	push out seq scan process node	0.142857
row variable (ndim=2	row name dtype	0.050000
list of compilation flags from	libs flags libs_dir include_dir	0.052632
and	out	0.037037
multiplier	multiplier	1.000000
a leaf of a multiplication tree	leaf	0.066667
this op could be very	prod l op	0.033333
filters with a	signals filters	0.111111
of cost and/or from	cost	0.045455
evaluate because of	destroy	0.009709
the dimensions of	tensor	0.006431
that will be turned into macros	cop	0.028571
helper function to draw random integers	random integers helper random_state low high	1.000000
print a warning message on the	gof deprecated filename msg	0.041667
profiling to print the mflops	nnet conv op flops	0.125000
node inputs[i] to	graph change input node i	0.250000
sign of	tensor sgn	1.000000
a dense vector	svcsr	0.090909
this	op	0.009174
a bit like make_loop but when only	init_loop_orders olv_index dtypes inner_task	0.200000
a |	or	0.125000
x default reverse them	transpose x axes	0.200000
r's shape in	shape	0.010204
not the other implementation of mod	mod c code node name inputs	0.125000
n >= 3	3d	0.100000
informative	informative	1.000000
bartlett spectral window in	tensor bartlett	0.083333
confusion matrix	tensor nnet confusion matrix	0.166667
mflops	op flops inputs outputs	0.125000
that decrefs py_name	gof get c cleanup r name sub	0.250000
post some text to a gist	misc post gist	0.333333
wants to	optimizer	0.062500
symbolically cast	tensor cast	1.000000
return a string for making	gpuarray gpu careduce cuda makecall node name x	1.000000
to replace a leaf	replace leaf	0.100000
this apply instance in a new	gof apply clone with new inputs inputs	0.250000
and argmax over a given axis or	and argmax	0.166667
of order v real	v x	0.100000
return connection pattern	from graph connection pattern	0.076923
version	code cache version apply	0.125000
print the following	print global	1.000000
the original graph to	inputs outputs copy_inputs_and_orphans memo	0.029412
the connection pattern	connection pattern	0.032258
os	os	1.000000
basic variable class	variable	0.044444
allows replacing subgraphs	clone output replace strict share_inputs	0.071429
vector and t is a	node input_storage	0.038462
tensorvariable of this type	tensor type make variable	0.500000
return c code to declare variables	c declare name	0.500000
elementwise multiplication (inplace on a)	mul inplace a b	1.000000
reproducible case for problems during	compile function dump filename inputs outputs	0.166667
2d kernel for bilinear upsampling	bilinear	0.019231
the g++ version	gcc	0.023810
the dimensions of this variable optionally inserting broadcasted	tensor	0.006431
a variable on the gpu	gpuarray variable	0.166667
args are packed like this n_steps	args outs	1.000000
c type numpy typenum that corresponds	type dtype specs	0.071429
shared variable	shared variable	0.142857
draw samples from a poisson distribution	random streams base poisson size lam	1.000000
how to generate c	shape c	0.250000
computes the r operation on f	rop f	0.166667
re-raise an exception while annotating	op node thunk exc_info storage_map	0.250000
mrg stream state and	sandbox mrg random streams	0.033333
inner-most loop executes	loop	0.027778
using	mpisend	0.037037
return a code string specific to the apply	node name	0.033333
change references to variables into references to types	subtensor convert entry slice_ok	1.000000
this generates the c code for corr3dmm	tensor nnet base corr3d mm c code	0.090909
detect a	tensor detect	0.166667
idx_list with constant inputs replaced	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
convert python litterals	tensor make constant	0.100000
list of shape tuple or	shape	0.010204
pass to helper_c_code	get helper c code	0.142857
to make itself the default python	to	0.017544
decorator to merge	merge cls alpha_in beta_in	1.000000
with a sparse matrix	sparse	0.019231
openblas	tensor openblas	0.250000
while annotating	exc_info storage_map	0.250000
impl	impl	1.000000
important note this function uses set and dictionary	seq scan process node fgraph	0.142857
to allocate outputs	tensor make alloc loop_orders dtype sub fortran	0.200000
it into a canonical form	get canonical form	0.045455
batched dot product of two	batched dot	0.250000
shared variable	compile shared variable	0.083333
where x is an input vector and	node	0.007407
a memory alias that wasn't in the view_map	bad	0.013158
the variable v is positive semi-definite	v	0.011111
this function is only used to determine the	adv index broadcastable	0.050000
unroll the batch size	unroll batch	0.166667
gradients up to the end variables of a	end	0.040000
indicate an internal theano problem	debug mode	0.200000
if the named	fullname	0.066667
list of policies to name r	policy policy r name	0.250000
convert python litterals to	tensor	0.003215
gradient w r t its inputs	conv2d grad wrt inputs output_grad filters input_shape filter_shape	0.333333
implementation of mod	mod c	0.125000
tell specifyshape how to generate c code for	c code typ code version c_support_code_apply	1.000000
updates ordereddict the list of outputs and the	updates and outputs	0.333333
or more multinomial distributions defined by	tensor multinomial random_state	0.040000
the function name to load data	gpuarray load w dtype	0.200000
a version of var transferred to target	tensor transfer var target	0.200000
filters with	nnet conv3d signals filters	0.111111
constant scalar	get scalar constant	0.333333
a version of var	var	0.035714
modified bessel function of	tensor i0	1.000000
clone the graph and get a	function graph clone	0.166667
for every node that uses	gof function graph	0.031250
input a 4-d	same size input patch_size	0.166667
into a	make	0.017857
optionally	tensor tensor	0.014286
an [advanced]incsubtensor[1], whose increment is an alloc	local useless inc subtensor alloc node	0.166667
last access of a	last access	0.040000
cusolver gpu cholesky op	gpu cholesky	1.000000
a series of wrapper functions	many linkers wrappers	0.047619
by a specified factor takes as	tensor signal pool	0.142857
variable	tensor tensor py operators	0.015625
connection pattern of subfgraph defined by	from graph connection pattern node	0.076923
function to get the 0 based level	type get	0.050000
"reverse-mode" gradient [1]_ for the cholesky factorization	cholesky grad perform node inputs	0.500000
input a n-d tensor where n	input ws ignore_border stride	0.181818
the idx_list with constant inputs replaced by	constant idx inputs allow_partial only_process_constants elemwise	0.071429
to represent the dependence	dependence	0.035714
axes	axes	1.000000
variable and apply nodes in the original graph	outputs copy_inputs_and_orphans memo	0.029412
can replace that with the *args directly	csm properties csm	0.142857
cop	cop	0.142857
with a set of 3d	nnet conv3d input	0.125000
if	check inputs	0.125000
symbolically cast x to a tensor	cast x	0.200000
of	tensor tensor py	0.015873
apply nodes according to a list of	sort apply nodes inputs outputs cmps	0.050000
do it only on cpu here	pow specialize	0.250000
the output specs	std fgraph input_specs output_specs accept_inplace	0.142857
is the equivalent of localoptgroup for graphtogpu	opt group	0.043478
current op and reduce pattern has functioning c	gpu careduce cuda supports c	0.200000
supplied function as	compile as	0.050000
nit_sot output of scan	scan	0.017241
the confusion matrix	confusion matrix	0.166667
return selected slices of an array	tensor compress condition	1.000000
mrg stream state	sandbox mrg random streams	0.033333
an operation to wait on a previously	mpirecv wait	0.045455
implements the "reverse-mode"	perform node inputs	0.333333
by an op	clinker op	0.142857
important note this function	out seq scan process node fgraph node	0.142857
replace_all_validate revert the replacement if	validate replace all validate remove	0.111111
the subgraph between i and o	i o	0.083333
that use	op use	1.000000
construct a variable with a sparse matrix	sparse variable	0.250000
put in a functiongraph a constant	constant	0.016667
return a	get	0.020833
diagonalsubtensor and	diagonal subtensor view x i0	0.083333
of lib directories that are	header dirs	0.045455
used to determine	tensor adv index broadcastable	0.050000
function tries to recognize the updates ordereddict	get updates	0.034483
a symbolic vector	vector name dtype	0.166667
tuple of symbolic shape vars for tensor variable	shape feature shape tuple	1.000000
function that allows replacing subgraphs of a computational	clone output replace strict share_inputs	0.071429
symbolic row	row	0.034483
mod	scalar mod c code	0.125000
replace_all_validate revert the replacement	gof replace validate replace all validate remove	0.111111
important note this function uses set and dictionary	scan_module push out seq scan process node	0.142857
as python not the other implementation of mod	scalar mod c code node name inputs	0.125000
first functiongraph that has ever been associated to	gof	0.002381
list of variables [v1 v2 v3 ]	vm linker compute gc dependencies variables	0.250000
scalar values default int64 or float64	scalar	0.017857
attempting to use dnn conv algo_bwd	dnn algo bwd algo	0.166667
platform-dependent gcc	gof get gcc	0.333333
mini-batch of a stack of 2d	input_shape filter_shape	0.018519
storage_map	storage_map	0.454545
destroyhandler	destroy handler	0.055556
a	view	0.022727
"reverse-mode" gradient	grad perform	0.166667
hyperbolic sine	sinh	0.142857
set and dictionary data structures	out non seq scan	0.125000
an array from an npy file	path dtype broadcastable mmap_mode	1.000000
exception class to raise in self	core raise init	0.100000
the -x pattern	nnet is neg var	0.166667
clone the graph and get	function graph clone	0.166667
duplicate this apply instance in	gof apply clone with	0.166667
memory alias that wasn't in	bad	0.013158
flags from	flags	0.062500
output nodes of the graph	gof	0.002381
_maker which handles much of	maker i o m	0.066667
output shape of convolution operation	get conv shape 1axis image_shape kernel_shape border_mode subsample	0.333333
image shape	shape 1axis kernel_shape	0.250000
value after the import of theano	config param init default filter	0.040000
to make itself	to os environ	0.038462
variable optionally inserting broadcasted dimensions	tensor tensor py operators	0.015625
this convert allocempty to alloc of 0	alloc empty to zeros	0.333333
python litterals to	make	0.017857
b), axis=0) -> elemwise{scalar op}	local reduce join	0.111111
to get the 0 based	type get depth	0.050000
kernel that	kernel	0.133333
function	random_state n shape	0.500000
fourier transform of a real-valued input	tensor rfft inp norm	0.142857
attempt	attempt	1.000000
remove broadcastable dimensions from the shape of	tensor py operators squeeze	0.200000
for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view	0.083333
from d1 d2 to d1+size d2	scan_module expand empty tensor_var size	0.166667
reorder the dimensions	py operators dimshuffle	0.019231
into an aboslute path	gof cop get path cls f	0.166667
grad of this op	op	0.009174
if a subtensor is inside a dimshuffle	subtensor node	0.066667
full path	gof module	0.058824
compiles this linker's fgraph	make thunk input_storage output_storage storage_map keep_lock	0.333333
in a sparse	core sparse	0.066667
this op scale or inverse the gradient	grad scale	0.333333
input a 4-d tensor it sets all	same size input patch_size	0.166667
this generates the c code for corr3dmm	nnet base corr3d mm c code	0.090909
a six moves urllib	six	0.025000
mflops	tensor nnet base abstract conv flops inp outp	0.125000
respect to the weights	dnn	0.030303
inner graph	scan	0.017241
primitive c type of items into variables handled	c element	1.000000
names to an iterable of	names	0.047619
memoize	memoize	1.000000
l{codeblock} instances returns a string	gof code gen blocks	0.050000
a compiled module from the loaded cache or	cache get module	0.166667
and see whom can be removed from	can remove outs	0.250000
initializes py_name to py_none	init r	1.000000
c type numpy typenum that	tensor tensor type dtype specs	0.071429
gradient	grad inputs	0.111111
given graph contains a cycle parameters	gof contains cycle	0.333333
the c code for corrmm	tensor nnet base corr mm c code	0.090909
step] transform it into a canonical form that	tensor get canonical form	0.045455
a view in the forward but clip the	clip x lower_bound upper_bound	0.090909
c code when doing constant folding	python constant folding	0.142857
arcsine of	tensor arcsin	1.000000
matrix a and matrix	structured	0.071429
computes the output dimensions	output	0.017241
change all sigmoid to	sigmoid node	0.100000
ancestors	ancestors	1.000000
in a new graph	new	0.058824
v raises attributeerror	v	0.011111
uses the topooptimizer from the input nodes to	gof in2out	0.055556
to add some requirements to the fgraph this	optimizer add requirements fgraph	0.333333
dimensions of this variable	tensor tensor	0.014286
ultra_fast_sigmoid	ultra fast	0.333333
if we are able	dim_x dim_y	0.090909
form	form	0.666667
merge	seq optimizer merge	0.200000
low and	size low	1.000000
conj	conj	0.500000
function that gets a scan op	op not_required	0.071429
operation to wait on a	wait	0.045455
failure_callback for	inplace exc	0.500000
nodes that must be	gof	0.002381
the gradient the finite fourier	fourier grad	0.250000
the shape or	shape	0.010204
wrapper around c_extract that	gof	0.002381
the flattened version	flatnonzero	0.083333
gcc argument for shared libraries	gcc shared library arg	1.000000
convop that	kern d unroll_bsize unroll_ksize	0.166667
reshape t by inserting 1 at	tensor shape padaxis t	0.333333
return the cross-entropy between an approximating distribution and	nnet categorical crossentropy coding_dist true_dist	0.111111
the bartlett spectral window in the	tensor bartlett m	0.083333
full path of the dynamic lib	gof module	0.058824
into macros for use within the op	cop get op	0.200000
determine the name the	name obj	0.111111
scalar	scalar name	0.500000
maps a failure code to the task	task	0.083333
is inside a dimshuffle which only drop	node	0.007407
graph	function graph import	0.125000
the type's :attr	gpuarray gpu array	0.062500
function that allows replacing subgraphs	compile rebuild collect shared outputs inputs	0.500000
the dimensions of	tensor py	0.015873
compute 1d kernel for bilinear upsampling	tensor nnet bilinear	0.111111
neq	neq	0.625000
useless dimshuffle operation inside reshape reshape(vector	local useless dimshuffle in reshape node	0.500000
memory alias that wasn't in	view	0.022727
only used to determine the broadcast pattern	tensor adv index broadcastable pattern a idx	0.066667
get a memo a dict that	gof function	0.043478
converts self _grad_op from user supplied form	compile op from graph recompute grad op	0.200000
scan return	push out scan	0.050000
tries to	top_shape	0.137931
inner graph to ensure that	scan validate inner graph	0.035714
dirs	dirs	0.357143
to evaluate because of aliasing and destructive	destroy	0.009709
the dimensions of this	tensor tensor py operators	0.015625
bartlett spectral window in the	bartlett	0.058824
and return full path of the dynamic lib	module name from	0.076923
this generates the c code for gpucorrmm (direction="forward"),	gpu corr mm c code	0.090909
this function	ext function	1.000000
r t its inputs	wrt inputs output_grad filters input_shape filter_shape	0.333333
object a that	gof pure	0.033333
to expm1 a	expm1	0.050000
hash equal for	type hash	0.166667
uses shared variable names when persisting	shared variable id	0.142857
remove broadcastable dimensions from	tensor py operators squeeze	0.200000
b are unified given the	b	0.014925
the op	profile stats op	0.500000
the c code for this composite op	scalar composite init c code	0.333333
the bartlett spectral window in the time-domain	bartlett m	0.083333
a hash	hash	0.055556
multinomial distributions	multinomial random_state	0.040000
lib directories that are needed	header dirs	0.045455
this return the initial value for	gpu	0.011765
help the navigator deal with	gof navigator optimizer	0.038462
connection pattern of subfgraph defined	graph connection pattern node	0.076923
we do it	node	0.007407
of this op could be very easy	l op	0.033333
functiongraph that	gof	0.002381
cholesky op then replace it with a triangular	triangular	0.076923
replace_all_validate revert the replacement if the ops	replace all validate	0.111111
data by walking the cache	module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
object with debug	with op node	0.166667
f wrt to wrt	rop f wrt	0.200000
treeset	treeset	1.000000
factored	factored	1.000000
x	tensor nnet local	0.400000
same kinds of tensortype	type	0.011905
of this op could be very	op	0.009174
for a convolution with the specified	gpu dnn conv get	0.200000
extract test value from v raises attributeerror if	gof get test value v	0.250000
:param execute if true execute a	misc execute execute verbose m n	0.250000
and "code_cleanup" together	cleanup node	0.142857
coll	coll	1.000000
from the loaded cache or	gof module cache get	0.250000
all asserts from the	all assert	0.250000
of ops contained	gof ops	0.083333
olv_index	olv_index	1.000000
feature	gof feature	0.125000
replacement if the ops in the	replace all	0.050000
the grad of this op could be very	tensor prod l op	0.033333
det x and there is already an l=cholesky	local det	0.166667
apply node	d3viz apply	0.333333
a convolution with	dnn conv get	0.100000
see theano tensor sum	py operators sum axis	1.000000
post some text to a	misc post	0.200000
using cudnn from nvidia	gpuarray dnn conv3d img kerns	1.000000
input that	bad	0.013158
takes as input a n-d tensor where	tensor signal pool 2d input ws ignore_border stride	0.100000
(function input_variables output_variables) where function is a thunk	thunk	0.021277
getter method for self _rop_op	get rop op	1.000000
to manipulate the	r new_r reason verbose	0.071429
searchsorted	searchsorted	1.000000
a dimshuffle which only adds dimension to	tensor local dimshuffle	0.052632
the output	dims output	0.333333
an op	clinker op	0.071429
perform the permutation by doing a recursion over	tensor permute row elements rec perform node x	1.000000
with debug	with	0.076923
output of scan return true iff	out scan	0.035714
sample from one or more	random_state size	0.250000
beta * y + alpha * dot	gemv c code y	0.333333
dimensions of	tensor	0.006431
the form x[0 :] -> x[0]	local useless slice node	0.250000
is	path importer is	0.250000
index array and a set of arrays	a choices out mode	0.111111
diagonal of an empty matrix it does	diag	0.023810
optional return true for small or builtin	is simple	0.200000
3d convolution for	base abstract conv conv	0.125000
list of lib directories that are	header dirs	0.045455
a symbolic row variable	tensor row	0.050000
this generates the c code for corrmm	nnet base corr mm c code	0.090909
wait on a previously	mpisend wait	0.045455
constants and the rest	rest inputs	0.125000
helper function for diagonalsubtensor	get diagonal subtensor	0.083333
cycle	cycle	1.000000
a thunk that operates	gof linker make thunk	0.045455
return a reshaped view/copy of this	tensor py operators reshape shape ndim	0.111111
alias that wasn't in	view	0.022727
x y idx idx) -> y	local	0.014085
object but we don't clone the	constant clone	0.250000
that with the *args directly	sparse local csm properties csm	0.142857
replace_all_validate revert the replacement if the	replace validate replace all validate	0.111111
max_recur	max_recur	1.000000
return label of apply node	d3viz apply label node	0.500000
header	header	0.700000
this function compute the output	tensor nnet	0.017544
a helper	diagonal a	1.000000
set and dictionary data structures	non	0.071429
this	core	0.111111
cache if available	call cache call fn args key	0.200000
reshapes the output after pad_dims	unpad dims output	0.333333
every	every	1.000000
gpu convolution using cudnn from nvidia	gpuarray dnn conv3d img kerns border_mode subsample	0.500000
dictionary of arguments to pass	args	0.025641
of the __unify_walk__ method for	gof unify walk a b u	0.037037
confusion matrix of	confusion matrix	0.166667
reorder	py operators dimshuffle	0.019231
not the other implementation of mod	scalar mod c	0.125000
this type	gof clinker type	0.066667
execute	gof function graph execute	1.000000
to the provided l{functiongraph} it	gof optimizer apply	0.166667
the dimensions of this variable optionally inserting broadcasted	py operators	0.015625
user's home directory	core get home dir	1.000000
along the axis that was used	inputs g_outputs	0.090909
of this variable optionally	tensor tensor py operators	0.015625
scan return true iff the	scan_module push out scan	0.050000
a mode	mode	0.062500
over each shape that broadcast them to	generate broadcasting	0.066667
compute conv output gradient	tensor nnet conv2d grad	0.333333
cross-entropy between an approximating distribution and a true	tensor nnet categorical crossentropy coding_dist true_dist	0.111111
graph leading to r to given depth	r prefix depth done	0.500000
performs batch normalization of	batch normalization	0.125000
context associated with a	get context	0.111111
reorder the dimensions of this variable optionally	py	0.014286
type that	type	0.011905
a context by mapping it to a name	gpuarray reg context name ctx	0.500000
of a compiled theano function's ops supports	core pydotprint fct outfile compact format	0.250000
reorder the dimensions of this	tensor	0.006431
rows	rows	0.750000
return connection pattern of subfgraph defined	op from graph connection pattern node	0.076923
convert addsd to	sparse local addsd ccode node	0.250000
replacement if the	gof replace validate replace	0.050000
the view_map	bad	0.013158
used is the	gof	0.002381
attempting to use dnn conv workmem	safe no dnn workmem workmem	0.166667
es of a	dtype	0.022727
a list of shape tuple or	shape feature default infer shape	0.066667
neibs2images <theano sandbox neighbours neibs2images>	tensor nnet neibs2images neibs neib_shape original_shape mode	0.333333
with	like	0.111111
baddestroymap if	node	0.007407
row variable (ndim=2 broadcastable=[true false])	tensor row name	0.050000
the c code for corrmm (direction="forward"), corrmm_gradweights	corr mm c code	0.090909
leaf of a multiplication	leaf	0.066667
output dimensions of	op get output	0.047619
on the inputs and put	pure op perform node inputs output_storage params	0.047619
represent the dependence of nodes in	gof make dependence	0.043478
a and b are unified given the	a b	0.066667
the existence of the __unify_walk__ method	gof unify walk a b u	0.037037
this function compute	tensor nnet	0.035088
the convolution gradient with respect to the inputs	dnn conv grad i	0.125000
the replacement if the ops in	replace all	0.050000
return an iterable	gpuarray gpu kernel base gpu kernels node name	0.166667
maximum	maximum x	0.142857
to reps	reps ndim	0.500000
inputs according to	inputs	0.012658
v by a with a modulo	v	0.011111
ones	ones	1.000000
outputs defined by indices out_idxs	out_idxs	0.050000
shape of	shape image_shape	0.500000
symbolic row variable (ndim=2	row	0.034483
inputs and put the variables in	gof pure op perform node inputs output_storage params	0.047619
true if l has any duplicates (according to	has duplicates l	0.111111
to the apply to be	apply node	0.031250
implements the "reverse-mode" gradient	grad perform node	0.166667
variable v	v	0.011111
or -exp	tensor nnet is	1.000000
transfer	tensor tensor py operators transfer	0.125000
mul	tensor local mul	1.000000
variable on the gpu	as gpuarray variable	0.166667
"init_code"	init code struct	0.125000
important note this	process node fgraph	0.142857
the dimensions of	operators	0.017241
reshaped view/copy of	tensor py operators reshape shape ndim	0.111111
lib directories that are needed by one or	clinker header dirs	0.055556
of lib directories that	lib dirs	0.045455
return c code	name	0.022222
inverse complementary error function for gpu	gpu erfcinv	1.000000
an empty matrix	alloc	0.012500
work around windows behavior that open windows	misc subprocess popen command	0.142857
gpu pooling using cudnn from nvidia	gpuarray dnn pool img ws stride mode	1.000000
a new graph	clone with new inputs inputs	0.166667
crossentropysoftmax1hotwithbiasdx op whose incoming	useless crossentropy softmax 1hot with bias dx	0.111111
apply nodes in the original graph to a	inputs outputs copy_inputs_and_orphans memo	0.029412
to make itself the default python	to os environ pathlist var	0.038462
to write	gpuarray write	0.200000
required return c	name	0.011111
the node i pairs such	function graph clients	0.200000
optionally inserting	tensor	0.006431
an alloc of a	alloc node	0.037037
reorder the dimensions of this variable optionally inserting	dimshuffle	0.014493
like make_thunk() but only makes python thunks	py thunk node storage_map compute_map no_recycling	1.000000
output_variables) where function is a thunk that operates	gof linker make thunk	0.045455
scan	push out scan	0.050000
given an fgraph and a	fgraph outputs_to_disown	0.047619
instance associated with a particular	item	0.076923
of localoptimizer and applies	local opt group	0.052632
similar behaviour as haskell' foldr	scan_module foldr fn sequences outputs_info non_sequences	1.000000
det x and there is	linalg local det	0.166667
that wasn't in	bad destroy	0.034483
to sharedvariable instances of suitable dummy values	meta optimizer provide	0.200000
compute sum	constant signature get sum	0.142857
call the supplied function as	as	0.024390
the same op twice gives inconsistent outputs	bad thunk output	0.200000
compiler	compiler	0.833333
denum removes	denum	0.125000
f wrt to wrt	core rop f wrt	0.200000
inserting broadcasted dimensions	operators	0.017241
replace that with the *args directly	local csm properties csm node	0.142857
represent the dependence of nodes in	dependence	0.035714
module is a	compat six meta path importer is	0.250000
comparator	cmp	0.058824
as replace_all_validate revert the	all validate remove fgraph	0.166667
a diff to make code	diff code filename	0.333333
for openblas threads interface	openblas threads text	0.250000
context object mapped to	gpu array type context	0.090909
output dimensions of convolving an image of	nnet conv op get output	0.047619
an input vector and	node	0.007407
new graph	clone with new	0.166667
instances of suitable dummy values	provide	0.100000
image shape of convolution gradweights	get conv gradweights shape 1axis image_shape	0.500000
anymore and should be removed and	outs	0.050000
a variable with a sparse matrix	sparse variable x	0.250000
raised by get_scalar_constant_value if called	error	0.025000
to boundvariable(other_object)	fv o u	0.200000
tangent of a	tensor tan a	1.000000
op	op	0.211009
mflops	base gpu corr3d mm flops inp outp	0.125000
of tensortype	tensor	0.006431
is found	gof is	0.250000
haskell's	outputs_info non_sequences	0.500000
a broadcasted dense vector element wise	sdcsr	0.250000
addition (inplace on a)	tensor add inplace a	1.000000
some requirements to the fgraph this is	requirements fgraph	0.250000
respect to wrt,	subgraph grad wrt	0.062500
move the abs toward the input	tensor local abs lift node	0.333333
generates the c code for corrmm	tensor nnet base corr mm c code	0.090909
of	operators dimshuffle	0.019231
two kinds of useless reshape	useless reshape node	0.200000
dimensionality of the var is equal	is flat var	0.200000
the diagonal of an empty matrix it	diag	0.023810
parametrize it to make it work	max_input_fct maker	0.083333
this	operators dimshuffle	0.019231
c code to extract a	type c extract	0.500000
expanded_inputs	expanded_inputs	1.000000
return a symbolic	ndim dtype	0.333333
deepcopy in	deepcopy	0.125000
context object mapped to the type's :attr context_name	gpu array type context	0.090909
reorder the dimensions of	tensor tensor py operators	0.015625
the same parameter as other scalar op	scalar	0.035714
the updates ordereddict the list of outputs and	scan_module get updates and outputs	0.333333
connection pattern of subfgraph defined by inputs and	op from graph connection pattern	0.076923
for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x i0	0.083333
that runs	gof wrap	0.083333
the topooptimizer from the input nodes to output	in2out	0.043478
that must be	gof	0.002381
outputs inplace	inplace fgraph	0.142857
re-raise an exception while annotating	exc_info storage_map	0.250000
hack in profiling to print the mflops	op flops	0.125000
offers to make itself the default python and	to os environ pathlist	0.038462
svd of a matrix	svd	0.034483
detect if the	gof	0.002381
some text to a gist and	gist	0.040000
pieces of vectors	sparse block outer make	0.066667
stream state and they are spaced by	random streams	0.058824
an unification in u	u	0.100000
along the given axis es of	axis dtype op	0.083333
a with	a replace p	0.250000
computes the sum	sum	0.038462
op that copies	alloc	0.012500
the lock is	lock	0.100000
between i and o parameters	i o	0.083333
that wasn't in	destroy	0.009709
as python not the other implementation of mod	scalar mod c code	0.125000
this function is	constant	0.016667
operation inside reshape reshape(vector	in reshape node	1.000000
to the fgraph	fgraph	0.012195
of specific ops of a compiled graph	trace f_or_fgraph ops_to_check bug_print	0.035714
the c code for corr3dmm (direction="forward"), corr3dmm_gradweights	tensor nnet base corr3d mm c code	0.090909
compile lock to	to	0.017544
convolution with the specified	dnn conv	0.090909
sample from one	random_state size n	0.250000
the tensor operators to	tensor	0.006431
optionally inserting broadcasted dimensions	py	0.014286
tabs	misc hooks	0.250000
a <=	tensor le a	1.000000
of	tensor	0.196141
a series	wrap linker many linkers wrappers	0.047619
output shape for a convolution with the specified	gpu dnn conv get out shape	0.142857
false we can't change the value after	init default filter	0.040000
if a dimshuffle is inside an alloc and	tensor local alloc dimshuffle node	0.166667
localoptgroup instead of a	group	0.047619
their apply_node if	gof	0.002381
sparse format	sparse	0.019231
the folowing changes in	tensor local mul switch sink node	0.045455
grad of	grad	0.010417
stack trace	stack trace	0.055556
implements numpy's isclose on	isclose a	0.500000
compiles the source code for	compile cmodule location	0.038462
remove two kinds of useless	local useless	0.111111
given a inner	inner sitsot	0.083333
graph and get a memo a	function graph	0.040000
start gradients up to the end variables	end start	0.166667
function performs the matrix inverse	matrix inverse	0.111111
n-d tensor where n	ws ignore_border stride	0.090909
of the type	tensor type	0.034483
r shape[i] for tensor variable r int i	tensor shape feature shape ir i r	0.500000
the output	output input	0.333333
of the specified pieces of vectors and matrices	sparse block gemv make	0.066667
nodes of the	gof	0.002381
than numpy round	round	0.076923
gradients up to the end variables	end	0.040000
dimensions of this variable optionally	tensor py operators dimshuffle	0.019231
the fgraph outputs	fgraph expanded_inputs	0.058824
not support the types involved in this node	tensor inc subtensor do type checking node	0.250000
offers to make itself the default python and	to os environ	0.038462
mflops	corr mm flops inp outp	0.125000
elemwise round half to even of	sparse rint	0.250000
the grad of this op could	l op	0.033333
unfortunately conda offers to make	to os environ pathlist	0.038462
operators	tensor	0.006431
is a thunk that operates	gof linker make thunk	0.045455
if target is 'cpu' this will transfer to	transfer target	0.500000
return complex-valued tensor with real and imag components	tensor complex real imag	1.000000
same kinds	type	0.011905
slice [start	slice	0.038462
+ alpha * dot	tensor gemv c code	1.000000
a tuple of integers indicating the version	code cache version	0.125000
important note this function uses	seq scan process node	0.142857
all sigmoid	sigmoid node	0.100000
elemwise maximum see max for the maximum in	tensor maximum	0.142857
to split x	tensor split grad	0.500000
unlocker	unlocker	1.000000
or more multinomial distributions defined	multinomial	0.024390
a meta path importer to import	meta path importer	0.166667
version of var transferred to target	tensor transfer var target	0.200000
the specified pieces of vectors and matrices	sparse block outer make node o x	0.066667
subclass to add the tensor operators to	tensor	0.006431
variables and apply_nodes to this graph	gof function graph	0.031250
the inner graph to ensure that	scan_module scan validate inner graph	0.035714
this preserve some variables attributes and tag	preserve variable attributes	0.500000
in the	bad view	0.027027
op that copies a vector to the diagonal	diag	0.023810
not attempting to use dnn conv workmem	core safe no dnn workmem workmem	0.166667
all the node i pairs such that node	gof function graph clients	0.100000
with a modulo of m1	m1	0.027027
message on	msg	0.083333
string (comma-separated key=value components) into a dict	string config_string issue_warnings	0.333333
some elementary validations on the inner	scan_module scan validate inner	0.142857
total	total	1.000000
minlength	minlength	1.000000
and argmax over a given	and argmax	0.166667
and "init_code"	struct node name sub	0.500000
and return full path of	gof module name	0.076923
optimize the possible advsub1(advincsub1	local adv sub1 adv inc sub1 node	1.000000
profilemode to print the mflops	gpuarray base gpu corr mm flops inp outp	0.125000
denum	denum	0.750000
the max and average pooling	pool	0.066667
c code when doing constant folding	tensor elemwise python constant folding	0.142857
a with b	a	0.008065
generates the c code for gpucorrmm	gpu corr mm c code	0.090909
uses the	gof	0.002381
wait on a previously sent array using mpi	wait	0.022727
wait on a previously received array using mpi	mpirecv wait	0.045455
given an apply_node recursively search from this	import apply_node	0.066667
it work for elemwise and gpuelemwise op	local elemwise fusion op op	0.200000
this op	gof clinker object c	0.500000
2d kernel for bilinear upsampling this function	bilinear	0.019231
convert degree a to radian(inplace on a)	deg2rad inplace a	1.000000
that can be specialized for the	gof	0.004762
unfortunately conda offers to make	to	0.017544
converts	compile char from	1.000000
return label of	label	0.111111
idx_list with constant inputs replaced by	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
normal	random streams base normal	0.500000
attempt to convert x into a variable on	gpuarray variable x context_name	0.166667
form [0 0	crossentropy categorical1hot	0.166667
mod	mod c code node name	0.125000
remove reduction over broadcastable dimensions	tensor local reduce broadcastable	1.000000
reorder the	tensor tensor	0.014286
into macros for use	cop	0.028571
if those nodes	gof	0.002381
a base class with a metaclass	with metaclass meta	0.333333
for any python object a that would	gof pure	0.033333
of _maker which handles much of the	maker i o m	0.066667
on the inner graph	validate inner graph	0.035714
return true for small or builtin c types	gof clinker type c is simple	0.250000
setsubtensor(x x[idx], idx) -> x when x is	tensor local setsubtensor of constants node	0.250000
hyperbolic cosine of a	tensor cosh a	1.000000
the dependence of nodes in	make dependence	0.043478
operation on f	rop f	0.166667
broadcasting	broadcasting	1.000000
gradient w r t its weights	grad wrt weights input output_grad filter_shape input_shape	0.333333
only used to determine	adv index broadcastable	0.050000
some perform() or c_code() modified	destroy map	0.142857
replace_all_validate revert	validate	0.090909
ger	ger	1.000000
to choose	choose	0.111111
to the apply to be inserted in the	apply	0.016667
create an functiongraph	function graph	0.040000
represents their unification	gof unification	0.125000
a previously sent array using mpi	mpisend	0.037037
sum(a axis={ }) / b	sum prod div dimshuffle	1.000000
instance of _maker which handles much of the	debug mode function maker i o m	0.066667
the largest eigenvalue of square symmetrix matrix	sandbox linalg spectral radius	0.166667
tuple of symbolic shape vars for tensor variable	feature shape tuple	1.000000
gradients of cost	cost	0.045455
expression vector 1-dimensional	jacobian expression	0.500000
the idx_list with constant inputs replaced by their	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
computes the standard deviation along	tensor std	0.111111
diff to make code	diff code filename	0.333333
required return the c implementation of an op	op c code node name inputs outputs	0.500000
c code to	gof clinker op c code	0.333333
function that gets a scan op	op not_required inputs	0.071429
how to generate c	register shape i c	0.250000
to make itself the default	to os environ pathlist var newpath	0.038462
build_inv	build_inv	1.000000
tell specifyshape how to generate c code for	shape c code typ code version c_support_code_apply	1.000000
more multinomial distributions defined by one-dimensional	tensor multinomial random_state	0.040000
confusion matrix	nnet confusion matrix	0.166667
toposort return an ordering	function graph toposort	0.125000
convolution gradient with respect	dnn conv grad	0.125000
of scan return true iff the	out scan	0.035714
converts self _rop_op from user supplied form	compile op from graph recompute rop	0.200000
perform	perform	0.352941
specified factor takes as input a	tensor signal pool 2d input	0.090909
if fgraph	fgraph	0.012195
enabled change all sigmoid to ultra_fast_sigmoid	ultra fast sigmoid	0.200000
the equivalent of localoptgroup for	gpulocal opt group	0.055556
function that gets a scan op a list	op not_required	0.071429
to the fgraph outputs that will replace	fgraph	0.012195
schedule	gof sort schedule	0.333333
op scale or	scale	0.047619
2d or 3d convolution for debugmode	tensor nnet base abstract conv conv	0.125000
a	a replace	0.666667
revert the replacement if	replace	0.032258
ddof	ddof	1.000000
of scan return	out scan	0.035714
code string specific to the apply to be	apply node	0.031250
of average	average	0.200000
reorder the dimensions of this variable optionally	tensor py operators dimshuffle	0.019231
to concatenate tensortypes	join	0.090909
dependence of	dependence	0.035714
of apply nodes according to a	gof sort apply nodes inputs outputs cmps	0.050000
disconnected	disconnected	1.000000
of each value in array of ints	weights minlength assert_nonneg	0.125000
replacements	replacements	0.555556
a slice [start stop step]	slice	0.038462
fgraph this	fgraph	0.012195
true if l has any duplicates (according	scan_module has duplicates l	0.111111
of this mode	mode	0.125000
comparator to represent the dependence of	dependence cmp	0.111111
reshapes the input	input	0.023810
a <= b inplace on a	tensor le inplace a b	0.500000
step	step	1.000000
this compiles the source	cmodule location	0.038462
extract test value from v raises	test value v	0.250000
if	inputs	0.012658
to the apply to be inserted	apply	0.016667
python not the other implementation of mod	scalar mod c code node name	0.125000
important note this function uses set	scan process node fgraph	0.142857
g++	gof	0.002381
the user is not attempting to use dnn	core safe no dnn	0.125000
more multinomial distributions defined by	multinomial	0.024390
directories that are	dirs	0.071429
and only if this enum has this	gof enum type has	0.111111
division inverse	div	0.166667
inner-most loop executes code	make reordered loop	0.111111
wait on a	mpirecv wait	0.045455
exc_info	exc_info	1.000000
this preserve some variables names during optimization	preserve names	1.000000
compiled module from the loaded cache or	cache get module	0.166667
fourier transform of a real-valued input on	gpuarray curfft inp norm	0.066667
previously received array	mpirecv	0.037037
proxy for	scalar div proxy	0.125000
outputs of specific ops of	trace f_or_fgraph ops_to_check bug_print	0.035714
that initializes py_name to py_none	gof get c init r	0.250000
this op could be very easy if	l op	0.033333
the grad of this op could be	tensor prod l op	0.033333
connection	connection	0.700000
functiongraph listeners to help the navigator	gof navigator optimizer attach	0.038462
standard elements of an op	object	0.083333
to	tensor make constant	0.100000
unique names to an iterable of variables	variables names variables	0.333333
print a warning message on	gof deprecated filename msg	0.041667
folowing changes in the graph t	tensor local mul switch sink node	0.045455
string representation of	to str b	0.250000
to compute the image shape of convolution gradinputs	nnet get conv gradinputs shape kernel_shape	0.500000
generates the c code for corr3dmm (direction="forward"),	nnet base corr3d mm c code	0.090909
comparing	constant	0.016667
canonical form that respects the	get canonical form	0.045455
remove two kinds of useless	useless	0.076923
duplicate this apply instance	gof apply clone with	0.166667
in profiling to print the mflops	conv op flops inputs	0.125000
in which each row is a mrg stream	sandbox mrg random streams	0.033333
indicating the version	code cache version	0.125000
first outdim-1 dimension size s of x the	x ndim outdim	0.333333
c_init that	gof get	0.100000
as replace_all_validate revert the replacement if the	gof replace validate replace all validate	0.111111
the context object mapped to the type's	gpuarray gpu array type context	0.090909
of the __unify_walk__ method for one of the	gof unify walk a b u	0.037037
return the [elementwise] smallest of	tensor smallest	0.333333
one or more multinomial distributions defined by one-dimensional	tensor multinomial random_state	0.040000
-> sum(a axis={ }) / b	tensor local sum prod div dimshuffle	0.333333
takes as input a 4-d	signal max pool 2d same size input patch_size	0.250000
and its idx_list reorders	idx_list get_count	0.090909
which will print a warning message on	gof deprecated filename msg	0.041667
we can replace that with the *args directly	local csm properties csm	0.142857
version	c code cache version apply node	0.125000
replacement if	validate replace	0.050000
transfer function	transfer fn	0.125000
override clinkertype c_declare	tensor tensor type c declare name sub check_input	1.000000
module_hash	module_hash	1.000000
the initial value for myresult	gpuarray gpu careduce cuda assign init first_item	0.166667
graph is impossible to	destroy	0.009709
a slice [start	slice	0.038462
a view	view tree set	0.500000
that v	set v	0.125000
reproducible case for	compile function dump filename	0.166667
maps old nodes	equiv check_integrity attach_feature	0.200000
clone the graph and get a	function graph clone get	0.333333
crossentropysoftmax1hotwithbiasdx op whose incoming gradient	useless crossentropy softmax 1hot with bias dx	0.111111
log(softmax x and replace it with logsoftmax x	tensor nnet local logsoftmax node	0.142857
this function	gof ext function	1.000000
return the function name to write data	gpuarray write w dtype	0.200000
variable v is positive semi-definite i e	v	0.011111
getter method for self _rop_op	op from graph get rop op	1.000000
stack trace from one or more	copy stack trace	0.055556
the minimum in one	tensor minimum x y	0.090909
square root of a (inplace on a)	tensor sqrt inplace a	1.000000
full path of the dynamic lib in	module	0.033333
that need not be checked for nan	is numeric value arr var	0.166667
op could be	tensor prod l op	0.033333
dot22 computing an outer-product -> ger	tensor local dot22 to ger or gemv	1.000000
must be always true	ws	0.166667
initializes py_name to py_none	c init r name sub	1.000000
use complex numbers	mod check x y	0.166667
the inner-most loop executes	reordered loop	0.111111
alias that wasn't in	bad view	0.027027
batch normalization of	batch normalization	0.125000
internal function that constructs a new variable from	scan_module safe new	0.333333
apply instance in a	apply	0.016667
metaclass	add metaclass metaclass	0.125000
pieces of vectors and	sparse block outer make	0.066667
return a c contiguous version of the input	contiguous	0.058824
exception object with	raise with op node	0.333333
the "reverse-mode" gradient	grad perform node	0.166667
gpuincsubtensor	setsubtensor	0.111111
the context object mapped to	gpuarray gpu array type context	0.090909
in profilemode to print the mflops	gpuarray base gpu corr3d mm flops inp outp	0.125000
hyperbolic arc tangent of	tensor arctanh	1.000000
as input makes it use new_r instead	replace all pairs reason	0.333333
lazy loading of moved objects in six moves	six moves urllib error	0.142857
it only on cpu here	pow specialize	0.250000
convolution with the specified	dnn conv get	0.100000
upper	tensor triu	1.000000
tensorvariable whose type	as	0.024390
a canonical form that respects	canonical form	0.045455
elements obtained by iterating over given axis	tensor min x axis keepdims	0.500000
if allow_override is false we can't change	filter allow_override	0.142857
on cpu	a full_matrices compute_uv	1.000000
to help the navigator deal	navigator optimizer attach updater	0.038462
tensor from	from	0.050000
transfer to a tensortype if not already	tensor py operators transfer	0.125000
method is primarily used by tensor rop	pure op r op inputs eval_points	0.125000
bartlett spectral window in	bartlett m	0.083333
to help the navigator deal	gof navigator	0.038462
this convert allocempty to alloc of 0	tensor local alloc empty to zeros	0.333333
a canonical form	canonical form	0.045455
convolution gradient with respect to the weights	dnn conv grad	0.062500
python object a that would	gof	0.002381
function is basically a call to tensor get_scalar_constant_value	tensor extract constant x elemwise only_process_constants	0.058824
in	replace	0.032258
op could be very	l op	0.033333
register a transfer function for alternative	register transfer fn	0.250000
replace that with the *args directly	sparse local csm properties csm node	0.142857
work for elemwise and gpuelemwise	local elemwise fusion	0.166667
required return data	gof pure	0.033333
of shape tuple	shape feature default infer shape	0.066667
if	compile	0.076923
to represent the dependence of nodes in a	gof make dependence	0.043478
data structures	out non	0.125000
determine the broadcast pattern for advancedsubtensor output variable	tensor adv index broadcastable pattern a	0.066667
accept complex otherwise call upgrade_to_float()	scalar upgrade to float no complex	0.333333
print the mflops	conv op flops inputs	0.125000
an exception class to raise in self	core raise init	0.100000
global	global	0.714286
helper function to	helper random_state low high	0.500000
specific ops of a compiled	trace f_or_fgraph ops_to_check bug_print	0.035714
get	type get	0.050000
optimizer that reduces scan memory consumption	scan save mem	0.200000
this apply instance in a new	gof apply clone with new inputs inputs strict	0.250000
hack in profiling to print the mflops	tensor nnet conv op flops inputs outputs	0.125000
input by a specified factor takes as	tensor signal pool	0.142857
tensor_from_scalar(scalar_from_tensor	tensor scalar tensor node	1.000000
reorder the dimensions of this variable optionally inserting	tensor py operators	0.015625
half by b	b	0.014925
dot22 computing an outer-product ->	local dot22 to	1.000000
to compile c code when doing constant folding	python constant folding	0.142857
the list remove are still in	replacements remove reason	0.055556
returns a module	module	0.033333
of this variable	tensor py	0.015873
sin	sin	0.714286
function	tensor	0.003215
implement the same rounding than numpy round	round	0.076923
updates ordereddict the list of outputs and the	scan_module get updates and outputs	0.333333
fill a with b	second inplace a	0.333333
to replace a leaf of a multiplication tree	replace leaf	0.100000
in the cache	cache	0.034483
/ dimshuffle{ } b axis=l) ->	tensor local	0.025641
to a new node a clone	clone	0.020833
pieces of vectors and matrices	sparse block outer make node o x	0.066667
reverse-mode gradient updates for matrix solve	solve grad inputs output_gradients	0.333333
is an alloc of a scalar variable	alloc node	0.037037
one-dimensional slices in pvals	n pvals	0.125000
writeme	from function optimizer	1.000000
replacement if the ops in	validate replace all	0.050000
the computation to be	node	0.007407
attempt to replace a leaf	nnet replace leaf arg leaves new_leaves op	0.250000
output	get output	0.047619
retrive the context associated with a name	gpuarray get context name	0.333333
return the cumulative sum of	tensor cumsum x	0.333333
transfer to a	tensor py operators transfer	0.125000
merge multiplication by a scalar on the	alpha merge	0.076923
optimization local_useless_rebroadcast and local_rebroadcast_lift	rebroadcast opt rval	0.200000
a contiguous buffer	gpuarray inline reduce n buf pos count	0.142857
type's	gpuarray gpu	0.045455
updates ordereddict the list	get updates	0.034483
makes the folowing changes in the	mul switch sink node	0.045455
batched tensordot product	tensor batched tensordot x y axes	0.333333
full path of the dynamic lib in	gof module	0.058824
and apply nodes in the original graph	inputs outputs copy_inputs_and_orphans memo	0.029412
cache data by walking the cache directory structure	module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
access of a given	access time	0.200000
makes the folowing changes in	mul switch sink	0.045455
of this variable optionally inserting broadcasted dimensions	py	0.014286
supplied function as its	compile as	0.050000
the	operators	0.017241
is basically a call to tensor get_scalar_constant_value	extract constant x elemwise only_process_constants	0.058824
a particular	getitem item	0.125000
given axis	x axis	0.400000
is primarily used by tensor	pure op r op inputs eval_points	0.125000
that removes all	tensor local remove all	0.166667
list of shape	feature default infer shape	0.066667
tries	image_shape top_shape border_mode subsample	0.166667
outputs according to the flags use_list and use_tuple	core format as use_list use_tuple outputs	1.000000
of this variable optionally inserting broadcasted dimensions	tensor py operators dimshuffle	0.019231
associated with a particular stream	random streams setitem item val	0.142857
an ndarray	ndarray data	1.000000
according to the flags use_list and use_tuple	core format as use_list use_tuple	1.000000
of apply nodes	apply nodes	0.200000
perform the permutation by doing a recursion over	tensor permute row elements rec perform node	1.000000
the mflops	base gpu corr3d mm flops inp outp	0.125000
recognize the updates ordereddict the	updates	0.029412
we can't change the value after the import	init default filter	0.040000
apply to be inserted in	apply node	0.031250
iter	iter	1.000000
the alloc	tensor alloc	0.333333
more multinomial distributions defined	multinomial	0.024390
lazy loading of moved objects in	moves urllib error	0.250000
this object but we don't clone the data	gof constant clone	0.166667
the bartlett spectral window in	bartlett m	0.083333
if cond then ift else iff	tensor switch cond ift iff	0.500000
c code for corrmm (direction="forward"), corrmm_gradweights	tensor nnet base corr mm c code	0.090909
suitable dummy values	provide	0.100000
moved objects in six moves urllib_robotparser	six moves urllib robotparser	0.333333
the 2d kernel that can be used	kernel 2d	0.050000
a crossentropysoftmax1hotwithbiasdx op whose incoming gradient	local useless crossentropy softmax 1hot with bias dx	0.111111
repeat	repeat	1.000000
it into a canonical	canonical	0.076923
revert the replacement if	replace validate replace	0.050000
obtain lock on compilation directory	gof get lock lock_dir	1.000000
access of a given file	access	0.100000
main diagonal set to a	tensor fill diagonal offset a val offset	0.100000
return the	name	0.011111
trace from one	trace	0.052632
like zeros_like but forces the	core float zeros like	0.200000
print the following stats	compile print global stats	1.000000
determine the broadcast pattern for advancedsubtensor output variable	adv index broadcastable pattern a	0.066667
only one	only	0.050000
to print the mflops	nnet conv op flops inputs	0.125000
should decref	fail	0.166667
the inputs required to compute the given variables	gof inputs variable_list blockers	0.058824
can't change the value after the import of	default filter	0.040000
constant representing a value on a certain	array constant	0.500000
list of lib directories that are needed by	header dirs	0.045455
converts samples from a uniform into sample	uniform	0.086957
implements the "reverse-mode" gradient for the eigensystem of	tensor eigh grad perform node inputs outputs	0.333333
for graphtogpu	opt	0.043478
minimum	minimum	0.500000
around c_init that initializes py_name to py_none	gof get c init r name	0.250000
optionally	py	0.014286
remove incsubtensor when we overwrite	local useless inc subtensor node	0.066667
of nodes that must be	gof	0.002381
real-valued input on the	gpuarray curfft inp norm	0.066667
up to the end variables	grad wrt end	0.050000
and converts this to expm1 a	local expm1 node	0.066667
main diagonal set to a specified	tensor fill diagonal offset a val offset	0.100000
called by functiongraph attach_feature the method that attaches	gof bookkeeper on attach	0.142857
connection pattern of	compile op from graph connection pattern node	0.076923
"reverse-mode" gradient [1]_ for the cholesky factorization	cholesky grad perform node inputs outputs	0.500000
transfer data to cpu	host from gpu	1.000000
to wrt, computes gradients	subgraph grad	0.062500
tensorconstant instances	tensor	0.003215
search through a graph either breadth- or depth-first	gof stack search start expand mode build_inv	1.000000
start gradients up to the	start	0.040000
to generate permutations from integers	permutation	0.090909
will be turned into macros	cop	0.028571
folowing changes	mul switch sink node	0.045455
needed but will not be deleted after	tmp_dir timeout min_wait max_wait	0.500000
input	bad destroy	0.034483
reorder the dimensions of this	tensor tensor py	0.015873
the standard deviation along the	std	0.058824
replacement if the ops in the list	replace validate replace	0.050000
the output dimensions of convolving	output	0.017241
registered context names	gpuarray list contexts	1.000000
is no change in the shape of the	node	0.014815
none	i_shapes	0.050000
context object mapped	gpu array type context	0.090909
2d inputs with a set of 2d filters	filters	0.064516
can't change the value after the import	config param init default filter	0.040000
elements of the main diagonal set to a	fill diagonal offset a val offset	0.100000
module initialization	init	0.058824
cpu	full_matrices compute_uv	1.000000
multinomial distributions defined by one-dimensional slices	multinomial	0.024390
gradients along the axis that was used	inputs g_outputs	0.090909
when allow_gc = false clear the	compile function free	0.250000
tensor filled with ones closer	tensor ones shape dtype	0.250000
compute conv output	nnet conv3d	0.142857
the dimensions of this variable optionally	dimshuffle	0.014493
dict	d3viz dict	0.333333
after a given	default	0.030303
of lib directories	lib dirs	0.045455
iff	iff	1.000000
is a	compat six meta path importer is	0.250000
this op could be	l op	0.033333
-x pattern	is neg var	0.166667
like make_thunk() but only makes python thunks	thunk node storage_map compute_map no_recycling	1.000000
the updates ordereddict	scan_module get updates	0.034483
c	compile register shape i c	0.250000
updater	updater	1.000000
computes the standard deviation	tensor std	0.111111
tuple of integers indicating the version	c code cache version apply node	0.125000
raised when grad	grad error	1.000000
add	add node	1.000000
not attempting to use dnn conv workmem	dnn workmem workmem	0.166667
list of op classes	local optimizer tracks	0.200000
add two matrices at least one of which	add x y	0.333333
some functiongraph listeners to help the navigator deal	navigator optimizer	0.037037
library	library	1.000000
that	gof cthunk	1.000000
construct a variable with	variable x	0.083333
performs the matrix	matrix	0.055556
dictionary data structures	scan_module push out non	0.125000
the cache and none otherwise	cache	0.034483
with a modulo of m1 and the	m1	0.027027
returns a short mostly hexadecimal hash of a	core hex digest x	0.083333
ones with	ones like	0.333333
out the	out	0.018519
of header	header	0.100000
elementary validations on the inner graph to ensure	scan_module scan validate inner graph	0.035714
"reverse-mode" gradient	grad perform node inputs	0.166667
bessel function of	tensor j0 x	1.000000
transform of a real-valued input on the	curfft inp norm	0.066667
one or more multinomial distributions defined	tensor multinomial random_state	0.040000
abstract class	navigator optimizer	0.037037
x the same	tensor flatten x	0.166667
changes node inputs[i] to	change input node i	0.250000
and the	and	0.111111
instance of _maker which handles much of	debug mode function maker i o m	0.066667
output dimensions of	tensor nnet conv op get output	0.047619
clone the graph and get a	graph clone get	0.333333
remove reduction over broadcastable dimensions	local reduce broadcastable	1.000000
tensor	operators	0.017241
small or builtin c types	type c is simple	0.250000
modified bessel function	i0 x	1.000000
i of	tensor shape feature set shape i r i	0.500000
a short mostly hexadecimal hash of a	core hex digest x	0.083333
split for gpu	gpu split	1.000000
implementation of mod	mod c code	0.125000
specific to the apply to	apply node	0.031250
inside the scan that depend only on non-sequences	non seq scan	0.090909
function for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x	0.083333
idx_list with constant inputs replaced by their python	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
fgraph outputs that	fgraph expanded_inputs	0.058824
allocate initialized memory	alloc	0.012500
a broadcasted dense vector element wise	svcsr	0.090909
all unknown variables and apply_nodes to this graph	graph import	0.125000
new node a clone in a new	clone	0.020833
nb_class	nb_class	1.000000
elementwise conjugate	conj	0.100000
that allows replacing subgraphs of	clone output replace strict share_inputs	0.071429
pass to helper_c_code	tensor inc subtensor get helper c code	0.250000
upper bound on	bound	0.043478
dimensions of this variable	dimshuffle	0.014493
this generates the c code for corrmm	base corr mm c code	0.090909
compute 1d	tensor nnet	0.017544
apply nodes according to a	apply nodes inputs outputs cmps	0.050000
of specific ops of a	trace f_or_fgraph ops_to_check bug_print	0.035714
to use with helper_c_code	gpuarray gpu inc subtensor get helper c code	1.000000
r to new_r	r	0.028571
a tuple of integers indicating the version	code cache version apply	0.125000
along the given axis es of a	axis dtype op	0.083333
it with logsoftmax x	local logsoftmax node	0.142857
validations on the inner graph to ensure	inner graph	0.035714
a metaclass	compat add metaclass metaclass	0.125000
to construct a variable with a	variable	0.022222
basic theano op that	op itypes otypes infer_shape	0.047619
row is a mrg	sandbox mrg	0.125000
dot product followed by	dot modulo	0.250000
filter	filter	1.000000
-1 and converts this to expm1	tensor local expm1	0.066667
the same on all device we do it	device node	0.045455
raise	raise init	0.200000
apply_node if	gof	0.002381
_maker which handles much	function maker i o m	0.066667
theano gof graph	graph	0.016393
macros for use within the op	cop get op params	0.200000
of maxandargmax	max and argmax	0.125000
2d or 3d convolution	conv conv	0.250000
parameters ----------	gpu dnn pool	0.500000
in profiling to print the mflops	op flops inputs	0.125000
dynamic	dynamic	1.000000
navigator deal with	navigator optimizer attach	0.038462
1/(1+exp x -> sigm -x	tensor nnet local inv 1 plus exp node	0.333333
unification in u	u	0.100000
work	work	1.000000
comparator to represent the dependence of nodes in	make dependence cmp	0.111111
create a new random stream in this container	random streams gen op	0.250000
function for diagonalsubtensor and	get diagonal subtensor view x i0 i1	0.083333
of _maker which handles much of	debug mode function maker i o m	0.066667
merge 2	merge	0.071429
kind of order v real	v	0.022222
shape of all intermediate variables given input shapes	tensor shape of variables fgraph input_shapes	0.100000
1/(1+exp x -> sigm	tensor nnet local inv 1 plus exp node	0.333333
gradients of cost and/or from existing start gradients	start cost	0.100000
is the llvm one or not	gof gcc llvm	0.200000
only used to determine the broadcast pattern for	adv index broadcastable pattern	0.066667
symbolic variables in inputs to sharedvariable	inputs	0.012658
indices obtained by iterating over	keepdims	0.052632
recognize the updates ordereddict the list of	updates	0.029412
offers to make itself the default	to	0.017544
the fgraph outputs that will replace	fgraph expanded_inputs	0.058824
compiles the source code	clinker compile cmodule location	0.038462
of sparseblockgemv check sparseblockgemv's docstring	sparse block gemv	0.166667
to merge multiplication by	alpha merge	0.076923
output dimensions of	output	0.017241
use_tuple	use_tuple	1.000000
return c code to declare variables that will	gof clinker type c declare name sub check_input	0.333333
takes as input a 4-d tensor it sets	signal max pool 2d same size input patch_size	0.250000
reduce	reduce	1.000000
gradient updates for matrix solve	solve grad	0.250000
erf/erfc opt to track less frequent op	tensor get clients2 node	0.200000
perform the permutation by doing	perform node x y inverse	0.166667
grad of this op could be	tensor prod l op	0.033333
to print the mflops	flops inputs outputs	0.125000
that must be evaluated	gof	0.002381
the navigator	gof navigator optimizer attach updater	0.038462
of 3d inputs with a set of 3d	nnet conv3d	0.071429
merge multiplication by	gpuarray alpha merge	0.076923
output after pad_dims	unpad dims output	0.333333
headers that are needed by one or more	headers	0.038462
|a| tensorvariable overloads	tensor abs a	0.333333
copies the stack trace	copy stack trace	0.055556
hessian	hessian	0.714286
categorical	categorical	1.000000
return permutations of	tensor permutation random_state size n	0.500000
raise baddestroymap	compile	0.076923
of a shared variable to 0	compile shared variable zero borrow	0.200000
modified bessel function	i1 x	1.000000
the list of policies to name r sub	policy policy r name sub	0.250000
to expm1 a	tensor local expm1 node	0.066667
function should	matrix inverse	0.111111
return c	name sub	0.050000
by the inputs and outputs	init inputs outputs	0.166667
more multinomial distributions	multinomial random_state	0.040000
make it work for elemwise and gpuelemwise	tensor local elemwise fusion	0.166667
img2d shape ==3 we todo	tensor nnet conv op perform node inp out	0.166667
inputs and put the	gof pure op perform node inputs output_storage params	0.047619
op	stats op	0.500000
by a broadcasted dense vector element wise	sdcsc	0.250000
hyperbolic arc tangent	arctanh	0.142857
maps a failure code to the task	cthunk find task	0.142857
base 2 logarithm of	tensor log2	1.000000
that wasn't in	bad view	0.027027
input_variables output_variables) where function is a thunk	linker make thunk	0.125000
a particular	item	0.076923
return connection pattern of	compile op from graph connection pattern node	0.076923
important note this function uses	out seq scan process node fgraph	0.142857
lib directories that are needed by	header dirs	0.045455
computes the mean value along	mean	0.062500
the broadcast pattern for	pattern a idx	0.066667
post some text	post	0.100000
reorder the dimensions of this variable optionally	tensor tensor py	0.015873
a list of shape	infer shape	0.066667
epoch of the last access	last access time	0.040000
directory and	dir	0.076923
compiled module from the loaded cache or the	cache get module name	0.166667
an instance of _maker which handles much of	maker i o m	0.066667
a random	random streams base random	0.500000
form that respects the	form	0.111111
this convert allocempty to alloc of	alloc empty to zeros node	0.333333
return selected slices only	tensor py operators compress a axis	1.000000
optimizer for pushing out	out	0.018519
compute	tensor constant signature get	1.000000
merge multiplication by a scalar on the output	alpha merge	0.076923
functiongraph listeners to help the navigator deal	gof navigator	0.038462
the data type for working memory	gpuarray work dtype dtype	0.200000
this function tries	top_shape border_mode subsample	0.166667
try to turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	nnet local softmax with bias node	0.200000
is basically a call to tensor	extract constant x elemwise only_process_constants	0.058824
list of headers that are needed by	headers	0.038462
shape ==3 we todo	nnet conv op perform node inp out	0.166667
dict d s t d[node] is a	function graph orderings	0.200000
arccosine of a	tensor arccos a	1.000000
a tensor from d1 d2 to d1+size d2	scan_module expand empty tensor_var size	0.166667
see theano tensor prod	tensor py operators prod axis dtype keepdims	1.000000
compiles this linker's fgraph	input_storage output_storage storage_map keep_lock	0.250000
of this variable optionally inserting broadcasted	operators	0.017241
set the values of a shared	shared	0.062500
the dimensions of	tensor tensor py	0.015873
to make itself	to os	0.038462
gradient is	grad x	0.333333
this is	graph to gpulocal opt	0.055556
that x and y have the same shape	tensor shape feature same shape x y	0.500000
see theano tensor argsort	tensor tensor py operators argsort axis	1.000000
using	mpirecv	0.037037
apply nodes such that	gof function	0.043478
navigator deal with	navigator	0.032258
the graph	gof	0.002381
i of	i r i	0.500000
dot product of the specified pieces of vectors	sparse block gemv make node o w h	0.066667
cross-entropy between an approximating distribution and a	tensor nnet categorical crossentropy coding_dist true_dist	0.111111
a given version	version	0.031250
a specified factor takes as input	tensor signal pool 2d input	0.090909
two kinds of useless	local useless	0.111111
the input specs and the output specs	fgraph input_specs output_specs accept_inplace	0.142857
to	gpuarray gpu	0.090909
a graph of apply	gof sort apply	0.200000
is	gpulocal opt	0.055556
3d convolution for debugmode	abstract conv conv	0.125000
shape and dtype as	dtype	0.022727
list of lib directories that are needed by	lib dirs	0.045455
optionally inserting	py operators	0.015625
dim_x	dim_x	1.000000
of a matrix :math a using magma library	gpu magma	0.142857
or more multinomial distributions defined	tensor multinomial random_state	0.040000
computes the standard deviation	std	0.058824
to work around windows behavior that open windows	misc subprocess popen command	0.142857
the task that is associated to	gof cthunk find task failure_code	0.083333
to make itself the default python and	to	0.017544
node inputs[i] to new_r	function graph change input node i new_r reason	0.500000
true if the named module is a package	meta path importer is package fullname	0.250000
contains a	gof contains	0.500000
sum(alloc constant shapes => constant*prod shapes	tensor local opt alloc node	1.000000
convolution gradient with respect to the	dnn conv grad i	0.125000
the type's	array type	0.055556
the sum	sum	0.076923
matrix	alloc	0.012500
enabled change all sigmoid to ultra_fast_sigmoid	tensor nnet local ultra fast sigmoid node	0.200000
basic variable class	tensor variable	0.166667
help the navigator deal	gof navigator	0.038462
to compute the	tensor nnet get	0.250000
failure_code	failure_code	0.625000
matrix inverse on	matrix inverse a	0.200000
elementary validations on the inner	inner	0.041667
function name to	gpuarray	0.046512
context object mapped to the type's	gpu array type context	0.090909
as replace_all_validate revert the replacement if	gof replace validate replace all validate remove	0.111111
is the equivalent of localoptgroup for graphtogpu	group	0.047619
local optimization	local	0.014085
user is not attempting to use dnn	safe no dnn	0.125000
import	import r variable	1.000000
the updates ordereddict the list	scan_module get updates	0.034483
not support the types involved in this node	inc subtensor do type checking node	0.250000
x -> sigm	local	0.014085
each row correspond to the one hot	one hot	0.142857
to theano constants in subtensor arguments	args	0.025641
input of given shape and flags	out shape imgshape ws ignore_border stride	0.200000
digest	digest	1.000000
with the same shape and dtype as	dtype	0.022727
function for diagonalsubtensor and	tensor nnet get diagonal subtensor view	0.083333
inputs and put	pure op perform node inputs output_storage params	0.047619
class to raise in	core raise init	0.100000
c	type c	0.071429
function is only used to determine	tensor adv index broadcastable	0.050000
this function tries to	top_shape	0.137931
orphans among them	orphans	0.090909
is	six meta path importer is	0.250000
the abs and rel error	abs rel	0.166667
when enabled change all sigmoid to ultra_fast_sigmoid	local ultra fast sigmoid node	0.200000
given a inner nit_sot output	inner	0.041667
the axis that was used to	grad inputs g_outputs	0.076923
list of shape tuple or	default infer shape	0.066667
important note this function	scan process node	0.142857
stack trace from one or	stack trace	0.055556
variable optionally inserting broadcasted	tensor tensor	0.014286
[true] division inverse of	tensor true div	0.250000
is no	node	0.014815
insert deepcopy in the fgraph to	insert deepcopy fgraph	0.500000
to the input specs and the output specs	std fgraph input_specs output_specs accept_inplace	0.142857
on all device we do	device node	0.045455
some elementary validations on the inner graph	scan_module scan validate inner graph	0.035714
to help the navigator deal with	navigator	0.032258
of the last access of	last access	0.040000
is not attempting to use dnn conv algo_bwd	dnn algo bwd algo	0.166667
the same op twice gives inconsistent outputs	bad thunk	0.200000
k	k	1.000000
a variable	a	0.008065
trace from one or more tensor	trace	0.052632
and return the url	content description filename auth	0.250000
symbolic row variable	tensor row name dtype	0.050000
outputs and the	and outputs	0.100000
function :func neibs2images	nnet neibs2images	0.333333
broadcast pattern	pattern a idx	0.066667
meta path importer	meta path importer	0.166667
inputs to	inputs node	0.100000
dimensions of this variable optionally inserting broadcasted	py operators dimshuffle	0.019231
fgraph and a list of variables	fgraph outputs_to_disown	0.047619
zero	zero	1.000000
hooks	hooks	1.000000
of apply nodes according to	sort apply nodes inputs outputs cmps	0.050000
output error and exit code in	misc output subprocess popen command	0.100000
indptr field	csm indptr csm	0.333333
the other arguments	ndim bcast ndim	0.333333
list remove are still in the	remove fgraph replacements remove reason	0.055556
an operation to wait on a previously	mpisend wait	0.045455
partition a list of	tensor scalarconsts	0.125000
idx_list with constant	get constant idx	0.250000
that x and y have the same shape	feature same shape x y	0.500000
2d	2d	0.545455
proxy for either true_div or int_div	tensor div proxy	0.125000
we can't change the value after the	core config param init default filter	0.040000
subgraph bound by the inputs	init inputs	0.083333
and converts this to expm1	tensor local expm1 node	0.066667
converts self _grad_op from user supplied form to	op from graph recompute grad	0.200000
function to get	get depth	0.050000
curfft	curfft	1.000000
of localoptgroup for	to gpulocal opt group	0.055556
number to	number number	0.125000
random stream in this container	random streams gen op	0.250000
mflops	gpu corr3d mm flops inp outp	0.125000
if allow_override	filter allow_override	0.142857
function	function	0.368421
weights	weights	0.416667
another op that takes the same inputs	op sub	0.066667
class for giving abbreviated tags like to	tag generator	0.333333
op that will call	op	0.009174
inverse	inverse	0.466667
a recursion over the input dimensions	permute row elements rec	0.047619
output dimensions of convolving an image of shape	get output	0.047619
an input that wasn't	bad	0.013158
be removed	compress outs	0.076923
can't change the value after the	param init default	0.040000
the supplied function as its implementation	compile as	0.050000
return a symbolic row variable (ndim=2	row name dtype	0.050000
any duplicates (according to	duplicates	0.125000
variable with the same shape and dtype	dtype	0.022727
has this	type has	0.500000
real and	complex real	0.500000
the source code for this linker and	compile cmodule location	0.038462
an op that	op	0.018349
in the flattened version of	tensor flatnonzero	0.166667
by an op that	clinker op	0.142857
cond then ift else iff	switch cond ift iff	0.500000
remove are still in the graph	remove fgraph replacements remove reason	0.055556
with constant inputs	get constant idx inputs	0.250000
in a new graph	new inputs inputs	0.166667
argmax1hot	argmax1hot	0.294118
the image	1axis kernel_shape	0.500000
&	tensor and	1.000000
c_init that initializes py_name to py_none	gof get c init r name sub	0.250000
in profiling to print the mflops	conv op flops inputs outputs	0.125000
to compute	nnet get	0.250000
an fgraph	fgraph outputs_to_disown	0.047619
determine the name the	name	0.011111
set	from graph set	1.000000
of policies	policy policy	0.125000
navigatoroptimizer	nav repl_pairs local_opt	0.500000
encoding of each	nb_class dtype	0.200000
local	local	0.084507
the dimensions	py operators	0.015625
of localoptgroup for	graph to gpulocal opt group	0.055556
product of the specified pieces of vectors and	sparse block gemv make node o w	0.066667
argmax over	argmax	0.066667
every node that uses r as	gof function graph	0.031250
similar behaviour as python's reduce	scan_module reduce fn sequences outputs_info non_sequences	1.000000
representating the cause of	bad optimization str diagnostic	0.043478
output for this node	output	0.017241
must return a thunk that is a	thunk	0.021277
to determine the broadcast pattern for	tensor adv index broadcastable pattern a	0.066667
max	tensor local max	0.250000
x and there is already an	local	0.014085
similar behaviour as python's map	scan_module map fn sequences non_sequences	1.000000
wants to add some requirements	optimizer add requirements	0.166667
remove subtensor/advancedsubtensor1 if it takes the	local useless subtensor node	0.200000
the named module is a package	compat six meta path importer is package fullname	0.250000
the mflops	op flops inputs outputs	0.125000
the given axis es of a tensor	axis ddof keepdims	0.083333
vector to the diagonal of an empty	alloc diag	0.027027
the equivalent	to gpulocal opt	0.055556
a mrg stream	sandbox mrg random streams	0.033333
uniform distribution	tensor uniform random_state	0.125000
source code for this linker and returns	compile cmodule location	0.038462
row is a mrg stream state and they	sandbox mrg random streams	0.033333
draw samples from a poisson distribution	poisson size lam ndim dtype	1.000000
the constant	constant	0.033333
inputs with a set of 2d filters	input filters	0.117647
compute 1d kernel for bilinear	tensor nnet bilinear	0.111111
value after the	param init default	0.040000
of shape	shape	0.020408
return connection pattern	op from graph connection pattern node	0.076923
subtraction (inplace on a)	tensor sub inplace a	1.000000
nested loop over several arrays and associate	loop	0.027778
of ints	bincount x weights minlength assert_nonneg	0.125000
data structures	push out non	0.125000
initializes py_name to py_none	c init r	1.000000
function to get	get	0.020833
a variable with a	variable x	0.083333
to split x	split	0.125000
using	gpuarray	0.023256
apply profiling informaton	apply profile node profile	1.000000
attempts to replace a	scan inplace optimizer attempt	0.500000
instance associated with a particular stream	random streams setitem item val	0.142857
infer the	tensor infer	0.142857
a convolution with the	conv get	0.100000
dependence	dependence	0.214286
functiongraph that has ever been associated	gof	0.002381
list of variables	variables	0.086957
a tensorvariable of this type	tensor type make variable	0.500000
allows replacing subgraphs of a	clone output replace strict share_inputs	0.071429
the	gpuarray gpu array	0.125000
a symbolic row variable (ndim=2 broadcastable=[true false])	row name	0.050000
the type's :attr context_name	gpuarray	0.023256
scan return true iff the	push out scan	0.050000
recursion over the	permute row elements rec	0.047619
of l{codeblock} instances returns a string that executes	gof code gen blocks	0.050000
takes as input a n-d tensor where	signal pool 2d input ws ignore_border stride	0.100000
a symbolic row variable (ndim=2 broadcastable=[true false])	tensor row name dtype	0.050000
to make itself the default python and	to os	0.038462
gpu convolution using cudnn from nvidia	gpuarray dnn conv img kerns border_mode subsample	1.000000
return full	module name from	0.076923
that this opt applies to	gof	0.002381
pattern has functioning	careduce cuda supports	0.166667
clone this object but we don't clone	gof constant clone	0.166667
of outputs and	and outputs	0.100000
connection pattern of subfgraph defined by	connection pattern	0.032258
this is the equivalent	opt	0.043478
type numpy typenum that corresponds	tensor tensor type dtype specs	0.071429
it work for elemwise and gpuelemwise	local elemwise	0.166667
a specified factor takes as input a	signal pool 2d input	0.090909
represent the dependence of nodes	dependence	0.035714
gradient	grad fun pt	0.500000
type2 from	type2	0.050000
a c contiguous version	contiguous	0.058824
inputs required to compute the	inputs variable_list blockers	0.058824
the dependence of nodes	dependence	0.035714
the inputs and put	pure op perform node inputs output_storage params	0.047619
wasn't	bad view	0.027027
specified factor takes as input a n-d	signal pool 2d input ws ignore_border stride	0.100000
and reduce pattern has functioning c code	gpu careduce cuda supports c code	0.250000
an axis	axis	0.025641
1d kernel for bilinear upsampling this function	bilinear	0.019231
runs a series of wrapper	wrap linker many linkers wrappers	0.047619
elementary validations on the inner	scan validate inner	0.142857
a string	string	0.111111
for corrmm (direction="forward"), corrmm_gradweights	nnet base corr mm	0.333333
override clinkerobject c_support_code	tensor tensor type c support code	1.000000
time	time	0.714286
symbolic row variable (ndim=2	tensor row	0.050000
where function is a thunk that operates on	gof linker make thunk	0.045455
subtract	sub	0.111111
function	random_state n	0.666667
function builds the 2d kernel that can be	kernel 2d	0.050000
hint that	sandbox linalg psd	0.500000
clone this	gof constant	0.333333
directories that are needed by	dirs	0.071429
where function is a thunk that operates	gof linker make thunk	0.045455
output_variables) where function is a thunk that	gof linker make thunk	0.045455
array of ints	x weights minlength assert_nonneg	0.125000
output shape of	output shape	0.250000
a memory alias that wasn't in the view_map	view	0.022727
moved objects in six moves urllib_response	six moves urllib response	0.333333
deepcopy	deepcopy	0.750000
replace a crossentropysoftmax1hotwithbiasdx op whose incoming gradient is	local useless crossentropy softmax 1hot with bias dx	0.111111
openblas	openblas	0.666667
tangent	tan	0.250000
replacement if the ops in the list	gof replace validate replace	0.050000
dimensions of this variable optionally inserting broadcasted	tensor tensor	0.014286
dependence of nodes in	gof make dependence	0.043478
the context associated	get context	0.111111
broadcasted dimensions	py operators dimshuffle	0.019231
the navigator deal	navigator optimizer attach updater fgraph	0.038462
or more multinomial distributions defined by	multinomial	0.024390
this variable optionally inserting	py operators	0.015625
for the minimum in one	tensor minimum x y	0.090909
to construct a variable	variable x name	0.083333
the inputs and put the variables in	pure op perform node inputs output_storage params	0.047619
the outer product	block outer	0.250000
code string specific to the apply to be	apply	0.016667
particular stream	random streams getitem item	0.142857
equilibriumoptimizer by calling the method query	graph to gpudb	0.142857
dimensions	tensor tensor py operators dimshuffle	0.038462
the specified pieces of vectors and matrices	sparse block gemv make node	0.066667
replace it with logsoftmax x	tensor nnet local logsoftmax node	0.142857
transfer to a tensortype if	transfer	0.058824
returns the bartlett spectral window in the time-domain	tensor bartlett m	0.083333
to outside of scan	scan	0.017241
the navigator deal with	navigator optimizer attach	0.038462
to determine the broadcast pattern for advancedsubtensor	tensor adv index broadcastable pattern	0.066667
that has ever been associated to self	gof	0.002381
method that	gof feature	0.125000
object with	with op	0.166667
samples from a uniform into sample from	from uniform	0.200000
to find broken	compile find	0.333333
helper function for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view	0.083333
idx_list with	idx	0.076923
the value after the import of theano	core config param init default	0.040000
to represent the dependence of	dependence	0.035714
we do it only	node	0.007407
function performs the matrix inverse on gpu	gpu matrix inverse a	0.200000
to type2 from type1	type1 type2	0.166667
shape that broadcast them to match	tensor generate broadcasting	0.066667
takes as input	signal pool 2d input	0.090909
dimshuffle{ } b axis=l) ->	local	0.014085
wrt, computes	subgraph grad wrt	0.062500
default 1) times from a multinomial distribution	streams multinomial	0.076923
similar behaviour as python's map	scan_module map fn sequences non_sequences truncate_gradient	1.000000
the module initialization code	init code	0.142857
first kind of order v	v x	0.100000
row variable (ndim=2	tensor row name dtype	0.050000
to the apply to	apply	0.016667
revert the replacement if the ops in	validate replace	0.050000
app inputs[i] changed from	destroy handler on change input fgraph app i	1.000000
uses set and dictionary data structures	out non	0.125000
simplify a multiplication tree	tensor nnet simplify mul tree	1.000000
draw random numbers using numpy's	a replace p	0.250000
outer product of	sparse block outer	0.047619
rstates	rstates	1.000000
stack	check stack	0.142857
compute the variable out for occurrences of	out	0.018519
a 4-d tensor it sets	patch_size	0.050000
c header for openblas threads	openblas threads	0.250000
to the type's :attr context_name	array type	0.055556
iff other is the same kind	other	0.090909
expand	expand	1.000000
the output error and exit code in	output subprocess popen command	0.100000
x is a matrix return its	x	0.008772
c-implementation of the dot product	dot csr c code node name inputs	1.000000
name to load	gpuarray load	0.200000
lib directories	lib dirs	0.045455
the given graph contains a cycle parameters	gof contains cycle	0.333333
dimensions of this variable	operators	0.017241
of the var	var	0.035714
unfortunately conda offers to make itself	to os environ pathlist	0.038462
the idx_list with constant inputs replaced	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
function expects the compile lock to	to	0.017544
operation to	mpisend	0.037037
functiongraph a constant that is cached	cached constant error	0.200000
constant inputs to	constant inputs	0.125000
this variable optionally	operators dimshuffle	0.019231
draw samples from a poisson distribution	streams base poisson size lam ndim dtype	1.000000
upper bound on the largest eigenvalue of	bound	0.043478
the source code	gof clinker compile cmodule location	0.038462
feature should remove any	feature on detach	0.200000
mini-batch of a	input_shape filter_shape	0.027778
the context object mapped to the type's :attr	gpu array type context	0.090909
debug	gof debug	1.000000
of lib directories that are needed	clinker header dirs	0.055556
to the subtensor and its idx_list reorders the	idx_list get_count	0.090909
self _grad_op from user supplied form	op from graph recompute grad op	0.200000
the biggest error between g_pt and self gf	numeric grad max err g_pt abs_tol rel_tol	0.500000
numpy ones_like	ones like model dtype	0.333333
representating the cause of the	bad optimization str diagnostic	0.043478
and dictionary data structures	push out non	0.125000
function implement the same rounding than numpy round	round	0.076923
new variable instance of type self	pure type	0.142857
the output	dims output input leftdims	0.333333
choose values from a with or without replacement	random streams base choice size a replace	0.333333
a gist and return the url	gist content description filename auth	0.250000
condition	condition	1.000000
compute sum of non nan	tensor tensor constant signature get sum	0.142857
that has ever	gof	0.002381
by transferring each of	remove	0.035714
subsample	subsample	1.000000
__unify_walk__ method for one	gof unify walk a b u	0.037037
a function into a basic theano op that	op itypes otypes infer_shape	0.047619
a new	new inputs inputs	0.166667
corresponding element of a dense vector	x s	0.142857
see theano tensor sort	py operators sort axis kind order	1.000000
a -1 and converts this to expm1 a	local expm1	0.066667
converts	char from	1.000000
this	gof clinker object c code cache	0.500000
is a mrg stream state and they are	sandbox mrg random streams	0.033333
of	type	0.011905
in profilemode to print the mflops	gpu corr3d mm flops inp outp	0.125000
numpy ndarray contains any np inf values	compile contains inf arr node var	0.500000
litterals	tensor make constant	0.100000
function to get the 0	get depth	0.050000
gradient w r t its weights	conv3d grad wrt weights input output_grad filter_shape input_shape	0.333333
equivalent of numpy ones_like	tensor ones like model dtype opt	0.333333
inverse error function	erfinv	0.166667
the constant scalar	scalar constant	0.285714
move constants into the	constants	0.142857
value after	config param init default filter	0.040000
of	opt	0.043478
clip x to be between	tensor clip x	0.250000
exception	thunk	0.042553
if the g++ version used is the	gof gcc	0.027778
for every node that	gof function graph	0.031250
grad of this op	prod l op	0.033333
gpu	gpuarray gpu	0.045455
list of headers that	clinker headers	0.047619
some functiongraph listeners to help the navigator	navigator optimizer attach	0.038462
the code for our fgraph	gof clinker get dynamic module	0.200000
op	clinker op c code cache	1.000000
wasn't in the	destroy	0.009709
a signature object for	signature	0.066667
this is the equivalent	graph to gpulocal opt	0.055556
to determine	tensor adv index broadcastable	0.050000
listeners to help the navigator deal with the	gof navigator optimizer attach	0.038462
c code	op c code	0.333333
types	type	0.023810
self _grad_op from user supplied form to type	op from graph recompute grad op	0.200000
a warning message on the	gof deprecated filename msg	0.041667
performs batch normalization of	dnn batch normalization	0.125000
and b	b	0.044776
corrmm_gradweights (direction="backprop weights"),	helper bottom weights top direction	0.055556
of average pooling	average pool grad	0.200000
the task	cthunk find task	0.142857
hash equal for same	type hash	0.166667
full path	module	0.033333
equivalent of localoptgroup for graphtogpu	gpulocal opt group	0.055556
re-initialize each random stream	mrg random streams seed seed	1.000000
r to	compile debugprint r	0.250000
given inputs and outputs	inputs outputs	0.066667
in the original	inputs outputs copy_inputs_and_orphans memo	0.029412
the outputs of specific ops of a compiled	trace f_or_fgraph ops_to_check bug_print	0.035714
performs batch normalization	batch normalization	0.125000
return full path of the	module name from	0.076923
specs and the output specs	std fgraph input_specs output_specs accept_inplace	0.142857
values from a with or without replacement	tensor random streams base choice size a	0.333333
the hack in profilemode to print the mflops	gpu corr3d mm flops inp outp	0.125000
tuple of arguments which must be hashable	gof memoize f	0.250000
for pushing out the variables	out	0.018519
an aboslute path	cop get path cls f	0.166667
the type's	gpu array	0.055556
baddestroymap if necessary update dr_vals	r_vals dr_vals	0.250000
compile a dummy file with these flags	flags cls flag_list preambule body	0.250000
a poisson	random streams base poisson	0.500000
version	code cache version apply node	0.125000
conv output gradient w r t its inputs	conv2d grad wrt inputs output_grad filters input_shape filter_shape	0.333333
was dumped to a zip file	f persistent_load	0.052632
all symbolic variables in inputs	inputs	0.012658
to determine the broadcast pattern for advancedsubtensor output	adv index broadcastable pattern	0.066667
and uses it instead of the	o	0.076923
convert addsd to faster	addsd ccode	0.250000
operations	destroy	0.009709
connection pattern of a subgraph	connection pattern	0.032258
spatio-temporal filters with a	conv3d signals filters	0.111111
as replace_all_validate revert the replacement	gof replace validate replace all validate	0.111111
reorder the dimensions of this variable	operators dimshuffle	0.019231
kinds of tensortype	tensor tensor type	0.041667
recognize the updates ordereddict the list	get updates	0.034483
the idx list to get the	get idx list	0.076923
inverse complementary error function	erfcinv a	1.000000
function name to load	gpuarray load	0.200000
value after the	init default filter	0.040000
has only	only	0.050000
return a tensorvariable of this type	tensor tensor type make variable name	1.000000
or more multinomial	multinomial random_state	0.040000
to	tensor make	0.076923
return full path of	module name	0.062500
or 3d convolution for debugmode	nnet base abstract conv conv	0.125000
for pushing out	push out	0.037037
inputs and put the variables in the	gof pure op perform node inputs output_storage params	0.047619
x and there is already an l=cholesky	local	0.014085
nit_sot output of scan	push out scan	0.050000
the output type dtype and broadcast there	local useless alloc	0.333333
function to get the 0	type get	0.050000
is a package	compat six meta path importer is package	0.500000
replace_all_validate revert the	validate remove fgraph	0.166667
an array with more than	node	0.014815
return those items	gof remove	0.250000
the mflops	mm flops inp outp	0.125000
connection pattern of	from graph connection pattern node	0.076923
important note this function uses set and	push out seq scan process node fgraph	0.142857
gpu correlation implementation using matrix multiplication	gpu corr mm	0.142857
for a	gpu	0.011765
function expects the compile lock to	module cache add to	0.142857
for use within the op	op	0.009174
width of python int c long int	core python int bitwidth	0.250000
axis	axis	0.205128
given a inner nit_sot output of	inner	0.041667
as python not the other implementation of mod	mod	0.071429
determine the broadcast pattern for	adv index broadcastable pattern a	0.066667
that is a zero-arguments function that	gof	0.002381
return real component of complex-valued tensor z	real z	1.000000
computes the output	output	0.017241
alloc	alloc node	0.037037
convolve spatio-temporal filters	tensor nnet conv3d signals filters	0.111111
the same op twice gives inconsistent outputs	bad	0.013158
verifies the dimensionality of the var is equal	tensor is flat var	0.200000
generate c	compile register specify shape c	0.250000
be referred to	compile register	0.200000
given an apply_node recursively search from this node	import apply_node check	0.066667
specified pieces of vectors	sparse block gemv make node o	0.066667
this generates the c code for corrmm (direction="forward"),	base corr mm c code	0.090909
navigator deal	navigator optimizer attach updater fgraph	0.038462
given an apply_node recursively search	apply_node check reason	0.066667
cache "filename" as a pickle	call cache persist filename	0.250000
return a symbolic row variable (ndim=2	row	0.034483
to replace a	nnet replace	0.250000
subtensor of the form x[0 :] -> x[0]	local useless slice node	0.250000
in this node	node	0.007407
of two	sparse	0.019231
mpi	mpisend	0.037037
array	mpirecv	0.037037
shape tuple or	shape	0.010204
the "reverse-mode" gradient [1]_ for the cholesky factorization	cholesky grad perform	0.500000
a global optimizer for pushing out	push out	0.037037
to theano constants in subtensor arguments	constant args	0.250000
used	used	1.000000
gets a scan op a list of	op not_required	0.071429
triangle of an array	m k	0.250000
mflops	nnet conv op flops inputs outputs	0.125000
tensorsolve	tensorsolve	1.000000
graph's apply nodes such that	gof function	0.043478
structures	out non	0.125000
for making	gpuarray gpu careduce cuda makecall node	0.333333
only	only	0.300000
connection pattern	connection pattern	0.064516
self _grad_op from user supplied form to	compile op from graph recompute grad	0.200000
c-implementation	csr c code node name	0.333333
c code when doing constant folding	constant folding	0.142857
to wrt, computes gradients	core subgraph grad wrt	0.062500
re-initialize each random stream	sandbox mrg random streams seed seed	1.000000
implements the "reverse-mode" gradient for the	grad perform	0.083333
multiplicative inverse also called reciprocal	inv	0.142857
to print the mflops	tensor nnet conv op flops inputs outputs	0.125000
cache directory	module cache	0.071429
output after pad_dims	gpuarray unpad dims output input	0.333333
conv output gradient w r	conv3d grad	0.111111
comparing tensorconstant instances	tensor	0.003215
symbolic 3-d	tensor3 name dtype	0.166667
the grad of this op could	tensor prod l op	0.033333
the 2d	2d	0.090909
from a uniform distribution	tensor uniform random_state	0.125000
on the largest eigenvalue of square symmetrix matrix	sandbox linalg spectral radius	0.166667
return a code	node name sub	0.111111
pattern of subfgraph	pattern	0.028571
concatenate a number of	make	0.017857
to use complex numbers	mod check x y	0.166667
variable	py	0.014286
fix done in august 2011	load shared variable val	0.142857
validations on the inner graph to	scan_module scan validate inner graph	0.035714
for diagonalsubtensor and	nnet get diagonal subtensor view x i0 i1	0.083333
pool	pool	0.333333
:attr context_name	gpu	0.011765
true if var is always equal to	gpuarray is equal var	0.250000
function builds the 1d kernel that	kernel 1d	0.050000
the value after	core config param init default	0.040000
filled with ones closer to numpy's syntax	ones shape dtype	0.200000
for the minimum in	tensor minimum x	0.142857
the convolution gradient with respect	dnn conv grad w	0.125000
the value after the import of	param init default filter	0.040000
addition	tensor add	1.000000
associate specific code to each level of nesting	loop_orders dtypes loop_tasks	0.125000
function that	function	0.052632
l{codeblock} instances returns a string that executes them	code gen blocks	0.050000
scan return	scan	0.017241
of collection for which predicate item is true	remove predicate coll	0.500000
type2 from type1 constitutes an upcast	tensor is an upcast type1 type2	0.333333
blockers	blockers	1.000000
add tag trace to an node or	gof add tag trace thing user_line	0.166667
nnet	tensor nnet	0.017544
reshape t by inserting 1 at the	tensor shape padaxis t	0.333333
replace it with logsoftmax x 's grad	nnet local logsoftmax grad node	0.200000
the hack in profiling to print the mflops	tensor nnet conv op flops inputs	0.125000
move the abs	abs	0.066667
return	name	0.200000
input a 4-d tensor it sets all non	same size input patch_size	0.166667
probably not activate envs	compat maybe add	0.200000
a list of shape	default infer shape	0.066667
nnet	nnet	0.096774
same kinds of	type	0.011905
is associated	failure_code	0.125000
vector 1-dimensional	jacobian	0.142857
context object mapped	type context	0.090909
this	gof seq optimizer	0.200000
indptr	indptr	0.750000
and dictionary data structures	out non	0.125000
second half by b with	b	0.014925
empty matrix it	alloc	0.012500
alias that	view	0.022727
to the diagonal of an	alloc diag	0.027027
lop	lop	1.000000
implement	softmax	0.090909
return a new variable	variable name	1.000000
like zeros_like but forces the object to have	core float zeros like x	0.200000
toposort return an ordering	graph toposort	0.125000
function builds the 2d	2d	0.090909
with a set of 2d filters	tensor nnet conv2d input filters	0.125000
input a 4-d tensor it sets all	input patch_size	0.166667
matrix along	sp	0.125000
file	key data	0.500000
variables in inputs to sharedvariable	inputs	0.012658
short mostly hexadecimal hash of a numpy	core hex digest x	0.083333
output	dims output	0.333333
helper	diagonal	0.111111
graph of apply nodes according to	gof sort apply nodes inputs outputs cmps	0.050000
put in a functiongraph a constant that is	constant error	0.166667
lt	lt	0.833333
output after pad_dims	gpuarray unpad dims output input leftdims rightdims	0.333333
sent array using mpi	mpisend	0.037037
extract test value from	gof get test value	1.000000
matrices at least one of which	x y	0.024390
series of wrapper functions instead	wrap linker many linkers wrappers	0.047619
node by one which computes	node	0.007407
navigator	gof navigator optimizer	0.038462
dimensions of this variable	tensor tensor py operators	0.015625
pattern has functioning c	careduce cuda supports c	0.200000
permutations	permutation random_state size n	0.500000
creating a class with a metaclass	compat add metaclass metaclass	0.125000
clone the graph and get	graph clone	0.166667
a set of arrays to choose	choose a choices out mode	0.200000
hack in profiling to print the mflops	nnet conv op flops inputs	0.125000
nit_sot output of scan return	out scan	0.035714
we parametrize it	max_input_fct maker	0.083333
cosine	cos	0.285714
all variables which may share the same	compile infer reuse pattern	0.100000
copies a vector	alloc	0.012500
change all sigmoid to ultra_fast_sigmoid	local ultra fast sigmoid node	0.200000
this function uses set and dictionary data structures	push out non	0.125000
output after pad_dims	gpuarray unpad dims output	0.333333
in this container	gen op	1.000000
and return full	gof module name	0.076923
access of a given file	access time	0.200000
of a cache directory	from dir	0.125000
the task	task	0.083333
offers to	to os environ pathlist var newpath	0.038462
reduce pattern has functioning c code	gpuarray gpu careduce cuda supports c code inputs	0.250000
multiline string representating the cause of the	bad optimization str diagnostic	0.043478
multiplication (inplace on a)	mul inplace a	1.000000
linkers	linkers	1.000000
tensor from polar	tensor complex from polar	0.250000
reduce{scalar op}(join(axis=0 a b), axis=0) -> elemwise{scalar op}	local reduce join	0.111111
return the abs and rel error of gradient	grad abs rel	0.333333
use dnn conv algo_bwd	safe no dnn algo bwd algo	0.166667
the hack in profilemode to print the mflops	corr3d mm flops inp outp	0.125000
sent array	mpisend	0.037037
functiongraph	gof function graph init	0.333333
or a tensorvariable whose	tensor as	0.066667
roll tensortypes	roll x shift	0.250000
inputs and put	gof pure op perform node inputs output_storage params	0.047619
change all sigmoid to ultra_fast_sigmoid	ultra fast sigmoid node	0.200000
see theano tensor sum	tensor py operators sum axis dtype keepdims acc_dtype	1.000000
a new random	random	0.055556
subgraph contained between	copy_inputs	0.125000
an input vector and t is a	node input_storage output_storage	0.038462
the list	list type	0.100000
c type numpy typenum that	tensor type dtype specs	0.071429
for scalar values default int64 or	scalar	0.017857
return a symbolic column variable (ndim=2 broadcastable=[false true])	tensor col name dtype	0.200000
the folowing changes in	tensor local mul switch sink	0.045455
wait on a	mpisend wait	0.045455
prefix	prefix	1.000000
permutation by doing a recursion over the	permute row elements rec	0.047619
reduce pattern has functioning	gpuarray gpu careduce cuda supports	0.166667
parses a config string	config string	0.333333
of moved objects in six moves urllib_robotparser	six moves urllib robotparser	0.333333
the folowing changes	tensor local mul switch sink node	0.045455
revert the replacement	replace all	0.050000
converts number to string by	char from number number	0.142857
has	type has	0.500000
reorder the dimensions of this variable optionally	operators	0.017241
a set of 3d	nnet conv3d input	0.125000
sigmoid to ultra_fast_sigmoid	ultra fast sigmoid node	0.200000
cudnn from nvidia	dnn conv3d img kerns	1.000000
return path to the module file	key data get entry	1.000000
view in the forward but clip	clip x lower_bound upper_bound	0.090909
this variable optionally inserting	tensor py	0.015873
min and max	min max	0.250000
change the value after the import	config param init default	0.040000
compute sum of non nan	tensor constant signature get sum	0.142857
load a file	misc load	0.250000
raise	node storage_map	0.166667
to split x	split grad	0.500000
in the cache and none otherwise	cache	0.034483
dimensions of this variable	py operators	0.015625
in a new	clone with new	0.166667
a variable	variable x	0.083333
replace all subtensor(make_vector) like [a b c][0] ->	tensor local subtensor make vector node	1.000000
>=	tensor ge	0.500000
the replacement if	replace validate replace all	0.050000
base op for cudnn	gpu dnn	0.066667
given an apply_node recursively search from this node	apply_node check	0.066667
theano scalar scalar and tensorvariable	scalar as common dtype	0.250000
state and they are spaced by 2**72 samples	get substream rstates n_streams dtype inc_rstate	0.142857
new node a clone	clone	0.020833
by one-dimensional slices in pvals	pvals	0.071429
the convolution gradient with respect to the weights	gpu dnn conv grad	0.062500
choose from	tensor choose	0.250000
according to the flags use_list and use_tuple	format as use_list use_tuple	1.000000
it with a triangular solve	sandbox linalg tag solve triangular node	0.142857
for a convolution with the	gpu dnn conv get	0.200000
of apply nodes according	apply nodes inputs outputs cmps	0.050000
the equivalent	opt	0.043478
operation to wait on	mpirecv wait	0.045455
same shape	same shape	0.333333
a recursion over the input	permute row elements rec	0.047619
advancedsubtensor	advanced subtensor	0.250000
the g++	gof gcc	0.027778
pattern has functioning c	gpu careduce cuda supports c	0.200000
output dimensions of convolving an	nnet conv op get output	0.047619
that reuse the python code from gpuarray	cpy	1.000000
key_data	key_data	1.000000
explicitly upcasts constant	constant	0.016667
equivalent	to gpulocal	0.055556
be turned into macros for use	cop get	0.033333
and only adds dimension	tensor local	0.025641
row variable (ndim=2 broadcastable=[true false])	row name	0.050000
the cross-entropy between an approximating distribution	tensor nnet categorical crossentropy coding_dist true_dist	0.111111
a <=	le	0.125000
to a gist and return	gist	0.040000
wants to add	optimizer add	0.500000
respect to wrt, computes	core subgraph	0.062500
same kinds of tensortype	tensor type	0.034483
the value after the import of theano	config param init default	0.040000
when we overwrite	local useless inc subtensor node	0.066667
permutation by doing a recursion over the input	tensor permute row elements rec	0.047619
nodes in the original graph to a	inputs outputs copy_inputs_and_orphans memo	0.029412
replace it with a triangular solve	tag solve triangular node	0.142857
by doing a recursion over	permute row elements rec	0.047619
false we can't change the value after the	core config param init default filter	0.040000
this op	gof clinker object c code cache	0.500000
to r	debugprint r	0.250000
to print the mflops	flops inp outp	0.125000
retrive the context associated with a	get context	0.111111
transfer function for	transfer fn	0.125000
to help the navigator deal	navigator optimizer attach	0.038462
to print the mflops	conv flops inp outp	0.125000
a subgraph defined by given inputs and	inputs	0.012658
list of lib directories that are	clinker lib dirs	0.055556
cost	cost	0.318182
the image	1axis image_shape	0.500000
optimization makes the folowing changes	mul switch sink	0.045455
hack in profiling to print the mflops	abstract conv flops inp outp	0.125000
string specific to the apply to be inserted	apply	0.016667
less memory but is more restrictive	checkpoints fn sequences outputs_info non_sequences	1.000000
cache directory and return full path	gof module name from dir	0.071429
a mrg stream state and they are	sandbox mrg random streams	0.033333
to sharedvariable instances of suitable dummy values	local meta optimizer provide	0.200000
can't change the value after the import of	config param init default filter	0.040000
using external persistence	dump obj file_handler protocol persistent_id	0.500000
variable optionally	tensor py operators dimshuffle	0.019231
a mrg stream	mrg random streams	0.033333
an alloc and only	tensor local alloc	0.111111
from r	r	0.028571
indicating the version	code cache version apply node	0.125000
raise	check inputs node	0.166667
add a new variable to theano config	core add config var name doc	1.000000
legal value for a	is valid value a	0.076923
shape that broadcast them	tensor generate broadcasting	0.066667
this optimization makes the folowing changes in	mul switch sink	0.045455
a simple algorithm	reasons r_vals	0.333333
we parametrize it to make	max_input_fct maker	0.083333
the llvm one or not	llvm	0.100000
inline	inline	1.000000
in a new	with new inputs inputs	0.166667
a new	clone with new	0.166667
wrt, computes	subgraph grad	0.062500
gpu version of sparseblockouter see sparseblockouter's docstring for	gpu sparse block outer	1.000000
the value after the import of	param init default	0.040000
a badviewmap exception when it detects	compile check viewmap node	0.111111
see theano tensor std	tensor py operators std	1.000000
supported	supported	1.000000
removes all from the clients list of r	function graph remove client r client_to_remove reason	1.000000
feature should remove	gof feature on	0.200000
this op scale	scale	0.047619
this op could be very easy if	op	0.009174
x and	x	0.008772
division (inplace on a)	true div inplace a	1.000000
on f wrt to wrt	f wrt	0.200000
and dtype as the	dtype	0.022727
fill s v -> alloc(v shape	local fill to alloc node	0.250000
validator	validator	1.000000
for diagonalsubtensor	nnet get diagonal subtensor	0.083333
the specified pieces of vectors and	sparse block gemv make node o	0.066667
folowing changes	mul switch sink	0.045455
if allow_override is	filter allow_override	0.142857
along the axis that was used	grad inputs g_outputs	0.076923
connection pattern of subfgraph defined by inputs	op from graph connection pattern node	0.076923
grad	grad grad	0.166667
gamma	gamma	1.000000
return full	gof module name	0.076923
is not the same on all device we	device node	0.045455
using cudnn	gpuarray dnn	1.000000
abs and rel error	abs rel	0.166667
of scan return true	scan_module push out scan	0.050000
symbolic row	row name	0.050000
"lifts" dimshuffle through elemwise operations and	tensor local dimshuffle lift node	0.250000
policies	policy policy	0.125000
compute	signature get	1.000000
and only	tensor local	0.025641
is a package	is package	0.500000
associate linker with fgraph	op wise clinker accept fgraph	1.000000
start gradients up to the end variables of	wrt end start	0.166667
to expm1 a	local expm1 node	0.066667
maps from variable and apply	get equiv	0.142857
a hash from an ndarray	tensor hash from ndarray data	0.333333
takes as	signal max pool	1.000000
command	command	1.000000
specified pieces of vectors and	sparse block outer make node	0.066667
value after the import	core config param init default	0.040000
base 2 logarithm	log2	0.142857
s	s	0.428571
some requirements	requirements	0.125000
makes the folowing changes	local mul switch sink	0.045455
the gradient function should return	tensor matrix inverse r op inputs eval_points	0.500000
image shape of	shape kernel_shape	0.250000
symbolic integer scalar for the shape element s_i	shape feature unpack s_i var	1.000000
type's :attr	gpu array	0.055556
to assert that x	x	0.008772
raise baddestroymap if necessary update dr_vals	inputs node storage_map r_vals dr_vals	0.250000
wraplinker that runs a series of	gof wrap linker many linkers wrappers	0.071429
for gpuincsubtensor	local inplace setsubtensor	0.250000
a and b are unified given the unification	a b	0.066667
uses set and dictionary data structures	scan_module push out non	0.125000
version of var transferred to	tensor transfer var	0.100000
this function compute	nnet	0.032258
new variable	var name doc configparam	0.250000
subgraph	subgraph	0.238095
numpy-compatibility method if x	diag x	0.200000
and remove	tensor local	0.025641
list remove are still	remove fgraph replacements remove reason	0.055556
idx_list with constant inputs replaced by their	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
is an alloc of	alloc node	0.074074
value has	value trace	0.333333
choice	tensor choice	0.500000
remove incsubtensor when we overwrite the	local useless inc subtensor node	0.066667
are views of v given that v	v	0.011111
some elementary validations on the inner graph to	scan validate inner graph	0.035714
all outputs defined by indices out_idxs and	out_idxs	0.050000
type numpy typenum that corresponds to self	tensor tensor type dtype specs	0.071429
log(softmax x and replace it with logsoftmax x	tensor nnet local logsoftmax	0.076923
c code to initialize the	c	0.017857
return string representation	to str b	0.250000
legal value for a variable of this type	pure type is valid value a	1.000000
json	json	1.000000
replacement if the ops in	gof replace validate replace	0.050000
main interface to manipulate the subgraph in functiongraph	function graph replace r new_r reason verbose	0.250000
code to the task	task	0.083333
type2	type2	0.300000
in a sparse format	core sparse	0.066667
stack trace for an exception	thunk trace value	1.000000
config string (comma-separated key=value components) into	config string config_string issue_warnings	0.166667
of an	alloc	0.012500
reorder the	tensor py	0.015873
basic theano op	op itypes otypes infer_shape	0.047619
of useless reshape	useless reshape	0.200000
to the type's :attr context_name	array	0.041667
compare true iff other is the same kind	type eq other	0.250000
as replace_all_validate revert the replacement	replace all validate	0.111111
dimensions of this variable	tensor	0.006431
order v real	v	0.022222
an input	bad destroy	0.034483
for use	gpu	0.023529
a variable with the same shape and dtype	dtype	0.022727
use_list and	format as use_list	1.000000
that will be instantiated by	gof	0.002381
compute the variable out for	out	0.018519
given axis es of a tensor	axis dtype	0.083333
unary(alloc x shp -> alloc(unary x shp)	tensor local alloc unary	0.250000
compare true iff other is	tensor type eq other	0.250000
borrow	borrow	1.000000
reorder	tensor py operators dimshuffle	0.019231
product along the given axis	axis	0.025641
context object mapped to the type's :attr context_name	array type context	0.090909
we can't change the value after	core config param init default filter	0.040000
the output shape for a convolution with the	gpu dnn conv get out shape	0.142857
scan the contents of	dirname err files	0.083333
it with a triangular solve	solve triangular	0.142857
a new instance of this mode	compile mode clone link_kwargs optimizer	0.333333
u and uses it instead of the	o u	0.037037
triangle of an	m k	0.250000
generate c	register specify shape c	0.250000
work for elemwise and gpuelemwise	local elemwise	0.166667
memo	memo	1.000000
connection pattern of subfgraph defined	op from graph connection pattern node	0.076923
that	alloc	0.012500
if the alloc would be useless	alloc call	0.333333
the broadcast pattern	pattern a	0.066667
the maximum	maximum	0.083333
tensorvariable of this type	tensor tensor type make variable	0.500000
shape "kshp"	shape inshp kshp stride mode	0.142857
optimizer for pushing out the	out	0.018519
the equivalent of localoptgroup	graph to gpulocal opt group	0.055556
a nested loop	loop	0.027778
which computes the specified outputs	fgraph	0.012195
warning message on the	gof deprecated filename msg	0.041667
fullpath	fullpath	1.000000
create a new instance of this mode	compile monitor mode clone link_kwargs optimizer	0.333333
default output for this	apply default output	0.250000
to output nodes	gof	0.002381
optional return	c	0.017857
return a list of shape tuple or	tensor shape feature default infer shape	0.066667
thunk that operates on the	gof linker make thunk	0.045455
fill a with b	tensor second inplace a	0.333333
rel_tol	rel_tol	0.833333
make a nested loop over several arrays and	make loop	0.200000
offers to make	to os environ pathlist var	0.038462
apply_node recursively search from this node to	apply_node check	0.066667
if present	gof	0.002381
the bartlett spectral window in	tensor bartlett	0.083333
exception raised to indicate an internal theano problem	debug mode error	1.000000
array using mpi	mpirecv	0.037037
a list to	tensor gemm from factored list	0.500000
division (inplace on a)	tensor int div inplace a	1.000000
c_cleanup that decrefs py_name	gof get c cleanup r name sub	0.250000
with constant inputs replaced by their python	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
optionally inserting broadcasted dimensions	tensor	0.006431
nit_sot output of scan return true iff	scan_module push out scan	0.050000
listeners to help the navigator	navigator optimizer attach updater	0.038462
application of another op that takes	op sub	0.066667
it with logsoftmax x	nnet local logsoftmax node	0.142857
makes the folowing changes in the graph	local mul switch sink node	0.045455
this op is used only internally by theano	output guard	1.000000
lists/tuples/other objects into	compile flatten l	0.200000
elemwise x * x	sparse sqr x	1.000000
prod	prod	1.000000
batch	gpuarray dnn batch	0.500000
c type numpy typenum that corresponds to self	tensor tensor type dtype specs	0.071429
optionally inserting	dimshuffle	0.014493
the grad of	grad grad	0.166667
the variable v	v	0.011111
a warning message on the	deprecated filename msg	0.041667
important note this function uses set	process node	0.142857
inserting broadcasted dimensions	tensor tensor py operators dimshuffle	0.019231
this variable optionally inserting	tensor py operators	0.015625
function checks if the outputs of specific ops	trace f_or_fgraph ops_to_check bug_print	0.035714
lib directories that are needed	lib dirs	0.045455
input to	input	0.023810
turned into macros for use	cop get	0.033333
the batch	batch	0.055556
dimshuffle which only adds dimension to the left	local dimshuffle	0.052632
idx list to get the	tensor get idx list	0.076923
in the destroy_map	bad destroy	0.034483
as replace_all_validate revert the replacement if the	validate replace all validate remove fgraph	0.111111
the replacement if	replace validate replace	0.050000
the specified pieces of vectors and	sparse block outer make node	0.066667
parse	nnet parse mul	0.500000
checks if theano graphs represent the same computations	computations xs ys in_xs in_ys	0.333333
the navigator	navigator optimizer attach updater	0.038462
if a subtensor	subtensor	0.058824
for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x	0.083333
has	scan_module has	1.000000
recognize the updates ordereddict the	scan_module get updates	0.034483
scale each columns	col scale x	1.000000
inverse fast fourier transform with real-valued output	gpuarray cuirfft inp norm is_odd	0.333333
be inserted in the module initialization code	init code	0.142857
product	block	0.166667
remove broadcastable dimensions from the	tensor py operators squeeze	0.200000
the 1d kernel that	kernel 1d	0.050000
the same rounding than numpy round half to	round half to	0.166667
remove base directories 'cutils_ext', 'lazylinker_ext' and 'scan_perform'	module cache clear base files	1.000000
value for	gpu	0.011765
constant that	gof params	0.200000
for creating a class with a metaclass	add metaclass metaclass	0.125000
getstate	getstate	1.000000
c code for gpucorrmm (direction="forward"),	gpu corr mm c code	0.090909
the equivalent	to	0.017544
return the platform-dependent gcc	gof get gcc	0.333333
elemwise tanh of	sparse tanh	1.000000
thunk that operates on	gof linker make thunk	0.045455
a symbolic row variable (ndim=2 broadcastable=[true	row name dtype	0.050000
change the value after the import of theano	core config param init default	0.040000
ir	ir	1.000000
data by walking the	refresh age_thresh_use delete_if_problem cleanup	0.166667
return connection pattern of	graph connection pattern	0.076923
dimensions of this variable optionally	tensor tensor py	0.015873
apply instance in a new	apply clone with new inputs inputs	0.500000
test a	fun pt n_tests	0.500000
zeros are present in a given "group" (ie	inp out grads	0.166667
the outputs from	outputs mode accept_inplace	0.166667
function builds the 2d kernel that can	kernel 2d	0.050000
called whenever node inputs[i] is changed	change input function_graph node i	0.333333
takes as input a n-d tensor where n	tensor signal pool 2d input ws ignore_border stride	0.100000
the output dimensions of convolving	tensor nnet conv op get output	0.047619
that with the *args directly	local csm properties csm node	0.142857
remove broadcastable dimensions from the	tensor tensor py operators squeeze	0.200000
bessel function	j0 x	1.000000
the "reverse-mode" gradient for	grad perform node inputs	0.083333
operand optimized for calculating the dot product	dot csr	0.111111
scal	scal	1.000000
to	gpuarray gpu array	0.062500
the input by a specified factor takes as	tensor signal pool 2d	0.142857
important note	process node	0.142857
decorator to merge multiplication by a scalar on	alpha merge cls alpha_in beta_in	0.200000
of l{codeblock} instances returns a string	code gen blocks	0.050000
* y + alpha * dot a	gemv c code y a	0.333333
if called on something that is not	not	0.076923
useless dimshuffle operation inside reshape reshape(vector	tensor local useless dimshuffle in reshape node	0.500000
for the existence of the __unify_walk__ method for	gof unify walk a b u	0.037037
change all sigmoid	sigmoid	0.055556
compile lock to	add to	0.142857
if	node storage_map	0.166667
merge 2 dicts by adding	gof merge dict d1 d2	0.333333
determine the broadcast pattern for advancedsubtensor output	adv index broadcastable pattern a idx	0.066667
wrt, computes gradients of	core subgraph	0.062500
theano function	py	0.014286
v	v x	0.100000
triangular solve	solve triangular	0.142857
the convolution gradient with respect to the	gpu dnn conv grad i	0.125000
more multinomial distributions defined by	tensor multinomial	0.037037
tries to recognize the updates ordereddict the	scan_module get updates	0.034483
a	bad	0.013158
type2 from type1 constitutes an upcast	is an upcast type1 type2	0.333333
kinds of	tensor	0.006431
pattern for	pattern a idx	0.066667
folowing changes in the	local mul switch sink node	0.045455
_maker which handles much of the debugging	compile debug mode function maker i o m	0.066667
return full path of the	gof module name	0.076923
of x default reverse them	tensor transpose x	0.200000
to add some requirements to the fgraph	optimizer add requirements fgraph	0.333333
by indices out_idxs	out_idxs	0.050000
convert addsd	addsd	0.111111
listeners to help the navigator	navigator optimizer	0.037037
simplify a	nnet simplify	0.500000
to the apply to be inserted	apply node	0.031250
functiongraph listeners to help the navigator deal with	navigator optimizer attach	0.038462
decorator to merge multiplication by a scalar	alpha merge cls alpha_in beta_in	0.200000
converts this to expm1	local expm1 node	0.066667
uniform distribution	tensor uniform random_state size	0.125000
wrt, computes gradients	subgraph grad	0.062500
sign of a (inplace on a)	tensor sgn inplace a	1.000000
representating the cause of the exception	bad optimization str diagnostic	0.043478
shape in	shape	0.010204
the topooptimizer from the	gof in2out	0.055556
which will print a warning	deprecated filename	0.166667
the confusion matrix	tensor nnet confusion matrix	0.166667
modifies input	give	0.142857
an apply_node recursively search from	import apply_node	0.066667
updates ordereddict the list	scan_module get updates	0.034483
optimization	optimization	1.000000
to replace a leaf of	replace leaf	0.100000
node inputs[i] is r	r	0.028571
axis=l) -> sum(a axis={ }) / b	local sum prod div dimshuffle	0.333333
mflops	abstract conv flops inp outp	0.125000
matrix :math a using magma library	gpu magma	0.142857
if the given graph contains a cycle parameters	gof contains cycle fgraph orderings	0.333333
of sparseblockgemv check sparseblockgemv's	gemv	0.100000
sharedvariable instances of suitable dummy values	meta optimizer provide inputs	0.200000
between i and o	i o	0.125000
a numpy ndarray contains any np nan values	compile contains nan arr	0.500000
c_extract_out that initializes	gof get c extract out	0.500000
:attr	gpuarray	0.023256
m1	m1	0.162162
subtensor is inside a dimshuffle which only	subtensor node	0.066667
optionally inserting broadcasted	dimshuffle	0.014493
apply as many times as required	apply	0.016667
a symbol	symbol	0.125000
[advanced]incsubtensor[1], whose increment is an alloc	tensor local useless inc subtensor alloc node	0.166667
the named	fullname	0.066667
1/(1+exp x -> sigm	tensor nnet local inv 1 plus exp	0.333333
and "code_cleanup"	cleanup node name inputs outputs	0.500000
if allow_override is	allow_override	0.083333
suitable dummy values	local meta optimizer provide	0.200000
apply to	apply	0.016667
inverse fast fourier transform with real-valued output	irfft inp norm is_odd	0.500000
reorder the dimensions of this variable optionally inserting	tensor py operators dimshuffle	0.019231
is a wrapper for numpy sort function	sort op	0.250000
pretty multiline string representating the cause of	bad optimization str diagnostic	0.043478
compute the image shape of convolution gradinputs	nnet get conv gradinputs shape 1axis kernel_shape	0.500000
c header for the fortran blas interface	blas header text	1.000000
input vector and t	node input_storage output_storage	0.038462
new graph	new	0.058824
if it takes	node	0.007407
of moved objects in six moves urllib_parse	six moves urllib parse	0.333333
provided l{functiongraph} it	gof optimizer apply	0.166667
alias that wasn't in the	bad	0.013158
memory	bad view	0.027027
access of	access time path	0.200000
c-implementation of	csr c code node name inputs outputs	0.333333
that are views of v given that v	set v	0.125000
dtype as the	dtype	0.022727
gpucorrmm, gpucorrmm_gradweights and gpucorrmm_gradinputs	corr	0.166667
badviewmap exception when it detects	compile check viewmap node	0.111111
impossible	destroy	0.009709
scan in an easy	scan args	0.250000
determine the broadcast pattern for advancedsubtensor	adv index broadcastable pattern	0.066667
turn it into a gemm	gemm	0.066667
return the inputs required to compute the	inputs variable_list blockers	0.058824
because of	destroy	0.009709
instances of suitable dummy values	meta optimizer provide inputs	0.200000
lower	tril	0.166667
see theano tensor argmax	tensor tensor py operators argmax	1.000000
call the supplied function as its implementation	compile as	0.050000
create pydot	to pdnode d	0.333333
this is the equivalent of	opt	0.043478
the list remove are still	remove fgraph replacements remove reason	0.055556
prod	prod axis dtype keepdims acc_dtype	1.000000
in profiling to print the mflops	nnet conv op flops inputs outputs	0.125000
abs toward the input	local abs lift node	0.333333
names when persisting	id	0.100000
overwrite the full inputs with	useless inc subtensor	0.125000
for matrix solve operation c	solve	0.032258
tensorconstant	tensor constant	0.055556
for diagonalsubtensor	tensor nnet get diagonal subtensor view	0.083333
see theano tensor argmin	tensor tensor py operators argmin axis keepdims	1.000000
create a comparator to represent the dependence	make dependence cmp	0.111111
use dnn	dnn	0.060606
fetch a compiled module from the loaded cache	gof module cache get module name	0.166667
same kinds of	tensor	0.006431
fixed	fixed	1.000000
removes all	local remove all	0.166667
self _rop_op from user supplied form	from graph recompute rop	0.200000
mrg stream state and they	mrg random streams	0.033333
is a thunk that operates on	gof linker make thunk	0.045455
2d or 3d convolution for debugmode	abstract conv conv	0.125000
return connection pattern of	graph connection pattern node	0.076923
return an instance of _maker which handles much	mode function maker i o m	0.066667
3d inputs with a set of 3d	tensor nnet conv3d	0.071429
on wraplinker that	gof	0.002381
return a new variable instance of type self	pure type call name	1.000000
a bit like make_loop but when only the	init_loop_orders olv_index dtypes inner_task	0.200000
unroll the batch	gen conv code unroll batch	0.166667
a safe shorter version of platform	platform	0.083333
add a new variable to theano config	core add config var name doc configparam root	1.000000
to generate c	register shape i c	0.250000
a and b can be considered approximately equal	values eq approx a b	1.000000
of another op that takes the same	op sub	0.066667
and set in a	a	0.008065
and should be removed and	compress outs	0.076923
helper function for diagonalsubtensor	nnet get diagonal subtensor view	0.083333
this	gpulocal opt	0.055556
replace_all_validate revert the replacement if the ops in	validate replace all validate	0.111111
compute sum of	tensor tensor constant signature get sum	0.142857
inputs and outputs	init inputs outputs	0.166667
total time icluding the	total times	0.200000
respect to wrt, computes gradients	subgraph	0.047619
the mean value	mean	0.062500
toposort return an ordering of the	function graph toposort	0.125000
the "reverse-mode" gradient for	grad perform node inputs outputs	0.083333
within the op code	op params	0.100000
convolution gradient with respect	dnn conv grad i	0.125000
function to get the 0 based level	get depth	0.050000
tree of multiplications starting at the given	tree	0.125000
of a shared variable	compile shared variable	0.083333
an	node inputs outputs	0.125000
compute a generalized dot product over provided axes	tensor tensordot a b axes	1.000000
pushing out	out	0.018519
of outputs and the stopping condition returned by	and outputs ls	0.166667
image shape of convolution gradweights	get conv gradweights shape 1axis	0.500000
scan return true iff	scan	0.017241
minimum see min for the minimum in one	tensor minimum x y	0.090909
the one	one	0.076923
navigator deal with the	navigator	0.032258
string (comma-separated key=value components) into a	string config_string issue_warnings	0.333333
according to a list of	inputs outputs cmps	0.166667
sample n (n	size n	0.090909
specifyshape how	c_support_code_apply	0.111111
none or a tensorvariable	tensor as	0.066667
with constant	constant idx	0.250000
of two sets of	sparse	0.019231
of lib directories	header dirs	0.045455
unfortunately conda offers to make itself the default	to os environ	0.038462
abs	abs	0.466667
by an op that will be	clinker op	0.142857
choose values from a with or	size a replace p	0.333333
round half to even of	rint	0.111111
a series of wrapper	many linkers wrappers	0.047619
flat	flat	1.000000
op then replace it with a triangular	triangular	0.076923
string	replace patterns	1.000000
takes as input a	signal pool 2d input	0.090909
a tuple of integers indicating the version	version apply node	0.125000
like zeros_like but forces the object to	core float zeros like x	0.200000
if and only if this enum has	gof enum type has	0.111111
update cache data by walking the cache	module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
useless reshape	useless reshape node	0.200000
update self rstate	rstate	0.090909
"init_code" together	struct	0.047619
c code for gpucorrmm	base gpu corr mm c code	0.090909
to the basic constant class	tensor constant	0.055556
dependence of nodes in a	dependence	0.035714
this computes the outer product of two	sparse block outer	0.047619
grad of average pooling	average pool grad	0.200000
attempt to replace a leaf	tensor nnet replace leaf arg leaves new_leaves op	0.250000
proxy for either true_div or	tensor div proxy	0.125000
return a list of shape tuple	tensor shape feature default infer shape	0.066667
a cache directory	from dir	0.125000
the grad of this op	op	0.009174
the image	kernel_shape	0.142857
the specified axis	x axis sparse_grad	0.333333
recognize the updates ordereddict the list	updates	0.029412
a function	function	0.052632
it to a max	local max	0.250000
compute the dot product of the	nnet	0.032258
incsubtensor x zeros idx -> x	tensor local incsubtensor of zeros node	1.000000
one or more multinomial distributions defined	multinomial random_state	0.040000
y t -> dot y t x t	local lift transpose through dot node	0.333333
dr_vals	dr_vals	0.555556
the type that represents an array	array type	0.055556
will print a warning message on	gof deprecated filename msg	0.041667
wrt, computes	core subgraph	0.062500
copies the stack trace from one or more	gof copy stack trace	0.055556
uses shared	shared	0.062500
an instance of _maker which handles much of	function maker i o m	0.066667
scalar	scalar shared	0.083333
the fgraph outputs that will	fgraph	0.012195
return the op	op	0.009174
a error if cudnn can't be used	gpuarray no cu dnnraise apply fgraph	0.200000
to outside of scan	scan output	0.125000
disabled by default that removes all	local remove all	0.166667
if a and b	a b	0.066667
apply the list of policies to name r	apply policy policy r name	1.000000
a uniform distribution	uniform	0.043478
thunk	linker make thunk	0.125000
convert degree a to	deg2rad a	0.333333
log(softmax x and replace it with logsoftmax	logsoftmax node	0.125000
that can	gof	0.004762
replace_all_validate revert the replacement if	replace all validate remove fgraph	0.111111
reorder the dimensions of this variable optionally inserting	py operators dimshuffle	0.019231
recursion over	tensor permute row elements rec	0.047619
replacement if the ops in	replace validate replace	0.050000
diff	diff	0.428571
failure_callback for	warn inplace exc	0.500000
this op could be very easy if	prod l op	0.033333
full	gof module	0.058824
and	tensor signal pool grad out	1.000000
do it only on cpu here	local pow specialize	0.250000
the subtensor and its idx_list reorders	idx_list get_count	0.090909
tensor operators to the basic constant class	tensor constant	0.055556
failure_callback for navigatoroptimizer	inplace exc nav repl_pairs local_opt	1.000000
nodes according	nodes inputs outputs cmps	0.166667
input a n-d tensor where n >= 3	3d input ws ignore_border stride	1.000000
transfer to a	py operators transfer	0.125000
if	compile check inputs node storage_map r_vals	0.166667
the source	cmodule location	0.038462
make	make	0.125000
<=	le	0.250000
the graph's apply nodes such that	gof function graph	0.031250
the task that	gof cthunk find task	0.142857
it with logsoftmax x	tensor nnet local logsoftmax	0.076923
cuda	cuda	1.000000
listeners to help the navigator deal	navigator optimizer attach updater fgraph	0.038462
return the idx_list with constant inputs replaced by	constant idx inputs allow_partial only_process_constants elemwise	0.071429
compute the	nnet get	0.250000
maximum see max for the maximum	maximum x	0.142857
erf/erfc opt to track less frequent op	get clients2 node	0.200000
gradient is an alloc of	alloc node	0.037037
determine	adv index broadcastable	0.050000
a series of	many linkers wrappers	0.047619
the inputs and put the variables in	gof pure op perform node inputs output_storage params	0.047619
variable and apply nodes in the original	inputs outputs copy_inputs_and_orphans memo	0.029412
this generates the c code for gpucorrmm	base gpu corr mm c code	0.090909
c code for corr3dmm	nnet base corr3d mm c code	0.090909
numeric	numeric	1.000000
b), axis=0) -> elemwise{scalar op} a	local reduce join	0.111111
diagonalsubtensor and	tensor nnet get diagonal subtensor view	0.083333
return true for any python object a that	gof	0.002381
list of shape	shape feature default infer shape	0.066667
conv2d interface	conv2d	0.142857
make it work for elemwise and gpuelemwise	tensor local elemwise	0.166667
stack	stack	0.466667
a file the graph of a compiled theano	fct	0.083333
see theano tensor sort	tensor tensor py operators sort	1.000000
stop step] transform it into a canonical form	tensor get canonical form	0.045455
with helper_c_code	helper c code	0.142857
apply the list of policies	apply policy policy	0.500000
this function is basically a call to	tensor extract constant x elemwise only_process_constants	0.058824
of l{codeblock} instances returns a string	gof code gen blocks	0.050000
copies the stack trace from one	copy stack trace	0.055556
this variable	tensor tensor py	0.015873
idx_list with constant inputs	subtensor get constant idx inputs	0.250000
to be inserted in the struct	struct node	0.062500
multinomial distributions defined by	multinomial random_state	0.040000
the connection pattern of a subgraph defined by	io connection pattern	0.055556
for corr3dmm (direction="forward"),	base corr3d mm	0.250000
signature object for comparing	signature	0.066667
subtract two matrices at least one of	sub x y	0.333333
to helper_c_code	inc subtensor get helper c code	0.142857
numpy sort function	sort	0.142857
kinds of tensortype	tensor	0.006431
a context	context	0.035714
a variable on the	gpuarray variable	0.166667
checks for the existence of the __unify_walk__ method	gof unify walk a b u	0.037037
for	constant	0.016667
in the graph and returns true if the	in	0.076923
perform the permutation by	perform node x y	0.166667
op that copies a vector	alloc	0.012500
a symbolic input	input	0.047619
false we can't change the value after	init default	0.040000
-> list of variables	variables	0.043478
only used to determine the	tensor adv index broadcastable	0.050000
destroyhandler class detects when a	destroy handler	0.055556
apply_node if those nodes	gof	0.002381
idx_list with constant inputs replaced by their python	constant idx inputs allow_partial only_process_constants elemwise	0.071429
generate c code to allocate outputs	tensor make alloc loop_orders dtype sub fortran	0.200000
allows replacing subgraphs	scan_module clone output replace strict share_inputs	0.071429
a graph is impossible to	destroy	0.009709
uses set and dictionary data structures	non	0.071429
hash from	tensor hash from	0.333333
for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view	0.083333
in profilemode to print the mflops	flops inp outp	0.083333
an array with	node inputs	0.086957
reshapes the input to a leftdims +	input leftdims	0.166667
orv(list1 \ list2)	o n u	1.000000
a vector to the diagonal of	alloc diag	0.027027
the same shape and dtype	dtype	0.022727
convolution gradient with respect to the	gpu dnn conv grad i	0.125000
after a failed fix done in august 2011	load shared variable val	0.142857
scalar 0-dimensional variable	core hessian	0.500000
transfer to a	transfer	0.058824
the first half of v	v	0.011111
that uses	gof function	0.043478
vars to a variable that represents their unification	gof unification	0.125000
get the 0 based level	type get depth	0.050000
iff other is	other	0.090909
turned into macros	cop get	0.033333
called by remove_feature	on detach function_graph	1.000000
form x[0 :] -> x[0]	tensor local useless slice node	0.250000
true for any python object a that would	gof pure	0.033333
this should return an	node name	0.033333
version of var transferred to target	transfer var target	0.200000
numeric shape of all intermediate variables given	tensor shape of variables fgraph input_shapes	0.100000
graphtogpu	to gpulocal opt	0.055556
a sparse format	core sparse	0.066667
delete unversioned dynamic modules	clear unversioned min_age	1.000000
are	tensor check	1.000000
inner graph to ensure	validate inner graph	0.035714
new graphtogpu optimizer	gpuarray register opt2 tracks	0.250000
scalar values default	scalar	0.017857
for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view	0.083333
output dimensions of convolving an image of shape	op get output	0.047619
dot	dot	0.250000
structured addition of a sparse	structured add	0.142857
and b are unified given the	b	0.014925
tuple containing image dimensions	imgshp	1.000000
replace_all_validate revert the replacement if	replace all validate remove	0.111111
list of op classes that this opt	gof local optimizer tracks	0.071429
the inner graph to ensure that it	scan_module scan validate inner graph	0.035714
on f wrt to wrt	core lop f wrt	0.200000
get	get	0.166667
config string	parse config string	0.333333
offers to make	to os environ pathlist	0.038462
return label of apply	apply label node	0.500000
x and replace it with logsoftmax	logsoftmax	0.076923
computes the svd of a	svd	0.034483
be reused in scalar	gof clinker cmodule key fgraph no_recycling compile_args libraries	0.200000
the output error and exit code in a	misc output subprocess popen command	0.100000
times from a multinomial distribution defined by probabilities	sandbox mrg random streams multinomial	0.250000
to help the navigator deal with	navigator optimizer attach updater fgraph	0.038462
to the fgraph this is	fgraph	0.012195
dot product of the specified pieces of vectors	sparse block outer make node o x y	0.066667
input a 4-d tensor it	same size input patch_size	0.166667
the context associated	gpuarray get context	0.111111
attempting to use dnn conv algo_bwd	no dnn algo bwd algo	0.166667
logsoftmax activation function :math \varphi(\mathbf{x})_j =	log softmax	1.000000
it into a canonical form that respects the	tensor get canonical form	0.045455
convert radian	rad2deg	0.166667
is	graph to gpulocal	0.055556
-> softmax_w_bias	nnet local softmax with	0.200000
to type2 from type1 constitutes an upcast	is an upcast type1 type2	0.333333
var transferred to	tensor transfer var	0.100000
raises a badviewmap exception when it detects	compile check viewmap node	0.111111
or none for the outputs	i_shapes	0.050000
whose type is in t float_scalar_types	scalar res dtype	0.333333
print the mflops	nnet conv op flops	0.125000
true for any python object a that	gof	0.002381
convert addsd to faster	local addsd ccode node	0.250000
theano utilization of numpy linalg tensorsolve	tensor solve	0.038462
if converting to type2	type2	0.050000
new instance of this mode	mode clone link_kwargs optimizer	0.333333
makes the folowing changes in the	tensor local mul switch sink node	0.045455
vector variable	vector name	0.500000
inserting broadcasted dimensions	tensor tensor py	0.015873
eq x x -> 1	local useless elemwise	1.000000
allows replacing subgraphs of a	scan_module clone output replace strict share_inputs	0.071429
versions of	sparse local	1.000000
failure_callback for navigatoroptimizer ignore all errors	optimizer warn ignore exc nav repl_pairs local_opt	1.000000
order a graph of apply nodes	sort apply nodes	0.200000
a diff	diff	0.071429
4-d tensor it sets all non	patch_size	0.050000
function that will calculate the	compile orig function	0.166667
diff to make	diff	0.071429
replace_all_validate revert the replacement if the ops in	validate replace all validate remove	0.111111
c code to	type c	0.214286
basic slow python 2d or 3d convolution for	base abstract conv conv img kern mode dilation	1.000000
to convert x into a	x context_name	0.100000
the replacement if the ops in	replace	0.032258
multinomial distributions defined by one-dimensional slices in	tensor multinomial	0.037037
x_pos	x_pos	1.000000
array using	mpisend	0.037037
tensorvariable whose type is in t float_scalar_types	tensor as scalar res dtype	0.500000
lib directories that are needed by one	clinker header dirs	0.055556
_maker which handles much of	mode function maker i o m	0.066667
of the last access	last access time path	0.040000
constant with value	constant	0.016667
this is the equivalent	gpulocal opt	0.055556
to the diagonal	diag	0.023810
the contents of a cache directory and return	name from dir dirname err files	0.500000
deepcopyop	deep copy op	0.250000
output gradient w	conv2d grad	0.111111
and t	node input_storage output_storage	0.038462
raise baddestroymap if	check	0.083333
the r operation on f	f	0.052632
help the navigator	navigator optimizer attach	0.038462
register r's shape	shape	0.010204
_maker which handles much of	compile debug mode function maker i o m	0.066667
g++ version used is the	gof	0.002381
as replace_all_validate revert the	all validate remove	0.166667
support the types involved in this node	inc subtensor do type checking node	0.250000
the n-th order discrete difference	diff x n	0.333333
output type dtype and broadcast there is no	local canonicalize alloc node	0.333333
a random	tensor random streams base random	0.500000
on f wrt to wrt	rop f wrt	0.200000
variable k -> list of variables	variables	0.043478
a symbolic row variable (ndim=2 broadcastable=[true	tensor row name dtype	0.050000
-> single prod()	tensor local op of op	0.500000
variable optionally	dimshuffle	0.014493
x to be	x	0.008772
mod	mod c code	0.125000
return true for small or builtin c	c is simple	0.200000
present in a given "group" (ie	inp out grads	0.166667
wrapped_inputs	wrapped_inputs	1.000000
implement the grad of	grad	0.010417
second half by b	b	0.014925
that unroll the batch	gen conv code unroll batch	0.166667
of integers indicating the version	c code cache version	0.125000
connection pattern	gof io connection pattern	0.055556
solve	sandbox linalg tag solve	1.000000
self _grad_op from user supplied form to	op from graph recompute grad	0.200000
that will be turned into macros for	cop	0.028571
for corr3dmm	base corr3d mm	0.250000
return the indptr	indptr	0.125000
specified axes	tensor addbroadcast x	0.142857
and also their apply_node if those nodes	gof	0.002381
the mflops	flops inp outp	0.125000
dimensions of this	tensor tensor	0.014286
the output dimensions of convolving an	get output	0.047619
print the mflops	base gpu corr3d mm flops inp outp	0.125000
dimensions of this variable optionally inserting	tensor py operators dimshuffle	0.019231
operation for efficiently calculating the dot product	true dot	0.166667
graph	gof function graph import	0.125000
an operation to wait on	mpirecv wait	0.045455
grad of this op	l op	0.033333
an alloc of a scalar	alloc node	0.037037
list of nodes that must	gof	0.002381
v by a with a	v	0.011111
signe	sgn	0.125000
shape_i	check_input	0.111111
implement x[ilist] where ilist is a	advanced subtensor1	0.200000
full	module	0.033333
recognize the updates	get updates	0.034483
inplace optimization that deals with allocempty this will	inplace allocempty op idx	0.166667
return a list of shape tuple or	shape feature default infer shape	0.066667
change the value after the import of theano	config param init default filter	0.040000
apply_nodes to this graph	function graph	0.040000
replace_all_validate revert the replacement if the	replace all validate	0.111111
on the	gpuarray gpu	0.045455
some functiongraph listeners to help the navigator	gof navigator optimizer attach updater	0.038462
is a stabilization optimization	crossentropy to crossentropy with softmax with bias fgraph	0.333333
node i pairs such that node	gof function graph clients	0.100000
this linker's fgraph	make thunk input_storage output_storage storage_map keep_lock	0.333333
constant with value	tensor constant	0.055556
extract a	extract	0.111111
value through	value	0.043478
to pass to helper_c_code	helper c code	0.142857
implement the grad of downsample with max	downsample factor max grad grad	0.333333
attempt to replace a leaf of a	nnet replace leaf arg leaves new_leaves op	0.250000
fgraph	fgraph	0.134146
draw samples from a poisson distribution	base poisson size lam	1.000000
constant scalar 0-d value underlying	scalar constant value	0.333333
logsoftmax x	nnet local logsoftmax	0.076923
variables that	gof clinker	0.033333
symbolic variables in inputs to sharedvariable instances	inputs	0.012658
important note this function uses set and dictionary	out seq scan process node	0.142857
file that was dumped to	f persistent_load	0.052632
exception	bad destroy	0.034483
idx_list with constant inputs	get constant idx inputs	0.250000
similar behaviour as haskell's	fn sequences outputs_info non_sequences	0.333333
reorder the dimensions of	tensor	0.006431
copy of the type	type	0.011905
remove subtensor/advancedsubtensor1 if it takes	useless subtensor node	0.200000
and their	tensor max and argmax a	0.250000
operation to wait on a previously received array	wait	0.022727
is impossible to evaluate because	destroy	0.009709
std	std axis	1.000000
unfortunately conda offers to make	to os environ pathlist var	0.038462
an apply_node recursively search from	import apply_node check	0.066667
the fortran blas	tensor blas	0.333333
inplace on a	inplace a	0.303030
of scan return	scan_module push out scan	0.050000
specific ops of a compiled graph	trace f_or_fgraph ops_to_check bug_print	0.035714
mpi	mpirecv	0.037037
[start stop step] transform it into a canonical	get canonical	0.125000
same on all device we	device node	0.045455
new variable whose gradient will be stored	grad var	0.333333
on the inner graph	scan validate inner graph	0.035714
symbolically cast x to	tensor cast x	0.200000
to raise	raise init	0.200000
an item to six moves	move move	0.250000
if the given graph contains a cycle parameters	contains cycle fgraph	0.333333
to	optimizer	0.062500
gets a scan op	op not_required inputs	0.071429
navigator deal with	gof navigator optimizer	0.038462
an fgraph and a list of variables	fgraph outputs_to_disown	0.047619
given an apply_node recursively search	import apply_node check	0.066667
compute the kernel shape of convolution gradweights	nnet get conv gradweights shape	0.500000
converts this to expm1 a	local expm1 node	0.066667
type's	gpuarray gpu array	0.062500
a compiled theano function's ops supports	core pydotprint fct outfile compact format	0.250000
compute the dot product of	nnet	0.032258
gpuarray	gpuarray	0.116279
a inner nit_sot output of	inner sitsot	0.083333
convolve spatio-temporal filters with	filters	0.032258
some elementary validations on the inner graph to	inner graph	0.035714
graph and get a	graph	0.016393
if the g++ version used is	gof gcc	0.027778
bit like make_loop but when only the inner-most	init_loop_orders olv_index dtypes inner_task	0.200000
the element-wise exponential linear activation	elu x alpha	0.500000
that with the *args directly	local csm properties csm	0.142857
perform the permutation	perform node x y	0.166667
as replace_all_validate revert the replacement if	replace validate replace all validate remove fgraph	0.111111
return a module from the	module from	0.166667
we can't change the value after	default filter	0.040000
c type	c	0.017857
h	h	1.000000
a new variable instance of type self	gof pure type call	0.500000
-> softmax_w_bias matrix bias	nnet local softmax with bias	0.200000
implements the "reverse-mode"	perform node inputs outputs	0.333333
and return full path of the	gof module name from	0.076923
detect if the g++ version used is the	gcc	0.023810
shape tuple or	tensor shape feature default infer shape	0.066667
it with logsoftmax x	local logsoftmax	0.076923
if the named module is a package	meta path importer is package fullname	0.250000
version used	gof	0.002381
return a code string specific	name sub	0.050000
still in the graph	replacements	0.111111
for every node that uses r as	gof function graph	0.031250
c code to extract a	gof clinker type c extract	0.500000
value after	config param init default	0.040000
pattern has functioning c code	gpuarray gpu careduce cuda supports c code	0.250000
global optimizer for pushing out	push out	0.037037
replace_all_validate revert the replacement if the	replace validate replace all validate remove fgraph	0.111111
failure_callback for navigatoroptimizer ignore all errors	ignore exc nav repl_pairs local_opt	1.000000
the -x pattern	is neg	0.166667
to print the mflops	conv op flops inputs	0.125000
convert degree a to radian	deg2rad a	0.333333
special compound l{op} for	crossentropy softmax argmax1hot	0.083333
module from the cache compiling it if necessary	module cache module from key key lnk keep_lock	1.000000
is false we can't change the value after	init default filter	0.040000
* y	y	0.026316
that	gof feature	0.125000
the same on all device we	device node	0.045455
see theano tensor	tensor tensor py operators	0.171875
ops contained within the subgraph	gof ops	0.083333
input_variables output_variables) where function is a thunk	make thunk	0.125000
wraplinker that	gof	0.002381
return connection pattern of subfgraph	op from graph connection pattern node	0.076923
of the type optionally with a new dtype	type clone dtype	0.333333
subgraph between i and o	i o	0.083333
to boundvariable(other_object)	gof unify walk fv o u	0.200000
new random stream	random streams	0.058824
string specifying to the user what obj	core min informative str obj indent_level _prev_obs _tag_generator	0.333333
to help the navigator deal with the	navigator	0.032258
output of scan	out scan	0.035714
of this variable optionally	tensor	0.006431
compute conv output gradient w	tensor nnet conv2d grad	0.333333
assign	assign	1.000000
to this graph	function graph	0.040000
load	misc load	0.250000
features	features	1.000000
with	compat with	0.500000
modulo	mod	0.071429
uniform	tensor uniform random_state size	0.125000
a copy of the type	tensor tensor type	0.041667
ops_to_check	ops_to_check	1.000000
start gradients up to the end variables	grad wrt end start	0.166667
is an alloc of a fully	alloc node	0.037037
by	gof refresh	0.125000
cutils	cutils	1.000000
convert addsd	sparse local addsd ccode	0.250000
string specific to the apply to be	apply	0.016667
copies a	alloc	0.012500
converts number to string by rendering	char from number number	0.142857
an input vector and t is	node input_storage	0.038462
instances	tensor constant	0.055556
the dependence	make dependence	0.043478
row variable (ndim=2 broadcastable=[true false])	row name dtype	0.050000
build the symbolic graph for convolving a mini-batch	input_shape filter_shape	0.027778
optimizer that reduces scan	scan	0.017241
a dense vector	x s	0.142857
lib directories that are	clinker lib dirs	0.055556
fgraph outputs that	fgraph	0.012195
elemwise square root of	sparse sqrt	1.000000
unitary	unitary	1.000000
tensor from polar coordinate specification	tensor complex from polar abs angle	0.250000
of the specified pieces of vectors	sparse block gemv make node o	0.066667
scan to outside of scan	out scan output	0.125000
integers indicating the version	c code cache version apply node	0.125000
a compiled module	module name	0.062500
type numpy typenum that corresponds to	tensor type dtype specs	0.071429
dense vector	svcsr	0.090909
inserting broadcasted dimensions	py	0.014286
that broadcast them	tensor generate broadcasting	0.066667
lower	tensor tril	1.000000
subgraph in functiongraph	function graph replace	0.500000
and only if this enum has this alias	gof enum type has alias alias	0.333333
the "theano config compiledir"	compiledir content	0.166667
if target is 'cpu' this will transfer to	py operators transfer target	0.500000
assert_nonneg	assert_nonneg	1.000000
get the	tensor get	0.250000
is the	gpulocal opt	0.055556
see theano tensor argsort	py operators argsort	1.000000
of scan to outside of scan	scan	0.017241
assign the shape	set shape	0.333333
allow_partial	allow_partial	1.000000
of the __unify_walk__ method for one of the	unify walk a b u	0.037037
compute	tensor tensor constant signature get	1.000000
can't change the value after the import of	default	0.030303
such that	gof function graph	0.031250
for corr3dmm (direction="forward"),	nnet base corr3d mm	0.333333
device	device	0.461538
repeats	repeats	1.000000
gpu_from_host abstractconv ->	local conv gpu conv node	1.000000
the abs and	abs	0.066667
this op scale or inverse the gradient in	core grad scale	0.333333
this variable optionally inserting broadcasted	tensor tensor py	0.015873
of cost	cost	0.045455
a 'requirement' of the destroyhandler	destroy handler	0.055556
operation for efficiently calculating the dot product	dot x	0.166667
this should return	name	0.011111
return connection pattern of subfgraph	graph connection pattern	0.076923
topooptimizer from the input	in2out	0.043478
map old	check_integrity	0.090909
to turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	nnet local softmax with bias	0.200000
popen	popen	1.000000
type's	array type	0.055556
functiongraph listeners to help the navigator deal	gof navigator optimizer attach	0.038462
uniform into sample without replacement from a	choice from uniform	0.166667
a list to	gemm from factored list	0.500000
a max	max	0.062500
determine the broadcast pattern	adv index broadcastable pattern a	0.066667
a module from the cache	gof module cache module from	0.333333
has any duplicates (according	has duplicates	0.333333
returns the bartlett spectral window in	tensor bartlett	0.083333
connection pattern	op from graph connection pattern node	0.076923
dict that	gof function	0.086957
function	random_state low	0.500000
dtype	dtype	0.136364
required return c code to declare variables	c declare name	0.500000
an array	node	0.014815
integers indicating the version	version apply node	0.125000
takes as input a	tensor signal max pool 2d same size input	0.500000
of cost and/or	cost	0.045455
basic theano op that will call the supplied	op itypes otypes infer_shape	0.047619
a package	package	0.142857
a value on the output	output	0.017241
an inplace	inplace	0.025641
return the idx_list with constant inputs replaced by	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
precision	precision	1.000000
f to	f	0.052632
the navigator deal	navigator optimizer	0.037037
old_r to	old_r	0.125000
replacement if	gof replace validate replace	0.050000
of the __unify_walk__ method for	unify walk a b u	0.037037
idx_list with constant inputs replaced by	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
an instance of _maker which handles much of	mode function maker i o m	0.066667
deal with the ignore_trees-related functionality	optimizer attach updater fgraph importer pruner chin	0.250000
is not attempting to use dnn conv algo_bwd	safe no dnn algo bwd algo	0.166667
the outputs from the	outputs	0.045455
can be considered exactly	type values	1.000000
connection pattern	io connection pattern	0.055556
gets a scan	not_required	0.111111
a mini-batch of a stack of 2d inputs	input_shape filter_shape	0.018519
simplify a multiplication tree	simplify mul tree	1.000000
the specified pieces of vectors	sparse block outer make node o x	0.066667
sample from	size	0.076923
compiled graph have a stack	check stack	0.142857
optionally inserting broadcasted dimensions	tensor tensor py operators dimshuffle	0.019231
converts self _grad_op from user supplied form	op from graph recompute grad	0.200000
to the idx list to get the right	get idx list	0.076923
and replace it with logsoftmax x 's grad	tensor nnet local logsoftmax grad	0.200000
not attempting to use dnn conv algo_bwd	dnn algo bwd algo	0.166667
a list of shape tuple	shape feature default infer shape	0.066667
hyperbolic cosine of	tensor cosh	1.000000
return a version of var transferred	tensor transfer var	0.100000
merge-based implementation of theano gof graph	graph	0.016393
can't change the value after the import	core config param init default	0.040000
this function tries	image_shape top_shape border_mode	0.166667
a version of var transferred to	transfer var	0.100000
modifies	give	0.142857
test a gradient	grad fun pt n_tests	1.000000
in defining the gradient	grad	0.010417
equivalent of	graph to gpulocal	0.055556
return a reshaped view/copy of	tensor py operators reshape shape ndim	0.111111
that removes all asserts	tensor local remove all assert	0.055556
c contiguous version	gpu contiguous	0.083333
apply_node recursively search from this	import apply_node check	0.066667
all sigmoid to ultra_fast_sigmoid	tensor nnet local ultra fast sigmoid	0.200000
attributes and tag during optimization	variable attributes	0.333333
none or a tensorvariable whose type	as	0.024390
the dimension axis	axis	0.025641
change the value after the import	param init default	0.040000
the type's	gpu array type	0.062500
integrity	integrity	1.000000
reorder the	tensor py operators	0.015625
register a transfer function for	tensor register transfer fn	0.250000
idx_list with constant inputs replaced	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
run all nodes list of input containers list	perform linker make all input_storage output_storage storage_map	0.333333
compute	nnet get	0.250000
to construct a variable with a sparse matrix	sparse variable x	0.250000
in the graph and returns true	in	0.076923
the destroyhandler class detects when a graph is	destroy handler	0.055556
important note this function uses set and dictionary	scan process node fgraph node	0.142857
remove two kinds of useless reshape	useless reshape	0.200000
context object mapped to the	context	0.035714
variable optionally inserting broadcasted dimensions	operators dimshuffle	0.019231
input_shapes	input_shapes	1.000000
set and dictionary data structures	push out non seq scan	0.125000
are present in a given "group" (ie	inp out grads	0.166667
from the loaded cache	gof module cache get	0.250000
do some special work if	gof	0.002381
gradient function should return	tensor matrix inverse r op inputs eval_points	0.500000
shape of convolution gradinputs	get conv gradinputs shape	1.000000
type localoptgroup instead of a global optimizer	group db	0.500000
replace a	replace	0.032258
cache or the	cache	0.034483
tensor_var	tensor_var	1.000000
out_in	out_in	0.833333
generate c	c	0.071429
around c_extract that initializes py_name from storage	gof get c extract r name sub	0.250000
create a six moves urllib namespace that	module six	0.043478
of all the node i pairs such	function graph clients	0.200000
can't change the value after	param init default filter	0.040000
pairs that will be turned into macros for	cop	0.028571
suitable dummy values	optimizer provide	0.200000
and a set of arrays	a choices out mode	0.111111
same kinds of	tensor type	0.034483
to make itself the default python and	to os environ pathlist	0.038462
"inshp" with kernels of shape "kshp"	shape inshp kshp stride mode	0.142857
body	body	1.000000
unification	unification	0.461538
return connection pattern of subfgraph defined by inputs	from graph connection pattern	0.076923
compute	tensor nnet conv2d	0.333333
the diagonal of	diag	0.023810
optionally inserting broadcasted	tensor tensor py operators dimshuffle	0.019231
the hack in profilemode to print the mflops	gpuarray base gpu corr mm flops inp outp	0.125000
the variable v is positive semi-definite i	v	0.011111
batch normalization of the given	batch normalization	0.125000
ndarray	tensor	0.003215
1/(1+exp x ->	nnet local inv 1 plus exp node	0.333333
comment	comment	1.000000
use dnn conv algo_bwd	no dnn algo bwd algo	0.166667
computes the output	nnet conv op get output	0.047619
the optimization to the provided l{functiongraph} it may	gof optimizer apply fgraph	0.200000
dimensions of this	tensor tensor py operators	0.015625
of lib directories that are needed by one	header dirs	0.045455
implements the r-operator	max pool rop	0.142857
return a tuple of integers indicating the version	code cache version apply node	0.125000
structured addition of a sparse matrix	structured add	0.142857
variable optionally inserting	tensor py operators	0.015625
alloc of	local alloc	0.111111
a series of wrapper functions instead of just	wrap linker many linkers wrappers	0.047619
parents	parents	0.600000
change the value after	config param init default filter	0.040000
optionally inserting broadcasted	py operators dimshuffle	0.019231
and figure out	scan_module traverse out x x_copy d	0.047619
pattern of subfgraph defined	pattern	0.028571
gpuelemwise	gpu elemwise	1.000000
makes the folowing changes	local mul switch sink node	0.045455
the dimensions of this variable	operators	0.017241
the mflops	nnet conv op flops inputs	0.125000
infer the number of	tensor infer	0.142857
3d inputs with a set of 3d	nnet conv3d input	0.125000
f_or_fgraph	f_or_fgraph	1.000000
psi	psi	1.000000
it work for elemwise and gpuelemwise	tensor local elemwise	0.166667
register a context	gpuarray reg context	0.333333
matrix along the	sp	0.125000
a specified factor takes as	signal pool 2d	0.142857
the idx list to get the right values	get idx list	0.076923
diagonalsubtensor	nnet get diagonal subtensor view x i0	0.083333
by re-writing the file containing the	gof refresh	0.125000
compiles the source code for this linker and	cmodule location	0.038462
some perform() or c_code() modified	bad destroy map	0.142857
to use dnn conv workmem	safe no dnn workmem workmem	0.166667
as replace_all_validate revert	validate remove	0.166667
entry_1	entry_1	1.000000
the type's :attr context_name	gpuarray gpu array type	0.062500
is	opt	0.043478
instance of _maker which handles much of	maker i o m	0.066667
of m1 and the	m1	0.027027
so pyd dll or py file	gof dlimport fullpath suffix	0.333333
from r to new_r	r	0.028571
slice [start stop step] transform it into a	slice	0.038462
changes node inputs[i]	function graph change input node i	0.250000
named module is a package	importer is package fullname	0.250000
node inputs[i] to	change input node i	0.250000
3d	conv3d input	0.125000
value on the output	gpuarray output	0.200000
of suitable dummy values	meta optimizer provide inputs	0.200000
list of lib directories that are needed	header dirs	0.045455
replacement if the ops in the	validate replace	0.050000
sparse node	sparse type	1.000000
a tensorvariable whose	tensor as	0.066667
a gemm	tensor gemm	0.166667
up to the end variables of a	wrt end	0.050000
of max pooling	max pool	0.500000
a series	many linkers wrappers	0.047619
to the type's	type	0.011905
return full path	gof module name from	0.076923
set of ops contained within the subgraph	gof ops	0.083333
transform it into a canonical form	get canonical form	0.045455
that will be instantiated	gof	0.002381
apply nodes according	apply nodes inputs outputs cmps	0.050000
compiles the source code	gof clinker compile cmodule location	0.038462
which each row is a mrg stream state	sandbox mrg random streams	0.033333
from x	x	0.008772
lock to	cache add to	0.142857
this method is primarily used by tensor	gof pure op r op inputs eval_points	0.250000
:param execute if true execute a	misc execute execute verbose m	0.250000
careduce	gpu careduce	1.000000
the rest	rest inputs elemwise	0.125000
basic theano op that will	op itypes otypes infer_shape	0.047619
is the equivalent of localoptgroup	gpulocal opt group	0.055556
fill s v -> alloc(v shape s this	tensor local fill	0.250000
uniform distribution	uniform	0.043478
function performs the svd	tensor svd	0.200000
signature object	constant signature	0.100000
will print a warning message on	deprecated filename msg	0.041667
variables in inputs to sharedvariable instances	inputs	0.012658
we clone this	gof constant	0.333333
integers indicating the version	clinker object c code cache version	0.125000
transform of a real-valued input on the	gpuarray curfft inp norm	0.066667
the values of a shared variable to 0	shared variable zero borrow	0.200000
the dimensions of this variable optionally inserting broadcasted	tensor py	0.015873
symbolic column variable (ndim=2 broadcastable=[false true])	tensor col name dtype	0.200000
a inner nit_sot	inner sitsot	0.083333
same shape	feature same shape	0.333333
returning the output	output	0.017241
any python object a that would	gof	0.002381
be a legal value for a	is valid value a	0.076923
tensortype	tensor	0.006431
the mflops	base gpu corr mm flops inp outp	0.125000
constant scalar 0-d value underlying variable v	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
a < b	lt a b	1.000000
op could be very easy	tensor prod l op	0.033333
of an op	op	0.009174
returns upper bound on the largest eigenvalue of	bound	0.043478
file that was dumped to a zip file	f persistent_load	0.052632
small or builtin c types	gof clinker type c is simple	0.250000
to output nodes of the graph	gof	0.002381
c code	gof clinker op c code	0.333333
true if the named module	fullname	0.066667
order a graph of apply nodes	apply nodes	0.200000
inshp	inshp	1.000000
fill s v -> alloc(v	tensor local fill to	0.250000
replace_all_validate revert the replacement	validate replace all validate remove	0.111111
pieces of vectors and matrices	sparse block outer make node o	0.066667
that need not be checked for nan	compile is numeric value arr var	0.166667
and "init_code" together	init code struct node	0.125000
values from a with or without replacement	random streams base choice size a replace	0.333333
max_input_fct	max_input_fct	1.000000
to make itself the	to os	0.038462
existing start gradients up to the	start	0.040000
optimization makes the folowing changes in the	mul switch sink	0.045455
inserting broadcasted	tensor py operators dimshuffle	0.019231
replace it with logsoftmax x	local logsoftmax	0.076923
initializes py_name to py_none	init r name sub	1.000000
other implementation of mod	scalar mod	0.125000
offers to make itself the default python and	to os environ pathlist var	0.038462
an exception class to raise	raise init	0.200000
version used	gcc	0.023810
a memo a dict that	gof function	0.043478
keepdims	keepdims	0.263158
draw samples from a poisson distribution	base poisson size lam ndim	1.000000
the cache	gof module cache	0.166667
context associated with a name	get context name	0.333333
a new variable instance of type self	pure type	0.142857
as replace_all_validate revert the replacement	replace validate replace all validate	0.111111
for cudnn batch	gpu dnn batch	0.333333
return connection pattern of subfgraph defined by	op from graph connection pattern node	0.076923
scale or inverse the gradient in	core grad scale	0.333333
to print the mflops	tensor nnet conv op flops	0.125000
set in a	a	0.008065
still in	remove fgraph replacements	0.250000
for comparing tensorconstant instances	tensor constant	0.055556
hash from an ndarray	tensor hash from ndarray data	0.333333
function that gets a scan op a	op not_required inputs	0.071429
to the type's	gpuarray	0.023256
none	none	1.000000
of this variable	tensor tensor py	0.015873
only one	sitsot only	0.066667
conv2d	conv2d	0.857143
graph to a new node a clone	clone	0.020833
be a legal value for a variable of	is valid value a	0.076923
multinomial	tensor multinomial	0.074074
proxy for either true_div or int_div depending on	scalar div proxy	0.125000
flatten	flatten	1.000000
to make itself the default python	to os environ pathlist	0.038462
of lib	lib	0.125000
nit_sot output of scan return true iff the	out scan	0.035714
this function uses set and dictionary data structures	push out non seq scan	0.125000
op}(join(axis=0 a b), axis=0) -> elemwise{scalar op} a	local reduce join	0.111111
row variable (ndim=2	row	0.034483
value has	value	0.043478
the input nodes to output nodes of the	gof	0.002381
reason	reason	0.833333
tanh	tanh	0.750000
multiplications and/or divisions	local greedy distributor node	0.166667
version of var transferred to	transfer var	0.100000
a -1 and converts this to expm1	expm1 node	0.066667
multiplication by a	alpha	0.083333
slice [start stop	slice	0.038462
helper function to	helper random_state low	0.500000
c code to	gof clinker type c	0.166667
see theano tensor argmin	tensor py operators argmin	1.000000
helper function for diagonalsubtensor	diagonal subtensor view	0.083333
for creating a class with a metaclass	metaclass metaclass	0.125000
function must return a thunk	thunk	0.021277
elementary validations on the inner	validate inner	0.142857
of this variable optionally inserting	operators dimshuffle	0.019231
transform it into a canonical	tensor get canonical	0.125000
where function is a thunk	linker make thunk	0.125000
expression	expression	1.000000
into macros for use within the op code	cop get op params	0.200000
following	global	0.142857
a variable with the -x pattern	neg var	0.166667
this function is basically a call to tensor	extract constant x elemwise only_process_constants	0.058824
that initializes	gof	0.002381
distribution	streams	0.076923
file using external persistence	dump obj file_handler protocol persistent_id	0.500000
for corr3dmm (direction="forward"), corr3dmm_gradweights	base corr3d mm	0.250000
baddestroymap	inputs	0.012658
helper function for diagonalsubtensor and	nnet get diagonal subtensor view	0.083333
bitwise a ^ b	xor a b	1.000000
none or a tensorvariable whose	as	0.024390
optimizer for	optimizer	0.062500
a symbolic row variable (ndim=2 broadcastable=[true	tensor row name	0.050000
the inputs required	gof inputs variable_list blockers	0.058824
to	compile	0.076923
sum(a / dimshuffle{ } b axis=l) ->	tensor local	0.025641
for corrmm	base corr mm	0.250000
two kinds scalar constants and the rest	rest inputs	0.125000
reg	reg	1.000000
i of	feature set shape i r i	0.500000
the 1d kernel that can be used	kernel 1d	0.050000
this function uses set and dictionary data structures	out non	0.125000
3d inputs with a set of 3d filters	nnet conv3d input filters	0.142857
important note this function uses set	push out seq scan process node fgraph node	0.142857
concatenate tensortypes along the given axis	join axis	0.333333
attaches	bookkeeper on attach fgraph	1.000000
legal value for	is valid value	0.250000
is the	to	0.017544
would be a legal value for a	is valid value a	0.076923
add	key data add	0.500000
the same rounding than numpy round half	round half	0.100000
the last access of	last access time path	0.040000
required the optimization local_useless_rebroadcast and local_rebroadcast_lift	rebroadcast opt rval	0.200000
see theano tensor prod	py operators prod axis dtype keepdims	1.000000
diagonal of an empty matrix	diag	0.023810
of the specified pieces of vectors	sparse block gemv make node o w h	0.066667
cross-entropy between an approximating distribution and	nnet categorical crossentropy coding_dist true_dist	0.111111
compute a generalized dot product over provided	tensordot a b	1.000000
return a	name sub	0.050000
merge some gpucareducecuda and gpuelemwise	elemwise careduce node	1.000000
tries to recognize the updates ordereddict	scan_module get updates	0.034483
variable and apply nodes in the original graph	inputs outputs copy_inputs_and_orphans memo	0.029412
half of v by a with a	v	0.011111
function tries to recognize the updates	scan_module get updates	0.034483
solve operation c	tensor solve	0.038462
equivalent	to	0.017544
to detect	tensor detect	0.166667
reorder the dimensions of this variable	tensor tensor py	0.015873
cumulative sum	cumsum x	0.333333
elemwise and gpuelemwise	local elemwise	0.166667
computes the sum along	sum	0.038462
that will	gof clinker	0.033333
elementwise division (inplace on a)	int div inplace a b	1.000000
the type's :attr	gpu array	0.055556
directory and return full	gof module name from dir	0.071429
helper function for diagonalsubtensor	get diagonal subtensor view	0.083333
this function is only used to determine	tensor adv index broadcastable	0.050000
the source code for	gof clinker compile cmodule location	0.038462
the version	c code cache version	0.125000
c_cleanup that decrefs py_name	gof get c cleanup r	0.250000
equivalent of localoptgroup for graphtogpu	to gpulocal opt group	0.055556
a >	tensor gt	0.333333
this function is basically a call to tensor	tensor extract constant x elemwise only_process_constants	0.058824
+ alpha * dot	gemv c	1.000000
add tag trace	add tag trace thing user_line	0.166667
function :func images2neibs <theano tensor nnet neighbours images2neibs>	nnet images2neibs ten4 neib_shape neib_step mode	0.333333
pooling gradient	gpu dnn pool grad	1.000000
it with logsoftmax x 's grad	nnet local logsoftmax grad	0.200000
x and there is already an	sandbox linalg local	0.142857
image shape of	shape 1axis image_shape	0.250000
pattern of	pattern	0.057143
a | b inplace on a	tensor or inplace a b	0.333333
dependence of nodes in a graph	make dependence	0.043478
connection pattern of subfgraph defined by inputs and	from graph connection pattern node	0.076923
expm1 a	local expm1	0.066667
symbolic input	input error	0.500000
connection pattern of subfgraph	graph connection pattern node	0.076923
a particular stream	tensor random streams getitem item	0.142857
modulo of m1	m1	0.027027
r_vals	r_vals	0.454545
inner graph to ensure	scan validate inner graph	0.035714
function is basically a	tensor extract constant x elemwise only_process_constants	0.058824
other scalar op	scalar	0.035714
moved objects in six moves urllib_error	module six moves urllib error	0.142857
op could be very easy if	tensor prod l op	0.033333
create a six moves	module six	0.043478
to turn softmax(sum_of_stuff) -> softmax_w_bias	nnet local softmax with	0.200000
referred to	compile register linker	0.250000
exception object with	raise with op	0.333333
return a function that will calculate the	compile orig function	0.166667
numpy ones_like	tensor ones like model dtype opt	0.333333
a compiled module	module	0.033333
op classes that this opt	gof local optimizer tracks	0.071429
of this variable optionally inserting broadcasted	tensor tensor py operators	0.015625
important note this function uses set and dictionary	seq scan process node	0.142857
as replace_all_validate	validate remove fgraph	0.166667
the dimensions of this variable optionally inserting broadcasted	tensor tensor	0.014286
called whenever node inputs[i] is	gof feature on change input function_graph node i	0.333333
break aliasing of outputs	wrapped_inputs wrapped_outputs	0.166667
return full path of	module name from	0.076923
policies to name r	policy policy r name	0.250000
c-implementation of	csr c code node name inputs	0.333333
function computes the output shape	get out shape ishape kshape border_mode	0.500000
default that removes all asserts	remove all assert	0.055556
specified pieces of vectors and matrices	sparse block gemv make node	0.066667
return a reshaped view/copy of this	tensor tensor py operators reshape shape ndim	0.111111
gcc	gcc	0.142857
a specified factor takes as input a n-d	tensor signal pool 2d input ws ignore_border stride	0.100000
the parents	parents	0.100000
sparseblockgemv check sparseblockgemv's	gemv	0.100000
struct	struct	0.333333
gpucorrmm (direction="forward"), gpucorrmm_gradweights	corr mm	0.083333
list of the parents	parents	0.100000
help the navigator	navigator optimizer attach updater	0.038462
other implementation of mod	scalar mod c code node name	0.125000
not_required	not_required	0.555556
upsampling this function builds the 1d kernel that	kernel 1d	0.050000
r-operator for the downsample	max pool rop	0.142857
remove subtensor/advancedsubtensor1 if it takes the full input	local useless subtensor node	0.200000
replace_all_validate revert the	validate	0.090909
dict op -> total number of nodes	compile profile stats op nodes	1.000000
to a gist and return the	gist	0.040000
sample from one or more multinomial distributions	multinomial random_state size n	0.333333
a memory alias that wasn't	bad view	0.027027
nodes of	gof	0.002381
of aliasing and destructive	destroy	0.009709
"reverse-mode" gradient [1]_ for the cholesky factorization	cholesky grad perform node	0.500000
on compute capability 2	dev20	0.166667
convert data to something which can be associated	tensor type filter data strict allow_downcast	1.000000
uniform	tensor uniform random_state	0.125000
factor takes as input a n-d tensor where	tensor signal pool 2d input ws ignore_border stride	0.100000
grad	grad dict var_to_app_to_idx grad_dict	1.000000
class whose c_code and perform implementations didn't match	compile bad thunk output offending	1.000000
the image shape of convolution gradweights	conv gradweights shape 1axis	0.500000
directory	gof	0.002381
value through it	value	0.043478
a variable	variable	0.088889
detect	gof	0.002381
>= b inplace on a	tensor ge inplace a b	0.500000
inverse fast fourier transform with real-valued output	cuirfft inp norm is_odd	0.333333
to the apply to be	apply	0.016667
arctangent of a (inplace on a)	tensor arctan inplace a	1.000000
transform it into a canonical form that	get canonical form	0.045455
matrix solve	solve	0.032258
graph optimizer that reduces scan	scan	0.017241
disconnected_inputs	disconnected_inputs	1.000000
the confusion matrix of	nnet confusion matrix	0.166667
stream in this container	streams gen op	1.000000
reorder the dimensions of this	tensor py operators dimshuffle	0.019231
that converts a function into a basic theano	itypes otypes infer_shape	0.142857
the broadcast pattern for advancedsubtensor output variable	pattern a	0.066667
zero dimensional	empty	0.166667
the c code for corrmm	nnet base corr mm c code	0.090909
caller is replace_all_validate just raise the exception	gof validator validate fgraph	0.125000
that gets a scan	not_required	0.111111
return a	name x z	0.333333
the gradient function should	matrix inverse r op inputs eval_points	0.500000
basic theano op that will call	op itypes otypes infer_shape	0.047619
clip x	tensor clip x	0.250000
add f to :doc oplist	tensor constructor f	1.000000
a poisson	tensor random streams base poisson	0.500000
duplicate this apply instance in a new graph	gof apply clone with new inputs inputs strict	0.250000
function to get the	get	0.020833
this is	graph to gpulocal	0.055556
to a new node a clone in	clone	0.020833
offers to make itself the default	to os environ pathlist var newpath	0.038462
makes the folowing changes in the graph t	local mul switch sink node	0.045455
input by a specified factor takes as	signal pool	0.142857
with a triangular solve	sandbox linalg tag solve triangular node	0.142857
return connection pattern of	op from graph connection pattern	0.076923
revert the replacement if	replace all	0.050000
changes node inputs[i] to new_r	gof function graph change input node i new_r	0.500000
manipulate the subgraph in functiongraph	gof function graph replace r new_r reason verbose	0.250000
initializes py_name from storage	get c extract r name	1.000000
n	n	0.611111
is a	importer is	0.250000
a inner nit_sot output	inner sitsot	0.083333
code to the task	cthunk find task	0.142857
that unroll the batch size loop	conv code unroll batch	0.166667
change all sigmoid to ultra_fast_sigmoid	tensor nnet local ultra fast sigmoid node	0.200000
offers to make itself the default python	to os environ	0.038462
diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x i0 i1	0.083333
function for diagonalsubtensor and	diagonal subtensor view x i0	0.083333
on the inputs and put the variables in	pure op perform node inputs output_storage params	0.047619
type's :attr	gpuarray gpu	0.045455
shorthand for product between several dots	tensor matrix dot	0.333333
names when persisting to zip file	id	0.100000
to the diagonal of	alloc diag	0.027027
we can't change the value after	param init default	0.040000
with logsoftmax x 's grad	tensor nnet local logsoftmax grad	0.200000
variable optionally inserting broadcasted dimensions	py operators	0.015625
of the class file into an aboslute path	gof cop get path cls f	0.166667
openblas threads interface	tensor openblas threads text	0.250000
a -1 and converts this to expm1 a	expm1	0.050000
add an item to	compat add	0.250000
determine the	adv index broadcastable	0.050000
that will be instantiated by c_extract	gof clinker	0.033333
baddestroymap	compile check inputs node	0.166667
associate linker with	gof op wise clinker accept	1.000000
jacobian	jacobian	0.714286
python not the other implementation of mod	mod	0.071429
a canonical	tensor get canonical	0.125000
epoch of the last access of a	last access	0.040000
for	tensor type	0.034483
for comparing tensorconstant instances	constant	0.016667
multinomial	multinomial random_state	0.040000
that 1) this	gof	0.002381
received array	mpirecv	0.037037
list remove	remove	0.035714
to the	array type	0.055556
this to expm1 a	tensor local expm1	0.066667
optimization makes the folowing changes in the graph	tensor local mul switch sink	0.045455
print the mflops	gpuarray base gpu corr mm flops inp outp	0.125000
for diagonalsubtensor	tensor nnet get diagonal subtensor	0.083333
cache or the disk	gof module cache	0.083333
2d filters	input filters	0.117647
to compute	tensor nnet get	0.250000
to roll tensortypes along the given axis	tensor roll x shift axis	0.333333
of lib directories	clinker lib dirs	0.055556
cache if available	cache call fn args key	0.200000
copies the stack trace from one or more	stack trace	0.055556
sum	sum axis dtype keepdims acc_dtype	1.000000
the new graphtogpu optimizer	register opt2 tracks	0.250000
return a thunk that is a	thunk	0.021277
initializes py_name to py_none	get c init r	1.000000
recognize the updates	updates	0.029412
names to	names	0.047619
given a slice	slice	0.038462
replace_all_validate revert the replacement if the ops	gof replace validate replace all validate remove fgraph	0.111111
generates the c code for corr3dmm	tensor nnet base corr3d mm c code	0.090909
the dimensions of	operators dimshuffle	0.019231
if the given graph contains a cycle parameters	gof contains cycle fgraph	0.333333
to break aliasing	wrapped_inputs wrapped_outputs	0.166667
the user is not attempting to use dnn	no dnn	0.125000
shape of convolution gradweights	get conv gradweights shape	0.333333
apply_node recursively search from this node	import apply_node	0.066667
inner graph	inner graph	0.035714
the folowing changes	tensor local mul switch sink	0.045455
given a slice [start stop step]	slice	0.038462
lazy loading of moved objects	moves urllib	0.115385
platform	platform	0.500000
gradients up to the end variables of	wrt end	0.050000
perform the	perform node	0.083333
along the given axis es of	axis ddof keepdims	0.083333
of a file	file	0.125000
name to	gpuarray	0.046512
to make itself the default	to os environ pathlist	0.038462
type numpy typenum that	type dtype specs	0.071429
required anymore and should be removed and	outs	0.050000
get the 0 based level of the list	list type get	1.000000
variable optionally	tensor tensor py	0.015873
this function tries	top_shape	0.137931
compute 1d kernel for	tensor nnet	0.017544
helper function for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x	0.083333
gradient the finite fourier transform is viewed as	fourier grad inputs cost_grad	1.000000
offers to make itself the default python	to	0.017544
constitutes an upcast	tensor is an upcast	1.000000
will print a warning message on the	deprecated filename msg	0.041667
variable optionally inserting	operators	0.017241
with respect to wrt, computes	subgraph	0.047619
navigator deal with the	navigator optimizer attach updater	0.038462
by reducing the number of multiplications and/or divisions	local greedy distributor node	0.166667
share_memory	share_memory	1.000000
compute 1d kernel for	nnet	0.016129
of this op could be very	prod l op	0.033333
the struct initialization code	init code struct	0.125000
of given shape and flags	out shape imgshape ws ignore_border stride	0.200000
the *args directly	local csm properties csm node	0.142857
feature should remove any dynamically-added	feature	0.083333
we can't change the value after the	config param init default	0.040000
wraplinker that runs a series of wrapper	gof wrap linker many linkers wrappers	0.071429
to print the mflops	corr mm flops inp outp	0.125000
-1 and converts this to expm1 a	local expm1 node	0.066667
outputs from the inputs	inputs outputs	0.066667
proxy for either true_div or	proxy	0.095238
load an array from disk	load from disk	1.000000
dim_y	dim_y	1.000000
log gamma function	tensor gammaln a	1.000000
value after the import of theano	param init default filter	0.040000
"lifts" dimshuffle through elemwise operations and merges	local dimshuffle lift	0.250000
replace_all_validate revert	validate remove	0.166667
raises a badviewmap exception when it detects the	compile check viewmap node storage_map	0.111111
wants to add some requirements to the	optimizer add requirements	0.166667
by probabilities	sandbox mrg random	0.333333
optionally inserting broadcasted	operators	0.017241
variable with	variable x name	0.083333
still in	fgraph replacements	0.250000
implements the "reverse-mode" gradient	grad perform node inputs outputs	0.166667
a function that reduces a contiguous buffer	gpuarray inline reduce n buf pos count	0.142857
computes the svd of a matrix :math a	svd	0.034483
along the given axis es of a	axis ddof keepdims	0.083333
each row is a mrg stream state	mrg random streams	0.033333
special compound l{op} for the	argmax1hot	0.058824
of	operators	0.017241
out in a prior reduction of x	x	0.008772
a == b	tensor eq a b	1.000000
to the type's	gpu array	0.055556
memory alias that	bad	0.013158
by transferring each	remove	0.035714
scan function that uses	scan_module scan	0.333333
permutations of	tensor permutation random_state size n	0.500000
to wait on a previously	mpirecv wait	0.045455
this compiles the source	gof clinker compile cmodule location	0.038462
columns	col	0.142857
base 2 logarithm of a	tensor log2 a	0.500000
of cutils_ext	gof compile cutils	0.166667
and applies them to	local	0.014085
replace_all_validate revert the replacement if the ops	validate replace all validate remove	0.111111
override clinkertype c_declare	type c declare name sub check_input	0.333333
op	compile profile stats op	0.166667
to determine the broadcast pattern	adv index broadcastable pattern a idx	0.066667
return complex-valued tensor from polar	complex from polar	0.250000
helper function for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x i0	0.083333
computes the confusion matrix of	confusion matrix	0.166667
convert x into a	x context_name	0.100000
change	change	1.000000
performs batch normalization of the	gpuarray dnn batch normalization	0.125000
removes	tensor local remove	0.166667
important note this	out seq scan process node fgraph node	0.142857
scan to outside of scan	out scan	0.035714
is an alloc	alloc node	0.074074
return the	node name	0.033333
gpu	gpu optimizer	0.500000
all the node i pairs such that	gof function graph clients	0.100000
topooptimizer from the input nodes to	in2out	0.043478
gradients up to the end	wrt end	0.050000
version of platform platform()	core short platform r p	0.142857
convert degree a to radian(inplace on a)	tensor deg2rad inplace a	1.000000
we can replace that with the *args directly	sparse local csm properties csm node	0.142857
returns the connection pattern	io connection pattern	0.055556
add tag trace to an node	gof add tag trace thing user_line	0.166667
to concatenate tensortypes	tensor join	0.250000
a memory alias that	bad	0.013158
c type numpy typenum that	type dtype specs	0.071429
conda offers to	to os environ	0.038462
accept	accept	1.000000
series of wrapper functions instead of just	linker many linkers wrappers	0.047619
multinomial distributions defined by one-dimensional	multinomial	0.024390
transform it into a canonical form	canonical form	0.045455
return a list of the parents	get parents	1.000000
out for occurrences of values identical with	forced replace out	0.500000
last access of a given file	last access time	0.040000
use	use	1.000000
as replace_all_validate revert the replacement if the ops	validate replace all validate remove fgraph	0.111111
choice function	choice	0.125000
each shape that broadcast them to	generate broadcasting	0.066667
cache	call cache	0.200000
sharedvariable constructor for	value name strict allow_downcast	0.500000
reorder	tensor tensor	0.014286
x y idx idx) ->	local	0.014085
all unknown variables and apply_nodes to this graph	function graph import	0.125000
a memory alias that	bad view	0.027027
return true if the named	fullname	0.066667
of x default reverse them	tensor transpose x axes	0.200000
return connection pattern of	from graph connection pattern node	0.076923
along the given axis es of a tensor	axis dtype	0.083333
a new graph	with new inputs	0.166667
to determine the broadcast pattern	tensor adv index broadcastable pattern a	0.066667
attempt to replace a leaf of a multiplication	nnet replace leaf arg leaves new_leaves op	0.250000
softmax(sum_of_stuff) -> softmax_w_bias	nnet local softmax with	0.200000
complex conjugate of z	tensor conj z	0.333333
context object mapped to the type's :attr	gpuarray gpu array type context	0.090909
cache directory	from dir	0.125000
obtain lock on compilation directory	get lock lock_dir	1.000000
insert deepcopy in	compile insert deepcopy	0.333333
the one	tensor to one	0.125000
vector 1-dimensional variable	jacobian	0.142857
dummy file with these	cls flag_list preambule body	0.250000
an optimization disabled	node	0.007407
a b), axis=0) -> elemwise{scalar op}	tensor local reduce join node	0.111111
compute sum of non nan / inf values	tensor constant signature get sum	0.142857
ger	ger or gemv node	1.000000
new fgraph check that 1) this destroyhandler wasn't	gof destroy handler	0.250000
an input that wasn't in the	bad	0.013158
with the -x pattern	nnet is neg var	0.166667
elemwise arctanh of	sparse arctanh	1.000000
changes node inputs[i]	change input node i	0.250000
optional	clinker op c	0.500000
we can't change the value after	param init default filter	0.040000
convert addsd to	local addsd	0.250000
find	find bad optimizations2	0.333333
the convolution gradient with respect to the	gpu dnn conv grad	0.125000
input to a	input	0.023810
pattern of a subgraph defined	pattern	0.028571
with debug info	with op node	0.166667
used to determine the broadcast pattern for	adv index broadcastable pattern a idx	0.066667
convert	tensor make	0.076923
symbolic integer scalar	unpack	0.125000
code to allocate outputs	tensor make alloc loop_orders dtype sub fortran	0.200000
graph to ensure that	graph	0.016393
and only adds dimension to	tensor local	0.025641
meta path importer to import six moves	six meta path importer	0.333333
a mrg stream state and they are	mrg random streams	0.033333
inner graph to ensure	scan_module scan validate inner graph	0.035714
decrefs py_name	get c cleanup r name sub	1.000000
node	node	0.074074
a previously received array using	mpirecv	0.037037
unfortunately conda offers to make itself the default	to os environ pathlist	0.038462
compare true iff other is the	tensor tensor type eq other	0.250000
the c code for corrmm (direction="forward"),	tensor nnet base corr mm c code	0.090909
make a schedule	sort schedule	0.333333
i	shape feature set shape i r i	0.500000
of this variable optionally inserting broadcasted dimensions	tensor tensor	0.014286
pieces of vectors and	sparse block gemv make node o	0.066667
max and argmax over	max and argmax	0.125000
of the last access of a given file	last access time path	0.040000
computes the r operation on f	f	0.052632
cost and/or from existing	cost	0.045455
on all device we do it only	device node	0.045455
2d filters	filters	0.064516
to a max	tensor local max	0.250000
the output after pad_dims	unpad dims output input leftdims rightdims	0.333333
with constant inputs replaced by	constant idx inputs allow_partial only_process_constants elemwise	0.071429
recognize the updates ordereddict	updates	0.029412
wait on a previously sent array using	wait	0.022727
moved objects in six moves	six moves urllib error	0.142857
data structures	push out non seq scan	0.125000
python not the other implementation of mod	scalar mod c code node name inputs	0.125000
save_pkl	save_pkl	1.000000
compute conv output gradient w r	nnet conv3d grad	0.333333
repeat	repeat repeats	1.000000
tuple of integers indicating the version	cache version	0.125000
node	node output_indices alloc_ops	0.142857
inserting broadcasted	dimshuffle	0.014493
the list remove	remove	0.035714
that unroll the batch size loop	nnet gen conv code unroll batch	0.166667
of moved objects in six moves urllib_request	module six moves urllib request	0.333333
in the list remove are	remove reason	0.142857
the *args directly	csm properties csm	0.142857
sharedvariable constructor for	constructor value name strict allow_downcast	1.000000
the 2d kernel that can be used to	kernel 2d	0.050000
copies the stack	copy stack	0.333333
determine the broadcast pattern	adv index broadcastable pattern	0.066667
list of outputs	outputs	0.045455
specified factor takes as	signal pool 2d	0.142857
to wrt, computes	core subgraph	0.062500
apply to be inserted in the	apply node	0.031250
a new	with new inputs	0.166667
or more multinomial distributions defined by one-dimensional	tensor multinomial random_state	0.040000
folowing changes in the graph	local mul switch sink	0.045455
for pushing out the variables inside	out	0.018519
row variable (ndim=2 broadcastable=[true	tensor row	0.050000
scan makes it run inplace	scan inplace	1.000000
replace element i of shape_of[r] by s_i	shape i r i s_i	1.000000
for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view	0.083333
replacement if the ops in	replace validate replace all	0.050000
safely compute ceil(float_division	ceil intdiv	1.000000
of shape tuple or	feature default infer shape	0.066667
getter method for self _rop_op	from graph get rop op	1.000000
the other implementation of mod	mod c code node name inputs	0.125000
if the g++ version used is	gof	0.002381
some functiongraph listeners to help the navigator deal	navigator	0.032258
a list of compilation flags from config blas	libs flags libs_dir include_dir	0.052632
optional return	object c	1.000000
stream	tensor random streams	0.285714
for comparing tensorconstant	tensor constant	0.055556
replace all subtensor(make_vector) like [a b c][0] ->	local subtensor make vector node	1.000000
returns	gof local	0.250000
clip x to be between min	clip x min	0.500000
as other scalar op	scalar	0.035714
the other implementation of mod	mod c code node name	0.125000
dictionary of arguments to pass to	args	0.025641
the values of a shared variable to 0	compile shared variable zero borrow	0.200000
as	compile as	0.050000
values identical with	forced replace	0.333333
f	f	0.421053
function to get the 0	get	0.020833
from a with or without replacement	tensor random streams base choice size a replace	0.333333
op}(join(axis=0 a b), axis=0) -> elemwise{scalar op} a	local reduce join node	0.111111
helper function for diagonalsubtensor and	get diagonal subtensor view x	0.083333
an input vector and t is a	node	0.007407
this function copied function will	compile function	0.250000
in a functiongraph a constant that is	constant error	0.166667
function to get the 0 based level	type get depth	0.050000
the output error and	misc output	0.066667
do it only on cpu here	tensor local pow specialize	0.250000
workmem	workmem	1.000000
shape	compile shape	0.250000
print the mflops	flops inp outp	0.125000
of this variable	operators	0.017241
context object mapped to the type's :attr context_name	type context	0.090909
default that removes	local remove	0.166667
to compute the kernel shape of convolution gradweights	nnet get conv gradweights shape	0.500000
as python not the other implementation of mod	mod c code node name inputs	0.125000
of this variable optionally inserting	py operators dimshuffle	0.019231
input a 4-d tensor it	pool 2d same size input patch_size	0.166667
performs batch normalization of	gpuarray dnn batch normalization	0.125000
to outside of scan	out scan output	0.125000
compute sum of non nan /	tensor constant signature get sum	0.142857
remove0	remove0	1.000000
replace_all_validate revert the	all validate remove fgraph	0.166667
nodes according to a list	nodes inputs outputs cmps	0.166667
2^a	exp2	0.166667
min	min	0.500000
by doing a recursion over the	tensor permute row elements rec	0.047619
one or more multinomial distributions	tensor multinomial	0.037037
kinds	type	0.011905
shape tuple or	infer shape	0.066667
kinds scalar constants and the rest	rest inputs	0.125000
important note	scan_module push out seq scan process node fgraph	0.142857
an instance of _maker which handles much	compile debug mode function maker i o m	0.066667
pushing out the variables	push out	0.037037
get the	get	0.041667
with a metaclass	compat add metaclass metaclass	0.125000
it with logsoftmax x	nnet local logsoftmax	0.076923
generic exception raised to	error	0.025000
the equivalent	graph to gpulocal opt	0.055556
wrap an existing proxy	proxy db	1.000000
be inserted in the module initialization	init	0.058824
clear	clear	1.000000
**inherit from**: - :class enumlist	cenum type	1.000000
the constant scalar	core get scalar constant	0.333333
representing a value on a certain gpu	gpu array	0.055556
x is	x	0.008772
dimensions of this variable optionally inserting	tensor tensor py	0.015873
and its substitute take different runtime values	bad optimization	0.333333
to the task	task	0.083333
in	bad view	0.027027
this op	gof clinker op c	0.250000
than numpy round half to even	round half to even	0.166667
old_r to new_r	old_r	0.125000
a cache directory and return full	gof module name from dir	0.071429
offers to make itself	to os	0.038462
graph and get a memo	function graph	0.040000
the navigator	gof navigator optimizer attach updater fgraph	0.038462
inner	inner sitsot	0.083333
compiled graph have a stack	gof check stack	0.142857
is an input vector and	node input_storage output_storage	0.038462
the original	inputs outputs copy_inputs_and_orphans memo	0.029412
doing a recursion over the	permute row elements rec	0.047619
input_variables output_variables) where function is a thunk	thunk	0.021277
:type cost scalar 0-dimensional variable	hessian cost wrt consider_constant disconnected_inputs	0.500000
this op	l op	0.033333
wrt, computes gradients	subgraph	0.047619
detect if	gof gcc	0.027778
inserted to maintain order	searchsorted x v side sorter	0.142857
which can be referred to by	compile register	0.200000
this function uses set and dictionary data structures	scan_module push out non	0.125000
compiles the source code for	gof clinker compile cmodule location	0.038462
to expm1 a	local expm1	0.066667
c header for openblas threads interface	openblas threads text	0.250000
remove incsubtensor when we overwrite the full	tensor local useless inc subtensor node	0.066667
return	name sub check_input	0.500000
the connection pattern of a subgraph	connection pattern	0.032258
defined by indices out_idxs and	out_idxs	0.050000
numpy-compatibility method if x is	diag x	0.200000
of this	tensor tensor py	0.015873
return the idx_list with constant inputs replaced by	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
1d kernel that can be used to upsample	kernel 1d ratio normalize	1.000000
function for diagonalsubtensor	tensor nnet get diagonal subtensor view	0.083333
make code	code	0.050000
of suitable dummy values	local meta optimizer provide	0.200000
this	gof seq	0.200000
complete hashable signature of the module we	clinker cmodule key	0.166667
operation for efficiently calculating the dot product	true dot x	0.166667
vector	vector	0.466667
important note	seq scan process node fgraph node	0.142857
a graph is impossible	destroy	0.009709
pack c types back into a	clinker type c sync	0.111111
can't change the value after the import	core config param init default filter	0.040000
index	index	1.000000
on the inner	inner	0.041667
to help the navigator deal	navigator optimizer	0.037037
context associated with a	context	0.035714
the replacement if the ops in the	replace	0.032258
equivalent of numpy ones_like	tensor ones like model dtype	0.333333
function performs the svd on cpu	svd a full_matrices compute_uv	0.200000
original graph to a new	inputs outputs copy_inputs_and_orphans memo	0.029412
function for diagonalsubtensor	nnet get diagonal subtensor	0.083333
inputs outputs variables orphans temps and node_order fields	gof clinker fetch variables	1.000000
type c type numpy typenum that corresponds	tensor type dtype specs	0.071429
write	write	1.000000
replace_all_validate revert	all validate remove	0.166667
the following stats	global stats	0.500000
axis=l) -> sum(a axis={ }) / b	tensor local sum prod div dimshuffle	0.333333
raised when grad	error	0.025000
multiplications and/or divisions	tensor local greedy distributor node	0.166667
performs batch normalization of	nnet batch normalization	0.125000
and outputs	outputs	0.045455
break aliasing of	wrapped_inputs wrapped_outputs	0.166667
the compile lock to	gof module cache add to	0.142857
this compiles the source code for this	clinker compile cmodule location	0.038462
function tries	top_shape border_mode subsample	0.166667
navigator	gof navigator optimizer attach	0.038462
functiongraph listeners to help the navigator deal	navigator optimizer	0.037037
the specified axes	tensor addbroadcast x	0.142857
compute	nnet	0.193548
diagonalsubtensor	diagonal subtensor view	0.083333
run after c_code whether it failed	cleanup node	0.142857
diagonalsubtensor and	diagonal subtensor view x i0 i1	0.083333
c	clinker op c	0.250000
generate a diff to make code correctly indented	misc hooks get correct indentation diff code filename	0.333333
the destroyhandler	add destroy handler	0.125000
return the inputs required to compute the	gof inputs variable_list blockers	0.058824
function is a thunk	linker make thunk	0.125000
unroll the batch size loop	unroll batch	0.166667
hash equal for same kinds of	tensor tensor type hash	0.166667
object with	with op node thunk	0.166667
input vector and	node	0.007407
gets a scan op	op not_required	0.071429
the context object mapped to the type's	array type context	0.090909
to replace a leaf of a multiplication tree	tensor nnet replace leaf	0.100000
series of wrapper functions instead of just one	wrap linker many linkers wrappers	0.047619
2 profiles returned by this	gof seq optimizer	0.200000
validations on the inner graph to ensure that	validate inner graph	0.035714
the value after	config param init default filter	0.040000
vector 1-dimensional variable	core jacobian	0.500000
a signature object for comparing tensorconstant	signature	0.066667
and "code_cleanup"	cleanup node name inputs	0.500000
this variable	operators dimshuffle	0.019231
clone this object but we don't clone the	gof constant clone	0.166667
exception raised to indicate an	error	0.025000
factor takes as input a n-d	signal pool 2d input ws ignore_border stride	0.100000
the dimensions of this variable optionally	py	0.014286
usage inplaceelemwiseoptimizer op optimize fgraph	inplace elemwise optimizer apply fgraph	1.000000
variable type	d3viz type	0.333333
twice gives inconsistent outputs	bad	0.013158
add	inc subtensor add	1.000000
function to get the 0 based level of	get depth	0.050000
dimshuffle is inside an alloc and only adds	local alloc dimshuffle node	0.166667
the -x pattern	nnet is neg	0.166667
in the fgraph to break aliasing	fgraph wrapped_inputs wrapped_outputs	0.111111
some functiongraph listeners to help the navigator	navigator optimizer attach updater fgraph	0.038462
an	mpisend	0.037037
triangular solve	solve triangular node	0.142857
context by mapping it to a name	reg context name ctx	0.500000
old	check_integrity	0.090909
return a symbolic scalar	tensor scalar name dtype	0.166667
image shape of convolution gradweights	conv gradweights shape 1axis image_shape	0.500000
the dimensions of	tensor py operators	0.015625
log(softmax x and replace it with logsoftmax	logsoftmax	0.076923
op a list of indices indicating which	op	0.009174
a that would be a	gof	0.002381
canonical form that	tensor get canonical form	0.045455
the convolution gradient with respect to the inputs	gpu dnn conv grad i	0.125000
according	inputs outputs cmps	0.166667
to help the navigator	gof navigator optimizer	0.038462
with the -x pattern	tensor nnet is neg var	0.166667
referred to	compile register	0.200000
implementation of mod	scalar mod	0.125000
int_div depending on types of x	x	0.017544
alloc would be useless	alloc call	0.333333
that need not be checked for nan and	compile is numeric value arr var	0.166667
duplicate this apply instance in a new graph	gof apply clone with new inputs inputs	0.250000
navigator deal with the	gof navigator	0.038462
wasn't	destroy	0.009709
and rel error	rel	0.111111
scalarconsts	scalarconsts	0.384615
factor takes as	tensor signal pool	0.142857
remove reduction over broadcastable dimensions	local reduce broadcastable node	1.000000
is meant as a shortcut	optimizer optimize fgraph	0.200000
that broadcast them to match	generate broadcasting	0.066667
ger	ger or gemv	1.000000
function is basically a call	tensor extract constant x elemwise only_process_constants	0.058824
or more multinomial distributions defined	tensor multinomial	0.037037
add	add	0.482759
outputs from	outputs mode accept_inplace	0.166667
representing a computation on a certain gpu	gpu array	0.055556
product of the specified pieces of vectors	sparse block outer make node o x	0.066667
c code to extract	clinker type c extract	0.500000
consider_constant	consider_constant	1.000000
the specified pieces of vectors and matrices	sparse block outer make node o	0.066667
if true	verbose m n	1.000000
a vector to the diagonal of an empty	diag	0.023810
more multinomial distributions	multinomial	0.024390
if current paramstype contains the specified theano	theano_type	0.125000
the g++ version used	gof	0.002381
is a	six meta path importer is	0.250000
urllib_parse	parse	0.125000
same	type	0.011905
compute sum	signature get sum	0.142857
loading of moved objects	moves urllib	0.115385
a cache directory and	from dir	0.125000
from variable	get	0.020833
subgraph contained between i and o	gof clone i o copy_inputs	0.333333
execute callbacks calls getattr feature name (*args)	graph execute callbacks name	0.500000
get	type get depth	0.050000
variable with	variable x	0.083333
/ b	b	0.014925
same kinds	tensor type	0.034483
replacement if	replace all	0.050000
the class file into an aboslute path	gof cop get path cls f	0.166667
forward but clip the gradient	grad clip x lower_bound upper_bound	0.250000
str of variable	to str t	0.166667
for corrmm	nnet base corr mm	0.333333
wait on a previously	mpirecv wait	0.045455
connection	from graph connection	0.500000
transform of a real-valued input	rfft inp norm	0.142857
connection pattern of subfgraph defined by	compile op from graph connection pattern node	0.076923
variant on wraplinker that	gof wrap	0.083333
roots	roots	1.000000
memory alias that	view	0.022727
prune	prune	1.000000
return	apply node name	0.500000
b axis=l) -> sum(a axis={ }) / b	local sum prod div dimshuffle	0.333333
a subtensor is inside a	subtensor node	0.066667
for diagonalsubtensor and	diagonal subtensor view x i0	0.083333
apply nodes according to a list of	gof sort apply nodes inputs outputs cmps	0.050000
method query	graph to gpudb	0.142857
replace_all_validate revert the replacement	replace validate replace all validate remove fgraph	0.111111
indicating the version	version	0.093750
-> x remove split with only 1 split	local useless split	1.000000
conda offers to make itself the	to os environ	0.038462
r's shape in the	shape	0.010204
can't change the value after	config param init default filter	0.040000
output	out	0.018519
folowing changes in the graph	tensor local mul switch sink node	0.045455
see theano tensor argmin	tensor tensor py operators argmin	1.000000
required return the	node name	0.033333
of lib	clinker lib	0.333333
module component with similar interface to numpy random	random streams	0.058824
a basic theano	itypes otypes infer_shape	0.142857
removes all asserts from the graph	remove all assert	0.055556
code when doing constant folding	elemwise python constant folding	0.142857
uniform	uniform	0.260870
to this functiongraph and also their apply_node if	gof	0.002381
zip file	misc	0.142857
fv	fv	1.000000
useless reshape	local useless reshape node	0.200000
a bug in the default blas in macos	macos sdot bug	0.500000
alloc val [x y] -> alloc(val[ ])	tensor local subtensor of alloc node	1.000000
fill a	tensor second inplace a	0.333333
signals_shape	signals_shape	1.000000
the	py operators	0.015625
for pushing out	out	0.018519
reorder	operators	0.017241
raise	inputs node	0.100000
specific ops of a compiled graph have a	trace f_or_fgraph ops_to_check bug_print	0.035714
builds the 1d kernel that	kernel 1d	0.050000
the output shape	out shape	0.250000
helper function for diagonalsubtensor and	get diagonal subtensor view	0.083333
detect log(softmax x and replace it with logsoftmax	logsoftmax node	0.125000
input_shape	input_shape	1.000000
function expects the compile lock to	add to	0.142857
the	gpu array	0.111111
outputs of node	node	0.007407
will be inserted at struct	struct	0.047619
vars to a variable that represents their unification	unification	0.076923
a optimizer	optimizer	0.062500
the function on the inputs and put	pure op perform node inputs output_storage params	0.047619
dirname	dirname	1.000000
c	i c	0.250000
the existence of the __unify_walk__ method for one	gof unify walk a b u	0.037037
convolution	get conv	0.500000
the value after	default filter	0.040000
v raises attributeerror if there is none	v	0.011111
will be created if	gof	0.002381
replace element i of shape_of[r] by s_i	set shape i r i s_i	1.000000
if	compile check inputs	0.166667
perform the permutation	perform	0.058824
do not try to use complex numbers	mod check x y	0.166667
and uses it instead	o	0.076923
operation to wait on a previously sent array	mpisend wait	0.045455
a batched tensordot product	tensor batched tensordot x y axes	0.333333
template filled by broadcasting value through it	tensor broadcast like value template fgraph	0.125000
we can't change the value after the	default filter	0.040000
correspond to the one hot	tensor to one hot	0.142857
every node that uses r	gof function graph	0.031250
} b axis=l) ->	tensor local	0.025641
return connection pattern of	connection pattern	0.032258
ops in the list remove are still	remove fgraph replacements remove reason	0.055556
array from an index array and a	a	0.008065
c_code for convop that	kern d unroll_bsize unroll_ksize	0.166667
a series of wrapper functions	wrap linker many linkers wrappers	0.047619
this compiles the source code for this linker	compile cmodule location	0.038462
op and reduce pattern has functioning c code	gpuarray gpu careduce cuda supports c code inputs	0.250000
object with debug info	with op	0.166667
type	type c code cache	1.000000
the types involved	inc subtensor do type checking	0.142857
graph of apply nodes	gof sort apply nodes	0.200000
fgraph and	fgraph	0.012195
optimizer for pushing out the variables	out	0.018519
offers to make itself the default python	to os	0.038462
converts number to string	compile char from number number	0.142857
of var transferred to	tensor transfer var	0.100000
pairs that will be turned into macros	cop	0.028571
the context object mapped to the type's :attr	gpuarray gpu array type context	0.090909
type numpy typenum that corresponds to	tensor tensor type dtype specs	0.071429
trace to an	trace	0.052632
dimensionality of the var is equal to	tensor is flat var	0.200000
that would be	gof pure	0.033333
operation	mpirecv	0.037037
listeners to help the navigator deal	gof navigator optimizer attach	0.038462
to represent the dependence of nodes in	dependence	0.035714
bound on the largest eigenvalue	bound	0.043478
vector and t is a	node input_storage output_storage	0.038462
a n-d tensor where n >=	ws ignore_border stride	0.090909
we can't change the value after the	param init default	0.040000
symbolic type representing	tensor type	0.034483
label	label	0.666667
unfortunately conda offers to make itself the default	to	0.017544
converts self _rop_op from user supplied form to	from graph recompute rop op	0.200000
doing a recursion over the input	tensor permute row elements rec	0.047619
print the mflops	flops inputs	0.125000
class file into an aboslute path	gof cop get path cls f	0.166667
a comparator to represent the dependence of nodes	dependence cmp	0.111111
to help the navigator	navigator optimizer attach	0.038462
variable	tensor py operators dimshuffle	0.019231
specific to the apply to be inserted in	apply node	0.031250
normalization	norm train	1.000000
scan return	scan_module push out scan	0.050000
variable optionally inserting broadcasted	tensor py operators dimshuffle	0.019231
the gpu? currently	gpu data	0.500000
variable optionally inserting	tensor tensor py operators	0.015625
an fgraph and	fgraph	0.012195
dimshuffle is inside an alloc and only adds	tensor local alloc dimshuffle node	0.166667
-x pattern	nnet is neg var	0.166667
a new graph	new inputs	0.166667
this	tensor tensor py operators	0.015625
to be held	to cache module key module_hash	0.166667
as replace_all_validate revert the replacement	validate replace all validate remove fgraph	0.111111
changes node inputs[i] to new_r	function graph change input node i new_r reason	0.500000
2d or 3d convolution for debugmode	nnet base abstract conv conv	0.125000
to raise	raise	0.076923
elementwise multiplication (inplace on a)	tensor mul inplace a b	1.000000
connection pattern of subfgraph defined by inputs and	graph connection pattern node	0.076923
broadcast pattern for	pattern a idx	0.066667
c code for corrmm	base corr mm c code	0.090909
in the	in	0.076923
return complex-valued tensor from	from	0.050000
each row correspond to the one	to one	0.125000
parses a config string	parse config string	0.333333
dimshuffle which only adds dimension to the	tensor local dimshuffle	0.052632
leaves of a search through consecutive view_map()s	view roots r	0.200000
of lib directories that are needed by one	clinker lib dirs	0.055556
required return c code to declare variables	clinker type c declare name	0.500000
and converts this to expm1 a	expm1	0.050000
apply_node recursively search from this	apply_node check	0.066667
<=	tensor le	0.666667
output dimensions of convolving	conv op get output	0.047619
a compiled module from the loaded cache or	cache get module name	0.166667
stopping condition returned by the	ls	0.090909
special compound l{op}	softmax argmax1hot	0.083333
number to string by	number number	0.125000
convolution with the specified parameters	dnn conv	0.090909
input a 4-d tensor it sets all non	input patch_size	0.166667
to the	gpuarray gpu	0.045455
a gist and	gist	0.040000
expm1	expm1 node	0.066667
a == b inplace on a	eq inplace a b	0.500000
this variable optionally inserting broadcasted dimensions	tensor py operators dimshuffle	0.019231
the replacement if the	replace validate replace all	0.050000
return a symbolic column variable (ndim=2 broadcastable=[false true])	col name dtype	0.200000
mod	scalar mod c code node name	0.125000
to	make	0.017857
list of lib directories that are needed	lib dirs	0.045455
a > b	gt a b	1.000000
multinomial distributions defined by	tensor multinomial	0.037037
a number of scalars together into	make	0.017857
this compiles the source code for	cmodule location	0.038462
gradient wrt filters for corrmm	corr mm grad weights	1.000000
return a code string specific to	name sub	0.050000
if cond then ift else iff	cond ift iff	0.500000
return the idx_list with constant inputs replaced	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
implement crossentropysoftmaxargmax1hotwithbias	crossentropy softmax argmax1hot with bias	1.000000
caller is replace_all_validate just raise	validator validate fgraph	0.125000
specified factor takes as	tensor signal pool 2d	0.142857
python not the other implementation of mod	scalar mod c code node name inputs outputs	0.125000
return connection pattern of subfgraph defined by	compile op from graph connection pattern	0.076923
instance of _maker which handles much of the	mode function maker i o m	0.066667
not attempting to use dnn	safe no dnn	0.125000
important note this	scan_module push out seq scan process node	0.142857
half of v	v	0.011111
lnk	lnk	1.000000
a string for making	gpuarray gpu careduce cuda makecall node	0.333333
replacement if the ops in	replace all	0.050000
implement softmax	softmax	0.090909
optimization	local	0.014085
this compiles the source code for this	gof clinker compile cmodule location	0.038462
image shape of	shape 1axis kernel_shape	0.250000
the dimensions of this variable optionally	tensor py operators dimshuffle	0.019231
this class returns the bartlett spectral window in	bartlett m	0.083333
a legal value for	is valid value	0.250000
replace a symbol	symbol	0.125000
type numpy typenum that corresponds to self	tensor type dtype specs	0.071429
to merge multiplication	alpha merge	0.076923
to compute the image shape of convolution gradweights	nnet get conv gradweights shape 1axis	0.500000
full path of	gof module	0.058824
listeners to help the navigator deal with the	gof navigator	0.038462
see theano tensor sort	tensor tensor py operators sort axis	1.000000
c contiguous version of	contiguous	0.058824
to raise in	core raise	0.100000
deal with the ignore_trees-related functionality	attach updater fgraph importer pruner chin	0.250000
converts self _rop_op from user supplied form to	from graph recompute rop	0.200000
this	tensor nnet	0.017544
needed but will not be deleted after the	tmp_dir timeout min_wait max_wait	0.500000
for gpucorrmm, gpucorrmm_gradweights and gpucorrmm_gradinputs	gpu corr mm	0.142857
python	tensor make	0.076923
grad	grad	0.093750
specified	sparse_grad	0.125000
this compiles the source code	gof clinker compile cmodule location	0.038462
the dimensions of this variable	tensor tensor py operators dimshuffle	0.019231
validations on the inner	validate inner	0.142857
the tensor operators to the basic variable class	variable	0.022222
full path of the dynamic	module	0.033333
out for occurrences of	out	0.018519
_maker which handles much of the debugging work	compile debug mode function maker i o m	0.066667
the graph and get a memo	graph	0.016393
boundvariable(other_object)	walk fv o u	0.200000
important note this	push out seq scan process node fgraph node	0.142857
the folowing changes	mul switch sink node	0.045455
this variable optionally inserting broadcasted	tensor tensor	0.014286
has an unification in u and uses	o u	0.037037
order a graph of apply nodes according	gof sort apply nodes inputs outputs cmps	0.050000
of broadcastable	d3viz broadcastable	0.500000
a shared variable to	shared variable	0.071429
and its idx_list reorders the inputs according to	inputs idx_list get_count	0.100000
if the named module is a package	importer is package fullname	0.250000
object but we don't clone	clone	0.020833
a list of l{codeblock} instances returns a string	code gen blocks	0.050000
on the inner graph to ensure	scan validate inner graph	0.035714
it into a canonical form that respects	tensor get canonical form	0.045455
is a mrg stream state and	mrg random streams	0.033333
to help the navigator deal with the	navigator optimizer	0.037037
standard deviation along	std	0.058824
turned into macros	cop	0.028571
this will attempt to convert x into a	x context_name	0.100000
of _maker which handles much of the debugging	function maker i o m	0.066667
make a schedule function from comparators	sort schedule fn	0.333333
node id	node id node	1.000000
implements the "reverse-mode" gradient for the	grad perform node inputs outputs	0.083333
return	name x z	0.333333
given a slice [start stop step] transform	slice	0.038462
convolution with the specified parameters	conv	0.037037
elementary validations on the inner graph to	validate inner graph	0.035714
an instance of _maker which handles much	maker i o m	0.066667
instance of _maker which handles much of	mode function maker i o m	0.066667
optionally inserting	tensor tensor py	0.015873
of localoptgroup	opt group	0.043478
connection pattern of subfgraph defined by inputs and	from graph connection pattern	0.076923
functiongraph listeners to help the navigator deal with	navigator	0.032258
an apply_node recursively search from this	import apply_node check	0.066667
(direction="forward"), corrmm_gradweights (direction="backprop weights"),	helper bottom weights top direction	0.055556
standard deviation	std	0.058824
min	min axis	1.000000
return a tensorvariable of this type	tensor type make variable name	1.000000
compute 2d kernel for bilinear upsampling	nnet bilinear	0.111111
apply as many times as required the	apply	0.016667
lib directories that are	lib dirs	0.045455
as replace_all_validate revert the replacement if	validate replace all validate	0.111111
2^a (inplace on a)	exp2 inplace a	1.000000
new random	random	0.055556
>	tensor gt	0.666667
that unroll the batch size loop	unroll batch	0.166667
for any python object a that	gof	0.002381
nodes in the original graph to	outputs copy_inputs_and_orphans memo	0.029412
_maker which handles much	compile debug mode function maker i o m	0.066667
replace a leaf of a	nnet replace leaf	0.100000
tries	top_shape border_mode subsample	0.166667
a variable on	gpuarray as gpuarray variable	0.166667
the tensor operators to the basic constant class	tensor constant	0.055556
return a symbolic row variable (ndim=2 broadcastable=[true false])	row	0.034483
to pass to helper_c_code	tensor inc subtensor get helper c code	0.250000
numpy	tensor	0.003215
list of shape tuple	feature default infer shape	0.066667
to evaluate	destroy	0.009709
access of a given file	access time path	0.200000
_maker which handles much of the debugging work	mode function maker i o m	0.066667
new instance of	clone link_kwargs optimizer	0.111111
element of a dense vector	s	0.071429
llvm one or not	gof gcc llvm	0.200000
is a mrg stream state and they	sandbox mrg random streams	0.033333
fgraph outputs that will replace their	fgraph	0.012195
create a comparator to represent the dependence of	make dependence cmp	0.111111
raise baddestroymap if	compile check inputs node storage_map	0.166667
help the navigator	gof navigator optimizer attach updater	0.038462
dimensions of this variable optionally inserting broadcasted dimensions	tensor tensor py	0.015873
into a gemm	gemm	0.066667
the replacement if the ops in the list	replace validate replace all	0.050000
for	type	0.011905
the output shape	shape	0.010204
initializes py_name to	r	0.028571
none for the outputs of node	node i_shapes	0.333333
set of 2d filters	conv2d input filters	0.125000
connection pattern of subfgraph defined by inputs	compile op from graph connection pattern node	0.076923
variable optionally inserting broadcasted	dimshuffle	0.014493
a uniform distribution	tensor uniform	0.125000
localoptgroup for graphtogpu	gpulocal opt group	0.055556
optionally	tensor tensor py	0.015873
compiled module from the loaded cache or the	module cache get module name	0.166667
mrg	sandbox mrg	0.125000
create a new instance of	clone link_kwargs optimizer	0.111111
!=	neq	0.250000
together into a vector	make vector	0.125000
for openblas	tensor openblas	0.250000
reshapes the output after pad_dims	gpuarray unpad dims output	0.333333
an input vector and t is	node	0.007407
functiongraph	function graph init	0.333333
dimensions of this	operators dimshuffle	0.019231
to	gof module cache add to	0.142857
upgrade	scalar upgrade	1.000000
self _rop_op from user supplied form to	op from graph recompute rop op	0.200000
function is basically a call to tensor	tensor extract constant x elemwise only_process_constants	0.058824
the method that	gof feature	0.125000
c code for corr3dmm (direction="forward"),	base corr3d mm c code	0.090909
raise baddestroymap if	r_vals	0.090909
x // 1 -> x	tensor local intdiv by one	1.000000
a tensorvariable whose type is in t float_scalar_types	as scalar res dtype	0.500000
attempt to replace a leaf of a	tensor nnet replace leaf arg leaves new_leaves op	0.250000
type's :attr context_name	gpu	0.011765
:todo in many expressions there are many ways	from node2 node	1.000000
a and b	a b	0.200000
unroll the batch size loop	gen conv code unroll batch	0.166667
baddestroymap	compile check inputs node storage_map	0.166667
to the apply	apply node	0.031250
to ultra_fast_sigmoid	tensor nnet local ultra fast	0.500000
is	error	0.025000
for abstractconv2d	abstract conv2d	1.000000
of shape tuple	feature default infer shape	0.066667
x -> sigm -x	nnet local	0.200000
"reverse-mode" gradient for the eigensystem	eigh grad perform	0.333333
scaled complementary error function	tensor erfcx a	1.000000
inserting	tensor py	0.015873
argmax over a	argmax	0.066667
writeme	from function local optimizer	1.000000
remove two kinds of useless reshape	local useless reshape node	0.200000
is a mrg stream	mrg random streams	0.033333
constant scalar 0-d value underlying variable	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
a base class with a metaclass	compat with metaclass meta	0.333333
exception class to raise	raise	0.076923
apply as	tensor apply	0.142857
a mrg stream state and they	mrg random streams	0.033333
kinds	tensor tensor type	0.041667
python type c type numpy typenum that	tensor type dtype specs	0.071429
pack c types back into a	gof clinker type c sync	0.111111
input vector and	node input_storage output_storage	0.038462
max and average pooling	pool	0.066667
wrapper around c_cleanup that decrefs py_name	gof get c cleanup r name	0.250000
loading of moved objects in six moves urllib_parse	six moves urllib parse	0.333333
factor takes as	signal pool	0.142857
profiling to print the mflops	flops inputs	0.125000
should remove any dynamically added functionality	node finder on detach fgraph	1.000000
links all the	merge new_best	0.142857
specified axes	addbroadcast x	0.142857
the source	compile cmodule location	0.038462
checking	checking	1.000000
this for a diagnosis if things go awry	gof function graph check integrity	0.250000
scan	out scan output	0.125000
random stream	sandbox mrg random streams	0.033333
graph is impossible to evaluate because of aliasing	destroy	0.009709
a basic theano op that	op itypes otypes infer_shape	0.047619
for corrmm (direction="forward"),	base corr mm	0.250000
theano scalar scalar	scalar	0.017857
graph is impossible to evaluate because	destroy	0.009709
a real-valued input	rfft inp norm	0.142857
create a	maker create input_storage trustme storage_map	0.500000
return str of	to str t	0.166667
as replace_all_validate revert the replacement if	replace validate replace all validate remove	0.111111
base class for operations that need	gpu kernel base	0.333333
see theano tensor argsort	py operators argsort axis	1.000000
one tensor	x y	0.048780
gradient function should	matrix inverse grad inputs	0.500000
for a convolution with the specified parameters	gpu dnn conv	0.200000
numpy ndarray contains any np inf values	compile contains inf arr node	0.500000
the type's :attr	gpuarray gpu array type	0.062500
this	function	0.052632
defaults	defaults	1.000000
object with debug info	with	0.076923
array of ints	bincount x weights minlength assert_nonneg	0.125000
variable (ndim=2 broadcastable=[false true])	col name dtype	0.200000
change the value after	default	0.030303
row variable (ndim=2	tensor row name	0.050000
elemwise and gpuelemwise	local elemwise fusion	0.166667
the context object mapped	context	0.035714
the supplied function as	compile as	0.050000
of headers that are needed	clinker headers	0.047619
op and reduce pattern has functioning	gpu careduce cuda supports	0.166667
op could be very easy if it is	tensor prod l op	0.033333
value after the import	init default	0.040000
of the specified pieces of vectors and	sparse block outer make node o	0.066667
of scalars	make	0.017857
two matrices at least one of which	x y	0.024390
computes the outer product	block outer	0.250000
search through a graph either breadth- or depth-first	search start expand mode build_inv	1.000000
for use in	gpu	0.023529
and "init_code" together	struct node	0.062500
[1]_ for the cholesky factorization	cholesky	0.166667
respect to wrt, computes gradients of	core subgraph grad	0.062500
the inputs	gof inputs	0.333333
register a transfer	register transfer	1.000000
failure_callback for	optimizer warn inplace exc	0.500000
"reverse-mode" gradient for	grad perform node	0.083333
signal	signal	0.833333
a canonical form	get canonical form	0.045455
with respect to wrt, computes	core subgraph grad	0.062500
this explicitly upcasts constant inputs	constant inputs node	0.125000
this is the equivalent of localoptgroup for graphtogpu	to gpulocal opt group	0.055556
is basically a call	tensor extract constant x elemwise only_process_constants	0.058824
be raised by functions defined as part of	method not defined	0.333333
runs a series of wrapper	linker many linkers wrappers	0.047619
of this variable optionally inserting broadcasted dimensions	py operators	0.015625
updates ordereddict the list	updates	0.029412
into two kinds scalar constants and the rest	rest inputs elemwise only_process_constants	0.125000
necessary update dr_vals	check inputs node storage_map r_vals dr_vals	0.250000
implements the "reverse-mode" gradient for the eigensystem of	tensor eigh grad perform node	0.333333
false we can't change the value after	core config param init default	0.040000
> b inplace on a	tensor gt inplace a b	0.500000
rounding than numpy round half to even	round half to even	0.166667
remove two kinds of useless reshape	useless reshape node	0.200000
the value after the import of theano	init default filter	0.040000
the rest	rest	0.076923
is impossible to evaluate	destroy	0.009709
to wrt, computes	subgraph	0.047619
replace_all_validate revert the replacement	replace validate replace all validate	0.111111
print the mflops	nnet base abstract conv flops inp outp	0.125000
indices out_idxs	out_idxs	0.050000
of integers indicating the version	cache version	0.125000
parse a	parse	0.125000
"reverse-mode"	perform	0.117647
loading of moved objects in six moves urllib_error	module six moves urllib error	0.142857
trace from one or more tensor variables to	trace	0.052632
a special compound l{op} for the	argmax1hot	0.058824
a crossentropysoftmax1hotwithbiasdx op whose incoming gradient	crossentropy softmax 1hot with bias dx	0.111111
return a symbolic row variable (ndim=2	row name	0.050000
equivalent of localoptgroup for graphtogpu	group	0.047619
value for myresult	gpuarray gpu careduce cuda assign init first_item	0.166667
a using magma library	gpu magma	0.142857
the navigator deal	gof navigator optimizer attach updater	0.038462
determine the broadcast pattern for advancedsubtensor output	tensor adv index broadcastable pattern a idx	0.066667
after	default	0.030303
sum	tensor sum	0.111111
variable optionally inserting broadcasted dimensions	tensor tensor py	0.015873
of 3d inputs with a set of 3d	tensor nnet conv3d	0.071429
dimshuffle is inside an alloc	alloc dimshuffle node	0.333333
previously	mpisend	0.037037
by the application of another op that takes	op sub	0.066667
return connection pattern of subfgraph defined by	compile op from graph connection pattern node	0.076923
function tries to recognize the updates ordereddict the	updates	0.029412
nodes in the original graph to a	outputs copy_inputs_and_orphans memo	0.029412
return the variables in inputs that	gof	0.002381
dot product followed by a modulo operation	dot modulo	0.250000
of a real-valued input on	gpuarray curfft inp norm	0.066667
delete_if_problem	delete_if_problem	1.000000
calls subprocess_popen returning the output	misc output	0.066667
by default that removes all asserts from the	remove all assert	0.055556
a six moves urllib namespace that resembles	module six	0.043478
variables and apply_nodes to this graph	graph import	0.125000
broadcasted dense vector element wise	sdcsc	0.250000
of m1 and	m1	0.027027
replace a leaf of a	replace leaf	0.100000
this op __init__ fct don't have the	composite make new inplace output_types_preference name	0.142857
^ b inplace on a	tensor xor inplace a b	0.333333
structured elemwise sigmoid	sparse structured sigmoid x	1.000000
structured	structured	0.642857
since the epoch of the last access of	last access time path	0.040000
connection pattern of subfgraph defined by	from graph connection pattern	0.076923
compiles the source code for this linker	gof clinker compile cmodule location	0.038462
fix done in august 2011	tensor load shared variable val	0.142857
a search through consecutive view_map()s	view roots r	0.200000
function expects the compile lock to be held	cache add to cache module key module_hash	0.166667
the mflops	flops inputs	0.125000
none for the outputs of	i_shapes	0.050000
number	make	0.017857
to make itself the default	to os environ	0.038462
matrix solve operation c =	tensor solve	0.038462
that attaches	gof bookkeeper on attach fgraph	0.142857
functiongraph listeners to help the navigator deal	navigator optimizer attach updater fgraph	0.038462
the output dimensions of convolving an image	output	0.017241
var transferred to target	transfer var target	0.200000
dict op -> total number of nodes	compile profile stats class nodes	1.000000
psd	psd	1.000000
can't change the value after the import of	core config param init default filter	0.040000
1 default 1) times from a multinomial distribution	streams multinomial	0.076923
same rounding than numpy round half to	round half to	0.166667
outside of scan	out scan output	0.125000
a real-valued input on	curfft inp norm	0.066667
return permutations	permutation random_state size n ndim	0.500000
python litterals to theano constants in subtensor arguments	args	0.025641
cache data by walking the cache	gof module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
by doing a recursion over the	permute row elements rec	0.047619
a variable with the -x pattern	is neg var	0.166667
self _grad_op from user supplied form	from graph recompute grad op	0.200000
change the value after the import of theano	init default filter	0.040000
extracts list of variables within input	gof variables	0.125000
full path of the dynamic lib	module	0.033333
var1	var1	1.000000
by default that removes all asserts from the	local remove all assert	0.055556
last access of a given file	last access time path	0.040000
is a	meta path importer is	0.250000
a hash from an ndarray	hash from ndarray data	0.333333
inserting	tensor tensor	0.014286
and	node input_storage output_storage	0.038462
convert python litterals to	constant	0.016667
this explicitly upcasts constant inputs	constant inputs	0.125000
function for diagonalsubtensor	get diagonal subtensor	0.083333
a new graph	new inputs inputs	0.166667
op code	get op	0.100000
randomstate	tensor randomstate	1.000000
existing start gradients up to the end	wrt end start	0.166667
2 profiles returned by this	gof seq	0.200000
for graphtogpu	to	0.017544
a symbolic scalar	scalar name dtype	0.166667
set_instead_of_inc	set_instead_of_inc	1.000000
a simple c snippet using current flags	march flag flags	0.333333
to wrt, computes	core subgraph grad	0.062500
indices out_idxs and	out_idxs	0.050000
that would be	gof	0.002381
filters	nnet conv2d input filters	0.125000
the type's	gpuarray gpu	0.045455
the idx_list with constant inputs	get constant idx inputs	0.250000
list of lib directories that are	clinker header dirs	0.055556
unary(alloc x shp -> alloc(unary x	local	0.014085
zero dimensional constant	empty constant	1.000000
the end variables	wrt end	0.050000
connection pattern of subfgraph defined by	compile op from graph connection pattern	0.076923
does	alloc	0.012500
the variables that	gof clinker type	0.066667
tensor from polar coordinate specification	complex from polar abs angle	0.250000
converts self _grad_op from user supplied form	from graph recompute grad	0.200000
a special compound l{op} for the	softmax argmax1hot	0.083333
for same kinds of	tensor type	0.034483
elemwise and gpuelemwise op	local elemwise fusion op op	0.200000
the mflops	flops	0.076923
all device	device	0.076923
dimensions scrap the dimshuffle and index the	local dimshuffle	0.052632
input a 4-d tensor it sets all non	max pool 2d same size input patch_size	0.166667
all device we do it	device node	0.045455
see theano tensor std	py operators std axis	1.000000
thunk that is a zero-arguments	thunk	0.021277
or more multinomial distributions	tensor multinomial random_state	0.040000
and only adds dimension to the left	tensor local	0.025641
a file that was dumped	f persistent_load	0.052632
scan return	out scan	0.035714
rstate	rstate	0.454545
parses a config	config	0.100000
of numpy ones_like	tensor ones like model dtype	0.333333
compiled module from the loaded cache or	module cache get module	0.166667
is the equivalent of localoptgroup	to gpulocal opt group	0.055556
dependence of nodes in a	make dependence	0.043478
help the navigator deal	navigator optimizer attach updater fgraph	0.038462
to use dnn conv workmem	dnn workmem workmem	0.166667
replace_all_validate revert the replacement if the	validate replace all validate remove	0.111111
list of headers that are needed by	clinker headers	0.047619
module if	module	0.033333
navigator deal	gof navigator optimizer attach updater fgraph	0.038462
reorder	py	0.014286
the dependence of nodes in a	gof make dependence	0.043478
raise baddestroymap	inputs	0.012658
of this variable optionally inserting	tensor py operators dimshuffle	0.019231
permutations of	tensor permutation random_state	0.500000
step] transform it into a canonical	tensor get canonical	0.125000
the	graph	0.016393
this to expm1	local expm1	0.066667
x is an input vector and t	node input_storage output_storage	0.038462
the second half by b with	b	0.014925
linker's fgraph	input_storage output_storage storage_map keep_lock	0.500000
conda offers to	to	0.017544
raise	check	0.083333
concatenate tensortypes along the given axis	tensor join axis	0.333333
text to a gist and return the	gist	0.040000
x // 1 -> x	local intdiv by one	1.000000
does the	alloc	0.012500
convert degree a	tensor deg2rad a	0.333333
assemble the c code for this composite op	scalar composite init c code	0.333333
remove base directories 'cutils_ext', 'lazylinker_ext' and 'scan_perform' if	gof module cache clear base files	1.000000
make it work for elemwise and gpuelemwise	local elemwise	0.166667
compute 1d kernel for bilinear upsampling this	nnet bilinear	0.111111
replace a	tensor nnet replace	0.250000
returns the bartlett spectral window in	bartlett m	0.083333
returns the bartlett spectral window in the	bartlett	0.058824
shared variable names when persisting to	shared variable id	0.142857
the inputs required to compute the given	gof inputs variable_list blockers	0.058824
for corr3dmm (direction="forward"), corr3dmm_gradweights	nnet base corr3d mm	0.333333
stream	tensor random streams getitem	1.000000
mean value along	tensor mean	0.111111
for any python object a that would be	gof pure	0.033333
the inputs and outputs	inputs outputs	0.066667
a gradient	grad	0.010417
type's :attr	gpuarray gpu array type	0.062500
and reduce pattern has functioning	careduce cuda supports	0.166667
caller is replace_all_validate just raise the exception	validator validate fgraph	0.125000
calculate the sum	sum x	0.333333
if a subtensor is inside a	subtensor node	0.066667
returns the signature for this function	function method decl	0.333333
turned into macros for use within	cop	0.028571
constant scalar 0-d value underlying	get scalar constant value	0.333333
es	dtype op	0.250000
determine the broadcast pattern for advancedsubtensor	adv index broadcastable pattern a idx	0.066667
alloc2	alloc2	1.000000
the equivalent of localoptgroup for graphtogpu	group	0.047619
cache	gof module cache	0.250000
transfer to a tensortype if not	transfer	0.058824
or int_div depending on types of x y	x y	0.048780
match a variable with the -x pattern	neg	0.083333
called whenever node inputs[i] is changed	on change input function_graph node i	0.333333
to generate c	compile register specify shape c	0.250000
clip x to	clip x	0.250000
that use	tensor nnet conv op use	1.000000
the *args directly	sparse local csm properties csm	0.142857
callable that persists certain objects in	persistent_id	0.166667
original	outputs copy_inputs_and_orphans memo	0.029412
of this variable optionally	tensor py	0.015873
a leftdims	leftdims	0.090909
reorder the	py	0.014286
implementation of mod	scalar mod c code node name	0.125000
stack	gof copy stack	0.333333
performs the matrix inverse	matrix inverse	0.111111
can't change the value after the import of	param init default	0.040000
use dnn conv workmem	safe no dnn workmem workmem	0.166667
to print the mflops	base gpu corr mm flops inp outp	0.125000
of _maker which handles much	mode function maker i o m	0.066667
by this	gof clinker type	0.066667
dimensions	tensor py operators	0.031250
asserts from	assert	0.111111
function is only used to determine the	adv index broadcastable	0.050000
where x is an input vector and	node input_storage	0.038462
python	tensor	0.003215
computes the output dimensions of	op get output	0.047619
to the basic constant class	constant	0.016667
an inplace optimization that deals with allocempty	gpuarray inplace allocempty op idx	0.166667
the op	op	0.027523
a new variable	var name doc configparam root	0.250000
sharedvariable instances of suitable dummy values	local meta optimizer provide	0.200000
cache	module cache	0.214286
received	mpirecv	0.037037
output dimensions	output	0.017241
any python object a that	gof	0.002381
the source code for this linker and returns	compile cmodule location	0.038462
full "updates" dictionary mapping from functiongraph input variables	compile fgraph updated vars	1.000000
y + alpha * dot a x	tensor gemv c code y a x	0.500000
where function is a thunk	thunk	0.021277
turn it into a gemm	tensor gemm	0.166667
function expects the compile lock to	cache add to	0.142857
factor takes as	signal pool 2d	0.142857
the stack	gof copy stack	0.333333
reshaped view/copy of this variable	tensor py operators reshape shape ndim	0.111111
single prod()	op of op node	1.000000
a reshaped view/copy of	tensor py operators reshape shape ndim	0.111111
object to pickle	obj	0.083333
scan that	scan	0.034483
the navigator	navigator	0.032258
dot csr is like dot except	dot csr	0.111111
c code for corrmm (direction="forward"),	tensor nnet base corr mm c code	0.090909
complete hashable signature of the module	clinker cmodule key	0.166667
enabled change all sigmoid to ultra_fast_sigmoid	nnet local ultra fast sigmoid node	0.200000
compiled module from the loaded	get module	0.200000
sample n (n needs to be >=	size n	0.090909
optionally inserting broadcasted	py operators	0.015625
inner graph	scan_module scan validate inner graph	0.035714
dimensions of	operators	0.017241
node by one	node output_indices	0.142857
inputs required to	inputs variable_list blockers	0.058824
function that allows replacing subgraphs of	scan_module clone output replace strict share_inputs	0.071429
variable optionally inserting broadcasted	operators	0.017241
of apply nodes according to a list of	sort apply nodes inputs outputs cmps	0.050000
anymore and should be removed	scan_module compress outs	0.076923
x -> gpu_contiguous x	gpuarray local gpu contiguous gpu contiguous node	0.500000
code string specific to the apply to	apply	0.016667
we can't change the value after the	core config param init default	0.040000
useless reshape	tensor local useless reshape node	0.200000
comparator to represent the dependence	dependence cmp	0.111111
see theano tensor repeat	tensor py operators repeat repeats	1.000000
a config	config	0.100000
instance associated with a particular	getitem item	0.125000
delete_updates	delete_updates	1.000000
op	tensor prod l op	0.033333
the orphans among them	and orphans	0.166667
use dnn conv algo_bwd	dnn algo bwd algo	0.166667
row	row name	0.050000
gradient wrt filters for abstractconv3d	abstract conv3d grad weights	1.000000
diagonalsubtensor and	tensor nnet get diagonal subtensor	0.083333
important note this	scan process node fgraph	0.142857
helpful function that gets a scan op a	op not_required	0.071429
this function expects the compile lock to	to	0.017544
by 2**72 samples	get substream rstates n_streams dtype inc_rstate	0.142857
reorder the dimensions of	py	0.014286
scan return true	scan	0.017241
a new variable to theano config	config var name doc configparam	0.500000
this op	gof clinker	0.033333
source code for this	compile cmodule location	0.038462
convolution	dnn conv get	0.100000
grad	grad dict var_to_app_to_idx	1.000000
a mrg	sandbox mrg	0.125000
gradinputs	gradinputs	1.000000
or more multinomial distributions defined	multinomial random_state	0.040000
u and uses	o u	0.037037
the ops in the list remove are still	remove fgraph replacements remove reason	0.055556
pushing out the variables inside	push out	0.037037
a dict that	gof	0.004762
on the inner graph to	validate inner graph	0.035714
gpuincsubtensor	setsubtensor node	0.250000
and apply_nodes to this graph	graph import	0.125000
computes the outer product of two sets of	sparse block outer	0.047619
product of the specified pieces of vectors and	sparse block gemv make	0.066667
variable optionally	py	0.014286
required	pure	0.166667
the name the object should be saved under	misc persistent ndarray id resolve name	0.500000
the input nodes to output nodes	gof	0.002381
reorder the dimensions of this variable	tensor py	0.015873
for corr3dmm	corr3d mm	0.333333
performs batch	nnet batch	0.500000
the types involved	tensor inc subtensor do type checking	0.142857
compute the image shape of convolution gradinputs	tensor nnet get conv gradinputs shape 1axis kernel_shape	0.500000
device we	device node	0.045455
listeners to help the navigator deal with	navigator optimizer	0.037037
multinomial distributions defined	tensor multinomial	0.037037
to wrt,	core subgraph	0.062500
given an apply_node recursively search from this node	import apply_node	0.066667
matrix	matrix	0.611111
of	tensor py	0.015873
a series of wrapper functions instead of	linker many linkers wrappers	0.047619
feature should remove	feature	0.083333
c code	gof clinker type c	0.166667
change the value after	core config param init default	0.040000
the product along the given axis	axis	0.025641
helper function to generate permutations from integers	permutation helper random_state n	0.333333
cache data by walking the cache	module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
merge 2 dicts by	gof merge	1.000000
total time icluding the time for parents	total times	0.200000
small or builtin c types	clinker type c is simple	0.250000
a compiled module from the loaded cache	module cache get module	0.166667
over given axis	x axis	0.200000
arctangent	arctan2	0.166667
input broadcastable in the specified axes	tensor addbroadcast x	0.142857
this class	gof clinker	0.166667
r t its weights	wrt weights input output_grad filter_shape input_shape	0.333333
by a specified factor takes as input a	signal pool 2d input	0.090909
this function builds the 1d	1d	0.090909
in the cache and none	cache	0.034483
kernels	kernels	1.000000
tensordot	tensordot	1.000000
shape	shape shape	1.000000
with logsoftmax x 's grad	local logsoftmax grad node	0.200000
associated with a particular stream	tensor random streams getitem item	0.142857
return	name x	0.333333
function that makes a value from an integer	gof cdata type get func	0.500000
see theano tensor max	tensor py operators max axis keepdims	1.000000
the 1d kernel that can be used to	kernel 1d	0.050000
or int_div depending on types of x	x	0.017544
row is a mrg stream state	sandbox mrg random streams	0.033333
the	tensor tensor	0.014286
useless dimshuffle operation inside reshape reshape(vector	useless dimshuffle in reshape node	0.500000
and b are unified given the unification	b	0.014925
multiply the first half of v by	v	0.011111
return a	node name	0.066667
a special compound l{op} for	crossentropy softmax argmax1hot	0.083333
standard deviation std	std	0.117647
using advanced indexing	advanced	0.333333
return full path of the dynamic lib in	module name	0.062500
entry_2	entry_2	1.000000
replaced by	allow_partial only_process_constants elemwise	0.166667
change the value after the import of	init default	0.040000
the outputs from the	outputs mode	0.166667
det x and there	local det	0.166667
y with length one	y axis	0.125000
environ	environ	1.000000
value after the import of theano	init default filter	0.040000
with constant inputs replaced	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
function tries to	top_shape	0.137931
return full path of the dynamic lib	gof module name from	0.076923
is a package	path importer is package	0.500000
computes the sum	tensor sum	0.111111
graph optimizer for scan makes it run inplace	scan inplace optimizer	0.500000
image	1axis	0.142857
a c contiguous version of the input	gpu contiguous	0.083333
exception some perform() or c_code() modified an	map	0.047619
the inner graph to ensure that	scan validate inner graph	0.035714
this explicitly upcasts constant	constant	0.016667
maps from variable and apply nodes in	get equiv inputs	0.142857
the same type	typed list type	0.250000
an operator that wraps sympy's c code generation	sym py ccode	1.000000
return a hash	tensor hash	0.333333
checks if the outputs of specific ops of	trace f_or_fgraph ops_to_check bug_print	0.035714
which each row is a mrg	mrg	0.076923
an op that will be	op	0.018349
wrt	wrt	0.714286
"reverse-mode" gradient for the eigensystem of	tensor eigh grad perform node inputs outputs	0.333333
generate a diff to make code	diff code	0.333333
epoch of the last access of	last access time	0.040000
a functiongraph a constant that is	constant error	0.166667
see max for the maximum in	tensor maximum	0.142857
the navigator deal with	navigator optimizer attach updater	0.038462
wrapper to make an inplace	inplace	0.025641
c types	clinker type c	0.166667
this op could be very easy if	tensor prod l op	0.033333
r-operator for the downsample operation	max pool rop	0.142857
to the user what obj is	obj	0.083333
to print the mflops	nnet base abstract conv flops inp outp	0.125000
encoding of each element in	nb_class dtype	0.200000
of specific ops of a compiled	trace f_or_fgraph ops_to_check bug_print	0.035714
apply_nodes to this graph	graph import	0.125000
specified factor takes as input a	signal pool 2d input	0.090909
gradient updates for matrix solve operation	tensor solve grad	0.250000
+ rightdims	rightdims	0.142857
a module if the	module	0.033333
merge multiplication by a scalar	gpuarray alpha merge	0.076923
remove subtensor/advancedsubtensor1 if it takes the full	useless subtensor node	0.200000
generates the c code for corr3dmm (direction="forward"), corr3dmm_gradweights	nnet base corr3d mm c code	0.090909
converts self _rop_op from user supplied form	compile op from graph recompute rop op	0.200000
wrt,	core subgraph grad wrt	0.062500
six moves urllib_error	six	0.025000
0 / x -> 0	local zero div node	1.000000
variable v if v is the output	v	0.011111
for gpuincsubtensor	inplace setsubtensor	0.250000
node by one	node output_indices alloc_ops	0.142857
since the epoch of the last access of	last access	0.040000
eigensystem of	tensor eigh	0.333333
of ints	tensor bincount x weights minlength assert_nonneg	0.125000
important note this function	seq scan process node	0.142857
gradients up to the end	end	0.040000
a dictionary unary_out_lookup({int8 int32 float32 complex128})	unary out lookup	0.250000
version	cache version	0.125000
convolution gradient with	conv grad w	0.500000
values of a shared variable	shared variable	0.071429
exception	bad	0.026316
that operates on the	gof linker make	0.250000
return the inputs required	gof inputs variable_list blockers	0.058824
has a 'requirement' of the destroyhandler	destroy handler	0.055556
convolve spatio-temporal filters with a	filters	0.032258
updates for matrix solve operation c = a	solve	0.032258
d2	d2	1.000000
to detect	detect	0.090909
the value after the	config param init default filter	0.040000
the minimum in	tensor minimum x	0.142857
of lib directories that are needed by one	lib dirs	0.045455
useless dimshuffle	useless dimshuffle	0.500000
hook	hook	1.000000
largest	largest	0.625000
more multinomial distributions defined by one-dimensional slices	tensor multinomial random_state	0.040000
this variable optionally	tensor tensor py operators	0.015625
of shape tuple	tensor shape feature default infer shape	0.066667
within the op code	op	0.009174
d1	d1	1.000000
ordereddict the list of outputs and the	and outputs	0.100000
a graph of apply nodes according to	apply nodes inputs outputs cmps	0.050000
cost and/or from existing start gradients	start cost	0.100000
that x and y have the same shape	same shape x y	0.500000
if the outputs of specific ops of a	trace f_or_fgraph ops_to_check bug_print	0.035714
an alloc	alloc node	0.037037
class for giving abbreviated tags like	tag generator	0.333333
also work for gpuincsubtensor	local inplace setsubtensor node	0.250000
c code for corrmm (direction="forward"),	corr mm c code	0.090909
only one client	sitsot only	0.066667
check that 1) this	gof	0.002381
similar behaviour as haskell' foldr	foldr fn sequences outputs_info	1.000000
storage	storage	1.000000
conda offers to make	to os	0.038462
a that would be	gof pure	0.033333
symbolic graph for convolving a mini-batch	input_shape filter_shape	0.027778
/	div node	1.000000
set to a specified scalar	a	0.008065
names	names	0.333333
return a tuple of integers indicating the version	version apply node	0.125000
variable optionally inserting broadcasted dimensions	tensor py operators dimshuffle	0.019231
x and y have the same shape	shape feature same shape x y	0.500000
bessel function	jv inplace	1.000000
function to get the 0	type get depth	0.050000
we can't change the value after the	init default	0.040000
help the navigator deal with the	gof navigator optimizer attach updater fgraph	0.038462
optimization to the provided l{functiongraph} it may	gof optimizer apply fgraph	0.200000
the dot product dot(x, y t) = z	sampling dot	0.500000
type numpy typenum that	tensor tensor type dtype specs	0.071429
directory and return full path of	gof module name from dir	0.071429
the tree and figure out	scan_module traverse out x x_copy d	0.047619
the image shape of	shape 1axis	0.250000
a scan	scan	0.017241
4-d tensor it sets	patch_size	0.050000
connection pattern of subfgraph defined	compile op from graph connection pattern	0.076923
similar behaviour as python's	fn sequences outputs_info non_sequences	0.333333
delete unversioned dynamic modules	module cache clear unversioned min_age	1.000000
scan the contents of a cache directory and	from dir dirname err files	0.166667
from a uniform distribution	uniform	0.043478
original graph to	inputs outputs copy_inputs_and_orphans memo	0.029412
the navigator	navigator optimizer	0.037037
return a c contiguous version	gpu contiguous	0.083333
number of scalars together into a vector	make vector	0.125000
try to compile a	try	0.111111
comparator to represent the	cmp	0.058824
return a tuple of integers indicating the version	c code cache version	0.125000
the topooptimizer from the input nodes to output	gof in2out	0.055556
shape that broadcast them to match	generate broadcasting	0.066667
apply_node recursively search from	import apply_node	0.066667
roll	tensor roll	0.250000
around c_cleanup that decrefs py_name	gof get c cleanup r name	0.250000
to be inserted in the struct initialization code	init code struct	0.125000
see 'theano tensor ptp'	tensor py operators ptp axis	1.000000
with a set of 2d filters	conv2d input filters	0.125000
input a 4-d tensor it sets	pool 2d same size input patch_size	0.166667
// 1	intdiv by one	1.000000
op then replace it with a triangular solve	sandbox linalg tag solve triangular	0.142857
grad of this op could be	op	0.009174
step] transform it into a canonical form that	canonical form	0.045455
main diagonal set to a specified	fill diagonal offset a val offset	0.100000
for convolving a mini-batch of a stack	input_shape filter_shape	0.027778
a set of arrays to choose from	tensor choose a choices out mode	0.200000
function to get	type get depth	0.050000
pattern has functioning c code	careduce cuda supports c code inputs	0.250000
pattern of subfgraph defined by inputs and outputs	pattern node	0.125000
gradients of cost and/or from existing start	start cost	0.100000
gradient updates for matrix	grad	0.010417
new fgraph check that 1) this	gof	0.002381
compiles the source code for this linker	compile cmodule location	0.038462
the caller is replace_all_validate just raise	validator validate fgraph	0.125000
cost and/or from	cost	0.045455
the dependence of	dependence	0.035714
rebuild	rebuild	1.000000
of v by a with a	v	0.011111
code to each level of nesting	loop_orders dtypes loop_tasks	0.125000
array and a	a	0.008065
reorder the dimensions	tensor tensor	0.014286
output of scan	scan_module push out scan	0.050000
the current op and reduce pattern has functioning	careduce cuda supports	0.166667
the shape field	csm shape csm	0.333333
half of v by	v	0.011111
predicate	predicate	1.000000
reshapes the output	output input leftdims rightdims	0.333333
shp -> alloc(unary x shp)	tensor local alloc unary node	0.250000
a new variable	var name	0.250000
d	d	1.000000
value	value error	1.000000
deepcopyop how to	deep copy op	0.250000
should remove any dynamically added functionality	bookkeeper on detach fgraph	1.000000
only used to determine the broadcast pattern	adv index broadcastable pattern	0.066667
equivalent of localoptgroup for graphtogpu	graph to gpulocal opt group	0.055556
optimization makes the folowing changes in the graph	local mul switch sink node	0.045455
a series of wrapper functions instead of	wrap linker many linkers wrappers	0.047619
apply the list	gof apply	0.090909
function tries	top_shape border_mode	0.166667
i of	r i	0.500000
_maker which handles much of the debugging	debug mode function maker i o m	0.066667
a failure code to the task	find task	0.142857
detect a	detect	0.090909
of scalars together into a	make	0.017857
with logsoftmax x 's grad	nnet local logsoftmax grad node	0.200000
arcsin	arcsin	0.833333
apply the list of	gof apply	0.090909
traverse	traverse	1.000000
to compute the image shape of convolution gradinputs	tensor nnet get conv gradinputs shape 1axis kernel_shape	0.500000
print profilestat objects in _atexit_print_list to _atexit_print_file	compile atexit print fn	1.000000
copy of the type	tensor tensor type	0.041667
the navigator deal with the	navigator	0.032258
elements obtained by iterating over given axis	axis keepdims	0.400000
of the __unify_walk__ method for one of	unify walk a b u	0.037037
a module from the cache	cache module from	0.333333
object with	with op node	0.166667
to print the mflops	abstract conv flops inp outp	0.125000
set the values of a shared variable	shared variable	0.071429
generates the c	c	0.053571
required return c code to declare variables	clinker type c declare name sub check_input	0.500000
raised when subtensor is asked to perform	indexing error	1.000000
product of the specified pieces of vectors and	sparse block gemv make node o w h	0.066667
print the mflops	mm flops inp outp	0.125000
unroll	tensor nnet gen conv code unroll	0.250000
this function uses set and dictionary data structures	scan_module push out non seq	0.125000
convolution gradweights	conv gradweights	0.500000
broadcasted dimensions	tensor py operators	0.015625
that	gof clinker	0.066667
tensor3	tensor3	0.714286
op that will call the supplied	op	0.009174
2d or 3d convolution for debugmode	base abstract conv conv	0.125000
import	import	1.000000
this	gof	0.045238
true if a and	a	0.008065
grad of this op could be very	tensor prod l op	0.033333
1d kernel that can be	kernel 1d	0.050000
return a symbolic row	tensor row name dtype	0.050000
the output type dtype and broadcast there	local canonicalize alloc	0.333333
this	operators	0.017241
folowing changes in	tensor local mul switch sink	0.045455
a and b can be considered approximately equal	gof pure type values eq approx a b	1.000000
helper function for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x	0.083333
the context associated	context	0.035714
dict d s t d[node] is	function graph orderings	0.200000
to recognize the updates ordereddict the list of	updates	0.029412
to make	to os	0.038462
the shape or the other arguments	ndim bcast ndim shape	0.250000
list of shape tuple	shape	0.010204
litterals	make	0.017857
c code for corr3dmm (direction="forward"),	tensor nnet base corr3d mm c code	0.090909
a !=	neq a	1.000000
of integers indicating the version	code cache version apply node	0.125000
lock to	to	0.017544
item from the cache	gof call cache	0.200000
and a set of arrays to choose from	tensor choose a choices out mode	0.200000
return a hash from	hash from	0.333333
listeners to help the navigator deal	navigator	0.032258
with the ignore_trees-related functionality	importer pruner chin	0.250000
share_inputs	share_inputs	1.000000
given axis es of	axis ddof keepdims	0.083333
is node	node	0.007407
this optimization makes the folowing changes in the	mul switch sink node	0.045455
also work for gpuincsubtensor	setsubtensor	0.111111
ignore_trees-related functionality	importer pruner chin	0.250000
the form x[0 :] -> x[0]	tensor local useless slice node	0.250000
op for input of given shape and flags	pool grad out shape imgshape ws ignore_border stride	0.200000
perform some elementary validations on the inner	scan_module scan validate inner	0.142857
var has an unification in u and	o u	0.037037
the other implementation of mod	scalar mod c code node name	0.125000
the idx list to get the right values	tensor get idx list	0.076923
blas	blas	0.750000
x and replace it with logsoftmax x	tensor nnet local logsoftmax	0.076923
the folowing changes in	mul switch sink node	0.045455
the dimensions of	tensor tensor py operators dimshuffle	0.019231
the	bad view	0.027027
for matrix solve operation c = a	tensor solve	0.038462
override clinkertype c_sync	type c sync name sub	1.000000
maybe	maybe	1.000000
multiline string representating the cause of the exception	bad optimization str diagnostic	0.043478
load	load	0.750000
this is the equivalent	to	0.017544
when we overwrite the full inputs	local useless inc subtensor node	0.066667
level of the list	list type	0.100000
a comparator to represent the dependence of	make dependence cmp	0.111111
from a uniform distribution between low and high	tensor uniform random_state size low high	0.333333
of the parents	parents	0.100000
round_half_to_even_inplace	tensor round half to even	1.000000
return the list of op classes	local optimizer tracks	0.200000
function uses set and dictionary data structures	push out non seq scan	0.125000
to concatenate tensortypes along the given axis	join axis	0.333333
to compute the kernel shape of convolution gradweights	tensor nnet get conv gradweights shape image_shape	0.500000
of scan return true iff	scan	0.017241
default	default	0.151515
lifter	lifter	1.000000
col	col	0.714286
a failure code to the task	cthunk find task	0.142857
is the	graph to	0.055556
elemwise arcsinh of x	sparse arcsinh x	1.000000
and converts this to expm1 a	expm1 node	0.066667
the version	cache version apply node	0.125000
a view	view tree	0.500000
small or builtin c	c is simple	0.200000
alpha	alpha	0.416667
depend only on	seq	0.166667
the n-th order discrete difference along given axis	tensor diff x n axis	0.500000
tensortype	tensor type	0.034483
short mostly hexadecimal hash of a	core hex digest x	0.083333
upsampling this function builds the 2d	2d	0.090909
out for occurrences of values identical with x	scan_module forced replace out x y	1.000000
contains	contains	0.857143
unfortunately conda offers to make itself	to os	0.038462
helper function for diagonalsubtensor	nnet get diagonal subtensor view x i0	0.083333
previously sent array using	mpisend	0.037037
new variable to	var name	0.250000
manipulate	r new_r reason verbose	0.071429
new	clone with new inputs inputs strict	0.166667
this function tries to recognize the updates ordereddict	get updates	0.034483
pass to helper_c_code	helper c code	0.142857
is the llvm one or not	gcc llvm	0.200000
in profiling to print the mflops	flops inputs outputs	0.125000
a six moves urllib namespace	module six	0.043478
two-level hierarchical softmax	nnet h softmax x batch_size n_outputs n_classes	1.000000
output shape of	shape	0.010204
choose values from a with or without replacement	random streams base choice size a replace p	0.333333
gets a scan op a	op not_required	0.071429
modulo (inplace on a)	tensor mod inplace a	1.000000
a series of wrapper functions instead of just	linker many linkers wrappers	0.047619
a set of 3d filters	conv3d input filters	0.142857
tries to	image_shape top_shape	0.166667
the hack in profiling to print the mflops	op flops	0.125000
avail on compute capability 2	dev20	0.166667
for cudnn batch normalization	gpu dnn batch norm inference	0.333333
not required anymore and should be removed and	outs	0.050000
it into a canonical form that respects the	canonical form	0.045455
g++ version	gof gcc	0.027778
m1 and the second half by b	m1 b	0.333333
deal with the ignore_trees-related functionality	updater fgraph importer pruner chin	0.250000
new node a clone in	clone	0.020833
from existing start gradients up to	start	0.040000
equivalent of var	var	0.035714
compile c code when doing constant folding of	python constant folding	0.142857
dict op -> total number	compile profile stats op	0.166667
fill inputted tensor with the assigned value	py operators fill value	1.000000
"lifts" dimshuffle through elemwise operations and	local dimshuffle lift	0.250000
and converts this to expm1	expm1 node	0.066667
context_name	gpu array type	0.062500
get the right values	get	0.020833
see 'theano tensor ptp'	py operators ptp axis	1.000000
list of r	r client_to_remove	0.200000
if fgraph is the first functiongraph	fgraph no_recycling profile	0.200000
pairs that will be turned into macros	cop get	0.033333
file that was dumped to a	f persistent_load	0.052632
i	gof clone i	1.000000
type's	gpuarray	0.023256
construct pydotformatter object	d3viz py dot formatter init compact	1.000000
detach	detach	1.000000
output of scan return true	scan_module push out scan	0.050000
graph of scan to outside of scan	scan	0.017241
graph is	destroy	0.009709
gradient the finite fourier	fourier grad	0.250000
+ alpha * dot	tensor gemv	1.000000
a inner nit_sot output of	inner	0.041667
converts this to expm1	tensor local expm1	0.066667
output dimensions of convolving	nnet conv op get output	0.047619
in the list remove are still in the	replacements remove reason	0.055556
element of a dense vector	x s	0.142857
litterals to	tensor make	0.076923
apply_node recursively search from this node to	import apply_node check reason	0.066667
shape of convolution gradinputs	conv gradinputs shape	1.000000
the mflops	nnet base abstract conv flops inp outp	0.125000
a tuple of integers indicating the version	version	0.093750
transform	transform	1.000000
grad	grad dict	1.000000
of apply nodes according	sort apply nodes inputs outputs cmps	0.050000
from	tensor complex from	0.250000
attempts to replace	optimizer attempt	0.500000
bias	bias	0.833333
the mflops	gpuarray base gpu corr mm flops inp outp	0.125000
the value after the import of	config param init default	0.040000
apply as many	apply	0.016667
grads	grads	1.000000
series of wrapper functions instead of	wrap linker many linkers wrappers	0.047619
pad	pad	1.000000
sparse	core sparse	0.066667
need not be checked for nan and inf	is numeric value arr var	0.166667
:attr context_name	gpu array	0.055556
returns	gof local meta	0.500000
node a clone in a	clone	0.020833
version	c code cache version apply	0.125000
return a symbolic row variable (ndim=2 broadcastable=[true false])	row name	0.050000
outp	outp	1.000000
return the idx_list with constant inputs replaced	constant idx inputs allow_partial only_process_constants elemwise	0.071429
wrapper around c_init that	gof get c	0.166667
and "init_code"	struct node	0.062500
zview	zview	1.000000
max for the maximum	maximum	0.083333
x*x -> sqr x this is faster on	local mul to sqr node	0.166667
in which each row is a mrg stream	mrg random streams	0.033333
for matrix solve operation c	tensor solve	0.038462
1d	1d	0.545455
for input of given shape and flags	grad out shape imgshape ws ignore_border stride	0.200000
replace_all_validate revert the	all validate	0.166667
unfortunately conda offers to make itself the default	to os	0.038462
partition	scalarconsts	0.076923
it to be reused in scalar	gof clinker cmodule key fgraph no_recycling compile_args libraries	0.200000
output_gradients	output_gradients	1.000000
self _grad_op from user supplied form to	op from graph recompute grad op	0.200000
loading of moved objects in six moves	module six moves urllib error	0.142857
sine of a	tensor sin a	1.000000
all intermediate variables given	of variables fgraph input_shapes	0.250000
method is primarily used	pure op r op inputs eval_points	0.125000
out for occurrences of values identical with x	scan_module forced replace out x	1.000000
return those	gof remove	0.250000
and only adds dimension to the left remove	local	0.014085
a uniform distribution	uniform random_state size	0.125000
of lib directories that	clinker header dirs	0.055556
log gamma function	gammaln inplace a	1.000000
convolution with the	dnn conv	0.090909
an array with	node inputs outputs	0.125000
generates the c code for gpucorrmm (direction="forward"), gpucorrmm_gradweights	gpu corr mm c code	0.090909
graph of apply nodes according to a list	gof sort apply nodes inputs outputs cmps	0.050000
a constant that	gof	0.002381
of	graph to gpulocal opt	0.055556
to wrt,	core subgraph grad wrt	0.062500
canonical form	get canonical form	0.045455
connection pattern of subfgraph defined by inputs and	graph connection pattern	0.076923
batch normalization of the	dnn batch normalization	0.125000
variable with the -x pattern	nnet is neg var	0.166667
six moves urllib_error	module six	0.043478
string for	gpu	0.011765
should return	name	0.011111
implements the "reverse-mode"	perform	0.117647
do a view in the forward but clip	clip x lower_bound upper_bound	0.090909
only one client and that client	only	0.050000
folowing changes	local mul switch sink node	0.045455
x -> sigm	tensor nnet local	0.200000
an alloc and	tensor local alloc	0.111111
navigator	navigator	0.193548
"reverse-mode" gradient [1]_ for the cholesky factorization	cholesky grad perform	0.500000
of this variable optionally	operators dimshuffle	0.019231
that unroll the	unroll	0.111111
rebroadcast how to generate c	rebroadcast c	1.000000
corr3dmm_gradweights (direction="backprop weights"), and	helper bottom weights top direction	0.055556
after pad_dims	gpuarray unpad	1.000000
alias that wasn't	bad view	0.027027
conda offers to make itself the default	to os environ pathlist	0.038462
alias that wasn't in the	view	0.022727
elementary validations on the inner graph	inner graph	0.035714
to help the navigator	gof navigator optimizer attach	0.038462
calling the same op twice gives inconsistent outputs	bad	0.013158
equivalent	graph to gpulocal opt	0.055556
has only one	only	0.050000
scale each	scale	0.095238
to the type's :attr	gpu	0.011765
given shape and flags	pool grad out shape imgshape ws ignore_border stride	0.200000
return the constant scalar 0-d value underlying	core get scalar constant value	0.333333
same kinds of tensortype	tensor	0.006431
the idx_list with constant	tensor subtensor get constant idx	0.250000
dict op ->	compile profile stats	1.000000
given graph contains a cycle parameters	contains cycle fgraph orderings	0.333333
execute	function graph execute	1.000000
can't change the value after the import of	init default	0.040000
image shape of convolution gradinputs	get conv gradinputs shape 1axis kernel_shape	0.500000
updates ordereddict	scan_module get updates	0.034483
returns the connection pattern of a	gof io connection pattern	0.055556
return c code to declare variables	c declare name sub check_input	0.500000
a that would be	gof	0.002381
the alloc would be useless	alloc call	0.333333
symbolic type representing a numpy	type	0.011905
var_to_app_to_idx	var_to_app_to_idx	1.000000
legal value for a variable of this type	type is valid value a	1.000000
over each shape that broadcast them	generate broadcasting	0.066667
compute 2d	nnet	0.016129
c-implementation	csr c code node	0.333333
this apply instance in a new graph	gof apply clone with new	0.250000
input a	pool 2d same size input	0.500000
the variable out for	out	0.018519
elementary validations on the inner graph to ensure	scan validate inner graph	0.035714
inputs with a set of 3d	conv3d input	0.125000
optionally inserting broadcasted dimensions	tensor py operators	0.015625
when one or all operands are	y grad_preserves_dense	1.000000
inner-most loop	loop	0.027778
pdnode	pdnode	1.000000
conv output gradient w	grad	0.031250
scan	scan_module push out scan	0.050000
3	3d	0.100000
connection pattern	compile op from graph connection pattern	0.076923
the	gpuarray gpu	0.090909
to help the navigator deal with the	navigator optimizer attach	0.038462
c code to extract	type c extract	0.500000
overwrite the full inputs with the	useless inc subtensor	0.125000
will calculate the	compile orig	0.500000
sample n (n needs	size n	0.090909
the svd on	svd a	0.200000
a multinomial	multinomial	0.024390
a badviewmap exception when it detects the	check viewmap node storage_map	0.111111
an empty matrix it does	alloc	0.012500
listeners to help the navigator deal with the	gof navigator optimizer	0.038462
triangle of	m k	0.250000
beta_in	beta_in	1.000000
given axis	axis	0.205128
unification in u and uses it instead	o u	0.037037
of the main diagonal set to a specified	fill diagonal offset a val offset	0.100000
a memory alias	bad	0.013158
set of 3d filters	nnet conv3d input filters	0.142857
bitwise ~a	tensor invert	1.000000
to detect a	detect	0.090909
localoptimizer	opt group	0.043478
3d inputs with a set of 3d filters	conv3d input filters	0.142857
make a nested loop over several	tensor make loop	0.200000
sample from one or	random_state size	0.250000
the svd of a matrix :math a	svd	0.034483
otherwise call upgrade_to_float()	upgrade to float no	1.000000
value after	param init default	0.040000
list	list	0.400000
is the	opt	0.043478
and t is	node	0.007407
also work for gpuadvancedincsubtensor1	local inplace incsubtensor1 node	1.000000
disabled by default that removes all asserts from	remove all assert	0.055556
c_code for convop that unroll the batch size	unroll batch kern d unroll_bsize unroll_ksize	0.125000
return the c implementation of an op	clinker op c code node name inputs outputs	0.500000
the method query	graph to gpudb	0.142857
if	inputs node storage_map	0.166667
c-implementation of the	csr c code node	0.333333
when we overwrite the full inputs with the	local useless inc subtensor node	0.066667
the stack trace from one or more	gof copy stack trace	0.055556
get the 0 based level of the	type get	0.050000
constant scalar	core get scalar constant	0.333333
six moves urllib namespace that resembles the	module six	0.043478
return the cumulative sum	cumsum x	0.333333
message on the first call	msg	0.083333
builds the 2d kernel that	kernel 2d	0.050000
operation	mpisend	0.037037
structures	scan_module push out non seq scan	0.125000
a symbolic input	input error	0.500000
source code for this	cmodule location	0.038462
generate c	compile register shape i c	0.250000
the equivalent of localoptgroup for	graph to gpulocal opt group	0.055556
tensortype	type	0.011905
hack in profilemode to print the mflops	flops inp outp	0.083333
constant inputs to elemwise	elemwise constant inputs node	0.250000
this is the equivalent of localoptgroup for	to gpulocal opt group	0.055556
3d convolution for debugmode	nnet base abstract conv conv	0.125000
respect to wrt,	core subgraph grad wrt	0.062500
function for diagonalsubtensor	tensor nnet get diagonal subtensor view x i0	0.083333
an alloc and only adds dimension	tensor local alloc	0.111111
tries	top_shape	0.137931
macros for	cop get	0.033333
register a transfer function for alternative targets	register transfer fn	0.250000
code to allocate outputs	make alloc loop_orders dtype sub fortran	0.200000
like	like x	0.250000
print the mflops	op flops inputs outputs	0.125000
pattern of subfgraph defined by	pattern	0.028571
subtensor(setsubtensor x y idx idx) ->	tensor local subtensor inc subtensor node	0.500000
the bartlett spectral window in	tensor bartlett m	0.083333
set of all variables which may share the	infer reuse pattern	0.100000
in the module initialization	init	0.058824
each row correspond to the one hot	to one hot	0.142857
vector variable	vector	0.066667
y	code y	0.333333
none for the outputs	i_shapes	0.050000
uses shared variable names when persisting to	shared variable id	0.142857
not the other implementation of mod	scalar mod c code node	0.125000
the dimensions of this variable optionally inserting broadcasted	tensor py operators	0.015625
reproducible case for problems during	compile function dump filename inputs	0.166667
converts number	compile char from number number	0.142857
function into a basic theano op that will	op itypes otypes infer_shape	0.047619
be a 1-d	random_state size a	0.333333
converts this to expm1 a	expm1 node	0.066667
value after	default filter	0.040000
module is a package	meta path importer is package	0.500000
reorder the dimensions of	tensor tensor	0.014286
pattern for advancedsubtensor	pattern a idx	0.066667
scan the contents of a cache	dirname err files	0.083333
to a leftdims + rightdims	leftdims rightdims	0.333333
one-dimensional slices in pvals	pvals	0.071429
-x pattern	neg var	0.166667
print a warning	gof deprecated filename	0.166667
a symbol definition with an elementwise version	scal inplace symbol	0.333333
min for the minimum	minimum	0.083333
of a search through consecutive view_map()s	view roots r	0.200000
proxy for either	tensor div proxy	0.125000
this	to gpulocal	0.055556
the type's	array	0.041667
the type's :attr context_name	array type	0.055556
six moves	move move	0.250000
elemwise arctan of	sparse arctan	1.000000
of the specified pieces of vectors and matrices	sparse block gemv make node o	0.066667
triangular solve	tag solve triangular node	0.142857
an array with more	node	0.014815
file that was dumped	f persistent_load	0.052632
fullname	fullname	0.333333
main interface to manipulate the subgraph in functiongraph	gof function graph replace r new_r reason verbose	0.250000
dims	dims	0.833333
lock is	lock	0.100000
n-d tensor where n >=	ws ignore_border stride	0.090909
sdot	sdot	0.833333
of op classes that this opt	gof local optimizer tracks	0.071429
for the existence of the __unify_walk__ method	unify walk a b u	0.037037
inputs that	gof	0.002381
has only one client and that client	sitsot only	0.066667
zip file using external persistence	misc dump obj file_handler protocol persistent_id	1.000000
of this variable	tensor	0.009646
c code to allocate outputs	make alloc loop_orders dtype sub fortran	0.200000
item from the cache if available	call cache call fn args key	0.200000
symbolic variables in inputs	inputs	0.012658
previously received array using	mpirecv	0.037037
constants and the rest	rest	0.076923
upper triangle of an	tensor triu m k	0.250000
create a comparator to represent the dependence	gof make dependence cmp	0.111111
of all variables which may share the same	compile infer reuse pattern	0.100000
their unification	gof unification	0.125000
this as a decorator or context manager	flags	0.062500
of var transferred to	transfer var	0.100000
computes the output	tensor nnet conv op get output	0.047619
aliasing	destroy	0.009709
needed as some features introduce instance methods	gof function graph getstate	0.500000
return a code string specific to the apply	node name sub	0.111111
profile	profile	1.000000
attach_feature the method that attaches	gof bookkeeper on attach	0.142857
s_i	s_i var	1.000000
concatenate a number of scalars	make	0.017857
generates the c code	c code	0.166667
compute conv output gradient w	nnet conv3d grad	0.333333
convert python litterals to	make	0.017857
an exception class to raise in	core raise init	0.100000
uses set and dictionary data structures	scan_module push out non seq scan	0.125000
fgraph and a list of variables	fgraph	0.012195
1d kernel for bilinear upsampling this function builds	bilinear	0.019231
will be inserted at struct	struct node	0.062500
the inverse of	inverse	0.066667
elements along a given axis	axis	0.025641
composite op	scalar composite init	0.200000
remove subtensor/advancedsubtensor1 if it takes the full input	tensor local useless subtensor node	0.200000
reorder the dimensions	tensor tensor py	0.015873
attempts to replace a	inplace optimizer attempt	0.500000
op that will	op	0.009174
maps from variable	get equiv	0.142857
and replace it with logsoftmax x 's grad	local logsoftmax grad	0.200000
a compiled module from the loaded cache	cache get module name	0.166667
values of a shared variable to	shared variable	0.071429
will call the supplied function as its	as	0.024390
a uniform distribution	tensor uniform random_state size	0.125000
important note this function uses set	process node fgraph node	0.142857
output type dtype and broadcast there is	local useless alloc node	0.333333
the apply	apply	0.016667
a list to	from factored list	0.500000
post some text to a gist and return	misc post gist	0.333333
python not the other implementation of mod	mod c code node name inputs outputs	0.125000
add a new variable to theano config	core add config var name	1.000000
compiled module from the loaded cache or the	module cache get module	0.166667
see theano tensor max	tensor tensor py operators max axis	1.000000
to the type's :attr context_name	gpuarray gpu array	0.062500
pieces of vectors and matrices	sparse block gemv make node o w h	0.066667
six moves urllib	six	0.025000
the specified pieces of vectors and	sparse block outer make	0.066667
reorder the dimensions of	tensor py	0.015873
diagonalsubtensor and	nnet get diagonal subtensor	0.083333
to expm1	local expm1 node	0.066667
if fgraph is the	fgraph no_recycling profile	0.200000
clear_base_files	clear_base_files	1.000000
-> softmax_w_bias	tensor nnet local softmax with	0.200000
a graph is impossible to evaluate because of	destroy	0.009709
infer the number of dimensions from the	tensor infer	0.142857
data by walking the cache directory structure	gof module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
return absolute and relative error between a and	core numeric grad abs rel err a	1.000000
that gets a scan op a list of	op not_required	0.071429
reorder the dimensions of x default reverse them	transpose x	0.200000
perform	perform node x	0.166667
the inputs required to compute	inputs variable_list blockers	0.058824
to use	gpuarray	0.023256
together into a	make	0.017857
to determine the	adv index broadcastable	0.050000
to the	gpu array type	0.062500
proxy for	div proxy	0.125000
subprocess_popen returning the output error	misc output	0.066667
function to get the	type get	0.050000
slice [start stop step] transform it into	slice	0.038462
graph to a new node a clone in	clone	0.020833
can replace that with the *args directly	sparse local csm properties csm node	0.142857
_maker which handles much of the debugging work	function maker i o m	0.066667
add	gof key data add	0.500000
computes the output shape	get out shape ishape kshape border_mode subsample	0.500000
return connection pattern of subfgraph defined	connection pattern node	0.076923
type's :attr context_name	array type	0.055556
gradient w r	grad	0.031250
crossentropysoftmax1hotwithbiasdx op whose incoming	crossentropy softmax 1hot with bias dx	0.111111
for gpucorrmm (direction="forward"),	gpu corr mm	0.142857
the convolution gradient with respect	gpu dnn conv grad w	0.125000
update	update	1.000000
the gradient	core grad	0.166667
gives unique names	names	0.047619
like make_thunk() but only makes python thunks	make py thunk node storage_map compute_map no_recycling	1.000000
last access of a given	last access time	0.040000
the given graph contains a cycle parameters	gof contains cycle fgraph orderings	0.333333
diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor	0.083333
dimensions of this variable optionally inserting	py operators	0.015625
have the same shape	feature same shape	0.333333
that	gof cthunk find	1.000000
scalar values	scalar shared	0.083333
haskell'	outputs_info	0.142857
to use dnn	dnn	0.060606
performs the svd	tensor svd	0.200000
function for diagonalsubtensor and	get diagonal subtensor view x	0.083333
non_sequences	non_sequences	1.000000
equivalent	graph to	0.055556
return unique	d3viz py dot formatter	1.000000
numpy randomstate instance associated with a particular stream	random streams setitem item val	0.142857
apply	apply node	0.031250
function name to load data	gpuarray load w dtype	0.200000
a wrapper for numpy sort function	sort op	0.250000
input broadcastable in the specified axes	addbroadcast x	0.142857
this is the equivalent of localoptgroup for graphtogpu	group	0.047619
parameters ----------	typed list type	0.250000
of suitable dummy values	optimizer provide inputs	0.200000
with constant inputs	subtensor get constant idx inputs	0.250000
existing start	start	0.040000
compute 2d kernel for bilinear upsampling this	tensor nnet bilinear	0.111111
calculate	calculate	1.000000
default	apply default	1.000000
to help the navigator deal with	gof navigator	0.038462
of the graph	gof	0.002381
input by a specified factor takes as input	signal pool 2d input	0.090909
to make it work for elemwise and gpuelemwise	tensor local elemwise fusion	0.166667
list of r	r	0.028571
dict op -> total number of thunk calls	compile profile stats class callcount	1.000000
the topooptimizer from	in2out	0.043478
by mapping it	ctx	0.125000
+ alpha * dot	tensor gemv c	1.000000
product of two sets of pieces	sparse block	0.111111
for corr3dmm (direction="forward"),	corr3d mm	0.111111
in the original	outputs copy_inputs_and_orphans memo	0.029412
numpy ndarray	tensor	0.003215
macros for use	cop get	0.033333
to determine	adv index broadcastable	0.050000
an apply_node recursively search from this	apply_node check	0.066667
given axis es	axis dtype op	0.083333
config blas ldflags	ldflags	0.111111
sum{0 1 n} -> sum{} or	local sum prod all to none node	1.000000
hash equal for same kinds	tensor type hash	0.166667
view in the forward but clip the	clip x lower_bound upper_bound	0.090909
if the g++	gof	0.002381
merge some gpucareducecuda and gpuelemwise	local gpu elemwise careduce node	1.000000
connection pattern of a subgraph defined by given	gof io connection pattern	0.055556
function drawing from	random_state n pvals size	1.000000
variables and apply_nodes to this graph	graph	0.016393
passed-in key is found	get from key key key_data	0.111111
a b), axis=0) -> elemwise{scalar op} a	tensor local reduce join node	0.111111
helper function	helper random_state n	0.666667
sum along the	tensor sum	0.111111
used is	gof gcc	0.027778
python litterals to theano constants in subtensor arguments	constant args	0.250000
reverse-mode gradient updates for matrix solve operation c	solve grad inputs output_gradients	0.333333
safely compute ceil(float_division a	ceil intdiv a	1.000000
of libraries	libraries	0.111111
the dimensions of this variable optionally	operators	0.017241
constant	constant	0.233333
compile c code when doing constant folding	python constant folding	0.142857
if those nodes are not	gof	0.002381
return indices over each	indices	0.076923
c type of	type c	0.071429
this op could be	prod l op	0.033333
and they are spaced by 2**72 samples	get substream rstates n_streams dtype inc_rstate	0.142857
for every node	graph	0.016393
inputs required	inputs variable_list blockers	0.058824
header for openblas	tensor openblas	0.250000
unroll the batch size	conv code unroll batch	0.166667
caller is replace_all_validate just raise	gof validator validate fgraph	0.125000
template filled by broadcasting value through	tensor broadcast like value template fgraph	0.125000
the output after pad_dims	gpuarray unpad dims output	0.333333
a uniform into sample without replacement from	choice from uniform	0.166667
profiling to print the mflops	op flops inputs outputs	0.125000
convert data to something which can be associated	type filter data strict	1.000000
between min	min	0.071429
version of maxandargmax	max and argmax	0.125000
new variable	var name doc configparam root	0.250000
c_code for convop that unroll the batch size	code unroll batch kern d unroll_bsize unroll_ksize	0.125000
sample from a normal distribution	tensor random streams base normal size	1.000000
convolution gradient with respect to the weights	gpu dnn conv grad w	0.125000
to print the mflops	conv op flops inputs outputs	0.125000
if this local optimization wants to add	gof local optimizer add	1.000000
transfer to a tensortype if	tensor py operators transfer	0.125000
or an appropriately wrapped/converted data	type filter data strict allow_downcast	0.250000
variable optionally inserting broadcasted	py	0.014286
initialize the variables that	gof clinker type	0.066667
list of headers that are needed	clinker headers	0.047619
litterals	tensor make	0.076923
symbolic row variable	row name dtype	0.050000
a special compound l{op}	softmax argmax1hot	0.083333
context object mapped	context	0.035714
function tries to recognize the updates ordereddict	scan_module get updates	0.034483
the method that	gof	0.004762
function :func images2neibs <theano tensor nnet	nnet images2neibs	0.333333
along given axis	axis	0.025641
dimshuffle and index the	tensor local dimshuffle	0.052632
fill	tensor second inplace	1.000000
be inserted to maintain order	tensor searchsorted x v side sorter	0.142857
an apply_node recursively search from this	apply_node	0.050000
for abstractconv3d	abstract conv3d	1.000000
of variable type	type	0.011905
scalar values default int64 or float64	scalar shared	0.083333
variable_list	variable_list	1.000000
transpose	transpose	0.833333
to the type's	gpuarray gpu array type	0.062500
helper function for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view x	0.083333
matrix solve operation	solve	0.032258
one or more multinomial distributions	tensor multinomial random_state	0.040000
to determine the broadcast	tensor adv index broadcastable	0.050000
into a canonical form	tensor get canonical form	0.045455
try to compile	gof compiler try	0.250000
of localoptgroup	graph to gpulocal opt group	0.055556
matrix where each row correspond to the one	one	0.076923
python litterals	tensor make constant	0.100000
computes the output dimensions of convolving	get output	0.047619
the output type dtype and broadcast there is	local useless alloc node	0.333333
current paramstype contains the specified theano type	gof params type has type theano_type	0.500000
clone the graph and get a memo a	function graph clone	0.166667
no	no	1.000000
a symbolic row variable	row name dtype	0.050000
of _maker which handles much of the	function maker i o m	0.066667
ys	ys	1.000000
-> x	local	0.014085
ge	ge	0.714286
output	tensor nnet conv op get output	0.047619
change the value after the import	default	0.030303
output gradient w r t its weights	conv3d grad wrt weights input output_grad filter_shape input_shape	0.333333
the scan that	scan	0.034483
was dumped to	f persistent_load	0.052632
of moved objects in six moves	six moves urllib	0.181818
access of a	access time path	0.200000
shape field	csm shape csm	0.333333
a new variable to	var name doc	0.250000
composite op	composite init	1.000000
elementwise addition (inplace on a)	tensor add inplace a b	1.000000
subtensor and its idx_list reorders the inputs	inputs idx_list get_count	0.100000
remove broadcastable dimensions from the shape of an	tensor py operators squeeze	0.200000
this op could be very	op	0.009174
input_variables output_variables) where function is a thunk that	gof linker make thunk	0.045455
g++ version	gof	0.002381
baddestroymap if	check inputs node	0.166667
subclass to add	tensor	0.006431
description	description	1.000000
this variable	tensor tensor py operators dimshuffle	0.019231
loading of moved objects in six moves urllib_error	module six moves urllib	0.090909
given axis es of a tensor input	input axis dtype keepdims	0.500000
to the diagonal of an empty matrix it	diag	0.023810
apply instance in a new	apply clone with new inputs inputs strict	0.500000
a failed fix done in august 2011	tensor load shared variable val	0.142857
decorator for creating a class with a metaclass	add metaclass metaclass	0.125000
a variable on the	gpuarray as gpuarray variable	0.166667
in u and uses	o u	0.037037
:attr	gpuarray gpu array type	0.062500
expects the compile lock to	add to	0.142857
parses a config string (comma-separated key=value components) into	core parse config string config_string issue_warnings	0.166667
"reverse-mode" gradient [1]_ for the cholesky factorization of	tensor cholesky grad perform node inputs outputs	1.000000
tile	tile	1.000000
for comparing tensorconstant	tensor	0.003215
wrapper around c_init that initializes py_name to py_none	gof get c init r	0.250000
key	key	1.000000
logsoftmax x 's	nnet local logsoftmax	0.076923
on all device	device	0.076923
be removed	outs	0.100000
linker's fgraph	clinker make thunk input_storage output_storage storage_map keep_lock	0.333333
compute 2d	tensor nnet	0.017544
that 1) this destroyhandler wasn't	gof destroy handler	0.250000
loop	reordered loop	0.111111
-x pattern	neg	0.083333
out_idxs	out_idxs	0.300000
basic theano	itypes otypes infer_shape	0.142857
the given axis es of	axis dtype op	0.083333
the value after	config param init default	0.040000
of this variable	tensor tensor py operators dimshuffle	0.019231
values of a shared variable to	compile shared variable	0.083333
of a shared variable to	shared variable	0.071429
output type dtype and broadcast there is	local canonicalize alloc node	0.333333
apply instance	apply	0.033333
return data	gof	0.002381
performs the matrix inverse on gpu	gpu matrix inverse a	0.200000
where x is an input vector and t	node input_storage output_storage	0.038462
:attr context_name	gpuarray gpu array type	0.062500
that respects the conventions imposed	theslice length	0.052632
this generates the c code for gpucorrmm	gpuarray base gpu corr mm c code	0.090909
inserting broadcasted	py	0.014286
the stopping condition returned	ls	0.090909
to wait on a	wait	0.045455
a cache directory and return full path of	module name from dir	0.071429
variables in inputs that are	gof	0.002381
returned by this	gof seq optimizer	0.200000
of shape "kshp"	shape inshp kshp stride mode	0.142857
interface to manipulate the	r new_r reason verbose	0.071429
and "code_cleanup" together	cleanup node name	0.500000
this function uses set and dictionary data structures	push out non seq	0.125000
the replacement if the ops	validate replace	0.050000
remove subtensor/advancedsubtensor1 if it takes the full	tensor local useless subtensor node	0.200000
uses shared variable	shared variable	0.071429
-1 and converts this to expm1 a	local expm1	0.066667
b), axis=0) -> elemwise{scalar op} a b	tensor local reduce join node	0.111111
the broadcast pattern for advancedsubtensor	pattern	0.028571
return a symbolic row variable (ndim=2	tensor row name dtype	0.050000
duplicate this apply instance in a new	gof apply clone with new	0.250000
a series of wrapper	linker many linkers wrappers	0.047619
reproducible case for	compile function dump filename inputs outputs mode	0.166667
the output specs	fgraph input_specs output_specs accept_inplace	0.142857
all variables which may share the	infer reuse pattern	0.100000
a	alloc	0.012500
product of the specified pieces of vectors	sparse block outer make node o	0.066667
number to string	number number	0.125000
if l has any duplicates (according to __eq__)	has duplicates l	0.111111
that operates	gof linker make	0.250000
self _rop_op from user supplied form to	compile op from graph recompute rop op	0.200000
from the cache if available	gof call cache call fn args key	0.200000
diagonal of an empty matrix it does the	diag	0.023810
raised by get_scalar_constant_value if called on something that	error	0.025000
of this variable	dimshuffle	0.014493
of moved objects in six moves urllib_request	six moves urllib request	0.333333
received array using mpi	mpirecv	0.037037
only	sitsot only	0.066667
self _rop_op from user supplied form to type	op from graph recompute rop op	0.200000
x shp -> alloc(unary x shp)	local alloc unary node	0.250000
structured addition of a sparse matrix and	structured add	0.142857
set of 3d	tensor nnet conv3d	0.071429
number	number	1.000000
uses set and dictionary data structures	out non seq	0.125000
return a function that	gof	0.002381
replace a leaf of	nnet replace leaf	0.100000
add some requirements to the	add requirements	0.333333
revert the replacement if the ops	gof replace validate replace	0.050000
advancedincsubtensor1(x x[ilist]+other ilist set_instead_of_inc=true) ->	tensor local set to inc subtensor node	1.000000
function for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view	0.083333
the output shape for a convolution with	gpu dnn conv get out shape	0.142857
graph and get	function graph	0.040000
nit_sot output of scan return true	out scan	0.035714
return a list of shape tuple or	default infer shape	0.066667
one hot encoding of each element in y	one hot y nb_class dtype	1.000000
an [advanced]incsubtensor[1], whose increment is an alloc of	local useless inc subtensor alloc node	0.166667
of values identical with	scan_module forced replace	0.333333
of _maker which handles much of	compile debug mode function maker i o m	0.066667
folowing changes in the	mul switch sink	0.045455
p	p	1.000000
output dimensions of convolving an image of	op get output	0.047619
hack in profiling to print the mflops	flops inputs	0.125000
dimensions of this variable optionally	py operators	0.015625
a	bad view	0.027027
decorator to merge addition by a value	merge cls alpha_in beta_in out_in	0.250000
loading of moved objects in six moves urllib_error	six moves urllib error	0.142857
by default that removes all asserts	local remove all assert	0.055556
the abs	abs	0.133333
unroll the	nnet gen conv code unroll	0.250000
r operation on f wrt to wrt	rop f wrt	0.200000
stop step] transform it into a canonical	get canonical	0.125000
load a file that was dumped to a	misc load f persistent_load	0.333333
the stack	copy stack	0.333333
to the	type	0.011905
this op	gof clinker object	0.250000
called by functiongraph attach_feature the method that	gof	0.002381
cache data by walking	refresh age_thresh_use delete_if_problem cleanup	0.166667
important note this function uses set	out seq scan process node fgraph	0.142857
vector and	node input_storage	0.038462
moved objects in six moves urllib_response	module six moves urllib response	0.333333
product of two	sparse block	0.111111
evaluate because of aliasing and destructive operations	destroy	0.009709
this function tries to	top_shape border_mode subsample	0.166667
the broadcast pattern for advancedsubtensor output	pattern	0.028571
to reps	reps	0.142857
and return full path of the dynamic lib	gof module name from	0.076923
pieces of vectors	sparse block gemv make node	0.066667
merge some gpucareducecuda	careduce	0.142857
replace replace in	replace	0.032258
same	feature same	0.333333
navigator deal with the	gof navigator optimizer	0.038462
deprecated use zero_grad() or disconnected_grad() instead	core consider constant x	1.000000
context_name	gpuarray gpu array type	0.062500
self _rop_op from user supplied form	op from graph recompute rop op	0.200000
to make	to os environ pathlist var newpath	0.038462
see theano tensor min	tensor py operators min axis	1.000000
replace_all_validate revert the replacement	gof replace validate replace all validate remove fgraph	0.111111
simple algorithm to find broken	compile find bad optimizations2 order reasons r_vals	0.111111
dependence of nodes	dependence	0.035714
of 2d filters	conv2d input filters	0.125000
l{codeblock} instances returns a	gof code gen blocks	0.050000
the value after the	default filter	0.040000
a real-valued input	tensor rfft inp norm	0.142857
the source code for	compile cmodule location	0.038462
graph have a stack	gof check stack	0.142857
the rest	rest inputs elemwise only_process_constants	0.125000
the last access	last access	0.040000
we can't change the value after	init default	0.040000
with debug info	with op	0.166667
minimum see min for the minimum	minimum	0.083333
2d or 3d convolution for	nnet base abstract conv conv	0.125000
compiles the source code	cmodule location	0.038462
of each value in array of ints	x weights minlength assert_nonneg	0.125000
given a inner nit_sot output of	inner sitsot	0.083333
baddestroymap if	node storage_map r_vals	0.166667
a multiplication	mul	0.076923
to the end variables of a	end	0.040000
"init_code" together	init code struct	0.125000
broadcastable	broadcastable node	1.000000
pieces of vectors and	sparse block outer make node o x	0.066667
return c code to extract a	clinker type c extract name	0.250000
argmax over a given axis or	argmax	0.066667
op twice gives inconsistent outputs	bad	0.013158
return a symbolic row variable (ndim=2 broadcastable=[true	row name dtype	0.050000
similar behaviour as haskell'	fn sequences outputs_info non_sequences	0.333333
new variable instance of type self	pure type call	0.500000
object but we don't clone the data	clone	0.020833
a and	a	0.024194
return complex-valued tensor from polar	from polar	0.250000
helpful function that gets a scan	not_required	0.111111
pattern for advancedsubtensor output	pattern a	0.066667
initializes py_name to py_none	get c init r name sub	1.000000
this is meant as a shortcut	gof optimizer optimize fgraph	0.200000
"lifts" dimshuffle through elemwise operations	dimshuffle lift node	0.250000
"lifts" dimshuffle through elemwise operations and merges	dimshuffle lift node	0.250000
not the other implementation of mod	mod c code node name	0.125000
the end variables of a	wrt end	0.050000
vector to the	alloc	0.012500
convert python litterals	tensor make	0.076923
remove rebroadcast if id does not actually change	tensor local useless rebroadcast node	1.000000
the	gpulocal opt	0.055556
list of lib directories that	clinker header dirs	0.055556
a 4-d tensor it	patch_size	0.050000
is the	no_recycling	0.111111
reshapes the output after pad_dims	unpad dims output input leftdims rightdims	0.333333
convert data to something which can be associated	tensor type filter data	1.000000
of integers indicating the version	version apply node	0.125000
non-zero in the flattened version of	tensor flatnonzero	0.166667
can't change the value after the	config param init default	0.040000
function for diagonalsubtensor	diagonal subtensor view x i0	0.083333
see theano tensor argmax	py operators argmax axis keepdims	1.000000
on wraplinker that runs	gof wrap linker	0.083333
for a diagnosis if things go awry	gof function graph check integrity	0.250000
this op	gof clinker op c code	0.333333
the given axis es of a tensor input	input axis dtype op	0.500000
to use	gpuarray gpu	0.045455
object a that would be a	gof	0.002381
to the type's :attr context_name	gpuarray	0.023256
iff other is the same kind of	other	0.090909
optionally inserting broadcasted	tensor py	0.015873
listener	listener	1.000000
this op could be	tensor prod l op	0.033333
return the c implementation	c code node name inputs outputs	0.250000
reintroduces in	tensor make keep dims	1.000000
fgraph this is the place to	fgraph	0.012195
this variable optionally inserting broadcasted dimensions	py operators dimshuffle	0.019231
optionally inserting broadcasted dimensions	py operators dimshuffle	0.019231
only on cpu here	local pow specialize	0.250000
same rounding than numpy round half to even	round half to even	0.166667
elemwise signe of	sparse sgn	1.000000
is the equivalent of	to gpulocal	0.055556
elemwise and gpuelemwise	tensor local elemwise	0.166667
new_best	new_best	1.000000
convolution	conv get	0.100000
type numpy typenum that corresponds	type dtype specs	0.071429
a tuple of integers indicating the version	c code cache version	0.125000
clone	clone	0.166667
dot product when one or all operands are	true dot x y grad_preserves_dense	1.000000
non-zero in the flattened version of a	tensor flatnonzero a	0.200000
converts number to string by rendering	from number number	0.142857
the bartlett spectral window in the time-domain	tensor bartlett	0.083333
the shape s to previously un-shaped variable r	shape r s override	0.500000
validations on the inner graph	validate inner graph	0.035714
be used to upsample	ratio normalize	0.200000
life	function graph event	1.000000
raise baddestroymap	inputs node storage_map r_vals	0.166667
name	name opt	0.333333
converts number	char from number number	0.142857
with constant inputs replaced by their python scalar	constant idx inputs allow_partial only_process_constants elemwise	0.071429
this is the equivalent	to gpulocal opt	0.055556
to the type's	array	0.041667
proxy for either true_div or int_div	div proxy	0.125000
for diagonalsubtensor	tensor nnet get diagonal subtensor view x	0.083333
mflops	tensor nnet conv op flops	0.125000
register r's shape in the shape_of dictionary	tensor shape feature init r r	0.333333
and converts this to expm1 a	local expm1	0.066667
convert radian a	rad2deg a	0.333333
conventions imposed by python and numpy	theslice length	0.052632
a specified factor takes as input a n-d	signal pool 2d input ws ignore_border stride	0.100000
drawing from	pvals size	1.000000
that would be a	gof pure	0.033333
data structures	out non seq	0.125000
the basic constant class	tensor constant	0.055556
feature should remove any	feature	0.083333
version used	gof gcc	0.027778
of the last access of a given	last access	0.040000
apply instance in	apply	0.016667
function into a basic theano	itypes otypes infer_shape	0.142857
turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	tensor nnet local softmax with bias node	0.200000
around c_init that	gof	0.002381
max pooling	max pool	0.500000
tensor	tensor	0.025723
deprecated use in instead	param	0.166667
a mrg stream state and they are spaced	sandbox mrg random streams	0.033333
calculating the dot product	true dot	0.166667
dictionary data structures	scan_module push out non seq scan	0.125000
replace it with a triangular	triangular	0.076923
fgraph and a list	fgraph outputs_to_disown	0.047619
functiongraph a constant that is	constant error	0.166667
+ alpha * dot	gemv c code	1.000000
and uses it instead of the var	o	0.076923
set of 3d	conv3d input	0.125000
the same shape	same shape	0.333333
a inner	inner	0.041667
reorder the dimensions of this variable optionally	tensor py operators	0.015625
all the node i pairs such	function graph clients	0.200000
to parse the tree and figure out	scan_module traverse out x x_copy d	0.047619
returns function	gof	0.002381
to replace a leaf of a multiplication	tensor nnet replace leaf	0.100000
same	same	0.750000
feature	gof feature on detach	0.200000
m1 and the second half by	m1	0.027027
in u and uses it instead of	o u	0.037037
a new instance of this mode	mode clone link_kwargs optimizer	0.333333
initial value for myresult	gpuarray gpu careduce cuda assign init first_item	0.166667
a poisson	poisson	0.142857
those nodes are not in this	function	0.052632
f	lop f	0.166667
input nodes to output nodes of the graph	gof	0.002381
string specific to the apply	apply	0.016667
clip x	clip x	0.250000
and only if this enum	gof enum	0.166667
that initializes	gof get	0.100000
list of variables	gof variables and	0.250000
occurrences of each value in array of ints	tensor bincount x weights minlength assert_nonneg	0.125000
that makes a value from an integer	gof cdata type get func	0.500000
coding_dist	coding_dist	1.000000
fill s v -> alloc(v shape	tensor local fill to alloc node	0.250000
replace that with the *args directly	csm properties csm	0.142857
register a transfer function for alternative targets	tensor register transfer fn	0.250000
c code to declare variables that	gof clinker type c declare	0.333333
of the last access of a given file	last access	0.040000
mrg stream	sandbox mrg random streams	0.033333
function is basically	extract constant x elemwise only_process_constants	0.058824
inserting gemm operations	gemm	0.066667
constitutes an upcast	is an upcast	1.000000
use within the op code	op	0.009174
var2	var2	1.000000
comparing tensorconstant	tensor constant	0.055556
the connection pattern of a subgraph defined by	gof io connection pattern	0.055556
convolution gradient with respect to	dnn conv grad w	0.125000
poisson	tensor random streams base poisson	0.500000
is basically a	tensor extract constant x elemwise only_process_constants	0.058824
variable	py operators	0.015625
computes the output dimensions of convolving an image	op get output	0.047619
policy	policy	1.000000
gpu version of maxandargmax	gpu max and argmax	1.000000
32 for 32bit arch 64 for 64bit arch	core local bitwidth	1.000000
stack trace from one or more tensor variables	copy stack trace	0.055556
functions raises a badviewmap exception when it detects	check viewmap node	0.111111
reorder the dimensions of this variable	py operators	0.015625
tensorvariable whose	tensor as	0.066667
freev is unified to boundvariable(other_object)	gof unify walk fv o u	0.200000
convolution with the	dnn conv get	0.100000
subtensor	inc subtensor	0.250000
min for the minimum in one	tensor minimum x y	0.090909
to new_r	new_r reason	0.500000
a symbolic vector	tensor vector name dtype	0.166667
as replace_all_validate revert the replacement if the	replace validate replace all validate remove	0.111111
access	access time path	0.200000
convenience function to roll tensortypes	roll x shift	0.250000
helper function to	helper random_state n	0.333333
class returns the bartlett spectral window in the	bartlett m	0.083333
^ b inplace on a	xor inplace a b	0.333333
this variable optionally inserting broadcasted	operators	0.017241
type optionally with a new dtype	tensor type clone dtype	0.333333
input that wasn't in the destroy_map	destroy	0.009709
a crossentropysoftmax1hotwithbiasdx op whose incoming gradient	useless crossentropy softmax 1hot with bias dx	0.111111
in the shape_of dictionary	feature init r r	0.333333
py_none	init	0.058824
:param execute if true execute a	execute execute verbose m	0.250000
listeners to help the navigator deal with the	navigator	0.032258
3d	conv3d	0.076923
foldl	scan_module foldl	1.000000
specified factor takes as input	tensor signal pool 2d input	0.090909
safe	safe	1.000000
list of compilation flags from config	libs flags libs_dir include_dir	0.052632
to the apply	apply	0.016667
a triangular solve	linalg tag solve triangular	0.142857
unique names to an iterable	names	0.047619
given that v	set v	0.125000
svd	tensor svd	0.200000
is the equivalent of	gpulocal opt	0.055556
return indices over each shape that broadcast them	generate broadcasting indices	1.000000
shape or the other arguments	ndim bcast ndim shape	0.250000
print the mflops	flops	0.076923
inner graph to ensure that it is	scan validate inner graph	0.035714
from config blas ldflags	ldflags	0.111111
pairs	pairs	1.000000
python not the other implementation of mod	mod c code node name inputs	0.125000
a crossentropysoftmax1hotwithbiasdx op whose incoming gradient is	local useless crossentropy softmax 1hot with bias dx	0.111111
a previously sent array	mpisend	0.037037
the fast fourier transform of a real-valued input	rfft inp norm	0.142857
code for	code	0.150000
the same shape	tensor shape feature same shape	0.333333
op that copies a vector to the diagonal	alloc diag	0.027027
symbolic row variable (ndim=2 broadcastable=[true	row name dtype	0.050000
maps	equiv inputs	1.000000
crossentropy	crossentropy	1.000000
given axis es of a tensor input	input axis ddof keepdims	0.500000
create pydot graph object from theano	py dot formatter	0.500000
the replacement if the	replace	0.032258
listeners to help the navigator	gof navigator optimizer attach updater fgraph	0.038462
try to use complex numbers	mod check x y	0.166667
lib	clinker lib	0.333333
sample from a uniform distribution	tensor uniform	0.125000
spatio-temporal filters with	nnet conv3d signals filters	0.111111
op could be very easy if it	tensor prod l op	0.033333
return c code	name sub	0.050000
numpy ndarray contains any np nan values	compile contains nan arr node var	0.500000
epoch of the last access of a	last access time path	0.040000
symbolic row variable (ndim=2 broadcastable=[true false])	row name	0.050000
end	grad wrt end	0.050000
of theano gof graph	graph	0.016393
vector to the diagonal	alloc diag	0.027027
that gets a scan op a	op not_required	0.071429
computes gradients of cost and/or from	cost	0.045455
try to turn softmax(sum_of_stuff) ->	local	0.014085
that was dumped	f persistent_load	0.052632
reorder the dimensions of x default reverse them	tensor transpose x	0.200000
to output	gof	0.002381
code when doing constant folding of	constant folding	0.142857
svd	svd	0.241379
output error	misc output	0.066667
that unroll the batch size loop	code unroll batch	0.166667
computes the standard deviation along	std	0.058824
this to expm1 a	local expm1 node	0.066667
the replacement if the ops in the list	replace validate replace	0.050000
types	gof clinker type	0.133333
minimum elements obtained by iterating over given axis	tensor argmin x axis keepdims	0.500000
add tag trace to an node or variable	add tag trace thing user_line	0.166667
given inputs and	inputs	0.012658
inputs with a set of 3d filters	conv3d input filters	0.142857
stream state and they are	random streams	0.058824
change the value after the	param init default filter	0.040000
argmax	argmax axis	1.000000
c code to declare variables that will	gof clinker type c declare	0.333333
a new variable	var name doc	0.250000
crossentropysoftmaxargmax1hotwithbias op	crossentropy softmax1hot with bias dx	0.500000
the	opt	0.043478
compute conv output	nnet	0.048387
compiled module from the loaded cache or	gof module cache get module	0.166667
the other implementation of mod	mod c code	0.125000
hint that the variable v is positive semi-definite	sandbox linalg psd v	0.250000
explicitly upcasts constant inputs to elemwise	elemwise constant inputs	0.250000
compute the variable out for occurrences	out	0.018519
computes the output dimensions of	nnet conv op get output	0.047619
the __unify_walk__ method for one of	gof unify walk a b u	0.037037
generate c	i c	0.250000
shape	shape feature set shape	0.333333
a convolution with the specified parameters	dnn conv get	0.100000
a memory alias that wasn't in the	view	0.022727
have the same	shape feature same	0.333333
an fgraph and	fgraph outputs_to_disown	0.047619
and applies them to the node	local	0.014085
grad of	grad grad	0.166667
and a set of arrays to choose from	choose a choices out mode	0.200000
"lifts" dimshuffle through elemwise operations and	dimshuffle lift	0.250000
remove are still in the graph	fgraph replacements remove reason	0.055556
tuple or none for the outputs of	i_shapes	0.050000
new graph	with new inputs inputs	0.166667
when we overwrite the full inputs with	local useless inc subtensor node	0.066667
numpy ndarray contains any np inf values	compile contains inf arr	0.500000
that compute the variable out for occurrences	out	0.018519
mult	mult	1.000000
transfer to	tensor py operators transfer	0.125000
confusion matrix of	nnet confusion matrix	0.166667
x shp -> alloc(unary x shp)	tensor local alloc unary node	0.250000
return a symbolic scalar variable	tensor scalar name dtype	0.166667
this apply instance in	gof apply clone	0.166667
decorator	cls alpha_in beta_in	0.500000
solve operation c = a \	solve	0.032258
a memory alias that wasn't in	bad view	0.027027
simple algorithm to find broken optimizations	compile find bad optimizations2 order reasons r_vals	0.111111
are non-zero in the flattened version of a	tensor flatnonzero a	0.200000
followed by	modulo	0.250000
specified outputs inplace	inplace fgraph	0.142857
for gpucorrmm (direction="forward"), gpucorrmm_gradweights	base gpu corr mm	0.250000
the grad of this op could be	prod l op	0.033333
removes all asserts from the	local remove all assert	0.055556
such that	gof	0.002381
maps from variable	get equiv inputs	0.142857
output type dtype and broadcast there is	tensor local useless alloc node	0.333333
type	cdata type	1.000000
need not be checked for nan and	is numeric value arr var	0.166667
the dimensions of this variable optionally inserting	operators	0.017241
will have separated maker	share_memory swap delete_updates name	0.250000
output gradient w r	grad	0.031250
function for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view x i0	0.083333
importer	importer	0.750000
solve	solve	0.225806
plus	plus	1.000000
the outer product of two sets	sparse block outer	0.047619
helper function for diagonalsubtensor and	nnet get diagonal subtensor view x i0	0.083333
failure_callback	exc	0.200000
of localoptimizer and applies them to the node	local opt group	0.052632
the clients list of r	r	0.028571
inner	scan validate inner	0.142857
a warning message	gof deprecated filename msg	0.041667
an aboslute path	gof cop get path cls f	0.166667
pattern	pattern	0.228571
a new variable	var name doc configparam	0.250000
the last access	last access time path	0.040000
returning the output error and exit code	misc output subprocess popen command	0.100000
and/or from existing start gradients up to	start	0.040000
function for diagonalsubtensor	get diagonal subtensor view	0.083333
set the values of a shared	compile shared	0.166667
access of	access time	0.200000
batch_size	batch_size	1.000000
of this variable optionally inserting broadcasted dimensions	operators dimshuffle	0.019231
set and dictionary data structures	scan_module push out non seq	0.125000
access of	access	0.100000
helper function for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor	0.083333
none or a tensorvariable whose type	tensor as	0.066667
symbolic row	tensor row	0.050000
of all variables which may share the	compile infer reuse pattern	0.100000
output gradient w r t its weights	grad wrt weights input output_grad filter_shape input_shape	0.333333
for seqoptimizer	optimizer	0.062500
idx list to get the right values	tensor get idx list	0.076923
n-th order discrete difference	tensor diff x n	0.333333
a canonical form that respects the	canonical form	0.045455
row variable (ndim=2 broadcastable=[true	row name	0.050000
is the	graph	0.016393
seconds since the epoch of the last	last	0.076923
id	id	0.600000
a symbolic	name dtype	0.333333
maps old nodes to new	equiv check_integrity attach_feature	0.200000
required return c code to extract a	gof clinker type c extract name	0.250000
source code for	clinker compile cmodule location	0.038462
print the mflops	abstract conv flops inp outp	0.125000
elemwise arcsinh of	sparse arcsinh	1.000000
or 3d convolution	conv conv	0.250000
the inner graph to ensure	validate inner graph	0.035714
centered on avg with the specified	avg	0.166667
bottom	bottom	1.000000
permutations	permutation random_state	0.500000
see max for the maximum in one tensor	tensor maximum x y	0.090909
register	tensor register	0.500000
wait on a previously received array using	wait	0.022727
a (inplace on a)	inplace a	0.333333
pieces of vectors and matrices	sparse block outer make node o x y	0.066667
compute 2d kernel for	tensor nnet	0.017544
helper function drawing from multinomial	tensor multinomial helper random_state n pvals size	0.500000
python	make	0.017857
converts number to string by rendering it in	from number number	0.142857
to wrt, computes gradients of	core subgraph	0.062500
list remove are still in	fgraph replacements remove reason	0.055556
vector	tensor vector	0.500000
partition a list of variables	scalarconsts	0.076923
to pass to helper_c_code	get helper c code	0.142857
helper function for diagonalsubtensor	get diagonal subtensor view x	0.083333
named module is a package	path importer is package fullname	0.250000
this op	prod l op	0.033333
bitwise a | b inplace on a	or inplace a b	0.333333
value pairs that will be turned into macros	cop	0.028571
a real-valued input on the gpu	gpuarray curfft inp norm	0.066667
return connection pattern of subfgraph defined by	op from graph connection pattern	0.076923
a code string specific to the apply to	apply	0.016667
broadcasted	tensor tensor py	0.015873
"lifts" dimshuffle through elemwise operations and merges consecutive	dimshuffle lift node	0.250000
op that copies a	alloc	0.012500
the list remove are still in the	fgraph replacements remove reason	0.055556
pathlist	pathlist	1.000000
of the subgraph between i and o parameters	i o	0.041667
convolution gradient with respect	dnn conv grad w	0.125000
that respects the conventions imposed by	theslice length	0.052632
specified pieces of vectors	sparse block outer make	0.066667
type's :attr	array type	0.055556
coordinate specification	abs angle	1.000000
element	element	1.000000
uses set and dictionary data structures	non seq scan	0.090909
(comma-separated key=value components) into	config_string issue_warnings	0.333333
of v by a with a modulo of	v	0.011111
rfft	rfft	1.000000
to turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	nnet local softmax with bias node	0.200000
the 1d	1d	0.090909
offers to make itself	to os environ	0.038462
ints	bincount x weights minlength assert_nonneg	0.125000
return a tensorvariable	make variable name	0.333333
destroyhandler wasn't already attached to some fgraph	destroy handler on attach fgraph	1.000000
multinomial distributions defined by one-dimensional slices	tensor multinomial random_state	0.040000
since the epoch of the last access	last access time	0.040000
inner nit_sot output of	inner	0.041667
is an input vector and t is a	node input_storage output_storage	0.038462
compile lock to be held	cache add to cache module key module_hash	0.166667
python 3 namespace	moves urllib	0.038462
raised by get_scalar_const_value if called on something that	error	0.025000
on the inner graph to ensure	validate inner graph	0.035714
the user is not attempting to use dnn	safe no dnn	0.125000
the clients list of r	r client_to_remove	0.200000
helper function for diagonalsubtensor and	nnet get diagonal subtensor view x i0 i1	0.083333
helper function for diagonalsubtensor	diagonal subtensor view x	0.083333
to represent the dependence of nodes in a	dependence	0.035714
of op classes that this opt applies	gof local optimizer tracks	0.071429
this function must return a thunk that	thunk	0.021277
moved objects in six moves	module six moves urllib error	0.142857
arccosine of	tensor arccos	1.000000
the c code for gpucorrmm (direction="forward"),	gpu corr mm c code	0.090909
and t is	node input_storage	0.038462
a tensorvariable whose	as	0.024390
node by one which computes the specified	node	0.007407
wise	wise	1.000000
x to a tensor of	x	0.008772
idx list to	idx list	0.250000
convolution	conv	0.296296
the data	data	0.090909
vector and	node input_storage output_storage	0.038462
takes as	signal max pool 2d same size	1.000000
diagonalsubtensor	tensor nnet get diagonal subtensor view x	0.083333
arguments which must be hashable	gof memoize f	0.250000
on the	gpuarray as gpuarray	0.333333
some perform() or c_code() modified an input that	map	0.047619
baddestroymap if	inputs	0.012658
the other implementation of mod	scalar mod c code node	0.125000
the hack in profiling to print the mflops	flops inputs outputs	0.125000
has only one client and	sitsot only	0.066667
this is a	nnet	0.016129
optimization makes the folowing changes in	tensor local mul switch sink	0.045455
this compiles the source code for this linker	cmodule location	0.038462
standard deviation along	tensor std	0.111111
bilinear upsampling	bilinear	0.038462
series of wrapper functions instead of	linker many linkers wrappers	0.047619
encoding	nb_class dtype	0.200000
kernel for bilinear upsampling this function builds	bilinear	0.038462
that was used to split x	tensor split grad inputs g_outputs	0.333333
reorder the dimensions of	operators	0.017241
post	post	0.600000
return the constant scalar 0-d value	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
return	d3viz	0.142857
that	op	0.018349
to the fgraph outputs that will replace their	fgraph	0.012195
an apply_node recursively search	apply_node	0.050000
config string (comma-separated key=value components) into	parse config string config_string issue_warnings	0.166667
of this variable optionally inserting	tensor tensor py operators dimshuffle	0.019231
sum of non nan / inf	sum	0.038462
incsubtensor when we overwrite the full	tensor local useless inc subtensor node	0.066667
3d inputs with a set of 3d filters	tensor nnet conv3d input filters	0.142857
then replace it with a triangular solve	tag solve triangular	0.142857
converts samples from a uniform into sample from	from uniform	0.200000
to add	optimizer add	0.500000
called whenever node inputs[i] is changed from	gof feature on change input function_graph node i	0.333333
inverse the gradient	grad	0.010417
function tries to	top_shape border_mode	0.166667
helper function for grad function	core populate grad dict var_to_app_to_idx grad_dict wrt	1.000000
gives unique names to	names	0.047619
the code for our fgraph	clinker get dynamic module	0.200000
this is the equivalent of	to gpulocal opt	0.055556
step] transform it into a canonical	canonical	0.076923
message	msg	0.166667
convenience function to roll	tensor roll	0.250000
a ^ b inplace on a	tensor xor inplace a b	0.333333
this variable optionally inserting broadcasted dimensions	tensor tensor py operators dimshuffle	0.019231
the first outdim-1 dimension size s of	ndim outdim	0.142857
c code to declare variables	clinker type c declare	1.000000
in the forward but clip the	clip x lower_bound upper_bound	0.090909
compile c code when doing constant folding of	constant folding	0.142857
profilemode to print the mflops	mm flops inp outp	0.125000
diagonal	diag	0.023810
node by one which computes	node output_indices alloc_ops	0.142857
source code for this linker and	cmodule location	0.038462
make a schedule	schedule	0.125000
the graph leading to r to given depth	compile debugprint r prefix depth done	0.500000
indicating the version	cache version apply node	0.125000
elementary validations on the inner graph to	scan validate inner graph	0.035714
of _maker which handles much	debug mode function maker i o m	0.066667
the output	misc output	0.066667
that wasn't in the destroy_map	bad destroy	0.034483
return a c contiguous version of the input	gpu contiguous	0.083333
stack trace from one or more tensor variables	stack trace	0.055556
the dimensions of this variable optionally inserting	tensor	0.006431
modulo of m1 and the	m1	0.027027
math where x is an input vector and	node	0.007407
current paramstype contains the specified theano type	params type has type theano_type	0.500000
computes the product along the given axis	axis	0.025641
a new	clone with new inputs	0.166667
folowing changes in	local mul switch sink	0.045455
function to roll tensortypes along the given axis	tensor roll x shift axis	0.333333
the equivalent of localoptgroup for	opt group	0.043478
an operation to wait on a previously sent	mpisend wait	0.045455
true if the named	fullname	0.066667
is	graph	0.016393
inserting	operators dimshuffle	0.019231
a new node a clone in a new	clone	0.020833
of downsample with	downsample factor	0.500000
node from dict	d3viz dict	0.333333
of moved objects in six moves urllib_parse	module six moves urllib parse	0.333333
return connection	compile op from graph connection	0.500000
push	push	1.000000
mflops	tensor nnet conv op flops inputs outputs	0.125000
return a code string specific	node name	0.066667
in inputs that are	gof	0.002381
to get the 0	get depth	0.050000
six moves and	six	0.025000
that was used to	grad inputs g_outputs	0.076923
for same kinds	tensor tensor type	0.041667
left out in a prior reduction of x	x	0.008772
return the [elementwise] smallest	smallest	0.125000
list remove are still in the graph	remove fgraph replacements remove reason	0.055556
some special work if	gof	0.002381
type c type numpy typenum that corresponds to	tensor tensor type dtype specs	0.071429
this function performs the matrix inverse	matrix inverse	0.111111
of localoptgroup for graphtogpu	gpulocal opt group	0.055556
has only one	sitsot only	0.066667
1/(1+exp x -> sigm -x	tensor nnet local inv 1 plus exp	0.333333
destroyhandler class detects when	handler	0.071429
find broken optimizations	find	0.125000
determine the broadcast pattern for advancedsubtensor output variable	tensor adv index broadcastable pattern	0.066667
return a	name	0.066667
u and uses it instead of the var	o u	0.037037
enabled change all sigmoid	sigmoid node	0.100000
a badviewmap exception when it detects the	compile check viewmap node	0.111111
python object a that would be	gof	0.002381
of an empty matrix it does the	alloc	0.012500
unroll the batch	code unroll batch	0.166667
reorder the dimensions of this	operators	0.017241
compute 2d kernel for bilinear upsampling this	nnet bilinear	0.111111
the dimensions of	tensor tensor	0.014286
c code to initialize the variables	c	0.017857
false we can't change the value after the	config param init default	0.040000
compare true iff other is the	type eq other	0.250000
of integers indicating the version	c code cache version apply	0.125000
and only adds dimension to	local	0.014085
gpuincsubtensor	inplace setsubtensor	0.250000
argument for shared libraries	shared library arg	1.000000
feature to this function_graph	feature feature	0.250000
return a version of var transferred to	transfer var	0.100000
maps from	get equiv inputs	0.142857
infer the number of dimensions from	infer	0.083333
t -> dot y t x t	local lift transpose through dot node	0.333333
a >	gt	0.125000
log(softmax x and replace it with logsoftmax x	nnet local logsoftmax node	0.142857
the equivalent of	graph to gpulocal opt	0.055556
conv workmem	workmem workmem	1.000000
computes the output dimensions of convolving	tensor nnet conv op get output	0.047619
0 / x -> 0	local zero div	1.000000
shape tuple or	feature default infer shape	0.066667
l{codeblock} instances returns a string	code gen blocks	0.050000
step] transform it into a canonical form	get canonical form	0.045455
factor takes as input a	signal pool 2d input	0.090909
tensorconstant instances	tensor constant	0.055556
of r	r	0.028571
given a inner nit_sot output	inner sitsot	0.083333
for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x i0	0.083333
neib_shape	neib_shape	1.000000
the __unify_walk__ method for one	unify walk a b u	0.037037
type	clinker type c	0.083333
have a stack	stack	0.066667
r	r client_to_remove reason	0.200000
number of dimensions from the shape	shape	0.010204
and applies	local	0.014085
unused inputs	unused inputs	1.000000
return the url	content description filename auth	0.250000
the epoch of the last access of a	last access time	0.040000
if cond then ift else iff	switch cond ift iff	0.500000
value after the import of theano	default filter	0.040000
replacement if the ops in the	replace validate replace all	0.050000
flattened version of a	tensor flatnonzero a	0.200000
toposort return an ordering of	function graph toposort	0.125000
signature for this function	ext function method decl	0.333333
performs batch	gpuarray dnn batch	0.500000
variables and apply_nodes to this graph	function graph	0.040000
:func neibs2images <theano sandbox neighbours neibs2images>	nnet neibs2images neibs neib_shape original_shape mode	0.333333
unroll the batch size loop	code unroll batch	0.166667
replacement if the ops	validate replace	0.050000
can be a 1-d	random_state size a replace	0.333333
replace that with the *args directly	csm properties csm node	0.142857
instance associated with a particular	setitem item val	0.125000
pow	pow	0.833333
argmax	argmax node	0.500000
only used to determine the broadcast pattern for	tensor adv index broadcastable pattern a idx	0.066667
turned into macros for	cop get	0.033333
list to	list	0.133333
is inside a dimshuffle which only	node	0.007407
"lifts" dimshuffle through elemwise operations and merges	tensor local dimshuffle lift	0.250000
help the navigator deal	gof navigator optimizer attach updater fgraph	0.038462
by keeping the first outdim-1 dimension size s	ndim outdim	0.142857
when enabled change all sigmoid to ultra_fast_sigmoid	nnet local ultra fast sigmoid	0.200000
access of a	access time	0.200000
delete unversioned dynamic modules	gof module cache clear unversioned min_age	1.000000
batch normalization	batch normalization	0.125000
preambule	preambule	1.000000
this is the	graph to	0.055556
cuirfft	cuirfft	1.000000
for diagonalsubtensor and	get diagonal subtensor view x i0	0.083333
helper function	helper random_state n shape	0.500000
optimization makes the folowing changes	local mul switch sink	0.045455
an array	node inputs outputs	0.125000
randomstate instance associated with a particular stream	random streams setitem item val	0.142857
config string	core parse config string	0.333333
from a	from	0.050000
optionally inserting broadcasted	tensor tensor	0.014286
to the type's :attr	type	0.011905
source code	compile cmodule location	0.038462
delete keys in old format from the compiledir	gof cleanup	0.333333
hash equal for	tensor type hash	0.166667
traversal and chooses the orphans among them	and orphans	0.166667
or none for the	i_shapes	0.050000
of this variable optionally	tensor tensor	0.014286
important note this function uses	scan process node fgraph	0.142857
then ift else iff	ift iff	1.000000
equivalent of numpy ones_like	ones like model dtype	0.333333
this function returns val	val	0.125000
:param execute if true execute a theano function	misc execute execute verbose	0.250000
pack c types back into a pyobject	clinker type c sync	0.111111
is the equivalent of localoptgroup for	group	0.047619
a cache directory and return full path of	gof module name from dir	0.071429
the numeric shape	shape	0.010204
for convolving a mini-batch of a stack of	input_shape filter_shape	0.027778
stack trace for an	trace value f	1.000000
or more multinomial	tensor multinomial random_state	0.040000
that gets a scan op	op not_required inputs	0.071429
scalar 0-dimensional	core hessian	0.500000
return a code string specific	node name sub	0.111111
of thunk calls	callcount	0.142857
return a code string	name	0.022222
return connection pattern	compile op from graph connection pattern	0.076923
the end variables	grad wrt end	0.050000
pattern of subfgraph defined by inputs and	pattern node	0.125000
message on the	msg	0.083333
removes all from the clients list of	function graph remove client	0.200000
:param execute if true execute a theano function	misc execute execute verbose m	0.250000
unknown variables and apply_nodes to this graph	function graph	0.040000
the number of dimensions from the shape or	shape	0.010204
gradient updates for matrix solve operation c	solve grad	0.250000
compute sum	tensor tensor constant signature get sum	0.142857
gradient function should return	tensor matrix inverse grad inputs	0.500000
for	tensor constant	0.055556
return the function name to write	gpuarray write	0.200000
exception object with debug	raise with op node thunk	0.333333
a scalar	scalar	0.017857
special compound l{op} for the output	argmax1hot	0.058824
headers that	clinker headers	0.047619
data by walking	refresh age_thresh_use delete_if_problem cleanup	0.166667
fetch a compiled module from the loaded	get module name	0.333333
returns a short mostly hexadecimal hash	core hex digest x	0.083333
the topooptimizer from the input	in2out	0.043478
from the	from	0.050000
the diagonal of an empty matrix it does	diag	0.023810
return a symbolic row variable	tensor row name	0.050000
replace a crossentropysoftmax1hotwithbiasdx op whose incoming gradient is	useless crossentropy softmax 1hot with bias dx	0.111111
epoch of the last	last	0.076923
with respect to wrt, computes gradients	subgraph grad wrt	0.062500
a special compound	argmax1hot	0.058824
returns true if l	l	0.111111
gradient function should return	tensor matrix inverse r op inputs	0.500000
upgrade	upgrade	1.000000
the compilation of cutils_ext	gof compile cutils	0.166667
can't change the value after	default	0.030303
times	times	0.500000
return connection pattern of subfgraph defined by	from graph connection pattern node	0.076923
1-sigm x -> sigm -x	tensor nnet local 1msigmoid node	1.000000
into a canonical form that respects the	tensor get canonical form	0.045455
see theano tensor sum	tensor tensor py operators sum axis dtype keepdims	1.000000
that will	gof clinker type	0.066667
z <-	z	0.111111
the template filled by broadcasting value through it	tensor broadcast like value template fgraph	0.125000
sub	sub	0.666667
special compound l{op} for the output of	crossentropy softmax argmax1hot	0.083333
of ops contained within	gof ops	0.083333
debug counter to	debug counter	0.500000
vector and t is a	node	0.007407
replace_all_validate revert the replacement if	replace all validate	0.111111
wrapper around c_cleanup that	gof	0.002381
used to determine the broadcast pattern for	tensor adv index broadcastable pattern	0.066667
order a graph of apply nodes according to	gof sort apply nodes inputs outputs cmps	0.050000
dimensions of this variable optionally inserting broadcasted	tensor tensor py operators	0.015625
op a list of	op	0.009174
forward but clip the gradient	core grad clip x lower_bound upper_bound	0.250000
of l{codeblock} instances returns a string that	code gen blocks	0.050000
of	tensor py operators dimshuffle	0.019231
the __unify_walk__ method for one of	unify walk a b u	0.037037
start gradients	start	0.040000
convert x into a variable on the gpu	gpuarray as gpuarray variable x context_name	0.166667
of this	tensor py operators dimshuffle	0.019231
windows behavior that open windows	misc subprocess popen command	0.142857
dense n-dimensional 'meshgrid' with equally spaced points	nd grid	0.500000
this op scale or inverse the	scale	0.047619
the data field	csm data csm	0.333333
spatio-temporal filters with	filters	0.032258
to	gof	0.002381
multinomial distributions defined	tensor multinomial random_state	0.040000
filters with a movie	filters signals_shape filters_shape	0.333333
enabled change all sigmoid to ultra_fast_sigmoid	tensor nnet local ultra fast sigmoid	0.200000
the output dimensions	nnet conv op get output	0.047619
functions that	scalar composite init py impls	0.166667
change the value after the import of	default	0.030303
dot22 computing an outer-product -> ger	local dot22 to ger or gemv node	1.000000
the fgraph to break aliasing of	fgraph wrapped_inputs wrapped_outputs	0.111111
run	run	1.000000
add a	core add	0.500000
revert the replacement if the ops in the	replace validate replace	0.050000
the navigator deal	gof navigator optimizer attach	0.038462
equivalent of var shape[i], but apply if possible	i var i fgraph	0.200000
of localoptimizer	opt group	0.043478
lib directories that are needed by one	lib dirs	0.045455
if	gcc	0.023810
in profilemode to print the mflops	corr mm flops inp outp	0.125000
shortcut to an in instance	compile convert function	1.000000
item to six moves	move move	0.250000
a mini-batch	input_shape filter_shape	0.027778
baddestroymap	r_vals	0.090909
perform the permutation by doing a recursion over	permute row elements rec perform node x y	1.000000
a normal	base normal	0.500000
see theano tensor sort	tensor tensor py operators sort axis kind order	1.000000
return a code string specific to the	node name sub	0.111111
scalar values default int64	scalar	0.017857
the shape	feature set shape	0.333333
==	eq	0.222222
to the input specs and the output specs	input_specs output_specs accept_inplace	0.142857
to merge multiplication by a scalar on	alpha merge	0.076923
inserting 1 at the	tensor shape padaxis	0.333333
of two sets of pieces of vectors updating	sparse	0.019231
to even of x	x	0.008772
kinds of useless reshape	local useless reshape	0.200000
argmin	argmin axis keepdims	1.000000
to determine the broadcast	adv index broadcastable	0.050000
specified factor takes as input a n-d tensor	tensor signal pool 2d input ws ignore_border stride	0.100000
and the output specs	std fgraph input_specs output_specs accept_inplace	0.142857
bit width of python int c long int	core python int bitwidth	0.250000
i	tensor shape feature set shape i r i	0.500000
variable	tensor py operators	0.015625
cache or the	gof module cache	0.083333
variables	gof variables and	0.250000
to merge multiplication by a scalar on the	alpha merge	0.076923
block	block	0.833333
a mrg	mrg	0.076923
dict op -> total time icluding	compile profile stats compute total times	0.333333
other implementation of mod	mod	0.071429
feature should	gof feature	0.125000
total time icluding the time	compute total times	0.200000
op scale or inverse the gradient in	core grad scale	0.333333
thunk that operates	gof linker make thunk	0.045455
reorder the dimensions of x default reverse them	transpose x axes	0.200000
of this	py operators dimshuffle	0.019231
nit_sot output of scan return true iff	out scan	0.035714
conda offers to make itself	to os environ pathlist var	0.038462
object	object	0.416667
merge multiplication	gpuarray alpha merge	0.076923
with a set of 3d filters	tensor nnet conv3d input filters	0.142857
image	kernel_shape	0.142857
inserting broadcasted dimensions	dimshuffle	0.014493
a	mpirecv	0.037037
to declare variables	declare	0.142857
from a multinomial distribution defined	streams multinomial	0.076923
an optimization disabled by default that	node	0.007407
generate a diff to make code correctly indented	misc hooks get correct indentation diff code	0.333333
helper function to generate permutations from	permutation helper random_state n	0.333333
a version of var transferred to target	transfer var target	0.200000
compute conv output gradient w r	tensor nnet conv2d grad	0.333333
local optimization wants	local	0.014085
it into a canonical form that	canonical form	0.045455
the second half by b	b	0.014925
for scalar	scalar shared	0.083333
return c code to declare variables	c declare name sub	0.500000
can replace that with the *args directly	local csm properties csm	0.142857
corr3d	corr3d	0.833333
enum has	enum type has	0.500000
of shape tuple or	shape	0.010204
shape	feature set shape	0.333333
to find	compile find	0.333333
as python not the other implementation of mod	mod c code node name	0.125000
post some text to	misc post	0.200000
of 3d inputs with a set of 3d	nnet conv3d input	0.125000
baddestroymap	inputs node storage_map	0.166667
previously received	mpirecv	0.037037
an operation	mpisend	0.037037
shape in the	shape	0.010204
the dot product dot(x, y t) = z	sampling dot csr	0.500000
compare true iff other is the same kind	eq other	0.166667
:attr context_name	array type	0.055556
the value after the import	param init default filter	0.040000
input a 4-d tensor	pool 2d same size input patch_size	0.166667
writeme	metadict	1.000000
remove broadcastable dimensions from	tensor tensor py operators squeeze	0.200000
the dimensions of this variable optionally	py operators	0.015625
of	to	0.017544
reorder the dimensions of this variable	operators	0.017241
a new random stream	tensor random streams	0.142857
apply as many times	apply	0.016667
return c code to	name	0.011111
to print the mflops	flops inputs	0.125000
flags from config	flags	0.062500
of localoptgroup	group	0.047619
the convolution gradient with respect to the inputs	dnn conv grad	0.062500
:attr	gpu array	0.055556
copy	copy	1.000000
nodes in the original graph to a new	inputs outputs copy_inputs_and_orphans memo	0.029412
config string (comma-separated key=value components) into a	config string config_string issue_warnings	0.166667
reorder the	tensor tensor py operators	0.015625
replace element i of shape_of[r] by s_i	feature set shape i r i s_i	1.000000
inserting broadcasted dimensions	py operators	0.015625
hack in profiling to print the mflops	conv flops inp outp	0.125000
helper function for diagonalsubtensor	get diagonal subtensor view x i0 i1	0.083333
computes the output dimensions of convolving an image	get output	0.047619
support	support	1.000000
transform a subgraph whose output	gof local optimizer transform	1.000000
cache data by walking the cache directory	module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
to expm1	tensor local expm1	0.066667
3-d	tensor3	0.142857
the equivalent of	graph to gpulocal	0.055556
broadcasted	tensor py operators	0.015625
normal	normal	0.777778
is	tensor is	1.000000
mrg stream state and they are spaced	mrg random streams	0.033333
fgraph outputs	fgraph	0.012195
an unification in u and uses it instead	o u	0.037037
computes the output dimensions of convolving an	get output	0.047619
converts self _rop_op from user supplied form	op from graph recompute rop op	0.200000
offers to	to os environ	0.038462
a optimizer which can be referred to by	compile register optimizer	0.500000
types of x y	x y	0.048780
make an inplace optimization that deals with allocempty	gpuarray inplace allocempty op idx	0.166667
x -> x	tensor local tensor	1.000000
l has any duplicates (according	scan_module has duplicates l	0.111111
duplicate this apply instance in a new graph	gof apply clone with new	0.250000
gradients up to the end variables of	end	0.040000
inputs required	gof inputs variable_list blockers	0.058824
module file	gof key data	0.333333
on the	as gpuarray	0.333333
explicitly upcasts constant inputs	constant inputs	0.125000
with respect to wrt, computes	subgraph grad wrt	0.062500
to the gpu to start the rolling wave	to gpu optimizer	1.000000
broadcasted dimensions	operators	0.017241
important note this function uses set and dictionary	scan process node	0.142857
the c code for corrmm (direction="forward"),	corr mm c code	0.090909
type numpy typenum that	tensor type dtype specs	0.071429
convolution implementation by sparse matrix	sparse sandbox convolve kerns kshp nkern images	0.500000
new	with new	0.166667
new graph	clone with new inputs inputs	0.166667
numpy typenum	dtype specs	1.000000
converts number to string	char from number number	0.142857
a new variable	variable	0.022222
post some text	misc post	0.200000
a tensor of type dtype	dtype	0.022727
class is a wrapper for	op	0.009174
the original graph to a new	outputs copy_inputs_and_orphans memo	0.029412
compute the numeric shape of	tensor shape	0.058824
function to	random_state low	0.500000
remove two kinds of useless reshape	tensor local useless reshape node	0.200000
navigator deal	gof navigator	0.038462
return	gof remove	0.250000
macros for use	cop	0.028571
dict op -> total time on thunks	compile profile stats class time	1.000000
arccosine of a (inplace on a)	tensor arccos inplace a	1.000000
log2	log2	0.714286
if this local optimization wants to	gof local optimizer	1.000000
broadcast them	generate broadcasting	0.066667
random stream	random streams	0.176471
a symbolic row variable (ndim=2	row	0.034483
not the same on all device we do	device node	0.045455
raise baddestroymap	check inputs node storage_map	0.166667
kinds of useless reshape	useless reshape	0.200000
from r to	r	0.028571
inserting broadcasted dimensions	operators dimshuffle	0.019231
the output after pad_dims	unpad dims output input	0.333333
type c type numpy typenum that	type dtype specs	0.071429
pretty multiline string representating the cause	bad optimization str diagnostic	0.043478
a special compound l{op} for the output	softmax argmax1hot	0.083333
to use complex numbers	tensor mod check x y	0.166667
not required anymore and should be removed and	compress outs	0.076923
baddestroymap	compile check inputs	0.166667
sample from a uniform	tensor uniform random_state size	0.125000
revert the replacement	gof replace validate replace	0.050000
the end variables of a	grad wrt end	0.050000
x_copy	x_copy	1.000000
for same	tensor tensor type	0.041667
wrapper around c_cleanup that decrefs py_name	gof get c cleanup r	0.250000
try	compiler try	0.250000
number to string by rendering it in	number number	0.125000
output error and	output	0.017241
reorder the dimensions	tensor py operators	0.015625
moved objects in six moves urllib_parse	module six moves urllib parse	0.333333
gist and return the	gist	0.040000
the input	gof	0.002381
compute sum of non nan	constant signature get sum	0.142857
the type's :attr	gpu array type	0.062500
a list of nodes that	gof	0.002381
for corrmm	corr mm	0.250000
baddestroymap	check inputs node	0.166667
moved objects in six moves	module six moves urllib	0.181818
specifyshape how to	c_support_code_apply	0.111111
repeat	repeat repeats axis	1.000000
allows replacing subgraphs of a computational	clone output replace strict share_inputs	0.071429
dict that	gof	0.004762
incsubtensor when we overwrite the full inputs	local useless inc subtensor node	0.066667
it with logsoftmax x 's grad	nnet local logsoftmax grad node	0.200000
reshape for gpu variables	gpu reshape	1.000000
as replace_all_validate revert	validate remove fgraph	0.166667
convert python	make	0.017857
variable with the -x pattern	tensor nnet is neg var	0.166667
used to determine the broadcast pattern for advancedsubtensor	tensor adv index broadcastable pattern	0.066667
the stack trace from	gof copy stack trace	0.055556
reorder the dimensions	py operators	0.015625
for the output of neural-net classifiers	with bias	0.166667
will call the supplied function as its implementation	as	0.024390
input	input input	0.333333
for gpuincsubtensor	local inplace setsubtensor node	0.250000
junk	junk	1.000000
other implementation of mod	mod c code	0.125000
computes the output dimensions of convolving	conv op get output	0.047619
output gradient w	grad	0.031250
for	tensor tensor	0.014286
false we can't change the value after	default filter	0.040000
some functiongraph listeners to help the navigator deal	navigator optimizer attach updater	0.038462
platform platform()	core short platform r p	0.142857
"code_cleanup"	cleanup	0.125000
the list of policies	policy policy	0.125000
return the shape field	csm shape csm	0.333333
return the platform-dependent extension for compiled modules	get lib extension	0.333333
-> softmax_w_bias matrix bias	nnet local softmax with bias node	0.200000
batch normalization	batch norm train	1.000000
drop broadcastable dimensions scrap the dimshuffle and index	tensor local dimshuffle	0.052632
dimensions from the shape	shape	0.010204
important note this function uses	seq scan process node fgraph	0.142857
register a context	context	0.035714
return none or a tensorvariable whose type	as	0.024390
a canonical form	tensor get canonical form	0.045455
class returns the bartlett spectral window in	tensor bartlett m	0.083333
the node i pairs such that	gof function graph clients	0.100000
prod(prod()) -> single prod()	local op of op	0.500000
insert inplace versions of remove0	sparse local inplace remove0 node	0.333333
a series of	linker many linkers wrappers	0.047619
with logsoftmax x	tensor nnet local logsoftmax node	0.142857
implements the "reverse-mode" gradient for the eigensystem	eigh grad perform node inputs outputs	0.333333
manipulate the subgraph in functiongraph	function graph replace r new_r reason verbose	0.250000
with respect to wrt, computes gradients of	subgraph grad wrt	0.062500
should remove any dynamically added functionality	listener on detach fgraph	1.000000
or none	i_shapes	0.050000
connection pattern	compile op from graph connection pattern node	0.076923
bilinear upsampling this function builds	bilinear	0.038462
proxy for either true_div	div proxy	0.125000
gradient w	grad	0.031250
dict op -> total number	compile profile stats class	0.250000
scalar 0-dimensional variable	hessian	0.142857
for	abstract	0.111111
of useless	tensor local useless	0.111111
es of a tensor	ddof keepdims	0.250000
filters with	filters	0.032258
the provided l{functiongraph} it may	gof optimizer apply fgraph	0.200000
python object a that	gof	0.002381
mod	mod c code node name inputs outputs	0.125000
given graph contains a cycle parameters	gof contains cycle fgraph	0.333333
file_path	file_path	1.000000
the grad of this op could be very	op	0.009174
calls subprocess_popen returning the output	output	0.017241
headers that are needed by one or	headers	0.038462
convolution gradient with respect to the	dnn conv grad w	0.125000
the type's :attr	gpuarray	0.023256
important note this	push out seq scan process node fgraph	0.142857
b), axis=0) -> elemwise{scalar op} a b	tensor local reduce join	0.111111
can be considered approximately equal	type values eq approx	1.000000
the output	get output	0.047619
or more multinomial distributions defined by one-dimensional	multinomial random_state	0.040000
dimensions of this variable optionally inserting broadcasted	tensor py	0.015873
particular	setitem item val	0.125000
replace a leaf	nnet replace leaf	0.100000
a nested loop over several arrays and associate	loop	0.027778
similar behaviour as python's	fn sequences non_sequences truncate_gradient	1.000000
libs_dir	libs_dir	1.000000
converts self _rop_op from user supplied form	from graph recompute rop	0.200000
return connection pattern of	op from graph connection pattern node	0.076923
a warning message on the first	gof deprecated filename msg	0.041667
lib	lib	0.750000
bound by the inputs and	init inputs	0.083333
navigator deal with	navigator optimizer attach updater	0.038462
return a function that	function	0.052632
task	cthunk find task	0.142857
like make_thunk() but only makes python thunks	storage_map compute_map no_recycling	1.000000
is an input vector and	node input_storage	0.038462
unroll the	gen conv code unroll	0.250000
lib directories that are needed by	clinker lib dirs	0.055556
the	gcc	0.047619
__unify_walk__ method for one of	gof unify walk a b u	0.037037
given an apply_node recursively search from	import apply_node check	0.066667
variable optionally inserting broadcasted dimensions	tensor tensor	0.014286
x	tensor flatten x	0.166667
otherwise call upgrade_to_float()	scalar upgrade to float no	1.000000
that runs a	gof wrap	0.083333
epoch of the last access	last access	0.040000
return connection pattern of subfgraph	op from graph connection pattern	0.076923
the value after the import	core config param init default filter	0.040000
the inverse of a	inverse	0.066667
this variable optionally inserting	dimshuffle	0.014493
created if	gof	0.002381
gemm acting on row or column	gemm to	1.000000
matrix of	matrix	0.055556
the source code for this linker	cmodule location	0.038462
this function is basically a call	tensor extract constant x elemwise only_process_constants	0.058824
complex-valued tensor from	complex from	0.250000
kernel for bilinear	bilinear	0.038462
convert addsd to	local addsd ccode	0.250000
the grad of this op could	op	0.009174
matrix a	structured	0.071429
cross-entropy between an approximating distribution and	tensor nnet categorical crossentropy coding_dist true_dist	0.111111
function on the inputs and put the variables	pure op perform node inputs output_storage params	0.047619
extract a list of compilation flags from	libs flags libs_dir include_dir	0.052632
important note this function uses set and	out seq scan process node	0.142857
that removes all asserts from the	remove all assert	0.055556
variable optionally	tensor tensor	0.014286
keep_lock	keep_lock	1.000000
constant that is cached	cached constant error	0.200000
output gradient w	conv3d grad	0.111111
compilation flags from config blas ldflags	ldflags libs flags libs_dir include_dir	0.333333
module from the cache	cache module from	0.333333
change the value after the import	core config param init default filter	0.040000
upper triangle of an array	tensor triu m k	0.250000
bessel function	j0	0.166667
a mrg stream state and	mrg random streams	0.033333
implements the "reverse-mode" gradient	grad perform	0.166667
this variable optionally inserting broadcasted dimensions	operators dimshuffle	0.019231
log(softmax x and replace it with logsoftmax x	nnet local logsoftmax	0.076923
and argmax over a given axis or over	and argmax	0.166667
self _grad_op from user supplied form to	from graph recompute grad	0.200000
is the equivalent of	to	0.017544
compute bilinear	nnet bilinear	0.111111
fgraph	fgraph outputs_to_disown	0.047619
exception value has a __thunk_trace__	thunk hook type value trace	1.000000
return indices over each shape	indices	0.076923
return true for small or builtin	is simple	0.200000
elements obtained by iterating	keepdims	0.052632
r operation on f wrt to wrt	f wrt	0.100000
maximum in	tensor maximum	0.142857
a list of shape tuple	tensor shape feature default infer shape	0.066667
x x ->	tensor local	0.025641
return c code to declare variables that will	gof clinker type c declare name sub	0.333333
incsubtensor x zeros idx -> x	local incsubtensor of zeros node	1.000000
first half of v by a with a	v	0.011111
any duplicates (according	duplicates	0.125000
the c code for corr3dmm	corr3d mm c code	0.090909
to the idx list to get the	tensor get idx list	0.076923
depending on types of x	x	0.017544
of x the	x	0.008772
given an apply_node recursively search	import apply_node check reason	0.066667
diagonalsubtensor	nnet get diagonal subtensor view	0.083333
a variable on	as gpuarray variable	0.166667
importer to import	importer	0.125000
of dimensions from the shape or the	shape	0.010204
elementwise [floor] division inverse of	int div a b	0.333333
sample from a uniform distribution	uniform random_state	0.125000
profilemode to print the mflops	corr3d mm flops inp outp	0.125000
subtensor and its idx_list reorders the inputs according	inputs idx_list get_count	0.100000
a new node a clone in a	clone	0.020833
load a file	load	0.083333
inner_task	inner_task	1.000000
important note this function uses set and	scan process node fgraph	0.142857
reproducible case for problems during theano compilation	compile function dump filename inputs	0.166667
a class for node-based optimizations	local optimizer	0.333333
is needed as some features introduce instance methods	function graph getstate	0.500000
base op for cudnn batch	gpu dnn batch	0.333333
this function compute the output	nnet	0.016129
occurrences of each value in array of ints	x weights minlength assert_nonneg	0.125000
to use with	gpuarray gpu inc subtensor	0.333333
to print the mflops	base gpu corr3d mm flops inp outp	0.125000
this optimization makes the folowing changes in the	tensor local mul switch sink	0.045455
pieces of vectors	sparse block outer make node o x y	0.066667
gradient	grad w	1.000000
for the shape element	tensor shape feature	1.000000
dimensions of this variable optionally inserting broadcasted	tensor tensor py operators dimshuffle	0.019231
det x and there is already	linalg local det	0.166667
performs the matrix inverse on	matrix inverse a	0.200000
is an alloc of a scalar variable or	alloc node	0.037037
deprecated old conv2d interface	nnet conv2d input filters image_shape filter_shape	0.500000
the value after the import of theano	init default	0.040000
return the complex conjugate of	tensor conj	0.250000
a dimshuffle which only adds dimension	dimshuffle	0.014493
fill inputted tensor with the assigned	operators fill	1.000000
remove rebroadcast if id does not actually change	useless rebroadcast node	1.000000
of apply nodes according to a list	apply nodes inputs outputs cmps	0.050000
convolve spatio-temporal filters with a	conv3d signals filters	0.111111
return a function that will calculate	compile orig function	0.166667
the context object mapped to the	context	0.035714
ops contained within	gof ops	0.083333
corrected	corrected	1.000000
of scan return true	push out scan	0.050000
conv output gradient w	conv3d grad	0.111111
gradient wrt inputs for gpucorr3dmm	gpu corr3d mm grad inputs	1.000000
one	one	0.461538
return a rows	rows	0.125000
y + alpha * dot a x	gemv c code y a x	0.500000
the supplied function as	as	0.024390
if necessary update dr_vals	compile check inputs node storage_map r_vals dr_vals	0.250000
add the tensor	tensor	0.006431
of convolution gradinputs	get conv gradinputs	0.500000
for the maximum in one tensor	tensor maximum x y	0.090909
in profiling to print the mflops	tensor nnet conv op flops	0.125000
as replace_all_validate revert the replacement if the	replace validate replace all validate	0.111111
function for diagonalsubtensor	get diagonal subtensor view x	0.083333
if it takes the full input in the	node	0.007407
function tries to	image_shape top_shape border_mode subsample	0.166667
sample from one or more multinomial distributions	tensor multinomial random_state size n	0.333333
convert addsd to	addsd ccode node	0.250000
change the value after the import of theano	default filter	0.040000
gradients along the axis that was used to	inputs g_outputs	0.090909
the original graph	inputs outputs copy_inputs_and_orphans memo	0.029412
the flattened version of	tensor flatnonzero	0.166667
"code_cleanup" together	cleanup	0.125000
for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x i0 i1	0.083333
x -> x	local tensor	1.000000
replacement if the ops in	replace	0.032258
dot product	true dot	0.166667
name in	name opt	0.333333
instances of suitable dummy values	local meta optimizer provide	0.200000
apply_node recursively search from this	apply_node	0.050000
expm1	local expm1 node	0.066667
variable and its substitute take different runtime values	bad optimization	0.333333
memory alias that wasn't in	bad view	0.027027
the dimensions of this	py	0.014286
the "reverse-mode" gradient for the eigensystem	eigh grad perform node inputs	0.333333
a max	tensor local max	0.250000
insert	insert	1.000000
construct a variable with a	variable x name	0.083333
call the supplied function as its implementation	as	0.024390
a vector	make vector	0.125000
removes all	remove all	0.166667
broadcasted	py operators dimshuffle	0.019231
r_op overrides see help theano opfromgraph for syntax	rop overrides rop_overrides	1.000000
conventions imposed by python and	theslice length	0.052632
l has any duplicates (according	has duplicates l	0.111111
gives unique names to an	names	0.047619
to the type's :attr	gpu array type	0.062500
exception some perform() or c_code() modified	destroy map	0.142857
which will print a warning	gof deprecated filename	0.166667
change the value after the import	default filter	0.040000
last access	last access	0.040000
where x is an input vector and t	node	0.007407
print the mflops	nnet conv op flops inputs	0.125000
is a package	six meta path importer is package	0.500000
lower triangle of	tensor tril m k	0.250000
the r-operator for	gpu max pool rop	0.333333
using magma library	gpu magma	0.142857
bincount	bincount	1.000000
for openblas threads interface	tensor openblas threads text	0.250000
connection pattern of subfgraph defined	connection pattern node	0.076923
module is a	is	0.066667
a slice [start stop step] transform it into	slice	0.038462
to get the 0 based level of the	type get	0.050000
see theano tensor nonzero_values	tensor tensor py operators nonzero values	1.000000
the gradient the finite fourier	tensor fourier grad	0.250000
set the values of a shared variable to	shared variable	0.071429
corrmm_gradweights (direction="backprop weights"), and	helper bottom weights top direction	0.055556
to the type's :attr context_name	gpu array type	0.062500
context_name	gpuarray	0.023256
the inverse fast fourier transform with real-valued output	irfft inp norm is_odd	0.500000
incsubtensor when we overwrite the full inputs	tensor local useless inc subtensor node	0.066667
if true	verbose m	1.000000
the folowing changes in the	tensor local mul switch sink node	0.045455
simplify	tensor nnet simplify	0.500000
this function performs the svd on	tensor svd a	0.200000
a variable on the gpu	gpuarray as gpuarray variable	0.166667
create a new random stream	random streams	0.058824
return the function name to load	gpuarray load	0.200000
validations on the inner graph to ensure that	scan_module scan validate inner graph	0.035714
profiling to print the mflops	conv op flops inputs	0.125000
runs a series of	many linkers wrappers	0.047619
dir	dir	0.384615
implement x[ilist] where ilist is a vector	advanced subtensor1	0.200000
symbol	symbol	0.750000
or a tensorvariable whose type	as	0.024390
given vectors containing actual observations and predicted observations	actual pred	1.000000
diagonalsubtensor and	nnet get diagonal subtensor view x i0 i1	0.083333
revert the replacement	validate replace all	0.050000
given an apply_node recursively search from this	apply_node	0.050000
tuple of integers indicating the version	object c code cache version	0.125000
of scan to outside of scan	out scan output	0.125000
output of scan	push out scan	0.050000
gradient w	conv2d grad	0.111111
spatio-temporal filters with a	signals filters	0.111111
matrix it does	alloc	0.012500
s_i	s_i	1.000000
replaced by their	allow_partial only_process_constants elemwise	0.166667
reorder the dimensions of this	py operators	0.015625
python	tensor make constant	0.100000
set to a specified	a	0.008065
in the cache and	cache	0.034483
approx	approx	0.833333
roll tensortypes along the given axis	roll x shift axis	0.333333
an apply_node recursively search from this node to	import apply_node check	0.066667
scalars together into a vector	vector	0.066667
a module from the	module from	0.166667
return a version of var transferred to target	transfer var target	0.200000
folowing changes in the graph t	local mul switch sink	0.045455
cost scalar 0-dimensional variable	core hessian cost	0.500000
to determine the broadcast pattern for	tensor adv index broadcastable pattern	0.066667
if target is 'cpu' this will	target	0.125000
gof graph	graph	0.016393
of 3d filters	nnet conv3d input filters	0.142857
type topooptimizer	topo db	0.166667
that with the *args directly	csm properties csm node	0.142857
the stack trace from one or	gof copy stack trace	0.055556
a hash	tensor hash	0.333333
that allows replacing subgraphs	compile rebuild collect shared outputs inputs	0.500000
headers that are needed	clinker headers	0.047619
listeners to help the navigator deal	navigator optimizer	0.037037
the inputs required	inputs variable_list blockers	0.058824
batch normalization of the given	tensor nnet batch normalization	0.125000
from a list of l{codeblock} instances returns	code gen blocks	0.050000
around windows behavior that open windows	misc subprocess popen command	0.142857
wraplinker that runs	gof wrap	0.083333
conv3d with respect	conv grad3d	0.333333
valid	valid	1.000000
perform the permutation by doing a	perform node x y inverse	0.166667
it has a 'requirement' of the destroyhandler	destroy handler	0.055556
inputs required to compute the given variables	inputs variable_list blockers	0.058824
variable optionally inserting broadcasted dimensions	py	0.014286
all sigmoid to ultra_fast_sigmoid	nnet local ultra fast sigmoid	0.200000
kinds of useless	useless	0.076923
a variable of this	a	0.008065
determine the	tensor adv index broadcastable	0.050000
grad of this op	tensor prod l op	0.033333
convolution with the specified parameters	conv get	0.100000
class to raise	raise	0.076923
the dimensions of this	tensor tensor py	0.015873
optionally inserting	tensor tensor	0.014286
output shape for a convolution with the	gpu dnn conv get out shape	0.142857
when enabled change all sigmoid to ultra_fast_sigmoid	nnet local ultra fast sigmoid node	0.200000
nodes such that	gof	0.002381
source	clinker compile cmodule location	0.038462
is the main interface to manipulate	r new_r reason verbose	0.071429
functiongraph listeners to help the navigator	navigator	0.032258
and "init_code"	code struct node	0.500000
the values of a shared variable to	shared variable	0.071429
l{codeblock} instances returns a string that	gof code gen blocks	0.050000
conv output gradient w r t its weights	conv3d grad wrt weights input output_grad filter_shape input_shape	0.333333
function	random_state a	0.500000
inserting broadcasted	py operators	0.015625
x ->	local	0.028169
optimization makes the folowing changes in	mul switch sink node	0.045455
ordering of the graph's apply nodes such that	gof	0.002381
tag trace to an	tag trace thing user_line	0.166667
replace	nnet replace	0.250000
source code	cmodule location	0.038462
to generate c	compile register shape i c	0.250000
this optimization makes the folowing changes	local mul switch sink	0.045455
as replace_all_validate revert the replacement if the ops	gof replace validate replace all validate remove fgraph	0.111111
of 2d filters	nnet conv2d input filters	0.125000
is an input vector and t is a	node	0.007407
this is meant as a shortcut to opt	gof optimizer optimize fgraph	0.200000
real and	real	0.125000
variable	variable	0.355556
c-implementation of the	csr c code node name	0.333333
argmin	argmin axis	1.000000
value in array of ints	tensor bincount x weights minlength assert_nonneg	0.125000
the navigator	navigator optimizer attach	0.038462
validations on the inner graph to	inner graph	0.035714
or 3d convolution for debugmode	tensor nnet base abstract conv conv	0.125000
variable optionally inserting	py	0.014286
allows replacing subgraphs of a computational graph	scan_module clone output replace strict share_inputs	0.071429
structures	out non seq scan	0.125000
input a 4-d tensor it	size input patch_size	0.166667
raise	compile check inputs	0.166667
remove incsubtensor when we overwrite the	tensor local useless inc subtensor node	0.066667
the first functiongraph that has ever been	gof	0.002381
node by one which computes	node output_indices	0.142857
the diagonal of	alloc diag	0.027027
add a new variable to theano config	add config var name doc configparam root	1.000000
replaces	sub	0.111111
execute	graph execute	1.000000
chi squared survival function	tensor chi2sf x k	1.000000
the inputs and put the variables	gof pure op perform node inputs output_storage params	0.047619
track matrix properties	hints	1.000000
we can't change the value after the import	core config param init default filter	0.040000
|a| tensorvariable overloads the tensorvariable	tensor abs a	0.333333
raises a badviewmap exception when it detects the	check viewmap node storage_map	0.111111
used to determine the broadcast pattern	adv index broadcastable pattern a idx	0.066667
a with	a	0.016129
y + alpha * dot	gemv c code y	0.333333
grad of this op could	tensor prod l op	0.033333
matrix by a broadcasted dense vector element wise	sdcsr	0.250000
module	module	0.266667
clients2	clients2	1.000000
mod	scalar mod c code node	0.125000
convolution gradient with respect to the weights	dnn conv grad w	0.125000
to the end	wrt end	0.050000
first half of v by	v	0.011111
sum along	sum	0.038462
template filled by broadcasting value	tensor broadcast like value template fgraph	0.125000
filename being considered used in diff generation only	filename	0.166667
the stack trace from one	copy stack trace	0.055556
execute callbacks calls getattr feature name (*args) for	execute callbacks name	0.500000
use within the op code	op params	0.100000
axis that was used to	grad inputs g_outputs	0.076923
the scan that depend only on non-sequences	non seq scan	0.090909
connection pattern of subfgraph defined by	op from graph connection pattern node	0.076923
return permutations	permutation random_state size n	0.500000
respects the conventions imposed by	theslice length	0.052632
to replace a	replace	0.032258
replace excepthook and do some special work if	gof	0.002381
raise a error if cudnn can't be used	gpuarray no cu dnnraise apply fgraph	0.200000
fill s v -> alloc(v shape s	local fill to	0.250000
version	clinker object c code cache version	0.125000
replace_all_validate revert the replacement if	replace validate replace all validate remove	0.111111
from a uniform	uniform random_state size	0.125000
a with a modulo of m1 and the	m1	0.027027
by the corresponding element of a dense vector	x s	0.142857
more multinomial distributions	tensor multinomial	0.037037
return a new variable instance of type self	gof pure type call name	1.000000
a uniform into sample	uniform	0.086957
enabled change all sigmoid to	sigmoid	0.055556
see theano tensor argsort	tensor tensor py operators argsort	1.000000
output of scan return	push out scan	0.050000
inverse of	inverse	0.066667
see min for the minimum in	tensor minimum x	0.142857
client	client	1.000000
counter	counter	0.857143
the source code for this linker and	cmodule location	0.038462
first outdim-1 dimension size s of	ndim outdim	0.142857
fill s v -> alloc(v shape s this	local fill to alloc node	0.250000
abs toward the input	tensor local abs lift node	0.333333
compute the dot product	nnet	0.032258
gradients of cost and/or	cost	0.045455
convolve spatio-temporal filters	nnet conv3d signals filters	0.111111
matrix inverse	matrix inverse	0.111111
function for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x i0	0.083333
see theano tensor argmin	py operators argmin	1.000000
inside the scan that depend only on	seq scan	0.250000
baddestroymap	check	0.083333
a symbolic row variable (ndim=2 broadcastable=[true	tensor row	0.050000
the same	feature same	0.333333
pattern	pattern a idx	0.066667
type	type c code	1.000000
for input of given shape and flags	pool grad out shape imgshape ws ignore_border stride	0.200000
for a constant that	gof	0.002381
for corr3dmm (direction="forward"), corr3dmm_gradweights	corr3d mm	0.111111
mode	monitor mode	0.333333
max and argmax over a	max and argmax	0.125000
and figure out which nodes	scan_module traverse out x x_copy d	0.047619
gradient w	conv3d grad	0.111111
the matrix inverse on gpu	gpuarray gpu matrix inverse a	0.200000
transferred	tensor transfer	0.500000
the c code for this	c code	0.055556
function to get the 0 based	type get	0.050000
to use dnn conv workmem	no dnn workmem workmem	0.166667
the dimshuffle and index	tensor local dimshuffle	0.052632
failure_callback	navigator optimizer warn inplace exc	0.500000
open	open	1.000000
string representation of broadcastable	d3viz broadcastable to str b	0.500000
used to determine the broadcast pattern	adv index broadcastable pattern	0.066667
we can't change the value after	core config param init default	0.040000
symbolic type representing a numpy ndarray value	type	0.011905
merge some gpucareducecuda and gpuelemwise	gpuarray local gpu elemwise careduce node	1.000000
conda offers to make itself the	to	0.017544
extract test value from	get test value	1.000000
important note this function	scan_module push out seq scan process node	0.142857
the mean value along the	tensor mean	0.111111
if allow_override is false we	filter allow_override	0.142857
functiongraph that has ever	gof	0.002381
schedule function from comparators	schedule fn	0.333333
header for openblas threads	tensor openblas threads	0.250000
shape of convolution gradweights	conv gradweights shape	0.333333
self _grad_op from user supplied form to type	op from graph recompute grad	0.200000
input	gof	0.002381
input a	same size input	0.500000
c code to pack c types back into	clinker type c sync	0.111111
only one client and	only	0.050000
ordereddict the list of outputs and	and outputs	0.100000
the dependence of nodes	gof make dependence	0.043478
dot product dot(x, y t) = z	sampling dot	0.500000
must return a thunk that is a zero-arguments	thunk	0.021277
is associated to it	failure_code	0.125000
canonical form that	get canonical form	0.045455
a version of var transferred	transfer var	0.100000
a schedule	sort schedule	0.333333
vector variable	tensor vector	0.500000
sum(a axis=[])	useless reduce	1.000000
list of lib directories	clinker header dirs	0.055556
function for	fn	0.083333
and "init_code"	code struct node name	0.500000
a shared	compile shared	0.166667
canonical	canonical	0.461538
clear all elements	clear unversioned_min_age clear_base_files delete_if_problem	1.000000
make a nested loop over	tensor make loop	0.200000
the source code for this linker and returns	clinker compile cmodule location	0.038462
tree and figure out	scan_module traverse out x x_copy d	0.047619
kernel for bilinear upsampling this function builds the	bilinear	0.038462
optimization that deals with allocempty	allocempty op idx	0.250000
multinomial distributions defined by one-dimensional slices	multinomial random_state	0.040000
or more multinomial distributions defined by one-dimensional slices	tensor multinomial random_state	0.040000
has only one client	only	0.050000
a set of 2d filters	input filters	0.117647
dict op -> total time on thunks	compile profile stats op time	1.000000
shared variable names when persisting	persistent shared variable id	0.142857
list of headers	headers	0.038462
implement softmaxwithbias	softmax with bias	0.142857
:type expression vector 1-dimensional	core jacobian expression wrt consider_constant disconnected_inputs	0.500000
spatio-temporal filters	nnet conv3d signals filters	0.111111
replaced by their python scalar	allow_partial only_process_constants elemwise	0.166667
the navigator	gof navigator optimizer	0.038462
performs batch normalization of the	batch normalization	0.125000
filters	conv3d signals filters	0.111111
from a uniform into sample from	from uniform	0.200000
the context associated with a name	gpuarray get context name	0.333333
an alloc and only adds	tensor local alloc	0.111111
copies the stack trace from one or	stack trace	0.055556
a cache directory	dir	0.076923
removes useless dimshuffle	useless dimshuffle	0.500000
return a code string	node name	0.066667
row	row name dtype	0.050000
by a broadcasted dense vector element wise	sdcsr	0.250000
elemwise sinh of x	sparse sinh x	1.000000
a particular stream	random streams getitem item	0.142857
specified pieces of vectors	sparse block outer make node o x	0.066667
bitwise a ^ b inplace on a	tensor xor inplace a b	0.333333
type c type numpy typenum that	tensor type dtype specs	0.071429
returns indices of minimum elements obtained by iterating	keepdims	0.052632
specific to the apply to be inserted	apply	0.016667
baddestroymap if	check inputs node storage_map	0.166667
concatenate tensortypes	join	0.090909
compilation flags from config	libs flags libs_dir include_dir	0.052632
a special compound	crossentropy softmax argmax1hot	0.083333
function to	random_state n shape	0.500000
fgraph outputs	fgraph expanded_inputs	0.058824
default 1) times from a multinomial distribution defined	streams multinomial	0.076923
l{op} for the output of neural-net classifiers	with bias	0.166667
choose values from a with or without replacement	random streams base choice size a	0.333333
if current paramstype contains the specified theano type	type has type theano_type	0.500000
batch normalization of the	batch normalization	0.125000
batched	tensor batched	0.333333
apply	gof apply	0.090909
computes	ishape kshape	0.250000
functiongraph attach_feature the method that attaches	gof bookkeeper on attach	0.142857
:param execute if true execute a theano function	execute execute verbose	0.250000
toposort return	toposort	0.076923
the caller is replace_all_validate just raise the	gof validator validate fgraph	0.125000
reorder the dimensions of	tensor py operators dimshuffle	0.019231
computes the mean value along	tensor mean	0.111111
elemwise log(1 + x)	sparse log1p x	1.000000
the list remove are still in the	remove fgraph replacements remove reason	0.055556
see theano tensor argmax	py operators argmax	1.000000
the svd	svd	0.068966
unified to boundvariable(other_object)	unify walk fv o u	0.200000
op then replace it with a triangular solve	sandbox linalg tag solve triangular node	0.142857
return permutations	permutation random_state size	0.500000
or none for	i_shapes	0.050000
function is a thunk that operates on the	gof linker make thunk	0.045455
computes the confusion	confusion	0.125000
updates ordereddict the	updates	0.029412
orphans among them	and orphans	0.166667
use_list and	core format as use_list	1.000000
a specified factor takes as input	signal pool 2d input	0.090909
instance of _maker which handles much of the	function maker i o m	0.066667
a new random stream in this container	tensor random streams gen op	0.250000
batch normalization of the given	dnn batch normalization	0.125000
return a list of shape tuple	feature default infer shape	0.066667
iterable	gpuarray gpu kernel base gpu kernels	1.000000
alloc of 0	tensor local alloc	0.111111
the same computations	computations	0.125000
transfer	py operators transfer	0.125000
a uniform distribution	uniform random_state	0.125000
install some functiongraph listeners to help the navigator	navigator optimizer attach updater fgraph	0.038462
mod	mod	0.428571
dependence of	gof make dependence	0.043478
to parse the tree and figure out which	scan_module traverse out x x_copy d	0.047619
pieces of vectors and matrices	sparse block gemv make node o	0.066667
important note this function uses set and	seq scan process node	0.142857
optimization disabled by default that removes all	local remove all	0.166667
reproducible case for problems during	compile function dump filename	0.166667
a diagnosis if things go awry	gof function graph check integrity	0.250000
seed	seed	1.000000
scalar variable value from the tree at v	gpuarray grab cpu scalar v nd	1.000000
that are	gof	0.002381
some perform() or c_code() modified an input that	bad destroy map	0.142857
where x is an input vector and	node input_storage output_storage	0.038462
a <= b inplace on a	le inplace a b	0.500000
the list of policies to name r	policy policy r name	0.250000
return true for small or builtin c types	clinker type c is simple	0.250000
when one or all operands are	x y grad_preserves_dense	1.000000
this function copied function	compile function	0.250000
patch_size	patch_size	0.250000
a tensor input	input	0.071429
inner graph of scan to outside of scan	out scan output	0.125000
of headers that	headers	0.038462
fgraph check that 1) this destroyhandler wasn't	gof destroy handler	0.250000
>=	ge	0.142857
gradient wrt filters for gpucorr3dmm	gpu corr3d mm grad weights	1.000000
reduce pattern has functioning c	gpuarray gpu careduce cuda supports c	0.200000
some perform() or c_code() created	map	0.047619
simplify a multiplication tree	nnet simplify mul tree	1.000000
a new variable to	var name	0.250000
to draw random integers	random integers	0.500000
* y	c code y	0.333333
that broadcast them to match	tensor generate broadcasting	0.066667
performs batch normalization	dnn batch normalization	0.125000
list of compilation flags from config blas ldflags	tensor ldflags libs flags libs_dir include_dir	0.333333
toposort return an ordering of the graph's	function graph toposort	0.125000
add tag trace to an node or	add tag trace thing user_line	0.166667
the source code	cmodule location	0.038462
caller is replace_all_validate just raise the	gof validator validate fgraph	0.125000
tensor operators to the basic variable class	variable	0.022222
__unify_walk__ method for one of the	unify walk a b u	0.037037
for pushing out the	out	0.018519
fill s v -> alloc(v	tensor local fill	0.250000
insert deepcopy in the fgraph to break aliasing	insert deepcopy fgraph wrapped_inputs wrapped_outputs	1.000000
product of two sets of	sparse block	0.111111
helper function to	helper random_state	0.428571
node i pairs such that	gof function graph clients	0.100000
according to the idx list to get the	tensor get idx list	0.076923
helper function to draw random integers	tensor random integers helper random_state low high size	1.000000
offers to make itself the default python	to os environ pathlist var newpath	0.038462
gradient updates for matrix solve	tensor solve grad	0.250000
reshape t by	t	0.285714
infer_shape	infer_shape	1.000000
the convolution gradient with respect to	dnn conv grad w	0.125000
if	compile check	0.166667
size	size	0.384615
addition	out_in	0.166667
equivalent of	to gpulocal	0.055556
to the type's :attr context_name	gpu array	0.055556
shape[i] for tensor variable r int i	tensor shape feature shape ir i r	0.500000
module is a package	is package	0.500000
values of corresponding keys	walk d1 d2 u	0.333333
v by a with a modulo of	v	0.011111
is basically a	extract constant x elemwise only_process_constants	0.058824
a symbolic scalar variable	tensor scalar name dtype	0.166667
interface	text	0.166667
dimensions of this variable optionally	operators dimshuffle	0.019231
try to compile a	gof compiler try	0.250000
abs and rel error of gradient	grad abs rel	0.333333
"lifts" dimshuffle through elemwise operations and	dimshuffle lift node	0.250000
| b inplace on a	or inplace a b	0.333333
apply_node recursively search from this node to know	import apply_node check reason	0.066667
to	add to	0.142857
the mflops	gpu corr mm flops inp outp	0.125000
some functiongraph listeners to help the navigator deal	navigator optimizer attach updater fgraph	0.038462
on the output	output	0.017241
optionally inserting broadcasted	tensor py operators dimshuffle	0.019231
zero-arguments function that	gof	0.002381
raise baddestroymap if	check inputs node storage_map	0.166667
return connection	op from graph connection	0.500000
lib directories that are needed	clinker lib dirs	0.055556
the dot product	true dot	0.166667
op a list of indices indicating which outputs	op	0.009174
return the idx_list with constant inputs	get constant idx inputs	0.250000
their apply_node if those nodes	gof	0.002381
fetch a compiled module from the loaded cache	gof module cache get module	0.166667
confusion matrix of	tensor nnet confusion matrix	0.166667
in profiling to print the mflops	tensor nnet conv op flops inputs	0.125000
to the end variables of a	grad wrt end	0.050000
revert the replacement if the ops in the	gof replace validate replace	0.050000
to expm1	local expm1	0.066667
determine the broadcast pattern	tensor adv index broadcastable pattern	0.066667
this	gof cdata	1.000000
computes the specified outputs	fgraph	0.012195
to make itself	to os environ pathlist var	0.038462
sparse format	core sparse	0.066667
c	shape i c	0.250000
source code for this linker and returns a	compile cmodule location	0.038462
the same type	typed_list typed list type	0.333333
shape	feature shape	1.000000
gpu? currently	gpu data	0.500000
to convert x into a variable on the	gpuarray variable x context_name	0.166667
that decrefs py_name	gof get c cleanup r name	0.250000
return full	module name	0.062500
a number	make	0.017857
still in the graph	remove fgraph replacements	0.250000
is the	to gpulocal opt	0.055556
some perform() or c_code() created a	map	0.047619
important note	out seq scan process node fgraph node	0.142857
dictionary of arguments to	args	0.025641
a set of 2d filters	filters	0.064516
common	common	1.000000
matrix	structured	0.071429
for diagonalsubtensor and	nnet get diagonal subtensor view x i0	0.083333
given axis es of	axis dtype keepdims	0.083333
caller is replace_all_validate just raise the	validator validate fgraph	0.125000
toposort return an ordering of the graph's apply	graph toposort	0.125000
factor takes as input a n-d tensor	tensor signal pool 2d input ws ignore_border stride	0.100000
aliasing and destructive	destroy	0.009709
dimensions of	tensor py operators	0.015625
beta * y	code y	0.333333
the inputs according to the idx list to	idx list inputs	0.500000
existence of the __unify_walk__ method for one of	unify walk a b u	0.037037
return label of apply	apply label	0.500000
series of wrapper functions instead of just one	many linkers wrappers	0.047619
of this variable optionally inserting	tensor tensor py	0.015873
returns the connection pattern of a subgraph	io connection pattern	0.055556
from a with or without replacement	tensor random streams base choice size a	0.333333
to	to os environ pathlist var newpath	0.038462
functions that compute each output	scalar composite init py impls	0.166667
name r sub	r name sub	0.250000
real-valued input on the	curfft inp norm	0.066667
compute_uv	compute_uv	1.000000
from the loaded cache	module cache get	0.250000
a <	tensor lt a	1.000000
with constant inputs replaced by their	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
vector to the diagonal of an	diag	0.023810
(direction="forward"), gpucorrmm_gradweights (direction="backprop weights"), and	helper bottom weights top direction	0.055556
return c code to extract	c extract name	0.250000
compare true iff other is	eq other	0.166667
be removed from the	remove outs	0.500000
in this	function	0.052632
lib directories that are	header dirs	0.045455
a 4-d tensor it sets all	patch_size	0.050000
implementation of mod	mod	0.071429
parameters ----------	scan	0.017241
a list of nodes that must be evaluated	gof	0.002381
of two sets of pieces of	sparse	0.019231
hack in profiling to print the mflops	conv op flops inputs	0.125000
asserts from the	assert	0.111111
as replace_all_validate revert the replacement if the	replace all validate remove fgraph	0.111111
var has an unification in u	u	0.100000
a vector to the diagonal	diag	0.023810
self _grad_op from user supplied form to	compile op from graph recompute grad op	0.200000
removes all from	function graph remove client	0.200000
sparse	sparse true	1.000000
attempt to convert x into	x context_name	0.100000
unroll the batch	nnet gen conv code unroll batch	0.166667
slicing list of	subtensor1	0.333333
for use within the op code	get op	0.100000
output error	output	0.017241
see theano tensor argsort	tensor tensor py operators argsort axis kind	1.000000
the variables in inputs that	gof	0.002381
fct	fct	0.416667
the type's	gpu	0.011765
lock to	add to	0.142857
a context	reg context	0.333333
create a new random	tensor random	0.166667
this is just speed opt not for stability	fast scalar sigmoid	1.000000
required anymore and should be removed	compress outs	0.076923
print the mflops	tensor nnet conv op flops inputs outputs	0.125000
and see whom can be removed from the	can remove outs	0.250000
like zeros_like but forces the object	core float zeros like	0.200000
list of lib directories that are needed	clinker lib dirs	0.055556
types as strings check if converting to type2	type2	0.050000
or	or	0.625000
i1	i1	0.833333
in profilemode to print the mflops	base gpu corr3d mm flops inp outp	0.125000
defining the gradient the finite	grad	0.010417
prod	prod axis	1.000000
correspond to the one	one	0.076923
python type c type numpy typenum that corresponds	tensor type dtype specs	0.071429
context object mapped to the type's :attr context_name	context	0.035714
an input vector and t is	node input_storage output_storage	0.038462
detect if the g++ version used	gof gcc	0.027778
of	tensor tensor py operators dimshuffle	0.019231
context object mapped to the type's	gpuarray gpu array type context	0.090909
gradients up to the end	grad wrt end	0.050000
badviewmap exception when it detects the	check viewmap node	0.111111
determine the broadcast pattern for	tensor adv index broadcastable pattern	0.066667
modulo of m1 and	m1	0.027027
gradient wrt inputs for abstractconv3d	abstract conv3d grad inputs	1.000000
logsoftmax x 's grad	nnet local logsoftmax grad node	0.200000
a set of 3d	conv3d input	0.125000
to help the navigator deal with the	gof navigator optimizer attach updater	0.038462
structures	out non seq	0.125000
inputs with a set of 3d filters	tensor nnet conv3d input filters	0.142857
series of wrapper functions instead of just one	linker many linkers wrappers	0.047619
file_handler	file_handler	1.000000
a message	msg a	1.000000
graph optimizer for	optimizer	0.187500
implement the max and average pooling	pool	0.066667
the shape s to previously un-shaped variable r	set shape r s override	0.500000
fgraph and a list of	fgraph outputs_to_disown	0.047619
determine the broadcast pattern for advancedsubtensor	tensor adv index broadcastable pattern	0.066667
the epoch of the last access of	last access time path	0.040000
of integers indicating the version	c code cache version apply node	0.125000
randomstate instance associated with a particular stream	tensor random streams setitem item val	0.142857
useless reshape	tensor local useless reshape	0.200000
how to generate c	i c	0.250000
of x	x axes	0.200000
this op	gof	0.002381
specified pieces of vectors	sparse block gemv make node o w	0.066667
returns a module	gof module	0.058824
fn	fn	0.500000
the dimensionality of the var is equal	tensor is flat var	0.200000
return a list of shape tuple	shape feature default infer shape	0.066667
the inputs and	inputs	0.012658
is the main interface to manipulate the	r new_r reason verbose	0.071429
is the	graph to gpulocal	0.055556
diagonal of	diag	0.023810
makes the folowing changes in the graph t	tensor local mul switch sink	0.045455
pairs	pairs pairs	1.000000
shape or	shape	0.010204
helper function for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x i0 i1	0.083333
graphtogpu	graph to	0.055556
this variable optionally inserting broadcasted	py	0.014286
minimum see min for the minimum in	tensor minimum x	0.142857
variables var1 and var2	graph var1 var2	1.000000
generates the c code for corr3dmm (direction="forward"), corr3dmm_gradweights	corr3d mm c code	0.090909
to use	gpuarray gpu inc subtensor get	0.333333
grad of this op could be very	op	0.009174
of this variable	tensor py operators	0.015625
return connection pattern of subfgraph	compile op from graph connection pattern node	0.076923
the r operation on f wrt to wrt	f wrt	0.100000
cumsum	cumsum	1.000000
removes all from the clients list of r	gof function graph remove client r	1.000000
the compilation of cutils_ext	compile cutils	0.166667
tries to recognize the updates	updates	0.029412
stop step] transform it into a canonical form	canonical form	0.045455
duplicate this apply instance in a	gof apply clone	0.166667
input a 4-d tensor it sets all	2d same size input patch_size	0.166667
setsubtensor(x x[idx], idx) -> x when	local setsubtensor of constants node	0.250000
sparse matrix and a dense vector	svcsr	0.090909
module from the cache compiling it if necessary	cache module from key key lnk keep_lock	1.000000
op then replace it with a triangular solve	tag solve triangular node	0.142857
profiling to print the mflops	tensor nnet conv op flops inputs outputs	0.125000
listeners to help the navigator deal with	gof navigator optimizer attach	0.038462
equivalent of localoptgroup	to gpulocal opt group	0.055556
from a with or without replacement	random streams base choice size a replace p	0.333333
will calculate	compile orig	0.500000
connection pattern of subfgraph defined by inputs and	connection pattern	0.032258
add some requirements to the fgraph this	add requirements fgraph	0.333333
the	tensor py operators dimshuffle	0.019231
transfer	tensor py operators transfer	0.125000
shared	shared	0.562500
sample from one or more	size	0.076923
on the inner graph to ensure that	validate inner graph	0.035714
the inverse fast fourier transform with real-valued output	cuirfft inp norm is_odd	0.333333
of this variable optionally inserting broadcasted dimensions	operators	0.017241
see theano tensor std	tensor tensor py operators std	1.000000
the	py	0.014286
a series of wrapper functions instead of just	many linkers wrappers	0.047619
converts a function into a basic theano	itypes otypes infer_shape	0.142857
called on something that is not	not	0.076923
compute conv	nnet conv3d	0.142857
uses set and dictionary data structures	scan_module push out non seq	0.125000
this is	to	0.017544
variables and apply_nodes to this graph	gof function graph import	0.125000
subtensor and its idx_list reorders the	idx_list get_count	0.090909
need not be checked for nan and	compile is numeric value arr var	0.166667
the g++ version used is the	gcc	0.023810
which will print a warning message on	deprecated filename msg	0.041667
graph of apply nodes according to	apply nodes inputs outputs cmps	0.050000
c type numpy typenum that corresponds to	type dtype specs	0.071429
variables [v1 v2 v3 ]	vm linker compute gc dependencies variables	0.250000
compress	compress	1.000000
listeners to help the navigator deal	gof navigator optimizer	0.038462
optionally inserting	tensor tensor py operators dimshuffle	0.019231
output dimensions of convolving an image of	conv op get output	0.047619
this variable optionally inserting broadcasted dimensions	tensor py operators	0.015625
clinker	clinker	0.700000
kinds scalar constants and the rest	rest	0.076923
the navigator deal with the	navigator optimizer attach updater	0.038462
the inputs	init inputs	0.083333
theano constants in subtensor arguments	tensor make constant args	0.250000
an apply_node recursively search from this	import apply_node check reason	0.066667
between min and	min	0.071429
we can replace that with the *args directly	csm properties csm node	0.142857
baddestroymap	compile check inputs node storage_map r_vals	0.166667
clip x to be	clip x	0.250000
tries to recognize the updates ordereddict the list	scan_module get updates	0.034483
to be reused in scalar	gof clinker cmodule key fgraph no_recycling compile_args libraries	0.200000
an op by transferring each of its outputs	op remove	0.250000
x*x -> sqr x this is	local mul to sqr node	0.166667
the type's :attr context_name	gpu array type	0.062500
a subtensor is inside	subtensor node	0.066667
where each row correspond to the one	one	0.076923
according to the idx list to get the	get idx list	0.076923
register	register	0.600000
with a triangular solve	solve triangular node	0.142857
if necessary update dr_vals	check inputs node storage_map r_vals dr_vals	0.250000
exception some perform() or c_code() created a	view map	0.142857
conv	conv	0.185185
the type	tensor type	0.034483
convolve spatio-temporal filters with a movie	filters signals_shape filters_shape	0.333333
apply nodes in the original graph	inputs outputs copy_inputs_and_orphans memo	0.029412
equivalent	gpulocal	0.055556
it only on cpu here	local pow specialize	0.250000
mflops	op flops	0.125000
gets a scan op a	op not_required inputs	0.071429
inputs required to	gof inputs variable_list blockers	0.058824
the -x pattern	neg var	0.166667
should be removed and	scan_module compress outs	0.076923
arcsine	arcsin	0.166667
same on all device we do	device node	0.045455
recursion over the input dimensions	permute row elements rec	0.047619
return true if a	a	0.008065
draw samples from a poisson distribution	random streams base poisson size lam ndim	1.000000
of _maker which handles much of	mode function maker i o m	0.066667
of this op	tensor prod l op	0.033333
a new	with new inputs inputs	0.166667
the connection pattern of a	connection pattern	0.032258
the original graph to a	inputs outputs copy_inputs_and_orphans memo	0.029412
by the inputs and	init inputs	0.083333
number of dimensions from the shape or	shape	0.010204
the current op and reduce pattern has functioning	gpuarray gpu careduce cuda supports	0.166667
eye for gpu	gpu eye	1.000000
row variable (ndim=2	tensor row	0.050000
the numpy randomstate instance associated with a particular	item	0.076923
biggest error between g_pt and self gf	numeric grad max err g_pt abs_tol rel_tol	0.500000
is the equivalent of	to gpulocal opt	0.055556
deal with the ignore_trees-related functionality	importer pruner chin	0.250000
op that will call the supplied function as	compile as op	1.000000
the	scan execute node	1.000000
for diagonalsubtensor and	get diagonal subtensor	0.083333
a class with a metaclass	metaclass metaclass	0.125000
pack c types back into	gof clinker type c sync	0.111111
comparator to represent the dependence of	gof make dependence cmp	0.111111
len img2d shape ==3 we todo	tensor nnet conv op perform node inp out	0.166667
that	gof get c	0.333333
return a list of shape	default infer shape	0.066667
the variables that	gof	0.002381
this variable optionally inserting broadcasted dimensions	tensor tensor py operators	0.015625
function tries to recognize the updates ordereddict	updates	0.029412
c code to extract a	c extract	0.500000
pattern for advancedsubtensor output variable	pattern	0.028571
generate permutations from	permutation	0.090909
compute a batched	batched	0.111111
reshapes the output	output input	0.333333
partition a list of variables into two	tensor scalarconsts	0.125000
list of shape tuple	default infer shape	0.066667
version of sparseblockouter see sparseblockouter's	outer	0.083333
string x	d3viz replace patterns x	1.000000
function to	random_state low high size	0.500000
one which computes the specified outputs inplace	inplace fgraph	0.142857
required return c code	name	0.011111
the version	version	0.093750
if the	gof gcc	0.027778
diagonalsubtensor	inc diagonal subtensor	1.000000
compute the image shape of convolution gradweights	nnet get conv gradweights shape 1axis	0.500000
new variable to theano config	config var name doc configparam root	0.500000
to extract	extract	0.111111
csr	csr	1.000000
list of l{codeblock} instances returns	code gen blocks	0.050000
series of wrapper functions	linker many linkers wrappers	0.047619
return a list of shape	shape	0.010204
type wrapper for numpy random randomstate	random state type	0.500000
rad2deg	rad2deg	0.833333
see theano tensor std	tensor py operators std axis	1.000000
of	gpulocal opt	0.055556
this optimization makes the folowing changes in	tensor local mul switch sink	0.045455
the value after the import	default	0.030303
numeric shape	shape	0.010204
and b are	b	0.014925
replacement if the ops in the list	gof replace validate replace all	0.050000
for corrmm (direction="forward"),	nnet base corr mm	0.333333
this variable optionally	py operators dimshuffle	0.019231
_maker which handles much of the debugging	function maker i o m	0.066667
for the minimum in	tensor minimum	0.142857
diagonalsubtensor and	diagonal subtensor view x	0.083333
copies a vector to the diagonal	diag	0.023810
to the diagonal	alloc diag	0.027027
this linker's fgraph	clinker make thunk input_storage output_storage storage_map keep_lock	0.333333
graph	function graph	0.040000
reorder the dimensions of this	py	0.014286
equivalent of	opt	0.043478
to compute	tensor nnet	0.070175
to helper_c_code	tensor inc subtensor get helper c code	0.250000
from the loaded cache or the	cache get	0.250000
generate c code but g++ is not available	missing gxx	1.000000
revert the replacement if the ops in the	replace validate replace all	0.050000
attempting to use dnn	dnn	0.060606
b are unified given the unification that	b	0.014925
graphtogpu	opt	0.043478
diagonalsubtensor	diagonal subtensor view x i0 i1	0.083333
scan the contents of a cache directory and	dir dirname err files	0.166667
new random	tensor random	0.166667
conv output gradient w	conv2d grad	0.111111
variable optionally inserting	tensor tensor	0.014286
a string specifying to the user what obj	min informative str obj indent_level _prev_obs _tag_generator	0.333333
this	gof constant	0.333333
uses the topooptimizer from the input nodes	in2out	0.043478
mini-batch of a stack	input_shape filter_shape	0.027778
of this	tensor	0.009646
object a that would	gof	0.002381
the convolution gradient with respect to	gpu dnn conv grad w	0.125000
the g++ version	gof	0.002381
second	second	1.000000
value for a variable of	value a	0.250000
the mflops	op flops inputs	0.125000
import six moves and its	six	0.025000
computes the output dimensions	nnet conv op get output	0.047619
can be considered exactly	values	0.166667
to replace a leaf of a	nnet replace leaf	0.100000
specify	specify	1.000000
computes the confusion matrix	confusion matrix	0.166667
replace a leaf of a multiplication tree	nnet replace leaf	0.100000
structures	push out non	0.125000
and reduce pattern has functioning c	careduce cuda supports c	0.200000
this is the	to gpulocal	0.055556
of sparse	sparse	0.019231
profilestat objects in _atexit_print_list to _atexit_print_file	compile atexit	1.000000
a message explaining the output of is_valid_value	pure type value validity msg a	1.000000
the navigator deal with	navigator	0.032258
listeners to help the navigator deal with	navigator	0.032258
a dimshuffle which only adds dimension to the	tensor local dimshuffle	0.052632
of moved objects in	moves urllib error	0.250000
helper function for grad function	core populate grad	1.000000
see theano tensor prod	tensor py operators prod	1.000000
feature should remove any	gof feature	0.125000
compute 1d kernel for bilinear upsampling this function	nnet bilinear	0.111111
false we can't change the value after the	config param init default filter	0.040000
choose	choose	0.666667
this will attempt to convert x into	x context_name	0.100000
inserting broadcasted	tensor tensor	0.014286
the gradient function should return	tensor matrix inverse grad inputs	0.500000
types involved in this node	inc subtensor do type checking node	0.250000
the args are packed like this n_steps	scan_module scan execute node args outs	1.000000
to	gpuarray gpu inc subtensor get	0.333333
x and y have the same shape	feature same shape x y	0.500000
evaluation mode that detects internal theano errors	debug mode	0.200000
extract test value from v raises	gof get test value v	0.250000
op	fusion op op	1.000000
computes the svd	svd	0.034483
of mod	scalar mod	0.125000
dimensions of this variable optionally inserting	dimshuffle	0.014493
argmax over a given axis	argmax	0.066667
remove subtensor/advancedsubtensor1	tensor local useless subtensor	1.000000
matrix :math a using magma library	gpu magma matrix	0.333333
indices obtained by iterating over given axis	axis keepdims	0.200000
of a compiled graph have a stack	stack	0.066667
op	profile stats op	0.500000
op could be very easy if it is	l op	0.033333
dependence of nodes in a graph	gof make dependence	0.043478
output	nnet conv op get output	0.047619
the cache directory structure	module cache	0.071429
that	gof clinker type	0.133333
to the user what obj is the	obj	0.083333
re-initialize each	seed seed	1.000000
outputs from	outputs mode	0.166667
to the diagonal of an empty matrix	diag	0.023810
row variable (ndim=2 broadcastable=[true	tensor row name dtype	0.050000
one hot encoding of each element in y	tensor to one hot y nb_class dtype	1.000000
-x pattern	tensor nnet is neg var	0.166667
class for giving abbreviated tags like to objects	tag generator	0.333333
bartlett spectral window in the	bartlett m	0.083333
dimensions of	operators dimshuffle	0.019231
return the constant scalar	core get scalar constant	0.333333
in profiling to print the mflops	op flops inputs outputs	0.125000
inner graph to	scan_module scan validate inner graph	0.035714
subclass to add the tensor operators to the	tensor	0.006431
default failure_callback	seq optimizer warn exc	1.000000
filters	nnet conv3d signals filters	0.111111
non-zero in the flattened version	flatnonzero	0.083333
optimization disabled by default that removes all	tensor local remove all	0.166667
a new	with new inputs inputs strict	0.166667
callbacks	callbacks	1.000000
the replacement if the ops in	validate replace all	0.050000
the connection pattern of a subgraph defined by	connection pattern	0.032258
add a new variable to theano config	add config var name doc	1.000000
it into a canonical	get canonical	0.125000
transfer to a tensortype if not	py operators transfer	0.125000
the l operation on f	core lop f	0.166667
process	process	1.000000
bcast	bcast	1.000000
the first outdim-1 dimension size s of x	x ndim outdim	0.333333
apply	tensor apply	0.142857
variable	operators dimshuffle	0.019231
m1 and the second	m1	0.027027
broadcast them to	tensor generate broadcasting	0.066667
dot22 computing an outer-product -> ger	tensor local dot22 to ger or gemv node	1.000000
that unroll the batch	code unroll batch	0.166667
reorder the dimensions of this variable	tensor tensor py operators	0.015625
elementwise [true] division inverse	true div a b	0.333333
clone in a new	clone	0.020833
the contents	dirname err files	0.083333
i and o	gof clone i o	1.000000
see min for the minimum in one tensor	tensor minimum x y	0.090909
1) times from a multinomial distribution defined	streams multinomial	0.076923
using mpi	mpisend	0.037037
x and	sandbox linalg local	0.142857
to get the 0	type get	0.050000
the list remove are still in the graph	fgraph replacements remove reason	0.055556
modified bessel function of	tensor iv inplace	1.000000
of an empty matrix	alloc	0.012500
x * x	sqr x	1.000000
hack in profilemode to print the mflops	gpu corr mm flops inp outp	0.125000
can't change the value after the import	init default	0.040000
c code for corrmm (direction="forward"), corrmm_gradweights	corr mm c code	0.090909
return the initial value for myresult	gpuarray gpu careduce cuda assign init first_item	0.166667
evaluated at points given in eval_points	eval_points consider_constant	1.000000
the navigator deal with the ignore_trees-related functionality	navigator optimizer attach updater fgraph importer pruner chin	0.333333
change references to variables into references to types	tensor subtensor convert entry slice_ok	1.000000
turned into macros for use within the	cop	0.028571
last	last	0.461538
similar behaviour as haskell's	fn sequences outputs_info	0.500000
the original graph to a new node	outputs copy_inputs_and_orphans memo	0.029412
loop executes code	reordered loop	0.111111
the dimensions of this variable optionally inserting broadcasted	py operators dimshuffle	0.019231
apply the list of policies to name r	gof apply policy policy r name	1.000000
1axis	1axis	0.714286
rounding than numpy round	round	0.076923
helper function to	helper random_state low high size	0.500000
the navigator deal	navigator	0.032258
only used to determine the broadcast	tensor adv index broadcastable	0.050000
implement the grad	grad	0.010417
prod(prod()) -> single prod()	tensor local op of op node	0.500000
and apply nodes in the original graph	outputs copy_inputs_and_orphans memo	0.029412
convert addsd	addsd ccode	0.250000
only one client and	sitsot only	0.066667
the equivalent of localoptgroup for	to gpulocal opt group	0.055556
of platform	platform	0.083333
for	gpulocal opt	0.055556
output	output input leftdims rightdims	0.333333
input vector and t is a	node input_storage output_storage	0.038462
more multinomial distributions defined by one-dimensional slices	multinomial	0.024390
the hack in profilemode to print the mflops	gpu corr mm flops inp outp	0.125000
and a set of arrays to	a choices out mode	0.111111
required to	variable_list blockers	0.166667
the graph and	function graph	0.080000
to each level of nesting	loop_orders dtypes loop_tasks sub	0.125000
replacement if	replace validate replace	0.050000
conda offers to make	to	0.017544
of numpy ones_like	tensor ones like model dtype opt	0.333333
the unification u	u	0.050000
return the inputs required to compute	inputs variable_list blockers	0.058824
same type	typed list type	0.250000
connection pattern of subfgraph defined by inputs	op from graph connection pattern	0.076923
instance of _maker which handles much of	compile debug mode function maker i o m	0.066667
a -1 and converts this to expm1	tensor local expm1	0.066667
hack in profiling to print the mflops	nnet conv op flops inputs outputs	0.125000
reproducible case for problems during theano compilation	compile function dump filename inputs outputs mode	0.166667
and return full path of the dynamic lib	module name	0.062500
to determine the broadcast pattern	tensor adv index broadcastable pattern	0.066667
determine the name the object	name	0.011111
broadcasted	dimshuffle	0.014493
update cache data by walking the cache directory	module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
replace that with the *args directly	local csm properties csm	0.142857
that wasn't	destroy	0.009709
the c code	c code	0.166667
python not the other implementation of mod	scalar mod	0.125000
wasn't in	bad destroy	0.034483
decl	decl	1.000000
for	graph	0.016393
function for diagonalsubtensor and	tensor nnet get diagonal subtensor view x i0	0.083333
rebroadcast how to generate	rebroadcast	0.111111
generates the c code for gpucorrmm (direction="forward"),	gpuarray base gpu corr mm c code	0.090909
list remove are still in	remove fgraph replacements remove reason	0.055556
convolution using cudnn from nvidia	gpuarray dnn conv3d img kerns border_mode subsample	0.500000
together into	make	0.017857
grad of this op could be very easy	op	0.009174
if and only if this enum	gof enum	0.166667
input a 4-d tensor	same size input patch_size	0.166667
shape tuple	tensor shape feature default infer shape	0.066667
true iff x and y are equal	check equal numpy x y	0.500000
if	inputs node	0.200000
attempting to use dnn	no dnn	0.125000
det	det	1.000000
to generate permutations from	permutation	0.090909
of mod	mod	0.071429
the fgraph to break aliasing	fgraph wrapped_inputs wrapped_outputs	0.111111
if an alloc is inside	alloc node	0.037037
a six moves urllib namespace	six	0.025000
unroll the batch size	gen conv code unroll batch	0.166667
this generates the c code	c code	0.166667
dictionary data structures	out non seq scan	0.125000
output type dtype and broadcast there is	tensor local canonicalize alloc node	0.333333
a failed fix done in august 2011	load shared variable val	0.142857
graph have a stack	check stack	0.142857
of localoptgroup for	opt group	0.043478
row	tensor row name dtype	0.050000
only one client and that	sitsot only	0.066667
shape and	out shape	0.500000
rel error	rel	0.111111
diagonal of an empty	diag	0.023810
and argmax over a	and argmax	0.166667
raised by get_scalar_const_value	error	0.025000
memory alias that wasn't	bad view	0.027027
inner graph	scan validate inner graph	0.035714
merge 2 profiles returned	seq optimizer merge	0.200000
the specified pieces of vectors and	sparse block outer make node o	0.066667
has any duplicates (according	scan_module has duplicates	0.333333
constant	python constant	1.000000
compiled module from the loaded cache	cache get module name	0.166667
a memory	view	0.022727
a symbolic constant with value x	tensor constant x name ndim dtype	0.333333
be between min and	min	0.071429
and replace it with logsoftmax	logsoftmax	0.076923
creating a class with a metaclass	add metaclass metaclass	0.125000
help the navigator	gof navigator	0.038462
that removes all asserts from the graph	local remove all assert	0.055556
the stack	stack	0.066667
add a	gof key data add	0.500000
name the	name obj	0.111111
to get the 0	type get depth	0.050000
merge 2 profiles	merge	0.071429
change the value after the import of theano	default	0.030303
convolution with the specified	conv get	0.100000
given axis es of a tensor input	input axis dtype op	0.500000
that was dumped to a zip	f persistent_load	0.052632
tag trace to an node	tag trace thing user_line	0.166667
batch normalization of the	gpuarray dnn batch normalization	0.125000
list of policies to name r sub	policy policy r name sub	0.250000
type's :attr	gpu array type	0.062500
choice function	tensor choice	0.500000
data to	data strict	0.500000
graph of apply nodes according to	sort apply nodes inputs outputs cmps	0.050000
compute conv	nnet conv2d	0.333333
the navigator deal	navigator optimizer attach	0.038462
optionally inserting broadcasted	tensor	0.006431
attempts to replace a scan	scan inplace optimizer attempt scan	1.000000
and t	node	0.007407
empty matrix	alloc	0.012500
maps from variable and	get equiv	0.142857
end variables of a	wrt end	0.050000
type's :attr context_name	gpuarray gpu	0.045455
sharedvariable constructor for gpuarraytype	gpuarray gpuarray shared constructor value name strict allow_downcast	1.000000
for gpucorrmm	base gpu corr mm	0.250000
fill a	second inplace a	0.333333
help the navigator	navigator optimizer attach updater fgraph	0.038462
this compiles the source code for	clinker compile cmodule location	0.038462
to recognize the updates	updates	0.029412
obj	obj	0.416667
the output	out	0.018519
orphans	orphans	0.454545
with a modulo of m1 and the second	m1	0.027027
for diagonalsubtensor	get diagonal subtensor view x	0.083333
scalar	core get scalar	1.000000
can't change the value after the import	param init default filter	0.040000
end variables of	wrt end	0.050000
of useless reshape	local useless reshape	0.200000
converting to type2 from	type2	0.050000
any duplicates (according to __eq__)	duplicates	0.125000
listeners to help the navigator deal	gof navigator optimizer attach updater	0.038462
image shape of convolution gradinputs	conv gradinputs shape 1axis kernel_shape	0.500000
upsampling this function builds the 2d kernel that	kernel 2d	0.050000
multinomial distributions defined by one-dimensional slices in	multinomial	0.024390
zeros	zeros	1.000000
dictionary data structures	out non	0.125000
every node	graph	0.016393
elementary validations on the inner graph	validate inner graph	0.035714
connection pattern of subfgraph	connection pattern node	0.076923
connection pattern of a subgraph defined by given	connection pattern	0.032258
to assert that x and	x	0.008772
composite op	scalar composite	1.000000
clone the graph and get a	graph clone	0.166667
is found	is	0.066667
unfortunately conda offers to	to	0.017544
builds the 2d	2d	0.090909
ndarray	ndarray	1.000000
raise baddestroymap	check inputs node storage_map r_vals	0.166667
important note this function uses set and	out seq scan process node fgraph node	0.142857
listeners to help the navigator deal with the	navigator optimizer	0.037037
variable optionally	py operators	0.015625
py	py	0.071429
prof1	prof1	1.000000
that unroll the batch size	conv code unroll batch	0.166667
clip x to be between	clip x	0.250000
optimize the possible advsub1(advincsub1	tensor local adv sub1 adv inc sub1 node	1.000000
that	conv op	1.000000
turned into macros for use within	cop get	0.033333
determine the broadcast pattern for advancedsubtensor	tensor adv index broadcastable pattern a	0.066667
for efficiently calculating the dot product	dot x	0.166667
and return full path	gof module name from	0.076923
this explicitly upcasts constant inputs to	constant inputs node	0.125000
add	tensor	0.006431
use with helper_c_code	subtensor get helper c code	0.142857
recognize the updates ordereddict	get updates	0.034483
compiles the source code for	cmodule location	0.038462
g++	gof gcc	0.027778
transform it into a canonical	get canonical	0.125000
subprocess_popen returning the output error	output	0.017241
by default that removes all asserts from	remove all assert	0.055556
b), axis=0) -> elemwise{scalar op} a	tensor local reduce join node	0.111111
"inshp" with kernels of shape	shape	0.010204
output dimensions of convolving an image	output	0.017241
node inputs[i] to	function graph change input node i	0.250000
proxy for either true_div or int_div depending on	div proxy	0.125000
zeros	of zeros node	1.000000
to represent the dependence of	gof make dependence	0.043478
checks if theano graphs represent the	xs ys in_xs in_ys	0.111111
c code for gpucorrmm (direction="forward"),	base gpu corr mm c code	0.090909
for diagonalsubtensor	nnet get diagonal subtensor view x i0	0.083333
this is the main interface to manipulate	r new_r reason verbose	0.071429
insert deepcopy in the fgraph to	compile insert deepcopy fgraph	0.500000
c	gof clinker op c	0.250000
ger	ger or	1.000000
destructive	destroy	0.009709
node a clone in a new	clone	0.020833
and a set of arrays to choose	choose a choices out mode	0.200000
if it takes the full input in	node	0.007407
a config string	config string	0.333333
to boundvariable(other_object)	unify walk fv o u	0.200000
the navigator	gof navigator optimizer attach	0.038462
increments a subtensor	subtensor	0.058824
unfortunately conda offers to make itself the default	to os environ pathlist var newpath	0.038462
compute conv	nnet	0.048387
bilinear upsampling this function	bilinear	0.038462
since the epoch of the last access of	last access time	0.040000
decorator to merge addition by a	merge cls alpha_in beta_in out_in	0.250000
a new instance of this mode	monitor mode clone link_kwargs optimizer	0.333333
false we can't change the value after	param init default filter	0.040000
a uniform	tensor uniform random_state	0.125000
just the compilation of cutils_ext	compile cutils	0.166667
merge abs generated by local_abs_lift when	local abs merge node	0.333333
are	reason	0.166667
series of wrapper functions instead of just	wrap linker many linkers wrappers	0.047619
diff to	diff	0.071429
for diagonalsubtensor	diagonal subtensor	0.083333
node	graph	0.016393
view	compile view tree set	0.500000
their apply_node if those nodes are	gof	0.002381
change the value after	core config param init default filter	0.040000
alias	view	0.022727
scalar constants and the rest	rest	0.076923
variable of this type	pure type	0.142857
reorder the	tensor tensor py operators dimshuffle	0.019231
pushing out the variables	out	0.018519
compare true iff other is the same	tensor tensor type eq other	0.250000
runs a series of wrapper functions instead of	many linkers wrappers	0.047619
function to get the	type get depth	0.050000
retrive the context associated with a	context	0.035714
a config string (comma-separated key=value components) into a	parse config string config_string issue_warnings	0.166667
interface to clone that allows you to pass	scan_module reconstruct graph	1.000000
reorder the dimensions	tensor py operators dimshuffle	0.019231
return the variables in inputs that are	gof	0.002381
the equivalent	graph to	0.055556
copies the stack	gof copy stack	0.333333
in array of ints	tensor bincount x weights minlength assert_nonneg	0.125000
many	many	1.000000
a list of shape tuple	shape	0.010204
list of localoptimizer	opt group	0.043478
from type1	type1	0.142857
specified pieces of vectors and	sparse block outer make node o x y	0.066667
b u returns	gof	0.002381
stack trace from one or	gof copy stack trace	0.055556
list of lib directories	lib dirs	0.045455
navigator deal with	gof navigator optimizer attach	0.038462
some functiongraph listeners to help the navigator	gof navigator optimizer	0.038462
work for elemwise and gpuelemwise op	tensor local elemwise fusion op op	0.200000
a gemm	gemm	0.066667
deal with the ignore_trees-related functionality	fgraph importer pruner chin	0.250000
some functiongraph listeners to help the navigator deal	navigator optimizer attach	0.038462
that need not be checked for nan and	is numeric value arr var	0.166667
the abs and rel error of	abs rel	0.166667
replacement if the ops in the list	validate replace	0.050000
detect if the g++ version used is	gof gcc	0.027778
the grad	grad grad	0.166667
constant inputs	constant inputs node	0.125000
"reverse-mode" gradient for	grad perform	0.083333
converts number to string by rendering it in	compile char from number number	0.142857
to wait on a previously sent	wait	0.022727
abstract	abstract	0.555556
change the value after the	param init default	0.040000
replace_all_validate	all validate remove fgraph	0.166667
the list of outputs and	and outputs	0.100000
the	gof gcc	0.055556
an fgraph and a list of variables returns	fgraph	0.012195
check_integrity	check_integrity	0.454545
scan makes it run	scan	0.017241
out the variables	out	0.018519
1d kernel for bilinear upsampling	bilinear	0.019231
for any python object a that would be	gof	0.002381
return connection pattern	from graph connection pattern node	0.076923
split x	split	0.125000
variable with a	variable x	0.083333
the value after the import of	init default	0.040000
generate c	specify shape c	0.250000
assert that x and y have the	x y	0.024390
specified pieces of vectors and	sparse block gemv make node o w	0.066667
of this variable optionally inserting broadcasted dimensions	tensor tensor py operators	0.015625
contents of a	dirname err files	0.083333
l operation on f	f	0.052632
to override this should return an iterable	gpuarray gpu kernel base gpu kernels node name	0.166667
compiles this	gof clinker compile	1.000000
of headers that are needed by	headers	0.038462
this function compute the	tensor nnet	0.035088
to make itself the	to os environ	0.038462
optionally	tensor py operators	0.015625
to get the	type get depth	0.050000
and o	o	0.076923
symbolically cast x	tensor cast x	0.200000
remove are still	fgraph replacements remove reason	0.055556
parameter as other scalar op	scalar	0.035714
specified axis	axis sparse_grad	0.333333
a convolution	conv	0.037037
rstrip	rstrip	1.000000
erfc	erfc	1.000000
a tuple of integers indicating the version	cache version	0.125000
return str of variable type	type to str t	0.500000
for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x	0.083333
special compound l{op} for the output of	argmax1hot	0.058824
tracks	tracks	1.000000
for corr3dmm (direction="forward"),	tensor nnet base corr3d mm	0.333333
all asserts from the graph	all assert	0.250000
log(1 + x)	log1p x	1.000000
symbolic row variable	row	0.034483
cpus in	misc cpu count	1.000000
graph of apply nodes	sort apply nodes	0.200000
to replace a leaf	nnet replace leaf	0.100000
optionally	operators	0.017241
listeners to help the navigator deal with	navigator optimizer attach updater fgraph	0.038462
stats	stats	1.000000
return a module from the cache	cache module from	0.333333
eigensystem	eigh	0.125000
don't accept complex otherwise call upgrade_to_float()	upgrade to float no complex	0.333333
reshaped view/copy	py operators reshape shape ndim	0.333333
impossible to evaluate because of aliasing and destructive	destroy	0.009709
mini-batch of a stack of 3d	input_shape filter_shape	0.009259
a graph of apply nodes according to	sort apply nodes inputs outputs cmps	0.050000
attempt to convert x into a variable on	as gpuarray variable x context_name	0.166667
of the specified pieces of vectors	sparse block outer make node o	0.066667
uses shared variable names when persisting to	persistent shared variable id	0.142857
the function on the inputs and	node inputs	0.043478
and destructive	destroy	0.009709
pattern	pattern node	0.125000
compute conv output	tensor nnet conv3d	0.142857
based level of the list	list type	0.100000
the exp x or -exp x patterns	is exp	0.333333
real-valued input on	curfft inp norm	0.066667
upper triangle	triu m k	0.250000
look for a constant that	gof params	0.200000
that are non-zero in the flattened version	flatnonzero	0.083333
that will be instantiated by c_extract	gof clinker type	0.066667
meta path importer to import	meta path importer	0.166667
specified pieces of vectors	sparse block gemv make node	0.066667
if a and	a	0.008065
class to raise in self	core raise	0.100000
through a graph either breadth- or depth-first	start expand mode build_inv	0.333333
replacement if	validate replace all	0.050000
respect to wrt, computes gradients	subgraph grad wrt	0.062500
replace_all_validate revert the replacement if	replace validate replace all validate	0.111111
allocate outputs	tensor make alloc loop_orders dtype sub fortran	0.200000
input that wasn't in the	destroy	0.009709
trailing spaces tabs	misc hooks	0.250000
of inplace	inplace check	0.500000
not attempting to use dnn	dnn	0.060606
when we overwrite	tensor local useless inc subtensor node	0.066667
a compiled module from the loaded cache	gof module cache get module	0.166667
var transferred	tensor transfer var	0.100000
in defining the gradient the finite	grad	0.010417
shared variable names when persisting	shared variable id	0.142857
in the original graph to a new	outputs copy_inputs_and_orphans memo	0.029412
draw random integers	random integers	0.500000
this variable optionally	tensor tensor	0.014286
reducing the number of multiplications and/or divisions	tensor local greedy distributor node	0.166667
root	root	1.000000
this is the equivalent of localoptgroup	opt group	0.043478
efficiently calculating the dot product	dot	0.035714
values	values	0.833333
that initializes py_name to py_none	gof get c init r name	0.250000
after a	default	0.030303
epoch of the last access of a given	last access time path	0.040000
op and reduce pattern has functioning c code	careduce cuda supports c code	0.250000
context object mapped to the type's	context	0.035714
to evaluate because of aliasing	destroy	0.009709
fgraph and a list	fgraph	0.012195
to replace a leaf of a	replace leaf	0.100000
that was dumped to a zip file	f persistent_load	0.052632
inplace	inplace	0.256410
attach_feature the method that attaches	gof bookkeeper on attach fgraph	0.142857
this variable optionally	dimshuffle	0.014493
the value after	core config param init default filter	0.040000
of scan to outside of scan	out scan	0.035714
the numeric shape of all intermediate variables given	tensor shape of variables fgraph input_shapes	0.100000
a reshaped view/copy of this variable	tensor tensor py operators reshape shape ndim	0.111111
softmax(sum_of_stuff) -> softmax_w_bias matrix bias	nnet local softmax with bias	0.200000
version	version apply node	0.125000
fgraph outputs that will	fgraph expanded_inputs	0.058824
given axis es of a	axis dtype	0.083333
removes useless dimshuffle operation inside reshape reshape(vector	tensor local useless dimshuffle in reshape node	0.500000
as the output type dtype and broadcast there	tensor local canonicalize alloc	0.333333
a metaclass	add metaclass metaclass	0.125000
will be turned into macros for	cop get	0.033333
import six moves	six	0.025000
false we can't change the value after	config param init default	0.040000
access of a given	access time path	0.200000
the end variables	end	0.040000
function for diagonalsubtensor	diagonal subtensor view	0.083333
class returns the bartlett spectral window in the	tensor bartlett	0.083333
main interface to manipulate	r new_r reason verbose	0.071429
the mflops	op flops	0.125000
matrix solve operation c = a \ b	solve	0.032258
with constant inputs replaced by	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
op}(join(axis=0 a b), axis=0) -> elemwise{scalar op} a	tensor local reduce join	0.111111
4-d tensor it sets all non maximum	patch_size	0.050000
each shape that broadcast them	tensor generate broadcasting	0.066667
a list of shape tuple or	shape	0.010204
removes all from the clients list	gof function graph remove client	0.200000
get a dict	get	0.020833
node by	node	0.007407
validations on the inner	scan_module scan validate inner	0.142857
attaches	bookkeeper on attach	1.000000
a warning message on the first call	deprecated filename msg	0.041667
grad of average	average	0.200000
return a module from	module from	0.166667
filters	conv2d input filters	0.125000
runs a series of wrapper functions	wrap linker many linkers wrappers	0.047619
existing start gradients up to the end variables	wrt end start	0.166667
addbroadcast	addbroadcast	1.000000
to ultra_fast_sigmoid	nnet local ultra fast	0.500000
string representating the cause of the exception	bad optimization str diagnostic	0.043478
return an instance of _maker which handles much	function maker i o m	0.066667
insert deepcopy	insert deepcopy	0.333333
this variable optionally	tensor py operators	0.015625
cost and/or from existing start gradients up	start cost	0.100000
a symbolic row variable (ndim=2	row name dtype	0.050000
a numpy ndarray contains any np inf values	compile contains inf arr node var	0.500000
and a	a	0.008065
required return c code to extract	gof clinker type c extract name	0.250000
x*x -> sqr	tensor local mul to sqr node	0.166667
get	get depth	0.050000
the dimensions of this	py operators	0.015625
helper function to generate permutations from integers	tensor permutation helper random_state n	0.333333
hash of	hash	0.055556
this to expm1 a	expm1	0.050000
the n-th order discrete difference	tensor diff x n	0.333333
of compilation flags	libs flags libs_dir include_dir	0.052632
helpful function that gets a scan op a	op not_required inputs	0.071429
for corr3dmm	tensor nnet base corr3d mm	0.333333
reps	reps ndim	0.500000
poisson	random streams base poisson	0.500000
x -> x	tensor local	0.025641
each tuple of arguments which must be hashable	gof memoize f	0.250000
get the 0	get	0.020833
method is primarily used by tensor	pure op r op inputs eval_points	0.125000
the last access of a given file	last access	0.040000
svd on	svd a	0.200000
by re-writing	gof refresh	0.125000
has	has	0.875000
raises a badviewmap exception when it detects	compile check viewmap node storage_map	0.111111
alloc	tensor local alloc	0.111111
required return c code to declare variables	type c declare name	0.500000
try to	try	0.111111
wait on a previously received array	wait	0.022727
the cache directory	gof module cache	0.083333
search through a	gof stack search	0.333333
raised	error	0.050000
to r to	compile debugprint r	0.250000
apply_node recursively search from this node	apply_node check reason	0.066667
important note this function uses set	scan_module push out seq scan process node fgraph	0.142857
a memory	bad view	0.027027
symbolic thing to print	obj	0.083333
if it	node	0.007407
variables	gof variables	0.125000
first outdim-1 dimension size s	ndim outdim	0.142857
of scan return true iff	scan_module push out scan	0.050000
badviewmap exception when it detects	check viewmap node	0.111111
the fgraph outputs that will replace their	fgraph expanded_inputs	0.058824
see theano tensor argmax	py operators argmax axis	1.000000
return a string for making	gpuarray gpu careduce cuda makecall node name	1.000000
of x the same	flatten x	0.166667
symbolic vector	vector name dtype	0.166667
dot product	dot x	0.166667
symbolic row variable	row name	0.050000
of lib directories that are needed	header dirs	0.045455
that has	gof	0.002381
context object mapped	array type context	0.090909
full path of the	module	0.033333
detect if the g++ version used	gcc	0.023810
return a symbolic row variable (ndim=2	tensor row	0.050000
x -> sigm -x	tensor nnet local	0.200000
struct initialization code	init code struct node	0.125000
output for this	output	0.017241
hyperbolic arc sine of	tensor arcsinh	1.000000
"lifts" dimshuffle through elemwise operations and merges consecutive	local dimshuffle lift node	0.250000
2d kernel that can be used to upsample	kernel 2d ratio normalize	1.000000
sigmoid to ultra_fast_sigmoid	ultra fast sigmoid	0.200000
for abstractconv parameters	abstract conv	0.333333
used is the	gof gcc	0.027778
factor takes as input	signal pool 2d input	0.090909
context associated with	context	0.035714
enabled change all sigmoid to ultra_fast_sigmoid	local ultra fast sigmoid node	0.200000
batched dot product	batched dot	0.250000
lib directories	header dirs	0.045455
connection pattern of	from graph connection pattern	0.076923
this variable optionally inserting	tensor tensor py operators	0.015625
optimization is not the same on all device	device	0.076923
the fgraph this is the	fgraph	0.012195
topooptimizer from the input nodes	gof in2out	0.055556
return the constant scalar 0-d value underlying	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
indices field	csm indices csm	0.333333
spatio-temporal filters with a movie	tensor nnet conv3d signals filters signals_shape filters_shape	0.333333
x	x	0.421053
see theano tensor min	tensor py operators min axis keepdims	1.000000
strings check if converting to type2 from type1	type1 type2	0.166667
metaclass	metaclass metaclass	0.125000
leftdims + rightdims	leftdims rightdims	0.333333
python object a that would be a	gof	0.002381
runs a series of wrapper functions instead	linker many linkers wrappers	0.047619
hack in profiling to print the mflops	flops inputs outputs	0.125000
hash equal	tensor type hash	0.166667
pattern of	pattern node	0.125000
more multinomial	tensor multinomial random_state	0.040000
returns the bartlett spectral window in the time-domain	bartlett m	0.083333
first half of v by a	v	0.011111
mapping all symbolic variables in inputs	inputs	0.012658
of type topooptimizer	topo db	0.166667
is only used to determine the broadcast pattern	adv index broadcastable pattern	0.066667
the replacement if the ops	replace validate replace	0.050000
of the specified pieces of vectors	sparse block outer make node o x	0.066667
is the equivalent	gpulocal	0.055556
converts	from	0.050000
usage inplaceelemwiseoptimizer op optimize fgraph	tensor inplace elemwise optimizer apply fgraph	1.000000
return a symbolic row variable (ndim=2 broadcastable=[true	row	0.034483
in the theano enumeration types wrapped into current	type enum from	0.333333
indices	indices	0.461538
one or more multinomial	tensor multinomial	0.037037
a new	clone with new inputs inputs strict	0.166667
of type localoptgroup instead of a global optimizer	group db	0.500000
self _rop_op from user supplied form	compile op from graph recompute rop op	0.200000
image shape of	shape 1axis	0.250000
to make it work for elemwise and gpuelemwise	local elemwise	0.166667
failure_callback for navigatoroptimizer ignore all errors	navigator optimizer warn ignore exc nav repl_pairs local_opt	1.000000
the dot product	dot	0.107143
the gradient function should	matrix inverse grad inputs g_outputs	0.500000
:param execute if true execute a	execute execute verbose	0.250000
an	an	1.000000
is a package	importer is package	0.500000
class for node-based optimizations	local optimizer	0.333333
handler	handler	0.357143
struct initialization code	init code struct	0.125000
same parameter as other scalar op	scalar	0.035714
correspond to the one hot	to one hot	0.142857
a shared variable	compile shared variable	0.083333
important note this function uses set and dictionary	push out seq scan process node fgraph node	0.142857
subtensor is inside a dimshuffle which only drop	subtensor node	0.066667
set of 3d	nnet conv3d	0.071429
return a symbolic row variable	row name	0.050000
return the inputs required to compute the given	gof inputs variable_list blockers	0.058824
transfer function for alternative targets	transfer fn	0.125000
wrapper around c_init that initializes py_name to py_none	gof get c init r name	0.250000
only used to determine the broadcast pattern	adv index broadcastable pattern a idx	0.066667
merge some gpucareducecuda and	careduce node	1.000000
of var shape[i], but apply if possible	i var i fgraph	0.200000
apply instance from set which must be computed	handler on prune fgraph app reason	1.000000
the inner graph to ensure that	inner graph	0.035714
movie	signals_shape filters_shape	0.333333
a that would	gof pure	0.033333
the complex conjugate	conj	0.100000
the args	scan_module scan execute node args	1.000000
the cache directory	module cache	0.071429
some elementary validations on the inner graph to	validate inner graph	0.035714
gpucorrmm	corr mm	0.250000
sum(a / dimshuffle{ } b axis=l) ->	local	0.014085
see min for the minimum	minimum x	0.142857
-> alloc(unary x shp)	local alloc unary	0.250000
b are unified given	b	0.014925
the dependence	gof make dependence	0.043478
the "reverse-mode" gradient for the eigensystem of	tensor eigh grad perform	0.333333
list of shape	infer shape	0.066667
image of shape "inshp" with kernels of shape	shape	0.010204
disabled by default that removes all asserts	local remove all assert	0.055556
gradient is an alloc of a	alloc node	0.037037
dimensions from the shape or	shape	0.010204
of op classes	local optimizer tracks	0.200000
the dimensions of this variable optionally inserting	tensor py operators	0.015625
batch normalization of	nnet batch normalization	0.125000
for comparing	tensor constant	0.055556
pushing out the	push out	0.037037
optimizer for scan makes it run inplace	scan inplace optimizer	0.500000
comparing	tensor constant	0.055556
the output dimensions of convolving an	nnet conv op get output	0.047619
next stream	sandbox mrg random streams inc	1.000000
dimensions of this variable optionally inserting broadcasted dimensions	tensor py operators	0.015625
when allow_gc = false clear the variables in	compile function free	0.250000
upper triangle	tensor triu m k	0.250000
one hot	to one hot	0.142857
baddestroymap if	inputs node storage_map r_vals	0.166667
to run after c_code whether it failed	cleanup node	0.142857
a random	base random	0.500000
if a dimshuffle is inside	dimshuffle node	0.333333
an op that will	op	0.018349
prof2	prof2	1.000000
connection pattern of subfgraph	compile op from graph connection pattern	0.076923
this mode	mode	0.125000
is the first functiongraph	no_recycling	0.111111
a c contiguous version of	gpu contiguous	0.083333
type numpy typenum that corresponds to	type dtype specs	0.071429
replace it with logsoftmax x 's grad	local logsoftmax grad	0.200000
raise baddestroymap if	storage_map	0.090909
construct a variable	variable x name	0.083333
op could be very easy	l op	0.033333
of nodes that	gof	0.002381
convop that unroll the batch	unroll batch kern d unroll_bsize unroll_ksize	0.125000
cache of dynamically compiled modules on disk	module cache	0.071429
can't change the value after the	default filter	0.040000
the dimensions of this variable	tensor py operators	0.015625
return apply profiling informaton	d3viz apply profile node profile	1.000000
shift	shift	1.000000
the corresponding element of a dense vector	s	0.071429
uses the topooptimizer from	gof in2out	0.055556
new random stream	tensor random streams	0.142857
removes useless dimshuffle	local useless dimshuffle	0.500000
string representation	to str b	0.250000
maps a failure code to the task	find task	0.142857
the replacement if the	replace validate replace	0.050000
if target is 'cpu' this will transfer	tensor py operators transfer target	0.500000
unroll the batch size	nnet gen conv code unroll batch	0.166667
row correspond to the one hot	to one hot	0.142857
grad of average pooling	average pool	0.200000
input a 4-d tensor it sets	size input patch_size	0.166667
end variables of a	grad wrt end	0.050000
to get the 0 based	get	0.020833
sparseblockgemv(inplace=false) -> sparseblockgemv(inplace=true)	tensor nnet local inplace sparse block gemv node	1.000000
takes as input a 4-d tensor	signal max pool 2d same size input patch_size	0.250000
unused	unused	1.000000
true if we are able	dim_x dim_y	0.090909
variable with a	variable x name	0.083333
connection pattern of a subgraph defined	gof io connection pattern	0.055556
dot csr is like dot except that	dot csr	0.111111
atexit	atexit	1.000000
1-sigm x -> sigm -x	local 1msigmoid node	1.000000
to a mode	mode	0.062500
raise baddestroymap	node	0.007407
performs the svd on	svd a	0.200000
gradient the finite fourier transform is viewed as	tensor fourier grad inputs cost_grad	1.000000
the same type	list type	0.100000
of a cache directory and return	name from dir	0.250000
returns the connection	connection	0.100000
from the loaded cache or the disk	gof module cache get	0.250000
bound on the largest eigenvalue of	bound	0.043478
as replace_all_validate revert the replacement	validate replace all validate remove	0.111111
class	class	1.000000
exception object with debug	raise with op node	0.333333
a numpy ndarray contains any np nan values	compile contains nan arr node var	0.500000
helper function for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x i0	0.083333
this function is basically a call	extract constant x elemwise only_process_constants	0.058824
helper_c_code	inc subtensor get helper c code	0.285714
remove rebroadcast if id does not actually change	local useless rebroadcast node	1.000000
nodes to	gof	0.002381
class with a metaclass	compat add metaclass metaclass	0.125000
of shape	infer shape	0.066667
return a message explaining the output of is_valid_value	gof pure type value validity msg a	1.000000
batch normalization of	gpuarray dnn batch normalization	0.125000
-1 and converts this to expm1	local expm1 node	0.066667
return data or	gof	0.002381
return c code to extract	clinker type c extract name	0.250000
on the inner graph	inner graph	0.035714
connection pattern of a subgraph defined by given	io connection pattern	0.055556
gradient wrt filters for corr3dmm	corr3d mm grad weights	1.000000
optimization to insert inplace versions of remove0	sparse local inplace remove0 node	0.333333
of moved objects in six moves urllib_robotparser	module six moves urllib robotparser	0.333333
c code to	c code	0.055556
vector and	node	0.007407
a sparse format instead	sparse	0.019231
the fgraph this is	fgraph	0.012195
tensor operators to	tensor	0.006431
exp2	exp2	0.833333
the n-th order discrete difference along given axis	diff x n axis	0.500000
internal function that constructs a	scan_module safe	0.500000
important note this function uses set and	process node fgraph node	0.142857
used to determine the broadcast	adv index broadcastable	0.050000
of this variable optionally inserting broadcasted	py operators dimshuffle	0.019231
from the inputs	inputs	0.012658
hash equal	hash	0.055556
instances of suitable dummy values	meta optimizer provide	0.200000
with the *args directly	local csm properties csm node	0.142857
in profiling to print the mflops	conv flops inp outp	0.125000
elemwise maximum see max for the maximum	maximum	0.083333
sigmoid to ultra_fast_sigmoid	local ultra fast sigmoid node	0.200000
six moves urllib	module six	0.043478
as replace_all_validate revert the replacement if the	replace all validate	0.111111
duplicate this apply instance in a	gof apply clone with	0.166667
over each shape that broadcast them	tensor generate broadcasting	0.066667
a symbolic vector variable	vector name dtype	0.166667
replace_all_validate revert the replacement if the ops in	replace all validate	0.111111
return a symbolic row variable (ndim=2 broadcastable=[true	tensor row name dtype	0.050000
release	release	1.000000
get the 0 based level of	get	0.020833
to convert x into a variable on the	as gpuarray variable x context_name	0.166667
an	op	0.018349
intdiv	intdiv	1.000000
exception object with debug info	raise with op	0.333333
convolution gradient with respect	gpu dnn conv grad i	0.125000
on f wrt to wrt	core rop f wrt	0.200000
when we overwrite the full	local useless inc subtensor node	0.066667
maximum	maximum	0.500000
the value after the import of theano	core config param init default filter	0.040000
cache "filename" as a	gof call cache persist filename	0.250000
we can't change the value after	config param init default	0.040000
the folowing changes in	local mul switch sink	0.045455
a | b inplace on a	or inplace a b	0.333333
shape	tensor shape feature set shape	0.333333
symbolically cast x to a tensor of	cast x	0.200000
threads interface	threads text	1.000000
graph and get a memo a dict that	gof function graph	0.031250
the navigator deal with	navigator optimizer	0.037037
reorder the	operators dimshuffle	0.019231
done in august 2011	tensor load shared variable val	0.142857
a >	tensor gt a	1.000000
variable	tensor	0.006431
is an input vector and t is	node input_storage output_storage	0.038462
on wraplinker that runs	gof wrap	0.083333
apply the list of policies to	apply policy policy	0.500000
a uniform into sample from	from uniform	0.200000
1d kernel that can	kernel 1d	0.050000
for scalar values	scalar	0.017857
is	compat six meta path importer is	0.250000
uses set and dictionary data structures	push out non seq	0.125000
of a cache directory and return full path	gof module name from dir	0.071429
see theano tensor max	py operators max axis	1.000000
some elementary validations on the inner	scan validate inner	0.142857
return a list	get	0.020833
input a 4-d tensor it sets all non	2d same size input patch_size	0.166667
the dimensions	tensor py operators	0.015625
from existing start	start	0.040000
a to radian	a	0.008065
name	name	0.133333
transfer to a tensortype if not already one	py operators transfer	0.125000
respect to wrt, computes	subgraph grad	0.062500
merge 2 profiles returned by	optimizer merge	0.200000
apply as	apply	0.016667
for the maximum in	tensor maximum x	0.142857
it with logsoftmax	logsoftmax node	0.125000
suitable dummy values	meta optimizer provide inputs	0.200000
dictionary data structures	push out non	0.125000
the contents of a cache directory and	from dir dirname err files	0.166667
doing a recursion over	permute row elements rec	0.047619
output error and exit code	misc output subprocess popen command	0.100000
dimensions of this variable optionally inserting	tensor py	0.015873
type	list type	0.100000
op}(join(axis=0 a b), axis=0) -> elemwise{scalar op}	tensor local reduce join	0.111111
be inserted in the struct	struct node	0.062500
that are views of v given that v	tree set v	0.125000
shape "inshp" with kernels of shape	shape	0.010204
a tensorvariable of this type	type make variable	0.500000
the idx list to get	get idx list	0.076923
in a new	with new inputs	0.166667
0 / x -> 0	tensor local zero div node	1.000000
l{codeblock} instances returns a string that executes them	gof code gen blocks	0.050000
convolving a mini-batch of a stack of 3d	input_shape filter_shape	0.009259
respect	gpu dnn	0.133333
an un-computable symbolic variable of type x type	core grad not implemented op x_pos x comment	1.000000
try	gof compiler try	0.250000
return	call name	0.500000
implements the "reverse-mode" gradient	grad perform node inputs	0.166667
shared variable names when persisting to zip	persistent shared variable id	0.142857
a symbolic 3-d	tensor3 name dtype	0.166667
node	pattern sub transform node	1.000000
gradient overrides see help theano opfromgraph for syntax	grad overrides grad_overrides	1.000000
determine	tensor adv index broadcastable	0.050000
for gpuincsubtensor	tensor local inplace setsubtensor	0.250000
with a particular	getitem item	0.125000
along the given axis es	axis dtype keepdims	0.083333
x is a matrix	x	0.008772
comparing tensorconstant	constant	0.016667
headers that are needed	headers	0.038462
permutations	permutation random_state size	0.500000
that will	gof	0.002381
memory alias that wasn't in the view_map	bad view	0.027027
l{linker}s which keep all nodes in the graph	local linker	0.333333
reshapes the output	output	0.017241
of	py operators dimshuffle	0.019231
vect	vect	1.000000
op __init__ fct don't have the	composite make new inplace output_types_preference name	0.142857
can't change the value after the import	default filter	0.040000
arcsine of a	tensor arcsin a	1.000000
grad of this op could	prod l op	0.033333
inputs and outputs	inputs outputs	0.133333
stack trace from one or more tensor	gof copy stack trace	0.055556
will attempt to convert x into	x context_name	0.100000
the replacement if the	gof replace validate replace all	0.050000
of 3d inputs with a set of 3d	conv3d input	0.125000
existing start gradients up to the end variables	grad wrt end start	0.166667
ignore all errors	ignore	0.166667
inserting 1 at the	shape padaxis	0.333333
op code	op params	0.100000
to the fgraph outputs that will	fgraph	0.012195
the context associated with	gpuarray get context	0.111111
this compiles the source code for this linker	clinker compile cmodule location	0.038462
subprocess_popen returning the output error and	output	0.017241
kinds of useless reshape	useless reshape node	0.200000
as the template filled by broadcasting value	broadcast like value template fgraph	0.125000
nit_sot output of scan return	scan_module push out scan	0.050000
broadcasted dimensions	tensor py	0.015873
the	gpulocal	0.055556
_maker which handles much of	debug mode function maker i o m	0.066667
input	destroy	0.009709
up to the end	wrt end	0.050000
list of op classes that this	gof local optimizer tracks	0.071429
bartlett spectral window in the	tensor bartlett	0.083333
important note this function	scan process node fgraph	0.142857
dimensions of this variable	tensor py	0.015873
vector 1-dimensional	core jacobian	0.500000
tuple or none for	i_shapes	0.050000
value for a variable of this	value a	0.250000
for operations that need to	gpu kernel	0.250000
set	graph set	1.000000
relative tolerance used as threshold for gradient	rel_tol	0.166667
the dimensions of this variable	tensor	0.006431
in u and uses it instead of the	o u	0.037037
edges	edges	1.000000
only used to determine	tensor adv index broadcastable	0.050000
apply the	gof apply	0.090909
to the type's :attr	array type	0.055556
the connection pattern of	io connection pattern	0.055556
for corrmm (direction="forward"),	tensor nnet base corr mm	0.333333
is a thunk that	gof linker make thunk	0.045455
dimensions of this	tensor tensor py operators dimshuffle	0.019231
an input that wasn't in the	destroy	0.009709
keeping the first outdim-1 dimension size s of	ndim outdim	0.142857
softmax(sum_of_stuff) -> softmax_w_bias matrix bias	tensor nnet local softmax with bias	0.200000
axis2	axis2	1.000000
that removes	remove	0.035714
convenience function to roll	roll	0.111111
l{op} for the output of neural-net multiclass classifiers	softmax with bias	0.142857
the g++ version used	gof gcc	0.027778
copies the stack trace from	gof copy stack trace	0.055556
override clinkertype c_sync	tensor type c sync name sub	1.000000
specified pieces of vectors and matrices	sparse block gemv make	0.066667
true if the named module is a package	compat six meta path importer is package fullname	0.250000
wrt, computes gradients of	subgraph grad	0.062500
disabled by default that removes	tensor local remove	0.166667
views of v given that v	tree set v	0.125000
ops in the list remove are still in	replacements remove reason	0.055556
of moved objects in six moves urllib_error	module six moves urllib	0.090909
division	tensor true div	0.250000
replace it with logsoftmax x	tensor nnet local logsoftmax	0.076923
simply checks	unify walk v	1.000000
a wrapper for	op	0.009174
subgraph contained between i and o	i o copy_inputs	0.333333
return connection pattern of subfgraph	from graph connection pattern	0.076923
alias that wasn't	bad	0.013158
modified bessel function	iv inplace	1.000000
return connection pattern	compile op from graph connection pattern node	0.076923
check	check	0.416667
value pairs that will be turned into macros	cop get	0.033333
gradient function should	matrix inverse r op inputs eval_points	0.500000
profilemode to print the mflops	gpuarray base gpu corr3d mm flops inp outp	0.125000
c code when doing constant folding	elemwise python constant folding	0.142857
stream state and they are spaced	random streams	0.058824
computes the confusion matrix	nnet confusion matrix	0.166667
context by mapping it to a name	gpuarray reg context name ctx	0.500000
enumeration types	params type	0.250000
chi2sf	chi2sf	1.000000
ops in the list remove	remove	0.035714
a leaf of a	leaf	0.066667
simple	simple	1.000000
without replacement a can be a 1-d array	choice random_state size a replace	1.000000
for	tensor nnet base abstract	0.500000
the shape	tensor shape feature set shape	0.333333
sum	sum	0.346154
function :func neibs2images <theano sandbox neighbours neibs2images>	tensor nnet neibs2images neibs neib_shape original_shape mode	0.333333
variable on the gpu	gpuarray variable	0.166667
rest	rest	0.461538
a dimshuffle is inside an alloc and	local alloc dimshuffle node	0.166667
the context associated with a name	get context name	0.333333
b are	b	0.014925
idx_list with constant inputs replaced by their python	subtensor get constant idx inputs allow_partial only_process_constants elemwise	0.071429
elemwise tanh of x	sparse tanh x	1.000000
attach	attach	1.000000
new variable to	var name doc	0.250000
openmp	openmp	1.000000
as python not the other implementation of mod	scalar mod c code node name	0.125000
for	graph to gpulocal	0.055556
to the fgraph this is the	fgraph	0.012195
matrices returning pieces of vectors :	sparse block gemv	0.166667
actual	actual	1.000000
module initialization code	init code	0.142857
the given axis es of a tensor input	input axis ddof keepdims	0.500000
r's shape	shape	0.010204
of a dense vector	s	0.071429
full path of the dynamic	gof module	0.058824
-a inplace on	tensor neg inplace	1.000000
functiongraph listeners to help the navigator deal with	navigator optimizer attach updater fgraph	0.038462
of the last access	last access	0.040000
return connection pattern of subfgraph	compile op from graph connection pattern	0.076923
detect if the	gof gcc	0.027778
all sigmoid to ultra_fast_sigmoid	local ultra fast sigmoid node	0.200000
series of wrapper	wrap linker many linkers wrappers	0.047619
bit like make_loop but when only the	init_loop_orders olv_index dtypes inner_task	0.200000
nodes of the graph	gof	0.002381
node	d3viz	0.142857
to the	gpu	0.011765
default that removes all asserts from	remove all assert	0.055556
b axis=l) ->	local	0.014085
mflops	conv op flops	0.125000
the main diagonal set to a specified	tensor fill diagonal offset a val offset	0.100000
tensorvariable whose type	tensor as	0.066667
to the	gpuarray	0.023256
fastest	local meta optimizer	1.000000
into a canonical	get canonical	0.125000
respect to wrt, computes gradients	core subgraph	0.062500
respect to wrt, computes	subgraph grad wrt	0.062500
value after the import of	param init default	0.040000
add a optimizer	optimizer	0.062500
function to get	type get	0.050000
helper function to draw random integers	random integers helper random_state low high size	1.000000
know how often we go through some piece	name every	1.000000
inner nit_sot output	inner sitsot	0.083333
symbolic type representing a numpy ndarray	type	0.011905
constant scalar 0-d value	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
localoptimizer and applies them	local opt group	0.052632
offers to make itself the default python and	to os environ pathlist var newpath	0.038462
determine the broadcast pattern for advancedsubtensor	tensor adv index broadcastable pattern a idx	0.066667
the inner-most loop executes code	make reordered loop	0.111111
source code for this linker and	clinker compile cmodule location	0.038462
epoch of the last access	last access time path	0.040000
add some requirements to	add requirements	0.333333
string	patterns	0.166667
sample from	random_state size n	0.250000
of	gpulocal	0.055556
a list of shape	tensor shape feature default infer shape	0.066667
navigator	navigator optimizer attach updater fgraph	0.038462
sumsqr2dot	sumsqr2dot	1.000000
reduces a contiguous buffer	gpuarray inline reduce n buf pos count	0.142857
adding the	dict d1 d2	0.333333
dimensions of	dimshuffle	0.014493
spaces tabs	misc hooks	0.250000
symbol definition with an elementwise version	scal inplace symbol	0.333333
dot22 computing an outer-product -> ger	local dot22 to ger or gemv	1.000000
n (n	n	0.055556
the variable out for occurrences	out	0.018519
inserting broadcasted dimensions	tensor	0.006431
of lib directories that are	clinker header dirs	0.055556
input_storage	input_storage	0.833333
helper function for diagonalsubtensor and	tensor nnet get diagonal subtensor view x	0.083333
should return an	node name	0.033333
returning the output error and	misc output	0.066667
updates ordereddict the list of	updates	0.029412
transform is viewed as	inputs cost_grad	1.000000
function that gets a scan op a list	op not_required inputs	0.071429
associate linker with fgraph	gof op wise clinker accept fgraph no_recycling profile	1.000000
is the equivalent of localoptgroup for graphtogpu	to gpulocal opt group	0.055556
a != b	tensor neq a b	1.000000
with	idx	0.076923
a module	get module	0.200000
navigator deal with the	navigator optimizer attach updater fgraph	0.038462
as replace_all_validate	all validate remove fgraph	0.166667
disabled by default that removes all	remove all	0.166667
conda offers to	to os environ pathlist var newpath	0.038462
checkpoints	checkpoints	1.000000
the dimensions of this variable optionally	tensor	0.006431
log gamma function	gamma ln	1.000000
the hack in profiling to print the mflops	flops inputs	0.125000
called whenever node inputs[i] is changed	gof feature on change input function_graph node i	0.333333
return a new variable instance of type self	gof pure type make variable name	1.000000
inplace optimization that deals with allocempty this	gpuarray inplace allocempty op idx	0.166667
set of all variables which may share	infer reuse pattern	0.100000
mean value along the	mean	0.062500
setsubtensor(x x[idx], idx) -> x	local setsubtensor of constants node	0.250000
a rows x cols matrix implementing	sandbox dct matrix rows cols unitary	0.333333
existence of the __unify_walk__ method for one	gof unify walk a b u	0.037037
computes the output	op get output	0.047619
revert the replacement if the	replace validate replace all	0.050000
the output after pad_dims	gpuarray unpad dims output input leftdims	0.333333
ops contained within the	gof ops	0.083333
hack in profiling to print the mflops	flops	0.076923
parse the tree and figure out	scan_module traverse out x x_copy d	0.047619
that	gof wrap	0.083333
for diagonalsubtensor and	get diagonal subtensor view x i0 i1	0.083333
parse	nnet parse	0.500000
bitwidth	bitwidth	1.000000
a symbolic row variable	row	0.034483
confusion matrix	confusion matrix	0.166667
the cumulative sum of	tensor cumsum x	0.333333
mflops	gpuarray base gpu corr mm flops inp outp	0.125000
grad of this op could be very easy	l op	0.033333
calculate the function on the inputs and	node inputs	0.043478
one hot encoding of each element in y	to one hot y nb_class dtype	1.000000
symbolic scalar variable	scalar name dtype	0.166667
topooptimizer from the input nodes to output nodes	gof in2out	0.055556
a simple c snippet using current	march flag	0.250000
that respects the conventions imposed by python	theslice length	0.052632
return an instance of _maker which handles much	compile debug mode function maker i o m	0.066667
respect to the	dnn	0.060606
into a basic theano op	op itypes otypes infer_shape	0.047619
function for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor	0.083333
of compilation flags from config blas	libs flags libs_dir include_dir	0.052632
return the inputs required to	inputs variable_list blockers	0.058824
variant on wraplinker that	gof	0.002381
the provided l{functiongraph} it	gof optimizer apply	0.166667
node inputs[i] to new_r	graph change input node i new_r reason	0.500000
ndarray value	tensor	0.003215
return a copy of the type	tensor type	0.034483
return a c contiguous version of	gpu contiguous	0.083333
of headers that are needed by one or	clinker headers	0.047619
see 'theano tensor ptp'	tensor tensor py operators ptp axis	1.000000
graph of a compiled theano	fct	0.083333
the type's	gpuarray	0.023256
the output shape	get out shape	0.500000
an [advanced]incsubtensor[1], whose increment is an alloc	tensor local useless inc subtensor alloc node	0.166667
nodes in the original	outputs copy_inputs_and_orphans memo	0.029412
replace it with a triangular solve	linalg tag solve triangular	0.142857
map old node to new	check_integrity	0.090909
removes all from the clients list of	gof function graph remove client	0.200000
abs and rel error of	abs rel	0.166667
particular stream	tensor random streams getitem item	0.142857
sign of a	tensor sgn a	1.000000
step] transform it into a canonical form	canonical form	0.045455
code to run after c_code whether it failed	code cleanup node	1.000000
other implementation of mod	mod c code node name	0.125000
register a context	reg context	0.333333
shared	compile shared	0.166667
work for elemwise and gpuelemwise op	local elemwise fusion op op	0.200000
from an index array and a	a	0.008065
the "reverse-mode" gradient for the	grad perform node	0.083333
integers indicating the version	cache version apply node	0.125000
-1 and converts this to expm1 a	tensor local expm1	0.066667
sigmoid to	sigmoid	0.055556
>= b inplace on a	ge inplace a b	0.500000
vars	vars	1.000000
implements the "reverse-mode" gradient for	grad perform node	0.083333
for pushing out the variables	push out	0.037037
dimshuffle which only adds dimension	tensor local dimshuffle	0.052632
of this	tensor py operators	0.015625
gradient wrt filters	grad weights	1.000000
fgraph	fgraph no_recycling	0.200000
cache "filename" as	call cache persist filename	0.250000
execute callbacks calls getattr feature name (*args)	gof function graph execute callbacks name	0.500000
get the 0 based level	get	0.020833
to	to os environ pathlist var	0.038462
graph to ensure	graph	0.016393
function tries to recognize the updates	get updates	0.034483
sparse and a dense matrix	sd ccode	0.250000
tries to recognize the updates ordereddict the	get updates	0.034483
:param execute if true execute	execute execute verbose m	0.250000
the output dimensions of convolving	op get output	0.047619
function-constructor for graphs with	compile pfunc params outputs mode updates	0.200000
nkern	nkern	1.000000
reorder the dimensions	py	0.014286
the op	get op params	0.100000
duplicate this	gof	0.002381
the output error and	output	0.017241
more multinomial distributions defined by	tensor multinomial random_state	0.040000
2d filters	conv2d input filters	0.125000
input a 4-d tensor it sets all non	pool 2d same size input patch_size	0.166667
op	l op	0.033333
of this variable optionally	dimshuffle	0.014493
raise in self	core raise	0.100000
diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view x i0	0.083333
and should be removed	scan_module compress outs	0.076923
ignore_trees-related functionality	fgraph importer pruner chin	0.250000
turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	tensor nnet local softmax with bias	0.200000
optimization to the provided l{functiongraph} it	optimizer apply	0.166667
and reduce pattern has functioning	gpuarray gpu careduce cuda supports	0.166667
on	a	0.024194
elementwise [true] division inverse of	tensor true div a b	0.333333
a symbolic row variable (ndim=2	tensor row name dtype	0.050000
default output for this node	apply default output	0.250000
copies the stack trace from	stack trace	0.055556
the dimensions of this variable optionally inserting broadcasted	tensor tensor py	0.015873
provide	provide	0.500000
shape ==3 we todo	tensor nnet conv op perform node inp out	0.166667
of a cache directory and	from dir	0.125000
default that removes all asserts from the graph	remove all assert	0.055556
the topooptimizer from	gof in2out	0.055556
returning the output error	misc output	0.066667
op	op op	1.000000
profiling to print the mflops	flops inp outp	0.041667
the dimensions of x	x	0.008772
3d	nnet conv3d input	0.125000
apply_node recursively search from this node	apply_node	0.050000
is only used to determine the broadcast pattern	adv index broadcastable pattern a	0.066667
as replace_all_validate revert the replacement if the ops	gof replace validate replace all validate	0.111111
theano's operations compilation optimization execution	profile stats	0.250000
of headers that are needed by one	headers	0.038462
full path of	module	0.033333
within the op code	get op params	0.100000
of type	type call	0.500000
find broken optimizations	find bad	0.333333
removes all from the clients list of r	gof function graph remove client r client_to_remove	1.000000
copies the subgraph contained between i and o	gof clone i o copy_inputs	0.333333
legal	is valid	0.250000
legal value for a variable	is valid value a	0.076923
diagonalsubtensor	diagonal subtensor view x	0.083333
product of the specified pieces of vectors and	sparse block outer make node o	0.066667
slicing list of index	subtensor1	0.333333
computes the standard deviation along the	std	0.058824
same computations	computations	0.125000
"lifts" dimshuffle through elemwise operations	tensor local dimshuffle lift node	0.250000
complex-valued tensor from polar	tensor complex from polar	0.250000
pretty multiline string representating the cause of the	bad optimization str diagnostic	0.043478
generates the c code for corrmm (direction="forward"), corrmm_gradweights	tensor nnet base corr mm c code	0.090909
from a with or	size a replace	0.333333
wrt filters	weights	0.583333
if target is 'cpu' this will transfer to	tensor py operators transfer target	0.500000
some perform() or c_code() created a memory alias	bad view map	0.142857
pattern for advancedsubtensor	pattern	0.028571
use dnn conv workmem	dnn workmem workmem	0.166667
replace a crossentropysoftmax1hotwithbiasdx op whose incoming	useless crossentropy softmax 1hot with bias dx	0.111111
self _grad_op from user supplied form to type	from graph recompute grad	0.200000
an apply_node recursively search from this node	import apply_node check	0.066667
python object a that	gof pure	0.033333
depth	depth	1.000000
on all device we	device node	0.045455
op	clinker op c	0.250000
parse a	tensor nnet parse mul	0.500000
post some text to	post	0.100000
name	linker name	1.000000
pieces of vectors and	sparse block gemv make	0.066667
graphtogpu	graph to gpulocal	0.055556
match a variable with either of the	var	0.035714
a b), axis=0) -> elemwise{scalar op} a	tensor local reduce join	0.111111
for convolving a mini-batch of	input_shape filter_shape	0.027778
removes all asserts	tensor local remove all assert	0.055556
es of a tensor	dtype	0.022727
the shape	tensor shape	0.117647
while annotating	node thunk exc_info storage_map	0.250000
scalar variable	scalar	0.017857
delete unversioned dynamic modules	cache clear unversioned min_age	1.000000
this is	graph	0.016393
set the values of a shared variable	compile shared variable	0.083333
were declared by self	init	0.058824
vector to the diagonal of an empty matrix	alloc diag	0.027027
of collection for which predicate item is true	predicate coll	0.500000
proxy for either true_div or int_div depending	scalar div proxy	0.125000
content	content	1.000000
to wrt, computes gradients	core subgraph	0.062500
that unroll	code unroll	0.250000
indicating the version	version apply	0.125000
default output	apply default output	0.250000
a set of arrays to choose	tensor choose a choices out mode	0.200000
the grad of max pooling	max pool grad	0.333333
scan	scan_module scan	0.333333
similar behaviour as haskell's foldl	scan_module foldl fn sequences outputs_info non_sequences	1.000000
the alloc	alloc	0.012500
| b inplace on a	tensor or inplace a b	0.333333
operators to the basic variable class	tensor variable	0.166667
function builds the 1d	1d	0.090909
generate	generate	1.000000
to make an inplace	gpuarray inplace	0.200000
n-d	ws ignore_border stride	0.090909
the svd on cpu	svd a full_matrices compute_uv	0.200000
return a tensorvariable of this	make variable name	0.333333
a uniform	tensor uniform	0.125000
and apply nodes in the original	outputs copy_inputs_and_orphans memo	0.029412
by default that removes all asserts from the	tensor local remove all assert	0.055556
type	clinker type c code cache	1.000000
compiled module from the loaded	get module name	0.333333
this computes the outer product of	sparse block outer	0.047619
the idx list to get the	tensor get idx list	0.076923
calls subprocess_popen returning the output error and	output	0.017241
respect to the weights	gpu dnn	0.066667
name the object should be saved under	misc persistent ndarray id resolve name	0.500000
the folowing changes in the graph t	mul switch sink node	0.045455
complex conjugate of	tensor conj	0.250000
label of	label node	0.250000
:func neibs2images <theano sandbox neighbours neibs2images>	tensor nnet neibs2images neibs neib_shape original_shape mode	0.333333
the same on all device we do	device node	0.045455
a legal value for a variable	is valid value a	0.076923
convop that unroll the batch size	unroll batch kern d unroll_bsize unroll_ksize	0.125000
changes node inputs[i] to new_r	graph change input node i new_r reason	0.500000
helper function for diagonalsubtensor and	diagonal subtensor view	0.083333
turned into macros for	cop	0.028571
return the indptr field	csm indptr csm	0.333333
correlation implementation using matrix multiplication	corr3d mm	0.111111
l_node	l_node	1.000000
reproducible case for problems during theano	compile function dump filename inputs	0.166667
a set of 3d filters	tensor nnet conv3d input filters	0.142857
age_thresh_use	age_thresh_use	1.000000
axis=l) ->	local	0.014085
as its	compile as	0.050000
the dimensions of this variable optionally inserting broadcasted	dimshuffle	0.014493
is only used to determine the broadcast pattern	tensor adv index broadcastable pattern a	0.066667
to break aliasing of outputs	wrapped_inputs wrapped_outputs	0.166667
how to generate c	shape i c	0.250000
3d inputs with a set of 3d	conv3d	0.076923
persistent_id	persistent_id	0.833333
is	to gpulocal opt	0.055556
conda offers to make itself the default	to os environ	0.038462
function that will calculate	compile orig function	0.166667
1d kernel that can be used to	kernel 1d	0.050000
that unroll	conv code unroll	0.250000
in the forward but clip the gradient	grad clip x lower_bound upper_bound	0.250000
by doing a recursion over	tensor permute row elements rec	0.047619
adds new optimization instances	register	0.100000
for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x i0 i1	0.083333
rebroadcast how to	rebroadcast	0.111111
the complex conjugate of	tensor conj	0.250000
op	prod l op	0.033333
an apply_node recursively search from this	apply_node check reason	0.066667
dimensions of this variable optionally inserting broadcasted	tensor py operators	0.015625
function :func images2neibs <theano tensor nnet neighbours images2neibs>	tensor nnet images2neibs ten4 neib_shape neib_step mode	0.333333
sample from one or	random_state size n	0.250000
this function tries to recognize the updates	updates	0.029412
c code to initialize	clinker type c	0.083333
equivalent of	to	0.017544
important note this function	process node fgraph	0.142857
c code for gpucorrmm	gpu corr mm c code	0.090909
offers to make	to os environ pathlist var newpath	0.038462
returned by this	gof seq	0.200000
if the g++ version used	gof	0.002381
generate a diff to make code	diff code filename	0.333333
this apply instance in	gof apply	0.090909
from a multinomial distribution	streams multinomial	0.076923
return	get	0.020833
det x and there is already an	sandbox linalg local det	0.166667
an op that will be inserted	op	0.009174
remove are	remove reason	0.142857
spatio-temporal filters with a movie	filters signals_shape filters_shape	0.333333
function as its	compile as	0.050000
functiongraph that has ever been associated to self	gof	0.002381
the inverse fast fourier transform with real-valued output	gpuarray cuirfft inp norm is_odd	0.333333
stack trace for an	trace value	1.000000
newpath	newpath	1.000000
add	add node node	1.000000
from config blas ldflags	tensor ldflags	0.250000
other implementation of mod	mod c code node name inputs	0.125000
multiplication (inplace on a)	tensor mul inplace a	1.000000
respect to wrt,	core subgraph	0.062500
see theano tensor min	py operators min axis	1.000000
a memory alias that wasn't in	view	0.022727
the image shape of convolution gradweights	get conv gradweights shape 1axis image_shape	0.500000
for a convolution	gpu dnn conv get	0.200000
time icluding the time	times	0.100000
dimshuffle which only adds dimension to the left	tensor local dimshuffle	0.052632
first outdim-1 dimension size s of x the	flatten x ndim outdim	0.333333
the type optionally with a new dtype	tensor type clone dtype	0.333333
associated with a particular stream	tensor random streams setitem item val	0.142857
the shape s to previously un-shaped variable r	shape feature set shape r s override	0.500000
to the one	tensor to one	0.125000
ext	ext	1.000000
the outer nit_sot	last step used var scan_args	0.200000
instance	link_kwargs optimizer	0.333333
the apply to be inserted	apply node	0.031250
profiling to print the mflops	flops inputs outputs	0.125000
a symbolic	dtype	0.068182
elements obtained by iterating over	keepdims	0.105263
a signature object for comparing tensorconstant	tensor constant signature	0.100000
an empty matrix it does the	alloc	0.012500
transform a subgraph whose output	local optimizer transform	1.000000
graphtogpu	graph to gpulocal opt	0.055556
to the type's :attr context_name	type	0.011905
<= b inplace on a	le inplace a b	0.500000
generates the c code for gpucorrmm	gpuarray base gpu corr mm c code	0.090909
gist and return	gist	0.040000
return none or a tensorvariable whose type	tensor as	0.066667
given axis es of a tensor	axis ddof keepdims	0.083333
convert degree a to radian	tensor deg2rad a	0.333333
and also their apply_node if those nodes are	gof	0.002381
performs the svd on cpu	svd a full_matrices compute_uv	0.200000
dictionary mapping all symbolic variables in inputs	inputs	0.012658
ordereddict the list of outputs	outputs	0.045455
fill s v -> alloc(v	local fill to alloc	0.250000
functions raises a badviewmap exception when it detects	check viewmap node storage_map	0.111111
a dimshuffle which only adds dimension	local dimshuffle	0.052632
dictionary data structures	scan_module push out non seq	0.125000
a list of compilation flags	libs flags libs_dir include_dir	0.052632
from existing start gradients up to the end	grad wrt end start	0.166667
from the loaded cache	cache get	0.250000
the dimshuffle and index	local dimshuffle	0.052632
:param execute if true execute a theano	misc execute execute verbose	0.250000
add an	add	0.034483
returns a module if the	gof module	0.058824
:param execute if true execute	misc execute execute verbose	0.250000
an unification in u and uses	o u	0.037037
topooptimizer from the input nodes	in2out	0.043478
return a symbolic vector variable	tensor vector name dtype	0.166667
numpy's isclose on	tensor isclose a	0.500000
find	find	0.750000
2d kernel that can be used	kernel 2d	0.050000
list of localoptimizer and applies them	local opt group	0.052632
half by b with	b	0.014925
low	low	0.750000
respect	dnn	0.060606
execute callbacks calls getattr feature name (*args) for	gof function graph execute callbacks name	0.500000
that removes all asserts from the	local remove all assert	0.055556
slicing list	subtensor1	0.333333
six moves	module six	0.260870
takes as	signal pool 2d	0.142857
scan	scan output	0.125000
convert addsd to faster addsd_ccode	sparse local addsd	0.250000
the apply to be inserted in the struct	struct node	0.062500
move	move	1.000000
if those nodes are not in this	gof function	0.043478
variable optionally	tensor py operators	0.015625
dependence of nodes in	dependence	0.035714
fill s v -> alloc(v shape s	tensor local fill to	0.250000
one	to one	0.125000
convolve spatio-temporal filters with a movie	conv3d signals filters signals_shape filters_shape	0.333333
type c type numpy typenum that corresponds to	tensor type dtype specs	0.071429
deepcopy in the fgraph to break aliasing	deepcopy fgraph wrapped_inputs wrapped_outputs	0.500000
function that gets a scan op a	op not_required	0.071429
return a code	node name	0.066667
diagonalsubtensor and	tensor nnet get diagonal subtensor view x	0.083333
of minimum elements obtained by iterating	keepdims	0.052632
the graph and get	graph	0.016393
the inner graph to	scan_module scan validate inner graph	0.035714
svd of	svd	0.034483
compile lock to	cache add to	0.142857
an alloc of a scalar variable or one	alloc node	0.037037
defining the gradient the finite fourier	tensor fourier grad	0.250000
this variable	variable	0.022222
a b), axis=0) -> elemwise{scalar op} a	local reduce join node	0.111111
inverse complementary error function	tensor erfcinv a	1.000000
specific to the apply	apply node	0.031250
configparam	configparam	0.833333
hyperbolic arc sine of a	tensor arcsinh a	1.000000
the specified pieces of vectors and matrices	sparse block gemv make	0.066667
apply to be inserted in the struct	struct node	0.062500
the value after the import of theano	param init default filter	0.040000
triangular	triangular node	0.125000
argmax	argmax axis keepdims	1.000000
runs a series of wrapper functions instead of	wrap linker many linkers wrappers	0.047619
a dictionary of arguments to pass to helper_c_code	helper c code args	0.250000
x the	flatten x	0.166667
optimizer for pushing out the variables inside the	push out	0.037037
output dimensions of convolving	get output	0.047619
match a variable with the -x pattern	is neg var	0.166667
of the last access of a given	last access time	0.040000
value has a	value trace	0.333333
true if the named module is a package	six meta path importer is package fullname	0.250000
connection pattern of a	connection pattern	0.032258
lock to be held	add to cache module key module_hash	0.166667
this is the equivalent	graph to	0.055556
idx_list with constant inputs replaced by their	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
to wait on a previously sent array	mpisend wait	0.045455
for diagonalsubtensor and	get diagonal subtensor view	0.083333
filters	signals filters	0.111111
the list remove are still in the	replacements remove reason	0.055556
connection pattern of	compile op from graph connection pattern	0.076923
scale each	scale x	0.500000
apply instance in a new	apply clone with new	0.500000
reshapes the output after pad_dims	unpad dims output input	0.333333
this compiles the source	compile cmodule location	0.038462
that	gof linker make	0.250000
fgraph check that 1) this	gof	0.002381
removes useless	useless	0.076923
maps	equiv	0.166667
try to detect a	detect	0.090909
like zeros_like but forces the object	core float zeros like x	0.200000
nested loop	loop	0.027778
maintain order	searchsorted x v side sorter	0.142857
remove subtensor/advancedsubtensor1 if it	useless subtensor node	0.200000
!= b inplace on a	neq inplace a b	0.500000
symbolic row variable (ndim=2 broadcastable=[true	tensor row name dtype	0.050000
the c code for corrmm (direction="forward"),	nnet base corr mm c code	0.090909
by re-writing the file containing the owner's	gof refresh	0.125000
dot a	dot	0.035714
transferred to	transfer	0.058824
baddestroymap	node storage_map r_vals	0.166667
and dictionary data structures	non seq scan	0.090909
this function performs the svd on cpu	tensor svd a full_matrices compute_uv	0.200000
a config string (comma-separated key=value components) into	parse config string config_string issue_warnings	0.166667
the dependence of nodes in a graph	make dependence	0.043478
op}(join(axis=0 a b), axis=0) -> elemwise{scalar op}	local reduce join	0.111111
returns upper bound on	bound	0.043478
graph and get a memo a dict	graph	0.016393
create a new random stream	tensor random streams	0.142857
will print a warning message on the	gof deprecated filename msg	0.041667
optimization disabled by default that removes all	remove all	0.166667
the inner graph	validate inner graph	0.035714
function :func neibs2images	tensor nnet neibs2images	0.333333
the inputs and put	gof pure op perform node inputs output_storage params	0.047619
some perform() or c_code() modified an input	map	0.047619
a failure code to the task that	gof cthunk find task	0.142857
a leaf	leaf	0.066667
the bartlett spectral window in the time-domain	bartlett	0.058824
of type	type	0.023810
data by walking the cache directory	gof module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
not for stability	fast scalar	1.000000
of aliasing	destroy	0.009709
replace it with logsoftmax	logsoftmax	0.076923
lock	lock	0.600000
b axis=l) ->	tensor local	0.025641
the op code	op params	0.100000
str of	to str t	0.166667
of the specified pieces of vectors and matrices	sparse block gemv make node o w	0.066667
functiongraph attach_feature the method that attaches	gof bookkeeper on attach fgraph	0.142857
dnn conv workmem	safe no dnn workmem workmem	0.166667
a triangular	triangular	0.076923
tensor variable r	r	0.028571
ignore all errors	optimizer warn ignore	1.000000
2d filters	nnet conv2d input filters	0.125000
toposort	function graph toposort	0.125000
/	div	0.083333
3d	nnet conv3d	0.071429
conda offers to make itself the default	to os environ pathlist var	0.038462
in profiling to print the mflops	nnet conv op flops	0.125000
does not support the types involved	tensor inc subtensor do type checking	0.142857
print the mflops	base gpu corr mm flops inp outp	0.125000
type c type numpy typenum that corresponds to	type dtype specs	0.071429
this mode	compile monitor mode	0.333333
and	and	0.777778
of minimum elements obtained by iterating over	keepdims	0.052632
exception some perform() or c_code() created a memory	map	0.047619
convolve spatio-temporal filters	signals filters	0.111111
a and b are	a b	0.066667
logsoftmax x	nnet local logsoftmax node	0.142857
neib_step	neib_step	1.000000
legal value for a variable of this	is valid value a	0.076923
dot product	true dot x	0.166667
one or more multinomial distributions defined by one-dimensional	multinomial	0.024390
optionally inserting broadcasted	tensor tensor py	0.015873
in a new graph	clone with new inputs	0.166667
start gradients up to the end variables of	grad wrt end start	0.166667
apply nodes according	gof sort apply nodes inputs outputs cmps	0.050000
the end	grad wrt end	0.050000
a tensorvariable whose type is in t float_scalar_types	tensor as scalar res dtype	0.500000
the c code for gpucorrmm (direction="forward"), gpucorrmm_gradweights	base gpu corr mm c code	0.090909
output	output input	0.333333
instance of _maker which handles much	maker i o m	0.066667
fill s v -> alloc(v shape	tensor local fill to alloc	0.250000
a new variable to theano config	config var name	0.500000
^	xor	0.125000
mrg stream	mrg random streams	0.033333
compute conv	tensor nnet conv3d	0.142857
"reverse-mode" gradient for the	grad perform node inputs outputs	0.083333
of headers that are needed	headers	0.038462
return path to the module file	gof key data get entry	1.000000
batch normalization	nnet batch normalization	0.125000
variables	variables and	0.250000
convop that unroll the batch size	code unroll batch kern d unroll_bsize unroll_ksize	0.125000
optimizer for pushing out the variables inside	push out	0.037037
optional return true for small or builtin c	c is simple	0.200000
expression	x	0.008772
a config string (comma-separated key=value components) into	config string config_string issue_warnings	0.166667
the output	output input leftdims rightdims	0.333333
apply instance in a new graph	apply clone with new inputs inputs strict	0.500000
update self rstate to be	rstate	0.090909
the inputs	inputs	0.050633
directory and return full path of the dynamic	module name from dir	0.071429
two kinds of useless reshape	tensor local useless reshape	0.200000
matrix a and	structured	0.071429
with a triangular solve	tag solve triangular node	0.142857
the dimensions of this variable	py operators	0.015625
to help the navigator	navigator optimizer	0.037037
x the same	x	0.008772
-1 and converts this to expm1	local expm1	0.066667
broadcasted	tensor tensor py operators dimshuffle	0.019231
dot product followed by a	dot modulo	0.250000
values from a with or	size a	0.333333
uses shared variable names when persisting	persistent shared variable id	0.142857
all sigmoid to ultra_fast_sigmoid	local ultra fast sigmoid	0.200000
to the fgraph outputs that will	fgraph expanded_inputs	0.058824
numpy-compatibility method if x is a matrix	diag x	0.200000
litterals	tensor	0.003215
of the last access of	last access time path	0.040000
have a stack	check stack	0.142857
1d kernel that	kernel 1d	0.050000
perform the permutation by doing a recursion over	permute row elements rec perform node	1.000000
wait on a	wait	0.045455
order a graph of apply nodes	gof sort apply nodes	0.200000
dnn	core safe no dnn	0.125000
two kinds of useless reshape	useless reshape	0.200000
return a list of shape tuple or	shape	0.010204
return full path of the	gof module name from	0.076923
diff to make code correctly indented	misc hooks get correct indentation diff code filename	0.333333
instance of _maker which handles much of the	compile debug mode function maker i o m	0.066667
inner-most loop	make reordered loop	0.111111
compute sum of non nan / inf values	constant signature get sum	0.142857
a thunk that is a	thunk	0.021277
the equivalent	gpulocal	0.055556
nodes according to	nodes inputs outputs cmps	0.166667
rtol	rtol	1.000000
lists/tuples/other objects into a list	compile flatten l	0.200000
conda offers to make itself the default python	to os environ pathlist var newpath	0.038462
type c type numpy typenum that corresponds	type dtype specs	0.071429
for convop that unroll the batch	conv code unroll batch kern d unroll_bsize unroll_ksize	0.125000
the replacement if	validate replace	0.050000
the value after	init default filter	0.040000
"reverse-mode" gradient for the eigensystem of	tensor eigh grad perform node	0.333333
are not required anymore and should be removed	compress outs	0.076923
half to	half to	1.000000
foldl	foldl	1.000000
can't change the value after the	default	0.030303
to the subtensor and its idx_list reorders	idx_list get_count	0.090909
proxy	tensor div proxy	0.125000
mrg	mrg	0.461538
the forward but clip the gradient	core grad clip x lower_bound upper_bound	0.250000
object but we don't clone the data	constant clone	0.250000
top	top	1.000000
false we can't change the value after the	param init default	0.040000
new graph	with new inputs	0.166667
respect to wrt, computes gradients of	subgraph grad wrt	0.062500
because of aliasing	destroy	0.009709
none that need not be checked for nan	is numeric value arr var	0.166667
remove two kinds of useless	tensor local useless	0.111111
perform the permutation by doing a recursion over	permute row elements rec perform	1.000000
which each row is a mrg stream state	mrg random streams	0.033333
a basic theano op that will call the	op itypes otypes infer_shape	0.047619
contents of a cache	dirname err files	0.083333
performs batch normalization of the given	nnet batch normalization	0.125000
return a symbolic vector	tensor vector name dtype	0.166667
a list of localoptimizer	opt group	0.043478
true_dist	true_dist	1.000000
of aliasing and destructive operations	destroy	0.009709
with helper_c_code	gpu inc subtensor get helper c code	0.333333
function computes the output shape	out shape ishape kshape border_mode	0.500000
reorder the dimensions of this variable optionally inserting	tensor tensor py operators dimshuffle	0.019231
outdim	outdim	0.857143
see theano tensor argsort	tensor py operators argsort axis	1.000000
transfer to	py operators transfer	0.125000
returns upper bound on the largest	bound	0.043478
variable optionally inserting broadcasted	tensor py	0.015873
of dimensions from the shape	shape	0.010204
tell specifyshape how to generate c code for	specify shape c code typ code version c_support_code_apply	1.000000
gpucorrmm (direction="forward"),	corr mm	0.083333
which can be referred to	compile register	0.200000
of the last access of	last access time	0.040000
the inner-most loop	reordered loop	0.111111
the inner	scan_module scan validate inner	0.142857
of integers indicating the version	cache version apply node	0.125000
node	node output_indices	0.142857
the grad of average pooling	average pool grad	0.200000
misc	misc	0.714286
compute 1d kernel	nnet	0.016129
the specified pieces of vectors and matrices	sparse block outer make node	0.066667
wrapper for numpy argsort function	arg sort op	1.000000
and	local	0.042254
the tensor operators	tensor	0.006431
radius	radius	1.000000
install some functiongraph listeners to help the navigator	navigator optimizer attach	0.038462
c code to initialize the	type c	0.071429
helper function to generate permutations	permutation helper random_state	0.333333
inputs and put the variables	pure op perform node inputs output_storage params	0.047619
multiplication tree	mul tree	0.250000
c_cleanup that	gof	0.002381
output type dtype and broadcast there is no	local useless alloc node	0.333333
important note this function	out seq scan process node fgraph	0.142857
unroll	gen conv code unroll	0.250000
kinds scalar constants and the rest	rest inputs elemwise only_process_constants	0.125000
named module is a package	compat six meta path importer is package fullname	0.250000
node inputs[i]	gof function graph change input node i	0.250000
object a that would be	gof pure	0.033333
connection pattern of a subgraph defined by	gof io connection pattern	0.055556
bug_print	bug_print	1.000000
is a mrg stream state and they	mrg random streams	0.033333
var has an unification in u and uses	o u	0.037037
time on thunks	time	0.285714
and dictionary data structures	push out non seq scan	0.125000
return a tuple of integers indicating the version	c code cache version apply	0.125000
xs	xs	1.000000
important note	push out seq scan process node fgraph node	0.142857
:func images2neibs <theano tensor	images2neibs	0.111111
function is a thunk	thunk	0.021277
function to get the 0 based level of	get	0.020833
more multinomial distributions defined	tensor multinomial random_state	0.040000
the given axis es of a tensor	axis dtype keepdims	0.083333
previously	mpirecv	0.037037
product of the specified pieces of vectors and	sparse block gemv make node o	0.066667
along the given axis es	axis ddof keepdims	0.083333
return full path	gof module name	0.076923
override	override	0.714286
the dimensions of this variable optionally inserting broadcasted	tensor py operators dimshuffle	0.019231
the fgraph	fgraph	0.024390
variable optionally inserting broadcasted	operators dimshuffle	0.019231
value after	default	0.030303
new instance of this mode	monitor mode clone link_kwargs optimizer	0.333333
a view	view	0.022727
can't change the value after	init default filter	0.040000
to sharedvariable instances of suitable dummy values	provide	0.100000
the input specs and the output specs	input_specs output_specs accept_inplace	0.142857
the specified pieces of vectors and	sparse block gemv make	0.066667
raise baddestroymap	storage_map r_vals	0.166667
different	tag	0.166667
to recognize the updates	scan_module get updates	0.034483
of tensortype	tensor tensor type	0.041667
a new graph	clone with new inputs	0.166667
implement the same rounding than numpy round half	round half	0.100000
merge multiplication by a scalar on	gpuarray alpha merge	0.076923
variables within input	gof variables	0.125000
distribution defined by probabilities	sandbox mrg random streams	0.033333
new graph	new inputs	0.166667
package	package	0.857143
es of a tensor	dtype op	0.250000
to the	gpu array	0.055556
be removed and	scan_module compress outs	0.076923
perform the permutation by doing a	perform node x	0.166667
more multinomial distributions defined by one-dimensional slices	multinomial random_state	0.040000
hack in profilemode to print the mflops	gpuarray base gpu corr3d mm flops inp outp	0.125000
c code for corrmm	tensor nnet base corr mm c code	0.090909
new	clone with new inputs inputs	0.166667
will call the supplied function as	as	0.024390
a file	file	0.125000
to help the navigator deal with the	navigator optimizer attach updater fgraph	0.038462
to roll tensortypes	roll x shift	0.250000
a dimshuffle which only adds dimension to	local dimshuffle	0.052632
x y -> x	tensor local	0.025641
the dimensions of this	dimshuffle	0.014493
that wasn't	view	0.022727
use function only avail on compute capability 2	dev20	0.166667
search paths	dirs	0.071429
output	conv op get output	0.047619
buf	buf	1.000000
raise baddestroymap if	compile check inputs node storage_map r_vals	0.166667
out the variables inside	push out	0.037037
reorder the dimensions of this variable optionally inserting	py	0.014286
this type	type	0.023810
a function into a basic theano	itypes otypes infer_shape	0.142857
litterals	make constant	0.100000
minimum in	tensor minimum x	0.142857
the contents of a cache directory and	dir dirname err files	0.166667
spatio-temporal filters	conv3d signals filters	0.111111
all variables which may share the same underlying	infer reuse pattern	0.100000
nodes that	gof	0.002381
the value after the	param init default	0.040000
list of outputs and the	and outputs	0.100000
the specified pieces of vectors and	sparse block gemv make node	0.066667
x y	x y	0.048780
graph of apply	gof sort apply	0.200000
if it takes the	node	0.007407
op	op c code cache	1.000000
any python object a that would be	gof	0.002381
sparseblockouter see sparseblockouter's docstring	sparse block outer	0.047619
reduce pattern has functioning c code	gpuarray gpu careduce cuda supports c code	0.250000
convolution gradient with respect to the	dnn conv grad	0.125000
merge multiplication by a scalar on	alpha merge	0.076923
theano graphs represent the	xs ys in_xs in_ys	0.111111
with respect to wrt, computes	core subgraph grad wrt	0.062500
same rounding than numpy round	round	0.076923
and the rest	rest inputs elemwise only_process_constants	0.125000
and t is a	node	0.007407
to expm1 a	tensor local expm1	0.066667
the l operation on f wrt to wrt	core lop f wrt	0.200000
of scalars together	make	0.017857
removes all from the clients	gof function graph remove client	0.200000
baddestroymap if necessary update dr_vals	check inputs node storage_map r_vals dr_vals	0.250000
ceil of a (inplace on a)	tensor ceil inplace a	1.000000
convolution gradient with respect to	gpu dnn conv grad	0.125000
can't change the value after the import of	param init default filter	0.040000
broadcast pattern for advancedsubtensor output	pattern a	0.066667
august 2011	tensor load shared variable val	0.142857
initializes py_name to	r name sub	0.250000
pieces of vectors and matrices	sparse block gemv make	0.066667
x*x -> sqr	local mul to sqr node	0.166667
optional return	name inputs	0.500000
value after	init default filter	0.040000
of apply nodes	sort apply nodes	0.200000
feature should	gof feature on	0.200000
convert degree	deg2rad	0.111111
only used to determine the	adv index broadcastable	0.050000
can't change the value after	init default	0.040000
compiled module from the loaded cache or	module cache get module name	0.166667
random stream	mrg random streams	0.033333
series	wrap linker many linkers wrappers	0.047619
to construct a variable with a sparse matrix	sparse variable	0.250000
item	item	0.384615
runs a series	linker many linkers wrappers	0.047619
an alloc of a scalar variable or	alloc node	0.037037
function to get the 0 based	type get depth	0.050000
to be held	cache add to cache module key module_hash	0.166667
input a 4-d tensor	max pool 2d same size input patch_size	0.166667
triangular solve	tag solve triangular	0.142857
function	random_state low high size	0.500000
change the value after the import of	config param init default	0.040000
function for diagonalsubtensor	diagonal subtensor	0.083333
op and reduce pattern has functioning c code	careduce cuda supports c code inputs	0.250000
low and	random_state size low	1.000000
elemwise tan of x	sparse tan x	1.000000
operation on f wrt to wrt	lop f wrt	0.200000
nodes according to a	nodes inputs outputs cmps	0.166667
of neural-net classifiers	with bias	0.166667
filters	tensor nnet conv3d signals filters	0.111111
the dimensions of this variable optionally inserting	tensor tensor py operators dimshuffle	0.019231
maps old nodes to new ones parameters	equiv check_integrity attach_feature	0.200000
check if converting to type2 from	type2	0.050000
be turned into macros for	cop	0.028571
a stack	check stack	0.142857
wrt,	subgraph grad	0.062500
a mini-batch of a stack	input_shape filter_shape	0.027778
inputs according	inputs	0.012658
list of l{codeblock} instances returns a string	code gen blocks	0.050000
load a file that was dumped to a	load f persistent_load	0.333333
with a set of 3d	conv3d input	0.125000
for same	tensor	0.006431
op without affecting	op	0.009174
with a movie	signals_shape filters_shape	0.333333
exception while annotating	thunk exc_info storage_map	0.250000
maps from variable and apply nodes	get equiv inputs	0.142857
replace_all_validate revert the replacement if the	gof replace validate replace all validate remove	0.111111
source code	gof clinker compile cmodule location	0.038462
a class with a metaclass	add metaclass metaclass	0.125000
zeros_like but forces the object	core float zeros	0.166667
grad of this op could be very easy	prod l op	0.033333
of order v	v x	0.100000
__unify_walk__ method for	unify walk a b u	0.037037
comparator to represent the dependence	gof make dependence cmp	0.111111
variable optionally inserting	tensor tensor py	0.015873
on wraplinker that runs a series of	gof wrap linker many linkers wrappers	0.071429
cpu correlation implementation using matrix multiplication	corr mm	0.083333
module component with similar interface	streams	0.153846
by finite difference method raise error on failure	core verify	1.000000
the specified pieces of vectors	sparse block gemv make	0.066667
see theano tensor sum	py operators sum axis dtype	1.000000
exception some perform() or c_code() created a	map	0.047619
of each value in array of ints	tensor bincount x weights minlength assert_nonneg	0.125000
in u and uses it instead	o u	0.037037
the op	stats op	0.500000
function to get the 0 based level of	type get depth	0.050000
main diagonal set to a specified scalar value	fill diagonal offset a val offset	0.100000
is unified to boundvariable(other_object)	fv o u	0.200000
global optimizer for pushing out the variables inside	push out	0.037037
node inputs[i] to new_r	graph change input node i new_r	0.500000
post some text to a gist and return	post gist	0.333333
a variable with	variable x name	0.083333
with constant	tensor subtensor get constant idx	0.250000
source code for this linker	cmodule location	0.038462
of compilation flags from config blas ldflags	tensor ldflags libs flags libs_dir include_dir	0.333333
the value after	param init default filter	0.040000
label of	label	0.111111
we do not try to use complex numbers	tensor mod check x y	0.166667
constant with value x	constant x	1.000000
reshapes the input to a leftdims + rightdims	input leftdims rightdims	0.333333
the inner-most loop executes	make reordered loop	0.111111
a new graph	clone with new inputs inputs strict	0.166667
the connection pattern of a subgraph	gof io connection pattern	0.055556
of this variable optionally inserting	operators	0.017241
values from a with or without replacement	random streams base choice size a	0.333333
item from the cache	call cache	0.200000
headers that are needed by one	clinker headers	0.047619
on f	core rop f	0.166667
return a list of shape tuple or	feature default infer shape	0.066667
tuple python type c type numpy typenum that	tensor tensor type dtype specs	0.071429
given shape and flags	signal pool out shape imgshape ws ignore_border stride	0.200000
of x default reverse them	transpose x	0.200000
dimensions of	py	0.014286
helper_c_code	tensor inc subtensor get helper c code	0.250000
feature should remove any dynamically-added	gof feature on	0.200000
of sparseblockouter see sparseblockouter's	outer	0.083333
b axis=l) -> sum(a axis={ }) / b	tensor local sum prod div dimshuffle node	0.333333
ops when those ops do implicit upcasting anyway	tensor local upcast	1.000000
data structures	non	0.071429
return c code to extract a	type c extract name	0.250000
upsampling this function builds the 1d	1d	0.090909
a sparse format	sparse	0.019231
modulo of m1 and the second	m1	0.027027
ultra_fast_sigmoid	nnet local ultra fast	0.500000
time icluding the time for	times	0.100000
tree and figure out which	scan_module traverse out x x_copy d	0.047619
for graphtogpu	gpulocal opt	0.055556
that wasn't in	view	0.022727
4-d tensor it sets all non maximum values	patch_size	0.050000
if the alloc would be useless	tensor alloc call	0.333333
compute	compute	1.000000
symbolic constant with value x	tensor constant x name ndim dtype	0.333333
elemwise degree	sparse deg2rad x	0.333333
new variable instance of type self	gof pure type make variable	0.333333
incsubtensor when we overwrite	tensor local useless inc subtensor node	0.066667
solve operation c =	solve	0.032258
c_extract_out	get c extract out	1.000000
reshape	reshape	1.000000
object a that would be	gof	0.002381
is_odd	is_odd	1.000000
into a basic theano	itypes otypes infer_shape	0.142857
a module if	gof module	0.058824
values of a shared variable	compile shared variable	0.083333
of mod	mod c code node name inputs outputs	0.125000
the graph leading to r to given depth	r prefix depth done	0.500000
-> x	tensor local	0.025641
the ignore_trees-related functionality	optimizer attach updater fgraph importer pruner chin	0.250000
'valid' generates output only when kernel and	mode	0.062500
not the other implementation of mod	scalar mod	0.125000
all intermediate variables	of variables fgraph input_shapes	0.250000
for graphtogpu	graph	0.016393
op	get op	0.100000
for pushing out the variables inside	push out	0.037037
this node	gof node	1.000000
number to string by rendering	number number	0.125000
compute sum of non nan	signature get sum	0.142857
last access of a	last access time	0.040000
replace element i of shape_of[r] by s_i	i s_i	1.000000
series of wrapper functions instead	many linkers wrappers	0.047619
create a new random	random	0.055556
type's	gpu array type	0.062500
ints	x weights minlength assert_nonneg	0.125000
that has ever been associated to	gof	0.002381
updates	get updates	0.034483
diagonalsubtensor	get diagonal subtensor view x i0	0.083333
:func neibs2images <theano sandbox neighbours neibs2images>	neibs2images neibs neib_shape original_shape mode	0.333333
overwrite the full inputs with the new	useless inc subtensor	0.125000
with constant inputs replaced by their python	constant idx inputs allow_partial only_process_constants elemwise	0.071429
convert data to something which can be associated	tensor tensor type filter data strict allow_downcast	1.000000
the fgraph this is the place to	fgraph	0.012195
inserted at struct	struct	0.047619
of mod	mod c	0.125000
changes node inputs[i] to new_r	change input node i new_r reason	0.500000
with constant inputs	tensor subtensor get constant idx inputs	0.250000
type's	array	0.041667
equivalent	opt	0.043478
subclass	tensor	0.006431
tree	tree	0.750000
defining the gradient the finite fourier	fourier grad	0.250000
returns the connection pattern of	io connection pattern	0.055556
a memory	bad	0.013158
choice	choice	0.750000
tries to recognize the updates	scan_module get updates	0.034483
for diagonalsubtensor and	diagonal subtensor view x i0 i1	0.083333
return a code string specific to the apply	apply node name	0.500000
revert the replacement	replace	0.032258
parses a config	core parse config	0.333333
as replace_all_validate revert the replacement if	replace validate replace all validate	0.111111
type's :attr context_name	array	0.041667
of useless reshape	useless reshape node	0.200000
separated maker	share_memory swap delete_updates	0.500000
variant on wraplinker that runs a series of	gof wrap linker many linkers wrappers	0.071429
unversioned	unversioned	1.000000
symbolic row	row name dtype	0.050000
perform the permutation by doing a recursion over	permute row elements rec perform node x	1.000000
val	val	0.750000
x // 1 -> x	tensor local intdiv by one node	1.000000
for	ultra fast	0.333333
the graph that compute the variable out for	out	0.018519
return a reshaped view/copy of this variable	tensor tensor py operators reshape shape ndim	0.111111
a view	compile view tree set	0.500000
following stats	global stats	0.500000
be removed and	outs	0.050000
print the	compile print	0.500000
with a particular	item	0.076923
see theano tensor prod	tensor py operators prod axis dtype	1.000000
have the same shape	same shape	0.333333
uniform distribution	uniform random_state	0.125000
first_item	first_item	1.000000
replace it with logsoftmax x	local logsoftmax node	0.142857
the kernel shape of convolution gradweights	get conv gradweights shape image_shape	0.500000
c code for corr3dmm (direction="forward"), corr3dmm_gradweights	corr3d mm c code	0.090909
apply as many times as required the	tensor apply	0.142857
connection pattern of subfgraph defined by	graph connection pattern node	0.076923
the basic constant class	constant	0.016667
grad and replace it with logsoftmax x 's	local logsoftmax	0.076923
call the supplied function as its	as	0.024390
original graph to	outputs copy_inputs_and_orphans memo	0.029412
(direction="backprop weights"), and	helper bottom weights top direction	0.166667
can't change the value after the import of	core config param init default	0.040000
a triangular solve	solve triangular	0.142857
see theano tensor argsort	tensor tensor py operators argsort axis kind order	1.000000
the dimensions of this variable optionally inserting	tensor py operators dimshuffle	0.019231
compiles the source	compile cmodule location	0.038462
on the inner graph to	scan validate inner graph	0.035714
==3 we todo	tensor nnet conv op perform node inp out	0.166667
as replace_all_validate revert the replacement if the	validate replace all validate remove	0.111111
connection pattern of	graph connection pattern node	0.076923
two matrices at least one of which is	x y	0.024390
grad of max pooling	max pool grad	0.333333
properties	properties	1.000000
function expects the compile lock to be held	to cache module key module_hash	0.166667
the gradient the	grad	0.010417
graph and get a memo	graph	0.016393
start	start	0.240000
in a sparse format instead of dense	core sparse	0.066667
and the second half by b	b	0.014925
raise baddestroymap if	check inputs node storage_map r_vals	0.166667
of this op could be very	tensor prod l op	0.033333
the "reverse-mode" gradient for the eigensystem	eigh grad perform	0.333333
with a	compat with	0.500000
convolution gradient with respect to	dnn conv grad i	0.125000
unroll the	unroll	0.111111
this function uses set and dictionary data structures	out non seq	0.125000
consider an expression	x	0.008772
offers to make itself the default python and	to	0.017544
add the tensor operators to	tensor	0.006431
a uniform	tensor uniform random_state size	0.125000
by a with a modulo of m1 and	m1	0.027027
and dictionary data structures	out non seq	0.125000
for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor	0.083333
removes all from the clients	function graph remove client	0.200000
of given shape and flags	grad out shape imgshape ws ignore_border stride	0.200000
test	n_tests rng	0.500000
respect to wrt, computes gradients	core subgraph grad wrt	0.062500
see theano tensor sum	py operators sum axis dtype keepdims	1.000000
with the same shape and dtype as the	dtype	0.022727
flag	flag	1.000000
gradients up to the end variables of a	grad wrt end	0.050000
function is only used to determine the	tensor adv index broadcastable	0.050000
function as its implementation	compile as	0.050000
exception some perform() or c_code() modified	bad destroy map	0.142857
contexts	contexts	1.000000
convolution for debugmode	tensor nnet base abstract conv conv	0.125000
to	gpuarray	0.093023
incsubtensor1	incsubtensor1	1.000000
square of a	tensor sqr a	1.000000
changes node inputs[i] to	graph change input node i	0.250000
drop broadcastable dimensions scrap the dimshuffle and index	local dimshuffle	0.052632
converts number to string by	from number number	0.142857
node i pairs such that node inputs[i]	gof function graph clients	0.100000
represents an array	array	0.041667
to be raised by functions defined as part	method not defined	0.333333
of outputs	outputs	0.045455
the type that	type	0.011905
of this variable optionally inserting broadcasted	operators dimshuffle	0.019231
important note this function uses set	seq scan process node	0.142857
the value after the import	default filter	0.040000
modified bessel function	i0 inplace x	1.000000
replace a crossentropysoftmax1hotwithbiasdx op whose incoming gradient is	crossentropy softmax 1hot with bias dx	0.111111
to generate c	c	0.071429
with some variables	scan_module map variables	1.000000
dfs traversal and chooses the orphans among them	and orphans	0.166667
or more multinomial distributions defined by one-dimensional slices	tensor multinomial	0.037037
kind of order v	v	0.022222
operation to wait on a previously received	mpirecv wait	0.045455
add two matrices at least one of	add x y	0.333333
variables inside the scan that depend only on	seq scan	0.250000
from the cache if available	call cache call fn args key	0.200000
advancedincsubtensor1(x x[ilist]+other ilist set_instead_of_inc=true)	set to inc subtensor	1.000000
important note this	scan_module push out seq scan process node fgraph	0.142857
that	gof	0.066667
variables	variables	0.434783
code to pack c types back into a	clinker type c sync	0.111111
topooptimizer from	gof in2out	0.055556
a thunk	thunk	0.042553
for a	gpu dnn	0.066667
gradient wrt filters for abstractconv	abstract conv grad weights	1.000000
function :func neibs2images	neibs2images	0.125000
variable out for occurrences of	out	0.018519
copies a vector to the	alloc	0.012500
return connection pattern of subfgraph defined by inputs	from graph connection pattern node	0.076923
to initialize the variables that	gof	0.002381
hash equal for same kinds	hash	0.055556
all device we do it only	device node	0.045455
return a tuple of integers indicating the version	object c code cache version	0.125000
to draw random integers	tensor random integers	0.500000
val [x y] -> alloc(val[ ])	tensor local subtensor of	1.000000
the task that is associated	gof cthunk find task failure_code	0.083333
headers that are needed by one	headers	0.038462
of given shape and flags	signal pool out shape imgshape ws ignore_border stride	0.200000
a ==	tensor eq a	1.000000
dnn	dnn	0.212121
v raises attributeerror if there is	v	0.011111
the output of neural-net multiclass classifiers	softmax with bias	0.142857
an input that	bad	0.013158
this op scale or inverse	scale	0.047619
copy using advanced indexing	advanced	0.333333
the	node	0.029630
bartlett spectral window in the time-domain	tensor bartlett m	0.083333
string specific to the apply to	apply	0.016667
parametrize it to make it work for	max_input_fct maker	0.083333
other implementation of mod	scalar mod c	0.125000
dot22 computing an outer-product -> ger	tensor local dot22 to ger	1.000000
output gradient w r t its inputs	conv3d grad wrt inputs output_grad filters input_shape filter_shape	0.333333
to determine the broadcast pattern for advancedsubtensor	adv index broadcastable pattern	0.066667
takes as input a n-d tensor where n	signal pool 2d input ws ignore_border stride	0.100000
gradient updates	grad	0.010417
broadcast pattern for	pattern	0.028571
topooptimizer from the input nodes to output	gof in2out	0.055556
concatenate a	make	0.017857
will have	name	0.011111
graph of scan to outside of scan	scan output	0.125000
bound	bound	0.260870
to write data	gpuarray write w dtype	0.200000
converts self _rop_op from user supplied form	from graph recompute rop op	0.200000
extract list of variables	gof variables and	0.250000
must return a thunk that is	thunk	0.021277
basic constant class	constant	0.016667
type's :attr	gpu	0.011765
draw random integers	tensor random integers	0.500000
"reverse-mode" gradient [1]_ for the cholesky factorization of	tensor cholesky grad perform node inputs	1.000000
done in august 2011	load shared variable val	0.142857
nit_sot output of scan	out scan	0.035714
gradient	grad i	1.000000
of integers indicating the version	version apply	0.125000
the replacement if the ops	gof replace validate replace all	0.050000
bitwise a |	or a	1.000000
cause of	bad optimization str diagnostic	0.043478
* y	code y	0.333333
b), axis=0) -> elemwise{scalar op} a	local reduce join node	0.111111
constants	constants	0.714286
softmax	softmax	0.727273
multinomial	multinomial	0.219512
forward but clip the	clip x lower_bound upper_bound	0.090909
test a	pt n_tests rng	0.500000
see theano tensor nonzero_values	py operators nonzero values	1.000000
this is	graph to	0.055556
converts self _grad_op from user supplied form to	compile op from graph recompute grad op	0.200000
the signature for this function	ext function method decl	0.333333
default that removes all asserts from the	local remove all assert	0.055556
stack	copy stack	0.333333
the svd	tensor svd	0.200000
from the cache	call cache	0.200000
maps from variable and apply nodes in the	get equiv inputs	0.142857
the inner graph to ensure that it	inner graph	0.035714
its idx_list reorders the inputs	inputs idx_list get_count	0.100000
the convolution gradient with respect to the inputs	gpu dnn conv grad	0.062500
replace it with a triangular solve	solve triangular	0.142857
destroyhandler wasn't already attached to some	destroy handler on attach	1.000000
the fgraph outputs	fgraph	0.012195
memo a dict that	gof	0.002381
help the navigator deal	navigator	0.032258
ratio	ratio	1.000000
offers to make itself	to os environ pathlist var newpath	0.038462
permutation by doing a recursion over	tensor permute row elements rec	0.047619
similar behaviour as haskell's foldl	scan_module foldl fn sequences outputs_info	1.000000
context associated with	get context	0.111111
copies the subgraph contained between i and o	clone i o copy_inputs	0.333333
replace_all_validate revert the replacement	gof replace validate replace all validate	0.111111
compiles this linker's fgraph	clinker make thunk input_storage output_storage storage_map keep_lock	0.333333
addition of a sparse matrix and	add	0.034483
for creating a class with a metaclass	compat add metaclass metaclass	0.125000
only if this enum	gof enum	0.166667
contiguous buffer	gpuarray inline reduce n buf pos count	0.142857
c-implementation of the dot product	dot csr c code node name inputs outputs	1.000000
make an inplace	gpuarray inplace	0.200000
gpucorr3dmm, gpucorr3dmm_gradweights and gpucorr3dmm_gradinputs	corr3d	0.166667
a symbolic row variable (ndim=2 broadcastable=[true false])	tensor row name	0.050000
incsubtensor when we overwrite the full inputs with	tensor local useless inc subtensor node	0.066667
from a list of l{codeblock} instances returns a	gof code gen blocks	0.050000
optionally inserting	py operators dimshuffle	0.019231
image	1axis kernel_shape	0.500000
copies the stack trace	stack trace	0.055556
wrt, computes gradients	core subgraph grad wrt	0.062500
listeners to help the navigator deal	navigator optimizer attach	0.038462
profiles returned by this	gof seq	0.200000
es	dtype	0.022727
an op that	alloc	0.012500
important note this function uses set and	out seq scan process node fgraph	0.142857
node_formatter	node_formatter	1.000000
"reverse-mode" gradient for the eigensystem	eigh grad perform node	0.333333
function as its	as	0.024390
compute sum of non nan / inf values	signature get sum	0.142857
trace from	trace	0.052632
the specified pieces of vectors and	sparse block outer make node o x y	0.066667
see theano tensor sum	tensor py operators sum axis dtype	1.000000
replacement if the ops	gof replace validate replace all	0.050000
fill inputted tensor with the assigned	tensor tensor py operators fill	1.000000
the navigator deal with	navigator optimizer attach updater fgraph	0.038462
compare true	tensor tensor type eq	1.000000
exception object with debug info	raise with op node	0.333333
value after the import	config param init default	0.040000
replace a leaf of a	tensor nnet replace leaf	0.100000
bad	bad	0.065789
replacement if the ops	validate replace all	0.050000
lib directories that	header dirs	0.045455
connection pattern of subfgraph defined by	op from graph connection pattern	0.076923
the function on the inputs and put	gof pure op perform node inputs output_storage params	0.047619
x shp -> alloc(unary x	tensor local	0.025641
floor of a	tensor floor a	1.000000
c code when doing constant folding of	tensor elemwise python constant folding	0.142857
filters with a movie	signals filters signals_shape filters_shape	0.333333
order a graph of apply nodes according to	apply nodes inputs outputs cmps	0.050000
idx_list with constant inputs replaced	constant idx inputs allow_partial only_process_constants elemwise	0.071429
of mod	scalar mod c code	0.125000
op	clinker op	0.071429
a tree of multiplications starting at the given	mul tree	0.250000
a symbolic row variable	row name	0.050000
to make itself the	to os environ pathlist var newpath	0.038462
macros	cop	0.028571
implement advancedincsubtensor1	advanced inc subtensor1	1.000000
a new fgraph check that 1) this	gof	0.002381
pieces of vectors and matrices	sparse block outer make	0.066667
the other implementation of mod	scalar mod	0.125000
this function uses set and dictionary data structures	non seq scan	0.090909
given graph contains a cycle parameters	gof contains cycle fgraph orderings	0.333333
function to	random_state a	0.500000
path importer to	path importer	0.333333
input vector and t	node	0.007407
list of shape tuple or	shape feature default infer shape	0.066667
the specified pieces of vectors and	sparse block gemv make node o w	0.066667
the apply to be	apply node	0.031250
converts this to expm1	expm1 node	0.066667
fill inputted tensor with the assigned value	tensor py operators fill value	1.000000
broadcasted dimensions	tensor tensor	0.014286
when enabled change all sigmoid to ultra_fast_sigmoid	ultra fast sigmoid	0.200000
optionally	dimshuffle	0.014493
get the 0 based level	type get	0.050000
can be used to upsample	ratio normalize	0.200000
default that removes all asserts	tensor local remove all assert	0.055556
input a 4-d tensor it sets all	max pool 2d same size input patch_size	0.166667
unroll the batch	conv code unroll batch	0.166667
that removes	tensor local remove	0.166667
create a six moves	six	0.025000
test	fun pt n_tests	0.500000
the input to a leftdims	input leftdims	0.166667
function computes the	ishape kshape border_mode	0.250000
if len img2d shape ==3 we todo	nnet conv op perform node inp out	0.166667
an apply_node recursively search from this node	apply_node	0.050000
cumulative sum of	tensor cumsum x	0.333333
validations on the inner graph to	scan validate inner graph	0.035714
pydot graph and write to	d3viz d3write fct path	0.166667
without replacement a can be a 1-d array	tensor choice random_state size a	1.000000
with respect to wrt, computes gradients	core subgraph	0.062500
the version	cache version	0.125000
return	name inputs	0.500000
connection pattern of subfgraph	op from graph connection pattern node	0.076923
has only one client and that client	only	0.050000
of this variable optionally inserting broadcasted	tensor py operators dimshuffle	0.019231
op that	op	0.009174
return a new	name	0.011111
passing a dictionary unary_out_lookup({int8 int32 float32 complex128})	unary out lookup	0.250000
apply nodes	gof sort apply nodes	0.200000
g++ version used	gcc	0.023810
between low and high	low high	0.666667
that unroll the batch	unroll batch	0.166667
a c contiguous version of the input	contiguous	0.058824
b), axis=0) -> elemwise{scalar op} a b	local reduce join	0.111111
standard elements of an	object	0.083333
elements obtained by iterating over given axis	min x axis keepdims	0.500000
adds new optimization instances to	register	0.100000
converts self _rop_op from user supplied form to	compile op from graph recompute rop op	0.200000
a clone	clone	0.020833
to determine the broadcast pattern for advancedsubtensor	tensor adv index broadcastable pattern a idx	0.066667
new variable	new	0.058824
the replacement if the ops in the list	gof replace validate replace all	0.050000
r	r client_to_remove	0.200000
symbolic row variable (ndim=2	row name dtype	0.050000
list of headers that are needed	headers	0.038462
c code for corrmm (direction="forward"), corrmm_gradweights	nnet base corr mm c code	0.090909
recursion over	permute row elements rec	0.047619
the output	get out	0.500000
is only used to determine the	adv index broadcastable	0.050000
stabilization optimization that is more general than crossentropy_to_crossentropy_with_softmax_with_bias	tensor nnet crossentropy to crossentropy with softmax	1.000000
or inverse the gradient in	core grad	0.166667
kinds scalar constants and the rest	rest inputs elemwise	0.125000
according to the idx list to get	get idx list	0.076923
to import six moves and its submodules	six	0.025000
>= 1 default 1) times from a multinomial	multinomial	0.024390
this variable	tensor tensor py operators	0.015625
see theano tensor sort	tensor py operators sort	1.000000
converts number to string	from number number	0.142857
a poisson	streams base poisson	0.500000
dimshuffle operation inside reshape reshape(vector	dimshuffle in reshape node	1.000000
reproducible case for problems	compile function dump filename	0.166667
raise	compile check inputs node storage_map r_vals	0.166667
batch size	batch	0.055556
an iterable	gpuarray gpu kernel base gpu kernels node	1.000000
a list of shape tuple or	tensor shape feature default infer shape	0.066667
function is basically a call	extract constant x elemwise only_process_constants	0.058824
duplicate this apply instance in a	gof apply	0.090909
a reshaped view/copy of	tensor tensor py operators reshape shape ndim	0.111111
function for diagonalsubtensor	nnet get diagonal subtensor view x i0 i1	0.083333
on the inner graph to	scan_module scan validate inner graph	0.035714
variable names when persisting to zip	variable id	0.250000
to the type's	array type	0.055556
operators	operators	0.086207
the graph and get a memo a	function graph	0.040000
within the op	op	0.009174
other implementation of mod	mod c code node	0.125000
return c code to declare variables	type c declare name sub check_input	0.333333
suitable dummy values	optimizer provide inputs	0.200000
mrg stream state and they	sandbox mrg random streams	0.033333
e^a - 1	tensor expm1 a	1.000000
error and exit code in a tuple	subprocess popen command	0.083333
is false we can't change the value after	core config param init default	0.040000
the same computations	scan_module equal computations	0.333333
change the value after the	core config param init default	0.040000
see theano tensor std	tensor tensor py operators std axis	1.000000
symbolic type representing a numpy ndarray value	tensor type	0.034483
see theano tensor sort	tensor py operators sort axis kind	1.000000
maximum in one	tensor maximum x y	0.090909
the dimensions of this variable optionally inserting	operators dimshuffle	0.019231
helper function	helper random_state a	0.500000
list remove are still in the graph	replacements remove reason	0.055556
the "reverse-mode"	perform node	0.166667
overwrite the full inputs with the new value	useless inc subtensor	0.125000
inner	inner	0.291667
for any python object a that would	gof	0.002381
x*x -> sqr x	tensor local mul to sqr node	0.166667
the shape s to previously un-shaped variable r	feature set shape r s override	0.500000
passed-in key is found in	get from key key key_data	0.111111
to make itself	to os environ pathlist	0.038462
of lib directories that are	lib dirs	0.045455
the basic variable class	tensor variable	0.166667
to	make constant	0.100000
merge 2 profiles returned	merge	0.071429
a module from the cache	module cache module from	0.333333
inputs replaced	inputs allow_partial only_process_constants elemwise	0.166667
convert x into a variable on	gpuarray variable x context_name	0.166667
retrive the context associated	gpuarray get context	0.111111
is not attempting to use dnn conv algo_bwd	no dnn algo bwd algo	0.166667
the inverse fast fourier transform with real-valued output	tensor irfft inp norm is_odd	0.500000
degree to	deg2rad	0.111111
this linker's fgraph	thunk input_storage output_storage storage_map keep_lock	0.333333
convert radian a to	rad2deg a	0.333333
the dimensions of this variable optionally	tensor tensor py operators dimshuffle	0.019231
gt	gt	0.625000
in the original graph to a	outputs copy_inputs_and_orphans memo	0.029412
tries to recognize the updates ordereddict the list	get updates	0.034483
mini-batch	input_shape filter_shape	0.027778
the source code for this	compile cmodule location	0.038462
dimshuffle is inside	dimshuffle node	0.333333
of v by	v	0.011111
product of the specified pieces of vectors and	sparse block gemv make node	0.066667
the maximum in one	tensor maximum x y	0.090909
variable	variable x	0.083333
addition of a sparse matrix	add	0.034483
return a dictionary of arguments to pass	args	0.025641
it into a gemm	gemm	0.066667
computes the r operation on f	core rop f	0.166667
a variable on the gpu	as gpuarray variable	0.166667
compiled theano function's ops supports	core pydotprint fct outfile compact format	0.250000
matrix by a broadcasted dense vector element wise	svcsr	0.090909
return connection	graph connection	0.500000
initialize the variables that	gof clinker	0.033333
that unroll the batch size	nnet gen conv code unroll batch	0.166667
raise baddestroymap	r_vals	0.090909
important note	scan process node fgraph node	0.142857
the version	code cache version apply	0.125000
up to the end variables	end	0.040000
the grad of	grad	0.010417
numpy's isclose on tensors	isclose a b rtol atol	0.500000
that removes	local remove	0.166667
with a set of 3d	tensor nnet conv3d input	0.125000
recursion over the	tensor permute row elements rec	0.047619
scan in an easy to	scan args	0.250000
iff other is the same	other	0.090909
r	compile debugprint r	0.250000
should be removed and	outs	0.050000
output gradient w r t its inputs	grad wrt inputs output_grad filters input_shape filter_shape	0.333333
reshapes the input to a	input	0.023810
true if we are able to assert that	dim_x dim_y	0.090909
function for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view	0.083333
slice_ok	slice_ok	1.000000
to find broken	compile find bad	0.333333
constant	tensor elemwise python constant	1.000000
important note	out seq scan process node fgraph	0.142857
and a set of arrays to choose	tensor choose a choices out mode	0.200000
the output dimensions of	get output	0.047619
the value after the	core config param init default	0.040000
into macros for use within	cop	0.028571
3d convolution for debugmode	tensor nnet base abstract conv conv	0.125000
just the compilation of cutils_ext	gof compile cutils	0.166667
return the inputs required to compute the given	inputs variable_list blockers	0.058824
each columns	col	0.142857
match out_shape	out_shape	0.125000
a inner	inner sitsot	0.083333
compute conv output gradient	nnet conv3d grad	0.333333
failure code to the task	cthunk find task	0.142857
create	create	1.000000
with a particular stream	random streams getitem item	0.142857
convert addsd to	sparse local addsd	0.250000
an op that copies	alloc	0.012500
within the subgraph between i and o parameters	i o	0.041667
increments a subtensor	inc subtensor	0.250000
important note this function uses set and	push out seq scan process node	0.142857
multiplication by a scalar	alpha	0.083333
config string (comma-separated key=value components) into a dict	config string config_string issue_warnings	0.166667
to represent the dependence of nodes	dependence	0.035714
that wasn't in the	bad	0.026316
raise baddestroymap	compile check inputs node	0.166667
subtraction	sub	0.111111
it into a canonical form that respects the	get canonical form	0.045455
std	std axis ddof keepdims corrected	1.000000
will print a warning	gof deprecated filename	0.166667
inputidx	inputidx	1.000000
an op that copies a	alloc	0.012500
dimensions of this variable optionally inserting	operators	0.017241
inserting	dimshuffle	0.014493
reorder the dimensions of this	dimshuffle	0.014493
if target is 'cpu' this will transfer to	tensor tensor py operators transfer target	0.500000
corr3dmm_gradweights (direction="backprop weights"),	helper bottom weights top direction	0.055556
of _maker which handles much of the debugging	maker i o m	0.066667
a warning message on	gof deprecated filename msg	0.041667
inverse fast fourier transform with real-valued output on	gpuarray cuirfft inp norm is_odd	0.333333
as replace_all_validate revert	all validate remove fgraph	0.166667
of moved objects in six moves urllib_response	six moves urllib response	0.333333
bitwise a ^ b inplace on a	xor inplace a b	0.333333
a badviewmap exception when it detects	check viewmap node	0.111111
this op could be very	l op	0.033333
ones with	ones like x	0.333333
__unify_walk__ method for one of the	gof unify walk a b u	0.037037
numpy-compatibility method if	diag	0.023810
crossentropysoftmax1hotwithbiasdx op whose incoming gradient is	crossentropy softmax 1hot with bias dx	0.111111
for gpucorrmm (direction="forward"),	base gpu corr mm	0.250000
scalar constants and the rest	rest inputs elemwise	0.125000
end	end	0.240000
a new	with new	0.166667
the image shape of convolution gradinputs	get conv gradinputs shape kernel_shape	0.500000
see theano tensor min	tensor tensor py operators min axis keepdims	1.000000
a batched	tensor batched	0.333333
the updates ordereddict the list	get updates	0.034483
slice	slice	0.230769
accept_inplace	accept_inplace	1.000000
to the type's :attr context_name	gpu	0.011765
basic slow python 2d or 3d convolution	conv conv img kern mode dilation	1.000000
compiles the source code for this	clinker compile cmodule location	0.038462
use dnn conv algo_bwd	core safe no dnn algo bwd algo	0.166667
helper_c_code	subtensor get helper c code	0.285714
context associated	get context	0.111111
function	random_state low high	0.500000
memory alias that	bad view	0.027027
of shape	shape feature default infer shape	0.066667
will match self	var	0.035714
random	mrg random	0.333333
tensor from	tensor complex from	0.250000
into macros for use within the op code	cop get op	0.200000
bitwise a ^	tensor xor a	1.000000
the grad of average	average	0.200000
that	bad view	0.027027
same kinds	tensor tensor	0.014286
on the inner graph to ensure	scan_module scan validate inner graph	0.035714
scan	scan	0.206897
required return c code to declare variables that	gof clinker type c declare name sub check_input	0.333333
specified pieces of vectors and matrices	sparse block gemv make node o w h	0.066667
by an op that will be inserted	clinker op	0.071429
as replace_all_validate revert the replacement	gof replace validate replace all validate remove fgraph	0.111111
on the gpu	as gpuarray	0.333333
outputs of specific ops of a compiled	trace f_or_fgraph ops_to_check bug_print	0.035714
dimensions of this	tensor	0.006431
recursion over the input dimensions	tensor permute row elements rec	0.047619
replace that with the *args directly	sparse local csm properties csm	0.142857
of this op could be	tensor prod l op	0.033333
this node	node	0.007407
y	c code y	0.333333
the 0 based level of the list	list	0.066667
on cpu here	pow specialize	0.250000
end variables	end	0.040000
load a file that was dumped to	load f persistent_load	0.333333
reproducible case	compile function dump filename inputs outputs mode	0.166667
number of scalars together into a	make	0.017857
we do not try to use complex numbers	mod check x y	0.166667
to output nodes of	gof	0.002381
set of 2d filters	nnet conv2d input filters	0.125000
:param execute if true execute a	misc execute execute verbose	0.250000
add	data add	0.500000
have the same	same	0.125000
and exit code in	subprocess popen command	0.083333
and "init_code" together	init code struct node name sub	0.500000
* y + alpha * dot a	tensor gemv c code y a	0.333333
cost_grad	cost_grad	1.000000
default that removes all asserts from	local remove all assert	0.055556
determine the broadcast	adv index broadcastable	0.050000
loading of moved objects in six moves urllib_parse	module six moves urllib parse	0.333333
over each shape that broadcast them to match	generate broadcasting	0.066667
deepcopyop how to generate	deep copy op	0.250000
invert	invert	0.833333
that this opt applies	gof	0.002381
for gpuincsubtensor	tensor local inplace setsubtensor node	0.250000
polar coordinate specification	polar abs angle	1.000000
what does this variable specify?	doc	0.166667
more multinomial distributions defined by	multinomial random_state	0.040000
function tries to recognize the updates ordereddict the	scan_module get updates	0.034483
a symbolic row variable (ndim=2 broadcastable=[true false])	tensor row	0.050000
convolve	convolve	1.000000
this op could	op	0.009174
variable on the	gpuarray variable	0.166667
returns upper bound on the	bound	0.043478
ones with the	ones like x	0.333333
will print a warning message	deprecated filename msg	0.041667
computes the confusion	nnet confusion	0.333333
to break aliasing of	wrapped_inputs wrapped_outputs	0.166667
computes	ishape kshape border_mode	0.250000
sample from a uniform distribution	uniform	0.043478
random	sandbox mrg random	0.333333
can't change the value after	param init default	0.040000
nit_sot output of scan return	push out scan	0.050000
returns the connection pattern	gof io connection pattern	0.055556
order a graph of apply nodes according to	sort apply nodes inputs outputs cmps	0.050000
removes useless	local useless	0.111111
text to a gist	gist	0.040000
a that would	gof	0.002381
and return full path of the	module name from	0.076923
register a	tensor register	0.500000
gradient is taken	grad	0.010417
mod	mod c	0.125000
connection pattern of a subgraph	io connection pattern	0.055556
a triangular solve	sandbox linalg tag solve triangular	0.142857
unroll the batch	unroll batch	0.166667
to construct a variable with a sparse matrix	sparse variable x name	0.250000
function is a thunk that operates on	gof linker make thunk	0.045455
unified to boundvariable(other_object)	walk fv o u	0.200000
perform	perform node x y	0.166667
in a new graph	with new inputs inputs	0.166667
input a n-d tensor where n >= 2	input ws ignore_border stride	0.090909
path	path	1.000000
1hot	1hot	1.000000
create a comparator to	cmp	0.058824
some perform() or c_code() modified an	destroy map	0.142857
necessary update dr_vals	dr_vals	0.111111
-> x remove split with only 1 split	local useless split node	1.000000
tensor from	complex from	0.250000
minimal type used for passing contexts to	gpu context type	0.333333
:param execute if true execute a theano	misc execute execute verbose m n	0.250000
can't change the value after the import of	config param init default	0.040000
the navigator deal	gof navigator optimizer	0.038462
wraplinker that runs a series of wrapper functions	gof wrap linker many linkers wrappers	0.071429
a special compound l{op} for	argmax1hot	0.058824
multiply the first half of v	v	0.011111
base 10 logarithm of	tensor log10 a	0.500000
associate linker with fgraph	gof op wise clinker accept fgraph no_recycling	1.000000
the shape	shape feature set shape	0.333333
i	r i	0.500000
return a hash from an ndarray	hash from ndarray data	0.333333
flags from config blas	flags	0.062500
for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor	0.083333
replace_all_validate revert the replacement	replace all validate	0.111111
partition a list of variables into two	scalarconsts	0.076923
with logsoftmax x	tensor nnet local logsoftmax	0.076923
offers to make itself the	to os environ	0.038462
defining the gradient	grad	0.010417
see theano tensor argsort	py operators argsort axis kind	1.000000
inputs required to compute the given variables	gof inputs variable_list blockers	0.058824
gradients of cost and/or from	cost	0.045455
used is	gcc	0.023810
operation to wait on a previously sent	mpisend wait	0.045455
all nodes list of input containers list	perform linker make all input_storage output_storage storage_map	0.333333
of the specified pieces of vectors and	sparse block outer make node	0.066667
matrices at least one	x y	0.024390
1d kernel for bilinear upsampling this	bilinear	0.019231
replace it with a triangular solve	sandbox linalg tag solve triangular	0.142857
wrapper around c_extract_out that initializes py_name	gof get c extract out r name	0.333333
sum(a axis={ }) / b	sum prod div dimshuffle node	1.000000
comparator to	cmp	0.058824
output error and	misc output	0.066667
raised when grad is asked	error	0.025000
optionally	py operators dimshuffle	0.019231
idx_list with constant inputs	tensor subtensor get constant idx inputs	0.250000
"reverse-mode"	perform node	0.166667
according to the flags use_list and use_tuple	as use_list use_tuple	1.000000
failure code to the task that	gof cthunk find task	0.142857
series of wrapper	linker many linkers wrappers	0.047619
self _rop_op from user supplied form to type	from graph recompute rop	0.200000
print the mflops	op flops	0.125000
make a nested loop over several	make loop	0.200000
2d kernel that can be	kernel 2d	0.050000
raise baddestroymap if	check inputs	0.125000
required anymore and should be removed and	compress outs	0.076923
efficiently calculating the dot product	true dot	0.166667
the signature for	method decl	0.333333
setsubtensor	setsubtensor	0.555556
a shared variable to 0	compile shared variable zero borrow	0.200000
the standard deviation along	tensor std	0.111111
a view	compile view tree	0.500000
transfer to a	operators transfer	0.125000
transfer to a tensortype if not already	transfer	0.058824
can't change the value after the	init default filter	0.040000
permutation by doing a recursion over	permute row elements rec	0.047619
use_goto	use_goto	1.000000
scale or	scale	0.047619
python type c type numpy typenum that	type dtype specs	0.071429
a short mostly hexadecimal hash	core hex digest x	0.083333
return a module from the cache	module cache module from	0.333333
tag trace to	tag trace thing user_line	0.166667
numpy randomstate instance associated with a particular stream	tensor random streams getitem item	0.142857
an array with more than one element	node	0.014815
the stack trace from one or more	stack trace	0.055556
this function performs the matrix inverse on gpu	gpu matrix inverse a	0.200000
x	x axes	0.200000
abstractconv	conv	0.037037
is only used to determine	adv index broadcastable	0.050000
computes the inverse	inverse	0.066667
elemwise floor of	sparse floor	1.000000
epoch of the last access of	last access time path	0.040000
transfer to a tensortype if not already	tensor tensor py operators transfer	0.125000
in profilemode to print the mflops	base gpu corr mm flops inp outp	0.125000
a comparator to represent the dependence of nodes	gof make dependence cmp	0.111111
proxy for either true_div	proxy	0.095238
inverse of a matrix	matrix inverse	0.111111
idx list to get	get idx list	0.076923
the replacement if the ops	gof replace validate replace	0.050000
remove	scan_module remove	1.000000
change the value after the	init default filter	0.040000
localoptgroup	group	0.047619
version used is	gof	0.002381
of l{codeblock} instances returns a	gof code gen blocks	0.050000
retrieve item from the cache if available	gof call cache call fn args key	0.200000
warning about cuda should be displayed only when	gpuarray use device force default_to_move_computation_to_gpu move_shared_to_gpu	0.333333
the output shape for a convolution	gpu dnn conv get out shape	0.142857
kernel for bilinear upsampling this function	bilinear	0.038462
validations on the inner graph	scan_module scan validate inner graph	0.035714
we parametrize it to make it	max_input_fct maker	0.083333
the cache if available	cache call fn args key	0.200000
a subtensor	subtensor	0.176471
a cache directory and return	name from dir	0.250000
extract test value from v raises	get test value v	0.250000
warning message on the first call	deprecated filename msg	0.041667
used is the	gcc	0.023810
computes the output shape	get out shape ishape kshape	0.500000
comparing tensorconstant instances	tensor constant	0.055556
same kinds of	tensor tensor type	0.041667
list of shape	tensor shape feature default infer shape	0.066667
label of apply node	d3viz apply label node	0.500000
to manipulate	r new_r reason verbose	0.071429
with respect to wrt, computes gradients of	core subgraph grad	0.062500
is the equivalent of	graph to gpulocal	0.055556
[elementwise] largest	largest	0.125000
python type c type numpy typenum that corresponds	type dtype specs	0.071429
as replace_all_validate revert the replacement if	validate replace all validate remove fgraph	0.111111
and replace it with logsoftmax x 's grad	local logsoftmax grad node	0.200000
output dimensions of convolving an	conv op get output	0.047619
insert inplace versions of remove0	sparse local inplace remove0	0.333333
the dimensions of this variable optionally inserting	dimshuffle	0.014493
pattern of a subgraph defined by given	pattern	0.028571
dtypes	dtypes	1.000000
save	save	1.000000
set of arrays	choices out mode	0.500000
item from the cache	cache	0.034483
careduce that reuse the python code from gpuarray	gpu careduce cpy	1.000000
the axis that was used to	inputs g_outputs	0.090909
spatio-temporal filters with	tensor nnet conv3d signals filters	0.111111
offers to make	to os environ	0.038462
alloc val [x y] -> alloc(val[ ])	tensor local subtensor of alloc	1.000000
headers that are needed by	clinker headers	0.047619
to the end	grad wrt end	0.050000
to import six moves and	six	0.025000
remove are still in the	replacements remove reason	0.055556
reorder the dimensions of x	x axes	0.200000
sample from one or more multinomial distributions	tensor multinomial random_state size	0.333333
if	check inputs node	0.166667
move constants into	constants	0.142857
compute 1d kernel for bilinear upsampling this function	tensor nnet bilinear	0.111111
composite op	composite	0.166667
list of headers	clinker headers	0.047619
_maker which handles much	debug mode function maker i o m	0.066667
called by remove_feature feature should remove	gof feature on detach function_graph	0.200000
the outer product of two sets of	sparse block outer	0.047619
vector and t is	node	0.007407
memory alias that wasn't in the	bad	0.013158
module is	importer is	0.250000
instance full of the code for our fgraph	clinker get dynamic module	0.200000
of this op could be very easy	tensor prod l op	0.033333
and "code_cleanup"	cleanup node name	0.500000
row variable (ndim=2 broadcastable=[true false])	tensor row name dtype	0.050000
navigator	navigator optimizer attach updater	0.038462
the inner graph to	inner graph	0.035714
kinds of useless	local useless	0.111111
of this variable optionally inserting	py	0.014286
gradweights	gradweights	1.000000
from the loaded cache or	module cache get	0.250000
x is a	x	0.008772
a module from	module from	0.166667
makes the folowing changes in the	local mul switch sink node	0.045455
this is the equivalent of	graph to gpulocal opt	0.055556
run after c_code whether it failed or	cleanup node	0.142857
return the function name to	gpuarray	0.046512
to replace a	tensor nnet replace	0.250000
the gradient the finite	grad	0.010417
return the inputs required	inputs variable_list blockers	0.058824
solve operation c = a	tensor solve	0.038462
the replacement if the ops in	validate replace	0.050000
rightdims	rightdims	0.857143
wrapped_outputs	wrapped_outputs	1.000000
is a zero-arguments function that	gof	0.002381
integers indicating the version	c code cache version	0.125000
views of v given that v	v	0.011111
of the specified pieces of vectors and matrices	sparse block outer make node o	0.066667
convolution gradweights	get conv gradweights	0.500000
convolving a mini-batch	input_shape filter_shape	0.027778
when we overwrite the full inputs with	tensor local useless inc subtensor node	0.066667
v by a	v	0.011111
as python not the other implementation of mod	mod c	0.125000
and uses	o	0.076923
"kshp"	inshp kshp stride mode	1.000000
convert addsd to faster	sparse local addsd ccode	0.250000
it into a canonical form that respects	get canonical form	0.045455
b inplace on a	inplace a b	0.875000
distributions defined by one-dimensional slices in pvals	n pvals	0.125000
module from the cache	gof module cache module from	0.333333
allows replacing subgraphs of a computational graph	clone output replace strict share_inputs	0.071429
==3 we todo	nnet conv op perform node inp out	0.166667
use a simple algorithm	optimizations2 order reasons r_vals	0.333333
the mflops	corr mm flops inp outp	0.125000
apply nodes in the original	inputs outputs copy_inputs_and_orphans memo	0.029412
cxx	cxx	1.000000
the graph and	graph	0.032787
c code to extract a	clinker type c extract	0.500000
matrix it	alloc	0.012500
form that respects	form	0.111111
converts number to string by rendering it in	char from number number	0.142857
signature object for comparing tensorconstant	tensor constant signature	0.100000
update self rstate to be skipped	rstate	0.090909
to get the 0 based level	get	0.020833
this function must return a thunk	thunk	0.021277
__thunk_trace__	hook type	0.333333
same type	list type	0.100000
multiplication by a scalar on the output	alpha	0.083333
the equivalent of	opt	0.043478
input a 4-d tensor	2d same size input patch_size	0.166667
specified factor takes as input	signal pool 2d input	0.090909
product of the specified pieces of vectors and	sparse block outer make node o x y	0.066667
convert	convert	1.000000
returns the bartlett spectral window in the time-domain	bartlett	0.058824
a function into a basic theano op	op itypes otypes infer_shape	0.047619
named module is a package	six meta path importer is package fullname	0.250000
can't change the value after the import	config param init default	0.040000
navigator	gof navigator optimizer attach updater fgraph	0.038462
c_init that	gof get c	0.166667
the convolution gradient with respect to the	dnn conv grad i	0.125000
used	gof gcc	0.027778
outputs of specific ops of a compiled graph	trace f_or_fgraph ops_to_check bug_print	0.035714
computes the output dimensions of	get output	0.047619
proxy for either true_div or	div proxy	0.125000
batch normalization of the	tensor nnet batch normalization	0.125000
from existing start gradients up	start	0.040000
output of scan	scan	0.017241
reshapes the output	output input leftdims	0.333333
multiplication by	alpha	0.083333
axis that was used to split x	tensor split grad inputs g_outputs	0.333333
this is the	graph to gpulocal opt	0.055556
to	to os	0.038462
and only adds dimension to the	tensor local	0.025641
a global optimizer for pushing out the variables	push out	0.037037
input that wasn't	bad	0.013158
optimization to the provided l{functiongraph} it	gof optimizer apply	0.166667
(function input_variables output_variables) where function is a thunk	linker make thunk	0.125000
generates the c code for corr3dmm (direction="forward"), corr3dmm_gradweights	base corr3d mm c code	0.090909
if this enum	gof enum type	0.166667
the confusion matrix of	tensor nnet confusion matrix	0.166667
apply nodes according to	sort apply nodes inputs outputs cmps	0.050000
structures	scan_module push out non seq	0.125000
if theano graphs represent the same	xs ys in_xs in_ys	0.111111
this variable	tensor py	0.015873
batch normalization of the given	gpuarray dnn batch normalization	0.125000
replace_all_validate revert the replacement if the ops	validate replace all validate remove fgraph	0.111111
an array with more	node inputs	0.086957
without replacement from a multinomial	choice from	0.333333
convolution with the	conv	0.037037
a subtensor is inside a dimshuffle which only	subtensor node	0.066667
reorder the dimensions of this	py operators dimshuffle	0.019231
feature to	feature feature	0.250000
or 3d convolution for	base abstract conv conv	0.125000
the sum	tensor sum	0.111111
that initializes py_name to py_none	gof get c init r name sub	0.250000
august 2011	load shared variable val	0.142857
subtensor and its idx_list reorders	idx_list get_count	0.090909
list of compilation flags from config blas	libs flags libs_dir include_dir	0.052632
compile c code when doing constant folding	elemwise python constant folding	0.142857
to replace a leaf of a multiplication tree	nnet replace leaf	0.100000
optimization makes the folowing changes in the	local mul switch sink	0.045455
is inside a	node	0.007407
raise	compile check inputs node storage_map	0.166667
for use in the	gpu	0.023529
canonizer	canonizer	1.000000
raise baddestroymap if	node storage_map r_vals	0.166667
if cond	switch cond	0.500000
g++ version used	gof	0.002381
thunk that	thunk	0.021277
of mod	scalar mod c code node name	0.125000
chi squared survival function	chi2sf x k	1.000000
subgraph bound by the inputs and outputs	inputs outputs	0.066667
the image shape of convolution gradinputs	conv gradinputs shape 1axis kernel_shape	0.500000
can't change the value after the import	default	0.030303
utilitary	typed_list typed	0.333333
the compilation lock if someone else has it	gof force unlock	0.500000
to a mode	compile mode	0.166667
the value after the	param init default filter	0.040000
a compiled module from the loaded cache	module cache get module name	0.166667
revert the replacement if the ops in	replace validate replace all	0.050000
stream state and	random streams	0.058824
and only adds	local	0.014085
context associated with a name	context name	0.333333
x	x log2_exponent	1.000000
copies the stack	stack	0.066667
elementary validations on the inner graph	scan validate inner graph	0.035714
of arguments which must be hashable	gof memoize f	0.250000
that runs	gof	0.002381
the output type dtype and broadcast there is	tensor local canonicalize alloc node	0.333333
cost and/or from existing start	start cost	0.100000
input that wasn't	destroy	0.009709
that unroll the	gen conv code unroll	0.250000
check if converting to type2	type2	0.050000
use with helper_c_code	gpu inc subtensor get helper c code	0.333333
subprocess_popen returning the output	misc output	0.066667
values from a with or	size a replace	0.333333
listeners to help the navigator deal	navigator optimizer attach updater	0.038462
of square symmetrix matrix	sandbox linalg spectral radius	0.166667
required anymore and should be removed	scan_module compress outs	0.076923
and uses it instead of	o	0.076923
kernels of shape	shape	0.010204
y with length one the axes of	y axis	0.125000
will be turned into macros for	cop	0.028571
tensor filled with ones closer to numpy's syntax	tensor ones shape dtype	0.250000
the inner graph to ensure that it is	validate inner graph	0.035714
merge multiplication	alpha merge	0.076923
this function	gpuarray	0.023256
m2	sandbox mult mat vect	1.000000
a and b can be considered approximately equal	pure type values eq approx a b	1.000000
important note this function uses	process node	0.142857
a canonical form that respects the	get canonical form	0.045455
cudnn can't be used	gpuarray no cu dnnraise apply fgraph	0.200000
a meta	meta	0.142857
c code for corr3dmm (direction="forward"), corr3dmm_gradweights	base corr3d mm c code	0.090909
order v	v	0.022222
in a sparse format instead of dense	sparse	0.019231
the graph's apply nodes such that	gof	0.002381
the named module is a package	path importer is package fullname	0.250000
change the value after the	default filter	0.040000
variable optionally	tensor	0.006431
is the equivalent of	graph to gpulocal opt	0.055556
and reduce pattern has functioning c	gpu careduce cuda supports c	0.200000
the original graph to a new node	inputs outputs copy_inputs_and_orphans memo	0.029412
of _maker which handles much of the debugging	compile debug mode function maker i o m	0.066667
builds the 1d kernel that can be used	kernel 1d	0.050000
scan the contents of a cache directory	from dir dirname err files	0.166667
an apply_node recursively search from this node	apply_node check	0.066667
return a tuple of integers indicating the version	code cache version	0.125000
not the other implementation of mod	mod	0.071429
graph optimizer for inserting gemm operations	gemm optimizer	1.000000
node by one which computes the specified outputs	fgraph node output_indices	1.000000
the original graph to a	outputs copy_inputs_and_orphans memo	0.029412
an inplace optimization that deals with allocempty this	gpuarray inplace allocempty op idx	0.166667
profiles returned by this	gof	0.002381
of this type	type	0.023810
graph is impossible to evaluate	destroy	0.009709
input	2d same size input	0.500000
overrides	overrides	1.000000
the template filled by broadcasting value through it	broadcast like value template fgraph	0.125000
even of x	x	0.008772
macros for	cop	0.028571
subtensor using advanced indexing	advanced inc subtensor	0.333333
along given axis	x axis	0.200000
diagonalsubtensor	diagonal subtensor view x i0	0.083333
to draw random numbers using numpy's	a replace p	0.250000
broadcast pattern for	pattern a	0.066667
axis=l) -> sum(a axis={ }) / b	local sum prod div dimshuffle node	0.333333
the bartlett spectral window in the	bartlett m	0.083333
outputs according to the flags use_list and use_tuple	as use_list use_tuple outputs	1.000000
to a new node a clone in a	clone	0.020833
an expression constant when computing gradients	core zero grad x	0.333333
the replacement if the ops	validate replace all	0.050000
important note this function uses set and dictionary	push out seq scan process node fgraph	0.142857
to add	tensor	0.006431
_maker which handles much of the	function maker i o m	0.066667
an operation to wait on a	mpirecv wait	0.045455
randomstate instance associated with a particular stream	random streams getitem item	0.142857
f	core rop f	0.166667
in an easy to manipulate format	args	0.025641
to run after c_code whether it failed or	cleanup node	0.142857
install some functiongraph listeners to help the navigator	gof navigator optimizer attach updater fgraph	0.038462
implement the grad of average	average	0.200000
this apply instance in a new	gof apply clone with new inputs	0.250000
to wrt, computes	subgraph grad	0.062500
convolution gradient with respect to the	gpu dnn conv grad	0.125000
svd on cpu	tensor svd a full_matrices compute_uv	0.200000
proxy for either true_div or int_div depending on	tensor div proxy	0.125000
with constant inputs replaced by their python	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
exception object	raise	0.076923
a recursion over	permute row elements rec	0.047619
with a set of 3d	tensor nnet conv3d	0.071429
of this variable optionally inserting broadcasted	dimshuffle	0.014493
a list of shape tuple or	infer shape	0.066667
special compound l{op} for the	softmax argmax1hot	0.083333
the [elementwise] smallest of	tensor smallest	0.333333
only used to determine the broadcast	adv index broadcastable	0.050000
new	new inputs inputs strict	0.166667
the mean value	tensor mean	0.111111
perform the permutation by	perform node x y inverse	0.166667
perform some elementary validations on the inner graph	inner graph	0.035714
localoptgroup for graphtogpu	graph to gpulocal opt group	0.055556
perform the permutation by doing	perform	0.058824
for debugmode	tensor nnet base abstract	0.500000
localoptgroup for	opt group	0.043478
compute the image shape of convolution gradweights	tensor nnet get conv gradweights shape 1axis	0.500000
is no change in the shape of	node	0.014815
indices of minimum elements obtained by iterating over	keepdims	0.052632
generates the c code for gpucorrmm (direction="forward"),	base gpu corr mm c code	0.090909
ten4	ten4	1.000000
connection pattern of subfgraph defined	from graph connection pattern	0.076923
an exception while annotating	thunk exc_info storage_map	0.250000
code when doing constant folding of this node	constant folding node	1.000000
this to expm1	expm1	0.050000
the mflops	conv op flops inputs	0.125000
the input specs and the output specs	std fgraph input_specs output_specs accept_inplace	0.142857
for same kinds	tensor	0.006431
explicitly upcasts constant inputs to elemwise	elemwise constant inputs node	0.250000
apply the list of	apply	0.016667
leaves of a search through consecutive view_map()s	gof view roots r	0.200000
subtensor is inside	subtensor node	0.066667
bilinear	bilinear	0.153846
version	cache version apply	0.125000
all sigmoid to	sigmoid node	0.100000
matrix bias	bias node	1.000000
1msigmoid	1msigmoid	1.000000
unroll	unroll	0.666667
of this variable optionally	tensor py operators dimshuffle	0.019231
theano constants in subtensor arguments	constant args	0.250000
wasn't in the	view	0.022727
profiling to print the mflops	conv flops inp outp	0.125000
a canonical form that respects	get canonical form	0.045455
replacement	validate replace	0.050000
of python int c long int	core python int bitwidth	0.250000
dimensionality of the var is equal to	is flat var	0.200000
create a six moves urllib	six	0.025000
variable optionally	py operators dimshuffle	0.019231
this is a	tensor nnet	0.017544
of a real-valued input on the	curfft inp norm	0.066667
localoptgroup for	gpulocal opt group	0.055556
it work for elemwise and gpuelemwise op	tensor local elemwise fusion op op	0.200000
return real component of complex-valued tensor z	tensor real z	1.000000
module component with similar interface to	streams	0.076923
a thunk that	gof linker make thunk	0.045455
respect to wrt, computes gradients of	core subgraph grad wrt	0.062500
names when persisting to zip	id	0.100000
links all the specified vars to	merge new_best	0.142857
makes the folowing changes in	tensor local mul switch sink	0.045455
within the subgraph between i and o	i o	0.041667
c type numpy typenum that corresponds to self	tensor type dtype specs	0.071429
distributor	distributor	1.000000
return a symbolic constant with value x	tensor constant x name ndim dtype	0.333333
the equivalent of localoptgroup for graphtogpu	gpulocal opt group	0.055556
allows replacing subgraphs of	clone output replace strict share_inputs	0.071429
validations on the inner graph to ensure that	inner graph	0.035714
dilation	dilation	1.000000
to the idx list to get	get idx list	0.076923
simple algorithm to find	compile find bad optimizations2 order reasons r_vals	0.111111
shape feature	compile shape	0.250000
the dependence	dependence	0.035714
implements the "reverse-mode" gradient for	grad perform	0.083333
connection pattern of a subgraph	gof io connection pattern	0.055556
an functiongraph	function graph init	0.333333
input vector and t is a	node input_storage	0.038462
by this	gof	0.002381
dimensions	tensor tensor	0.028571
optionally with a new dtype	clone dtype	1.000000
3-d variable	tensor3	0.142857
a special compound l{op} for the output of	softmax argmax1hot	0.083333
if those nodes are not in	gof	0.002381
original_shape	original_shape	1.000000
the folowing changes in the	local mul switch sink	0.045455
convert addsd to	addsd ccode	0.250000
the list remove are still in the graph	replacements remove reason	0.055556
clip x to be between min and	clip x min	0.500000
a stabilization optimization	crossentropy to crossentropy with softmax with bias fgraph	0.333333
for gpucorrmm (direction="forward"), gpucorrmm_gradweights	gpu corr mm	0.142857
inputs according to the idx list to get	get idx list inputs	1.000000
proxy	div proxy	0.125000
clone the graph and get a memo a	graph clone	0.166667
symbol definition with an elementwise version of	tensor scal inplace symbol	0.500000
unfortunately conda offers to	to os environ pathlist var newpath	0.038462
this is the equivalent of localoptgroup	to gpulocal opt group	0.055556
used to determine the broadcast pattern	tensor adv index broadcastable pattern a idx	0.066667
perform some elementary validations on the inner	scan validate inner	0.142857
the context object mapped to the type's :attr	context	0.035714
is a mrg stream state and they are	mrg random streams	0.033333
module is	path importer is	0.250000
for elemwise and gpuelemwise op	tensor local elemwise fusion op op	0.200000
change the value after the import	init default filter	0.040000
a <=	tensor le	0.333333
a dense vector	s	0.071429
optionally inserting broadcasted dimensions	tensor tensor py	0.015873
localoptgroup	to gpulocal opt group	0.055556
detect if the g++	gof	0.002381
loop over several arrays and associate specific	loop	0.027778
the lock is removed	lock	0.100000
in a new graph	clone with new	0.166667
end variables of	grad wrt end	0.050000
also work for gpuadvancedincsubtensor1	tensor local inplace incsubtensor1 node	1.000000
dimensions of this variable optionally inserting broadcasted dimensions	tensor tensor py operators	0.015625
a comparator to represent the dependence of	gof make dependence cmp	0.111111
return indices	indices	0.076923
reproducible case	compile function dump filename inputs outputs	0.166667
is the equivalent	graph to gpulocal	0.055556
replacement if the ops in the	replace	0.032258
compute_map_re	compute_map_re	1.000000
with respect to wrt, computes	subgraph grad	0.062500
to use dnn conv algo_bwd	core safe no dnn algo bwd algo	0.166667
replace_all_validate revert the replacement if the	replace all validate remove fgraph	0.111111
default 1) times from a multinomial	multinomial	0.024390
the supplied function as its implementation	as	0.024390
the cross-entropy between an approximating distribution and a	nnet categorical crossentropy coding_dist true_dist	0.111111
x with the given	x	0.008772
and also their apply_node if those	gof	0.002381
of var transferred	transfer var	0.100000
python	constant	0.016667
"lifts" dimshuffle through elemwise operations and merges consecutive	tensor local dimshuffle lift node	0.250000
the application of another op that takes the	op sub	0.066667
of the list	list	0.066667
std	std	0.352941
c type numpy typenum that corresponds to self	type dtype specs	0.071429
function for diagonalsubtensor	diagonal subtensor view x	0.083333
important note this function uses set and	process node	0.142857
-x pattern	tensor nnet is neg	0.166667
helper function for diagonalsubtensor and	get diagonal subtensor	0.083333
the stack trace from one or more tensor	gof copy stack trace	0.055556
internal function that constructs a new	scan_module safe new	0.333333
return true	gof is	0.500000
for operations that need to compile kernels	gpu kernel	0.250000
the replacement if the ops in	gof replace validate replace all	0.050000
to construct a variable	variable x	0.083333
a comparator to represent the dependence	make dependence cmp	0.111111
into a canonical form that respects	canonical form	0.045455
for gpucorrmm (direction="forward"), gpucorrmm_gradweights	gpuarray base gpu corr mm	0.333333
exception class to raise in	core raise init	0.100000
in the original graph to a new node	inputs outputs copy_inputs_and_orphans memo	0.029412
a ==	eq	0.111111
not for stability	scalar	0.017857
the standard deviation	std	0.058824
choose values from a with or	size a	0.333333
scalar scalar	scalar	0.017857
end variables of a	end	0.040000
with respect to wrt, computes gradients	core subgraph grad wrt	0.062500
as other scalar	scalar	0.035714
fgraph and a list of variables returns the	fgraph outputs_to_disown	0.047619
inputs of	check inputs	0.125000
recognize the updates ordereddict	scan_module get updates	0.034483
then replace it with a triangular solve	sandbox linalg tag solve triangular node	0.142857
batched tensordot product	batched tensordot x y axes	0.333333
also work for gpuincsubtensor	tensor local inplace setsubtensor	0.250000
elementwise addition (inplace on a)	add inplace a b	1.000000
suitable dummy values	meta optimizer provide	0.200000
that runs a	gof	0.002381
the dependence of nodes in a	dependence	0.035714
-a inplace on a	tensor neg inplace a	1.000000
from equilibriumoptimizer by calling the method query	graph to gpudb	0.142857
instance of _maker which handles much of the	maker i o m	0.066667
a with a modulo of m1	m1	0.027027
-> alloc(unary x	tensor local	0.025641
the equivalent	graph	0.016393
a prior reduction of x	x	0.008772
extract test value from v raises attributeerror if	get test value v	0.250000
this variable optionally inserting broadcasted	py operators	0.015625
verbose	verbose	0.833333
tree of multiplications starting at the given	mul tree	0.250000
output dimensions of convolving an image	op get output	0.047619
not attempting to use dnn conv workmem	no dnn workmem workmem	0.166667
of variable type	d3viz type	0.333333
find	find bad	0.333333
prod(prod()) -> single prod()	tensor local op of op	0.500000
range of values maximum - minimum along	tensor ptp a	0.333333
on the inputs and put the variables in	gof pure op perform node inputs output_storage params	0.047619
parse	parse	0.750000
and apply_nodes to this graph	graph	0.016393
and return	name	0.011111
as replace_all_validate revert the replacement if the ops	replace all validate	0.111111
the cache	module cache	0.142857
some functiongraph listeners to help the navigator deal	gof navigator optimizer	0.038462
image shape	shape 1axis image_shape	0.250000
shared variable to 0	compile shared variable zero borrow	0.200000
this	to	0.017544
change the value after the import	config param init default filter	0.040000
folowing changes in	local mul switch sink node	0.045455
optimization disabled by default that removes	tensor local remove	0.166667
connection pattern	graph connection pattern	0.076923
current op and reduce pattern has functioning c	careduce cuda supports c	0.200000
power	pow	0.166667
revert the replacement if the ops in the	validate replace all	0.050000
can't change the value after the	core config param init default filter	0.040000
to evaluate because of aliasing and	destroy	0.009709
helper function for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view	0.083333
remove subtensor/advancedsubtensor1 if it takes the full	local useless subtensor node	0.200000
the inner graph to ensure that it is	inner graph	0.035714
an alloc and only adds dimension to the	local alloc	0.111111
from a list of l{codeblock} instances returns	gof code gen blocks	0.050000
d3write	d3write	1.000000
print the mflops	conv flops inp outp	0.125000
match a variable with the -x pattern	is neg	0.166667
cmodule	cmodule	1.000000
of this	tensor py	0.015873
test	n_tests	0.142857
subtract two matrices at least one of which	sub x y	0.333333
a simple algorithm to find broken optimizations	compile find bad optimizations2 order reasons r_vals	0.111111
return str of variable type	d3viz type to str t	0.500000
raise baddestroymap	storage_map	0.090909
has an unification in u and uses it	o u	0.037037
same kinds of tensortype	tensor tensor	0.014286
return x with the given subtensor	subtensor x	1.000000
convert addsd to faster addsd_ccode	sparse local addsd ccode	0.250000
two kinds of useless reshape	tensor local useless reshape node	0.200000
of lib directories that	clinker lib dirs	0.055556
validations on the inner graph to ensure	validate inner graph	0.035714
directory and return full path of the	gof module name from dir	0.071429
x and replace it with logsoftmax x	local logsoftmax node	0.142857
variable with the -x pattern	tensor nnet is neg	0.166667
in the list remove are still	replacements remove reason	0.055556
from	get	0.020833
safe shorter version of platform platform()	core short platform r p	0.142857
callbacks calls getattr feature name (*args) for each	callbacks name	0.333333
list remove are still in the graph	fgraph replacements remove reason	0.055556
in u and	o u	0.037037
pattern of subfgraph defined by inputs	pattern node	0.125000
compute 1d kernel for bilinear	nnet bilinear	0.111111
a list of nodes that must	gof	0.002381
transform it into a canonical	canonical	0.076923
a six moves urllib namespace that	six	0.025000
shp -> alloc(unary x shp)	tensor local alloc unary	0.250000
default that removes all asserts	local remove all assert	0.055556
inserting broadcasted dimensions	tensor tensor py operators	0.015625
raise in self	core raise init	0.100000
a series of wrapper functions	linker many linkers wrappers	0.047619
zeros_like but forces the object to have a	core float zeros	0.166667
a compiled module from the loaded cache or	gof module cache get module name	0.166667
a function that	function	0.052632
if the g++ version used	gof gcc	0.027778
a > b	tensor gt a b	1.000000
this is the equivalent	gpulocal	0.055556
a ^	tensor xor	0.333333
associate linker with fgraph	op wise clinker accept fgraph no_recycling profile	1.000000
compute the dot product of	tensor nnet	0.035088
to the fgraph outputs	fgraph expanded_inputs	0.058824
scan return true	push out scan	0.050000
the dimensions of this variable optionally	tensor tensor py operators	0.015625
required to compute the	variable_list blockers	0.166667
to make itself the default python and	to os environ pathlist var	0.038462
of a real-valued input on the gpu	gpuarray curfft inp norm	0.066667
reduce{scalar op}(join(axis=0 a b), axis=0) -> elemwise{scalar op}	tensor local reduce join node	0.111111
an operation to wait on a	wait	0.045455
with a set of 3d	nnet conv3d	0.071429
to r	compile debugprint r	0.250000
tensor_from_scalar(scalar_from_tensor x -> x	local tensor scalar tensor node	1.000000
x y idx idx) ->	tensor local	0.025641
the __unify_walk__ method for one of the objects	unify walk a b u	0.037037
all variables which may share the	compile infer reuse pattern	0.100000
convolution	conv conv	0.250000
that will be inserted at struct	struct node	0.062500
the cache if available	call cache call fn args key	0.200000
in profilemode to print the mflops	gpuarray base gpu corr mm flops inp outp	0.125000
be between min	min	0.071429
a dictionary of arguments to pass to helper_c_code	get helper c code args	0.250000
graphtogpu	to gpulocal	0.055556
samples from a uniform into sample	uniform	0.086957
mode	mode	0.500000
after a failed fix done in august 2011	tensor load shared variable val	0.142857
we clone this object but we don't clone	gof constant clone	0.166667
mod	scalar mod	0.125000
op whose incoming	softmax 1hot	0.333333
opt2	opt2	1.000000
minimum	minimum x	0.142857
to a specified scalar	a	0.008065
return string representation of	to str b	0.250000
updates ordereddict the list of outputs and the	get updates and outputs	0.333333
this function performs the svd on cpu	svd a full_matrices compute_uv	0.200000
revert the replacement if	gof replace validate replace all	0.050000
images2neibs <theano tensor nnet neighbours images2neibs>	nnet images2neibs ten4 neib_shape neib_step mode	0.333333
and return full path of the	gof module name	0.076923
converts number to	from number number	0.142857
uniform into sample without replacement from a multinomial	choice from uniform	0.166667
lower_bound	lower_bound	1.000000
sharedvariable constructor for randomstate	randomstate constructor value name strict allow_downcast	1.000000
1/(1+exp	inv 1 plus exp node	1.000000
op __init__ fct don't have	composite make new inplace output_types_preference name	0.142857
the type's :attr	type	0.011905
conda offers to	to os environ pathlist var	0.038462
the source code for this linker	clinker compile cmodule location	0.038462
existence of the __unify_walk__ method for	unify walk a b u	0.037037
changes node inputs[i]	graph change input node i	0.250000
for convop that unroll the batch size	conv code unroll batch kern d unroll_bsize unroll_ksize	0.125000
c code to	clinker op c code	0.333333
the epoch of the last access of	last access time	0.040000
symbolic 3-d variable	tensor3 name dtype	0.166667
the g++ version used is the	gof gcc	0.027778
connection	gof io connection	0.333333
<theano sandbox neighbours neibs2images>	neibs neib_shape original_shape mode	1.000000
raised when	error	0.075000
and should be removed and	scan_module compress outs	0.076923
of 3d	nnet conv3d	0.071429
fill	fill	1.000000
of variables [v1 v2 v3 ]	vm linker compute gc dependencies variables	0.250000
variable on the gpu	gpuarray as gpuarray variable	0.166667
logsoftmax x 's grad	local logsoftmax grad	0.200000
required return c code to extract a	c extract name	0.250000
max and argmax over a given axis	max and argmax	0.125000
a specified factor takes as input a	tensor signal pool 2d input	0.090909
false we can't change the value after	core config param init default filter	0.040000
special compound l{op}	argmax1hot	0.058824
that gets a scan	not_required inputs	0.250000
for convop that unroll the batch	code unroll batch kern d unroll_bsize unroll_ksize	0.125000
helper function drawing from multinomial distributions	multinomial helper random_state n pvals size	0.500000
to sharedvariable instances of suitable dummy values	local meta optimizer provide inputs	0.200000
broadcasted dimensions	tensor tensor py	0.015873
this generates the c code for corrmm	corr mm c code	0.090909
the	view	0.022727
c code to declare variables that will be	gof clinker type c declare	0.333333
create	maker create input_storage trustme storage_map	0.500000
[true] division inverse	tensor true div	0.250000
if the passed-in key is found in	get from key key key_data	0.111111
map	map	0.285714
start gradients up to	start	0.040000
to wait on a	mpisend wait	0.045455
at struct	struct node	0.062500
match a variable with the -x pattern	nnet is neg var	0.166667
of this op could be very easy	prod l op	0.033333
return a thunk	thunk	0.021277
a mrg stream state and	sandbox mrg random streams	0.033333
list of compilation flags from config blas ldflags	ldflags libs flags libs_dir include_dir	0.333333
of nodes that must	gof	0.002381
links all the specified vars to a	merge new_best	0.142857
reorder the dimensions	tensor tensor py operators	0.015625
helper function to generate permutations from	tensor permutation helper random_state	0.333333
updates for matrix solve operation c =	solve	0.032258
for elemwise and gpuelemwise	tensor local elemwise fusion	0.166667
-> dot y t x t	local lift transpose through dot node	0.333333
replace element i of shape_of[r] by s_i	i r i s_i	1.000000
a sparse format instead	core sparse	0.066667
compiled module from the loaded cache or	gof module cache get module name	0.166667
with	scan_module	0.125000
a numpy	tensor	0.003215
remove broadcastable dimensions from the shape	tensor py operators squeeze	0.200000
dlimport	dlimport	1.000000
to helper_c_code	get helper c code	0.142857
exception while annotating	node thunk exc_info storage_map	0.250000
when enabled change all sigmoid to ultra_fast_sigmoid	tensor nnet local ultra fast sigmoid node	0.200000
mini-batch of a stack of 3d inputs	input_shape filter_shape	0.009259
along the specified	sparse_grad	0.125000
node	type	0.011905
recompute	recompute	1.000000
of var shape[i], but apply if possible the	i var i fgraph	0.200000
by b with	b	0.014925
comparator to represent the dependence of nodes in	dependence cmp	0.111111
specified pieces of vectors and	sparse block gemv make node	0.066667
something that is not	not	0.076923
subprocess_popen returning the output error and exit code	misc output subprocess popen command	0.100000
required return	name	0.055556
return a version of var transferred to target	tensor transfer var target	0.200000
replace_all_validate revert the replacement if the	replace all validate remove	0.111111
a slice	slice	0.038462
test	fun pt n_tests rng	0.500000
var	var	0.321429
batch normalization	dnn batch normalization	0.125000
with	dims	0.166667
node by one which	node output_indices alloc_ops	0.142857
compile	compile	0.384615
reduce{scalar op}(join(axis=0 a b), axis=0) -> elemwise{scalar op}	tensor local reduce join	0.111111
if fgraph is	fgraph no_recycling profile	0.200000
subtensor(setsubtensor x y idx idx) -> y	local subtensor inc subtensor	0.500000
the input to a	input	0.023810
return connection pattern of subfgraph defined	compile op from graph connection pattern	0.076923
multiplication	alpha	0.083333
grad of this op could	op	0.009174
graph	gof function graph	0.031250
r shape[i] for tensor variable r int i	shape feature shape ir i r	0.500000
value after the import	default filter	0.040000
if target is 'cpu' this will transfer	operators transfer target	0.500000
variable with a	variable	0.022222
directory and return full path	gof module name from dir	0.071429
shape that broadcast them	generate broadcasting	0.066667
compute the element-wise exponential linear activation function	nnet elu x alpha	1.000000
return complex-valued tensor with real and imag components	real imag	1.000000
of compilation flags from config blas ldflags	ldflags libs flags libs_dir include_dir	0.333333
scrap the dimshuffle and index the	tensor local dimshuffle	0.052632
replace it with logsoftmax x 's	local logsoftmax	0.076923
convolution implementation by	sandbox convolve kerns kshp nkern images	1.000000
function is a thunk that operates	gof linker make thunk	0.045455
dimensions of this variable optionally inserting broadcasted	tensor tensor py	0.015873
updates ordereddict the	get updates	0.034483
list of lib directories that	header dirs	0.045455
|a|	tensor abs	1.000000
that reduces scan	scan	0.017241
dependence of nodes in	make dependence	0.043478
see whom can be removed from the scan	scan can remove outs	1.000000
inserted in the struct initialization code	init code struct	0.125000
tuple	tuple	1.000000
a six moves	six	0.025000
if allow_override is false	filter allow_override	0.142857
by	clinker type	0.250000
of shape	tensor shape feature default infer shape	0.066667
for corr3dmm (direction="forward"), corr3dmm_gradweights	tensor nnet base corr3d mm	0.333333
scan to outside of scan	scan	0.017241
in the	bad destroy	0.034483
v given that v	v	0.011111
hash equal for same kinds of tensortype	hash	0.055556
convolving a mini-batch of a stack	input_shape filter_shape	0.027778
the compile lock to be held	module cache add to cache module key module_hash	0.166667
of a dense vector	x s	0.142857
high	high	1.000000
y have	y	0.026316
alloc of 0	alloc	0.012500
python litterals	make	0.017857
the given axis es	axis ddof keepdims	0.083333
conda offers to make itself the	to os environ pathlist	0.038462
an apply_node recursively search from	apply_node check reason	0.066667
apply_node recursively search from this	import apply_node check reason	0.066667
* y + alpha * dot a x	tensor gemv c code y a x	0.500000
spatio-temporal filters with	conv3d signals filters	0.111111
all intermediate variables given input	of variables fgraph input_shapes	0.250000
the permutation by doing a recursion over	tensor permute row elements rec	0.047619
dimensions of this variable optionally	py	0.014286
0-d value underlying variable v if v	value v	0.500000
n-th order discrete difference along given axis	diff x n axis	0.500000
dictionary data structures	non	0.071429
the outer product of two sets of pieces	sparse block outer	0.047619
series of	linker many linkers wrappers	0.047619
an index array and a	a	0.008065
this variable optionally inserting broadcasted	tensor py operators	0.015625
simple c snippet using current	march flag	0.250000
the specified pieces of vectors and	sparse block gemv make node o w h	0.066667
more multinomial distributions defined by one-dimensional slices in	multinomial	0.024390
set of arrays to	choices out mode	0.500000
of apply nodes	gof sort apply nodes	0.200000
specified outputs	fgraph	0.012195
return permutations	permutation random_state	0.500000
contents of a cache directory	from dir dirname err files	0.166667
this	nnet	0.016129
default that removes	remove	0.035714
impossible to	destroy	0.009709
for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x i0 i1	0.083333
return a tuple of integers indicating the version	c code cache version apply node	0.125000
val [x y] -> alloc(val[ ])	local subtensor of	1.000000
asserts from the graph	assert	0.111111
copy a function copied function have separate intermediate	function copy	1.000000
of this variable optionally inserting broadcasted	tensor py	0.015873
elemwise round half to even	sparse rint	0.250000
return a thunk that	thunk	0.021277
variable optionally inserting broadcasted dimensions	tensor tensor py operators dimshuffle	0.019231
diff to make code	diff code	0.333333
of each value in array of ints	bincount x weights minlength assert_nonneg	0.125000
a b), axis=0) -> elemwise{scalar op} a	local reduce join	0.111111
to the type's :attr context_name	gpuarray gpu	0.045455
search through a	search	0.125000
round_half_to_even_inplace	round half to even	0.166667
the replacement if the ops in the list	replace all	0.050000
only used to determine the broadcast pattern for	tensor adv index broadcastable pattern	0.066667
rng	rng	1.000000
u returns	gof	0.002381
graph that compute the variable out for occurrences	out	0.018519
full qr decomposition	qrfull	1.000000
interface definition for op subclasses compiled by clinker	clinker op	0.071429
mflops	flops inputs	0.125000
inverse on	inverse a	1.000000
listeners to help the navigator	navigator optimizer attach updater fgraph	0.038462
beta * y	y	0.026316
dimensions of	py operators dimshuffle	0.019231
variable optionally inserting broadcasted dimensions	py operators dimshuffle	0.019231
mini-batch of a stack of 3d inputs with	input_shape filter_shape	0.009259
to a leftdims	leftdims	0.090909
exception	raise	0.076923
convert python litterals to	tensor make constant	0.100000
function that allows replacing subgraphs	compile rebuild collect shared outputs inputs replace	0.500000
changes node inputs[i] to	gof function graph change input node i	0.250000
the mflops	corr3d mm flops inp outp	0.125000
compute 1d kernel	tensor nnet	0.017544
a special compound l{op}	argmax1hot	0.058824
if cudnn can't be used	gpuarray no cu dnnraise apply fgraph	0.200000
broadcasted dimensions	tensor py operators dimshuffle	0.019231
other implementation of mod	scalar mod c code	0.125000
multiline string representating the cause	bad optimization str diagnostic	0.043478
this is the equivalent of localoptgroup	gpulocal opt group	0.055556
to replace a leaf of a multiplication	replace leaf	0.100000
"reverse-mode" gradient	grad perform node	0.166667
graph of apply nodes according	gof sort apply nodes inputs outputs cmps	0.050000
that	gof function	0.173913
remove	tensor local subtensor remove	1.000000
hack in profiling to print the mflops	nnet conv op flops	0.125000
a -1 and converts this to expm1 a	tensor local expm1	0.066667
see theano tensor repeat	py operators repeat repeats	1.000000
convert degree a	deg2rad a	0.333333
op __init__ fct don't have the same	composite make new inplace output_types_preference name	0.142857
output of scan return	out scan	0.035714
the	type	0.023810
litterals to	make	0.017857
tangent of a (inplace on a)	tensor tan inplace a	1.000000
with logsoftmax x	nnet local logsoftmax node	0.142857
full "updates" dictionary mapping from functiongraph input variables	fgraph updated vars	1.000000
sum along	tensor sum	0.111111
poisson	streams base poisson	0.500000
the output specs	input_specs output_specs accept_inplace	0.142857
in the struct initialization code	init code struct	0.125000
to wait on a previously received	wait	0.022727
return c code to	name sub	0.050000
a list of localoptimizer and applies them to	local opt group	0.052632
helper function to generate permutations	tensor permutation helper random_state n	0.333333
to maintain order	searchsorted x v side sorter	0.142857
reproducible case	compile function dump filename inputs	0.166667
the argmax	argmax	0.066667
lists/tuples/other objects into a list of objects	compile flatten l	0.200000
the	tensor tensor py operators	0.015625
instance associated with a particular stream	tensor random streams getitem item	0.142857
sigmoid to ultra_fast_sigmoid	local ultra fast sigmoid	0.200000
a set of arrays	a choices out mode	0.111111
the l operation on f wrt to wrt	f wrt	0.100000
x is an input vector and t	node	0.007407
return a symbolic row variable (ndim=2	tensor row name	0.050000
inner graph to ensure that it is	validate inner graph	0.035714
structures	scan_module push out non	0.125000
optional return	name	0.011111
errors	errors	1.000000
stream state and they	random streams	0.058824
alloc would be useless this function returns val	tensor alloc call val	1.000000
true if current paramstype contains the specified theano	theano_type	0.125000
has an unification in u	u	0.100000
the output dimensions of convolving an image	nnet conv op get output	0.047619
current paramstype contains the specified theano	theano_type	0.125000
to merge multiplication by	gpuarray alpha merge	0.076923
reorder the	tensor py operators dimshuffle	0.019231
reverse-mode gradient updates for matrix solve operation	solve grad inputs output_gradients	0.333333
utilitary function	typed_list typed	0.333333
we can't change the value after the import	param init default filter	0.040000
computes the confusion matrix	tensor nnet confusion matrix	0.166667
used is the llvm one or not	llvm	0.100000
the output after pad_dims	unpad dims output input leftdims	0.333333
a mini-batch of a stack of 3d	input_shape filter_shape	0.009259
the grad of this op could be very	l op	0.033333
helper_c_code	get helper c code	0.285714
context_name	gpuarray gpu	0.045455
mflops	nnet conv op flops	0.125000
if allow_override	allow_override	0.083333
is an input vector and	node	0.007407
optionally inserting	py	0.014286
determine the broadcast pattern for advancedsubtensor output variable	adv index broadcastable pattern	0.066667
variable	variable x name	0.083333
diagonalsubtensor	tensor nnet get diagonal subtensor	0.083333
to pack c types back into a	clinker type c sync	0.111111
pydotprint	pydotprint	1.000000
and return full	module name	0.062500
performs batch normalization of the given	gpuarray dnn batch normalization	0.125000
diagonal	diagonal	0.666667
see theano tensor argmin	py operators argmin axis keepdims	1.000000
by mapping it to a name	name ctx	1.000000
for use within the op code	op params	0.100000
nested loop over several arrays and	loop	0.027778
to make itself the default python and those	to	0.017544
1/(1+exp x ->	local inv 1 plus exp node	0.333333
the variable out for occurrences of	out	0.018519
are not in this	function	0.052632
by the inputs and	inputs	0.012658
of mod	scalar mod c code node name inputs outputs	0.125000
proxy for either true_div or int_div	scalar div proxy	0.125000
symbolic vector variable	vector name dtype	0.166667
inputs to	inputs	0.012658
list of headers that are needed by one	headers	0.038462
solve operation c = a	solve	0.032258
moved objects in six moves urllib_parse	six moves urllib parse	0.333333
another op that takes the same	op sub	0.066667
offers to make itself	to	0.017544
op could be very easy if	op	0.009174
converts this to expm1 a	local expm1	0.066667
a random	streams base random	0.500000
sigmoid	sigmoid	0.388889
to convert x into a variable on	as gpuarray variable x context_name	0.166667
hyperbolic sine of a	tensor sinh a	1.000000
"reverse-mode" gradient for the	grad perform node inputs	0.083333
data by walking the cache directory	module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
compute 2d kernel for bilinear	tensor nnet bilinear	0.111111
computes the output dimensions of convolving	nnet conv op get output	0.047619
this is the equivalent of localoptgroup for graphtogpu	opt group	0.043478
specified pieces of vectors	sparse block gemv make node o w h	0.066667
list of localoptimizer and applies them to the	local opt group	0.052632
navigator	gof navigator optimizer attach updater	0.038462
from existing start gradients up to the end	wrt end start	0.166667
replace_all_validate	validate remove	0.166667
subtensor(setsubtensor x y idx idx) ->	tensor local subtensor inc subtensor	0.500000
aliasing and destructive operations	destroy	0.009709
the replacement if the ops in	gof replace validate replace	0.050000
all variables which may share	compile infer reuse pattern	0.100000
after pad_dims	unpad dims	1.000000
important note this	scan process node	0.142857
and the second half by b with	b	0.014925
transfer to a tensortype	tensor py operators transfer	0.125000
reverse-mode gradient updates for matrix solve operation c	tensor solve grad inputs output_gradients	0.333333
set of 2d filters	input filters	0.117647
folowing changes in the graph	mul switch sink	0.045455
of this	dimshuffle	0.014493
of header search paths	header dirs	0.045455
to recognize the updates	get updates	0.034483
is the	gcc	0.023810
if l has any duplicates (according to __eq__)	scan_module has duplicates l	0.111111
stabilization optimization that is more general than crossentropy_to_crossentropy_with_softmax_with_bias	nnet crossentropy to crossentropy with softmax fgraph	1.000000
revert the replacement if the ops in	gof replace validate replace all	0.050000
normal	tensor random streams base normal	0.500000
inplace optimization that deals with allocempty this will	gpuarray inplace allocempty op idx	0.166667
"reverse-mode" gradient for	grad perform node inputs	0.083333
compiled module from the loaded cache	module cache get module	0.166667
max_wait	max_wait	1.000000
operation on f	f	0.105263
for graphtogpu	graph to gpulocal	0.055556
the name the object	name	0.011111
order a graph of apply	apply	0.016667
version	cache version apply node	0.125000
gradient wrt inputs for abstractconv2d	abstract conv2d grad inputs	1.000000
to add the	tensor	0.006431
to get	get depth	0.050000
helper function for diagonalsubtensor	tensor nnet get diagonal subtensor view x i0	0.083333
that wasn't	bad view	0.027027
dimensionality of the var is equal	tensor is flat var	0.200000
badviewmap exception when it detects the following	check viewmap node	0.111111
the c code for this composite op	composite init c code	0.333333
apply nodes according to a list of	apply nodes inputs outputs cmps	0.050000
when we overwrite the full inputs	tensor local useless inc subtensor node	0.066667
reduce pattern has functioning c	careduce cuda supports c	0.200000
scalar	tensor scalar	0.333333
broadcasted	tensor	0.006431
make a nested	make	0.017857
its idx_list reorders the inputs according to	inputs idx_list get_count	0.100000
that is not	not	0.076923
remove broadcastable dimensions from	py operators squeeze	0.200000
a list of compilation flags from	libs flags libs_dir include_dir	0.052632
to the input specs and the output specs	compile std fgraph input_specs output_specs accept_inplace	0.142857
loop over several arrays and associate specific code	loop	0.027778
of apply nodes according	gof sort apply nodes inputs outputs cmps	0.050000
persist ndarrays in an object by saving them	ndarray id	1.000000
help the navigator deal with the	gof navigator	0.038462
by default that removes all	remove all	0.166667
c	shape c	0.250000
w	w	1.000000
trace to an node or variable	trace	0.052632
reorder the dimensions of this variable optionally	py operators dimshuffle	0.019231
from type1 constitutes an upcast	is an upcast type1	0.500000
column variable (ndim=2 broadcastable=[false true])	tensor col name dtype	0.200000
the theano enumeration types	params type	0.250000
wrapper around c_extract_out that initializes py_name from storage	gof get c extract out r name sub	0.333333
specified pieces of vectors	sparse block gemv make	0.066667
the	gof	0.011905
constant that is	constant error	0.166667
x and y	x y	0.024390
type's :attr context_name	gpuarray gpu array type	0.062500
non	non	0.357143
occurrences of values identical with	scan_module forced replace	0.333333
pieces of vectors	sparse block gemv make node o w h	0.066667
1 0/a	inv	0.142857
destroyhandler wasn't	destroy handler	0.055556
of average pooling	average pool	0.200000
convolution for	abstract conv conv	0.125000
of a cache directory and	dir	0.076923
this function tries to	image_shape top_shape border_mode subsample	0.166667
real-valued input on the gpu	gpuarray curfft inp norm	0.066667
end	wrt end	0.050000
det x and there is	local det	0.166667
product of	sparse block	0.111111
inner graph of scan to outside of scan	out scan	0.035714
of this variable optionally inserting	tensor py	0.015873
output_grad	output_grad	1.000000
the dimensions of this	tensor tensor	0.014286
op then replace it with a triangular solve	solve triangular node	0.142857
compute sum of non nan / inf values	tensor tensor constant signature get sum	0.142857
as replace_all_validate revert the replacement if the ops	replace validate replace all validate remove	0.111111
the equivalent	graph to gpulocal	0.055556
instance full of the code for our fgraph	gof clinker get dynamic module	0.200000
x shp -> alloc(unary x shp)	local alloc unary	0.250000
context object mapped to the type's :attr context_name	gpuarray gpu array type context	0.090909
:type cost	cost wrt consider_constant disconnected_inputs	1.000000
power	tensor pow	1.000000
convert radian	tensor rad2deg	1.000000
and should be removed and	outs	0.050000
by default that removes	tensor local remove	0.166667
of shape "inshp" with kernels of shape	shape	0.010204
replacement if the ops in	validate replace	0.050000
:attr	array	0.041667
sum(a axis=[]) ->	tensor local useless reduce node	0.500000
sharedvariable instances of suitable dummy values	optimizer provide inputs	0.200000
to recognize the updates ordereddict the list of	scan_module get updates	0.034483
is unified to boundvariable(other_object)	unify walk fv o u	0.200000
raise	check inputs	0.125000
generates the c code for corrmm (direction="forward"),	tensor nnet base corr mm c code	0.090909
helper function for diagonalsubtensor	nnet get diagonal subtensor view x	0.083333
merge 2 profiles returned	optimizer merge	0.200000
es of	dtype op	0.250000
duplicate this apply instance in a new graph	gof apply clone with new inputs	0.250000
output	gof	0.002381
wraplinker that runs a	gof wrap linker	0.083333
for operations that need	gpu kernel	0.250000
compute sum of non nan / inf	constant signature get sum	0.142857
permutations of	tensor permutation random_state size n ndim	0.500000
returns the connection pattern of	connection pattern	0.032258
module if the	gof module	0.058824
print the mflops	conv op flops	0.125000
sum(a axis=[]) -> a	tensor local useless reduce node	0.500000
that unroll the batch size loop	tensor nnet gen conv code unroll batch	0.166667
the original graph to a new node a	inputs outputs copy_inputs_and_orphans memo	0.029412
permute	permute	1.000000
compute sum of non nan / inf	tensor constant signature get sum	0.142857
return an instance of _maker which handles much	debug mode function maker i o m	0.066667
square of a (inplace on a)	tensor sqr inplace a	1.000000
meta path importer to import six moves and	six meta path importer	0.333333
is	no_recycling profile	0.250000
important note this function uses	push out seq scan process node	0.142857
when	error	0.025000
variable v is	v	0.011111
python litterals to	constant	0.016667
folding	folding	1.000000
six moves urllib namespace that	six	0.025000
the original	outputs copy_inputs_and_orphans memo	0.029412
the mflops	abstract conv flops inp outp	0.125000
unfortunately conda offers to make itself	to os environ	0.038462
the bartlett spectral window in the time-domain	tensor bartlett m	0.083333
graph's apply nodes such that	gof function graph	0.031250
function expects the compile lock to be held	add to cache module key module_hash	0.166667
for the maximum	maximum	0.083333
a variable that represents their unification	gof unification	0.125000
array	array	0.208333
number of scalars together	make	0.017857
directory	dir	0.076923
remove item from six moves	remove move name	1.000000
of an empty matrix it does	alloc	0.012500
nodes in the original graph to a new	outputs copy_inputs_and_orphans memo	0.029412
a tuple of integers indicating the version	cache version apply node	0.125000
contents of a cache directory and return full	gof module name from dir dirname err files	1.000000
function for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor	0.083333
kinds of tensortype	type	0.011905
we parametrize it to	max_input_fct maker	0.083333
optimization disabled by default that removes	local remove	0.166667
unique names to an iterable of variables modifies	give variables names variables	0.333333
create a six moves urllib namespace that	six	0.025000
implemented returns	gof	0.002381
from a with or	size a replace p	0.333333
change the value after	param init default	0.040000
specific ops	trace f_or_fgraph ops_to_check bug_print	0.035714
updates ordereddict	updates	0.029412
stack trace from one or more	stack trace	0.055556
return the inputs required to compute	gof inputs variable_list blockers	0.058824
of var transferred to target	transfer var target	0.200000
of nesting	loop_orders dtypes loop_tasks sub	0.125000
of the destroyhandler	add destroy handler	0.125000
representating the cause	bad optimization str diagnostic	0.043478
two kinds of useless reshape	local useless reshape	0.200000
existence of the __unify_walk__ method	unify walk a b u	0.037037
mat	mat	1.000000
raised when grad is asked to compute	error	0.025000
variable optionally inserting	tensor	0.006431
"init_code"	struct	0.047619
exp a -1 and converts this to expm1	tensor local expm1 node	0.066667
type optionally with a new dtype	tensor tensor type clone dtype	0.333333
to	gpuarray gpu inc subtensor	0.333333
the python 3 namespace	moves urllib	0.038462
list of l{codeblock} instances returns a string	gof code gen blocks	0.050000
scalars together into	make	0.017857
a cache directory and return full	module name from dir	0.071429
math where x is an input vector and	node input_storage output_storage	0.038462
bitwise a ^	tensor xor	0.333333
similar behaviour as haskell' foldr	scan_module foldr fn sequences outputs_info	1.000000
this function tries to	top_shape border_mode	0.166667
the source code for this	clinker compile cmodule location	0.038462
square symmetrix matrix	sandbox linalg spectral radius	0.166667
warning message on the first	gof deprecated filename msg	0.041667
same computations	scan_module equal computations	0.333333
toposort return an ordering of	graph toposort	0.125000
the given axis es of	axis ddof keepdims	0.083333
the "reverse-mode" gradient	grad perform	0.166667
for elemwise and gpuelemwise	local elemwise	0.166667
exit code	subprocess popen command	0.083333
calculating the dot product	dot x	0.166667
unroll the batch size loop	tensor nnet gen conv code unroll batch	0.166667
to a variable that represents their unification	gof unification	0.125000
variable with a sparse matrix	sparse variable x name	0.250000
to construct a variable with	variable x name	0.083333
operation on f	core rop f	0.166667
:param execute if true execute a theano function	misc execute execute verbose m n	0.250000
functionally identical	already there	1.000000
and return full path	module name	0.062500
a variable of	a	0.008065
a 'requirement' of the destroyhandler	add destroy handler	0.125000
the mflops	gpu corr3d mm flops inp outp	0.125000
nested loop over several arrays	loop	0.027778
convert data to something which can be associated	type filter data strict allow_downcast	0.250000
function for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x i0	0.083333
dimensions	dimshuffle	0.028986
error and exit code	subprocess popen command	0.083333
shared variable names when persisting to zip file	persistent shared variable id	0.142857
not attempting to use dnn conv algo_bwd	safe no dnn algo bwd algo	0.166667
gradient wrt inputs for corrmm	corr mm grad inputs	1.000000
complete hashable signature of the module we compiled	clinker cmodule key	0.166667
proxy for	proxy	0.095238
validations on the inner graph to ensure	scan validate inner graph	0.035714
bitwise a ^	xor	0.125000
conda offers to make itself	to	0.017544
local optimization wants to add some requirements	local optimizer add requirements	0.500000
kinds	tensor tensor	0.014286
the numpy randomstate instance associated with a particular	setitem item val	0.125000
the	gpu	0.035294
to get the 0 based level	get depth	0.050000
duplicate this apply instance	gof apply clone	0.166667
six moves urllib namespace that resembles the	six	0.025000
multinomial distributions defined by	tensor multinomial random_state	0.040000
map old node to new node	check_integrity	0.090909
work for gpuincsubtensor	setsubtensor node	0.250000
to determine the broadcast pattern for advancedsubtensor	adv index broadcastable pattern a	0.066667
the graph and get a	graph	0.016393
to be between min	min	0.071429
by this	gof clinker	0.033333
create a six moves urllib	module six	0.043478
used to upsample	ratio normalize	0.200000
revert the replacement if the ops in	replace all	0.050000
function computes the output shape	get out shape ishape kshape border_mode subsample	0.500000
the last access of a	last access time path	0.040000
with respect to wrt, computes gradients of	subgraph grad	0.062500
and only if this enum has	gof enum type has	0.111111
multinomial distributions defined	multinomial random_state	0.040000
a real-valued input on the	gpuarray curfft inp norm	0.066667
dict op -> total number	profile stats class	0.500000
for use within the op	op params	0.100000
revert the replacement	validate replace	0.050000
type's :attr	type	0.011905
helper function to draw random integers	tensor random integers helper random_state low	1.000000
remove	remove	0.285714
signature	signature	0.333333
input vector and t	node input_storage	0.038462
this type	gof clinker type c code	1.000000
to track less frequent op	tensor get clients2 node	0.200000
nested loop over several arrays and associate specific	loop	0.027778
r's shape in the	tensor shape	0.058824
of a sparse matrix and a dense vector	svcsr	0.090909
matrix	sp	0.125000
stop step] transform it into a canonical	canonical	0.076923
where n >= 3	3d	0.100000
that wasn't	bad destroy	0.034483
instances	tensor	0.003215
of this op could be	l op	0.033333
return connection pattern	connection pattern	0.032258
real	real	0.875000
sample from	size n	0.090909
value after the import of theano	config param init default	0.040000
compute the numeric shape of all intermediate variables	tensor shape of variables fgraph input_shapes	0.100000
computes the output shape for a convolution	gpu dnn conv get out shape ishape kshape	0.333333
the apply to be inserted in	apply node	0.031250
for a convolution with	gpu dnn conv	0.200000
the ops in the list remove are still	replacements remove reason	0.055556
the destroyhandler class detects when a	handler	0.071429
gradient w r	conv3d grad	0.111111
into macros for use	cop get	0.033333
op and reduce pattern has functioning	careduce cuda supports	0.166667
toposort return an ordering of the graph's	toposort	0.076923
<	lt	0.166667
converts self _rop_op from user supplied form to	op from graph recompute rop	0.200000
this compiles the source code	cmodule location	0.038462
according to a list of comparators	inputs outputs cmps	0.166667
code when doing constant folding of this node	tensor elemwise python constant folding node	1.000000
numeric shape of	tensor shape	0.058824
of two sets	sparse	0.019231
of mod	mod c code	0.125000
constant scalar	scalar constant	0.285714
that x and y have	x y	0.024390
accept complex	complex	0.125000
:func images2neibs <theano tensor nnet	nnet images2neibs	0.333333
transfer to a tensortype if not already	py operators transfer	0.125000
a numpy ndarray value	tensor	0.003215
if the alloc	tensor alloc	0.333333
helper function for diagonalsubtensor and	get diagonal subtensor view x i0	0.083333
generates the c code for corrmm (direction="forward"),	nnet base corr mm c code	0.090909
the folowing changes in	mul switch sink	0.045455
max	max axis keepdims	1.000000
computes the output dimensions of convolving	output	0.017241
convolving a mini-batch of	input_shape filter_shape	0.027778
this op could be very easy	l op	0.033333
raise	node storage_map r_vals	0.166667
this apply instance in a	gof apply	0.090909
will match self if	var	0.035714
to support version-based cache mechanism	gpuarray code version version	0.333333
offers to make itself the	to os environ pathlist var	0.038462
dimensions of this variable optionally	operators	0.017241
takes as input	signal max pool 2d same size input	0.500000
output	op get output	0.047619
that compute the variable out for	out	0.018519
specific ops of a compiled graph have	trace f_or_fgraph ops_to_check bug_print	0.035714
macros	cop get	0.033333
an operation	mpirecv	0.037037
to the fgraph outputs that	fgraph expanded_inputs	0.058824
the context object mapped to the type's :attr	type context	0.090909
see theano tensor prod	py operators prod axis dtype	1.000000
litterals to	constant	0.016667
another op that takes the	op sub	0.066667
uniform distribution between low and high	tensor uniform random_state size low high	0.333333
tries to recognize the updates ordereddict	updates	0.029412
offers to make itself	to os environ pathlist var	0.038462
to get the 0 based	get depth	0.050000
ordering of the graph's apply nodes such that	gof function graph	0.031250
of all the node i pairs such that	gof function graph clients	0.100000
decorator which will print a warning message on	gof deprecated filename msg	0.041667
eval	eval	1.000000
a legal value for a variable of	is valid value a	0.076923
function to get the 0 based	get	0.020833
gammaln	gammaln	1.000000
the conventions imposed by python	theslice length	0.052632
division	true div	0.250000
the output dimensions	output	0.017241
convolution with the specified parameters	dnn conv get	0.100000
of integers indicating the version	cache version apply	0.125000
*args directly	local csm properties csm	0.142857
symbolic	name ndim dtype	0.333333
don't accept complex otherwise call upgrade_to_float()	scalar upgrade to float no complex	0.333333
variable v is positive semi-definite	v	0.011111
reorder the dimensions of	operators dimshuffle	0.019231
functiongraph listeners to help the navigator deal with	gof navigator optimizer attach updater fgraph	0.038462
provided l{functiongraph} it may	gof optimizer apply fgraph	0.200000
setsubtensor(x x[idx], idx) -> x when x	tensor local setsubtensor of constants node	0.250000
help the navigator deal with	gof navigator optimizer attach updater	0.038462
output dimensions of convolving an image of shape	conv op get output	0.047619
log2_exponent	log2_exponent	1.000000
simple c snippet using current flags	march flag flags	0.333333
and	tensor local	0.076923
full of the code for our fgraph	gof clinker get dynamic module	0.200000
load a file that was dumped	misc load f persistent_load	0.333333
the	gpu array type	0.125000
input	pool 2d same size input	0.500000
symbolic vector variable	tensor vector name dtype	0.166667
function is a thunk that	gof linker make thunk	0.045455
an apply_node recursively search from	apply_node check	0.066667
- the argument to the exception exc -	exc	0.100000
to print the mflops	gpu corr mm flops inp outp	0.125000
:param execute if true execute a theano	execute execute verbose m	0.250000
for	base gpu	1.000000
out for	out	0.018519
new instance of this mode	compile mode clone link_kwargs optimizer	0.333333
variable	operators	0.017241
compute conv output gradient w	tensor nnet conv3d grad	0.333333
tensor variables to one or more tensor variables	from_var to_var	0.250000
dot product of the specified pieces of vectors	sparse block gemv make node o w	0.066667
recognize the updates	scan_module get updates	0.034483
of this variable optionally inserting broadcasted dimensions	tensor	0.006431
in array of ints	weights minlength assert_nonneg	0.125000
as replace_all_validate revert the replacement if the	replace validate replace all validate remove fgraph	0.111111
give	give	0.714286
that unroll the batch	nnet gen conv code unroll batch	0.166667
return connection pattern	graph connection pattern	0.076923
an instance of _maker which handles much	function maker i o m	0.066667
min_age	min_age	1.000000
variant on wraplinker that runs a series	gof wrap linker many linkers wrappers	0.071429
necessary update dr_vals	r_vals dr_vals	0.250000
to make code	code filename	0.333333
wait on a previously received array using	mpirecv wait	0.045455
op __init__ fct don't have the same	cast make new inplace output_types_preference name	0.142857
required return c code to	name sub	0.050000
list to	from factored list	0.500000
of apply nodes according to a list of	gof sort apply nodes inputs outputs cmps	0.050000
ignore_trees-related functionality	updater fgraph importer pruner chin	0.250000
kshape	kshape	1.000000
will attempt to convert x into a variable	variable x context_name	0.250000
as replace_all_validate revert the	validate remove fgraph	0.166667
raises a badviewmap exception when it detects the	compile check viewmap node	0.111111
code	code inputs	1.000000
for same kinds of	tensor tensor type	0.041667
revert the replacement if the	replace validate replace	0.050000
with constant	subtensor get constant idx	0.250000
meta compiler that offer some generic function	compiler	0.166667
be held	cache module key module_hash	1.000000
compute 1d	nnet	0.016129
sigmoid to ultra_fast_sigmoid	tensor nnet local ultra fast sigmoid node	0.200000
this for a diagnosis if things go awry	function graph check integrity	0.250000
real and	tensor complex real	0.500000
and	node name sub	0.111111
xor	xor	0.625000
associate linker with fgraph	op wise clinker accept fgraph no_recycling	1.000000
the given axis es of a tensor	axis dtype op	0.083333
along the given axis	axis	0.153846
function	fn	0.083333
compiles this linker's fgraph	thunk input_storage output_storage storage_map keep_lock	0.333333
row is a mrg	mrg	0.076923
scan memory consumption	scan save mem	0.200000
convert addsd to faster	local addsd ccode	0.250000
operation on f wrt to wrt	f wrt	0.200000
compiling it if necessary	key key lnk keep_lock	1.000000
gist and return the url	gist content description filename auth	0.250000
defined by indices out_idxs	out_idxs	0.050000
node inputs[i] to	gof function graph change input node i	0.250000
links all the specified vars to a variable	merge new_best	0.142857
broadcast	broadcast	1.000000
"lifts" dimshuffle through elemwise operations and merges	local dimshuffle lift node	0.250000
operators to the basic variable class	variable	0.022222
an input vector and t	node input_storage	0.038462
returns the bartlett spectral window in the	bartlett m	0.083333
returns the signature for this function	gof ext function method decl	0.333333
to find broken optimizations	compile find	0.333333
associated with a particular	setitem item val	0.125000
some perform() or c_code() modified an input	destroy map	0.142857
and reduce pattern has functioning c	gpuarray gpu careduce cuda supports c	0.200000
images	images	1.000000
is a thunk	thunk	0.021277
det x and	sandbox linalg local det	0.166667
object with debug	with op node thunk	0.166667
code string specific to the apply	apply	0.016667
to recognize the updates ordereddict	scan_module get updates	0.034483
inputs with a set of 3d filters	nnet conv3d input filters	0.142857
the diagonal of an empty matrix it	alloc diag	0.027027
output	get out	0.500000
respect to wrt, computes gradients	core subgraph grad	0.062500
to the diagonal of	diag	0.023810
random	tensor random	0.500000
see theano tensor sum	py operators sum	1.000000
remove incsubtensor when we overwrite the full	local useless inc subtensor node	0.066667
toposort	graph toposort	0.125000
the constant scalar 0-d value	get scalar constant value orig_v elemwise only_process_constants max_recur	0.090909
which can be referred to	compile register linker	0.250000
this is the equivalent	graph to gpulocal	0.055556
the gpu	gpu optimizer	0.500000
for	gpu	0.200000
apply_node recursively search from this node to	apply_node	0.050000
the g++	gof	0.002381
dict op -> total	compile profile stats class	0.500000
global optimizer for pushing out the	out	0.018519
op __init__ fct don't have the	cast make new inplace output_types_preference name	0.142857
functiongraph listeners to help the navigator deal	gof navigator optimizer attach updater	0.038462
input that wasn't in the	bad	0.013158
can't change the value after	core config param init default filter	0.040000
be turned into macros for use	cop	0.028571
a new graph	new inputs inputs strict	0.166667
return a symbolic scalar	scalar name dtype	0.166667
part	part	1.000000
decorator for the new graphtogpu optimizer	gpuarray register opt2 tracks	0.250000
the main interface to manipulate the	r new_r reason verbose	0.071429
defined by given inputs and	inputs	0.012658
bessel function of	tensor j0	1.000000
important note this	scan process node fgraph node	0.142857
make a	make	0.035714
apply_node recursively search from this	apply_node check reason	0.066667
in a new	new	0.058824
op could	op	0.009174
node by one which	node output_indices	0.142857
set of 3d filters	conv3d input filters	0.142857
gpucareducecuda is a reduction along some dimensions by	gpu careduce cuda	1.000000
lock is removed	lock	0.100000
updates for matrix solve operation c	tensor solve	0.038462
this variable optionally inserting broadcasted	tensor py	0.015873
x default reverse them	tensor transpose x	0.200000
return a symbolic 3-d	tensor tensor3 name dtype	0.166667
the c code for gpucorrmm (direction="forward"), gpucorrmm_gradweights	gpu corr mm c code	0.090909
in the default blas in macos	macos sdot	1.000000
or broadcastable pattern	broadcastable	0.111111
compute 2d kernel for bilinear upsampling	tensor nnet bilinear	0.111111
the indptr	indptr	0.125000
sharedvariable constructor for	shared value name strict allow_downcast	0.500000
this is the equivalent of	gpulocal opt	0.055556
empty matrix it does	alloc	0.012500
feature should remove	feature on	0.200000
return path to the module	get entry	0.333333
fetch	fetch	1.000000
n-d tensor where n >= 2	ws ignore_border stride	0.045455
by default that removes	local remove	0.166667
the destroyhandler class detects when a graph	handler	0.071429
a comparator to represent the dependence	dependence cmp	0.111111
returns the signature for	method decl	0.333333
indicating the version	c code cache version	0.125000
see theano tensor min	tensor tensor py operators min	1.000000
nit_sot output of scan return true iff the	push out scan	0.050000
the replacement	replace all	0.050000
of the main diagonal set to a specified	tensor fill diagonal offset a val offset	0.100000
list of l{codeblock} instances returns a	gof code gen blocks	0.050000
raised by get_scalar_constant_value if	error	0.025000
the apply to	apply	0.016667
the forward but clip	clip x lower_bound upper_bound	0.090909
to compute	nnet	0.064516
to generate c	register specify shape c	0.250000
that will be turned into macros	cop get	0.033333
for same kinds of tensortype	tensor	0.006431
subtensor(setsubtensor x y idx idx) -> y	local subtensor inc subtensor node	0.500000
incsubtensor when we overwrite the full	local useless inc subtensor node	0.066667
tile input array x according to	tile x	0.500000
of _maker which handles much of the debugging	mode function maker i o m	0.066667
to wrt,	subgraph	0.047619
to assert that x and y have	x y	0.024390
a mrg stream state and they	sandbox mrg random streams	0.033333
strings check if converting to type2	type2	0.050000
mean	mean	0.437500
a mini-batch of	input_shape filter_shape	0.027778
hyperbolic tangent of	tensor tanh	1.000000
convert x into a variable on the gpu	as gpuarray variable x context_name	0.166667
c code to initialize the variables	clinker type c	0.083333
to manipulate the subgraph in functiongraph	gof function graph replace r new_r reason verbose	0.250000
the compile lock to be held	to cache module key module_hash	0.166667
draw samples from a poisson distribution	poisson size lam	1.000000
wrapper around c_extract that initializes py_name from storage	gof get c extract r name	0.250000
dimshuffle is inside an alloc and only	local alloc dimshuffle node	0.166667
the updates ordereddict	get updates	0.034483
linker	linker	0.714286
offers to make	to os	0.038462
split{n_splits=1} x y -> x	tensor local	0.025641
each row correspond to the one hot	tensor to one hot	0.142857
unconditional start-to-finish program execution in python	loop	0.027778
partition a list of variables into	scalarconsts	0.076923
this function is only used to determine the	tensor adv index broadcastable	0.050000
the dimensions of	dimshuffle	0.014493
dimensions of	py operators	0.015625
std	std axis ddof	1.000000
the dimensions of this	operators dimshuffle	0.019231
the dimensions of x default reverse them	tensor transpose x axes	0.200000
from	compile op from	1.000000
(comma-separated key=value components) into a dict	config_string issue_warnings	0.333333
conv3d	conv3d	0.384615
the folowing changes	mul switch sink	0.045455
to determine the broadcast pattern	adv index broadcastable pattern	0.066667
an alloc and	local alloc	0.111111
loading of moved objects in six moves urllib_request	module six moves urllib request	0.333333
dimensions	py operators dimshuffle	0.038462
the specified pieces of vectors	sparse block gemv make node o w	0.066667
linker's fgraph	make thunk input_storage output_storage storage_map keep_lock	0.333333
for theano scalar scalar and tensorvariable	scalar as common dtype	0.250000
to use dnn conv workmem	core safe no dnn workmem workmem	0.166667
of 3d	nnet conv3d input	0.125000
an	mpirecv	0.037037
sharedvariable constructor for scalar values	scalar shared value name strict allow_downcast	0.200000
mflops	gpuarray base gpu corr3d mm flops inp outp	0.125000
full of the code for our fgraph	clinker get dynamic module	0.200000
it work for elemwise and gpuelemwise	tensor local elemwise fusion	0.166667
manipulate the	r new_r reason verbose	0.071429
the list remove are	remove reason	0.142857
the dimensions	operators	0.017241
wait on a previously sent array using	mpisend wait	0.045455
a set of 2d filters	conv2d input filters	0.125000
deepcopy in the fgraph to	deepcopy fgraph	0.500000
work if	gof	0.002381
if necessary update dr_vals	inputs node storage_map r_vals dr_vals	0.250000
op	op c code	0.333333
of v given that v	set v	0.125000
hash equal for	hash	0.055556
the c code for corrmm	corr mm c code	0.090909
return dict d s t d[node] is	function graph orderings	0.200000
has only	sitsot only	0.066667
x ->	tensor local	0.025641
tries to recognize the updates ordereddict the	updates	0.029412
update cache data by walking	refresh age_thresh_use delete_if_problem cleanup	0.166667
structures	push out non seq	0.125000
a list of l{codeblock} instances returns	code gen blocks	0.050000
return the constant scalar 0-d value underlying	get scalar constant value	0.333333
called by remove_feature feature should	gof feature on detach function_graph	0.200000
op code	op	0.009174
performs batch normalization of the given	tensor nnet batch normalization	0.125000
trace from one or more	trace	0.052632
1d kernel that can be used	kernel 1d	0.050000
cost scalar 0-dimensional	core hessian cost	0.500000
return imaginary component of complex-valued tensor z	tensor imag z	1.000000
as python not the other implementation of mod	scalar mod c	0.125000
shp)	alloc unary	1.000000
subtract two matrices at least one	sub x y	0.333333
a transfer function for	transfer fn	0.125000
code to the task	find task	0.142857
class to raise in self	core raise init	0.100000
exit code in a tuple	subprocess popen command	0.083333
be removed	scan_module compress outs	0.076923
unfortunately conda offers to make itself the	to os environ pathlist var	0.038462
similar behaviour as python's reduce	reduce fn sequences outputs_info non_sequences	1.000000
this method is primarily used by tensor rop	gof pure op r op inputs eval_points	0.250000
that copies a	alloc	0.012500
wrapper around c_extract_out that initializes py_name	gof get c extract out r name sub	0.333333
unique names to an iterable of	names	0.047619
version used is the	gcc	0.023810
makes the folowing changes	mul switch sink	0.045455
reorder the dimensions of this variable	tensor tensor	0.014286
copies the subgraph contained between	copy_inputs	0.125000
important note this function uses	out seq scan process node fgraph node	0.142857
badviewmap exception when it detects the following	compile check viewmap node	0.111111
the convolution gradient with	conv grad	0.500000
return connection pattern of subfgraph defined	from graph connection pattern	0.076923
config string (comma-separated key=value components) into	core parse config string config_string issue_warnings	0.166667
to d1+size d2	scan_module expand empty tensor_var size	0.166667
the mflops	nnet conv op flops inputs outputs	0.125000
of policies to name r	policy policy r name	0.250000
litterals to	tensor make constant	0.100000
dimensions of this	py	0.014286
of another op that takes	op sub	0.066667
of _maker which handles much of the	debug mode function maker i o m	0.066667
if a dimshuffle is inside an alloc	alloc dimshuffle node	0.333333
return c code to declare variables	clinker type c declare name	0.500000
retrive the context associated with	gpuarray get context	0.111111
reproducible case for	compile function dump filename inputs	0.166667
given an apply_node recursively search	apply_node	0.050000
functiongraph listeners to help the navigator deal	navigator optimizer attach updater	0.038462
to	constant	0.016667
folowing changes	local mul switch sink	0.045455
nit_sot output of scan return true	push out scan	0.050000
whose	local useless	0.111111
to construct a variable with a	variable x	0.083333
of cutils_ext	compile cutils	0.166667
the last access	last access time	0.040000
special compound l{op} for	softmax argmax1hot	0.083333
fgraph this is	fgraph	0.012195
simplify a multiplication	simplify mul	1.000000
computes the inverse of a	inverse	0.066667
required return data or an appropriately wrapped/converted data	gof pure type filter data strict allow_downcast	1.000000
an apply_node recursively search from	apply_node	0.050000
"reverse-mode" gradient for the eigensystem	eigh grad perform node inputs outputs	0.333333
and tensorvariable	as common dtype	1.000000
number of scalars together into	make	0.017857
return the initial value for	gpu	0.011765
see whom can be removed from the	can remove outs	0.250000
removes useless dimshuffle operation inside reshape reshape(vector	local useless dimshuffle in reshape node	0.500000
arctan	arctan	0.857143
see theano tensor argmax	tensor py operators argmax axis	1.000000
helper function for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor	0.083333
return connection pattern of subfgraph defined	graph connection pattern node	0.076923
important note this function uses set	scan_module push out seq scan process node	0.142857
names to an iterable of variables	variables names variables	0.333333
test	test	1.000000
behavior that open windows	misc subprocess popen command	0.142857
a modulo of m1 and the second	m1	0.027027
for matrix solve operation c = a \	tensor solve	0.038462
variable optionally	tensor tensor py operators	0.015625
an input that wasn't in	bad	0.013158
extract test value from	test value	1.000000
data structures	push out non seq	0.125000
variable optionally inserting	operators dimshuffle	0.019231
downsample with max	downsample factor max	0.500000
the minimum	minimum x	0.142857
navigator deal with the	navigator optimizer	0.037037
convolve spatio-temporal filters with a movie	tensor nnet conv3d signals filters signals_shape filters_shape	0.333333
return a	call name	0.500000
with a	a	0.008065
context associated with a	gpuarray get context	0.111111
a mini-batch of a stack of 3d inputs	input_shape filter_shape	0.009259
c code for this	init c code	0.333333
a global optimizer for pushing out	out	0.018519
to wrt, computes	core subgraph grad wrt	0.062500
tag trace to an node or variable	tag trace thing user_line	0.166667
equivalent of	graph to gpulocal opt	0.055556
source code for this linker and	compile cmodule location	0.038462
true if and only if this	gof	0.002381
broadcast pattern	pattern	0.028571
distributions defined by one-dimensional slices in pvals	pvals	0.071429
profilemode to print the mflops	base gpu corr mm flops inp outp	0.125000
basic slow python	img kern mode dilation	0.250000
to find	compile find bad	0.333333
generate a global optimizer of type topooptimizer	topo db	0.166667
this is the	to gpulocal opt	0.055556
a graph of apply nodes according to a	sort apply nodes inputs outputs cmps	0.050000
functiongraph listeners to help the navigator deal with	gof navigator optimizer attach updater	0.038462
op could be very easy if it	prod l op	0.033333
and exit code in a	subprocess popen command	0.083333
a tensorvariable of this type	tensor tensor type make variable	0.500000
on the inner graph to ensure that	scan validate inner graph	0.035714
replacement if the	replace validate replace	0.050000
register r's shape in the	tensor shape	0.058824
input by a specified factor takes as input	tensor signal pool 2d input	0.090909
diagonalsubtensor and	nnet get diagonal subtensor view	0.083333
each row correspond to the one	tensor to one	0.125000
operators to the	tensor	0.006431
compute the	tensor nnet get	0.250000
op a list of indices	op	0.009174
grad_preserves_dense	grad_preserves_dense	1.000000
hash equal	type hash	0.166667
of the list	list type	0.100000
create a six moves urllib namespace	six	0.025000
spectral	spectral	1.000000
product of two sets of pieces of vectors	sparse block	0.111111
wrap	wrap	1.000000
is the equivalent of localoptgroup	graph to gpulocal opt group	0.055556
this type	tensor type	0.034483
this apply instance in a	gof apply clone	0.166667
fgraph to break aliasing	fgraph wrapped_inputs wrapped_outputs	0.111111
find broken	find bad optimizations2	0.333333
to the type's :attr	array	0.041667
c_extract that initializes py_name from storage	gof get c extract r name	0.250000
minimal type used for passing contexts to nodes	gpu context type	0.333333
this op could be very easy	prod l op	0.033333
may	fgraph	0.012195
optionally inserting broadcasted dimensions	tensor tensor	0.014286
apply as many times	tensor apply	0.142857
optimization makes the folowing changes	local mul switch sink node	0.045455
add tag trace to an node or variable	gof add tag trace thing user_line	0.166667
shape in the	tensor shape	0.058824
apply_nodes to this graph	gof function graph import	0.125000
max and argmax over a given axis or	max and argmax	0.125000
headers that are needed by one or more	clinker headers	0.047619
listeners to help the navigator	navigator	0.032258
of neural-net multiclass classifiers	softmax with bias	0.142857
elemwise floor of x	sparse floor x	1.000000
return those items	gof	0.002381
eq	eq	0.555556
transferred to	tensor transfer	0.500000
thunk that is	thunk	0.021277
shape s to previously un-shaped variable r	shape feature set shape r s override	0.500000
given an fgraph	fgraph outputs_to_disown	0.047619
or 3d convolution for debugmode	abstract conv conv	0.125000
or more multinomial distributions defined by one-dimensional	multinomial	0.024390
normalization of the	normalization	0.153846
source code for this linker and	gof clinker compile cmodule location	0.038462
column variable (ndim=2 broadcastable=[false true])	col name dtype	0.200000
return label	label node	0.250000
cache "filename" as a pickle file	gof call cache persist filename	0.250000
finite fourier	tensor fourier	0.333333
returns	gof	0.023810
make a value	make value	1.000000
elements of each row inner-most dim of a	row elements	0.333333
contiguous	contiguous	0.294118
"lifts" dimshuffle through elemwise operations	dimshuffle lift	0.250000
that has ever been associated	gof	0.002381
sharedvariable instances of suitable dummy values	provide inputs	0.200000
platform-dependent extension for compiled modules	get lib extension	0.333333
elements obtained by iterating over given axis	tensor argmin x axis keepdims	0.500000
the standard deviation along the	tensor std	0.111111
keep	keep	1.000000
the gradient function should	matrix inverse r op	0.500000
return the constant scalar 0-d value underlying	scalar constant value	0.333333
helper function to generate permutations from	tensor permutation helper random_state n	0.333333
(inplace on a)	inplace a	0.333333
unification	gof unification	0.125000
for small or builtin c	c is simple	0.200000
helper function	helper random_state low high	0.500000
an array with more than one	node inputs	0.086957
composite	composite	0.833333
a list of l{codeblock} instances returns a string	gof code gen blocks	0.050000
in the fgraph	fgraph	0.012195
the equivalent of localoptgroup for graphtogpu	graph to gpulocal opt group	0.055556
is an alloc of a fully or partially	alloc node	0.037037
that unroll	nnet gen conv code unroll	0.250000
by one which computes the specified outputs	fgraph	0.012195
in defining the gradient the finite fourier	fourier grad	0.250000
dimensions of this variable optionally inserting	py	0.014286
raised by get_scalar_const_value if called on	error	0.025000
ptr	ptr	1.000000
has this	has	0.125000
of r	r client_to_remove reason	0.200000
*args directly	local csm properties csm node	0.142857
a modulo of m1 and the	m1	0.027027
helper function	helper random_state low	0.500000
scalar constant	scalar constant	0.142857
this is meant as	gof optimizer optimize fgraph	0.200000
with a triangular solve	linalg tag solve triangular	0.142857
compute a generalized dot product over provided axes	tensordot a b axes	1.000000
don't accept complex	complex	0.125000
reduce pattern has functioning c code	careduce cuda supports c code inputs	0.250000
compute conv output	nnet conv2d	0.333333
navigator deal with the	gof navigator optimizer attach updater fgraph	0.038462
a stack	gof check stack	0.142857
one or more multinomial distributions defined by	multinomial	0.024390
exception object with debug	raise with	0.333333
of shape tuple	default infer shape	0.066667
in profiling to print the mflops	nnet base abstract conv flops inp outp	0.125000
to recognize the updates ordereddict the	get updates	0.034483
the output shape	output shape	0.250000
r to	debugprint r	0.250000
tensordot product	tensordot x y axes	1.000000
bilinear upsampling this function builds the	bilinear	0.038462
failure_callback for navigatoroptimizer	navigator optimizer warn inplace exc nav repl_pairs local_opt	1.000000
exp x or -exp x patterns	tensor nnet is exp	0.333333
in a functiongraph a constant	constant	0.016667
real	tensor complex real	0.500000
exception some perform() or c_code() modified an input	destroy map	0.142857
used to determine the broadcast pattern for advancedsubtensor	adv index broadcastable pattern a idx	0.066667
of shape tuple	infer shape	0.066667
input	size input	0.500000
dot product of two variables	dot	0.035714
inputs and put the variables in the output	gof pure op perform node inputs output_storage params	0.047619
the module file	gof key data	0.333333
random stream in this container	tensor random streams gen op	0.250000
raise	storage_map	0.090909
variant on wraplinker that runs	gof wrap	0.083333
computes the output dimensions of convolving an image	tensor nnet conv op get output	0.047619
output of scan return true	scan	0.017241
if target is 'cpu' this will transfer	transfer target	0.500000
return a code string specific to the apply	name	0.011111
to wrt,	subgraph grad wrt	0.062500
output shape for a convolution	gpu dnn conv get out shape	0.142857
t its inputs	wrt inputs output_grad filters input_shape filter_shape	0.333333
performs the svd on cpu	tensor svd a full_matrices compute_uv	0.200000
source code for this linker and returns	clinker compile cmodule location	0.038462
to help the navigator deal with	navigator optimizer attach updater	0.038462
list of headers that are	headers	0.038462
outer	outer	0.500000
a symbolic row variable	tensor row name	0.050000
short mostly hexadecimal hash	core hex digest x	0.083333
type	clinker type	0.250000
python	make constant	0.100000
gradient updates for matrix solve operation c =	tensor solve grad	0.250000
writeme	op key optimizer	1.000000
kinds of	tensor tensor type	0.041667
reorder the dimensions of this variable	dimshuffle	0.014493
the destroyhandler	destroy handler	0.055556
transform it into a canonical form that respects	get canonical form	0.045455
haskell'	outputs_info non_sequences	0.500000
conventions imposed by	theslice length	0.052632
still	replacements	0.111111
dimensions	tensor py operators dimshuffle	0.038462
some requirements to	requirements	0.125000
types of x	x	0.017544
op classes that this opt applies to	gof local optimizer tracks	0.071429
between min and max	min max	0.250000
of arguments	args	0.051282
function is only used to determine	adv index broadcastable	0.050000
c code to	clinker type c	0.250000
convert radian a to degree	rad2deg a	0.333333
gpucorrmm_gradweights (direction="backprop weights"),	helper bottom weights top direction	0.055556
multinomial distributions defined	multinomial	0.024390
mod	mod c code node name inputs	0.125000
u	u	0.400000
diagonal of an empty matrix it	alloc diag	0.027027
code when doing constant folding of this node	elemwise python constant folding node	1.000000
compile c code when doing constant folding	tensor elemwise python constant folding	0.142857
:type expression vector 1-dimensional	jacobian expression wrt consider_constant disconnected_inputs	0.500000
return a symbolic 3-d	tensor3 name dtype	0.166667
upper_bound	upper_bound	1.000000
node i pairs such that node inputs[i] is	gof function graph clients	0.100000
into a canonical form that	get canonical form	0.045455
default if len img2d shape ==3 we todo	nnet conv op perform node inp out	0.166667
distribution of the form [0 0	crossentropy categorical1hot	0.166667
return a symbolic row variable (ndim=2 broadcastable=[true false])	row name dtype	0.050000
to represent the dependence of nodes in	make dependence	0.043478
of	graph	0.016393
of x	tensor flatten x	0.166667
function into a basic theano op that	op itypes otypes infer_shape	0.047619
and figure out which	scan_module traverse out x x_copy d	0.047619
this compiles the source code for this	compile cmodule location	0.038462
in a new	clone with new inputs inputs strict	0.166667
to help the navigator deal with	gof navigator optimizer attach updater fgraph	0.038462
sample from one	size n	0.090909
shape and	tensor signal pool grad out shape	1.000000
for same kinds of	tensor tensor	0.014286
variable with	variable	0.022222
a shared variable	shared variable	0.071429
this optimization makes the folowing changes in the	tensor local mul switch sink node	0.045455
return the abs and rel error of	abs rel	0.166667
dictionary mapping all symbolic variables in inputs to	inputs	0.012658
this variable optionally	tensor tensor py	0.015873
add an item to six moves	add move move	1.000000
of _maker which handles much	compile debug mode function maker i o m	0.066667
inserting broadcasted dimensions	tensor py	0.015873
the hack in profilemode to print the mflops	base gpu corr3d mm flops inp outp	0.125000
stack trace from one	stack trace	0.055556
used is	gof	0.002381
convolution gradient with	conv grad i	0.500000
memory alias	view	0.022727
construct a variable with	variable x name	0.083333
inserting 1 at	tensor shape padaxis	0.333333
attempt to replace a leaf of a multiplication	tensor nnet replace leaf arg leaves new_leaves op	0.250000
see theano tensor max	tensor tensor py operators max axis keepdims	1.000000
unify_walk a b u returns	gof	0.002381
the	bad	0.026316
parameters ----------	query	1.000000
to print the mflops	tensor nnet base abstract conv flops inp outp	0.125000
pvals	pvals	0.500000
dot product when one or all operands are	dot x y grad_preserves_dense	1.000000
offers to make itself the	to os environ pathlist var newpath	0.038462
return a code string specific to	name	0.022222
this generates the c code for corr3dmm (direction="forward"),	nnet base corr3d mm c code	0.090909
the inputs required to	gof inputs variable_list blockers	0.058824
the hack in profilemode to print the mflops	base gpu corr mm flops inp outp	0.125000
this to expm1	expm1 node	0.066667
this generates the c code for corr3dmm	corr3d mm c code	0.090909
dimshuffle{ } b axis=l) ->	tensor local	0.025641
some perform() or c_code() modified an input that	destroy map	0.142857
a that would be a	gof pure	0.033333
if we are able to assert that	dim_x dim_y	0.090909
a leaf of	leaf	0.066667
for the maximum	maximum x	0.142857
true if and only if this enum	gof enum	0.166667
passed-in key is found in the cache	cache get from key key key_data	0.333333
function for diagonalsubtensor and	nnet get diagonal subtensor	0.083333
of a cache directory and return full path	module name from dir	0.071429
this is	tensor nnet	0.017544
the type's :attr context_name	gpuarray gpu array	0.062500
the input nodes to	gof	0.002381
is basically a call	extract constant x elemwise only_process_constants	0.058824
gets a scan op a list	op not_required inputs	0.071429
context_name	array type	0.055556
on wraplinker that runs	gof	0.002381
computations	scan_module equal computations	0.333333
of this op could be	op	0.009174
wrt, computes gradients of	core subgraph grad wrt	0.062500
warning message on	gof deprecated filename msg	0.041667
for same kinds	tensor tensor	0.014286
distribution between low and high	random_state size low high	0.500000
with constant inputs replaced by their python scalar	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
type c type numpy typenum that corresponds	tensor tensor type dtype specs	0.071429
to	alloc	0.012500
directory whose base will be created if	gof	0.002381
conda offers to make itself the	to os	0.038462
the destroyhandler class detects when a graph	destroy handler	0.055556
inner graph to ensure that it is coherent	scan validate inner graph	0.035714
also work for gpuincsubtensor	inplace setsubtensor node	0.250000
schedule function from comparators	gof sort schedule fn	0.333333
this function is basically a	tensor extract constant x elemwise only_process_constants	0.058824
return a tuple of integers indicating the version	clinker object c code cache version	0.125000
performs batch normalization	gpuarray dnn batch normalization	0.125000
the named module is a package	is package fullname	0.250000
shape of all intermediate variables given	tensor shape of variables fgraph input_shapes	0.100000
parse	tensor nnet parse mul	0.500000
determine the broadcast pattern	adv index broadcastable pattern a idx	0.066667
neibs2images <theano sandbox neighbours neibs2images>	nnet neibs2images neibs neib_shape original_shape mode	0.333333
== b inplace on a	eq inplace a b	0.500000
functiongraph that has ever been associated to	gof	0.002381
helper function for diagonalsubtensor	tensor nnet get diagonal subtensor	0.083333
localoptgroup for	graph to gpulocal opt group	0.055556
the dimensions of	tensor tensor py operators	0.015625
confusion	tensor nnet confusion	0.333333
false we can't change the value after	default	0.030303
in a sparse format instead of	sparse	0.019231
config	core parse config	0.333333
arctangent of a /	tensor arctan2 a	1.000000
as replace_all_validate revert the replacement if	replace all validate	0.111111
navigator	navigator optimizer	0.037037
fortran blas	tensor blas	0.333333
a sparse format instead of dense	sparse	0.019231
add some requirements to the fgraph	add requirements fgraph	0.333333
the	dimshuffle	0.014493
transform it into a canonical form that	canonical form	0.045455
c-implementation of	csr c code node	0.333333
internal theano problem	debug mode	0.200000
bit like make_loop but when only	init_loop_orders olv_index dtypes inner_task	0.200000
to make itself	to	0.017544
list of lib directories that are needed by	clinker header dirs	0.055556
kind of order v	v x	0.100000
initialize the variables that	gof	0.002381
constant with value x	constant x name	1.000000
this function tries to recognize the updates	scan_module get updates	0.034483
gradient wrt	grad	0.083333
memory alias that wasn't	view	0.022727
to wrt,	core subgraph grad	0.062500
replace_all_validate	validate remove fgraph	0.166667
takes as	tensor signal max pool 2d same size	1.000000
det x and	local det	0.166667
value after the import of	core config param init default	0.040000
gpuincsubtensor	tensor local inplace setsubtensor node	0.250000
and dictionary data structures	scan_module push out non seq scan	0.125000
row is a mrg stream	mrg random streams	0.033333
pattern for	pattern	0.028571
variable v is positive	v	0.011111
this is the equivalent of localoptgroup for	gpulocal opt group	0.055556
variable and apply nodes in the original	outputs copy_inputs_and_orphans memo	0.029412
install some functiongraph listeners to help the navigator	navigator optimizer	0.037037
n (n needs to be >=	n	0.055556
a leaf of a multiplication	leaf	0.066667
given axis es of	axis dtype op	0.083333
log(softmax x and replace it with logsoftmax x	local logsoftmax	0.076923
inside	node	0.007407
help the navigator deal	gof navigator optimizer	0.038462
an inplace optimization that deals with allocempty	inplace allocempty op idx	0.166667
takes as	signal	0.166667
to help the navigator deal	gof navigator optimizer	0.038462
the folowing changes in the graph	tensor local mul switch sink	0.045455
for bilinear upsampling this function	bilinear	0.038462
c code for corrmm (direction="forward"),	base corr mm c code	0.090909
detect if the g++ version used is the	gof gcc	0.027778
helper function for diagonalsubtensor	nnet get diagonal subtensor view x i0 i1	0.083333
the updates ordereddict the	updates	0.029412
last access of a given file	last access	0.040000
revert the replacement if the	replace	0.032258
broadcasted dimensions	tensor	0.006431
u and uses it instead of	o u	0.037037
if this local optimization	gof local	0.250000
with a set of 2d filters	filters	0.064516
compare true iff other is the same	tensor type eq other	0.250000
fromalias	fromalias	1.000000
because of aliasing and destructive	destroy	0.009709
elementwise power (inplace on a)	tensor pow inplace a b	1.000000
task that	gof cthunk find task	0.142857
of r	r client_to_remove	0.200000
optimization makes the folowing changes in the graph	mul switch sink	0.045455
for debugmode	nnet base abstract	0.500000
node inputs[i] to new_r	function graph change input node i new_r	0.500000
the type's :attr context_name	gpu	0.011765
elu	elu	1.000000
a signature object for comparing	signature	0.066667
this optimization makes the folowing changes	local mul switch sink node	0.045455
input nodes	gof	0.002381
entry	entry	1.000000
x*x -> sqr x this is faster	local mul to sqr node	0.166667
is the	to gpulocal	0.055556
add a	data add	0.500000
this function builds the 2d	2d	0.090909
for scalar values default int64 or float64	scalar shared	0.083333
it into a canonical form that	get canonical form	0.045455
equiv	equiv	0.833333
into two kinds scalar constants and the rest	rest inputs	0.125000
that node inputs[i]	gof	0.002381
convert	tensor	0.003215
diagonal of an	diag	0.023810
a gist	gist	0.040000
one	x y	0.073171
to make itself	to os environ pathlist var newpath	0.038462
product of two sets	sparse block	0.111111
broadcastable dimensions scrap the dimshuffle and index the	local dimshuffle	0.052632
y + alpha * dot a	gemv c code y a	0.333333
the confusion matrix	nnet confusion matrix	0.166667
is not	not	0.076923
a variable	variable x name	0.083333
compute capability 2	dev20	0.166667
computes gradients of cost	cost	0.045455
sample from a uniform	tensor uniform	0.125000
a set of 2d filters	nnet conv2d input filters	0.125000
decorator to	cls alpha_in beta_in	0.500000
optimization disabled by default that removes all asserts	local remove all assert	0.055556
important note this function uses	push out seq scan process node fgraph node	0.142857
rint	rint	0.555556
the cause of the	bad optimization str diagnostic	0.043478
first half of v	v	0.011111
new	new	0.411765
subclass to add the	tensor	0.006431
sent	mpisend	0.037037
to generate c	shape c	0.250000
the output dimensions of convolving an	conv op get output	0.047619
op that will call the supplied function as	as op	1.000000
the number of multiplications and/or divisions	local greedy distributor node	0.166667
return label	label	0.111111
attempt	arg leaves new_leaves op	0.500000
given an fgraph and a list	fgraph outputs_to_disown	0.047619
connection pattern of subfgraph defined	graph connection pattern	0.076923
see whom can be removed from the scan	scan_module scan can remove outs	1.000000
to the end variables	end	0.040000
accept complex otherwise call upgrade_to_float()	upgrade to float no complex	0.333333
3d	tensor nnet conv3d input	0.125000
symbolic row variable (ndim=2 broadcastable=[true	row	0.034483
w r t its inputs	wrt inputs output_grad filters input_shape filter_shape	0.333333
the dimensions of this variable	tensor tensor	0.014286
the graph and get a memo a	graph	0.016393
a graph of apply nodes	apply nodes	0.200000
allow_override	allow_override	0.416667
this variable optionally inserting broadcasted dimensions	py	0.014286
the l operation on f	lop f	0.166667
that allows replacing subgraphs of a computational graph	scan_module clone output replace strict share_inputs	0.071429
convert data to something which can be associated	type filter data	1.000000
use a simple algorithm to find broken optimizations	compile find bad optimizations2 order reasons r_vals	0.111111
be inserted in the struct	struct	0.047619
product of the specified pieces of vectors	sparse block gemv make node o w h	0.066667
than numpy round half	round half	0.100000
this variable	dimshuffle	0.014493
array of ints	weights minlength assert_nonneg	0.125000
from the cache	gof call cache	0.200000
include_dir	include_dir	1.000000
batch normalization of the given	nnet batch normalization	0.125000
we put in a functiongraph a constant	constant	0.016667
ops	ops	1.000000
wrt to wrt	wrt	0.285714
exp a -1 and converts this to expm1	local expm1 node	0.066667
a list of shape tuple	default infer shape	0.066667
also work for gpuincsubtensor	inplace setsubtensor	0.250000
monitor	monitor	1.000000
convert addsd to faster addsd_ccode	local addsd ccode	0.250000
that compute the variable out for occurrences of	out	0.018519
be inserted to maintain order	searchsorted x v side sorter	0.142857
setsubtensor(x x[idx], idx) -> x when	tensor local setsubtensor of constants node	0.250000
of the specified pieces of vectors	sparse block outer make node	0.066667
a b), axis=0) -> elemwise{scalar op}	local reduce join node	0.111111
normal	streams base normal	0.500000
draw samples from a poisson distribution	tensor random streams base poisson size lam	1.000000
that unroll the batch size	gen conv code unroll batch	0.166667
and a dense matrix	sd ccode	0.250000
elemwise minimum see min for the minimum in	tensor minimum x	0.142857
with logsoftmax x	local logsoftmax	0.076923
performs batch normalization of	tensor nnet batch normalization	0.125000
excepthook and do some special work if	gof	0.002381
function is a thunk	make thunk	0.125000
v raises attributeerror if there	v	0.011111
client_to_remove	client_to_remove	1.000000
unroll the batch size	code unroll batch	0.166667
of each row	row	0.034483
of v by a	v	0.011111
1/(1+exp x ->	tensor nnet local inv 1 plus exp node	0.333333
particular	item	0.076923
indices of minimum elements obtained by iterating	keepdims	0.052632
an apply_node recursively search from this node	apply_node check reason	0.066667
unpack_single	unpack_single	1.000000
and "init_code"	init code struct node name	0.500000
as replace_all_validate revert the replacement	replace validate replace all validate remove	0.111111
zeros_like but forces	core float zeros	0.166667
inputs replaced by their	inputs allow_partial only_process_constants elemwise	0.166667
the scan	scan_module scan	0.333333
in	bad	0.026316
with the -x pattern	is neg var	0.166667
and	o	0.076923
without replacement a can be a 1-d array	tensor choice random_state size a replace	1.000000
the alloc would be useless	tensor alloc call	0.333333
numpy round half	round half	0.100000
crossentropysoftmax1hotwithbiasdx op whose incoming	local useless crossentropy softmax 1hot with bias dx	0.111111
all nodes list of input containers list of	perform linker make all input_storage output_storage storage_map	0.333333
use this as a decorator or context manager	flags	0.062500
c code for this composite op	scalar composite init c code	0.333333
sparse format instead of	core sparse	0.066667
convert addsd	addsd ccode node	0.250000
logsoftmax x	local logsoftmax node	0.142857
output	output	0.224138
graph leading to r to given depth	debugprint r prefix depth done	0.500000
scalar	get scalar	1.000000
platform-dependent gcc argument for shared libraries	get gcc shared library arg	0.333333
outer nit_sot	last step used var scan_args	0.200000
function :func images2neibs <theano tensor nnet	tensor nnet images2neibs	0.333333
of a matrix :math a using magma library	gpu magma matrix	0.333333
see theano tensor min	tensor py operators min	1.000000
that copies a vector to	alloc	0.012500
returning the output error	output	0.017241
in a new	new inputs inputs strict	0.166667
a hash from	hash from	0.333333
c header for openblas	tensor openblas	0.250000
return the idx_list with constant	tensor subtensor get constant idx	0.250000
the outputs from the inputs	inputs outputs mode accept_inplace	0.500000
string	string	0.777778
the output dimensions of convolving an image	tensor nnet conv op get output	0.047619
shape()	shape feature	0.500000
replacement if the	replace all	0.050000
elemwise tan of	sparse tan	1.000000
this variable optionally inserting	py operators dimshuffle	0.019231
specialize	specialize	1.000000
outputs from the	outputs mode accept_inplace	0.166667
compiled module	module	0.033333
some variables	map variables	1.000000
self _rop_op from user supplied form to type	compile op from graph recompute rop op	0.200000
compilation flags from config blas ldflags	tensor ldflags libs flags libs_dir include_dir	0.333333
not attempting to use dnn conv algo_bwd	no dnn algo bwd algo	0.166667
in the struct	struct	0.047619
defined by probabilities	sandbox mrg random	0.333333
op that	alloc	0.012500
from polar coordinate specification	tensor complex from polar abs angle	0.250000
to replace a leaf of a multiplication	nnet replace leaf	0.100000
lower triangle of an	tensor tril m k	0.250000
of 3d filters	conv3d input filters	0.142857
a graph of apply nodes according to	gof sort apply nodes inputs outputs cmps	0.050000
user_line	user_line	1.000000
determine the broadcast pattern for advancedsubtensor output	adv index broadcastable pattern a	0.066667
that broadcast them to	tensor generate broadcasting	0.066667
of values identical with	forced replace	0.333333
or more multinomial distributions	multinomial random_state	0.040000
of this variable optionally inserting	tensor	0.006431
the value after	init default	0.040000
if the g++	gcc	0.023810
config	config	0.600000
variable out for occurrences of values identical with	forced replace out	0.500000
proxy for either true_div or int_div depending on	proxy	0.095238
function on the inputs and put the	pure op perform node inputs output_storage params	0.047619
of scan return true iff	out scan	0.035714
y with length one the	y	0.026316
round half to even	rint	0.111111
x*x -> sqr x this	tensor local mul to sqr node	0.166667
vectors and matrices returning pieces of vectors :	sparse block gemv	0.166667
along the given axis es of	axis dtype keepdims	0.083333
average pooling	pool	0.066667
baddestroymap if	compile check	0.166667
the c code for corr3dmm (direction="forward"), corr3dmm_gradweights	corr3d mm c code	0.090909
source code for this linker and returns a	cmodule location	0.038462
inner graph to ensure that it is	inner graph	0.035714
reshaped view/copy of	tensor tensor py operators reshape shape ndim	0.111111
implements the "reverse-mode" gradient for the eigensystem	eigh grad perform node inputs	0.333333
baddestroymap	storage_map	0.090909
if we are able to	dim_x dim_y	0.090909
of this mode	compile monitor mode	0.333333
input to a leftdims	input leftdims	0.166667
with respect to wrt,	core subgraph grad	0.062500
policies to name r sub	policy policy r name sub	0.250000
the updates	updates	0.029412
op could be very easy if	l op	0.033333
change the value after the import	param init default filter	0.040000
can't change the value after the	param init default filter	0.040000
we can't change the value after	config param init default filter	0.040000
more multinomial	multinomial	0.024390
that will be instantiated by c_extract	gof	0.002381
of the last access of a	last access time	0.040000
see theano tensor sum	tensor tensor py operators sum	1.000000
subgraph bound by the inputs and	inputs	0.012658
level of nesting	loop_orders dtypes loop_tasks	0.125000
args	args	0.153846
implement the grad of	grad grad	0.166667
infer the number of dimensions from the	infer	0.083333
dependence	gof make dependence	0.043478
of lib directories that are needed by	clinker lib dirs	0.055556
warning	deprecated filename	0.166667
lib directories that are needed by one	clinker lib dirs	0.055556
for gpucorr3dmm, gpucorr3dmm_gradweights and gpucorr3dmm_gradinputs	gpu corr3d mm	0.250000
shared variable names when persisting to zip	shared variable id	0.142857
broadcasted	tensor py	0.015873
create a comparator to represent	cmp	0.058824
outer product of two sets	sparse block outer	0.047619
last access of	last access time	0.040000
name the object	name	0.011111
the eigensystem	eigh	0.125000
functions that compute each output of	scalar composite init py impls	0.166667
variable with the -x pattern	neg	0.083333
of this variable optionally inserting broadcasted	py operators	0.015625
and only adds dimension	local	0.014085
inserting 1 at the dimension	shape padaxis	0.333333
encoding of each element in y	y nb_class dtype	1.000000
tensorvariable	as	0.024390
an optimization disabled by default	node	0.007407
m	m	1.000000
a previously received array	mpirecv	0.037037
the epoch of the last access	last access time	0.040000
called by remove_feature	function_graph	0.125000
dimensions of	tensor tensor	0.014286
to the	tensor	0.006431
to this graph	graph import	0.125000
and/or from existing start gradients up	start	0.040000
and uses it	o	0.076923
output of scan return true iff the	scan_module push out scan	0.050000
batched	batched	0.777778
-1 and converts this to expm1	expm1 node	0.066667
code when doing constant folding of	elemwise python constant folding	0.142857
fgraph to	fgraph	0.012195
a normal	tensor normal	0.500000
outside of scan	scan output	0.125000
and only if this	gof	0.002381
debug counter	gof debug counter	0.500000
reorder the dimensions of	dimshuffle	0.014493
inner graph to ensure that it is coherent	scan_module scan validate inner graph	0.035714
elemwise minimum see min for the minimum in	tensor minimum	0.142857
convolution gradient with respect to	dnn conv grad	0.125000
that operates on the	gof linker	0.250000
the value after the import of	init default filter	0.040000
a specified scalar value	a	0.008065
to get the 0 based level of	get depth	0.050000
merge multiplication by a	alpha merge	0.076923
function to get the 0 based	get depth	0.050000
and replace it with logsoftmax x	local logsoftmax	0.076923
the source code for this	cmodule location	0.038462
compute sum of non nan / inf	signature get sum	0.142857
connection	compile op from graph connection	0.500000
inserting broadcasted	operators dimshuffle	0.019231
the apply to be inserted in the	apply	0.016667
in the graph and returns true if	in	0.076923
decorator for the new graphtogpu optimizer	register opt2 tracks	0.250000
theano enumeration types	params type	0.250000
that with the *args directly	csm properties csm	0.142857
tile input array x according	tile x	0.500000
nodes in the original graph to	inputs outputs copy_inputs_and_orphans memo	0.029412
for a convolution with the specified	gpu dnn conv	0.200000
class returns the bartlett spectral window in the	bartlett	0.058824
l has any duplicates (according to	scan_module has duplicates l	0.111111
tensorvariable	tensor as	0.066667
multiplication by a	gpuarray alpha	0.142857
&	and	0.111111
the template filled by broadcasting value through	broadcast like value template fgraph	0.125000
:type expression	expression wrt consider_constant disconnected_inputs	1.000000
alloc of	alloc	0.012500
matrix by a broadcasted dense vector element wise	sdcsc	0.250000
is not attempting to use dnn conv algo_bwd	core safe no dnn algo bwd algo	0.166667
i of	set shape i r i	0.500000
trunc of	tensor trunc	1.000000
lazy loading of moved objects in six moves	module six moves urllib	0.181818
functiongraph listeners to help the navigator deal with	navigator optimizer attach updater	0.038462
x and there	local	0.014085
a symbol definition with an elementwise version of	tensor scal inplace symbol	0.500000
convolution with	conv	0.037037
square root of	tensor sqrt	1.000000
theano tensor extractdiag it accepts tensor	offset axis1 axis2	0.500000
elementwise [floor] division inverse	int div a b	0.333333
the image shape	shape 1axis	0.250000
doc	doc	0.833333
get the 0	type get	0.050000
gradient in	core grad	0.166667
wrt, computes gradients of	subgraph	0.047619
an alloc and only adds	local alloc	0.111111
reorder the dimensions	operators dimshuffle	0.019231
c type of	c	0.017857
with a triangular	triangular node	0.125000
because of aliasing and	destroy	0.009709
implement the grad of average pooling	average pool	0.200000
main diagonal set to a specified scalar value	tensor fill diagonal offset a val offset	0.100000
sort	sort axis kind order	1.000000
device we do it only on cpu here	pow specialize device node	1.000000
mflops	conv op flops inputs outputs	0.125000
the tree and figure out which	scan_module traverse out x x_copy d	0.047619
move_shared_to_gpu	move_shared_to_gpu	1.000000
gpuelemwise	local gpu elemwise	1.000000
an fgraph and a list of variables returns	fgraph outputs_to_disown	0.047619
optimized for calculating the dot product	dot	0.035714
a gist and return the	gist	0.040000
return a symbolic row variable (ndim=2 broadcastable=[true	tensor row	0.050000
symbolic 3-d	tensor tensor3 name dtype	0.166667
optimization makes the folowing changes in the graph	local mul switch sink	0.045455
shape s to previously un-shaped variable r	tensor shape feature set shape r s override	0.500000
row variable	row name	0.050000
infer	tensor infer	0.142857
loop executes code	loop	0.027778
output dimensions of	conv op get output	0.047619
of scan return true	out scan	0.035714
apply_nodes to this graph	function graph import	0.125000
as its	as	0.024390
optimizer for pushing out the variables	push out	0.037037
the convolution gradient with respect to the	gpu dnn conv grad w	0.125000
triangular	triangular	0.461538
a global optimizer of type topooptimizer	topo db	0.166667
offending	offending	1.000000
of platform platform()	core short platform r p	0.142857
performs the matrix inverse on gpu	gpuarray gpu matrix inverse a	0.200000
the outputs from the inputs	inputs outputs mode	0.500000
draw samples from a poisson distribution	streams base poisson size lam ndim	1.000000
should remove any dynamically added functionality	finder on detach fgraph	1.000000
the type's :attr context_name	gpu array	0.055556
c code for corr3dmm (direction="forward"), corr3dmm_gradweights	tensor nnet base corr3d mm c code	0.090909
tries to recognize the updates	get updates	0.034483
the	bad destroy	0.034483
that removes all asserts	remove all assert	0.055556
a recursion over the input dimensions	tensor permute row elements rec	0.047619
c code for corr3dmm	base corr3d mm c code	0.090909
node2	node2	1.000000
row is a mrg stream state	mrg random streams	0.033333
of cost and/or from existing start gradients	start cost	0.100000
loop executes	make reordered loop	0.111111
if theano graphs represent the same computations	scan_module equal computations xs ys in_xs in_ys	0.333333
this is the equivalent of localoptgroup	group	0.047619
subprocess_popen returning the output error and	misc output	0.066667
if the given graph contains a cycle parameters	contains cycle	0.333333
to be inserted in the struct initialization code	init code struct node	0.125000
inc_rstate	inc_rstate	1.000000
for	to	0.017544
makes the folowing changes in the graph t	mul switch sink	0.045455
the __unify_walk__ method for	unify walk a b u	0.037037
functiongraph listeners to help the navigator deal with	navigator optimizer	0.037037
expm1 a	local expm1 node	0.066667
of localoptimizer and applies them to	local opt group	0.052632
a meta path importer	meta path importer	0.166667
baddestroymap if	inputs node storage_map	0.166667
of the code for our fgraph	clinker get dynamic module	0.200000
around c_init that initializes py_name to py_none	gof get c init r	0.250000
dnn	safe no dnn	0.125000
contents of a cache directory and return	name from dir dirname err files	0.500000
if available	call fn args key	1.000000
loop over several arrays and associate	loop	0.027778
elementwise [true] division inverse	tensor true div a b	0.333333
this op could be very easy	tensor prod l op	0.033333
the folowing changes in the graph t	tensor local mul switch sink node	0.045455
important note this function uses set and	scan process node fgraph node	0.142857
the stack trace from one or more tensor	copy stack trace	0.055556
an un-computable symbolic variable of type x type	core grad undefined op x_pos x comment	1.000000
return the idx_list with	idx	0.076923
from a with or without replacement	random streams base choice size a replace	0.333333
clip x to be	tensor clip x	0.250000
a so pyd dll or py file	gof dlimport fullpath suffix	0.333333
return the constant scalar	get scalar constant	0.333333
prod(prod()) -> single prod()	local op of op node	0.500000
bartlett spectral window in the time-domain	bartlett	0.058824
if converting to type2 from type1	type1 type2	0.166667
returns the bartlett spectral window in the	tensor bartlett	0.083333
will match	var	0.035714
of a compiled graph have a stack	check stack	0.142857
a tuple of integers indicating the version	version apply	0.125000
get the	get depth	0.050000
feature to this	feature feature	0.250000
replaced by their python	allow_partial only_process_constants elemwise	0.166667
the destroy_map	destroy	0.009709
of headers	headers	0.038462
make it work for elemwise and gpuelemwise op	tensor local elemwise fusion op op	0.200000
series of wrapper functions instead	linker many linkers wrappers	0.047619
takes as	tensor signal max pool	1.000000
convop that unroll the batch	code unroll batch kern d unroll_bsize unroll_ksize	0.125000
if the given graph contains a cycle parameters	contains cycle fgraph orderings	0.333333
round	round	0.461538
n (n needs	n	0.055556
if the g++ version used is	gcc	0.023810
hash equal for same kinds of	tensor type hash	0.166667
merge 2 profiles	seq optimizer merge	0.200000
image shape	shape 1axis	0.250000
python not the other implementation of mod	mod c code node name	0.125000
ldflags	ldflags	0.666667
self _rop_op from user supplied form to	op from graph recompute rop	0.200000
nav	nav	1.000000
a compiled graph have a stack	gof check stack	0.142857
code to the task that is associated	gof cthunk find task failure_code	0.083333
post some	misc post	0.200000
of var	var	0.071429
elementwise [true] division inverse of multiplication	tensor true div a b	0.333333
delete keys in old format from	gof cleanup	0.333333
ndim	ndim	1.000000
c type numpy typenum that corresponds to	tensor type dtype specs	0.071429
range of values maximum - minimum along an	tensor ptp a	0.333333
of type dtype	dtype	0.022727
memory consumption	save mem	1.000000
return a thunk that is	thunk	0.021277
inner graph to ensure that it	scan validate inner graph	0.035714
get the 0	get depth	0.050000
elemwise maximum see max for the maximum	maximum x	0.142857
return the abs and	abs	0.066667
the equivalent of	to gpulocal opt	0.055556
the shape or the	shape	0.010204
d1 d2 to d1+size d2	scan_module expand empty tensor_var size	0.166667
replace a leaf of a multiplication	replace leaf	0.100000
not required anymore and should be removed and	scan_module compress outs	0.076923
this apply instance	gof apply clone	0.166667
this is the equivalent of localoptgroup for	graph to gpulocal opt group	0.055556
main interface to manipulate the	r new_r reason verbose	0.071429
to convert x into a variable on	gpuarray as gpuarray variable x context_name	0.166667
the dimensions of this variable optionally inserting	tensor tensor py operators	0.015625
view in the forward but clip the gradient	grad clip x lower_bound upper_bound	0.250000
remove broadcastable dimensions from the	py operators squeeze	0.200000
only one client and that client is a	sitsot only	0.066667
return the c implementation of an op	op c code node name inputs outputs	0.500000
the main diagonal set to a specified scalar	fill diagonal offset a val offset	0.100000
and average pooling	pool	0.066667
be referred to	compile register linker	0.250000
unroll	conv code unroll	0.250000
the folowing changes in the	mul switch sink	0.045455
lazy loading of moved objects in six moves	six moves urllib	0.181818
reuse	reuse	1.000000
given axis es of a	axis dtype keepdims	0.083333
this variable optionally inserting	tensor py operators dimshuffle	0.019231
functiongraph listeners to help the navigator deal	navigator optimizer attach	0.038462
as replace_all_validate revert the replacement if	replace all validate remove fgraph	0.111111
and apply_nodes to this graph	function graph import	0.125000
definition with an elementwise version	scal inplace	1.000000
the value after the import of theano	default	0.030303
raise baddestroymap if	inputs node storage_map r_vals	0.166667
the inputs required to compute the given variables	inputs variable_list blockers	0.058824
compute sum of non	tensor tensor constant signature get sum	0.142857
return connection pattern of subfgraph defined by inputs	op from graph connection pattern node	0.076923
implement the same rounding	round half	0.100000
the c code for gpucorrmm (direction="forward"),	base gpu corr mm c code	0.090909
version	version apply	0.125000
dimensionality of the var is equal to outdim	is flat var outdim	1.000000
the fgraph this is the place to do	fgraph	0.012195
this explicitly upcasts constant inputs to elemwise	elemwise constant inputs node	0.250000
a cache directory and return full path	gof module name from dir	0.071429
rest	rest inputs elemwise only_process_constants	0.125000
run the thunk corresponding	stack run thunk of	1.000000
base class for gpucorr3dmm, gpucorr3dmm_gradweights and gpucorr3dmm_gradinputs	base gpu corr3d mm	1.000000
a >	gt a	1.000000
reorder the dimensions of this variable optionally	dimshuffle	0.014493
reorder the dimensions of x default reverse them	tensor transpose x axes	0.200000
apply_node recursively search from this node	apply_node check	0.066667
merge multiplication by a scalar on the	gpuarray alpha merge	0.076923
ones with the same	ones like x	0.333333
to manipulate the subgraph in functiongraph	function graph replace r new_r reason verbose	0.250000
elementary validations on the inner graph	scan_module scan validate inner graph	0.035714
the navigator deal	gof navigator optimizer attach updater fgraph	0.038462
contains a	contains	0.142857
partition a	scalarconsts	0.076923
logsoftmax x	local logsoftmax	0.076923
matrix in which each row is a mrg	sandbox mrg	0.125000
called by remove_feature feature	feature on detach function_graph	0.200000
same kinds of	tensor tensor	0.014286
helpful function that gets a scan op	op not_required inputs	0.071429
shared variable	persistent shared variable	0.500000
2d kernel for bilinear upsampling this function builds	bilinear	0.019231
optimization makes the folowing changes in the	tensor local mul switch sink node	0.045455
computes the dot product of two variables	tensor dot a b	0.333333
diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor	0.083333
as replace_all_validate revert the replacement if the	validate replace all validate	0.111111
a b), axis=0) -> elemwise{scalar op} a b	tensor local reduce join node	0.111111
bartlett spectral window in the time-domain	tensor bartlett	0.083333
on	gpu	0.011765
get the	type get	0.050000
inner	scan_module scan validate inner	0.142857
specified pieces of vectors	sparse block outer make node o	0.066667
add a new variable to theano config	add config var name	1.000000
reproducible case for problems during theano	compile function dump filename inputs outputs	0.166667
list of variables	variables and	0.250000
and dictionary data structures	non seq	0.111111
theano expression whose gradient should not be	x	0.008772
create a comparator to represent the dependence of	dependence cmp	0.111111
to expm1 a	expm1 node	0.066667
similar behaviour as haskell's foldl	foldl fn sequences outputs_info non_sequences	1.000000
set of 3d	tensor nnet conv3d input	0.125000
sign	sgn	0.250000
apply nodes in the original	outputs copy_inputs_and_orphans memo	0.029412
a legal value for a	is valid value a	0.076923
that node inputs[i] is	gof	0.002381
into a canonical form that respects the	get canonical form	0.045455
sample from a uniform	uniform random_state size	0.125000
batched dot product of	batched dot	0.250000
return complex-valued tensor from polar coordinate specification	tensor complex from polar abs angle	0.250000
helper function to generate permutations from	permutation helper random_state n shape	0.333333
gradient for the	grad	0.010417
listeners to help the navigator	gof navigator	0.038462
hash equal	tensor tensor type hash	0.166667
value after the import of	config param init default	0.040000
do not try to use complex numbers	tensor mod check x y	0.166667
split x	split grad	0.500000
x and y have the same shape	same shape x y	0.500000
true for small or builtin c types	gof clinker type c is simple	0.250000
lam	lam	1.000000
complex-valued tensor from polar coordinate specification	tensor complex from polar abs angle	0.250000
i	feature set shape i r i	0.500000
shape_of dictionary	feature init r r	0.333333
add a new variable to theano config	core add config var name doc configparam	1.000000
a compiled module from the loaded cache	gof module cache get module name	0.166667
and replace it with logsoftmax x	nnet local logsoftmax node	0.142857
mode	compile monitor mode	0.333333
returns dict variable k -> list of variables	variables	0.043478
a number of	make	0.017857
op scale or inverse the	scale	0.047619
called by functiongraph attach_feature the method that attaches	gof bookkeeper on attach fgraph	0.142857
function tries	image_shape top_shape border_mode subsample	0.166667
get a memo a dict that	gof	0.002381
variable with a sparse matrix	sparse variable x	0.250000
stabilization optimization that is more general than crossentropy_to_crossentropy_with_softmax_with_bias	nnet crossentropy to crossentropy with	1.000000
given that v	tree set v	0.125000
z	z	0.666667
create a new	clone	0.041667
which each row is a mrg stream	mrg random streams	0.033333
op for input of given shape and flags	out shape imgshape ws ignore_border stride	0.200000
that removes all asserts from the graph	tensor local remove all assert	0.055556
choose values from a with or without replacement	tensor random streams base choice size a	0.333333
to make itself the default python and those	to os environ pathlist var newpath	0.038462
cache data by walking the	refresh age_thresh_use delete_if_problem cleanup	0.166667
stack trace	copy stack trace	0.055556
merge abs generated by local_abs_lift when	tensor local abs merge node	0.333333
add the tensor operators to the	tensor	0.006431
pass to helper_c_code	subtensor get helper c code	0.142857
a / b	a b	0.066667
the dependence of nodes in a graph	dependence	0.035714
return a tuple of integers indicating the version	version apply	0.125000
remove subtensor/advancedsubtensor1 if it	tensor local useless subtensor node	0.200000
sine	sin	0.142857
alias that wasn't in	bad	0.013158
that	gof params	0.200000
[advanced]incsubtensor[1], whose increment is an alloc of	local useless inc subtensor alloc node	0.166667
dependence of nodes in a graph	dependence	0.035714
or more multinomial distributions	multinomial	0.024390
parse a	tensor nnet parse	0.500000
this convert allocempty to alloc	local alloc empty to zeros node	0.333333
convert	tensor make constant	0.100000
only one client	only	0.050000
to help the navigator deal	gof navigator optimizer attach updater fgraph	0.038462
of scan return true iff	push out scan	0.050000
raise baddestroymap	compile check inputs node storage_map	0.166667
with logsoftmax x 's grad	local logsoftmax grad	0.200000
outputs from the inputs	inputs outputs mode	0.500000
multinomial	tensor multinomial random_state	0.040000
cache directory and return full path	module name from dir	0.071429
fgraph this is the place to do	fgraph	0.012195
disabled by default that removes all asserts	remove all assert	0.055556
a compiled module from the loaded cache	cache get module	0.166667
a mrg stream state	sandbox mrg random streams	0.033333
match a	var	0.035714
evaluate because	destroy	0.009709
replace_all_validate revert the	all validate remove	0.166667
change all sigmoid to ultra_fast_sigmoid	ultra fast sigmoid	0.200000
this op __init__ fct don't have the	cast make new inplace output_types_preference name	0.142857
input	input	0.380952
this	tensor tensor py	0.015873
implemented returns	gof local	0.250000
a &	tensor and a	1.000000
if the g++ version	gof gcc	0.027778
factor takes as	tensor signal pool 2d	0.142857
the inner graph to ensure that it	validate inner graph	0.035714
to the one hot	tensor to one hot	0.142857
or none for the outputs of node	node i_shapes	0.333333
and	pool grad out	1.000000
has any duplicates (according to	has duplicates	0.333333
apply to be inserted in the	apply	0.016667
of the specified pieces of vectors	sparse block gemv make	0.066667
and replace it with logsoftmax x 's	nnet local logsoftmax	0.076923
scan return true	scan_module push out scan	0.050000
elementwise modulo (inplace on a)	tensor mod inplace a b	1.000000
transfer to	tensor tensor py operators transfer	0.125000
local optimization wants to	local optimizer	0.333333
print the	print	0.111111
to wait on a previously received	mpirecv wait	0.045455
dimensions of	tensor tensor py	0.015873
of this variable	tensor tensor py operators	0.015625
collection for which predicate item is true	remove predicate coll	0.500000
value has a	value	0.043478
convert degree a to	tensor deg2rad a	0.333333
the outputs from the	outputs mode accept_inplace	0.166667
dimensions of	tensor py operators dimshuffle	0.019231
sharedvariable instances of suitable dummy values	optimizer provide	0.200000
cache	gof call cache	0.200000
low and	low	0.250000
input by a specified factor takes as	tensor signal pool 2d	0.142857
x and there is	local	0.014085
context object mapped	gpuarray gpu array type context	0.090909
of shape	default infer shape	0.066667
of op classes that this	gof local optimizer tracks	0.071429
new	clone with new	0.166667
that wasn't in the	view	0.022727
updates for matrix solve	tensor solve	0.038462
simple algorithm	order reasons r_vals	0.333333
each row is a mrg	mrg	0.076923
detect if the	gcc	0.023810
an op	alloc	0.012500
the args	node args	1.000000
a ==	eq a	1.000000
perform the permutation by	perform node	0.083333
the numeric shape of all intermediate variables	tensor shape of variables fgraph input_shapes	0.100000
a series of wrapper	wrap linker many linkers wrappers	0.047619
baddestroymap	compile check	0.166667
kinds of	tensor tensor	0.014286
all variables which may share	infer reuse pattern	0.100000
weights	weights input	1.000000
return a list of shape tuple	shape	0.010204
input a	input	0.071429
the outputs of node	node	0.007407
this is	to gpulocal	0.055556
the folowing changes in the graph t	local mul switch sink	0.045455
directory	from dir	0.125000
dot product of the specified pieces of vectors	sparse block gemv make node	0.066667
the main diagonal set to a	tensor fill diagonal offset a val offset	0.100000
a canonical form that	get canonical form	0.045455
i of	shape feature set shape i r i	0.500000
for graphtogpu	graph to gpulocal opt	0.055556
context associated	context	0.035714
source code for this	gof clinker compile cmodule location	0.038462
abstract op for batch normalization	abstract batch norm inference	1.000000
the output dimensions of convolving	conv op get output	0.047619
ignore all errors	warn ignore	1.000000
or set of all variables which may share	infer reuse pattern	0.100000
change the value after the	init default	0.040000
implements the r-operator for	gpu max pool rop	0.333333
of localoptgroup for	group	0.047619
by functiongraph attach_feature the method that attaches	gof bookkeeper on attach	0.142857
applies the optimization to the provided l{functiongraph} it	optimizer apply	0.166667
in	core	0.222222
minimum in	tensor minimum	0.142857
the end variables of	wrt end	0.050000
in the view_map	view	0.022727
optionally inserting broadcasted dimensions	dimshuffle	0.014493
the mflops	base abstract conv flops inp outp	0.125000
a compiled theano	fct	0.083333
node by one which computes the	node output_indices alloc_ops	0.142857
[floor] division inverse of	int div	0.250000
product between several dots	matrix dot	0.333333
code to pack c types back into	gof clinker type c sync	0.111111
this variable	py operators dimshuffle	0.019231
values identical with	scan_module forced replace	0.333333
importer to	importer	0.125000
parameters ----------	svd	0.034483
constant inputs to elemwise	elemwise constant inputs	0.250000
change the value after the import of theano	core config param init default filter	0.040000
raise baddestroymap if	check inputs node	0.166667
target	target	0.750000
called by remove_feature feature should	feature on detach function_graph	0.200000
exception object with debug info	raise with op node thunk	0.333333
a comparator	cmp	0.058824
the output dimensions	op get output	0.047619
trace from one or more tensor variables	trace	0.052632
uniform distribution between low and high	uniform random_state size low high	0.333333
important note this function uses set and dictionary	seq scan process node fgraph node	0.142857
and "init_code" together	struct node name	0.500000
flattened version	flatnonzero	0.083333
return a symbolic	name ndim dtype	0.333333
clone the graph and get	graph clone get	0.333333
dependence of	make dependence	0.043478
will print a warning message	gof deprecated filename msg	0.041667
order a graph of apply nodes according	apply nodes inputs outputs cmps	0.050000
evaluated at points given in eval_points	eval_points	0.166667
inner graph to	validate inner graph	0.035714
connection pattern of subfgraph defined by	connection pattern node	0.076923
an op that copies a vector to the	alloc	0.012500
dimshuffle	tensor local dimshuffle	0.052632
cache "filename" as a	call cache persist filename	0.250000
return full path of the dynamic	gof module name from	0.076923
by default that removes all asserts	tensor local remove all assert	0.055556
typed	typed	1.000000
_tag_generator	_tag_generator	1.000000
and converts this to expm1 a	tensor local expm1 node	0.066667
a variable with a sparse matrix	sparse variable	0.250000
the compile lock to be held	cache add to cache module key module_hash	0.166667
each row is a mrg stream state and	sandbox mrg random streams	0.033333
print	print	0.888889
if	check	0.083333
inverse fast fourier transform with real-valued output	tensor irfft inp norm is_odd	0.500000
inputs and put the	pure op perform node inputs output_storage params	0.047619
and b are unified	b	0.014925
loading of moved objects in six moves urllib_response	six moves urllib response	0.333333
on the inner graph to ensure that it	inner graph	0.035714
matrix solve operation c = a \	solve	0.032258
to elemwise	elemwise	0.111111
op could	tensor prod l op	0.033333
out the variables inside	out	0.018519
function for diagonalsubtensor and	diagonal subtensor view x	0.083333
the image shape	shape kernel_shape	0.250000
updates ordereddict	get updates	0.034483
alloc_ops	alloc_ops	1.000000
navigator deal with	gof navigator optimizer attach updater	0.038462
into macros for use within the op	cop get op params	0.200000
the one hot	to one hot	0.142857
num	num	1.000000
usage inplaceelemwiseoptimizer op optimize	inplace elemwise optimizer apply	1.000000
same	tensor shape feature same	0.333333
for the existence of the __unify_walk__ method	gof unify walk a b u	0.037037
helper_c_code	helper c code	0.285714
from the cache	cache	0.034483
a view in the forward but clip	clip x lower_bound upper_bound	0.090909
a particular	setitem item val	0.125000
python not the other implementation of mod	scalar mod c code node	0.125000
that	gof pure	0.066667
elementwise power (inplace on a)	pow inplace a b	1.000000
outputs	outputs mode	0.166667
toposort return an ordering of the	graph toposort	0.125000
it into a canonical	tensor get canonical	0.125000
specified pieces of vectors and	sparse block outer make node o x	0.066667
moved objects	moves urllib	0.115385
proxy for either	div proxy	0.125000
solve operation c	solve	0.032258
reproducible case	compile function dump filename	0.166667
converts this to expm1	expm1	0.050000
function is basically a call to tensor get_scalar_constant_value	extract constant x elemwise only_process_constants	0.058824
output dimensions of convolving an	get output	0.047619
the existence of the __unify_walk__ method for	gof unify walk a b u	0.037037
has this alias	type has alias alias	1.000000
same	tensor	0.006431
return c code to declare variables	type c declare name	0.500000
number of scalars together into a vector	vector	0.066667
the specified pieces of vectors	sparse block gemv make node	0.066667
gradients of cost and/or from existing	cost	0.045455
copies the stack trace	gof copy stack trace	0.055556
allows replacing subgraphs of	scan_module clone output replace strict share_inputs	0.071429
use within the op	op params	0.100000
important note this	seq scan process node fgraph	0.142857
remove current lock	gof unlocker unlock force	1.000000
simple algorithm	optimizations2 order reasons r_vals	0.333333
sum	sum axis dtype keepdims	1.000000
of m1 and the second half	m1	0.027027
fill s v -> alloc(v	local fill	0.250000
existence of the __unify_walk__ method for one	unify walk a b u	0.037037
this class returns the bartlett spectral window in	tensor bartlett m	0.083333
variable optionally inserting	py operators dimshuffle	0.019231
is meant as a shortcut to	optimizer optimize fgraph	0.200000
b are unified given the unification	b	0.014925
a special compound l{op} for the output	crossentropy softmax argmax1hot	0.083333
original graph to a	inputs outputs copy_inputs_and_orphans memo	0.029412
a canonical form that respects the	tensor get canonical form	0.045455
crossentropysoftmax1hotwithbiasdx op whose incoming gradient	crossentropy softmax 1hot with bias dx	0.111111
then replace it with a triangular solve	sandbox linalg tag solve triangular	0.142857
type1 constitutes an upcast	is an upcast type1	0.500000
make	tensor make	0.076923
and return full path of the	module name	0.062500
to	cache add to	0.142857
determine the broadcast pattern for advancedsubtensor output variable	adv index broadcastable pattern a idx	0.066667
this op __init__ fct don't have	cast make new inplace output_types_preference name	0.142857
this optimizer is for debugging	print current function graph	1.000000
to	tensor	0.016077
will be turned into macros for use within	cop get	0.033333
the rest	rest inputs	0.125000
multinomial distributions	tensor multinomial random_state	0.040000
gpu convolution	border_mode subsample	0.500000
specific to the apply to be inserted	apply node	0.031250
first outdim-1 dimension size s of x the	tensor flatten x ndim outdim	0.333333
decrefs py_name	get c cleanup r name	1.000000
specified pieces of vectors and	sparse block outer make node o	0.066667
the __unify_walk__ method for	gof unify walk a b u	0.037037
diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view	0.083333
add	core add	0.500000
same	tensor type	0.034483
because of aliasing and destructive operations	destroy	0.009709
c code	c code inputs	1.000000
convolving a mini-batch of a stack of	input_shape filter_shape	0.027778
cache directory and	dir	0.076923
the method that calls	compile monitor mode eval i	0.500000
badviewmap exception when it detects the following	check viewmap node storage_map	0.111111
provided l{functiongraph} it	optimizer apply	0.166667
libraries	libraries	0.666667
r's shape in the shape_of dictionary	tensor shape feature init r r	0.333333
converts self _grad_op from user supplied form to	op from graph recompute grad op	0.200000
function for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x i0	0.083333
the	operators dimshuffle	0.019231
localoptgroup for	group	0.047619
symbolic type representing a	tensor type	0.034483
warning	gof deprecated filename	0.166667
node by one which computes the	node output_indices	0.142857
and warning about cuda should be displayed only	gpuarray use device force default_to_move_computation_to_gpu move_shared_to_gpu	0.333333
operation for efficiently calculating the dot product	dot	0.035714
input a 4-d	input patch_size	0.166667
1 default 1) times from a multinomial	multinomial	0.024390
to	gpu array type	0.062500
g++ version used is	gof gcc	0.027778
compiles the source code for this linker and	compile cmodule location	0.038462
elemwise arctan of x	sparse arctan x	1.000000
to convert x into a variable on	gpuarray variable x context_name	0.166667
dump this object into its key_pkl file	gof key data save pkl	0.500000
4-d tensor it	patch_size	0.050000
hack in profiling to print the mflops	tensor nnet base abstract conv flops inp outp	0.125000
"lifts" dimshuffle through elemwise operations and	local dimshuffle lift node	0.250000
the given axis es of a tensor input	input axis dtype	0.500000
function tries	image_shape top_shape border_mode	0.166667
if fgraph is the first	fgraph no_recycling	0.200000
the subtensor and its idx_list reorders the inputs	inputs idx_list get_count	0.100000
of the form x[0 :] -> x[0]	tensor local useless slice node	0.250000
see theano tensor sort	py operators sort axis kind	1.000000
the last access of	last access time	0.040000
pattern of a subgraph	pattern	0.028571
c_init that initializes py_name to py_none	gof get c init r name	0.250000
enabled change all sigmoid to ultra_fast_sigmoid	nnet local ultra fast sigmoid	0.200000
a new graph	clone with new	0.166667
a > b inplace on a	gt inplace a b	0.500000
convolve spatio-temporal filters with a	tensor nnet conv3d signals filters	0.111111
badviewmap exception when it detects the	check viewmap node storage_map	0.111111
the specified pieces of vectors	sparse block gemv make node o w h	0.066667
if the g++ version used is the	gcc	0.023810
refresh	refresh	1.000000
into a canonical form that	tensor get canonical form	0.045455
the grad of this op	prod l op	0.033333
impossible to evaluate because of	destroy	0.009709
with	like x	0.250000
roll tensortypes	tensor roll x shift	0.250000
contents of a cache directory and	from dir dirname err files	0.166667
for diagonalsubtensor	nnet get diagonal subtensor view	0.083333
variable with the -x pattern	is neg var	0.166667
the hack in profiling to print the mflops	op flops inputs outputs	0.125000
initializes py_name from storage	get c extract r	1.000000
for diagonalsubtensor and	get diagonal subtensor view x	0.083333
see theano tensor argsort	tensor py operators argsort	1.000000
change the value after	init default filter	0.040000
replacement if the ops in	gof replace validate replace all	0.050000
3d convolution	conv conv	0.250000
apply nodes in the original graph to	outputs copy_inputs_and_orphans memo	0.029412
internal function that constructs	scan_module safe	0.500000
of a compiled theano	fct	0.083333
a series of	wrap linker many linkers wrappers	0.047619
litterals	constant	0.016667
to wait on a previously	mpisend wait	0.045455
each value in array of ints	tensor bincount x weights minlength assert_nonneg	0.125000
the shape feature	shape	0.010204
a slice [start stop step] transform it	slice	0.038462
bitwise ~a inplace on	tensor invert inplace	1.000000
it with logsoftmax x 's grad	local logsoftmax grad node	0.200000
the dimensions of this variable	operators dimshuffle	0.019231
the apply	apply node	0.031250
rows x cols matrix implementing	sandbox dct matrix rows cols unitary	0.333333
sharedvariable instances of suitable dummy values	provide	0.100000
the list of policies to	policy policy	0.125000
of scan return true iff the	scan	0.017241
some perform() or c_code() created a memory	map	0.047619
for a convolution with	gpu dnn conv get	0.200000
variable optionally inserting broadcasted	tensor tensor py	0.015873
removes all	tensor local remove all	0.166667
of the last access of a	last access	0.040000
computes the output dimensions	tensor nnet conv op get output	0.047619
help the navigator deal with	navigator optimizer attach	0.038462
to replace a leaf of	tensor nnet replace leaf	0.100000
parametrize it to make	max_input_fct maker	0.083333
sp	sp	0.625000
tensorvariable of this type	type make variable	0.500000
numpy round half to	round half to	0.166667
generates the c code for corr3dmm (direction="forward"), corr3dmm_gradweights	tensor nnet base corr3d mm c code	0.090909
this	opt	0.043478
default if len img2d shape ==3 we todo	tensor nnet conv op perform node inp out	0.166667
this function performs the matrix	matrix	0.055556
removes all asserts from the graph	local remove all assert	0.055556
if and only if this enum has this	gof enum type has	0.111111
of the graph's apply nodes such that	gof function	0.043478
to choose from	tensor choose	0.250000
permutation by doing a recursion over the input	permute row elements rec	0.047619
reconstruct	reconstruct	1.000000
the	to	0.017544
as replace_all_validate revert the replacement if the ops	replace validate replace all validate	0.111111
re-raise an exception while annotating	node thunk exc_info storage_map	0.250000
average	average	0.200000
to replace a leaf of a	tensor nnet replace leaf	0.100000
return the platform-dependent	get	0.020833
return none or a tensorvariable whose	as	0.024390
the output error	misc output	0.066667
retrieve item from the cache if available	cache call fn args key	0.200000
inverse of a	inverse	0.066667
important note	scan process node fgraph	0.142857
utilitary function to	typed_list typed	0.333333
a b), axis=0) -> elemwise{scalar op} a b	local reduce join	0.111111
dimensions of x default reverse them	tensor transpose x	0.200000
constitutes an	tensor is an	1.000000
new	with new inputs	0.166667
exception some perform() or c_code() modified an	destroy map	0.142857
the diagonal of an empty matrix	alloc diag	0.027027
connection	graph connection	0.500000
that use	conv op use	1.000000
match a variable with the -x pattern	tensor nnet is neg var	0.166667
dimensions of this variable	tensor tensor py	0.015873
blas ldflags	tensor ldflags	0.250000
that	tensor nnet conv op	1.000000
data structures	non seq scan	0.090909
the folowing changes in	local mul switch sink node	0.045455
a version of var transferred	tensor transfer var	0.100000
dimensions scrap the dimshuffle and index	local dimshuffle	0.052632
maps from variable and apply nodes in the	get equiv	0.142857
op could be very easy if it is	prod l op	0.033333
dimensionality of the var is equal to outdim	tensor is flat var outdim	1.000000
exp a -1 and converts this to expm1	local expm1	0.066667
x the same	flatten x	0.166667
the maximum	maximum x	0.142857
and b are unified given	b	0.014925
cache or the disk	module cache	0.071429
some perform() or c_code() created a memory alias	view map	0.142857
cass apply() fct	profile prof1 prof2	1.000000
of compilation flags from config	libs flags libs_dir include_dir	0.052632
is only used to determine the broadcast	adv index broadcastable	0.050000
l{linker}s which keep all nodes in	local linker	0.333333
det x and there	linalg local det	0.166667
fgraph outputs that will replace	fgraph expanded_inputs	0.058824
can't change the value after	config param init default	0.040000
compile lock to	gof module cache add to	0.142857
the dimshuffle and index the	tensor local dimshuffle	0.052632
of v	v	0.011111
for input of given shape and flags	out shape imgshape ws ignore_border stride	0.200000
previously sent	mpisend	0.037037
if true then whenever this config option changes	in_c_key	1.000000
takes a list of localoptimizer and applies	local opt group	0.052632
replace a leaf of a multiplication	nnet replace leaf	0.100000
multiplication by a scalar on	alpha	0.083333
io	io	1.000000
key_pkl	key_pkl	1.000000
of the type	tensor tensor type	0.041667
according to	inputs outputs cmps	0.166667
replaces an [advanced]incsubtensor[1], whose increment is an alloc	local useless inc subtensor alloc node	0.166667
conv output gradient w r	grad	0.031250
this to expm1 a	expm1 node	0.066667
product of the specified pieces of vectors	sparse block gemv make node o w	0.066667
clients	clients	1.000000
output_indices	output_indices	1.000000
vector variable	tensor vector name	0.500000
converts this to expm1 a	expm1	0.050000
integers	integers	1.000000
into a canonical form that	canonical form	0.045455
replacement if the ops	replace validate replace	0.050000
replace element i of shape_of[r] by s_i	r i s_i	1.000000
dimensions of this variable optionally	py operators dimshuffle	0.019231
scalar	scalar	0.232143
for diagonalsubtensor and	diagonal subtensor	0.083333
"lifts" dimshuffle through elemwise operations	local dimshuffle lift node	0.250000
set of 3d filters	tensor nnet conv3d input filters	0.142857
connection	io connection	0.333333
c code for this composite op	composite init c code	0.333333
permutations	permutation random_state size n ndim	0.500000
takes as input a n-d	tensor signal pool 2d input ws ignore_border stride	0.100000
broadcasted dimensions	operators dimshuffle	0.019231
default failure_callback	gof seq optimizer warn exc	1.000000
version	c code cache version	0.125000
subgraph defined by given inputs and	inputs	0.012658
initializes py_name	r name sub	0.250000
function that allows replacing subgraphs	clone output replace strict share_inputs	0.071429
revert the replacement if the ops in the	validate replace	0.050000
unroll the batch size loop	conv code unroll batch	0.166667
version	object c code cache version	0.125000
function	random_state	0.333333
helper function for diagonalsubtensor	get diagonal subtensor view x i0	0.083333
gpu correlation implementation using matrix multiplication	gpu corr3d mm	0.250000
subgraph bound by the inputs and	init inputs	0.083333
scan the contents of a cache directory	dir dirname err files	0.166667
inputs with a set of 2d filters	filters	0.064516
unknown variables and apply_nodes to this graph	function graph import	0.125000
default failure_callback for seqoptimizer	seq optimizer warn exc optimizer	1.000000
forced	forced	1.000000
readable string representation	scalar composite init	0.200000
the cross-entropy between an approximating distribution	nnet categorical crossentropy coding_dist true_dist	0.111111
to represent the dependence	gof make dependence	0.043478
the basic variable	variable	0.022222
array of ints	tensor bincount x weights minlength assert_nonneg	0.125000
connection pattern of subfgraph defined by inputs	connection pattern node	0.076923
of order v real	v	0.022222
number of nodes	nodes	0.200000
a global optimizer for pushing out the	out	0.018519
3d inputs with a set of 3d	tensor nnet conv3d input	0.125000
the ignore_trees-related functionality	fgraph importer pruner chin	0.250000
pieces of vectors and	sparse block gemv make node	0.066667
that copies a vector to the diagonal	diag	0.023810
called whenever node inputs[i] is changed from r	on change input function_graph node i r	1.000000
this is the	opt	0.043478
litterals to theano constants in subtensor arguments	make constant args	0.250000
a symbolic row variable (ndim=2 broadcastable=[true	row	0.034483
destroyhandler class detects when a graph is	destroy handler	0.055556
return a dictionary that	gof	0.002381
if the caller is replace_all_validate just raise the	validator validate fgraph	0.125000
print the following	compile print global	1.000000
assemble the c code for this	c code	0.055556
to determine the broadcast pattern for	adv index broadcastable pattern a idx	0.066667
compilation of cutils_ext	compile cutils	0.166667
called whenever node inputs[i] is changed from r	change input function_graph node i r	1.000000
work for gpuincsubtensor	local inplace setsubtensor	0.250000
this convert allocempty to	empty to zeros node	1.000000
canonical form that respects the	tensor get canonical form	0.045455
has any duplicates (according to __eq__)	has duplicates	0.333333
the fgraph outputs that	fgraph	0.012195
arguments to pass to	args	0.025641
to	mpisend	0.037037
some text to a gist and return	gist	0.040000
ultra_fast_sigmoid	tensor nnet local ultra fast	0.500000
see theano tensor argmin	py operators argmin axis	1.000000
that runs a series of wrapper functions instead	gof wrap linker many linkers wrappers	0.071429
the given axis es of a	axis dtype op	0.083333
shape[i] for tensor variable r int i	shape feature shape ir i r	0.500000
listeners to help the navigator	gof navigator optimizer attach updater	0.038462
into macros	cop get	0.033333
the same rounding than numpy round	round	0.076923
navigator deal with the ignore_trees-related functionality	navigator optimizer attach updater fgraph importer pruner chin	0.333333
list of shape tuple	tensor shape feature default infer shape	0.066667
uniform distribution	uniform random_state size	0.125000
inputs and	init inputs	0.083333
useless	useless	0.538462
scalar values default	scalar shared	0.083333
is the	gof	0.002381
return full path of	gof module name from	0.076923
nonzero	nonzero	1.000000
on the inner graph	scan_module scan validate inner graph	0.035714
the broadcast pattern for advancedsubtensor output	pattern a	0.066667
evaluates this variable	gof variable eval inputs_to_values	1.000000
in a new graph	clone with new inputs inputs strict	0.166667
inputs of	inputs	0.012658
op	op c	0.250000
line	line	1.000000
self openmp	self openmp	1.000000
we can't change the value after the import	param init default	0.040000
shape tuple or	shape feature default infer shape	0.066667
product of the specified pieces of vectors	sparse block gemv make node	0.066667
variable optionally inserting broadcasted dimensions	tensor py	0.015873
the idx_list with constant	subtensor get constant idx	0.250000
u and uses it instead	o u	0.037037
false we can't change the value after the	init default	0.040000
a computation unit composed	code block	0.333333
an optimization disabled by	node	0.007407
mod	scalar mod c	0.125000
logsoftmax	logsoftmax	0.461538
gpucorr3dmm	corr3d mm	0.222222
two matrices at least one of	x y	0.024390
filters_shape	filters_shape	1.000000
es of	dtype	0.022727
scan the contents of a	dirname err files	0.083333
row is a mrg stream state and	sandbox mrg random streams	0.033333
unfortunately conda offers to make itself	to os environ pathlist var newpath	0.038462
extract a list of compilation flags from config	libs flags libs_dir include_dir	0.052632
a list of l{codeblock} instances returns	gof code gen blocks	0.050000
that will be	gof	0.002381
canonical form that	canonical form	0.045455
the provided l{functiongraph} it	optimizer apply	0.166667
flags	flags	0.500000
context	reg context	0.333333
updates for matrix solve operation	solve	0.032258
print a warning message on the	deprecated filename msg	0.041667
a name	name	0.022222
apply instance node	node node	0.333333
macros for use within the op	cop get op	0.200000
to print the mflops	nnet conv op flops inputs outputs	0.125000
to a gist	gist	0.040000
scale	scale x	0.500000
start gradients up	start	0.040000
and dictionary data structures	scan_module push out non	0.125000
tensor_from_scalar(scalar_from_tensor	scalar tensor node	1.000000
convop that unroll the batch	conv code unroll batch kern d unroll_bsize unroll_ksize	0.125000
unfortunately conda offers to make itself	to os environ pathlist var	0.038462
reverse-mode gradient updates for matrix solve	tensor solve grad inputs output_gradients	0.333333
the replacement if	gof replace validate replace all	0.050000
help the navigator deal with	gof navigator	0.038462
the cause of	bad optimization str diagnostic	0.043478
bookkeeper	bookkeeper	1.000000
fast fourier transform of a real-valued input on	curfft inp norm	0.066667
batch normalization	gpuarray dnn batch normalization	0.125000
compiled module from the loaded cache	gof module cache get module	0.166667
optional	gof clinker op c	0.500000
the dimensions	tensor tensor	0.014286
reorder the	tensor tensor py	0.015873
some	map	0.047619
c_extract that initializes py_name from storage	gof get c extract r name sub	0.250000
small or builtin	is simple	0.200000
generates the c code for gpucorrmm (direction="forward"), gpucorrmm_gradweights	base gpu corr mm c code	0.090909
stack trace from one	gof copy stack trace	0.055556
basic variable	variable	0.022222
0-d value underlying variable v if v is	value v	0.500000
on	on	1.000000
re-initialize each random stream	random streams seed seed	1.000000
x is an input vector and t	node input_storage	0.038462
this variable optionally inserting broadcasted	tensor py operators dimshuffle	0.019231
function that allows replacing subgraphs of a	clone output replace strict share_inputs	0.071429
matrices at least one of	x y	0.024390
dependence of nodes	make dependence	0.043478
with helper_c_code	subtensor get helper c code	0.142857
a / b (inplace on a)	inplace a b	0.125000
values from a with or without replacement	random streams base choice size a replace p	0.333333
python litterals to	tensor make constant	0.100000
dimensions of this	py operators dimshuffle	0.019231
to determine the broadcast pattern for advancedsubtensor	tensor adv index broadcastable pattern a	0.066667
convolution gradient with respect to	gpu dnn conv grad i	0.125000
the broadcast pattern	pattern	0.028571
for same kinds of tensortype	type	0.011905
already attached to some fgraph	on attach fgraph	1.000000
signature object for comparing tensorconstant	signature	0.066667
the stack trace	gof copy stack trace	0.055556
to make itself the default python and those	to os environ	0.038462
node inputs[i]	function graph change input node i	0.250000
in an easy to	args	0.025641
feature	gof feature on	0.200000
this compiles the source code for	gof clinker compile cmodule location	0.038462
set of all the node i pairs such	function graph clients	0.200000
when enabled change all sigmoid to ultra_fast_sigmoid	local ultra fast sigmoid	0.200000
replace_all_validate revert the replacement if the ops	replace validate replace all validate	0.111111
gradient function should return	tensor matrix inverse grad inputs g_outputs	0.500000
"reverse-mode" gradient [1]_ for the cholesky factorization of	tensor cholesky grad perform node	1.000000
to replace a leaf of	nnet replace leaf	0.100000
a type	type	0.023810
in profiling to print the mflops	flops inp outp	0.041667
arcsinh	arcsinh	0.857143
this op	op	0.009174
is an input vector and t is	node	0.007407
loop_tasks	loop_tasks	1.000000
reorder	tensor	0.006431
moved objects in	moves urllib error	0.250000
perform the permutation by doing	perform node x	0.166667
:param execute if true execute a	execute execute verbose m n	0.250000
g++ version used is	gof	0.002381
short mostly hexadecimal hash of	core hex digest x	0.083333
important note	push out seq scan process node	0.142857
tile input array x according to reps	tensor tile x reps ndim	1.000000
functions raises a badviewmap exception when it detects	compile check viewmap node storage_map	0.111111
for matrix solve operation c =	tensor solve	0.038462
for same	tensor type	0.034483
two kinds of useless reshape	local useless reshape node	0.200000
of integers indicating the version	object c code cache version	0.125000
softsign activation function	scalar softsign	1.000000
[1]_ for the cholesky factorization of	tensor cholesky	1.000000
to be held	module cache add to cache module key module_hash	0.166667
variable names when persisting to zip file	variable id	0.250000
a meta path importer to	meta path importer	0.166667
reshapes the output	dims output input leftdims	0.333333
dimensions of this	operators	0.017241
the dimensions of this variable	py	0.014286
copies the stack trace from one or more	copy stack trace	0.055556
into a canonical form	canonical form	0.045455
to find broken optimizations	compile find bad optimizations2 order	0.333333
used to determine the broadcast	tensor adv index broadcastable	0.050000
subgraph defined by given inputs and outputs	inputs outputs	0.066667
modulo of m1 and the second half by	m1	0.027027
and "init_code" together	code struct node name sub	0.500000
computes the output dimensions	conv op get output	0.047619
makes the folowing changes in the graph t	tensor local mul switch sink node	0.045455
the conventions imposed by python and	theslice length	0.052632
convolve spatio-temporal filters with a movie	signals filters signals_shape filters_shape	0.333333
respects the conventions imposed by python and numpy	theslice length	0.052632
six moves urllib namespace that resembles	module six	0.043478
and dtype as	dtype	0.022727
mod	scalar mod c code node name inputs outputs	0.125000
the navigator deal with the	navigator optimizer	0.037037
and "init_code" together	code struct node name	0.500000
det x and there is already	sandbox linalg local det	0.166667
dimensions of this variable optionally inserting broadcasted dimensions	operators	0.017241
for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view x i0	0.083333
value after the	default filter	0.040000
a clone in	clone	0.020833
revert the replacement	gof replace validate replace all	0.050000
tell rebroadcast how to generate c code for	compile register rebroadcast c code typ code	1.000000
dimensions of	tensor tensor py operators dimshuffle	0.019231
list of shape tuple or	infer shape	0.066667
half	half	1.000000
elements of the main diagonal set to a	tensor fill diagonal offset a val offset	0.100000
removes all asserts from	tensor local remove all assert	0.055556
to recognize the updates ordereddict	updates	0.029412
argmax over a given axis or over all	argmax	0.066667
a function that will calculate the	compile orig function	0.166667
compute 2d kernel	tensor nnet	0.017544
if len img2d shape ==3 we todo	tensor nnet conv op perform node inp out	0.166667
and converts this to expm1	tensor local expm1	0.066667
t its weights	wrt weights input output_grad filter_shape input_shape	0.333333
makes the folowing changes in the	local mul switch sink	0.045455
object should be saved under	misc persistent ndarray id resolve	0.333333
stack trace from one	copy stack trace	0.055556
inputs and put the variables in the output	pure op perform node inputs output_storage params	0.047619
stabilization optimization that is more general than crossentropy_to_crossentropy_with_softmax_with_bias	tensor nnet crossentropy to crossentropy with softmax fgraph	1.000000
true distribution of the form [0 0	crossentropy categorical1hot	0.166667
see theano tensor sum	tensor py operators sum	1.000000
fgraph this is the	fgraph	0.012195
should remove any dynamically added functionality	gof bookkeeper on detach fgraph	1.000000
dimensions of this variable optionally inserting	tensor tensor py operators dimshuffle	0.019231
is	importer is	0.250000
1 0/a inplace on	tensor inv inplace	1.000000
and should be removed	outs	0.050000
of moved objects in six moves	six moves urllib error	0.142857
dimensions of this variable optionally inserting broadcasted	operators dimshuffle	0.019231
convolution gradinputs	conv gradinputs	0.500000
to the task	find task	0.142857
converts self _grad_op from user supplied form to	from graph recompute grad	0.200000
a context	gpuarray reg context	0.333333
and also their apply_node if	gof	0.002381
moved objects in six moves	six moves urllib	0.181818
output_variables) where function is a thunk	make thunk	0.125000
to override this should return	name	0.011111
for matrix solve operation c =	solve	0.032258
return true if we are able to assert	dim_x dim_y	0.090909
a transfer function for alternative	transfer fn	0.125000
sparse matrix	sparse	0.038462
remove	useless	0.076923
converts number to string by rendering it	from number number	0.142857
transfer to a tensortype if	tensor tensor py operators transfer	0.125000
to wrt, computes gradients	subgraph grad wrt	0.062500
none that need not be checked for nan	compile is numeric value arr var	0.166667
1/(1+exp x ->	tensor nnet local inv 1 plus exp	0.333333
twice gives inconsistent outputs	bad thunk	0.200000
the source code for this linker and returns	cmodule location	0.038462
g++ version	gcc	0.023810
to detect a	tensor detect	0.166667
raise baddestroymap	inputs node	0.100000
interface to manipulate	r new_r reason verbose	0.071429
file	file	0.750000
use_list	use_list	1.000000
returns the connection pattern of a subgraph defined	connection pattern	0.032258
policies to	policy policy	0.125000
operation to wait on a	mpirecv wait	0.045455
special debugging functionmaker	maker	0.166667
removes all asserts from the	remove all assert	0.055556
context_name	gpu array	0.055556
inputs replaced by	inputs allow_partial only_process_constants elemwise	0.166667
equivalent of localoptgroup for graphtogpu	opt group	0.043478
optimization disabled by default that removes	remove	0.035714
help the navigator deal with	navigator	0.032258
calling the same op twice gives inconsistent outputs	bad thunk output	0.200000
for working memory	gpuarray work dtype dtype	0.200000
apply optimizations until equilibrium point	equilibrium optimizer	1.000000
that this	gof	0.002381
idx_list with constant inputs replaced by their	constant idx inputs allow_partial only_process_constants elemwise	0.071429
up to the end variables of	end	0.040000
fill a with	tensor second inplace a	0.333333
clone the graph and get a memo	graph clone	0.166667
pooling	pool grad	1.000000
rebroadcast	rebroadcast	0.666667
register r's shape	tensor shape	0.058824
helper function to draw random integers	tensor random integers helper random_state	1.000000
release lock on compilation	release lock	1.000000
a memory alias that	view	0.022727
global optimizer for pushing out	out	0.018519
changes node	node	0.007407
3d convolution for debugmode	base abstract conv conv	0.125000
empty matrix it does the	alloc	0.012500
to the fgraph outputs	fgraph	0.012195
this variable optionally inserting broadcasted dimensions	tensor py	0.015873
an inplace	gpuarray inplace	0.200000
of 3d inputs with a set of 3d	tensor nnet conv3d input	0.125000
than numpy round half to	round half to	0.166667
the __unify_walk__ method for one	gof unify walk a b u	0.037037
is primarily used by	pure op r op inputs eval_points	0.125000
var transferred to	transfer var	0.100000
to	type	0.011905
sequences	sequences	1.000000
is impossible to evaluate because of aliasing	destroy	0.009709
nit_sot output of scan return true	scan_module push out scan	0.050000
according to the idx list to	idx list	0.250000
division	int div	0.250000
return selected slices only	tensor tensor py operators compress a axis	1.000000
-> single prod()	local op of op	0.500000
mflops	conv flops inp outp	0.125000
create a function	function maker create input_storage trustme storage_map	1.000000
cache directory and	from dir	0.125000
equivalent	to gpulocal opt	0.055556
theano tensor extractdiag it accepts tensor with	offset axis1 axis2	0.500000
product of the specified pieces of vectors	sparse block outer make node	0.066667
and "init_code"	init code struct node	0.125000
dimensions of	tensor py	0.015873
beta * y	c code y	0.333333
-> alloc(unary x shp)	tensor local alloc unary node	0.250000
partition a list of variables	tensor scalarconsts	0.125000
type's :attr context_name	gpu array	0.055556
inner nit_sot output	inner	0.041667
if the passed-in key is found in the	get from key key key_data	0.111111
cache directory and return full path of	module name from dir	0.071429
attempts to replace	inplace optimizer attempt	0.500000
this	tensor py	0.015873
a special compound	softmax argmax1hot	0.083333
list of variables [v1 v2 v3 ]	gof vm linker compute gc dependencies variables	0.250000
see theano tensor sum	tensor tensor py operators sum axis	1.000000
that is	error	0.025000
is inside a dimshuffle which only drop broadcastable	node	0.007407
the cause	bad optimization str diagnostic	0.043478
get the right	tensor get	0.250000
each level of nesting	loop_orders dtypes loop_tasks	0.125000
in the specified axes	tensor addbroadcast x	0.142857
of this variable	operators dimshuffle	0.019231
new graph	clone with new inputs inputs strict	0.166667
the folowing changes in the graph	tensor local mul switch sink node	0.045455
the dot product	dot csr	0.111111
localoptgroup	gpulocal opt group	0.055556
all sigmoid to ultra_fast_sigmoid	tensor nnet local ultra fast sigmoid node	0.200000
filled with ones closer	ones shape dtype	0.200000
the g++ version used is	gof gcc	0.027778
we clone this	gof	0.002381
f	rop f	0.166667
parameters	gpu dnn	0.066667
grad of this op could be very	prod l op	0.033333
blocks	blocks	1.000000
the name the object	name obj	0.111111
the	tensor py operators	0.015625
map old node to	check_integrity	0.090909
the symbolic graph for convolving a mini-batch of	input_shape filter_shape	0.027778
turn softmax(sum_of_stuff) -> softmax_w_bias matrix bias	nnet local softmax with bias	0.200000
help the navigator deal with	gof navigator optimizer attach	0.038462
this variable optionally inserting	tensor tensor py	0.015873
filters with	conv3d signals filters	0.111111
of two sets of pieces	sparse	0.019231
wrt,	subgraph	0.047619
the broadcast pattern for advancedsubtensor	pattern a idx	0.066667
this mode	monitor mode	0.333333
the replacement if	replace all	0.050000
reorder the dimensions of this variable optionally inserting	tensor tensor	0.014286
insert deepcopy in the fgraph	compile insert deepcopy fgraph	0.500000
variant on wraplinker that runs a	gof wrap	0.083333
return a symbolic row variable	row	0.034483
this is	nnet	0.016129
with a set of 2d filters	nnet conv2d input filters	0.125000
the value after the import of	core config param init default	0.040000
in the graph	in	0.076923
return apply	d3viz apply	0.333333
search through	stack search	0.333333
its idx_list reorders	idx_list get_count	0.090909
the inputs required to	inputs variable_list blockers	0.058824
to make itself the default python and	to os environ	0.038462
conda offers to	to os environ pathlist	0.038462
base e logarithm of	tensor log a	0.500000
without replacement	choice	0.125000
in the theano enumeration types wrapped into	params type enum from	0.333333
folowing changes in the	tensor local mul switch sink node	0.045455
return full path of the dynamic lib	module name	0.062500
typed_list	typed_list	1.000000
x shp -> alloc(unary x	local	0.014085
reorder the dimensions of this variable optionally	tensor tensor py operators dimshuffle	0.019231
op for cudnn batch	gpu dnn batch	0.333333
of scan	scan output	0.125000
return the constant	constant	0.033333
symbolically cast x	cast x	0.200000
dimshuffle which only adds dimension	local dimshuffle	0.052632
the idx_list with constant inputs	constant idx inputs	0.250000
two lists are equal if they contain	eq other	0.166667
this is	opt	0.043478
version	gof gcc	0.027778
idx list to get the right values	get idx list	0.076923
returning the output error and	output	0.017241
correlation implementation using matrix multiplication	corr mm	0.083333
tuple of integers indicating the version	c code cache version	0.125000
the c code for corrmm	base corr mm c code	0.090909
wraplinker that runs	gof wrap linker	0.083333
bitwise ~a inplace on	invert inplace	1.000000
2d kernel that can	kernel 2d	0.050000
that was used to split x	split grad inputs g_outputs	0.333333
and replace it with logsoftmax x 's grad	nnet local logsoftmax grad node	0.200000
current op and reduce pattern has functioning c	gpuarray gpu careduce cuda supports c	0.200000
profiling to print the mflops	op flops inputs	0.125000
a variable with the -x pattern	is neg	0.166667
a helper function	tensor diagonal a	1.000000
of a matrix	matrix	0.055556
to insert inplace versions of remove0	sparse local inplace remove0	0.333333
of scan	scan	0.034483
unique names to an	names	0.047619
the dimensions of this variable optionally inserting	tensor tensor	0.014286
this convert allocempty to alloc of	alloc empty to zeros	0.333333
end variables of	end	0.040000
dimensions of this variable optionally inserting broadcasted	tensor	0.006431
exception some perform() or c_code() created a	bad view map	0.142857
fail	fail	0.833333
hyperbolic tangent of a	tensor tanh a	1.000000
gradient wrt filters for abstractconv2d	abstract conv2d grad weights	1.000000
bound on the largest	bound	0.043478
output shape for a convolution with	gpu dnn conv get out shape	0.142857
constant representing a value on a certain gpu	gpu array constant	1.000000
a that	gof pure	0.033333
see theano tensor sort	tensor tensor py operators sort axis kind	1.000000
node	node node	0.333333
apply nodes according to a	gof sort apply nodes inputs outputs cmps	0.050000
not the same on all device we	device node	0.045455
implement incsubtensor	inc subtensor	0.250000
dot product of the specified pieces of vectors	sparse block outer make	0.066667
not the other implementation of mod	scalar mod c code node name inputs outputs	0.125000
files	files	1.000000
that unroll the batch size loop	gen conv code unroll batch	0.166667
triangle	m k	0.250000
if this enum has	gof enum type has	0.111111
connection pattern of subfgraph defined	from graph connection pattern node	0.076923
of m1 and the second half by b	m1 b	0.333333
deprecated	deprecated	1.000000
device we do it only	device node	0.045455
or more multinomial	multinomial	0.024390
in a sparse format	sparse	0.019231
from_var	from_var	1.000000
support version-based cache mechanism	gpuarray code version version	0.333333
version of sparseblockgemv check sparseblockgemv's	gemv	0.100000
check that 1) this destroyhandler wasn't	gof destroy handler	0.250000
failure_callback for navigatoroptimizer	optimizer warn inplace exc nav repl_pairs local_opt	1.000000
dimshuffle which only adds dimension to	dimshuffle	0.014493
the [elementwise] smallest	smallest	0.125000
when we overwrite the	local useless inc subtensor node	0.066667
the other implementation of mod	scalar mod c code node name inputs	0.125000
convop that unroll the batch size loop	conv code unroll batch kern d unroll_bsize unroll_ksize	0.125000
merge abs generated by local_abs_lift when the	tensor local abs merge node	0.333333
a special compound l{op} for the output of	argmax1hot	0.058824
theano constants in subtensor arguments	args	0.025641
multiplication	gpuarray alpha	0.142857
standard elements of	object	0.083333
rounding than numpy round half to	round half to	0.166667
hash equal for same	tensor tensor type hash	0.166667
can	can	0.714286
an	alloc	0.025000
in2out	in2out	0.217391
functions that compute each	scalar composite init py impls	0.166667
alias	bad view	0.027027
of compilation flags from	libs flags libs_dir include_dir	0.052632
used to determine the broadcast pattern for	tensor adv index broadcastable pattern a idx	0.066667
subtensor copy using advanced indexing	advanced subtensor	0.250000
for diagonalsubtensor	get diagonal subtensor	0.083333
output dimensions of convolving an image of	get output	0.047619
important note this	out seq scan process node	0.142857
list of header search paths	header dirs	0.045455
function computes	ishape kshape border_mode	0.250000
used is the llvm one or not	gcc llvm	0.200000
make a nested loop over several arrays	tensor make loop	0.200000
arguments to pass to helper_c_code	subtensor get helper c code args	0.250000
neibs2images	tensor nnet neibs2images	0.333333
the gradient function should return	tensor matrix inverse grad inputs g_outputs	0.500000
last access of	last access time path	0.040000
each row is a mrg stream	mrg random streams	0.033333
in	bad destroy	0.034483
helper function for grad function	core populate grad dict var_to_app_to_idx grad_dict wrt cost_name	1.000000
unfortunately conda offers to	to os environ pathlist var	0.038462
profiling to print the mflops	op flops	0.125000
dimensions of this variable	py	0.014286
row variable	row name dtype	0.050000
optimization	tensor local	0.025641
optionally inserting	tensor tensor py operators	0.015625
of x the same	tensor flatten x	0.166667
function for diagonalsubtensor and	get diagonal subtensor view x i0	0.083333
given axis es of	axis dtype	0.083333
the hack in profiling to print the mflops	nnet conv op flops inputs	0.125000
toposort return an ordering of the graph's apply	toposort	0.076923
the idx_list with constant inputs replaced	constant idx inputs allow_partial only_process_constants elemwise	0.071429
convolution for	base abstract conv conv	0.125000
enabled change all sigmoid to ultra_fast_sigmoid	local ultra fast sigmoid	0.200000
broadcasted dense vector element wise	svcsr	0.090909
x and y have	x y	0.024390
merge 2	seq optimizer merge	0.200000
partition a list of	scalarconsts	0.076923
version	version	0.281250
elementwise subtraction (inplace on a)	sub inplace a b	1.000000
of scan	out scan output	0.125000
the	array	0.083333
that wasn't	bad	0.026316
do just the compilation of cutils_ext	gof compile cutils	0.166667
the [elementwise] largest	largest	0.125000
to the type's :attr	gpuarray gpu	0.045455
to assert that x and y	x y	0.024390
specific to the apply	apply	0.016667
the platform-dependent extension for compiled modules	gof get lib extension	0.333333
|a| tensorvariable overloads	abs a	0.333333
cache directory and return full path of	gof module name from dir	0.071429
the same rounding algo as c round() fct	round half away from zero	0.500000
that will call the supplied function as its	as	0.024390
instances of suitable dummy values	local meta optimizer provide inputs	0.200000
dimensions of this variable optionally inserting	tensor tensor py operators	0.015625
replace it with logsoftmax x	nnet local logsoftmax	0.076923
an alloc of a scalar variable	alloc node	0.037037
suffix	suffix	1.000000
apply_node recursively search	apply_node check	0.066667
it into a canonical form	canonical form	0.045455
a stack	stack	0.066667
two matrices at least one	x y	0.024390
the first half of v by a with	v	0.011111
memory	bad	0.013158
op without	op	0.009174
complementary error function	erfc inplace a	1.000000
the updates ordereddict the	scan_module get updates	0.034483
builds the 2d kernel that can	kernel 2d	0.050000
a matrix :math a using magma library	gpu magma	0.142857
numpy randomstate instance associated with a particular	setitem item val	0.125000
the g++ version used is	gcc	0.023810
remove broadcastable dimensions from the shape	py operators squeeze	0.200000
the conventions imposed	theslice length	0.052632
this convert allocempty to alloc	alloc empty to zeros node	0.333333
output dimensions of convolving an image	nnet conv op get output	0.047619
the apply to be inserted in the struct	struct	0.047619
represents a computation unit composed	code block	0.333333
hack in profiling to print the mflops	base abstract conv flops inp outp	0.125000
a signature object for comparing tensorconstant	constant signature	0.100000
a movie	signals_shape filters_shape	0.333333
the	to gpulocal	0.055556
any python object a that	gof pure	0.033333
to help the navigator deal	navigator	0.032258
lock_file	lock_file	1.000000
the struct	struct	0.047619
function for diagonalsubtensor and	get diagonal subtensor view	0.083333
to compute the	nnet get	0.250000
list of l{codeblock} instances returns a string that	gof code gen blocks	0.050000
mapping all symbolic variables in inputs to	inputs	0.012658
reorder the	operators	0.017241
a graph of apply	apply	0.016667
fetch a compiled module from the loaded cache	cache get module	0.166667
this is the equivalent of	graph to	0.055556
the args are packed like this n_steps	execute node args outs	1.000000
time icluding the	times	0.100000
return indices over each shape that	indices	0.076923
apply_node recursively search	import apply_node check	0.066667
merge 2 profiles returned by	merge	0.071429
dictionary of arguments	args	0.051282
this generates the c code for gpucorrmm	gpu corr mm c code	0.090909
converts number to string by rendering	compile char from number number	0.142857
choose values from a with or	size a replace	0.333333
return a reshaped view/copy of	tensor tensor py operators reshape shape ndim	0.111111
apply nodes according to	gof sort apply nodes inputs outputs cmps	0.050000
add an item to six moves	compat add move move	1.000000
the dimensions	py	0.014286
module if	gof module	0.058824
print a warning message on	deprecated filename msg	0.041667
method to override this should return	name	0.011111
value after the import of	config param init default filter	0.040000
modified bessel function of	tensor i0 inplace	1.000000
this op could be very easy if it	l op	0.033333
to print the mflops	op flops inputs	0.125000
tree and figure out which nodes it	scan_module traverse out x x_copy d	0.047619
-> dot y t x t	tensor local lift transpose through dot node	0.333333
validity	validity	1.000000
a global optimizer for pushing out the variables	out	0.018519
list of shape	shape	0.010204
object a that would	gof pure	0.033333
the type's :attr	array	0.041667
as replace_all_validate revert the replacement	replace validate replace all validate remove fgraph	0.111111
and figure out which nodes it needs	scan_module traverse out x x_copy d	0.047619
and return full path of the dynamic	module name from	0.076923
bitwise a |	or	0.125000
y have the	y	0.026316
pattern has functioning c code	careduce cuda supports c code	0.250000
expression constant when computing gradients	core zero grad x	0.333333
failed fix done in august 2011	load shared variable val	0.142857
python litterals	constant	0.016667
diagonalsubtensor and	diagonal subtensor view	0.083333
this function must return a thunk that is	thunk	0.021277
return connection pattern of subfgraph defined by	from graph connection pattern	0.076923
can be considered approximately equal	values eq approx	1.000000
the last access of a	last access time	0.040000
computes the output dimensions of convolving	op get output	0.047619
of 3d filters	tensor nnet conv3d input filters	0.142857
self _rop_op from user supplied form	op from graph recompute rop	0.200000
list of nodes that must be evaluated	gof	0.002381
dnn conv workmem	dnn workmem workmem	0.166667
listeners to help the navigator deal with	gof navigator	0.038462
destructive operations	destroy	0.009709
print the mflops	conv op flops inputs outputs	0.125000
for convop	kern d unroll_bsize unroll_ksize	0.166667
output dimensions of convolving an	tensor nnet conv op get output	0.047619
gpuelemwise	elemwise	0.111111
specific to the apply to be	apply	0.016667
each value in array of ints	bincount x weights minlength assert_nonneg	0.125000
4-d tensor it sets all	patch_size	0.050000
of l{codeblock} instances returns a	code gen blocks	0.050000
text	text	0.833333
op classes	local optimizer tracks	0.200000
specific code to each level of nesting	loop_orders dtypes loop_tasks sub	0.125000
gradient is	grad	0.010417
unroll the	code unroll	0.250000
with the same	like	0.111111
infer the number	tensor infer	0.142857
raise baddestroymap	compile check inputs	0.166667
two-level hierarchical softmax	tensor nnet h softmax x batch_size n_outputs n_classes	1.000000
in the flattened version	flatnonzero	0.083333
connection pattern of	op from graph connection pattern	0.076923
signature object	signature	0.066667
new variable to theano config	config var name doc configparam	0.500000
gradients up to the end variables	grad wrt end	0.050000
mean value along	mean	0.062500
of sparseblockouter see sparseblockouter's docstring	sparse block outer	0.047619
reproducible case for problems during theano	compile function dump filename	0.166667
offers to make itself the default	to os environ pathlist	0.038462
helper function for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view x i0	0.083333
gradients along the axis that was used to	grad inputs g_outputs	0.076923
boundvariable(other_object)	unify walk fv o u	0.200000
unpad	unpad	0.833333
mflops	corr3d mm flops inp outp	0.125000
convolution gradient with respect	gpu dnn conv grad w	0.125000
complex-valued tensor from	tensor complex from	0.250000
generates the c code for corr3dmm	nnet base corr3d mm c code	0.090909
only on cpu here	pow specialize	0.250000
variables in inputs to	inputs	0.012658
the epoch of the last access of a	last access	0.040000
parameter as other scalar	scalar	0.035714
of moved objects in six moves urllib_error	module six moves urllib error	0.142857
the given axis es of a	axis ddof keepdims	0.083333
code to	code	0.050000
the given axis es of	axis dtype	0.083333
var shape[i], but apply if possible the shape	compile shape i var i fgraph	1.000000
self _rop_op from user supplied form to type	op from graph recompute rop	0.200000
symbolic scalar variable	tensor scalar name dtype	0.166667
tmp_dir	tmp_dir	1.000000
replace_all_validate revert the replacement if the ops	replace validate replace all validate remove	0.111111
lock to	module cache add to	0.142857
concatenate	make	0.017857
used	gof	0.002381
the equivalent of localoptgroup for graphtogpu	to gpulocal opt group	0.055556
c type numpy typenum that corresponds	tensor tensor type dtype specs	0.071429
returns a module if	gof module	0.058824
register a transfer function	tensor register transfer fn	0.250000
true if the named module is a package	importer is package fullname	0.250000
more multinomial distributions defined by one-dimensional	multinomial	0.024390
none or a tensorvariable whose	tensor as	0.066667
functiongraph listeners to help the navigator	navigator optimizer attach	0.038462
raised by	error	0.050000
reorder the dimensions of this variable optionally	py operators	0.015625
nodes to output nodes of the	gof	0.002381
if target is 'cpu' this will transfer to	operators transfer target	0.500000
up to the end variables	wrt end	0.050000
struct	struct node	0.125000
this variable optionally inserting broadcasted	tensor	0.006431
if fgraph is the	fgraph no_recycling	0.200000
inner graph to	scan validate inner graph	0.035714
for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view x	0.083333
c code to extract	c extract	0.500000
of the	gpu	0.011765
return an instance of _maker which handles much	maker i o m	0.066667
cache directory and return full path of the	gof module name from dir	0.071429
r's shape	tensor shape	0.058824
perform the permutation by	perform	0.058824
where elements should be inserted to maintain order	searchsorted x v side sorter	0.142857
override this should return an iterable	gpuarray gpu kernel base gpu kernels node name	0.166667
a reshaped view/copy of this	tensor tensor py operators reshape shape ndim	0.111111
if	node storage_map r_vals	0.166667
as replace_all_validate revert the	validate	0.090909
see theano tensor std	tensor tensor py operators std axis ddof	1.000000
extract test value from v	get test value v	0.250000
litterals to theano constants in subtensor arguments	tensor make constant args	0.250000
convolve spatio-temporal filters	filters	0.032258
load a so pyd dll or py file	gof dlimport fullpath suffix	0.333333
sitsot	sitsot	1.000000
in the fgraph to break aliasing of	fgraph wrapped_inputs wrapped_outputs	0.111111
total time icluding	compute total times	0.200000
to print the mflops	nnet conv op flops	0.125000
from	size	0.076923
the c	c	0.071429
this	graph	0.016393
hack in profilemode to print the mflops	mm flops inp outp	0.125000
is basically	extract constant x elemwise only_process_constants	0.058824
the replacement if the ops in	replace validate replace	0.050000
a new variable instance of type self	pure type make variable	0.333333
a set of 3d	nnet conv3d	0.071429
use dnn	core safe no dnn	0.125000
conda offers to make itself the default	to os	0.038462
fill s v -> alloc(v	local fill to	0.250000
gradient	conv2d grad	0.111111
cache data by walking the cache directory	gof module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
a series of wrapper functions instead	linker many linkers wrappers	0.047619
of _maker which handles much of the debugging	debug mode function maker i o m	0.066667
it with logsoftmax x 's grad	tensor nnet local logsoftmax grad	0.200000
real	tensor real	1.000000
in_xs	in_xs	1.000000
class returns the bartlett spectral window in the	tensor bartlett m	0.083333
attach_feature	attach_feature	1.000000
clone in a new graph	clone	0.020833
the fgraph to	fgraph	0.012195
symbolic column variable (ndim=2 broadcastable=[false true])	col name dtype	0.200000
of convolution	get conv	0.500000
transform it into a canonical form that respects	canonical form	0.045455
converts self _grad_op from user supplied form	compile op from graph recompute grad	0.200000
op scale	scale	0.047619
context_name	gpu	0.011765
a short mostly hexadecimal hash of	core hex digest x	0.083333
the args	execute node args	1.000000
from variable and	get	0.020833
sharedvariable constructor for scalar	scalar shared value name strict allow_downcast	0.200000
change the value after the import of theano	param init default filter	0.040000
perform the permutation by doing	perform node	0.083333
equivalent of numpy ones_like	ones like model dtype opt	0.333333
the context object mapped	array type context	0.090909
that copies	alloc	0.012500
and only if this enum	gof enum type	0.166667
when enabled change all sigmoid to	sigmoid	0.055556
the input nodes to output nodes of	gof	0.002381
signature object for comparing	constant signature	0.100000
to wait on a previously received array	wait	0.022727
the cache directory structure	gof module cache	0.083333
inputs required to compute the	gof inputs variable_list blockers	0.058824
b	b	0.492537
variable that represents their unification	unification	0.076923
existing start gradients up to the end	end start	0.166667
new	with new inputs inputs strict	0.166667
type optionally with a new dtype	type clone dtype	0.333333
the gpu	gpu	0.011765
c-implementation	csr c code node name inputs	0.333333
set to a	a	0.008065
neighbours images2neibs>	ten4 neib_shape neib_step mode	1.000000
of the __unify_walk__ method for one of	gof unify walk a b u	0.037037
of scalars together into	make	0.017857
op around	op	0.009174
broadcast pattern for advancedsubtensor output	pattern a idx	0.066667
return a code string specific to the	name sub	0.050000
convolution gradient with respect to the inputs	gpu dnn conv grad	0.062500
the output shape of	shape	0.010204
c code to extract	gof clinker type c extract	0.500000
list remove are still	replacements remove reason	0.055556
last access of a given	last access time path	0.040000
perform the permutation by doing a recursion over	tensor permute row elements rec perform	1.000000
graph that compute the variable out for	out	0.018519
unknown variables and apply_nodes to this graph	gof function graph	0.031250
a meta path importer to import six moves	six meta path importer	0.333333
base class for gemm and dot22	gemm related	1.000000
clone the graph and get a memo	function graph clone	0.166667
optionally inserting broadcasted dimensions	tensor py operators dimshuffle	0.019231
broadcast pattern for advancedsubtensor output variable	pattern a	0.066667
a sparse format instead of	core sparse	0.066667
the last	last	0.076923
this	graph to gpulocal opt	0.055556
when allow_gc = false clear	compile function free	0.250000
connection pattern of subfgraph defined by inputs	from graph connection pattern	0.076923
out for occurrences	out	0.018519
arg	arg	1.000000
op a list of indices indicating	op	0.009174
a ^ b inplace on a	xor inplace a b	0.333333
the connection	gof io connection	0.333333
clip x to be between min and max	clip x min max	1.000000
op then replace it with a triangular solve	linalg tag solve triangular	0.142857
to roll tensortypes along the given axis	roll x shift axis	0.333333
a thunk that is	thunk	0.021277
still in the	remove fgraph replacements	0.250000
false we can't change the value after	param init default	0.040000
from a multinomial distribution defined by probabilities	sandbox mrg random streams multinomial	0.250000
context_name	gpuarray gpu array	0.062500
y with length	y	0.026316
gradient w r	conv2d grad	0.111111
a file that was dumped to a zip	f persistent_load	0.052632
variable names when persisting to	variable id	0.250000
macros for use within	cop get	0.033333
if current paramstype contains the specified theano type	has type theano_type	0.500000
remove shapefeature as an fgraph feature	un shape	1.000000
implement x[ilist] where ilist is a vector of	advanced subtensor1	0.200000
more multinomial	multinomial random_state	0.040000
filters with a movie	nnet conv3d signals filters signals_shape filters_shape	0.333333
of the code for our fgraph	gof clinker get dynamic module	0.200000
the destroyhandler class detects when a graph is	handler	0.071429
replacement if the ops	gof replace validate replace	0.050000
raise baddestroymap	compile check	0.166667
average pooling	average pool	0.200000
makes the folowing changes in the graph	tensor local mul switch sink node	0.045455
returns the connection pattern of a	connection pattern	0.032258
solve	tensor solve	0.038462
c contiguous version of the input	gpu contiguous	0.083333
for diagonalsubtensor and	nnet get diagonal subtensor view x	0.083333
to make an inplace	inplace	0.025641
this variable optionally	tensor py operators dimshuffle	0.019231
types involved in this node	tensor inc subtensor do type checking node	0.250000
moved objects	moved items	1.000000
inserting 1 at	shape padaxis	0.333333
is the equivalent of	graph	0.016393
the replacement if the ops in the	validate replace	0.050000
the idx_list with constant	get constant idx	0.250000
for diagonalsubtensor and	tensor nnet get diagonal subtensor	0.083333
return the shape	shape	0.010204
more multinomial distributions defined by one-dimensional slices in	tensor multinomial	0.037037
input	max pool 2d same size input	0.500000
svd of a	svd	0.034483
1 0/a inplace on a	inv inplace a	1.000000
computes the output dimensions of convolving an	tensor nnet conv op get output	0.047619
|a| tensorvariable overloads the	tensor abs a	0.333333
the context associated with a name	context name	0.333333
cleanup	cleanup	0.625000
to the end variables	wrt end	0.050000
of apply	sort apply	0.200000
total time icluding the time for parents	compute total times	0.200000
as the template filled by broadcasting value	tensor broadcast like value template fgraph	0.125000
called by remove_feature feature should remove	feature on detach function_graph	0.200000
namespace that resembles the python 3 namespace	moves urllib	0.038462
given an apply_node recursively search from this node	apply_node	0.050000
a six moves urllib namespace that	module six	0.043478
to use dnn	no dnn	0.125000
ops in the list remove are	remove reason	0.142857
cost_name	cost_name	1.000000
toposort return an ordering of the graph's	graph toposort	0.125000
compare true iff other is the same kind	tensor type eq other	0.250000
is the equivalent	graph	0.016393
same op twice gives inconsistent outputs	bad thunk output	0.200000
minimum see min for the minimum in	tensor minimum	0.142857
of this	py	0.014286
the input nodes to output	gof	0.002381
a crossentropysoftmax1hotwithbiasdx op whose incoming gradient is	useless crossentropy softmax 1hot with bias dx	0.111111
this object but we don't clone the	gof constant clone	0.166667
return a dictionary of arguments	args	0.051282
lower triangle of an	tril m k	0.250000
function to	random_state n	0.333333
also work for gpuincsubtensor	local inplace setsubtensor	0.250000
chol	chol	1.000000
or a tensorvariable whose type	tensor as	0.066667
update cache data by walking the	refresh age_thresh_use delete_if_problem cleanup	0.166667
around c_init that initializes py_name to py_none	gof get c init r name sub	0.250000
maps from variable and apply nodes in	get equiv	0.142857
matrices at least one of which is	x y	0.024390
zip file that func should write its data	zip_file	1.000000
explicitly upcasts constant inputs to	constant inputs	0.125000
class is a wrapper for numpy sort function	sort op	0.250000
inserting broadcasted	tensor py operators	0.015625
return a code string specific	name	0.022222
the application of another op that takes	op sub	0.066667
load	tensor load	1.000000
parse a	nnet parse	0.500000
change the value after the	config param init default	0.040000
input that wasn't in	bad	0.013158
return a safe shorter version of platform	platform	0.083333
special compound l{op} for the	crossentropy softmax argmax1hot	0.083333
uses the topooptimizer from the input	gof in2out	0.055556
sparse format instead	sparse	0.019231
to generate c	specify shape c	0.250000
if this local optimization wants	gof local	0.250000
loop executes	reordered loop	0.111111
the value after the import of theano	config param init default filter	0.040000
of a cache directory	dir	0.076923
value after	core config param init default	0.040000
an fgraph and a	fgraph	0.012195
broadcastable dimensions scrap the dimshuffle and index the	tensor local dimshuffle	0.052632
sharedvariable instances of suitable dummy values	local meta optimizer provide inputs	0.200000
the navigator deal with	gof navigator optimizer attach updater fgraph	0.038462
vector to the diagonal	diag	0.023810
necessary update dr_vals	storage_map r_vals dr_vals	0.250000
return the [elementwise] largest of	tensor largest	0.333333
indicating the version	object c code cache version	0.125000
access of a	access	0.100000
return string representation of broadcastable	d3viz broadcastable to str b	0.500000
pattern for advancedsubtensor output variable	pattern a	0.066667
decorator for fromfunctionoptimizer	gof optimizer f	1.000000
of the dot product	dot	0.035714
inputs and put the variables in	pure op perform node inputs output_storage params	0.047619
upper triangle of	tensor triu m k	0.250000
output type dtype and broadcast there	tensor local useless alloc	0.333333
half of v by a	v	0.011111
the c code for gpucorrmm (direction="forward"), gpucorrmm_gradweights	gpuarray base gpu corr mm c code	0.090909
this convert allocempty to alloc	tensor local alloc empty to zeros	0.333333
source code for	compile cmodule location	0.038462
feature should	gof feature on detach	0.200000
dot product of	dot	0.035714
would be a legal value for a variable	is valid value a	0.076923
the context object mapped to	type context	0.090909
__unify_walk__ method	unify walk a b u	0.037037
to initialize the variables that	gof clinker type	0.066667
important note this function uses set	process node fgraph	0.142857
for corrmm (direction="forward"),	corr mm	0.083333
with the ignore_trees-related functionality	fgraph importer pruner chin	0.250000
turned into macros for use within the op	cop get op params	0.200000
to help the navigator deal with the	gof navigator optimizer	0.038462
of shape "inshp" with kernels of shape "kshp"	shape inshp kshp stride mode	0.142857
lib directories that	clinker header dirs	0.055556
constant with value x	tensor constant x name	1.000000
post some	post	0.100000
anymore and should be removed	compress outs	0.076923
true	is	0.133333
have separated maker and	share_memory swap delete_updates name	0.250000
original graph to a new node	inputs outputs copy_inputs_and_orphans memo	0.029412
basic constant class	tensor constant	0.055556
for convop that unroll the batch size	unroll batch kern d unroll_bsize unroll_ksize	0.125000
wrappers	wrappers	1.000000
print the following stats	print global stats	1.000000
create a new instance	clone link_kwargs optimizer	0.111111
item from the cache if available	cache call fn args key	0.200000
computations	equal computations	0.333333
log1p	log1p	0.833333
an input that wasn't in	bad destroy	0.034483
output	misc output	0.066667
m1 and the second half by b with	m1 b	0.333333
the given graph contains a cycle parameters	gof contains cycle fgraph	0.333333
graph	graph import	0.125000
the kernel shape of convolution gradweights	get conv gradweights shape	0.333333
inserted at struct	struct node	0.062500
the name the	name	0.011111
reverse-mode	inputs output_gradients	1.000000
"reverse-mode" gradient [1]_ for the cholesky factorization of	tensor cholesky grad perform	1.000000
c code for corr3dmm (direction="forward"),	corr3d mm c code	0.090909
the output dimensions	conv op get output	0.047619
x to be between min and	x min	0.500000
a normal	random streams base normal	0.500000
see theano tensor max	tensor py operators max	1.000000
input specs and the output specs	std fgraph input_specs output_specs accept_inplace	0.142857
shared variable to	compile shared variable	0.083333
code to each level of nesting	loop_orders dtypes loop_tasks sub	0.125000
of var transferred	tensor transfer var	0.100000
is	meta path importer is	0.250000
offers to make itself the default	to os	0.038462
functiongraph listeners to help the navigator	gof navigator optimizer	0.038462
the list remove are still	fgraph replacements remove reason	0.055556
moved objects in six moves urllib_error	six moves urllib error	0.142857
of shape tuple	shape	0.010204
is a package	meta path importer is package	0.500000
concatenate a number of scalars together	make	0.017857
is the equivalent of localoptgroup for	opt group	0.043478
the __unify_walk__ method for one of the	unify walk a b u	0.037037
kernel that can be used to	kernel	0.133333
useless	tensor local useless	0.222222
to the idx list to get	tensor get idx list	0.076923
in a new	new inputs inputs	0.166667
detect if the g++ version used	gof	0.002381
of seconds since the epoch of the last	last	0.076923
the type's :attr context_name	type	0.011905
variable on	gpuarray variable	0.166667
reorder the dimensions of x	x	0.008772
scrap the dimshuffle and index	tensor local dimshuffle	0.052632
tell shape_i how to generate c code for	shape i c code typ code check_input version	1.000000
cache directory and return full	module name from dir	0.071429
implementation of mod	mod c code node name	0.125000
retrive the context associated with	get context	0.111111
to the end variables of	end	0.040000
ints	tensor bincount x weights minlength assert_nonneg	0.125000
mpop	mpop	1.000000
compute sum of non	signature get sum	0.142857
variable optionally inserting	tensor py	0.015873
computes the output shape for a convolution with	gpu dnn conv get out shape ishape kshape	0.333333
upsampling	upsampling	1.000000
haskell's	outputs_info	0.142857
zip file	persistent	0.166667
to make it work for elemwise and gpuelemwise	tensor local elemwise	0.166667
is associated to	failure_code	0.125000
an input	destroy	0.009709
exception some perform() or c_code() created a memory	view map	0.142857
this to expm1 a	local expm1	0.066667
rop_overrides	rop_overrides	1.000000
apply_node recursively search from this node	import apply_node check reason	0.066667
the inner-most loop executes code	reordered loop	0.111111
op could	prod l op	0.033333
not attempting to use dnn conv workmem	safe no dnn workmem workmem	0.166667
_maker which handles much of	function maker i o m	0.066667
llvm one or not	gcc llvm	0.200000
then replace it with a triangular solve	tag solve triangular node	0.142857
alloc of 0	local alloc	0.111111
inserting	tensor py operators dimshuffle	0.019231
reshapes the input to a leftdims	input leftdims	0.166667
with the -x pattern	neg var	0.166667
replace_all_validate revert the replacement if the ops in	gof replace validate replace all validate remove fgraph	0.111111
function computes the	ishape kshape border_mode subsample	0.250000
helper function for diagonalsubtensor and incdiagonalsubtensor	get diagonal subtensor view	0.083333
lower triangle	tril m k	0.250000
trunc of a	tensor trunc a	1.000000
symbolic integer scalar for the shape element	shape feature unpack	0.500000
in a prior reduction of x	x	0.008772
around c_extract that initializes py_name from storage	gof get c extract r name	0.250000
will call the supplied function as its implementation	compile as	0.050000
basic slow python 2d or	img kern mode dilation	0.250000
for comparing	tensor	0.003215
greedy	greedy	1.000000
or 3d convolution for	nnet base abstract conv conv	0.125000
that unroll the	code unroll	0.250000
of a shared	compile shared	0.166667
new	clone with new inputs	0.166667
remove	tensor local useless	0.111111
symbolic row variable (ndim=2 broadcastable=[true false])	tensor row name dtype	0.050000
functiongraph listeners to help the navigator	navigator optimizer	0.037037
x y -> x	local	0.014085
tuple or none	i_shapes	0.050000
are not required anymore and should be removed	scan_module compress outs	0.076923
that operates on the	gof	0.002381
gradient	grad	0.270833
expm1	tensor local expm1	0.066667
attempts to replace	scan inplace optimizer attempt	0.500000
takes as	tensor signal max	1.000000
as replace_all_validate revert the replacement if	gof replace validate replace all validate remove fgraph	0.111111
is the equivalent	to	0.017544
code when doing constant folding of this node	python constant folding node	1.000000
broadcastable in the specified axes	tensor addbroadcast x	0.142857
function uses set and dictionary data structures	non seq scan	0.090909
output dimensions of convolving an image of shape	tensor nnet conv op get output	0.047619
a vector to the diagonal of	diag	0.023810
the grad of this op could	prod l op	0.033333
be between min and max	min max	0.250000
sharedvariable instances of suitable dummy values	meta optimizer provide	0.200000
this apply instance in a new	gof apply clone with new	0.250000
to import six moves and its	six	0.025000
print the mflops	tensor nnet conv op flops inputs	0.125000
wrt, computes	core subgraph grad wrt	0.062500
to add some requirements to	optimizer add requirements	0.166667
previously un-shaped variable	override	0.142857
x and there is already an l=cholesky	sandbox linalg local	0.142857
respect to wrt, computes	core subgraph grad wrt	0.062500
kinds of useless reshape	local useless reshape node	0.200000
allow it to be reused in scalar	gof clinker cmodule key fgraph no_recycling compile_args libraries	0.200000
the list	list	0.066667
symbolically cast x to a tensor	tensor cast x	0.200000
image	1axis image_shape	0.500000
to wait on a previously received array using	wait	0.022727
return the constant scalar	scalar constant	0.285714
important note this	process node fgraph node	0.142857
the name	name obj	0.111111
1/(1+exp x -> sigm	local inv 1 plus exp	0.333333
replace_all_validate revert the replacement if the ops in	replace validate replace all validate	0.111111
the values of a shared variable	shared variable	0.071429
stack trace for an exception	thunk trace value f	1.000000
the value after the	init default	0.040000
to generate c	i c	0.250000
inputs[i] is r	r	0.028571
nit_sot output of scan return true iff the	scan_module push out scan	0.050000
ws	ws	0.833333
also their apply_node if those nodes are not	gof	0.002381
the output dimensions of convolving	get output	0.047619
c code for corrmm (direction="forward"),	nnet base corr mm c code	0.090909
has an unification in u and	o u	0.037037
return connection pattern of subfgraph defined	compile op from graph connection pattern node	0.076923
copies a vector to the diagonal of an	alloc diag	0.027027
existing start gradients up to the end variables	end start	0.166667
if this enum has this alias	gof enum type has alias alias	0.333333
timeout	timeout	1.000000
more multinomial distributions defined by one-dimensional slices	tensor multinomial	0.037037
to get	type get depth	0.050000
deepcopy in the	deepcopy	0.125000
this generates the c code for gpucorrmm (direction="forward"),	base gpu corr mm c code	0.090909
the context object mapped to the type's	gpu array type context	0.090909
is a wrapper for	op	0.009174
of localoptgroup	to gpulocal opt group	0.055556
according to the idx	idx	0.076923
the given axis es of	axis dtype keepdims	0.083333
generates the c code for corr3dmm	corr3d mm c code	0.090909
integers indicating the version	code cache version	0.125000
expm1	expm1	0.300000
a thunk	linker make thunk	0.125000
the given graph contains a cycle parameters	contains cycle fgraph orderings	0.333333
out for occurrences of values identical with	scan_module forced replace out	0.500000
functiongraph attach_feature the method that	gof	0.002381
dimensions of this variable optionally inserting broadcasted dimensions	tensor	0.006431
the template	template	0.125000
to generate permutations from	tensor permutation	0.166667
recognize the updates ordereddict the list	scan_module get updates	0.034483
at all outputs defined by indices out_idxs and	out_idxs	0.050000
to make itself the default python and those	to os environ pathlist	0.038462
the replacement	validate replace	0.050000
navigator deal with	navigator optimizer attach updater fgraph	0.038462
module is a	path importer is	0.250000
return those	gof	0.002381
the dimensions of x default reverse them	transpose x axes	0.200000
navigator deal	navigator optimizer	0.037037
because	destroy	0.009709
finite fourier	fourier	0.125000
n (n needs to be >= 1	n	0.055556
this op could be very easy if it	tensor prod l op	0.033333
the named module is a package	meta path importer is package fullname	0.250000
the "reverse-mode" gradient for	grad perform node	0.083333
headers	headers	0.230769
this	graph to gpulocal	0.055556
the same	same	0.125000
revert the replacement if the ops	gof replace validate replace all	0.050000
by erf/erfc opt to track less frequent op	get clients2 node	0.200000
the output dimensions of convolving an image	get output	0.047619
output specs	compile std fgraph input_specs output_specs accept_inplace	0.142857
and its idx_list reorders the inputs according	inputs idx_list get_count	0.100000
return	name from	0.500000
to name r	r name	0.250000
dimensions of this	tensor py operators dimshuffle	0.019231
true if we are able to assert	dim_x dim_y	0.090909
has any duplicates (according to	scan_module has duplicates	0.333333
first kind of order v real	v	0.022222
gpu_from_host abstractconv -> abstractconv(gpu_from_host)	gpuarray local conv gpu conv node	1.000000
the cross-entropy between an approximating distribution and	nnet categorical crossentropy coding_dist true_dist	0.111111
offers to	to	0.017544
key, to be used to find equal keys	get safe part key	1.000000
this	py operators dimshuffle	0.019231
cross-entropy between an approximating distribution and a true	nnet categorical crossentropy coding_dist true_dist	0.111111
the svd of a matrix :math	svd	0.034483
two sparses matrices	ss	1.000000
to raise in	core raise init	0.100000
base class	base	0.444444
the platform-dependent	gof get	0.100000
c code for corr3dmm	tensor nnet base corr3d mm c code	0.090909
adv	adv	1.000000
optimizer which can be referred to by name	compile register optimizer name opt	1.000000
if c_code does not support the types involved	tensor inc subtensor do type checking	0.142857
for corrmm (direction="forward"), corrmm_gradweights	tensor nnet base corr mm	0.333333
c types	gof clinker type c	0.333333
to replace a leaf	tensor nnet replace leaf	0.100000
sink	sink	1.000000
depending on types of x y	x y	0.048780
in a sparse format instead	sparse	0.019231
we can't change the value after the import	init default	0.040000
the context object mapped to	array type context	0.090909
conda offers to	to os	0.038462
gives unique names to an iterable	names	0.047619
this class returns the bartlett spectral window in	tensor bartlett	0.083333
register a transfer function	register transfer fn	0.250000
scalars	make	0.017857
given a inner	inner	0.041667
add an item	add	0.034483
a number of scalars together	make	0.017857
the diagonal	diag	0.023810
to ultra_fast_sigmoid	ultra fast	0.333333
pattern for advancedsubtensor	pattern a	0.066667
respect to wrt,	core subgraph grad	0.062500
pred	pred	1.000000
dump	dump	1.000000
when we overwrite the full	tensor local useless inc subtensor node	0.066667
op could be very easy if it	op	0.009174
compiles the source	clinker compile cmodule location	0.038462
to load data	gpuarray load w dtype	0.200000
trunc	trunc	1.000000
directories that are needed by one or more	dirs	0.071429
op for input of given shape and flags	grad out shape imgshape ws ignore_border stride	0.200000
a numpy ndarray contains any np nan values	compile contains nan arr node	0.500000
are views of v given that v	set v	0.125000
diagonalsubtensor	get diagonal subtensor	0.083333
to wait on a previously sent array	wait	0.022727
the indptr field	csm indptr csm	0.333333
n (n needs to be >= 1 default	n	0.055556
python not the other implementation of mod	scalar mod c code	0.125000
returns	gof clinker make thunk	1.000000
variable optionally inserting broadcasted	tensor tensor py operators	0.015625
see min for the minimum	minimum	0.083333
deepcopy in the fgraph to break aliasing of	deepcopy fgraph wrapped_inputs wrapped_outputs	0.500000
helper function for diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor	0.083333
the supplied function as its	as	0.024390
the last access of a	last access	0.040000
each level of nesting	loop_orders dtypes loop_tasks sub	0.125000
to help the navigator deal with the	gof navigator optimizer attach	0.038462
and b are unified given the unification that	b	0.014925
with constant inputs replaced by	get constant idx inputs allow_partial only_process_constants elemwise	0.071429
fill s v -> alloc(v	tensor local fill to alloc node	0.250000
compiles the source code for this	cmodule location	0.038462
wrapper for	op	0.009174
on the inner graph to ensure that	inner graph	0.035714
detect if the g++ version used is	gof	0.002381
the axis that was used	inputs g_outputs	0.090909
in a sparse	sparse	0.019231
clone the graph and	graph clone	0.333333
helper function for diagonalsubtensor	tensor nnet get diagonal subtensor view	0.083333
given an apply_node recursively search from	import apply_node check reason	0.066667
upper bound on the largest eigenvalue	bound	0.043478
the context object mapped to	gpu array type context	0.090909
a recursion over the	tensor permute row elements rec	0.047619
variable on the	gpuarray as gpuarray variable	0.166667
context object mapped to the	gpuarray gpu array type context	0.090909
output error and exit code in a	output subprocess popen command	0.100000
complementary error function	tensor erfc inplace a	1.000000
of the exp x or -exp x patterns	tensor nnet is exp	0.333333
inputs with a set of 3d	conv3d	0.076923
the cumulative sum	cumsum x	0.333333
enum has this alias	enum type has alias alias	1.000000
the op code	get op params	0.100000
mmap_mode	mmap_mode	1.000000
returns true if l has any duplicates (according	has duplicates l	0.111111
fill s v -> alloc(v shape s	tensor local fill to alloc	0.250000
pieces of vectors and	sparse block outer make node	0.066667
rel error of	rel	0.111111
revert the replacement if the ops in the	replace all	0.050000
mrg stream state and they are	sandbox mrg random streams	0.033333
functiongraph feature	feature	0.083333
reorder the dimensions of this	tensor tensor py operators	0.015625
a gist and return	gist	0.040000
expm1 a	expm1 node	0.066667
functiongraph attach_feature the method that attaches	gof feature on attach	1.000000
checks for the existence of the __unify_walk__ method	unify walk a b u	0.037037
apply nodes in the original graph to a	outputs copy_inputs_and_orphans memo	0.029412
the output error and exit code	misc output subprocess popen command	0.100000
the output	gpuarray output	0.200000
version used is	gcc	0.023810
was dumped	f persistent_load	0.052632
required return c code to declare variables that	gof clinker type c declare name	0.333333
this function tries to	image_shape top_shape	0.166667
when enabled change all sigmoid	sigmoid node	0.100000
is false we can't change the value after	core config param init default filter	0.040000
from a uniform	uniform random_state	0.125000
the ops in the list remove are still	fgraph replacements remove reason	0.055556
for calculating the dot product	dot csr	0.111111
row	tensor row	0.050000
to the one hot	one hot	0.142857
function for diagonalsubtensor	tensor nnet get diagonal subtensor view x	0.083333
the output after pad_dims	gpuarray unpad dims output input leftdims rightdims	0.333333
the inputs and put the	pure op perform node inputs output_storage params	0.047619
removes all from the clients list	function graph remove client	0.200000
match a variable with	var	0.035714
return the cross-entropy between an approximating distribution and	tensor nnet categorical crossentropy coding_dist true_dist	0.111111
of	alloc	0.012500
by a specified factor takes as	signal pool 2d	0.142857
the value after the	init default filter	0.040000
a b), axis=0) -> elemwise{scalar op}	tensor local reduce join	0.111111
its idx_list reorders the	idx_list get_count	0.090909
order a graph of apply	gof sort apply	0.200000
csx	csx	1.000000
this graph	function graph	0.040000
a set of 3d filters	nnet conv3d input filters	0.142857
:param execute if true execute a theano function	execute execute verbose m n	0.250000
symbolically cast	cast	0.166667
2d inputs with a set of 2d filters	conv2d input filters	0.125000
the inner-most loop	loop	0.027778
as python not the other implementation of mod	mod c code node	0.125000
return str of variable	to str t	0.166667
app	app	1.000000
for same	tensor tensor	0.014286
times from a multinomial distribution	streams multinomial	0.076923
numpy random	random	0.111111
for diagonalsubtensor	get diagonal subtensor view	0.083333
reintroduces in	make keep dims	1.000000
for convop that unroll the batch size	code unroll batch kern d unroll_bsize unroll_ksize	0.125000
generate permutations	permutation	0.090909
from a list of l{codeblock} instances returns a	code gen blocks	0.050000
the dimensions of this variable	dimshuffle	0.014493
a dense matrix	sd ccode	0.250000
thunk	thunk	0.170213
numpy randomstate instance associated with a particular	getitem item	0.125000
elemwise	elemwise	0.777778
not required anymore and should be removed	outs	0.050000
duplicates	duplicates	0.625000
reorder	py operators	0.015625
suitable dummy values	provide inputs	0.200000
blas	tensor blas	0.333333
exit code in	subprocess popen command	0.083333
alloc	alloc	0.112500
err	err	1.000000
random	tensor random streams base random	0.500000
two kinds of useless	tensor local useless	0.111111
flops	flops	0.384615
if allow_override is false we can't	filter allow_override	0.142857
that gets a scan op a	op not_required inputs	0.071429
implements the "reverse-mode" gradient for	grad perform node inputs	0.083333
shorthand for product between several dots	matrix dot	0.333333
used to determine the broadcast pattern for advancedsubtensor	adv index broadcastable pattern	0.066667
of this variable optionally inserting broadcasted	tensor	0.006431
return a list of shape	shape feature default infer shape	0.066667
mini-batch of a stack of 2d inputs	input_shape filter_shape	0.018519
the exp x or -exp x patterns	tensor nnet is exp	0.333333
elemwise degree to radian	sparse deg2rad x	0.333333
builds the 2d kernel that can be used	kernel 2d	0.050000
diagonalsubtensor and incdiagonalsubtensor	tensor nnet get diagonal subtensor view x	0.083333
rest	rest inputs elemwise	0.125000
which is sparse	sparse	0.038462
when enabled change all sigmoid to ultra_fast_sigmoid	tensor nnet local ultra fast sigmoid	0.200000
see theano tensor prod	tensor py operators prod axis dtype keepdims acc_dtype	1.000000
the forward but clip the	clip x lower_bound upper_bound	0.090909
and the output specs	compile std fgraph input_specs output_specs accept_inplace	0.142857
list	list type	0.100000
same type	type	0.011905
help the navigator	gof navigator optimizer	0.038462
performs batch	dnn batch	0.333333
with the same shape and dtype	dtype	0.022727
a shared	shared	0.125000
c code	type c	0.214286
signals	signals	1.000000
returns true if l has any duplicates (according	scan_module has duplicates l	0.111111
of scalars together into a vector	make vector	0.125000
outputs of specific ops	trace f_or_fgraph ops_to_check bug_print	0.035714
tile input array x according to reps	tile x reps	1.000000
(inplace on a)	inplace	0.307692
takes as input a	signal max pool 2d same size input	0.500000
the mflops	conv op flops	0.125000
symbolic row variable (ndim=2	tensor row name dtype	0.050000
sparse_grad	sparse_grad	0.625000
schedule function from comparators	sort schedule fn	0.333333
replace a crossentropysoftmax1hotwithbiasdx op whose incoming	crossentropy softmax 1hot with bias dx	0.111111
names to an iterable	names	0.047619
the input by a specified factor takes as	tensor signal pool	0.142857
for diagonalsubtensor	get diagonal subtensor view x i0	0.083333
input a 4-d	max pool 2d same size input patch_size	0.166667
only_process_constants	only_process_constants	1.000000
dot product	dot	0.178571
functiongraph listeners to help the navigator deal with	gof navigator optimizer attach	0.038462
tensor	tensor py operators	0.015625
default failure_callback	warn exc	1.000000
to a variable that represents their unification	unification	0.076923
local_opt	local_opt	1.000000
the inner graph to ensure	scan validate inner graph	0.035714
a constant	constant	0.033333
a b), axis=0) -> elemwise{scalar op}	local reduce join	0.111111
the dimensions	tensor py operators dimshuffle	0.019231
compute the kernel shape of convolution gradweights	tensor nnet get conv gradweights shape image_shape	0.500000
(direction="forward"), corr3dmm_gradweights (direction="backprop weights"), and	helper bottom weights top direction	0.055556
represent the dependence of	gof make dependence	0.043478
similar behaviour as haskell'	fn sequences outputs_info	0.500000
nodes that must	gof	0.002381
set and dictionary data structures	out non seq	0.125000
pieces of vectors and matrices	sparse block gemv make node	0.066667
return the platform-dependent extension for compiled modules	gof get lib extension	0.333333
diagonal of	alloc diag	0.027027
batch	tensor nnet batch	0.500000
a input	input input	0.333333
scale or inverse	scale	0.047619
label of apply	apply label node	0.500000
l	l	0.666667
filters	filters	0.290323
b with	b	0.014925
determine the broadcast pattern for advancedsubtensor output variable	tensor adv index broadcastable pattern a idx	0.066667
exception class to raise in	core raise	0.100000
stream state	random streams	0.058824
logsoftmax x 's grad	nnet local logsoftmax grad	0.200000
parses a config	parse config	0.333333
and y have	y	0.026316
real	complex real	0.500000
a real-valued input on the gpu	curfft inp norm	0.066667
an unification in u and uses it	o u	0.037037
helper function for diagonalsubtensor and	diagonal subtensor	0.083333
where x is an input vector and t	node input_storage	0.038462
in the module initialization code	init code	0.142857
normal	base normal	0.500000
import six moves and its submodules	six	0.025000
library search paths	lib dirs	0.045455
last access	last access time	0.040000
has a __thunk_trace__	hook type	0.333333
careduce	careduce	0.857143
shape_i how to	check_input	0.111111
some functiongraph listeners to help the navigator	navigator optimizer	0.037037
apply instance from set which must be computed	fgraph app reason	1.000000
if theano graphs represent	xs ys in_xs in_ys	0.111111
the given axis es	axis dtype	0.083333
fgraph outputs that will replace their values	fgraph	0.012195
distribution between low and high	size low high	0.500000
first kind of order v	v	0.022222
out the variables	push out	0.037037
the permutation by doing a recursion over the	permute row elements rec	0.047619
iff the outer nit_sot	last step used var scan_args	0.200000
helper function to draw random integers	tensor random integers helper random_state low high	1.000000
number	number number	0.125000
an exception class to raise	raise	0.076923
the stack trace from one or more	copy stack trace	0.055556
an empty	alloc	0.012500
cholesky op then replace it with a triangular	triangular node	0.125000
cache or	module cache	0.071429
infer	infer	0.500000
to determine the broadcast pattern for	tensor adv index broadcastable pattern a idx	0.066667
every node that uses	gof function graph	0.031250
return the complex conjugate	conj	0.100000
:func images2neibs <theano tensor nnet neighbours images2neibs>	tensor nnet images2neibs ten4 neib_shape neib_step mode	0.333333
helper function to generate permutations from	permutation helper random_state	0.333333
3d filters	nnet conv3d input filters	0.142857
this op could be very easy if it	prod l op	0.033333
node by	node output_indices alloc_ops	0.142857
data structures	scan_module push out non seq scan	0.125000
graphtogpu	graph	0.016393
the "reverse-mode" gradient [1]_ for the cholesky factorization	cholesky grad perform node	0.500000
initializes py_name to py_none	c init r name	1.000000
connection pattern of subfgraph defined by inputs	graph connection pattern node	0.076923
the output dimensions of convolving an image of	conv op get output	0.047619
the end variables of	end	0.040000
return a symbolic row	row	0.034483
helper function to generate permutations from	tensor permutation helper random_state n shape	0.333333
module is	six meta path importer is	0.250000
tell deepcopyop how to generate c code for	register deep copy op c code typ code	1.000000
encoding of each element	nb_class dtype	0.200000
incsubtensor when we overwrite the	local useless inc subtensor node	0.066667
uniform distribution	tensor uniform	0.125000
apply_node recursively search from this node to	import apply_node check	0.066667
sum	sum axis	1.000000
c_code for convop that unroll the batch	unroll batch kern d unroll_bsize unroll_ksize	0.125000
half of v by a with a modulo	v	0.011111
collection for which predicate item is true	predicate coll	0.500000
remove broadcastable dimensions from the shape of	py operators squeeze	0.200000
op could be	l op	0.033333
of two sets of pieces of vectors	sparse	0.019231
of suitable dummy values	optimizer provide	0.200000
gradient for the eigensystem of	tensor eigh grad	1.000000
an array with more than one element is	node	0.014815
that unroll the	tensor nnet gen conv code unroll	0.250000
this is the equivalent of localoptgroup for	group	0.047619
in the graph and	in	0.076923
this convert allocempty to	empty to zeros	1.000000
a inner nit_sot output	inner	0.041667
outputs of specific ops of a	trace f_or_fgraph ops_to_check bug_print	0.035714
if those	gof	0.002381
the function on the inputs and put the	pure op perform node inputs output_storage params	0.047619
apply_node recursively search from this node to	apply_node check reason	0.066667
logsoftmax	logsoftmax node	0.125000
of ops contained within the	gof ops	0.083333
s to	s	0.071429
v	tree set v	0.125000
random	base random	0.500000
view	view tree	0.500000
to	to os environ	0.038462
this function performs the matrix inverse on gpu	gpuarray gpu matrix inverse a	0.200000
given graph contains a cycle parameters	contains cycle fgraph	0.333333
baddestroymap if necessary update dr_vals	inputs node storage_map r_vals dr_vals	0.250000
this optimization makes the folowing changes in	local mul switch sink node	0.045455
with debug	with op node thunk	0.166667
gpu version	gpu	0.011765
convert x into a variable on	as gpuarray variable x context_name	0.166667
to print the mflops	base abstract conv flops inp outp	0.125000
be turned into macros for	cop get	0.033333
the folowing changes	local mul switch sink	0.045455
takes as	tensor signal max pool 2d same	1.000000
of the __unify_walk__ method	gof unify walk a b u	0.037037
the op	op params	0.100000
numpy ones_like	tensor ones like model dtype	0.333333
function expects the compile lock to	gof module cache add to	0.142857
a dictionary of arguments to pass to helper_c_code	subtensor get helper c code args	0.250000
efficiently calculating the dot product	dot x	0.166667
transfer to a tensortype if	operators transfer	0.125000
inner nit_sot	inner	0.041667
fill s v -> alloc(v shape s	local fill	0.250000
to the task that is associated to	gof cthunk find task failure_code	0.083333
scan return true iff	out scan	0.035714
the convolution gradient with respect	gpu dnn conv grad i	0.125000
this type	gof cdata type	1.000000
given an apply_node recursively search from this	import apply_node check reason	0.066667
is inside a dimshuffle	node	0.007407
it into a canonical form that respects	canonical form	0.045455
n-th order discrete difference along given axis	tensor diff x n axis	0.500000
if the named module	fullname	0.066667
nit_sot output of scan	scan_module push out scan	0.050000
itypes	itypes	1.000000
return true if and only if this enum	gof enum	0.166667
inverse the gradient in	core grad	0.166667
concatenate a number	make	0.017857
3d	tensor nnet conv3d	0.071429
see max for the maximum	maximum	0.083333
a mrg stream state	mrg random streams	0.033333
not	not	0.461538
we can't change the value after the import	default filter	0.040000
a can be a 1-d	random_state size a replace	0.333333
of a shared variable to	compile shared variable	0.083333
to determine the broadcast pattern for advancedsubtensor output	adv index broadcastable pattern a idx	0.066667
boundvariable(other_object)	gof unify walk fv o u	0.200000
comparator to represent the dependence of nodes	make dependence cmp	0.111111
dimensions of	tensor tensor py operators	0.015625
decorator to merge addition by	merge cls alpha_in beta_in out_in	0.250000
a -1 and converts this to expm1 a	tensor local expm1 node	0.066667
a functiongraph a constant that is cached	cached constant error	0.200000
get the 0 based level of the	get	0.020833
be raised by functions defined as part	method not defined	0.333333
from a with or	size a	0.333333
recursion over the input	tensor permute row elements rec	0.047619
this	dimshuffle	0.014493
an fgraph	fgraph	0.012195
required return c code to declare variables	c declare name sub check_input	0.500000
the url	content description filename auth	0.250000
when enabled change all sigmoid	sigmoid	0.055556
wrt, computes gradients	subgraph grad wrt	0.062500
the convolution gradient with respect to the weights	dnn conv grad	0.062500
function performs the svd on cpu	tensor svd a full_matrices compute_uv	0.200000
internal function that constructs a new variable	scan_module safe new	0.333333
a	tensor	0.006431
is	no_recycling	0.111111
normal	tensor normal	0.500000
log base 10	log10	0.166667
cache data by walking the cache directory structure	gof module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
source code for this linker	gof clinker compile cmodule location	0.038462
use within the op	get op params	0.100000
nodes to output	gof	0.002381
the specified pieces of vectors	sparse block gemv make node o	0.066667
confusion	nnet confusion	0.333333
of a shared variable to 0	shared variable zero borrow	0.200000
has only one client and	only	0.050000
inplace	inplace check	0.500000
value after the import of	core config param init default filter	0.040000
separated maker and	share_memory swap delete_updates	0.500000
array using mpi	mpisend	0.037037
required return c code to extract a	type c extract name	0.250000
override this should return an	node name	0.033333
factor takes as input a n-d tensor where	signal pool 2d input ws ignore_border stride	0.100000
fast fourier transform	fft	1.000000
seq1	seq1	1.000000
list of localoptimizer and applies them to	local opt group	0.052632
headers that are needed by one or	clinker headers	0.047619
proxy for either true_div or	scalar div proxy	0.125000
the sum along the	tensor sum	0.111111
an op by transferring each of	op remove	0.250000
cache or	cache	0.034483
subtensor is inside a dimshuffle which	subtensor node	0.066667
output dimensions	op get output	0.047619
comparator to represent the dependence of	make dependence cmp	0.111111
advanced indexing	advanced	0.333333
op code	get op params	0.100000
an inplace optimization that deals with allocempty this	inplace allocempty op idx	0.166667
compute bilinear	tensor nnet bilinear	0.111111
true if a and b	a b	0.066667
schedule	schedule	0.750000
by b	b	0.014925
template	template	0.750000
validations on the inner graph to ensure	scan_module scan validate inner graph	0.035714
freev is unified to boundvariable(other_object)	unify walk fv o u	0.200000
inverse complementary error function	erfcinv	0.166667
to get the 0 based level of	type get	0.050000
of scan return true iff the	scan_module push out scan	0.050000
this function builds the 1d kernel that	kernel 1d	0.050000
config string (comma-separated key=value components) into a	core parse config string config_string issue_warnings	0.166667
min and	min	0.071429
required return c code to declare variables	clinker type c declare name sub	0.500000
gradient wrt inputs for abstractconv	abstract conv grad inputs	1.000000
given an fgraph and a list of	fgraph outputs_to_disown	0.047619
this graph	function graph import	0.125000
a safe shorter version of platform platform()	core short platform r p	0.142857
on the inner graph to ensure	inner graph	0.035714
over	over	1.000000
with logsoftmax x 's	tensor nnet local logsoftmax	0.076923
value after the	config param init default	0.040000
special compound l{op} for the output	crossentropy softmax argmax1hot	0.083333
loading of moved objects in six moves	module six moves urllib	0.181818
canonicalize	canonicalize	1.000000
a convolution with the specified parameters	dnn conv	0.090909
wrt,	core subgraph grad	0.062500
dimshuffle which only adds dimension to the	dimshuffle	0.014493
functions that compute each output of self	scalar composite init py impls	0.166667
"lifts" dimshuffle through elemwise operations and merges consecutive	local dimshuffle lift	0.250000
data	data strict allow_downcast	0.500000
only used to determine the broadcast pattern for	adv index broadcastable pattern a	0.066667
input specs and the output specs	fgraph input_specs output_specs accept_inplace	0.142857
c code for gpucorrmm	gpuarray base gpu corr mm c code	0.090909
maps from	get equiv	0.142857
returns the bartlett spectral window in the	tensor bartlett m	0.083333
clients list of r	r client_to_remove reason	0.200000
assign the shape	feature set shape	0.333333
to initialize the variables that	gof clinker	0.033333
symbolic type representing	type	0.011905
runs a series of wrapper	many linkers wrappers	0.047619
squeeze	squeeze	1.000000
series of wrapper functions instead of just	many linkers wrappers	0.047619
of lib directories that are needed	clinker lib dirs	0.055556
python type c type numpy typenum that	tensor tensor type dtype specs	0.071429
to the task that is associated	gof cthunk find task failure_code	0.083333
or inverse the gradient	grad	0.010417
replacement	replace all	0.050000
change the value after the import	core config param init default	0.040000
chin	chin	1.000000
the context object mapped to the type's :attr	array type context	0.090909
memory alias that wasn't in the	view	0.022727
this op __init__ fct don't have	composite make new inplace output_types_preference name	0.142857
type for working memory	gpuarray work dtype dtype	0.200000
this computes the outer	outer	0.083333
to each level of nesting	loop_orders dtypes loop_tasks	0.125000
convolution	dnn conv	0.090909
the	graph to	0.055556
as replace_all_validate revert the replacement if the	replace all validate remove	0.111111
broadcasted dimensions	py operators	0.015625
of localoptgroup for graphtogpu	opt group	0.043478
some functiongraph listeners to help the navigator	gof navigator optimizer attach	0.038462
x default reverse them	tensor transpose x axes	0.200000
version of var transferred	tensor transfer var	0.100000
a context by mapping it to a name	reg context name ctx	0.500000
sparse and dense matrix	sd	1.000000
all device we do	device node	0.045455
only used to determine the broadcast pattern	tensor adv index broadcastable pattern	0.066667
is basically a call to	tensor extract constant x elemwise only_process_constants	0.058824
maximum in one tensor	tensor maximum x y	0.090909
cvm/vm linkers	if else	1.000000
the dimensions of this variable optionally	py operators dimshuffle	0.019231
cmps	cmps	1.000000
operation to wait on a previously	mpisend wait	0.045455
changed from r to new_r	r	0.028571
and reduce pattern has functioning c code	gpuarray gpu careduce cuda supports c code	0.250000
function builds the 1d kernel that can	kernel 1d	0.050000
variable instance of type self	pure type make variable	0.333333
the variables in inputs that are	gof	0.002381
equivalent	graph	0.016393
input a 4-d tensor it	2d same size input patch_size	0.166667
register r's shape in the shape_of dictionary	shape feature init r r	0.333333
cache directory	dir	0.076923
this variable optionally inserting	operators	0.017241
a new node a clone	clone	0.020833
a inplace on a	inplace a	0.030303
the specified	sparse_grad	0.125000
that	destroy	0.009709
log gamma function	gammaln a	1.000000
that wasn't in the	bad view	0.027027
run after c_code whether it failed or not	cleanup node	0.142857
calculate the function on the inputs and put	pure op perform node inputs output_storage params	0.047619
partition a list	scalarconsts	0.076923
litterals to theano constants in subtensor arguments	args	0.025641
the navigator deal with the	navigator optimizer attach updater fgraph	0.038462
an input vector and t	node	0.007407
that	view	0.022727
an exception while annotating	node thunk exc_info storage_map	0.250000
to the apply to be inserted in	apply	0.016667
as replace_all_validate	all validate	0.166667
of localoptimizer and applies them	local opt group	0.052632
comparing tensorconstant	tensor	0.003215
graphs	graphs	1.000000
dimshuffle	local dimshuffle	0.052632
elementwise	a b	0.133333
indicating the version	cache version	0.125000
for small or builtin c types	type c is simple	0.250000
given an fgraph	fgraph	0.012195
try to	compiler try	0.250000
is basically a call to tensor	tensor extract constant x elemwise only_process_constants	0.058824
typ	typ	1.000000
the c code for corr3dmm (direction="forward"),	base corr3d mm c code	0.090909
verifies the dimensionality of the var is equal	is flat var	0.200000
convert addsd to	sparse local addsd ccode	0.250000
theano enumeration types	type	0.011905
use a simple algorithm to find	compile find bad optimizations2 order reasons r_vals	0.111111
apply_node recursively search from this node	import apply_node check	0.066667
start gradients up to the end	wrt end start	0.166667
a config string (comma-separated key=value components) into a	core parse config string config_string issue_warnings	0.166667
some functiongraph listeners to help the navigator	gof navigator	0.038462
replace replace in string x	replace patterns x replace	1.000000
function to get the	get depth	0.050000
replace_all_validate revert the replacement if the	gof replace validate replace all validate remove fgraph	0.111111
only used to determine the broadcast pattern for	adv index broadcastable pattern a idx	0.066667
x to	x	0.017544
generate permutations from	tensor permutation	0.166667
to	mpirecv	0.037037
partition	tensor scalarconsts	0.125000
function for diagonalsubtensor	nnet get diagonal subtensor view	0.083333
an alloc and only	local alloc	0.111111
c code for this	c code	0.055556
elemwise sinus of x	sparse sin x	1.000000
to help the navigator	gof navigator optimizer attach updater fgraph	0.038462
a zero-arguments function that	gof	0.002381
instances of suitable dummy values	optimizer provide	0.200000
task that is associated	gof cthunk find task failure_code	0.083333
modified bessel function of	tensor i0 inplace x	1.000000
is	gof	0.002381
have the same shape	tensor shape feature same shape	0.333333
function that gets a scan	not_required	0.111111
an input vector and	node input_storage output_storage	0.038462
a list of shape tuple	feature default infer shape	0.066667
minimum see min for the minimum	minimum x	0.142857
for elemwise and gpuelemwise	tensor local elemwise	0.166667
of variables within input	gof variables	0.125000
a bug	bug	0.142857
make it work for elemwise and gpuelemwise op	local elemwise fusion op op	0.200000
to the apply to be inserted in	apply node	0.031250
the inputs and	node inputs	0.043478
unique names to	names	0.047619
variable we want its gradient inputs clipped	x	0.008772
a signature object	constant signature	0.100000
bound by the inputs	inputs	0.012658
broadcasted	py operators	0.015625
modified bessel function	i1 inplace x	1.000000
input a n-d tensor where n >=	input ws ignore_border stride	0.181818
the ops in the list remove are	remove reason	0.142857
c code for corr3dmm	corr3d mm c code	0.090909
the connection pattern of	gof io connection pattern	0.055556
should remove any dynamically added functionality	gof validator on detach fgraph	1.000000
theano graphs represent the same	xs ys in_xs in_ys	0.111111
exp a -1 and converts this to expm1	expm1	0.050000
outer product of two sets of	sparse block outer	0.047619
the version	c code cache version apply	0.125000
opt to track less frequent op	tensor get clients2 node	0.200000
and converts this to expm1	expm1	0.050000
of the class file into an aboslute path	cop get path cls f	0.166667
convert addsd to faster	local addsd	0.250000
existing start gradients up to the end	grad wrt end start	0.166667
a uniform	uniform	0.043478
at struct	struct	0.047619
this variable optionally inserting broadcasted	operators dimshuffle	0.019231
of an empty	alloc	0.012500
convolve spatio-temporal filters	conv3d signals filters	0.111111
graph of apply nodes according	sort apply nodes inputs outputs cmps	0.050000
function is only used to determine the broadcast	adv index broadcastable	0.050000
original graph	outputs copy_inputs_and_orphans memo	0.029412
to the gpu to start the rolling wave	to gpu	0.500000
specified pieces of vectors and matrices	sparse block outer make node o x	0.066667
hash equal for same kinds	tensor tensor type hash	0.166667
create a base class with a metaclass	compat with metaclass meta	0.333333
disabled by default that removes all	tensor local remove all	0.166667
dict	dict	0.750000
warning message on the first call	gof deprecated filename msg	0.041667
it into a canonical form that	tensor get canonical form	0.045455
a bug in the default blas in	sdot bug	0.500000
of lib directories that	header dirs	0.045455
as strings check if converting to type2 from	type2	0.050000
unify values	unify	0.125000
a symbolic scalar	tensor scalar name dtype	0.166667
specified pieces of vectors and matrices	sparse block outer make node	0.066667
if the g++ version	gcc	0.023810
dictionary data structures	non seq	0.111111
constant that	gof	0.002381
to the one	to one	0.125000
performs batch normalization of the given	batch normalization	0.125000
the name	name	0.011111
to the task that	gof cthunk find task	0.142857
transform of a real-valued input on	curfft inp norm	0.066667
permutations of	tensor permutation random_state size	0.500000
copied	compile	0.076923
i0	i0	0.833333
a batched	batched	0.111111
this variable	py	0.014286
a dimshuffle is inside an alloc and	tensor local alloc dimshuffle node	0.166667
with the	like x	0.250000
random	random	0.611111
returns the connection pattern of a subgraph	gof io connection pattern	0.055556
input a 4-d tensor it	max pool 2d same size input patch_size	0.166667
tuple of integers indicating the version	version apply node	0.125000
a != b inplace on a	neq inplace a b	0.500000
to evaluate because	destroy	0.009709
copies a vector to	alloc	0.012500
alias that wasn't in the view_map	view	0.022727
the compile lock to be held	add to cache module key module_hash	0.166667
clone the graph and get a dict	function graph clone get	0.333333
is a thunk	make thunk	0.125000
the c code for gpucorrmm	gpuarray base gpu corr mm c code	0.090909
apply_nodes to this graph	graph	0.016393
alias	bad	0.013158
a badviewmap exception when it detects the	check viewmap node	0.111111
into an aboslute path	cop get path cls f	0.166667
the contents of	dirname err files	0.083333
important note	seq scan process node fgraph	0.142857
if	inputs node storage_map r_vals	0.166667
six moves and its submodules	six	0.025000
baddestroymap if	compile check inputs node storage_map r_vals	0.166667
and apply_nodes to this graph	function graph	0.040000
the other implementation of mod	scalar mod c code	0.125000
tuple or none for the outputs of node	node i_shapes	0.333333
profiling to print the mflops	tensor nnet conv op flops	0.125000
failure_callback for navigatoroptimizer ignore all errors	warn ignore exc nav repl_pairs local_opt	1.000000
conv output gradient	conv2d grad	0.111111
arr	arr	1.000000
some elementary validations on the inner graph	validate inner graph	0.035714
not required anymore and should be removed	scan_module compress outs	0.076923
of integers indicating the version	code cache version	0.125000
row correspond to the one	tensor to one	0.125000
ambiguous tabs or other basic parsing issues	misc hooks get parse error	1.000000
compute the dot product	tensor nnet	0.035088
of	graph to gpulocal	0.055556
drawing from	n pvals size	1.000000
inner graph to ensure that it is	scan_module scan validate inner graph	0.035714
tuple or none for the outputs	i_shapes	0.050000
sparse format instead	core sparse	0.066667
into a canonical form that respects	get canonical form	0.045455
thunk that is a zero-arguments function	thunk	0.021277
get the 0 based	get	0.020833
sample from a uniform	uniform random_state	0.125000
the mflops	conv flops inp outp	0.125000
any python object a that would be	gof pure	0.033333
infer the number	infer	0.083333
shape	tensor shape feature default infer shape	0.066667
a badviewmap exception when it detects the following	check viewmap node	0.111111
makecall	makecall	1.000000
numpy-compatibility method if x is a	tensor diag x	0.200000
important note this function uses set	out seq scan process node fgraph node	0.142857
the c code for corr3dmm (direction="forward"),	corr3d mm c code	0.090909
posible candidate for as inputs of inplace operation	fast inplace check inputs	1.000000
a warning	deprecated filename	0.166667
change all sigmoid	sigmoid node	0.100000
pvals	pvals ndim	1.000000
batched dot product of two variables	batched dot	0.250000
first half of v by a with	v	0.011111
self _grad_op from user supplied form	from graph recompute grad	0.200000
tries	image_shape top_shape	0.166667
and	node	0.029630
inputs according to the	inputs	0.012658
can be referred to	compile register	0.200000
spatio-temporal filters with a movie	signals filters signals_shape filters_shape	0.333333
modified bessel function of	tensor i0 x	1.000000
l{codeblock} instances returns a string that executes	gof code gen blocks	0.050000
to get the 0 based level of the	type get depth	0.050000
an apply_node recursively search	import apply_node check reason	0.066667
return	name sub	0.200000
platform-dependent gcc	get gcc	0.333333
to a gist and return the url	gist content description filename auth	0.250000
the inner graph to ensure that it is	scan_module scan validate inner graph	0.035714
the grad of average pooling	average pool	0.200000
the standard deviation	tensor std	0.111111
elementwise [true] division inverse of multiplication	true div a b	0.333333
context associated with	gpuarray get context	0.111111
to make itself the default python	to os	0.038462
forward convolution	dnn conv	0.090909
image shape of convolution gradinputs	get conv gradinputs shape kernel_shape	0.500000
for gpu	gpu	0.058824
the grad of this op could be	op	0.009174
represent the dependence of	make dependence	0.043478
evaluate	destroy	0.009709
attempt to convert x into a variable on	gpuarray as gpuarray variable x context_name	0.166667
takes as input a 4-d tensor it	signal max pool 2d same size input patch_size	0.250000
graph to ensure that it	graph	0.016393
the value after the import	config param init default	0.040000
diagonalsubtensor	tensor nnet get diagonal subtensor view x i0	0.083333
subclass to add the tensor	tensor	0.006431
*args directly	sparse local csm properties csm node	0.142857
badviewmap exception when it detects the	compile check viewmap node	0.111111
axis that was used	grad inputs g_outputs	0.076923
use with helper_c_code	inc subtensor get helper c code	0.142857
dimensions scrap the dimshuffle and index	tensor local dimshuffle	0.052632
argsort	argsort axis	1.000000
important note	push out seq scan process node fgraph	0.142857
type	tensor tensor type	0.083333
inside the scan that	scan	0.034483
by the inputs	init inputs	0.083333
the output error	output	0.017241
vector to	alloc	0.012500
on the gpu	gpuarray as gpuarray	0.333333
in profilemode to print the mflops	corr3d mm flops inp outp	0.125000
with a particular stream	tensor random streams getitem item	0.142857
a subtensor using advanced indexing	advanced inc subtensor	0.333333
self _rop_op from user supplied form to	compile op from graph recompute rop	0.200000
a compiled module from the loaded cache or	module cache get module	0.166667
as replace_all_validate revert the replacement	validate replace all validate	0.111111
feature should remove any	gof feature on detach	0.200000
uses set and dictionary data structures	push out non seq scan	0.125000
macros for use within the	cop get	0.033333
the scan	scan	0.017241
the replacement if the ops	replace	0.032258
add an item to	add	0.034483
optionally	tensor tensor py operators dimshuffle	0.019231
return a copy of the type	tensor tensor type	0.041667
a list of shape tuple	infer shape	0.066667
sparseblockgemv(inplace=false) -> sparseblockgemv(inplace=true)	nnet local inplace sparse block gemv node	1.000000
detect if the g++ version used is the	gof	0.002381
division inverse of	div	0.166667
the dimensions of this	tensor tensor py operators dimshuffle	0.019231
numpy-compatibility method if	tensor diag	1.000000
input to a leftdims +	input leftdims	0.166667
by an op that will	clinker op	0.142857
important note this function uses set and	scan_module push out seq scan process node fgraph	0.142857
from	complex from	0.250000
node from dict	dict	0.125000
-> a	node	0.007407
this function expects the compile lock to	gof module cache add to	0.142857
exp a -1 and converts this to expm1	tensor local expm1	0.066667
runs a series of wrapper functions	many linkers wrappers	0.047619
convolve spatio-temporal filters with	nnet conv3d signals filters	0.111111
convolution with the specified	conv	0.037037
it has a 'requirement' of the destroyhandler	add destroy handler	0.125000
variable optionally	tensor py	0.015873
constant with value x	tensor constant x	1.000000
given an apply_node recursively search from	apply_node check	0.066667
convolution for	tensor nnet base abstract conv conv	0.125000
variable optionally inserting broadcasted dimensions	operators	0.017241
integers indicating the version	code cache version apply	0.125000
op then replace it with a triangular solve	solve triangular	0.142857
to turn it into a gemm	tensor gemm	0.166667
important note this function uses set and	scan process node	0.142857
is basically	tensor extract constant x elemwise only_process_constants	0.058824
connection pattern of subfgraph defined by inputs and	connection pattern node	0.076923
reduce{scalar op}(join(axis=0 a b), axis=0) -> elemwise{scalar op}	local reduce join node	0.111111
a memory alias that wasn't	view	0.022727
v raises attributeerror if	v	0.011111
match a variable	var	0.035714
important note this function uses set and dictionary	scan process node fgraph	0.142857
source code for this linker and returns	cmodule location	0.038462
the output shape of convolution operation	get conv output shape image_shape kernel_shape border_mode subsample	0.500000
for diagonalsubtensor and	tensor nnet get diagonal subtensor view x i0	0.083333
function for diagonalsubtensor	get diagonal subtensor view x i0	0.083333
to py_none	c init	0.500000
inputs with a set of 3d	tensor nnet conv3d input	0.125000
"init_code"	code struct	0.500000
its idx_list reorders the inputs according to the	inputs idx_list get_count	0.100000
print the mflops	tensor nnet conv op flops	0.125000
a convolution with the specified parameters	conv	0.037037
inserted in the module initialization	init	0.058824
gradient	core grad	0.166667
that copies a vector to the diagonal of	diag	0.023810
source	cmodule location	0.038462
given an fgraph and	fgraph outputs_to_disown	0.047619
is a thunk	linker make thunk	0.125000
mrg stream state	mrg random streams	0.033333
is the equivalent of localoptgroup for	gpulocal opt group	0.055556
implementation of mod	scalar mod c code node name inputs	0.125000
a so pyd dll or py file	dlimport fullpath suffix	0.333333
create a six moves urllib namespace that resembles	six	0.025000
associated with a particular	getitem item	0.125000
minimum elements obtained by iterating over given axis	axis keepdims	0.400000
eq x x -> 1	tensor local useless elemwise	1.000000
moved objects in six moves urllib_error	six moves urllib	0.090909
return a tensorvariable of	make variable name	0.333333
a type wrapper for numpy random randomstate	random state type	0.500000
expression vector 1-dimensional variable	core jacobian expression	0.500000
is	constant	0.016667
context object mapped to the type's	type context	0.090909
for for theano scalar scalar and tensorvariable	scalar as common dtype	0.250000
l operation on f wrt to wrt	lop f wrt	0.200000
do just the compilation of cutils_ext	compile cutils	0.166667
the c code for corr3dmm (direction="forward"),	nnet base corr3d mm c code	0.090909
the input nodes	gof	0.002381
the output dimensions of convolving an image of	nnet conv op get output	0.047619
to the	gpuarray gpu array	0.062500
wrt, computes	core subgraph grad	0.062500
lists/tuples/other objects into a list of	compile flatten l	0.200000
format	format	1.000000
source code for this linker and returns a	gof clinker compile cmodule location	0.038462
shared variable names when persisting to	persistent shared variable id	0.142857
removes useless dimshuffle operation inside reshape reshape(vector	useless dimshuffle in reshape node	0.500000
along the specified axis	x axis sparse_grad	0.333333
graph is impossible to evaluate because of	destroy	0.009709
a convolution with	conv	0.037037
of arguments to pass to helper_c_code	tensor inc subtensor get helper c code args	0.250000
the dimensions of	py operators dimshuffle	0.019231
a previously sent	mpisend	0.037037
of useless reshape	tensor local useless reshape node	0.200000
nd	nd	1.000000
-> alloc(unary x shp)	local alloc unary node	0.250000
key, to be used to find equal keys	gof get safe part key	1.000000
if fgraph is	fgraph no_recycling	0.200000
inputs that are	gof	0.002381
takes as input a	tensor signal pool 2d input	0.090909
outputs from the	outputs	0.045455
y	y axis	0.125000
copies the stack trace from one	stack trace	0.055556
an empty matrix it	alloc	0.012500
verify	verify	1.000000
of shape tuple or	shape feature default infer shape	0.066667
the one hot	one hot	0.142857
removes all asserts from the	tensor local remove all assert	0.055556
graphtogpu	gpulocal	0.055556
optimization makes the folowing changes in the	local mul switch sink node	0.045455
output specs	std fgraph input_specs output_specs accept_inplace	0.142857
that v	tree set v	0.125000
computes the output	conv op get output	0.047619
sparse format instead of dense	sparse	0.019231
to add the tensor	tensor	0.006431
performs the svd	svd	0.034483
first functiongraph that has ever been	gof	0.002381
of this variable optionally inserting broadcasted	tensor tensor	0.014286
replace a crossentropysoftmax1hotwithbiasdx op whose incoming gradient	useless crossentropy softmax 1hot with bias dx	0.111111
the updates ordereddict	updates	0.029412
in a sparse format instead of	core sparse	0.066667
a broadcasted dense vector element wise	sdcsc	0.250000
the folowing changes in the graph t	tensor local mul switch sink	0.045455
a symbolic row variable (ndim=2	tensor row	0.050000
with helper_c_code	get helper c code	0.142857
transfer to	operators transfer	0.125000
of the specified pieces of vectors	sparse block gemv make node o w	0.066667
if current paramstype contains the specified theano type	gof params type has type theano_type	0.500000
a compiled module from the loaded cache or	module cache get module name	0.166667
clip x to	tensor clip x	0.250000
changes node inputs[i]	gof function graph change input node i	0.250000
required return c code to extract	clinker type c extract name	0.250000
the dimensions of this variable	tensor py operators dimshuffle	0.019231
theano op around a function	function op	1.000000
convert x into a variable on the	as gpuarray variable x context_name	0.166667
the -x pattern	neg	0.083333
this computes the outer product	block outer	0.250000
that removes all asserts from	local remove all assert	0.055556
if converting to type2 from	type2	0.050000
apply to be inserted in	apply	0.016667
for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor	0.083333
with helper_c_code	inc subtensor get helper c code	0.142857
for pushing out the	push out	0.037037
spatio-temporal filters with a	filters	0.032258
elementwise [floor] division inverse	tensor int div a b	0.333333
represent the dependence of nodes in	make dependence	0.043478
called by remove_feature feature	gof feature on detach function_graph	0.200000
solve operation c = a \ b	solve	0.032258
to help the navigator deal with	navigator optimizer attach	0.038462
the mean value along	mean	0.062500
n_tests	n_tests	0.714286
a dictionary of arguments	args	0.051282
graph of scan to outside of scan	out scan	0.035714
the inputs and put the variables in the	gof pure op perform node inputs output_storage params	0.047619
raise baddestroymap if necessary update dr_vals	node storage_map r_vals dr_vals	0.250000
matrix in which each row is a mrg	mrg	0.076923
to the type's :attr context_name	gpuarray gpu array type	0.062500
a tuple of integers indicating the version	cache version apply	0.125000
transfer to a tensortype if	py operators transfer	0.125000
this op scale or	scale	0.047619
tril	tril	0.833333
removes all asserts from	remove all assert	0.055556
max and	max and	1.000000
moved objects in six moves urllib_error	module six moves urllib	0.090909
copies the stack trace from one	gof copy stack trace	0.055556
along the axis that was used to	grad inputs g_outputs	0.076923
reduction of x	x	0.008772
elements obtained by iterating over given axis	x axis keepdims	0.500000
dot product of the specified pieces of vectors	sparse block outer make node o x	0.066667
clone the graph and get a dict	graph clone get	0.333333
this	gof clinker	0.266667
some perform() or c_code() created a memory	bad view map	0.142857
and	node name inputs outputs	1.000000
change all sigmoid to ultra_fast_sigmoid	nnet local ultra fast sigmoid node	0.200000
alloc	local alloc	0.111111
sparseblockgemv check sparseblockgemv's docstring	sparse block gemv	0.166667
this preserve some variables	preserve	1.000000
:param execute if true execute	misc execute execute verbose m	0.250000
replacement if the	validate replace	0.050000
makes the folowing changes in the	tensor local mul switch sink	0.045455
generate c	shape c	0.250000
attempting to use dnn conv algo_bwd	safe no dnn algo bwd algo	0.166667
of mod	mod c code node name	0.125000
idx	idx	0.461538
not the other implementation of mod	mod c code node	0.125000
value	tensor	0.003215
row is a mrg stream	sandbox mrg random streams	0.033333
to generate permutations from integers	tensor permutation	0.166667
in a new graph	clone with new inputs inputs	0.166667
a vector	vector	0.066667
after a given version	default version	1.000000
hash equal for same	tensor type hash	0.166667
shp)	alloc unary node	1.000000
strict	strict	1.000000
is	gof gcc	0.027778
not attempting to use dnn	no dnn	0.125000
transform it into a canonical form	tensor get canonical form	0.045455
value	value trace	0.333333
* y + alpha * dot	tensor gemv c code y	0.333333
help the navigator	gof navigator optimizer attach updater fgraph	0.038462
cols	cols	1.000000
for diagonalsubtensor	diagonal subtensor view x i0 i1	0.083333
4-d tensor	patch_size	0.050000
through	through	1.000000
transform of a real-valued input on	gpuarray curfft inp norm	0.066667
return c code to declare variables	clinker type c declare name sub	0.500000
graph is_same_graph	is same graph with merge var1 var2 givens	0.166667
operation to wait on a	mpisend wait	0.045455
an fgraph and a list	fgraph outputs_to_disown	0.047619
gradient updates for	grad	0.010417
match a variable with the -x pattern	neg var	0.166667
perform the	perform	0.058824
return connection pattern of subfgraph defined by inputs	connection pattern node	0.076923
batch	dnn batch	0.333333
this type	tensor tensor type	0.041667
allow_downcast	allow_downcast	1.000000
wrapper around c_extract_out that initializes py_name from storage	gof get c extract out r name	0.333333
a dictionary of arguments to pass to	args	0.025641
proxy for either true_div	scalar div proxy	0.125000
array using	mpirecv	0.037037
to generate permutations	tensor permutation	0.166667
triangular solve	sandbox linalg tag solve triangular	0.142857
only one client and that	only	0.050000
and only adds	tensor local	0.025641
is meant as a	optimizer optimize fgraph	0.200000
and "init_code" together	code struct node	0.500000
erfcinv	erfcinv	0.833333
dimensions	operators dimshuffle	0.038462
node by one which computes the specified	node output_indices	0.142857
to use dnn conv algo_bwd	no dnn algo bwd algo	0.166667
c_extract that	gof	0.002381
see theano tensor argmax	tensor tensor py operators argmax axis	1.000000
join	join	0.545455
special work if	gof	0.002381
bitwise a & b	tensor and a b	1.000000
the outer product of two	sparse block outer	0.047619
as replace_all_validate revert the replacement	replace all validate remove	0.111111
the module initialization	init	0.058824
raise baddestroymap	compile check inputs node storage_map r_vals	0.166667
operation to wait on a previously	wait	0.045455
all outputs defined by indices out_idxs	out_idxs	0.050000
required return data or	gof pure	0.033333
computes the l operation on f	lop f	0.166667
to split x	tensor split	0.500000
the stack trace	copy stack trace	0.055556
of suitable dummy values	provide inputs	0.200000
type	d3viz type	0.333333
and dictionary data structures	scan_module push out non seq	0.125000
see theano tensor prod	tensor tensor py operators prod axis dtype keepdims	1.000000
doing a recursion over the input dimensions	tensor permute row elements rec	0.047619
optionally inserting	tensor py	0.015873
is false we can't change the value after	config param init default	0.040000
input nodes to output nodes of the	gof	0.002381
node by one	node	0.007407
of localoptgroup	gpulocal opt group	0.055556
kinds	tensor	0.006431
tensor_from_scalar(scalar_from_tensor	scalar tensor	1.000000
in profiling to print the mflops	base abstract conv flops inp outp	0.125000
that open windows	misc subprocess popen command	0.142857
this mode	compile mode	0.166667
load a file that was dumped to	misc load f persistent_load	0.333333
the outputs from	outputs mode	0.166667
list of policies	policy policy	0.125000
for use within the op code	get op params	0.100000
this generates the c code for corr3dmm	base corr3d mm c code	0.090909
a -1 and converts this to expm1	local expm1	0.066667
node inputs[i]	graph change input node i	0.250000
set and dictionary data structures	out non	0.125000
the function on the inputs and put the	gof pure op perform node inputs output_storage params	0.047619
implements the "reverse-mode"	perform node	0.166667
input nodes to output nodes	gof	0.002381
source code for this linker and returns a	clinker compile cmodule location	0.038462
the dimensions of this variable optionally inserting	py operators	0.015625
raise baddestroymap if	compile check inputs node	0.166667
c header for the fortran blas interface	tensor blas header text	1.000000
a recursion over	tensor permute row elements rec	0.047619
names to an iterable of variables modifies	give variables names variables	0.333333
simplify a multiplication	tensor nnet simplify mul	1.000000
the r-operator	max pool rop	0.142857
:attr	type	0.011905
attempting to use dnn conv workmem	core safe no dnn workmem workmem	0.166667
the c code for corr3dmm (direction="forward"), corr3dmm_gradweights	nnet base corr3d mm c code	0.090909
that way will probably not activate envs	compat maybe add	0.200000
tree and figure out which nodes	scan_module traverse out x x_copy d	0.047619
exc	exc	0.500000
passed-in key is found in the cache and	cache get from key key key_data	0.333333
with debug	with op	0.166667
their apply_node if those nodes are not in	gof	0.002381
x shp -> alloc(unary x shp)	tensor local alloc unary	0.250000
see theano tensor prod	tensor tensor py operators prod axis dtype	1.000000
apply_node recursively search from this node to know	import apply_node check	0.066667
variable representing a computation on a certain	array variable	0.500000
as	as	0.146341
compute sum of non nan /	tensor tensor constant signature get sum	0.142857
some requirements to the	requirements	0.125000
profiling to print the mflops	nnet conv op flops inputs outputs	0.125000
of moved objects in six moves	module six moves urllib	0.181818
mrg stream state and they are	mrg random streams	0.033333
not for stability	ultra fast scalar	1.000000
that allows replacing subgraphs of a computational graph	clone output replace strict share_inputs	0.071429
3d convolution for	tensor nnet base abstract conv conv	0.125000
convert addsd to faster addsd_ccode	local addsd ccode node	0.250000
max for the maximum in one	tensor maximum x y	0.090909
to	destroy	0.009709
to ultra_fast_sigmoid	local ultra fast	0.500000
only used to determine the broadcast pattern for	tensor adv index broadcastable pattern a	0.066667
"reverse-mode" gradient for the eigensystem	eigh grad perform node inputs	0.333333
sparseblockouter(inplace=false) -> sparseblockouter(inplace=true)	tensor nnet local inplace sparse block outer node	1.000000
a symbolic integer scalar	unpack	0.125000
only if this	gof	0.002381
from an ndarray	from ndarray data	1.000000
a slice [start stop	slice	0.038462
value underlying variable v	value	0.043478
some perform() or c_code() modified	map	0.047619
a constant that	gof params	0.200000
of the specified pieces of vectors	sparse block gemv make node	0.066667
function name to write	gpuarray write	0.200000
adds new optimization instances to a mode	mode register	1.000000
as replace_all_validate revert the replacement if the ops	validate replace all validate remove	0.111111
graph and get a memo a dict	function graph	0.040000
the same shape	feature same shape	0.333333
0 based level of the list	list type	0.100000
as a whole to gpu instead of transfering	to gpu	0.500000
variables inside the scan that	scan	0.034483
the same parameter as other scalar	scalar	0.035714
structures	non seq	0.111111
for same kinds of	type	0.011905
revert the replacement if	validate replace all	0.050000
the hack in profiling to print the mflops	flops inp outp	0.041667
images2neibs <theano tensor nnet	tensor nnet images2neibs	0.333333
add tag trace to	add tag trace thing user_line	0.166667
the hack in profilemode to print the mflops	flops inp outp	0.083333
compiled module from the loaded cache or	cache get module name	0.166667
of this variable optionally inserting broadcasted	py	0.014286
generates the c code for corrmm (direction="forward"), corrmm_gradweights	base corr mm c code	0.090909
a crossentropysoftmax1hotwithbiasdx op whose incoming	local useless crossentropy softmax 1hot with bias dx	0.111111
compute	constant signature get	1.000000
gradient function should	matrix inverse r op inputs	0.500000
returns minimum elements obtained by iterating over	keepdims	0.052632
transfer	operators transfer	0.125000
to print the mflops	op flops	0.125000
the dot product	dot x	0.166667
new graph	with new	0.166667
to make itself the	to	0.017544
that runs a	gof wrap linker	0.083333
profilemode to print the mflops	gpu corr mm flops inp outp	0.125000
that wasn't in the destroy_map	destroy	0.009709
list of l{codeblock} instances returns a string that	code gen blocks	0.050000
revert the replacement if the ops	replace validate replace	0.050000
required return	gof pure	0.033333
with logsoftmax	logsoftmax	0.076923
x is an input vector and	node input_storage	0.038462
fgraph and	fgraph outputs_to_disown	0.047619
extract test value from v	test value v	0.250000
default reverse them	transpose	0.166667
nit_sot output of scan return true	scan	0.017241
hash	tensor hash	0.333333
is the llvm one or not	llvm	0.100000
dimensions scrap the dimshuffle and index the	tensor local dimshuffle	0.052632
new instance	clone link_kwargs optimizer	0.111111
shape that broadcast them to	generate broadcasting	0.066667
the dimensions of this variable	tensor tensor py	0.015873
nested loop over several	loop	0.027778
is	to gpulocal	0.055556
by re-writing the file	gof refresh	0.125000
order a graph of apply	sort apply	0.200000
config string (comma-separated key=value components) into a dict	parse config string config_string issue_warnings	0.166667
to the task that is associated to it	gof cthunk find task failure_code	0.083333
shape of all intermediate variables	tensor shape of variables fgraph input_shapes	0.100000
node by one which computes the specified	node output_indices alloc_ops	0.142857
and	node input_storage	0.038462
useless	local useless	0.222222
----------	pool	0.066667
a symbolic scalar variable	scalar name dtype	0.166667
a symbolic row	row name dtype	0.050000
a crossentropysoftmax1hotwithbiasdx op whose incoming	crossentropy softmax 1hot with bias dx	0.111111
softmax activation function :math \varphi(\mathbf{x})_j =	softmax	0.090909
wait on a previously sent array	mpisend wait	0.045455
validations on the inner graph to ensure that	scan validate inner graph	0.035714
the broadcast pattern for	pattern	0.028571
specified pieces of vectors and matrices	sparse block outer make	0.066667
first functiongraph that has ever been associated	gof	0.002381
the equivalent of	to	0.017544
variables to one or more tensor variables	from_var to_var	0.250000
optimization disabled by default that removes all asserts	remove all assert	0.055556
shape	feature default infer shape	0.066667
the dependence of nodes in	gof make dependence	0.043478
count	count	1.000000
are views of v given that v	tree set v	0.125000
more multinomial	tensor multinomial	0.037037
on the inputs and put	gof pure op perform node inputs output_storage params	0.047619
replace a symbol definition with an elementwise version	scal inplace symbol	0.333333
respect to wrt,	subgraph grad	0.062500
see theano tensor max	tensor py operators max axis	1.000000
similar behaviour as python's map	map fn sequences non_sequences truncate_gradient	1.000000
bitwise a | b inplace on a	tensor or inplace a b	0.333333
diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view	0.083333
return c code to declare variables	type c declare name sub	0.500000
the type's :attr	array type	0.055556
create a base class with a metaclass	with metaclass meta	0.333333
print	print fn	1.000000
of given shape and flags	pool grad out shape imgshape ws ignore_border stride	0.200000
op could be very easy	op	0.009174
init_loop_orders	init_loop_orders	1.000000
home	home	1.000000
optimizer for pushing out	push out	0.037037
parses a config string	core parse config string	0.333333
an instance of _maker which handles much of	compile debug mode function maker i o m	0.066667
toposort return	graph toposort	0.125000
merge multiplication by	alpha merge	0.076923
indices that are non-zero in the flattened version	flatnonzero	0.083333
grad of this op could be very easy	tensor prod l op	0.033333
graph is impossible	destroy	0.009709
function for diagonalsubtensor and	diagonal subtensor	0.083333
a recursion over the input	tensor permute row elements rec	0.047619
raise baddestroymap	check inputs node	0.166667
s to previously un-shaped variable	s override	1.000000
the g++	gcc	0.023810
addition of	add	0.034483
when enabled change all sigmoid to ultra_fast_sigmoid	ultra fast sigmoid node	0.200000
given an apply_node recursively search from this	apply_node check	0.066667
if an alloc is inside a	alloc node	0.037037
for a diagnosis if things go awry	function graph check integrity	0.250000
and	signal pool grad out	1.000000
types	clinker type	0.500000
to the idx list to	idx list	0.250000
this convert allocempty to alloc of 0	alloc empty to zeros node	0.333333
on	gpuarray gpu	0.045455
code when doing constant folding	python constant folding	0.142857
original graph to a new	outputs copy_inputs_and_orphans memo	0.029412
of m1	m1	0.027027
return the list of op classes that this	gof local optimizer tracks	0.071429
with the ignore_trees-related functionality	optimizer attach updater fgraph importer pruner chin	0.250000
compiled module from the loaded cache or the	cache get module	0.166667
out for occurrences of values identical with x	forced replace out x	1.000000
conda offers to make itself the default	to	0.017544
graph and get a memo a	graph	0.016393
of node	node	0.007407
-> x remove split with only 1 split	tensor local useless split	1.000000
confusion	confusion	0.750000
apply_node if those nodes are not	gof	0.002381
tensorconstant	constant	0.016667
this function is basically	tensor extract constant x elemwise only_process_constants	0.058824
diagonalsubtensor and	get diagonal subtensor view x i0 i1	0.083333
the dimensions	tensor tensor py operators dimshuffle	0.019231
return full path of the dynamic lib in	gof module name from	0.076923
an input vector and t is a	node input_storage	0.038462
inputs	inputs node	0.100000
this convert allocempty to alloc	tensor local alloc empty to zeros node	0.333333
values of a shared variable to 0	compile shared variable zero borrow	0.200000
computes the outer product of two sets	sparse block outer	0.047619
bitwise a ^	xor a	1.000000
function tries to	image_shape top_shape	0.166667
the output	dims output input	0.333333
if the ops in the list remove	remove	0.035714
of _maker which handles much	maker i o m	0.066667
shape tuple	default infer shape	0.066667
the dimensions of this	operators	0.017241
by this	gof seq	0.200000
all unknown variables and apply_nodes to this graph	gof function graph	0.031250
eq x x -> 1	tensor local useless elemwise node	1.000000
scan the contents	dirname err files	0.083333
gradient updates for matrix solve operation	solve grad	0.250000
randomstate instance associated with a particular	getitem item	0.125000
"lifts" dimshuffle through elemwise operations	local dimshuffle lift	0.250000
tries	image_shape top_shape border_mode	0.166667
function drawing	random_state n pvals	1.000000
will match self if possible	var	0.035714
convop that unroll the batch size loop	code unroll batch kern d unroll_bsize unroll_ksize	0.125000
important note this function	out seq scan process node	0.142857
generates the c code for corr3dmm	base corr3d mm c code	0.090909
from variable and apply	get	0.020833
dict variable k -> list of variables	variables	0.043478
convert python litterals to	make constant	0.100000
specifyshape	c_support_code_apply	0.111111
theano graphs represent the same computations	scan_module equal computations xs ys in_xs in_ys	0.333333
simplify a multiplication	nnet simplify mul	1.000000
x and there is	sandbox linalg local	0.142857
types involved	tensor inc subtensor do type checking	0.142857
that has ever been associated to self else	gof	0.002381
graph of a compiled theano function's ops supports	core pydotprint fct outfile compact format	0.250000
rebroadcast how	rebroadcast	0.111111
supports	supports	1.000000
conda offers to make itself the default python	to	0.017544
true if the named module is a package	path importer is package fullname	0.250000
scan to parse the tree and figure out	scan_module traverse out x x_copy d	0.047619
threads	threads	1.000000
in	node	0.007407
the given graph contains a cycle parameters	contains cycle	0.333333
the inner graph	scan_module scan validate inner graph	0.035714
see theano tensor argmax	tensor py operators argmax axis keepdims	1.000000
to recognize the updates ordereddict the list of	get updates	0.034483
compiled module from the loaded cache	gof module cache get module name	0.166667
from nvidia	conv3d img kerns	1.000000
memory alias that wasn't in the	bad view	0.027027
the tensor operators to the basic constant class	constant	0.016667
grad	grad dict var_to_app_to_idx grad_dict wrt cost_name	1.000000
along the given axis es of a tensor	axis ddof keepdims	0.083333
a thunk that is a zero-arguments function	thunk	0.021277
gradient function should	matrix inverse grad inputs g_outputs	0.500000
that initializes py_name from storage	gof get c extract r name sub	0.250000
work for gpuincsubtensor	local inplace setsubtensor node	0.250000
op could be very easy	prod l op	0.033333
into macros for use within	cop get	0.033333
a warning	gof deprecated filename	0.166667
directory and return full path	module name from dir	0.071429
bitwise a & b	and a b	1.000000
nit_sot output of scan return	scan	0.017241
for scalar values default int64 or	scalar shared	0.083333
the c code for corrmm (direction="forward"), corrmm_gradweights	tensor nnet base corr mm c code	0.090909
y + alpha * dot a	tensor gemv c code y a	0.333333
see theano tensor std	tensor tensor py operators std axis ddof keepdims	1.000000
one or more multinomial distributions defined by one-dimensional	tensor multinomial	0.037037
convolve spatio-temporal filters with	conv3d signals filters	0.111111
specs	specs	1.000000
to import six moves	six	0.025000
3-d variable	tensor tensor3 name	0.500000
all sigmoid to ultra_fast_sigmoid	ultra fast sigmoid node	0.200000
important note this function uses	scan_module push out seq scan process node fgraph	0.142857
a new instance of	clone link_kwargs optimizer	0.111111
from the loaded cache or the	module cache get	0.250000
of shape tuple or	default infer shape	0.066667
vstack	vstack	1.000000
comparator to represent the dependence of nodes	gof make dependence cmp	0.111111
docstring	sparse block	0.222222
it with a triangular solve	sandbox linalg tag solve triangular	0.142857
override clinkertype c_sync	tensor tensor type c sync name sub	1.000000
python litterals to	tensor	0.003215
a canonical form that respects	tensor get canonical form	0.045455
a variable with a	variable x name	0.083333
hack in profiling to print the mflops	conv op flops	0.125000
see theano tensor	tensor py operators	0.171875
tuple python type c type numpy typenum that	tensor type dtype specs	0.071429
lift	lift	1.000000
write data	write w dtype	1.000000
__unify_walk__ method for one	unify walk a b u	0.037037
the __unify_walk__ method	gof unify walk a b u	0.037037
a variable on	gpuarray variable	0.166667
data by walking the cache	gof module cache refresh age_thresh_use delete_if_problem cleanup	0.125000
context object mapped to the type's :attr	gpu array type context	0.090909
as replace_all_validate revert the replacement if the	gof replace validate replace all validate remove	0.111111
numpy randomstate instance associated with a particular	item	0.076923
value from equilibriumoptimizer by calling the method query	graph to gpudb	0.142857
represent the dependence of	dependence	0.035714
fill s v -> alloc(v shape s this	tensor local fill to alloc	0.250000
the signature for this function	function method decl	0.333333
return a code string specific to	node name sub	0.111111
*args directly	sparse local csm properties csm	0.142857
seconds since the epoch of the last access	last access time	0.040000
make a schedule function from comparators	schedule fn	0.333333
header search paths	header dirs	0.045455
the platform-dependent gcc argument for shared libraries	get gcc shared library arg	0.333333
to wait on a previously sent	mpisend wait	0.045455
variable	py operators dimshuffle	0.019231
decorator to support version-based cache mechanism	gpuarray code version version	0.333333
when we overwrite the full inputs with the	tensor local useless inc subtensor node	0.066667
that	nnet conv op	1.000000
dict op -> total time icluding the	compile profile stats compute total times	0.333333
bound on	bound	0.043478
to even	to even	1.000000
perform the	perform node x y	0.166667
of this variable optionally	py operators dimshuffle	0.019231
(direction="backprop weights"),	helper bottom weights top direction	0.166667
constructor	constructor	1.000000
of apply	apply	0.033333
transfer to a tensortype if not already	operators transfer	0.125000
function for diagonalsubtensor and incdiagonalsubtensor	diagonal subtensor view x	0.083333
nodes to output nodes of the graph	gof	0.002381
in array of ints	bincount x weights minlength assert_nonneg	0.125000
revert the replacement if the ops	replace validate replace all	0.050000
:type cost scalar 0-dimensional	hessian cost wrt consider_constant disconnected_inputs	0.500000
output of scan return true iff the	out scan	0.035714
2 profiles returned by this	gof	0.002381
an apply_node recursively search	apply_node check	0.066667
the given axis es	axis dtype op	0.083333
from variable and apply nodes in the	get	0.020833
of this variable optionally	tensor tensor py operators dimshuffle	0.019231
and dtype	dtype	0.022727
internal class should not be used by clients	scalar	0.017857
list of library search paths	lib dirs	0.045455
hash equal for same kinds of	type hash	0.166667
this convert allocempty to alloc of	tensor local alloc empty to zeros	0.333333
is the method to override this should return	name	0.011111
base op for cudnn batch normalization	gpu dnn batch norm	1.000000
an op that will be inserted at	op	0.009174
sharedvariable constructor	shared value name strict allow_downcast	0.500000
the input broadcastable in the specified axes	tensor addbroadcast x	0.142857
returns the connection pattern of a subgraph	connection pattern	0.032258
important note this function uses set and	push out seq scan process node fgraph node	0.142857
print the mflops	base abstract conv flops inp outp	0.125000
lib directories that are needed by	clinker header dirs	0.055556
scalar values default int64 or	scalar	0.017857
get the 0 based	type get	0.050000
with logsoftmax x 's grad	nnet local logsoftmax grad	0.200000
list	gof	0.002381
an input that	bad destroy	0.034483
a thunk that operates on the	gof linker make thunk	0.045455
[v1 v2 v3 ]	vm linker compute gc dependencies	1.000000
recognize the updates ordereddict the list of	get updates	0.034483
the shape	compile shape	0.250000
apply instance in a new graph	apply clone with new inputs	0.500000
a leftdims + rightdims	leftdims rightdims	0.333333
a constant that is	constant error	0.166667
cache directory	gof module cache	0.083333
to turn softmax(sum_of_stuff) ->	local	0.014085
a functiongraph a constant	constant	0.016667
necessary update dr_vals	node storage_map r_vals dr_vals	0.250000
useless reshape	local useless reshape	0.200000
erfcx	erfcx	1.000000
optimization makes the folowing changes in	local mul switch sink	0.045455
this compiles the source code	compile cmodule location	0.038462
over given axis	tensor argmin x axis	0.500000
template filled by broadcasting value	broadcast like value template fgraph	0.125000
x	local	0.028169
get the	type get depth	0.050000
k -> list of variables	variables	0.043478
is only used to determine the broadcast pattern	tensor adv index broadcastable pattern a idx	0.066667
a cache directory and return full path	module name from dir	0.071429
that initializes	gof get c	0.166667
important note this function uses	scan process node fgraph node	0.142857
orderings	orderings	1.000000
inputs replaced by their python	inputs allow_partial only_process_constants elemwise	0.166667
function performs the matrix inverse on gpu	gpuarray gpu matrix inverse a	0.200000
not required anymore and should be removed	compress outs	0.076923
subtensor(setsubtensor	subtensor inc subtensor node	1.000000
-a inplace on	neg inplace	1.000000
performs batch normalization	tensor nnet batch normalization	0.125000
exception object with debug	raise with op	0.333333
optionally	tensor py	0.015873
function that allows replacing subgraphs	scan_module clone output replace strict share_inputs	0.071429
determine the broadcast pattern for	adv index broadcastable pattern a idx	0.066667
decorator which will print a warning	gof deprecated filename	0.166667
a schedule	gof sort schedule	0.333333
converts number to	compile char from number number	0.142857
decorator for creating a class with a metaclass	metaclass metaclass	0.125000
switch	switch	1.000000
any python object a that would be a	gof pure	0.033333
for pushing out the variables inside the	out	0.018519
should remove any dynamically added functionality	on detach fgraph	1.000000
list of lib directories that are needed by	clinker lib dirs	0.055556
the output dimensions of convolving an	op get output	0.047619
true iff the outer nit_sot	last step used var scan_args	0.200000
more multinomial distributions defined	multinomial random_state	0.040000
with	with op node thunk	0.166667
the inputs according to	inputs	0.012658
this function tries	image_shape top_shape	0.166667
help the navigator deal with the	navigator optimizer attach updater	0.038462
u and	o u	0.037037
connection pattern of subfgraph defined	compile op from graph connection pattern node	0.076923
for diagonalsubtensor and	diagonal subtensor view x	0.083333
subclass to	tensor	0.006431
from a uniform distribution between low and high	uniform random_state size low high	0.333333
expects the compile lock to be held	cache add to cache module key module_hash	0.166667
raise baddestroymap if	inputs node	0.100000
a special compound l{op} for the output of	crossentropy softmax argmax1hot	0.083333
a new random	tensor random	0.166667
slice [start stop step]	slice	0.038462
dimensions of this variable optionally inserting broadcasted dimensions	operators dimshuffle	0.019231
remove item from six moves	compat remove move name	1.000000
an alloc and only adds dimension	local alloc	0.111111
see theano tensor argmax	tensor py operators argmax	1.000000
variable optionally inserting broadcasted dimensions	dimshuffle	0.014493
reorder the dimensions of	tensor tensor py operators dimshuffle	0.019231
the specified pieces of vectors and matrices	sparse block gemv make node o w h	0.066667
hack in profilemode to print the mflops	base gpu corr mm flops inp outp	0.125000
we can't change the value after the import	core config param init default	0.040000
standard deviation along the	tensor std	0.111111
matrix inverse on gpu	gpuarray gpu matrix inverse a	0.200000
the input	gpu	0.011765
be created if	gof	0.002381
of arguments to pass to helper_c_code	subtensor get helper c code args	0.250000
true if and only if this enum	gof enum type	0.166667
perform some elementary validations on the inner graph	scan validate inner graph	0.035714
return true for any python object a that	gof pure	0.033333
the function name to write data	gpuarray write w dtype	0.200000
c contiguous version of the	gpu contiguous	0.083333
return line stripped of trailing spaces tabs newlines	misc hooks rstrip line junk	1.000000
convert python litterals	make	0.017857
determine the broadcast pattern for	tensor adv index broadcastable pattern a	0.066667
and their	max and argmax a	0.250000
one hot	one hot	0.142857
the fortran blas	blas	0.125000
device we do it only on cpu here	tensor local pow specialize device node	1.000000
get the 0 based	get depth	0.050000
convert x into a variable on the	gpuarray as gpuarray variable x context_name	0.166667
access	access	0.600000
name to load data	gpuarray load w dtype	0.200000
a graph of apply nodes according to a	gof sort apply nodes inputs outputs cmps	0.050000
op for cudnn	gpu dnn	0.133333
argsort	argsort axis kind	1.000000
convolution with	conv get	0.100000
numpy round half to even	round half to even	0.166667
sample	size	0.153846
revert the replacement if the	validate replace	0.050000
to be between min and max	min max	0.250000
see theano tensor repeat	tensor tensor py operators repeat repeats	1.000000
string representating the cause	bad optimization str diagnostic	0.043478
the dimensions of this variable optionally inserting broadcasted	operators	0.017241
optimization makes the folowing changes in the	tensor local mul switch sink	0.045455
return a code string	name sub	0.050000
the inner-most loop executes code	tensor make reordered loop	0.111111
1/(1+exp x -> sigm	local inv 1 plus exp node	0.333333
sigmoid to ultra_fast_sigmoid	nnet local ultra fast sigmoid	0.200000
it with a triangular solve	linalg tag solve triangular	0.142857
make an inplace optimization that deals with allocempty	inplace allocempty op idx	0.166667
c code to initialize the variables	type c	0.071429
multinomial distributions defined by one-dimensional	tensor multinomial	0.037037
by default that removes all	local remove all	0.166667
that gets a scan op	op not_required	0.071429
1/(1+exp x -> sigm -x	local inv 1 plus exp node	0.333333
to override this should return an	node name	0.033333
by reducing the number of multiplications and/or divisions	tensor local greedy distributor node	0.166667
compiles this linker's fgraph	gof clinker compile input_storage output_storage storage_map keep_lock	1.000000
[true] division inverse	true div	0.250000
tile input array x according to reps	tile x reps ndim	1.000000
mean value along the	tensor mean	0.111111
the shape	shape	0.061224
proxy for either true_div or int_div depending	proxy	0.095238
of x the	tensor flatten x	0.166667
a and b are unified	a b	0.066667
base op for	gpu	0.011765
a triangular solve	sandbox linalg tag solve triangular node	0.142857
the number of dimensions from the shape	shape	0.010204
that allows replacing subgraphs	compile rebuild collect shared outputs inputs replace updates	0.500000
of the subgraph between i and o	i o	0.041667
the dimensions of this variable optionally inserting	tensor py	0.015873
dimensions of this variable	py operators dimshuffle	0.019231
to help the navigator	navigator optimizer attach updater fgraph	0.038462
for openblas threads	tensor openblas threads	0.250000
c-implementation of the	csr c code	0.333333
of a shared	shared	0.062500
compute the dot	tensor nnet	0.035088
reorder	tensor tensor py operators	0.015625
:attr	gpu array type	0.062500
real-valued input on	gpuarray curfft inp norm	0.066667
d3viz	d3viz	0.714286
a six moves	module six	0.043478
to output nodes of the	gof	0.002381
a convolution with the specified parameters	conv get	0.100000
wait on a previously received array using mpi	wait	0.022727
function to concatenate tensortypes along the given axis	join axis	0.333333
from a multinomial	multinomial	0.024390
op then replace it with a triangular solve	tag solve triangular	0.142857
upper triangle of an	triu m k	0.250000
other	other	0.454545
cache "filename" as a pickle file	call cache persist filename	0.250000
this optimization makes the folowing changes in the	local mul switch sink node	0.045455
with a triangular	triangular	0.076923
same on all device	device	0.076923
important note this function uses	seq scan process node fgraph node	0.142857
grad of this op could	l op	0.033333
shape	shape feature default infer shape	0.066667
images2neibs	images2neibs	0.666667
sync	sync	1.000000
computes the mean value along the	mean	0.062500
the topooptimizer from the input nodes	in2out	0.043478
for the shape element	shape feature	0.500000
the value after the import	param init default	0.040000
change the value after	default filter	0.040000
of arguments to pass to	args	0.025641
the c code for corrmm (direction="forward"), corrmm_gradweights	base corr mm c code	0.090909
local optimizer	local	0.014085
still in	replacements	0.111111
for matrix solve operation c = a \	solve	0.032258
thunk	make thunk	0.125000
topooptimizer from the input	gof in2out	0.055556
broadcastable dimensions scrap the dimshuffle and index	tensor local dimshuffle	0.052632
change the value after the import of theano	init default	0.040000
shape	set shape	0.333333
2d inputs with a set of 2d filters	tensor nnet conv2d input filters	0.125000
moved objects in six moves urllib_request	six moves urllib request	0.333333
:param execute if true execute a theano	execute execute verbose	0.250000
elementwise conjugate	tensor conj	0.250000
op and reduce pattern has functioning c	gpu careduce cuda supports c	0.200000
computes the output	get output	0.047619
detect if	gof	0.002381
transfer to a tensortype if not already one	tensor py operators transfer	0.125000
a modulo	a	0.008065
gradient the	grad	0.010417
c	specify shape c	0.250000
tag trace to an node or	tag trace thing user_line	0.166667
from variable and apply nodes in	get	0.020833
apply instance in	apply clone	0.333333
this convert allocempty to alloc of 0	local alloc empty to zeros node	0.333333
tensor operators to the	tensor	0.006431
tries to recognize the updates ordereddict the list	updates	0.029412
*	sqr	0.125000
the connection pattern of a subgraph defined	connection pattern	0.032258
draw samples from a poisson distribution	tensor random streams base poisson size lam ndim	1.000000
new graph	new inputs inputs	0.166667
cmp	cmp	0.294118
broadcasted dimensions	dimshuffle	0.014493
from the loaded cache or the	gof module cache get	0.250000
compute conv output gradient	tensor nnet conv3d grad	0.333333
minimum elements obtained by iterating over given axis	tensor min x axis keepdims	0.500000
f wrt to wrt	core lop f wrt	0.200000
a six moves urllib namespace that resembles the	module six	0.043478
a thunk that	thunk	0.021277
unroll the batch size loop	nnet gen conv code unroll batch	0.166667
concatenate a number of scalars together into	make	0.017857
wraplinker that runs a series	gof wrap linker many linkers wrappers	0.071429
return a symbolic row variable	tensor row	0.050000
reduce pattern has functioning c code	careduce cuda supports c code	0.250000
the list of outputs	outputs	0.045455
usmm	usmm	1.000000
python not the other implementation of mod	mod c code	0.125000
shp -> alloc(unary x	tensor local	0.025641
that wasn't in the view_map	bad view	0.027027
default that removes all	local remove all	0.166667
elementary validations on the inner graph to ensure	inner graph	0.035714
help the navigator deal with the	navigator optimizer attach updater fgraph	0.038462
this variable optionally	tensor	0.006431
op and reduce pattern has functioning	gpuarray gpu careduce cuda supports	0.166667
mflops	gpu corr mm flops inp outp	0.125000
fgraph to break aliasing of	fgraph wrapped_inputs wrapped_outputs	0.111111
a file that was dumped to	f persistent_load	0.052632
the original graph	outputs copy_inputs_and_orphans memo	0.029412
bitwise a |	tensor or	0.333333
sum of non nan	sum	0.038462
the output specs	compile std fgraph input_specs output_specs accept_inplace	0.142857
this variable	operators	0.017241
the output after pad_dims	unpad dims output	0.333333
compute 2d kernel for bilinear	nnet bilinear	0.111111
slices in pvals	n pvals	0.125000
function on the inputs and put	gof pure op perform node inputs output_storage params	0.047619
value after the import of	init default filter	0.040000
in u and uses it	o u	0.037037
converts self _grad_op from user supplied form to	from graph recompute grad op	0.200000
end variables	wrt end	0.050000
make an inplace	inplace	0.025641
called by remove_feature	detach function_graph	1.000000
of convolution gradweights	get conv gradweights	0.500000
reshape t by inserting 1 at the dimension	shape padaxis t	0.333333
} b axis=l) ->	local	0.014085
the	to gpulocal opt	0.055556
used to determine the broadcast pattern	adv index broadcastable pattern a	0.066667
computes the output dimensions of convolving an	op get output	0.047619
definition with an elementwise version of	tensor scal inplace	1.000000
variable to 0	variable zero borrow	1.000000
into a basic theano op that will	op itypes otypes infer_shape	0.047619
reverse-mode gradient updates for matrix	grad inputs output_gradients	0.250000
attempt to replace a leaf of a multiplication	replace leaf arg leaves new_leaves op	0.250000
directories	dirs	0.071429
the navigator	gof navigator	0.038462
were declared	init	0.058824
called whenever	function_graph	0.125000
the value after the	default	0.030303
with constant	get constant idx	0.250000
baddestroymap if necessary update dr_vals	storage_map r_vals dr_vals	0.250000
object with	with	0.076923
header for openblas	openblas	0.111111
toposort return an	graph toposort	0.125000
object with debug info	with op node	0.166667
inner graph to ensure	inner graph	0.035714
parameters ----------	gpu cum op	1.000000
a list of compilation flags from config	libs flags libs_dir include_dir	0.052632
track less frequent op	tensor get clients2 node	0.200000
generates the c code for corr3dmm (direction="forward"),	tensor nnet base corr3d mm c code	0.090909
is the equivalent	to gpulocal	0.055556
to a leftdims +	leftdims	0.090909
attempts to replace a scan	optimizer attempt scan	1.000000
tensorconstant	tensor	0.003215
for elemwise and gpuelemwise	local elemwise fusion	0.166667
to the end variables of	wrt end	0.050000
wasn't	bad destroy	0.034483
the main diagonal set to a specified	fill diagonal offset a val offset	0.100000
the idx_list with constant	constant idx	0.250000
the stopping condition returned by the	ls	0.090909
the hack in profiling to print the mflops	conv flops inp outp	0.125000
that unroll	gen conv code unroll	0.250000
of this	tensor tensor py operators	0.015625
have a stack	gof check stack	0.142857
of the graph's apply nodes such that	gof	0.002381
convolution gradient with respect	gpu dnn conv grad	0.125000
the numpy randomstate instance associated with a particular	getitem item	0.125000
for small or builtin	is simple	0.200000
return a symbolic scalar variable	scalar name dtype	0.166667
needed as some features introduce instance methods	function graph getstate	0.500000
for diagonalsubtensor	diagonal subtensor view x	0.083333
and replace it with logsoftmax x 's	tensor nnet local logsoftmax	0.076923
a list of nodes that must be	gof	0.002381
provided l{functiongraph} it may	optimizer apply fgraph	0.200000
generate permutations	tensor permutation	0.166667
dot22 computing an outer-product	dot22 to	1.000000
specific code to each level of nesting	loop_orders dtypes loop_tasks	0.125000
reorder the dimensions of this variable optionally inserting	tensor	0.006431
function for diagonalsubtensor and	nnet get diagonal subtensor view x i0 i1	0.083333
inner-most loop executes	reordered loop	0.111111
row	row	0.241379
helper function to generate permutations	tensor permutation helper random_state	0.333333
transform of a real-valued input on the gpu	curfft inp norm	0.066667
clip x to be between min and	tensor clip x min	0.500000
a config	parse config	0.333333
output dimensions of convolving an image	conv op get output	0.047619
parse	tensor nnet parse	0.500000
raised by get_scalar_const_value if called	error	0.025000
inverse error function	erfinv a	1.000000
this to expm1 a	tensor local expm1 node	0.066667
lock_dir	lock_dir	1.000000
one or more tensor variables	from_var to_var	0.250000
a list of shape	feature default infer shape	0.066667
a symbolic row	tensor row name	0.050000
value after the import of theano	init default	0.040000
specs and the output specs	input_specs output_specs accept_inplace	0.142857
folowing changes in the graph t	local mul switch sink node	0.045455
key	key key	1.000000
and argmax over	and argmax	0.166667
theano	py	0.171429
template filled by broadcasting value through it	broadcast like value template fgraph	0.125000
this	gof clinker object c	0.500000
variable with the -x pattern	is neg	0.166667
alias	alias alias	0.333333
a specified scalar	a	0.008065
this is the equivalent of localoptgroup	graph to gpulocal opt group	0.055556
min_wait	min_wait	1.000000
match a variable with either	var	0.035714
functiongraph listeners to help the navigator	navigator optimizer attach updater	0.038462
compute the kernel shape of convolution gradweights	nnet get conv gradweights shape image_shape	0.500000
elementwise conjugate (inplace on a)	conj inplace a	1.000000
2d kernel that can be used to	kernel 2d	0.050000
upcasts constant inputs	constant inputs	0.125000
same kinds	tensor tensor type	0.041667
not support the types involved	inc subtensor do type checking	0.142857
supplied function as its implementation	as	0.024390
python litterals to	tensor make	0.076923
by re-writing the file containing the owner's unique	gof refresh	0.125000
logsoftmax x 's grad	tensor nnet local logsoftmax grad node	0.200000
will print a warning message on the first	deprecated filename msg	0.041667
raise baddestroymap	node storage_map	0.166667
a triangular solve	tag solve triangular	0.142857
solve operation c = a \ b	tensor solve	0.038462
instance of _maker which handles much	debug mode function maker i o m	0.066667
of convolution gradweights	conv gradweights	0.500000
see whom can	can	0.142857
batch	nnet batch	0.500000
in profiling to print the mflops	abstract conv flops inp outp	0.125000
of max	max	0.062500
directory and return full path of the dynamic	gof module name from dir	0.071429
compiles the source code for this linker and	clinker compile cmodule location	0.038462
the output dimensions	tensor nnet conv op get output	0.047619
true if l has any duplicates (according	has duplicates l	0.111111
recursion over the input	permute row elements rec	0.047619
of order v	v	0.022222
up to the end variables of	grad wrt end	0.050000
spatio-temporal filters with a	nnet conv3d signals filters	0.111111
whose	tensor local useless	0.111111
function for alternative	fn	0.083333
a 4-d tensor it sets all non	patch_size	0.050000
occurrences of values identical with	forced replace	0.333333
to sharedvariable instances of suitable dummy values	optimizer provide	0.200000
removes all from	gof function graph remove client	0.200000
:type expression vector 1-dimensional variable	core jacobian expression wrt consider_constant disconnected_inputs	0.500000
for matrix solve operation c = a	solve	0.032258
node inputs[i]	change input node i	0.250000
broadcastable dimension with index 0 or -1 a[:,:,:,0]	broadcastable index	1.000000
empty	alloc	0.012500
safely compute ceil(float_division	tensor ceil intdiv	1.000000
a set of 3d	conv3d	0.076923
symbolic row variable (ndim=2	tensor row name	0.050000
connection pattern of a subgraph defined by	io connection pattern	0.055556
string for making	gpuarray gpu careduce cuda makecall node	0.333333
wait on a previously sent array	wait	0.022727
reorder the dimensions of this variable	tensor	0.006431
this type	pure type	0.142857
compiles the source	gof clinker compile cmodule location	0.038462
diagonalsubtensor and	tensor nnet get diagonal subtensor view x i0	0.083333
the __unify_walk__ method	unify walk a b u	0.037037
in profiling to print the mflops	op flops	0.125000
cross-entropy between an approximating distribution and a	nnet categorical crossentropy coding_dist true_dist	0.111111
a constant representing a value on a certain	array constant	0.500000
first outdim-1 dimension size s of x	tensor flatten x ndim outdim	0.333333
graph of apply nodes according to a	gof sort apply nodes inputs outputs cmps	0.050000
to wrt, computes gradients of	subgraph	0.047619
replace all subtensor(make_vector) like [a b c][0]	subtensor make vector	1.000000
arguments to pass	args	0.025641
thunk fn	fn	0.083333
a normal	normal	0.222222
of tensortype	tensor type	0.034483
optionally inserting	tensor py operators dimshuffle	0.019231
register r's shape in	shape	0.010204
extract list of variables	gof variables	0.125000
reshape	reshape node	1.000000
folowing changes in the graph t	tensor local mul switch sink	0.045455
six moves urllib namespace that resembles	six	0.025000
a list of the parents	parents	0.100000
a gradient	grad fun	0.500000
input specs and the output specs	compile std fgraph input_specs output_specs accept_inplace	0.142857
that unroll the batch	tensor nnet gen conv code unroll batch	0.166667
the output	op get output	0.047619
code when doing constant folding	constant folding	0.142857
of useless reshape	tensor local useless reshape	0.200000
more multinomial distributions defined by one-dimensional slices in	multinomial random_state	0.040000
the outer product of	sparse block outer	0.047619
last access of a given	last access	0.040000
respects the conventions imposed by python and	theslice length	0.052632
it that way will probably not activate envs	compat maybe add	0.200000
attempting to use dnn conv workmem	dnn workmem workmem	0.166667
change the value after the import of theano	param init default	0.040000
that unroll	unroll	0.111111
toposort return an ordering of the graph's apply	function graph toposort	0.125000
to help the navigator deal with	gof navigator optimizer	0.038462
to the fgraph this is the place	fgraph	0.012195
ops in the list remove are still	replacements remove reason	0.055556
file into an aboslute path	gof cop get path cls f	0.166667
type	type	0.261905
see max for the maximum in	tensor maximum x	0.142857
the "reverse-mode" gradient for the eigensystem of	tensor eigh grad perform node inputs outputs	0.333333
bitwise a ^ b	tensor xor a b	1.000000
gradients along the axis that was used	grad inputs g_outputs	0.076923
unify	unify	0.750000
is primarily used	pure op r op inputs eval_points	0.125000
pairs that will be turned into macros for	cop get	0.033333
subtraction (inplace on a)	sub inplace a	1.000000
sample from one or more	random_state size n	0.250000
of	tensor tensor type	0.041667
this computes the outer product of two sets	sparse block outer	0.047619
by a broadcasted dense vector element wise	svcsr	0.090909
directories that are needed by one or	dirs	0.071429
return a new variable instance of type self	pure type make variable name	1.000000
revert the replacement if	replace validate replace all	0.050000
decorator to merge multiplication by a	alpha merge cls alpha_in beta_in	0.200000
attempting to use dnn	safe no dnn	0.125000
the function name to	gpuarray	0.046512
converting to type2 from type1	type1 type2	0.166667
dimshuffle	dim shuffle	0.500000
the matrix inverse on gpu	gpu matrix inverse a	0.200000
spatio-temporal filters with	signals filters	0.111111
that	gof get	0.300000
header for openblas threads interface	tensor openblas threads text	0.250000
is	gcc	0.023810
and dictionary data structures	push out non seq	0.125000
dict op -> total number of	compile profile stats class	0.250000
dimensions of x default reverse them	tensor transpose x axes	0.200000
new graph	with new inputs inputs strict	0.166667
the same on all device	device	0.076923
initializes py_name to	r name	0.250000
of apply nodes according to a list of	apply nodes inputs outputs cmps	0.050000
get the 0 based level of the list	list type get depth	1.000000
evaluate because of aliasing and	destroy	0.009709
convop that unroll the batch size	conv code unroll batch kern d unroll_bsize unroll_ksize	0.125000
broadcast them to match	tensor generate broadcasting	0.066667
mflops	conv op flops inputs	0.125000
is a mrg stream state	mrg random streams	0.033333
diagonalsubtensor	get diagonal subtensor view x i0 i1	0.083333
an alloc	alloc	0.025000
uses the topooptimizer from	in2out	0.043478
direction	direction	1.000000
is not attempting to use dnn conv workmem	dnn workmem workmem	0.166667
symbolic row variable	tensor row	0.050000
outputs	outputs	0.454545
apply_node recursively search from	import apply_node check	0.066667
nodename	nodename	1.000000
instances of suitable dummy values	optimizer provide inputs	0.200000
a tensorvariable of	make variable	0.166667
like zeros_like but forces the	core float zeros like x	0.200000
interface to manipulate the subgraph in functiongraph	function graph replace r new_r reason verbose	0.250000
followed by a modulo	modulo	0.250000
cache or	gof module cache	0.083333
return c code to extract a	c extract name	0.250000
symbolic integer scalar for the shape element	tensor shape feature unpack	0.500000
context by mapping it to a name	context name ctx	0.500000
respect to wrt, computes	subgraph	0.047619
function uses set and dictionary data structures	out non	0.125000
to numpy random	random	0.055556
:func images2neibs <theano tensor nnet neighbours images2neibs>	nnet images2neibs ten4 neib_shape neib_step mode	0.333333
the output dimensions of	nnet conv op get output	0.047619
from a uniform	tensor uniform	0.125000
op that copies a vector to	alloc	0.012500
merge 2 dicts by adding the values	gof merge dict d1 d2	0.333333
specified pieces of vectors	sparse block outer make node	0.066667
for a convolution with the	gpu dnn conv	0.200000
an array with more	node inputs outputs	0.125000
inputs and	inputs	0.025316
similar behaviour as haskell' foldr	foldr fn sequences outputs_info non_sequences	1.000000
variables given input shapes	variables	0.043478
perform the permutation by doing a	perform	0.058824
g_pt	g_pt	0.833333
factors	factors	1.000000
output of scan return true iff the	push out scan	0.050000
helper function for diagonalsubtensor and incdiagonalsubtensor	nnet get diagonal subtensor view x	0.083333
function for diagonalsubtensor	nnet get diagonal subtensor view x i0	0.083333
equivalent	graph to gpulocal	0.055556
to represent the dependence of nodes in a	make dependence	0.043478
return a tuple of integers indicating the version	cache version apply node	0.125000
this to expm1	tensor local expm1 node	0.066667
attempt to replace a leaf	replace leaf arg leaves new_leaves op	0.250000
assign the shape	tensor shape feature set shape	0.333333
the source code for this linker	gof clinker compile cmodule location	0.038462
integers indicating the version	c code cache version apply	0.125000
this variable can take any value	free variable	1.000000
a badviewmap exception when it detects the	compile check viewmap node storage_map	0.111111
important note this function	scan process node fgraph node	0.142857
given that v	v	0.011111
see max for the maximum in one	tensor maximum x y	0.090909
the type's :attr	gpu	0.011765
from the loaded	get	0.020833
by the corresponding element of a dense vector	s	0.071429
optimization makes the folowing changes in	mul switch sink	0.045455
return a list of shape	tensor shape feature default infer shape	0.066667
index array and a set of arrays to	a choices out mode	0.111111
sample from one or more multinomial	multinomial random_state size n	0.333333
sgn	sgn	0.625000
variable on	as gpuarray variable	0.166667
specified factor takes as input a n-d tensor	signal pool 2d input ws ignore_border stride	0.100000
helpful function that gets a scan op	op not_required	0.071429
convert python	tensor make	0.076923
->	local	0.098592
to compute the kernel shape of convolution gradweights	tensor nnet get conv gradweights shape	0.500000
apply instance in	apply clone with	0.333333
a specified factor takes as	tensor signal pool 2d	0.142857
between i and o nodes via dfs traversal	i o	0.041667
task	find task	0.142857
the type's :attr context_name	array	0.041667
function_graph	function_graph	0.625000
constants and the rest	rest inputs elemwise only_process_constants	0.125000
for small or builtin c types	gof clinker type c is simple	0.250000
the idx	idx	0.076923
signature object for comparing tensorconstant	constant signature	0.100000
b axis=l) -> sum(a axis={ }) / b	tensor local sum prod div dimshuffle	0.333333
declare	declare	0.714286
within the op code	get op	0.100000
be turned into macros for use within the	cop	0.028571
should be saved under	misc persistent ndarray id resolve	0.333333
compiles the source code for this linker and	gof clinker compile cmodule location	0.038462
a graph of apply nodes according	gof sort apply nodes inputs outputs cmps	0.050000
x and	local	0.014085
b axis=l) -> sum(a axis={ }) / b	local sum prod div dimshuffle node	0.333333
determine the broadcast pattern for	adv index broadcastable pattern	0.066667
reproducible case for problems	compile function dump filename inputs outputs	0.166667
the finite fourier	fourier	0.125000
failure_callback	optimizer warn inplace exc	0.500000
respect to wrt, computes gradients	subgraph grad	0.062500
exception while annotating	op node thunk exc_info storage_map	0.250000
c code to initialize the	clinker type c	0.083333
dimensions of x default reverse them	transpose x	0.200000
graph either breadth- or depth-first	start expand mode build_inv	0.333333
form that	form	0.111111
a new random stream	random streams	0.058824
dimensions of this variable	tensor py operators	0.015625
reorder the dimensions of this variable optionally inserting	operators dimshuffle	0.019231
1/(1+exp x -> sigm -x	nnet local inv 1 plus exp	0.333333
dimensions of this variable optionally	dimshuffle	0.014493
to pack c types back into a	gof clinker type c sync	0.111111
hash equal for same kinds of tensortype	tensor tensor type hash	0.166667
a batched tensordot product	batched tensordot x y axes	0.333333
output of scan return true iff	scan_module push out scan	0.050000
return a module from the cache	gof module cache module from	0.333333
unknown variables and apply_nodes to this graph	graph import	0.125000
profiling to print the mflops	flops	0.076923
task	task	0.500000
for	tensor	0.009646
g++	gcc	0.023810
time icluding the time for parents	times	0.100000
create a six moves urllib namespace that resembles	module six	0.043478
by functiongraph attach_feature the method that	gof	0.002381
for diagonalsubtensor and	tensor nnet get diagonal subtensor view	0.083333
this is the equivalent	graph	0.016393
exception	view	0.022727
construct a variable with	variable	0.022222
reorder the dimensions of	tensor tensor py	0.015873
of this op could be very easy	op	0.009174
the g++ version used is the	gof	0.002381
along an axis	axis	0.025641
to the idx	idx	0.076923
the passed-in key is found in the cache	cache get from key key key_data	0.333333
a ==	tensor eq	0.333333
kinds of tensortype	tensor tensor	0.014286
badviewmap exception when it detects the following	compile check viewmap node storage_map	0.111111
destroyhandler class detects when a graph	handler	0.071429
elementwise [floor] division inverse of multiplication	int div a b	0.333333
unary	unary	1.000000
compiled module from the loaded cache	cache get module	0.166667
the other implementation of mod	mod c code node	0.125000
calls subprocess_popen returning the output error and	misc output	0.066667
choose values from a with or without replacement	tensor random streams base choice size a replace	0.333333
type's	type	0.011905
is not attempting to use dnn	dnn	0.060606
neural-net classifiers	with bias	0.166667
a variable with the -x pattern	tensor nnet is neg var	0.166667
:attr context_name	gpuarray gpu	0.045455
to the type's :attr	gpuarray	0.023256
c contiguous version	contiguous	0.058824
an input that wasn't in the destroy_map	bad	0.013158
n (n needs to	n	0.055556
the convolution gradient with respect	gpu dnn conv grad	0.125000
l has any duplicates (according to __eq__)	scan_module has duplicates l	0.111111
row correspond to the one	to one	0.125000
some perform() or c_code() created a	bad view map	0.142857
this variable optionally inserting	tensor tensor	0.014286
data	data	0.727273
the matrix	matrix	0.055556
same rounding than numpy round half	round half	0.100000
merge	merge	0.571429
the apply to be inserted in the	apply node	0.031250
a previously	mpirecv	0.037037
return connection pattern of subfgraph	connection pattern node	0.076923
some functiongraph listeners to help the navigator	gof navigator optimizer attach updater fgraph	0.038462
compare true iff other is the	tensor type eq other	0.250000
reshapes the output after pad_dims	gpuarray unpad dims output input leftdims	0.333333
apply as many times as	tensor apply	0.142857
of the type optionally with a new dtype	tensor type clone dtype	0.333333
only if this enum has this alias	gof enum type has alias alias	0.333333
power (inplace on a)	tensor pow inplace a	1.000000
