all	tensor applies logical values specified along es axis
code	composed represents behavior struct certain support c++ complex computation things cleanup declare unit
aliased	aliased memory
scalar	function opt raised used constant softsign called activation clients get_scalar_constant_value stability scalar internal something speed class numerical helps
gc	python execution program start-to-finish unconditional
rop	implements r-operator operation downsample
query	---------- parameters
disk	load operation disk array
row	dim elements tensor inner-most permute row
inverse	inverse matrix :math magma library computes using
graph	specifies function set debugging variables bound functiongraph query ie event based transfer localoptgroup optimizers instead local method node inputs life optimizer graphtogpu subgraph outputs equivalent lists theano gpu flag's retrieves represents input transfering list value calling creates record graph output whole equilibriumoptimizer op
equivalence	detect tabs problems feature tries keeps functiongraph
to	instead transfering query even based rounding transfer localoptgroup node start retrieves input local method round function optimizer graphtogpu equivalent gpu wave half rolling gpus flag's data optimizers numpy list value calling graph implement whole equilibriumoptimizer
grad3d	gradient respect conv3d w
batched	product variables two computes batched dot
stochastic	identical exception optimizations give graph results repeated
norm	abstract batch cudnn base normalization op
division	'//' integers raised someone divide '/' tries instead
choice	multinomial uniform sample without converts samples replacement
updates	sharedvariable dict-like keys mapping values new
dim	broadcastable insert tensor allows remove dimshuffle gpu reorder dimensions
list	:class **inherit unifies special parameters - subclass add inner basic matched orvariable typed operators variable class enumtype from**: kind list values ----------
vector	concatenate scalars vector number together
indices	b matrix could csc convolve build sparse indices implement
round	function even c fct rounding round() algo half implement numpy round
cosh	+ / cosh 2 exp (exp x = -x
zero	c fct rounding round() algo implement
even	function even rounding half implement numpy round
sub	inputs replaces takes output replacing certain update application occurrences another pattern input @todo op
diag	given square return matrix copies diagonal vector allocates diagonals specified empty op
sum	tensor sums values specified along es axis
uniform	transposed compared multinomial uniform sample without converts samples output multinomialworeplacementfromuniform replacement
current	debugging optimizer
importer	submodules six importer meta import path moves
shared	persisting defaults zip certain functions value uses names file gpu variable shared representing appears
method	defined interface raised part functions
mem	optimizer consumption scan graph reduces memory
sinh	sinh - / 2 exp (exp x = -x
path	submodules six importer meta import path moves
change	use variables value manager decorator context theano config change
wait	received previously mpi using operation array sent wait
items	lazy loading moved objects
poisson	return sparse density random values poisson input mean
cenum	from**: - **inherit enumlist :class
fourier	use fft tensor supported ffts warning theano officially
maker	function debugging create class instances functionmaker special
shuffle	broadcastable insert tensor allows remove dimshuffle gpu reorder dimensions
convolution	b matrix could csc convolve build sparse indices implement
dtype	reduces scalar specified along operation es axis
sdcsc	dense matrix element vector sparse multiplication wise broadcasted
logical	comparison fixed value
subtensor1	function capability use vector compute increments index integers list advancedsubrensor1 avail 2 ilist advancedincsubtensor1 using slicing gpu implement subtensor x[ilist] advanced
names	preserve variables optimization names
apply	node represents producing instance graph :term application input variable apply nodes expression op
sdcsr	dense matrix element vector sparse multiplication wise broadcasted
eye	gpu eye
dnn	operations convolution descriptor cudnn pulls respect use parameters pooling gradient softmaxgrad libraries forward inputs handle base normalization builds batch creates headers weights softmax ---------- op
handler	operations functions destructive optimizer graph two 1) important 'requirement' performs evaluate destroyhandler impossible detects aliasing class
memory	aliased memory
type	default code protocol help don't true storage random f(*pt) colors / filters threshold expressions giving self.subsample[1] whose loading struct parameters graph :class csm.evaluate pickling complex dict clipped dtype input nodes main image indicating first certain accepts return around != represents python compiled unlimited bound b every variables difference python's weights string name from**: determines like adjacent -- tolerance list scalar remove "forward" contexts convenience randomstate -1 2d sparse batch x identifier grad kernels/filters upper smaller differencing. generation ops backpropagate built-in nulltype relative see result combining images fields 'full', pixel index scale tensor operation opaque gradient axis specification apply subclass filename current 'valid' 3 absolute max print sorrounding encounters method minimal lazy full run 'complex32' dictionary kernels zipped behaviour shape objects output interface configuration pixels tensordot c self.subsample[0] actual generator getting specify? taking persists allows ids times thing inputs base highlights attributes passing gemm changes puretype numerical op load tensortype raised convolution file's kernel color describes format symbolic number replace instances output. names respect array diff iterable toposort plot use graphs takes working area always create truncated two wrapper substitutions width returns intermediate call types way use. var ('str' numpy (i.e. store inner function generating zip passed tuple outputs flags forward clinkertype func gpu last details numpy.ndarrays representing must applied none save defines correlate unlike treat whether type inside computes value worth considered values float32 sum useable subclasses flatten program fgraph. want scan pool gpuarraytype pass receptive argument sample **inherit existing file handle 1 id option respective recursive etc generic convenient means - feature archive ndarrays maps setting arguments env test debugmode finite config node map differentiable used generates keys put object containing callable running intent printed variable pickle enumlist data class ndarray dimensions 2 lower undefined calls stepsize whenever maybe uses string) persistent inherit height provide depth write u expression fundamental ---------- theano clinkerop (default order bottom
until	function different used return scan things needs inner encode class
sort	sort function wrapper argsort numpy class
composite	operations c code takes produces composite graph scalar whole op
flags	use variables value manager decorator context theano config change
train	abstract batch normalization op
canonizer	used local_optimizer simplification tool variable best
hints	optimizer matrix track hintsfeature feature serves fgraph add functiongraph properties
none	generic c code working inherit
join	concatenate join gpu along multiple tensorvariables axis
mm	gpucorrmm_gradinputs gpucorrmm_gradweights corrmm_gradweights gpucorr3dmm_gradweights filters corr3dmm, corr3dmm matrix implementation wrt corr3dmm_gradweights gpucorr3dmm, gpu inputs gpucorrmm, gpucorr3dmm_gradinputs gpucorr3dmm base multiplication using class corrmm, corrmm_gradinputs gradient correlation corr3dmm_gradinputs corrmm cpu gpucorrmm
det	input determinant square matrix
erfinv	implements function inverse gpu error
didnt	thrown node exception replace_all_validate_remove remove optimization variable wanted
history	functiongraph history changes keep
scan	operations scan parses global non-sequences performed sequences run manipulate different end consumption graph ops inplace outside inner easy memory variables inputs optimizer format outputs pushing ---------- depend constants parameters inside optimization merges push makes reduces
tag	abbreviated like tags giving objects class
eigvalsh	eigensystem gradient positive definite hermitian generalized eigenvalues
csm	speficied matrix parameter indexing used part construct sparse data
six	submodules moved urllib_robotparser meta moves loading urllib_error create six namespace importer urllib 3 urllib_parse import lazy python objects path urllib_request urllib_response resembles
feature	fgraph variables functiongraph provided adds matrix graph feature class merged function optimizer track removing cannot properties calls shape() together base extensions keeps
csc	non-zero x elements like gradient * calculated structured except csc performs wrt sparse alpha propagated z expression + dot matrix
pure	instances specification :term variable interface operation type op
tile	pattern reps according construct x input array repeating
if	used graph cvm/vm conditional linkers provides evaluation op
pinv	:math computes pseudo-inverse matrix
csr	non-zero product elements like calculating dot(x, gradient optimized structured except matrix wrt sparse t) operand z = dot csr
profile	operations information execution profiling compilation object optimization theano's memory runtime store
map	exception view_map created wasn't modified destroy_map perform() alias c_code() memory input
max	implements given calculate downsample max argmax axes pooling version r-operator gpu implement axis grad operation maxandargmax
data	information used cache key store
response	lazy loading six moved objects urllib_response moves
alloc	square allocates shape initialized given tensor create initial copies vector allocate initializing gpu empty uninitialized memory diagonal desired matrix alloc value without implement cpu op
ss	used sparses two superclass comparisons matrices
corr3d	inputs corr3dmm_gradinputs matrix using gpucorr3dmm_gradinputs implementation gpucorr3dmm wrt cpu corr3dmm gradient gpucorr3dmm_gradweights base gpucorr3dmm, filters correlation multiplication gpu corr3dmm_gradweights class corr3dmm,
searchsorted	numpy wrapper searchsorted
generic	generic represents object python
block	composed version results pieces computes computation specified check unit sparseblockgemv outer sparseblockgemv's two cleanup gpu : product full returning docstring matrices updating matrix represents vectors see behavior sets sparseblockouter sparseblockouter's declare dot op
order	identical exception optimizations give graph results repeated
sd	used dense matrix add superclass sparse comparisons
sigmoid	opt stability speed
fft	fourier transform fast
y0	case compute log * 0 x = special
outer	product full outer matrix : vectors docstring computes two see version sets sparseblockouter gpu pieces sparseblockouter's results updating
monitor	function easily monitormode step mode debug execution
ln	function log gamma
argmax1hot	neural-net output l{op} crossentropysoftmaxargmax1hotwithbias compound gpu implement classifiers special
rnnblock	use rnn implementation object us cudnn v5 allow
non	pushing optimizer scan variables global non-sequences depend inside
indexing	raised perform indexing subtensor asked advanced
half	function even c fct rounding round() algo half implement numpy round
not	accessible raised via constant scalar defined forbidden functions value called field amount part values take not_options variable finite interface get_scalar_constant_value something
merge	identical different optimizer ops track graph variables scan parts together fgraph cannot cond merged merges redundant keeps
softmax	neural-net compound :math \varphi(\mathbf{x})_j cudnn special logsoftmax gradient softmaxgrad l{op} multiclass wrt gpu = function activation crossentropysoftmaxargmax1hotwithbias softmaxwithbias output classifiers softmax x implement op
dot22	dot22 gpu matrix-matrix compute product
inputs	inputs corr3dmm gradient gpucorr3dmm abstractconv wrt abstractconv2d corrmm abstractconv3d gpucorrmm
contiguous	c return contiguous else c-contiguous see version nothing input array check
numeric	function compute point scalar-valued numeric particular derivative
meta	node meta-optimizers set localoptimizers submodules six one replace try importer meta executes choose base import path moves class fastest
softmax1hot	crossentropysoftmax1hotwithbiasdx gradient crossentropysoftmaxargmax1hotwithbias wrt x gpu implement op
extract	matrix diagonal return diagonals specified
array	represents constant shared type value computation certain variable gpu array representing
out	operations scan variables global non-sequences performed sequences run depend graph inplace outside inner output_types_preference float32 int32 pushing optimizer unary_out_lookup({int8 dictionary get object end constants inside optimization push passing makes complex128})
container	variable value class joins computed
matrix	inverse matrix :math pseudo-inverse magma library computes using
gradient	gradient incorrect raised calculated error
inplace	run scan parametrise graph make work inplace elemwise gpuelemwise optimizer makes op
qrincomplete	qr decomposition incomplete
dot22scalar	matrix-matrix compute product
typed	parameters typed operators list subclass ---------- add basic variable class
print	debugging optimizer effect identity-like print side op
log2	2 base log
args	inputs scan outputs parses format easy manipulate
diagonal	return form gradient nd diagonal diagonalsubtensor subtensor
free	variable take value
base	operations gpucorrmm_gradweights cudnn gpucorr3dmm_gradweights gpucorrmm_gradinputs pulls need gpucorrmm, corr3dmm, kernels parameters corrmm_gradweights libraries corr3dmm_gradweights combine gpucorr3dmm, creates handle abstractconv gpucorr3dmm_gradinputs base class corrmm, cop corrmm_gradinputs compile headers gpukernelbase softmax corr3dmm_gradinputs op
proxy	wrap proxy existing
subtensor	advancedsubtensor return increments using gradient nd indexing diagonal diagonalsubtensor subtensor form increment incsubtensor gpu implement copy advanced view
w	gradient respect weights convolution
signature	comparing object instances tensorconstant signature
support	struct certain support c++ complex things
singleton	convenient subclass base attributes type class
already	attach identical raised already callback attempting feature on_attach feature's functiongraph method functionally
symbolic	function represents use symbolic output input functionmaker
modulo	product followed efficient numerically implementation stable modulo operation dot
navigator	abstract class
open	using openmp code inherit op
structured	non-zero elements dense like gradient calculated structured except csc addition wrt vector sparse propagated csr dot matrix
params	tensortype gpuarraytype one effectively wrap check struct convenient create internal method hash python safe objects class types hashable like many etc values theano
gpu	operations code crossentropysoftmax1hotwithbiasdx :math cudnn scalar gpucorrmm_gradinputs using sparseblockgemv inverse cholesky gemv parameters implementation graph wrt gpucorrmm_gradweights input nodes elemwise gemm uninitialized ger return reuse python crossentropysoftmaxargmax1hotwithbias wave softmaxwithbias gpus represents solver capability softmax dot22 grad gpucorrmm cusolver contiguous library magma initialized operation matrix join gradient version allocate shared minimal advancedsubtensor gpucorrmm, complementary gpucorr3dmm advancedsubrensor1 sparseblockouter base along c transfering subtensor pooling implement maxandargmax whole op convolution computes variables variable respect array kernels use eye transfer filters avail start reduction 2 gpucorr3dmm, memory forward gpu type function reshape gpucareducecuda gpucorr3dmm_gradinputs passing incsubtensor representing images2neibs compute builds contexts batch value compile descriptor error r-operator sparseblockouter's average constant certain gpucorr3dmm_gradweights rolling need check sparseblockgemv's downsample softmaxgrad split instead gpuarray node inputs used see max docstring multiplication computation dimshuffle data class normalization dimensions implements svd weights advancedincsubtensor1 correlation ---------- careduce cpu
ordered	frozendict sharedvariable set dict-like keys maintains mapping subclass order elements values key new remembers added
cdata	represents c around opaque intent passed data
gpudb	optimizer list value calling method retrieves query based flag's local equilibriumoptimizer optimizers
specify	shape graph l{op} user-provided puts
double	double tensor element
rebroadcast	broadcastable fields input's predetermined way change
expm	square compute matrix exponential gradient array
and	given calculate argmax max axes version gpu axis maxandargmax
linker	control debugging performlinker useful fgraph linker vm l{linker} thunk l{locallinker}s associated satisfies special given perform graph l{op} l{linker}s factory subclass parallel method acting offers basic nodes several clinker node run base interface using class runs calls l{functiongraph} keep easier writeme l{linker makes order
null	raised grad allows nulltype values type encounters
any	tensor applies bitwise values specified along es axis
downsample	downsample max implement grad gpu
destroy	operations destroy_map impossible functions detects graph two performs input aliasing optimizer evaluate 'requirement' perform() destroyhandler important c_code() destructive class exception wasn't modified 1)
optimizer	function node meta-optimizers redundant scan fgraph add abstract l{optimizer} inplace instances set executes graph localoptimizers apply gpuelemwise run remove sequentially provided one adds takes topooptimizer transfer make point hintsfeature transform topological start inserting parts choose input local gemm equilibrium identical node-based optimizations optimizer wave tries base fastest gpu rolling feature replace parametrise class operations applied reverse elemwise work list fusion l{functiongraph} try shapefeature writeme merges applies makes order serves op
object	elements standard used type clinker op
unused	function input symbolic passed needed
inconsistency	thrown exception listeners invalid state functiongraph graph's
average	pooling implement grad average gpu
request	lazy loading urllib_request six moved objects moves
ext	function put c dynamicmodule
mrg	numpy random component module interface similar
dot	followed variables numerically except computes operand modulo operation non-zero matrix gradient graph create structured two csc wrt batched stable propagated = csr function product elements dot(x, calculating object optimized implementation matrices like pydot calculated efficient sparse t) theano z dot
random	object draws numbers numpy random component module randomstate wrapper type interface similar op
cholesky	square return cusolver positive triangular matrix semi-definite x gpu op root cholesky
cache	compiled dynamically cache modules interface disk
dict	frozendict maintains key subclass order
factor	downsample max implement grad gpu
local	localoptimizer set associated localoptgroup global one thunk meta-optimizers executes localoptimizers useful optimizer node-based takes graph l{linker}s generate choose instead nodes local node optimizations run base fastest replace class type list keep try writeme applies
equilibrium	applied optimizations point reached order set arbitrary apply potential equilibrium
ger	ger matrix blas vectors update rank-1 general scalar x alpha gpu y' <- + defines
db	applied instead optimizations potential topooptimizer sequence type global localoptgroup generate proxy set optimizer arbitrary wrap reached existing local order equilibrium
dx	crossentropysoftmax1hotwithbiasdx gradient crossentropysoftmaxargmax1hotwithbias wrt x gpu implement op
compiler	generic function offer meta compiler
chi2sf	compute chi2 'survival (1 - (chi2 chi2_cdf x ie pvalue function')
cached	thrown exception constant cached put functiongraph
dev20	function capability use compute avail 2 advancedincsubtensor1 gpu implement
remove	corresponding outputs transferring removes applications input op
bad	view_map destroy_map inconsistent substitute different alias take memory input gives outputs perform() c_code() variable exception created wasn't modified calling twice values runtime op
x	case compute log * 0 x = special
fixed	comparison fixed value
grad	raised square convolution point numeric nulltype cudnn hermitian filters respect array derivative corr3dmm matrix gradient softmaxgrad definite abstractconv wrt generalized scalar-valued encounters abstractconv3d function inputs x eigensystem max gpucorr3dmm gpu downsample particular corrmm compute exponential average pooling weights softmax positive abstractconv2d eigenvalues implement gpucorrmm grad op
view	exception used internally created wasn't memory inplace perform() alias returns c_code() theano input view_map view
kernel	operations kernel kernels cop gpukernelbase together compile base combine groups attributes need gpu class
set	set added remembers elements order
gpuachoice	output compared multinomialworeplacementfromuniform transposed
seq	sequentially pushing optimizer takes scan variables global list non-sequences l{optimizer} instances sequences depend applies inside constants
argmax	given calculate argmax max axes version gpu axis maxandargmax
j0	function kind 0 bessel order first
j1	function kind 1 bessel order first
module	six dynamically moved urllib_robotparser moves loading urllib_error create cache namespace urllib disk 3 urllib_parse lazy python compiled objects interface urllib_request modules urllib_response resembles
arg	function numpy class wrapper argsort
stats	operations information execution profiling compilation object optimization theano's memory runtime store
tensor	tensor raised constant symbolic instances wrapper tensorvariable utilization create able subclass comparing add tensorinv() basic type function linalg as_tensor_variable operators object variable representing class ndarray tensorconstant numpy value signature theano isn't tensorsolve
pattern	replaces pattern update occurrences output input @todo
away	c fct rounding round() algo implement
transp3d	implements (conv3d matrix defined implicitly w multiplication "transpose" conv3d
erfcx	implements function exp(x**2)*erfc complementary large stable numerically scaled way error x
urllib	lazy loading urllib_error urllib_request namespace create six moved urllib_robotparser urllib python 3 objects urllib_response resembles urllib_parse moves
multinomial	succes return matrix probability density multinomial random number n sample experiment values sparse converts uniform samples
state	numpy random type randomstate wrapper
mul	dense matrix element vector sparse multiplication wise broadcasted
formatter	function pydot graph create object theano
preserve	preserve variables tag optimization names attributes
missing	available c code compute missing outputs symbolic raised try needed g++ error input generate
jv	function real kind v bessel order first
key	information used cache writeme key store
group	localoptimizer node optimizer takes localoptgroup equivalent global list type applies instead local generate graphtogpu
cop	c implementation external allow class op
softplus	stability numerical helps
streams	numpy random component module interface similar
context	used contexts passing type nodes minimal
attributes	preserve attributes variables tag optimization
conv2d	inputs convolution gradient abstract abstractconv2d wrt filters forward op
load	load zip arrays pickling persisted file operation disk numpy array
dot1	run scan graph inplace optimizer makes
sampling	product dot(x, calculating optimized t) operand z = dot
constant	raised constant certain dimensional variable scalar something functiongraph get_scalar_const_value tensorconstant comparing subclass field add basic gpu get_scalar_constant_value typed operators object cannot put representing class tensor thrown exception instances cached changed list zero value :term signature runtime called
gxx	available c code raised try g++ error generate
robotparser	lazy loading six moved urllib_robotparser objects moves
raise	whose exception perform() raises op
py	function c code pydot generation graph create object wraps sympy's operator theano
gemm16	float16 using nervena gemm kernels
crossentropy	neural-net crossentropysoftmax1hotwithbiasdx compound coding entropy special gradient l{op} cross classifiers 0 wrt gpu form crossentropysoftmaxargmax1hotwithbias output true implement [0 compute x distribution op
numpy	used python cast ints arrays numpy class floats
abstract	inputs convolution parameters gradient abstract batch abstractconv abstractconv2d wrt base filters forward abstractconv3d class normalization op
empty	uninitialized raised constant something initializing alloc cpu dimensional zero without allocate memory gpu implement called get_scalar_const_value
ccode	c code dense matrix generation wraps sympy's add sparse operator
else	used graph cvm/vm conditional linkers provides evaluation op
assert	implements graph computational assertion
disconnected	c compute disconnected gradient grad taking raised result x variable respect type asked indicating input
tanh	tanh + (exp(2*x) - / cosh sinh 1) x =
frozen	frozendict maintains key subclass order
batch	abstract batch cudnn base normalization op
value	exception value inconsistent output type op
optimization	exception different values take variable runtime substitute
eig	eigenvectors right compute square eigenvalues array
error	aliased shape inconsistent loading struct internal input create '//' someone as_tensor_variable indexing callbacks cannot advanced replace_all_validate_remove cached scalar remove output grad isn't computed nulltype zero c++ operation gradient state encounters lazy missing objects put wanted thrown integers example subtensor destroyhandler asked op raised divide symbolic unsupported dimensional respect tensorvariable needed support indicate way memory type get_scalar_constant_value function passed outputs exception compute disconnected '/' value listeners optimization error problem called something constant certain moved numbers functiongraph moves pruning get_scalar_const_value incorrect urllib_error generic things six able invalid complex instead node used tries variable changing graph's calls calculated perform theano
ultra	opt stability speed
loop	python execution program start-to-finish unconditional
opt	localoptimizer node takes localoptgroup equivalent list applies graphtogpu
conv	operations convolution dual defined abstract taught filters respect use matrix parameters gradient abstractconv wrt forward conv3d inputs implicitly processing base purpose multiplication class implements (conv3d builds vanilla serves 2d descriptor weights w implement "transpose" signal op
mpisend	remote previously send mpi host using operation asynchronously array sent wait
xlog	case compute log * 0 x = special
vm	factory linker vm class interface acting program theano __call__ evaluates satisfies object's method
iv	function real kind modified v bessel order first
parse	lazy loading six moved objects urllib_parse moves
bias	x neural-net crossentropysoftmax1hotwithbiasdx gradient l{op} compound crossentropysoftmaxargmax1hotwithbias multiclass softmaxwithbias wrt output gpu implement classifiers special op
cond	different optimizer ops graph cond merges
dual	runs performlinker fgraph clinker using parallel
in	function represents kind orvariable symbolic list use values inner unifies variable input functionmaker special matched
svcsr	dense matrix addition structured element vector sparse multiplication wise broadcasted
id	persisting saving zip object ndarrays uses names file persist variable shared
gpulocal	equivalent localoptgroup graphtogpu
perform	given calls perform l{op} subclass l{functiongraph} l{linker} l{linker basic order method
reshape	shp x perform reshape variables new shape gpu input operation
make	concatenate scalars vector number together
complex	raised used unsupported complex numbers operation
split	tensorvariable partition split gpu along axis
clinker	code fgraph make_thunk used loops use make_function make returns types type clinker arguments elements ops python compiled standard uses interface compiles definition c creates callables individual clinkerop subclasses specification comprise op
i1	function kind modified 1 bessel order first
i0	function kind modified 0 bessel order first
sym	c code generation wraps sympy's operator
topo	node generate topooptimizer reverse type global topological one tries optimizer apply local order
stack	evalution finish-to-start order thunks
i	inputs return convolution gradient l{op} shape respect matrix
solve	cusolver solver numpy utilization system linear solve linalg theano gpu tensorsolve equations op
grid	'meshgrid' equally dense n-dimensional create points spaced
protocol	node raised calls invalid callbacks way changing destroyhandler functiongraph example pruning
nd	'meshgrid' equally dense n-dimensional create points spaced
unary	int32 unary_out_lookup({int8 dictionary get object output_types_preference passing complex128}) float32
shape	shape() raised return matrix puts fgraph graph l{op} calls add serves feature shape cannot user-provided removing optimizer remove shapefeature computed
inference	abstract batch cudnn base normalization op
permute	dim elements tensor inner-most permute row
gemv	pieces computes specified check sparseblockgemv sparseblockgemv's + * version gpu : product returning docstring beta alpha matrices gemv vectors x expression dot op
generator	abbreviated like tags giving objects class
hint	provide information optimizer arbitrary
qrfull	full decomposition qr
param	deprecated use instead
add	function provided functions vector optimizer sparse addition 'requirement' structured two feature destroyhandler add important performs graph 1) adds dense matrix
input	raised symbolic respect use start gradient transfer needed passed gpu function missing outputs wave input rolling represents compute disconnected asked grad functionmaker
cum	---------- parameters
save	optimizer consumption scan graph reduces memory
gemm	inserting operations optimizer version graph accumulation multiplication base dot22 gpu matrix-matrix in-place class gemm
real	real coordinate complex extract number
psi	function derivative log gamma
cthunk	implementation thunk c
fusion	operations graph fusion optimizer elemwise
unification	represents group values variables possible unification tangible class
integer	'//' integers raised someone divide '/' tries instead
unique	wraps implemented gpu unique numpy op
advanced	raised ilist using subtensor use perform avail 2 gpu function advancedsubtensor return index advancedsubrensor1 indexing slicing increments copy x[ilist] advanced integers compute list capability vector advancedincsubtensor1 implement asked
desc	operations use convolution pooling descriptor op builds
arange	containing interval spaced create within given values array evenly
t	generic c code working inherit
sparse	results pieces computes specified check sparseblockgemv outer sparseblockgemv's create two version way gpu : node product full sparse returning docstring matrices updating matrix vectors see fundamental sets sparseblockouter sparseblockouter's dot op
output	operations scan symbolic performed use end graph outside inner gives function used outputs theano inconsistent represents exception internally calling twice optimization push output functionmaker op
replacement	thrown node exception replace_all_validate_remove remove optimization variable wanted
cusolver	gpu op cusolver solver
sequence	optimizations potential sequence
softsign	function activation softsign
magma	inverse matrix :math svd computes library magma using
flatten	flatten tensor
pickler	strips unnecessary subclass objects theano attributes pickler
nonzero	indices non-zero elements return
dense	+ * performs x alpha z expression
autocaster	used python cast ints arrays numpy class floats
unlocker	even around exits lock interrupted mechanism released program wrapper release class automatically crashing
tracker	detect tabs problems feature tries keeps functiongraph
prod	tensor values multiplies specified along es axis
conv3d	inputs minibatch multiple convolution gradient abstract 3d wrt filters forward abstractconv3d op
mode	raised monitormode computation occur detect generic graph indicate internal way nans evaluation function detects represents compiled compilation step errors link execution optimize easily exception makes mode error debug theano problem infs automatically
enum	from**: enumlist :class - **inherit main subclasses enumtype
debug	exception errors generic raised indicate internal mode theano problem detects evaluation
host	transfer data cpu gpu
binomial	succes return matrix probability density experiment random number n binomial values sparse
corr	inputs gpucorrmm, matrix using implementation gpucorrmm_gradweights filters cpu corrmm_gradinputs corrmm_gradweights gradient wrt base gpucorrmm_gradinputs correlation multiplication gpu corrmm corrmm, class gpucorrmm
from	load variables sample converts operation disk array compared rounding transfer uniform build samples basic gpu multinomialworeplacementfromuniform function inputs fct around multinomial outputs lists output data transposed c round() creates without writeme algo theano implement replacement cpu op
comparison	comparison used sparses matrix two value dense superclass sparse comparisons fixed matrices
mpirecv	received remote receive mpi previously host using operation asynchronously array wait
cuda	dimensions gpucareducecuda reduction scalar along op
or	accessible via could list value field values variable finite options
gamma	function log gamma
op	sort replacing dual processing fgraph certain variables two individual superclass another loops implement operation searchsorted convolution basic sparses tensor parameters purpose numpy inplace wrapper pureop application returns build outputs input type clinker argsort function inputs used corresponding around ops python compiled transferring removes bundle applications uses lists comparisons interface dense matrices class takes matrix definition replaces internally double vanilla serves element creates :term convenience taught writeme 2d sparse ---------- theano clinkerop view subclasses signal comprise op
float	temporarily adjust autocasting behavior
bound	accessible via bound value field variable
elemwise	generalizes make work elemwise scalar gpuelemwise gpu parametrise tensors op
guard	function used internally compiled compilation detect mode error theano nans infs makes occur automatically op
strip	strips unnecessary subclass objects theano attributes pickler
wrap	control run easier thunk l{locallinker}s offers parallel several makes class
log	function logsoftmax e log :math activation \varphi(\mathbf{x})_j base =
mpop	using openmp code inherit op
there	attach identical raised already callback attempting feature on_attach feature's functiongraph method functionally
fast	opt stability speed
ndarray	load pickling saving zip arrays object ndarrays persisted file persist numpy
log10	10 base log
function	specifies debugging set variables random bound instances numbers functiongraph ie event returned functions draws create build basic input type function life optimizer around subgraph object theano put class represents c numpy dynamicmodule record randomstate writeme output functionmaker op
autocast	temporarily adjust autocasting behavior
related	dot22 base class gemm
removed	thrown node exception replace_all_validate_remove remove optimization variable wanted
with	x neural-net crossentropysoftmax1hotwithbiasdx gradient l{op} compound crossentropysoftmaxargmax1hotwithbias multiclass softmaxwithbias wrt output gpu implement classifiers special op
images2neibs	tensor pooling example 2d reshapes input gpu images2neibs row
wise	ops fgraph python individual uses loops clinker comprise
default	default input takes value x
un	fgraph shapefeature optimizer feature remove
erfcinv	function gpu inverse error complementary
log1p	1+x log
cgpu	cop gpukernelbase combine class
supervisor	sure overwrites variables listener protected functiongraph operation makes events contents
defined	defined interface raised part functions
inv	function inverse reciprocal utilization wrapper class also tensorinv() multiplicative theano numpy called
moved	lazy loading moved objects
thunk	exception outputs calling twice inconsistent op gives
as	raised temporarily tensorvariable as_tensor_variable create able adjust behavior autocasting isn't
moves	lazy loading urllib_error urllib_request namespace create six moved urllib_robotparser urllib python 3 objects urllib_response resembles urllib_parse moves
inc	function capability use compute increments using index gradient indexing list diagonalsubtensor avail 2 advancedincsubtensor1 increment gpu slicing incsubtensor implement subtensor advanced
nan	function detect compiled compilation mode error theano nans infs makes occur automatically
invalid	exception value inconsistent output type op
event	record life event functiongraph
validator	variables expressed without invalid using check
lookup	int32 unary_out_lookup({int8 dictionary get object output_types_preference passing complex128}) float32
node	node theano graph
elements	dim elements tensor inner-most permute row
metadict	writeme
usmm	+ * performs x alpha z expression
variable	accessible preserve via file tabs certain variables bound tag names computation functiongraph attributes special functions detect serves zip graph feature subclass appears field add take not_options gpu finite operators forbidden inner node orvariable typed unifies basic problems kind tries base unification purpose shared variable representing class matched represents persisting could list value :term amount optimization values tensor defaults keeps uses expression options
pool	operations implements use patches builds parameters descriptor gradient max average different sum pooling downsample r-operator ---------- gpu implement grad operation op
frozendict	implements :py complete dictionaries mapping wrapper class collections immutable around
categorical1hot	[0 compute form coding cross 0 entropy distribution true
svd	matrix parameters :math svd computes library magma ---------- using
persistent	load persisting pickling saving zip arrays object ndarrays persisted uses names file persist variable shared numpy
cpy	python careduce code reuse gpuarray
eigh	eigenvectors eigensystem return matrix gradient symmetric hermitian eigenvalues
weights	corr3dmm gradient gpucorr3dmm abstractconv wrt filters abstractconv2d corrmm abstractconv3d gpucorrmm
push	operations pushing optimizer scan graph variables global non-sequences inplace end outside optimization run performed sequences push depend makes inside constants inner
careduce	es code operation dimensions gpucareducecuda python reduces reduce reduction reuse scalar commutative gpuarray specified along axis = careduce associative op
cpu	return contiguous else c-contiguous see nothing input array check
