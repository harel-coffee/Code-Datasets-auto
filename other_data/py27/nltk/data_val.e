return a string containing a pretty-printed representation of the given verbnet class's member verbs
create freqdist of trigrams within text
convert input sentences into syntactic trees
compute the extraneous material in the hypothesis
:rtype str :return a string representation of this probdist
recursively visit subexpressions apply 'function' to each
add a single edge to the chartview
calculate the update values for the classifier weights for this iteration of iis
return a list of panlex language varieties
a demonstration of how to read a string representation of a conll format dependency tree
:see expression findtype()
:param label_probdist p label the probability distribution over labels
generates likely collocations and their log-likelihood
return a list of languages supported by multilingual wordnet
return an iterator that generates this feature structure and each feature structure it contains
return true if this function is run within idle tkinter
print a string representation of this tree to 'stream'
change the width of this space widget
extracts a subset of the attributes from the given element and returns them in a dictionary
:type chunk_struct tree
display a column that has been hidden using hide_column()
this method can modify a tree in three ways 1
a function that calculates the modified dunning log-likelihood ratio scores for abbreviation candidates
return the offsets of the tokens in *s*, as a sequence of start end tuples by splitting the string at each occurrence of *sep*
:rtype str :return a string representation of this probdist
:param prover the theorem tool to execute with the assumptions
return true iff every term in 'self' is a term in 'other'
configure all table cells in the given column valid keyword
:param type_check bool should type checking be performed? to their types
remove any current highlighting and mark the given item
extract selected fields from a file of line-separated json tweets and write to a file in csv format
find a possible base form for the given form with the given part of speech by checking wordnet's list of exceptional
remove length three and length two suffixes in this order
pretty print a list of text tokens breaking lines on whitespace
return a sentence-tokenized copy of *text*, using nltk's recommended sentence tokenizer
add a binding to each tkinter label widget in this
return a string representation of this decision tree that expresses the decisions it makes as a nested set of pseudocode
if 'self' is bound by 'bindings', return the atomic to which it is bound
track keywords from the public streaming api and send output to terminal
add blank lines before all elements and subelements specified in blank_before
sort the given queue of edge objects placing the edge that should be tried first at the beginning of the queue
:return the given file s as a single string
scores ngrams using a variant of mutual information the keyword
create a new bracket widget
return the sample with the greatest probability if two or
if delete_on_gc was set to true when this picklecorpusview was created then delete the corpus view's
if this base value unifies with other, then return the unified value
load an sequence of annotation results appending to any data already loaded
this module parses the tri-tuple format that repp outputs using the "--format triple" option and returns an generator with tuple of string
deregister a callback function if func is none then
observed agreement between two coders on all items
temporarily duplicated from nltk sem util
return a string representation of this conditionalfreqdist
open a standard format marker string for sequential reading
:type graphs list dependencygraph
true if the token's first character is uppercase
use stanfordparser to parse a sentence takes a sentence as a string
count understemming index ui overstemming index oi and stemming weight sw
calculates values of a trigram contingency table or cube from marginal values
create a new context-free grammar from the given start state and set of probabilisticproductions
extract the contents of the zip file filename into the directory root
return the chunks which were included in the guessed chunk structures but not in the correct chunk structures listed in input order
return a new treeedge formed from the given production
stem an english word and return the stemmed form
convert a file of prolog clauses into a list of concept objects
a binding is consistent with the dict if its variable is not already bound or if its variable is already bound to its argument
create a new parenthasis widget
do some tree drawing tests
given a sentence or fe annotation set construct the width-limited string showing an ascii visualization of the sentence's annotations calling either
return the overall tag-based accuracy for all text that have been scored by this chunkscore, using the iob conll2000
return the total number of sample values (or "bins") that have counts greater than zero
given a sequence of tokens generates lists of tokens each list corresponding to a sentence
:see expression visit()
safe floating point division function does not allow division by 0
load language mappings between codes and description from table txt
create a new recursivedescentparser, that uses grammar to parse texts
replace any bound aliased vars with their binding and replace any unbound aliased vars with their representative var
:see expression findtype()
:return an iterator that generates parse trees for the sentence
helper function for pretty-printing a lexical unit
add a new edge to the chart using a pointer to the previous edge
use the laplace estimate to create a probability distribution for the experiment used to generate freqdist
calculate the r frontier where we must switch from nr to sr when estimating e[nr]
return the number of tokens in the corpus file underlying this corpus view
return the first-order logic formula tree for this underspecified representation using the plugging given
starting from each untranslated word find the longest
load information from the given 'sentence' element each
collapse/expand a tree
return the directory to which packages will be downloaded by default
probability of target sentence and an alignment given the source sentence
attempt to unify this clause with the other returning a list of resulting unified clauses
:param filename a name of a file in malt-tab format
construct a template for generating rules
derives parameters from a given training text or uses the parameters given
builds a lambda function representing a predicate on a tree node depending on the name of its node
:return the given file s as a list of sentences or utterances each encoded as a list of word strings
implements the "n-notation" used in artstein and poesio 2007
the observed disagreement for the weighted kappa coefficient
:return a string describing the value of the joint-feature whose index in the generated feature vectors is fid
:return the given file s as a list of sentences each sentence
:return the tree position of the index-th leaf in this tree
extract holes labels formula fragments and constraints from the hole semantics underspecified representation usr
add a sentence to the current discourse
print out the readings for the discourse or a single sentence
@return the given token's tag
return a verbose string representation of the probabilisticdependencygrammar
:return the text displayed by this text widget
load all synsets with a given lemma and part of speech tag
load a layer from an annotation set
this function returns the position of the first instance of the ngram appearing in a sentence
return a list of document identifiers for all documents in this corpus or for the documents with the given file s if
create a new corpus view based on the file fileid, and read with block_reader
replaces suffix of word with replacement
convert a string representation of a feature structure as displayed by repr into a featstruct
:return the list of category labels used by this classifier
return a html page for the given word
:return the parser's stack
return a chunk structure for a single sentence encoded in the given conll 2000 style string
use bllip to parse a sentence takes a sentence as a list of
:rtype tree or none
return an elementtree containing the xml for the specified verbnet class
extract selected fields from a file of line-separated json tweets and write to a file in csv format
return the total number of sample outcomes that have been recorded by this conditionalfreqdist
returns the full tweet objects as specified by twitter documentation on tweets
:see expression free()
return the labeled attachment score las and unlabeled attachment score uas
:see nltk featstruct rename_variables()
removes candidate ngrams w1 w2 where fn w1 w2
perform the actual proof store the result to prevent unnecessary
builds a lambda function representing a segmented pattern
scores bigrams using fisher's exact test pedersen 1996 less
grow the window if necessary
this is a factory method that instantiates and returns a subtype of drtabstractvariableexpression appropriate for the given variable
return a verbose string representation of the dependencyproduction
create a new sequence widget
return a pretty-printed string representation of a given edge in this chart
start parsing a given text this sets the parser's stack to
this method facilitates movement through the terms of 'other'
:param challenge version of the rte challenge (i e rte1 rte2 or rte3)
builds a lambda function representing a predicate on a tree node from a parenthetical notation
stem a norwegian word and return the stemmed form
:param features list of feature string which is needed to convert to binary features
call the boxer binary with the given input
:param command theoremtoolcommand to decorate
apply this rule to the given chunkstring see the
use nltk's currently recommended part of speech tagger to tag the given list of tokens
return a 'cookie' containing information about which row is selected and what color configurations have been applied
helper function for pretty-printing an annotated sentence from a full-text document
convert a logic expression to prover9 format
prints just the pos/neg scores for now
return the start symbol of the grammar
return a set of individual constants non-predicates
:return a pretty-printed string representation of this tree
allow equality between instances of abstractvariableexpression subtypes
print a concordance for word with the specified context window
returns an approximate significance level between two lists of independently generated test values
close a binary relation in the concept's extension set
:see readingcommand combine_readings()
scores ngrams by their frequency
check that interpret_sents() is compatible with legacy grammars that use a lowercase 'sem' feature
given a string containing a list of symbol names return a list of nonterminals constructed from those symbols
:param algorithm the algorithm option of this parser currently support arc-standard and arc-eager algorithm
count ui oi when stemming is done by truncating words at 'cutlength'
extracts the chunks in a bio chunk-tagged sentence
call the tadm binary with the given arguments
return all words and punctuation symbols in the corpus or in the specified file s
:see expression predicates()
construct a new unchunkrule
return a named statistic collected during training or a dictionary of all
return the feature structure that is obtained by replacing each variable bound by bindings with its binding
return all sentences in the corpus or in the specified file s
returns the spearman correlation coefficient for two rankings which should be dicts or sequences of key rank
:return list of xml descriptions for rolesets
score the accuracy of the chunker against the gold standard
construct a new uniform probability distribution that assigns equal probability to each sample in samples
:param ex abstractboxerdrs
from iddo lev's phd dissertation p108-109
change the symbol that is displayed by this symbol widget
basic method for tokenizing input into sentences
construct a new expandrightrule
:return a verbose string representation of this regexpparser
return a list of all samples that have nonzero probabilities
return the feature structure that is obtained by replacing any of this feature structure's variables that are in vars
tokenize multiple sentences using repp
the type with its final period removed if it has one
update *test_sents* by applying *rule* everywhere where its conditions are met
:return a new featuretreeedge formed from this edge
attempt to make an application expression the next tokens are
return a list concatenating self with itself count times
generate the entities from the model's domain that satisfy an open formula
template expand and feature expand are class methods facilitating
return a list of the verbnet class identifiers if a file
create a more pretty-printable version of the assignment
check validity of a credentials file
:param label the most likely label for tokens that reach this node in the decision tree
return the corpora in their raw form
:param beam_threshold hypotheses that score less than this factor of the best hypothesis are discarded from the stack
return a probabilistic pcfg corresponding to the input string s
read a string from the given stream that does not contain any un-closed tags
open the file stream associated with this corpus view this
:return the hash value of this dependencyspan
:return the given file s as a list of sentences or utterances each encoded as a list of word
:return the given file s as a list of sentences each encoded as a list of word tag tuples
given two numbers logx = *log x * and logy = *log y *, return *log x+y *
construct a new chart the chart is initialized with the
return 18 templates from the original nltk demo in multi-feature syntax
probability that position j in trg_sentence is aligned to
check whether a set represents a relation of any arity
returns a list of file identifiers for the fileids that make up this corpus
>>> t = tree fromstring("(s (np d the n dog (vp v chased (np d the n cat )))")
:return the current scroll region for the canvas managed by this canvasframe
create the training example in the libsvm format and write it to the input_file
the root of this tree i e the unique ancestor of this tree
helper function for __init__: add a new stage to the parser
:see expression free()
display help information summarizing the main methods
a demonstration of the probabilistic parsers the user is
merges nodes at given indices in the dendrogram the nodes will be
return the index of the first occurrence of value in this list that is greater than or equal to start and less than
return the treesegmentwidget for the specified subtree
check if we've moved to a new line if we have then remove
return a concise string representation of the probabilisticdependencygrammar
mark an edge
override counter setdefault() to invalidate the cached n
add a list of background assumptions for reasoning about the discourse
print a table showing the effect of each of the features in the given feature set and how they combine to determine the
true if left is a leftcorner of cat where left can be a terminal or a nonterminal
returns a list of synonyms for the current ngram
cohen 1960 averages naively over kappas for each coder pair
:return a list of the "known labels" -- i e all labels
this method attempts to unify two terms two expressions are unifiable
return the element object wrapped by this wrapper
:return the given file s as a list of words and punctuation symbols
import the module now
get the y-coordinate of the point that a figure should start at if its height is 'item_height' and it needs to be centered in an area that
returns true if the pair of tokens may form a collocation given log-likelihood statistics
as nodes are collapsed into others they are replaced by the new node in the graph but it's still necessary
add a binding to all nodes
builds a pyparsing-based parser object for tokenizing and interpreting tgrep search strings
sample the most probable alignments from the entire alignment space
:return number of vacant slots up to and including position
return all lemma names for all synsets for the given part of speech tag and language or languages
set the feature weight vector for this classifier
return the proof string
create a new feature dictionary with the specified features
use stanfordparser to parse multiple sentences takes multiple sentences as a
get the details for the specified frame using the frame's name
construct and return new feature encoding based on a given training corpus train_toks
abbreviate an ne class name
tag a list of sentences nb before using this function user should specify the mode_file either by
train a new hiddenmarkovmodeltagger using the given labeled and unlabeled training instances
return a string representation of this probdist
probability of target sentence and an alignment given the
uses the frame index which is much faster than looking up each frame definition if only the names and ids are needed
creates a new nonprojectivedependencyparser
return the region r1 that is used by the hungarian stemmer
:return a list of the items contained by this list
helper for build_index(): yield a list of tuples (pkg_xml zf subdir), where
:param rtepair a rtepair from which features should be extracted
since function is an implication return its consequent there should be
:param depgraphs the list of test sentence each sentence is represented as a dependency graph where the 'head' information is dummy
infer and check types raise exceptions if necessary
basic example of sentiment classification using liu and hu opinion lexicon
return the set of all words that the given category can start with
map tokens into a feature representation implemented as a {hashable int} dict
apply self span_tokenize() to each element of strings i e :
calculates the spearman's rho correlation coefficient given the *worder* list of word alignment from word_rank_alignment(), using the formula
:param freqdist the trigram frequency distribution upon which to base
return a new nonterminal whose symbol is a/b, where a is the symbol for this nonterminal and b is the symbol for rhs
return a string representation of this freqdist
a helper function for trees
:see expression _set_type()
reads one paragraph at a time
return a short description of the purpose and/or effect of this rule
retract assumptions from the assumption list
expand the first element of the frontier in particular if
create the semantic types index
:see expression visit()
helper function for pretty-printing any attrdict object
obtain details for lexical units
finds the clusters using the given set of vectors
:see readingcommand parse_to_readings()
apply the given transformation to the string encoding of this chunkstring
stemming a word token using the isri stemmer
play the given audio sample
call the megam binary with the given arguments
find the semantic representation at the root of a tree
:see expression _set_type()
obtain a list of frame relation types
:return the given file s as a single string
create a new path pointer for the given absolute path
initialize the indexes _lemma_to_class, _wordnet_to_class, and _class_to_fileid by scanning
remove twitter username handles from text
find the language with the min distance
remove a callback that was registered with bind_click
return a dot representation suitable for using with graphviz
discard rules with low accuracy this may hurt performance a bit
return uncurried arg-list
add a new function to extract features from a document this function will
construct and return a new feature structure if this
if i is a valid column index integer then return it as is
:see nltk featstruct find_variables()
dot representation of the aligned sentence
get the zero-based index of the last alphabetic character in this string
add the truth-in-a-model value to each semantic representation for each syntactic parse of each input sentences
obtain a list of semantic types
convert an element into an appropriate value for inclusion in the view
append _neg suffix to words that appear in the scope between a negation and a punctuation mark
:return the cmudict lexicon as a dictionary whose keys are lowercase words and whose values are lists of pronunciations
writes a file with context for each erroneous word after tagging testing data
:return the length of the longest hypernym path from this synset to the root
return the list of frequency distributions that this probdist is based on
:param fileids a list or regexp specifying the fileids that have to be returned as a raw string
finds bigram collocations in the files of the webtext corpus
iterate over all synsets with a given part of speech tag
given a short verbnet class identifier (eg '37 10'), map it
return a string representation for this nonterminal
register a callback function with the list this function
:return an iterator of the parses that have been found by this parser so far
given the stdout output generated by tadm when training a model return a numpy array containing the corresponding weight
probability of target sentence and an alignment given the
direct data to sys stdout
returns the number of right children under the node specified by the given address
return the overall recall for all texts that have been scored by this chunkscore
return a set of all the free non-bound variables this includes
a demonstration of the shift-reduce parser
replace the child oldchild with newchild
calculate the levenshtein edit-distance between two strings
return the aligned sentence pair reversing the directionality
train a new conditionalexponentialclassifier, using the given training samples using the external megam library
use the expected likelihood estimate to create a probability distribution for the experiment used to generate freqdist
construct a new probability distribution from the given dictionary which maps values to probabilities (or to log
add a line reviewline to the review
this method exists to be overridden
distributional similarity find other words which appear in the same contexts as the specified word list most similar words first
:param text a string optionally tokenized containing a comparation
adjust the view such that the given item is visible if
higher-order function to test presence of a given label
set a new url for the data server if we're unable to contact
decorator for demo functions
determines the approximate scores for translating every subsequence in src_sentence
:param root the root directory for this corpus
return the node with the given address
implements step 5a from "an algorithm for suffix stripping"
the index of this tree in its parent i e
returns all possible ngrams generated from a sequence of items as an iterator
:param alg_option the algorithm option of this parser currently support arc-standard and arc-eager algorithm
read a sequence of tokens from a stream where tokens begin with lines that match start_re
:type resource_name str or unicode
return the forward probability matrix a t by n array of log-probabilities where t is the length of the sequence and n is the
return a string representation of this freqdist
remove a sentence from the current discourse
:return the child widget contained by this container widget
allows memory use to be reduced after much training by removing data about rare tokens that are unlikely to have a statistical effect with
helper for _index()
get a list of lowest synset s that both synsets have as a hypernym
return the ngrams generated from a sequence of items as an iterator
ending step word of length six
