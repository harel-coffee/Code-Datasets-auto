core	LazyModule	__init__	name locals globals	create a lazymodule instance wrapping module name
core	LazyModule	__lazymodule_import		import the module now
core	LazyModule	__getattr__	name	import the module on demand and get the attribute
core	LazyModule	__setattr__	name value	import the module on demand and set the attribute
core	TreePrettyPrinter	nodecoords	tree sentence highlight	produce coordinates of nodes on a grid
core	TreePrettyPrinter	text	nodedist unicodelines html ansi	:return ascii art for a discontinuous tree
core	TreePrettyPrinter	svg	nodecolor leafcolor funccolor	:return svg representation of a tree
core		test		do some tree drawing tests
core	Downloader	status	info_or_id download_dir	return a constant describing the status of the given package or collection
core	Downloader	update	quiet prefix	re-download any packages whose status is stale
core	Downloader	_update_index	url	a helper function that ensures that self _index is
core	Downloader	index		return the xml index describing the packages available from the data server
core	Downloader	info	id	return the package or collection record for the given item
core	Downloader	xmlinfo	id	return the xml info record for the given item
core	Downloader	_get_url		the url for the data server's index file
core	Downloader	_set_url	url	set a new url for the data server if we're unable to contact
core	Downloader	default_download_dir		return the directory to which packages will be downloaded by default
core	Downloader	_get_download_dir		the default directory to which packages will be downloaded
core	DownloaderGUI	_package_to_columns	pkg	given a package return a list of values describing that package one for each column in self
core		md5_hexdigest	file	calculate and return the md5 checksum for a given file
core		unzip	filename root verbose	extract the contents of the zip file filename into the directory root
core		build_index	root base_url	create a new data xml index file by combining the xml description
core		_indent_xml	xml prefix	helper for build_index(): given an xml elementtree, modify it and its descendents text and tail attributes to generate
core		_check_package	pkg_xml zipfilename zf	helper for build_index(): perform some checks to make sure that the given package is consistent
core		_svn_revision	filename	helper for build_index(): calculate the subversion revision number for a given file (by using subprocess to run svn)
core		_find_collections	root	helper for build_index(): yield a list of elementtree element
core		_find_packages	root	helper for build_index(): yield a list of tuples (pkg_xml zf subdir), where
core		config_java	bin options verbose	configure nltk's java interface by letting nltk know where it can find the java binary and what extra options if any should be
core		java	cmd classpath stdin stdout	execute the given java command by opening a subprocess that calls java
core		read_str	s start_position	if a python string literal begins at the specified position in the given string then return a tuple (val end_position)
core		read_int	s start_position	if an integer begins at the specified position in the given string then return a tuple (val end_position) containing the
core		read_number	s start_position	if an integer or float begins at the specified position in the given string then return a tuple (val end_position)
core		overridden	method	:return true if method overrides some method with the same name in a base class
core		_mro	cls	return the method resolution order for cls -- i e a list
core		_add_epytext_field	obj field message	add an epytext @field to a given object's docstring
core		deprecated	message	a decorator used to mark functions as deprecated this will cause
core		find_file_iter	filename env_vars searchpath file_names	search for a file to be used by nltk
core		find_binary_iter	name path_to_bin env_vars searchpath	search for a file to be used by nltk
core		find_jar_iter	name_pattern path_to_jar env_vars searchpath	search for a jar that is used by nltk
core		_decode_stdoutdata	stdoutdata	convert data read from stdout/stderr to unicode
core		import_from_stdlib	module	when python is run from within the nltk/ directory tree the current directory is included at the beginning of the search path
core	ElementWrapper	__new__	cls etree	create and return a wrapper around a given element object
core	ElementWrapper	__init__	etree	initialize a new element wrapper for etree
core	ElementWrapper	unwrap		return the element object wrapped by this wrapper
core	ElementWrapper	__str__		:return the result of applying elementtree tostring() to
core		slice_bounds	sequence slice_obj allow_step	given a slice return the corresponding start stop bounds taking into account none indices and negative indices
core		collapse_unary	tree collapsePOS collapseRoot joinChar	collapse subtrees with a single child ie unary productions
core		demo		a demonstration showing how each tree transform can be used
core		in_idle		return true if this function is run within idle tkinter
core		pr	data start end	pretty print a sequence of data items
core		print_string	s width	pretty print a string breaking lines on whitespace
core		tokenwrap	tokens separator width	pretty print a list of text tokens breaking lines on whitespace
core		re_show	regexp string left right	return a string with markers surrounding the matched substrings
core		breadth_first	tree children maxdepth	traverse the nodes of a tree in breadth-first order
core		guess_encoding	data	given a byte string attempt to decode it
core		transitive_closure	graph reflexive	calculate the transitive closure of a directed graph optionally the reflexive transitive closure
core		invert_graph	graph	inverts a directed graph
core		flatten		flatten a list
core		pad_sequence	sequence n pad_left pad_right	returns a padded sequence of items before ngram extraction
core		ngrams	sequence n pad_left pad_right	return the ngrams generated from a sequence of items as an iterator
core		bigrams	sequence	return the bigrams generated from a sequence of items as an iterator
core		trigrams	sequence	return the trigrams generated from a sequence of items as an iterator
core		everygrams	sequence min_len max_len	returns all possible ngrams generated from a sequence of items as an iterator
core		skipgrams	sequence n k	returns all possible skipgrams generated from a sequence of items as an iterator
core		binary_search_file	file key cache cacheDepth	return the line from the file with first word key
core		set_proxy	proxy user password	set the http proxy for python to download through
core		elementtree_indent	elem level	recursive function to indent an elementtree _elementinterface
core		choose	n k	this function is a fast way to calculate binomial coefficients commonly known as nck i
core		split_resource_url	resource_url	splits a resource url into "<protocol>:<path>"
core		normalize_resource_url	resource_url	normalizes a resource url >>> windows = sys
core		normalize_resource_name	resource_name allow_relative relative_path	:type resource_name str or unicode
core	PathPointer	open	encoding	return a seekable read-only stream that can be used to read the contents of the file identified by this path pointer
core	PathPointer	file_size		return the size of the file pointed to by this path pointer in bytes
core	PathPointer	join	fileid	return a new path pointer formed by starting at the path identified by this pointer and then following the relative
core	FileSystemPathPointer	__init__	_path	create a new path pointer for the given absolute path
core	FileSystemPathPointer	path		the absolute path identified by this path pointer
core	BufferedGzipFile	__init__	filename mode compresslevel fileobj	return a buffered gzip file object
core	BufferedGzipFile	write	data size	:param data bytes to write to file or buffer
core	ZipFilePathPointer	__init__	zipfile entry	create a new path pointer pointing at the specified entry in the given zipfile
core	ZipFilePathPointer	zipfile		the zipfile zipfile object used to access the zip file
core	ZipFilePathPointer	entry		the name of the file within zipfile that this path pointer points to
core		find	resource_name paths	find the given resource by searching through the directories and zip files in paths where a none or empty string specifies an absolute path
core		retrieve	resource_url filename verbose	copy the given resource to a local file if no filename is
core		load	resource_url format cache verbose	load a given resource from the nltk data package the following
core		show_cfg	resource_url escape	write out a grammar file ignoring escaped and empty lines
core		clear_cache		remove all objects from the resource cache
core		_open	resource_url	helper function that returns an open file object for a resource given its resource url
core	OpenOnDemandZipFile	write		:raise notimplementederror openondemandzipfile is read-only
core	OpenOnDemandZipFile	writestr		:raise notimplementederror openondemandzipfile is read-only
core	SeekableUnicodeStreamReader	read	size	read up to size bytes decode them using this reader's encoding and return the resulting unicode string
core	SeekableUnicodeStreamReader	readline	size	read a line of text decode it using this reader's encoding and return the resulting unicode string
core	SeekableUnicodeStreamReader	readlines	sizehint keepends	read this file's contents decode them using this reader's encoding and return it as a list of unicode lines
core	SeekableUnicodeStreamReader	next		return the next decoded line from the underlying stream
core	SeekableUnicodeStreamReader	closed		true if the underlying stream is closed
core	SeekableUnicodeStreamReader	name		the name of the underlying stream
core	SeekableUnicodeStreamReader	mode		the mode of the underlying stream
core	SeekableUnicodeStreamReader	close		close the underlying stream
core	SeekableUnicodeStreamReader	seek	offset whence	move the stream to a new file position if the reader is
core	SeekableUnicodeStreamReader	char_seek_forward	offset	move the read pointer forward by offset characters
core	SeekableUnicodeStreamReader	_char_seek_forward	offset est_bytes	move the file position forward by offset characters ignoring all buffers
core	SeekableUnicodeStreamReader	tell		return the current file position on the underlying byte stream
core	SeekableUnicodeStreamReader	_read	size	read up to size bytes from the underlying stream decode them using this reader's encoding and return the resulting
core	SeekableUnicodeStreamReader	_incr_decode	bytes	decode the given byte string into a unicode string using this reader's encoding
core	Nonterminal	__init__	symbol	construct a new non-terminal from the given symbol
core	Nonterminal	symbol		return the node value corresponding to this nonterminal
core	Nonterminal	__eq__	other	return true if this non-terminal is equal to other in
core	Nonterminal	__repr__		return a string representation for this nonterminal
core	Nonterminal	__str__		return a string representation for this nonterminal
core	Nonterminal	__div__	rhs	return a new nonterminal whose symbol is a/b, where a is the symbol for this nonterminal and b is the symbol for rhs
core	Nonterminal	__truediv__	rhs	return a new nonterminal whose symbol is a/b, where a is the symbol for this nonterminal and b is the symbol for rhs
core		nonterminals	symbols	given a string containing a list of symbol names return a list of nonterminals constructed from those symbols
core		is_nonterminal	item	:return true if the item is a nonterminal
core		is_terminal	item	return true if the item is a terminal which currently is if it is hashable and not a nonterminal
core	Production	__init__	lhs rhs	construct a new production
core	Production	lhs		return the left-hand side of this production
core	Production	rhs		return the right-hand side of this production
core	Production	__len__		return the length of the right-hand side
core	Production	is_nonlexical		return true if the right-hand side only contains nonterminals
core	Production	is_lexical		return true if the right-hand contain at least one terminal token
core	Production	__str__		return a verbose string representation of the production
core	Production	__repr__		return a concise string representation of the production
core	Production	__eq__	other	return true if this production is equal to other
core	Production	__hash__		return a hash value for the production
core	DependencyProduction	__str__		return a verbose string representation of the dependencyproduction
core	ProbabilisticProduction	__init__	lhs rhs	construct a new probabilisticproduction
core	CFG	__init__	start productions calculate_leftcorners	create a new context-free grammar from the given start state and set of productions
core	CFG	fromstring	cls input encoding	return the cfg corresponding to the input string s
core	CFG	start		return the start symbol of the grammar
core	CFG	productions	lhs rhs empty	return the grammar productions filtered by the left-hand side or the first item in the right-hand side
core	CFG	leftcorners	cat	return the set of all nonterminals that the given nonterminal can start with including itself
core	CFG	is_leftcorner	cat left	true if left is a leftcorner of cat where left can be a terminal or a nonterminal
core	CFG	leftcorner_parents	cat	return the set of all nonterminals for which the given category is a left corner
core	CFG	check_coverage	tokens	check whether the grammar rules cover the given list of tokens
core	CFG	_calculate_grammar_forms		pre-calculate of which form s the grammar is
core	CFG	is_lexical		return true if all productions are lexicalised
core	CFG	is_nonlexical		return true if all lexical rules are "preterminals", that is unary rules which can be separated in a preprocessing step
core	CFG	min_len		return the right-hand side length of the shortest grammar production
core	CFG	max_len		return the right-hand side length of the longest grammar production
core	CFG	is_nonempty		return true if there are no empty productions
core	CFG	is_binarised		return true if all productions are at most binary
core	CFG	is_flexible_chomsky_normal_form		return true if all productions are of the forms a -> b c a -> b or a -> "s"
core	CFG	is_chomsky_normal_form		return true if the grammar is of chomsky normal form i e all productions
core	FeatureGrammar	__init__	start productions	create a new feature-based grammar from the given start state and set of productions
core	FeatureGrammar	fromstring	cls input features logic_parser	return a feature structure based featuregrammar
core	FeatureGrammar	productions	lhs rhs empty	return the grammar productions filtered by the left-hand side or the first item in the right-hand side
core	FeatureGrammar	leftcorners	cat	return the set of all words that the given category can start with
core	FeatureGrammar	leftcorner_parents	cat	return the set of all categories for which the given category is a left corner
core	FeatureGrammar	_get_type_if_possible	item	helper function which returns the type feature of the item,
core	DependencyGrammar	__init__	productions	create a new dependency grammar from the set of productions
core	DependencyGrammar	contains	head mod	:param head a head word
core	DependencyGrammar	__contains__	head mod	return true if this dependencygrammar contains a dependencyproduction mapping 'head' to 'mod'
core	DependencyGrammar	__str__		return a verbose string representation of the dependencygrammar
core	DependencyGrammar	__repr__		return a concise string representation of the dependencygrammar
core	ProbabilisticDependencyGrammar	contains	head mod	return true if this dependencygrammar contains a dependencyproduction mapping 'head' to 'mod'
core	ProbabilisticDependencyGrammar	__str__		return a verbose string representation of the probabilisticdependencygrammar
core	ProbabilisticDependencyGrammar	__repr__		return a concise string representation of the probabilisticdependencygrammar
core	PCFG	__init__	start productions calculate_leftcorners	create a new context-free grammar from the given start state and set of probabilisticproductions
core	PCFG	fromstring	cls input encoding	return a probabilistic pcfg corresponding to the input string s
core		induce_pcfg	start productions	induce a pcfg grammar from a list of productions
core		_read_cfg_production	input	return a list of context-free productions
core		_read_pcfg_production	input	return a list of pcfg probabilisticproductions
core		_read_fcfg_production	input fstruct_reader	return a list of feature-based productions
core		_read_production	line nonterm_parser probabilistic	parse a grammar rule given as a string and return a list of productions
core		read_grammar	input nonterm_parser probabilistic encoding	return a pair consisting of a starting category and a list of productions
core		cfg_demo		a demonstration showing how cfgs can be created and used
core		pcfg_demo		a demonstration showing how a pcfg can be created and used
core		dg_demo		a demonstration showing the creation and inspection of a dependencygrammar
core		sdg_demo		a demonstration of how to read a string representation of a conll format dependency tree
core	AbstractCollocationFinder	_build_new_documents	cls documents window_size pad_left	pad the document with the place holder according to the window_size
core	AbstractCollocationFinder	from_documents	cls documents	constructs a collocation finder given a collection of documents each of which is a list or iterable of tokens
core	AbstractCollocationFinder	_apply_filter	fn	generic filter removes ngrams from the frequency distribution if the function returns true when passed an ngram tuple
core	AbstractCollocationFinder	apply_freq_filter	min_freq	removes candidate ngrams which have frequency less than min_freq
core	AbstractCollocationFinder	apply_ngram_filter	fn	removes candidate ngrams w1 w2 where fn w1 w2
core	AbstractCollocationFinder	apply_word_filter	fn	removes candidate ngrams w1 w2 where any of (fn w1 fn w2
core	AbstractCollocationFinder	_score_ngrams	score_fn	generates of ngram score pairs as determined by the scoring function provided
core	AbstractCollocationFinder	score_ngrams	score_fn	returns a sequence of ngram score pairs ordered from highest to lowest score as determined by the scoring function provided
core	AbstractCollocationFinder	nbest	score_fn n	returns the top n ngrams when scored by the given function
core	AbstractCollocationFinder	above_score	score_fn min_score	returns a sequence of ngrams ordered by decreasing score whose scores each exceed the given minimum score
core	BigramCollocationFinder	__init__	word_fd bigram_fd window_size	construct a bigramcollocationfinder given freqdists for appearances of words and possibly non-contiguous bigrams
core	BigramCollocationFinder	from_words	cls words window_size	construct a bigramcollocationfinder for all bigrams in the given sequence
core	BigramCollocationFinder	score_ngram	score_fn w1 w2	returns the score for a given bigram using the given scoring function
core	TrigramCollocationFinder	__init__	word_fd bigram_fd wildcard_fd trigram_fd	construct a trigramcollocationfinder given freqdists for appearances of words bigrams two words with any word between them
core	TrigramCollocationFinder	from_words	cls words window_size	construct a trigramcollocationfinder for all trigrams in the given sequence
core	TrigramCollocationFinder	bigram_finder		constructs a bigram collocation finder with the bigram and unigram data from this finder
core	TrigramCollocationFinder	score_ngram	score_fn w1 w2 w3	returns the score for a given trigram using the given scoring function
core	QuadgramCollocationFinder	__init__	word_fd quadgram_fd ii iii	construct a quadgramcollocationfinder given freqdists for appearances of words bigrams trigrams two words with one word and two words between them three words
core		demo	scorer compare_scorer	finds bigram collocations in the files of the webtext corpus
core	FreqDist	__init__	samples	construct a new frequency distribution if samples is
core	FreqDist	N		return the total number of sample outcomes that have been recorded by this freqdist
core	FreqDist	__setitem__	key val	override counter __setitem__() to invalidate the cached n
core	FreqDist	__delitem__	key	override counter __delitem__() to invalidate the cached n
core	FreqDist	update		override counter update() to invalidate the cached n
core	FreqDist	setdefault	key val	override counter setdefault() to invalidate the cached n
core	FreqDist	B		return the total number of sample values (or "bins") that have counts greater than zero
core	FreqDist	hapaxes		return a list of all samples that occur once hapax legomena
core	FreqDist	r_Nr	bins	return the dictionary mapping r to nr the number of samples with frequency r where nr > 0
core	FreqDist	_cumulative_frequencies	samples	return the cumulative frequencies of the specified samples
core	FreqDist	freq	sample	return the frequency of a given sample the frequency of a
core	FreqDist	max		return the sample with the greatest number of outcomes in this frequency distribution
core	FreqDist	plot		plot samples from the frequency distribution displaying the most frequent sample first
core	FreqDist	tabulate		tabulate the given samples from the frequency distribution cumulative displaying the most frequent sample first
core	FreqDist	copy		create a copy of this frequency distribution
core	FreqDist	__add__	other	add counts from two counters
core	FreqDist	__sub__	other	subtract count but keep only results with positive counts
core	FreqDist	__or__	other	union is the maximum of value in either of the input counters
core	FreqDist	__and__	other	intersection is the minimum of corresponding counts
core	FreqDist	__repr__		return a string representation of this freqdist
core	FreqDist	pprint	maxlen stream	print a string representation of this freqdist to 'stream'
core	FreqDist	pformat	maxlen	return a string representation of this freqdist
core	FreqDist	__str__		return a string representation of this freqdist
core	ProbDistI	prob	sample	return the probability for a given sample probabilities
core	ProbDistI	logprob	sample	return the base 2 logarithm of the probability for a given sample
core	ProbDistI	max		return the sample with the greatest probability if two or
core	ProbDistI	samples		return a list of all samples that have nonzero probabilities
core	ProbDistI	discount		return the ratio by which counts are discounted on average c*/c
core	ProbDistI	generate		return a randomly selected sample from this probability distribution
core	UniformProbDist	__init__	samples	construct a new uniform probability distribution that assigns equal probability to each sample in samples
core	RandomProbDist	unirand	cls samples	the key function that creates a randomized initial distribution that still sums to 1
core	DictionaryProbDist	__init__	prob_dict log normalize	construct a new probability distribution from the given dictionary which maps values to probabilities (or to log
core	MLEProbDist	__init__	freqdist bins	use the maximum likelihood estimate to create a probability distribution for the experiment used to generate freqdist
core	MLEProbDist	freqdist		return the frequency distribution that this probability distribution is based on
core	MLEProbDist	__repr__		:rtype str :return a string representation of this probdist
core	LidstoneProbDist	__init__	freqdist gamma bins	use the lidstone estimate to create a probability distribution for the experiment used to generate freqdist
core	LidstoneProbDist	freqdist		return the frequency distribution that this probability distribution is based on
core	LidstoneProbDist	__repr__		return a string representation of this probdist
core	LaplaceProbDist	__init__	freqdist bins	use the laplace estimate to create a probability distribution for the experiment used to generate freqdist
core	LaplaceProbDist	__repr__		:rtype str :return a string representation of this probdist
core	ELEProbDist	__init__	freqdist bins	use the expected likelihood estimate to create a probability distribution for the experiment used to generate freqdist
core	ELEProbDist	__repr__		return a string representation of this probdist
core	HeldoutProbDist	__init__	base_fdist heldout_fdist bins	use the heldout estimate to create a probability distribution for the experiment used to generate base_fdist and
core	HeldoutProbDist	_calculate_Tr		return the list *tr*, where *tr[r]* is the total count in heldout_fdist for all samples that occur *r*
core	HeldoutProbDist	_calculate_estimate	Tr Nr N	return the list *estimate*, where *estimate[r]* is the probability estimate for any sample that occurs *r* times in the base frequency
core	HeldoutProbDist	base_fdist		return the base frequency distribution that this probability distribution is based on
core	HeldoutProbDist	heldout_fdist		return the heldout frequency distribution that this probability distribution is based on
core	HeldoutProbDist	__repr__		:rtype str :return a string representation of this probdist
core	CrossValidationProbDist	__init__	freqdists bins	use the cross-validation estimate to create a probability distribution for the experiment used to generate
core	CrossValidationProbDist	freqdists		return the list of frequency distributions that this probdist is based on
core	CrossValidationProbDist	__repr__		return a string representation of this probdist
core	WittenBellProbDist	__init__	freqdist bins	creates a distribution of witten-bell probability estimates this
core	WittenBellProbDist	__repr__		return a string representation of this probdist
core	SimpleGoodTuringProbDist	__init__	freqdist bins	:param freqdist the frequency counts upon which to base the estimation
core	SimpleGoodTuringProbDist	_r_Nr		split the frequency distribution in two list r nr where nr r > 0
core	SimpleGoodTuringProbDist	find_best_fit	r nr	use simple linear regression to tune parameters self _slope and
core	SimpleGoodTuringProbDist	_switch	r nr	calculate the r frontier where we must switch from nr to sr when estimating e[nr]
core	SimpleGoodTuringProbDist	_renormalize	r nr	it is necessary to renormalize all the probability estimates to ensure a proper probability distribution results
core	SimpleGoodTuringProbDist	smoothedNr	r	return the number of samples with count r
core	SimpleGoodTuringProbDist	prob	sample	return the sample's probability
core	SimpleGoodTuringProbDist	discount		this function returns the total mass of probability transfers from the seen samples to the unseen samples
core	SimpleGoodTuringProbDist	__repr__		return a string representation of this probdist
core	MutableProbDist	__init__	prob_dist samples store_logs	creates the mutable probdist based on the given prob_dist and using the list of samples given
core	MutableProbDist	update	sample prob log	update the probability for the given sample this may cause the object
core	KneserNeyProbDist	__init__	freqdist bins discount	:param freqdist the trigram frequency distribution upon which to base
core	KneserNeyProbDist	discount		return the value by which counts are discounted by default set to 0 75
core	KneserNeyProbDist	set_discount	discount	set the value by which counts are discounted to the value of discount
core	KneserNeyProbDist	__repr__		return a string representation of this probdist
core	ConditionalFreqDist	__init__	cond_samples	construct a new empty conditional frequency distribution in
core	ConditionalFreqDist	conditions		return a list of the conditions that have been accessed for this conditionalfreqdist
core	ConditionalFreqDist	N		return the total number of sample outcomes that have been recorded by this conditionalfreqdist
core	ConditionalFreqDist	plot		plot the given samples from the conditional frequency distribution
core	ConditionalFreqDist	tabulate		tabulate the given samples from the conditional frequency distribution
core	ConditionalFreqDist	__add__	other	add counts from two conditionalfreqdists
core	ConditionalFreqDist	__sub__	other	subtract count but keep only results with positive counts
core	ConditionalFreqDist	__or__	other	union is the maximum of value in either of the input counters
core	ConditionalFreqDist	__and__	other	intersection is the minimum of corresponding counts
core	ConditionalFreqDist	__repr__		return a string representation of this conditionalfreqdist
core	ConditionalProbDistI	conditions		return a list of the conditions that are represented by this conditionalprobdist
core	ConditionalProbDistI	__repr__		return a string representation of this conditionalprobdist
core	ConditionalProbDist	__init__	cfdist probdist_factory	construct a new conditional probability distribution based on the given conditional frequency distribution and probdist
core	DictionaryConditionalProbDist	__init__	probdist_dict	:param probdist_dict a dictionary containing the probdists indexed
core		add_logs	logx logy	given two numbers logx = *log x * and logy = *log y *, return *log x+y *
core	ProbabilisticMixIn	__init__		initialize this object's probability this initializer should
core	ProbabilisticMixIn	set_prob	prob	set the probability associated with this object to prob
core	ProbabilisticMixIn	set_logprob	logprob	set the log probability associated with this object to logprob
core	ProbabilisticMixIn	prob		return the probability associated with this object
core	ProbabilisticMixIn	logprob		return log p , where p is the probability associated with this object
core		_create_rand_fdist	numsamples numoutcomes	create a new frequency distribution with random samples the
core		_create_sum_pdist	numsamples	return the true probability distribution for the experiment _create_rand_fdist numsamples x
core		demo	numsamples numoutcomes	a demonstration of frequency distributions and probability distributions
core		getinfo	func	returns an info dictionary containing
core		new_wrapper	wrapper model	an improvement over functools update_wrapper the wrapper is a generic
core		decorator_factory	cls	take a class with a caller method and return a callable decorator
core		decorator	caller	general purpose decorator factory takes a caller function as input and returns a decorator with the same attributes
core		getattr_	obj name default_thunk	similar to setdefault in dictionaries
core	AbstractLazySequence	__len__		return the number of tokens in the corpus file underlying this corpus view
core	AbstractLazySequence	iterate_from	start	return an iterator that generates the tokens in the corpus file underlying this corpus view starting at the token number
core	AbstractLazySequence	__getitem__	i	return the *i* th token in the corpus file underlying this corpus view
core	AbstractLazySequence	__iter__		return an iterator that generates the tokens in the corpus file underlying this corpus view
core	AbstractLazySequence	count	value	return the number of times this list contains value
core	AbstractLazySequence	index	value start stop	return the index of the first occurrence of value in this list that is greater than or equal to start and less than
core	AbstractLazySequence	__contains__	value	return true if this list contains value
core	AbstractLazySequence	__add__	other	return a list concatenating self with other
core	AbstractLazySequence	__radd__	other	return a list concatenating other with self
core	AbstractLazySequence	__mul__	count	return a list concatenating self with itself count times
core	AbstractLazySequence	__rmul__	count	return a list concatenating self with itself count times
core	AbstractLazySequence	__repr__		return a string representation for this corpus view that is similar to a list's representation but if it would be more
core	AbstractLazySequence	__hash__		:raise valueerror corpus view objects are unhashable
core	LazySubsequence	__new__	cls source start stop	construct a new slice from a given underlying sequence the
core	LazyMap	__init__	function	:param function the function that should be applied to elements of lists
core	LazyZip	__init__		:param lists the underlying lists
core	LazyEnumerate	__init__	lst	:param lst the underlying list
core	LazyIteratorList	iterate_from	start	create a new iterator over this list starting at the given offset
core	LazyIteratorList	__add__	other	return a list concatenating self with other
core	LazyIteratorList	__radd__	other	return a list concatenating other with self
core	Trie	__init__	strings	builds a trie object which is built around a defaultdict if strings is provided it will add the strings, which
core	Trie	insert	string	inserts string into the trie
core	Trie	as_dict	d	convert defaultdict to common dict representation
core	StandardFormat	open	sfm_file	open a standard format marker file for sequential reading
core	StandardFormat	open_string	s	open a standard format marker string for sequential reading
core	StandardFormat	raw_fields		return an iterator that returns the next field in a marker value tuple
core	StandardFormat	fields	strip unwrap encoding errors	return an iterator that returns the next field in a marker value tuple where marker and value are unicode strings if an encoding
core	StandardFormat	close		close a previously opened standard format marker file or string
core	ToolboxData	_record_parse	key	returns an element tree structure corresponding to a toolbox data file with all markers at the same level
core	ToolboxData	_chunk_parse	grammar root_label trace	returns an element tree structure corresponding to a toolbox data file parsed according to the chunk grammar
core		to_sfm_string	tree encoding errors unicode_fields	return a string with a standard format representation of the toolbox data in tree tree can be a toolbox database or a single record
core	ToolboxSettings	parse	encoding errors	return the contents of toolbox settings file with a nested structure
core		remove_blanks	elem	remove all elements and subelements with no text and no child elements
core		add_default_fields	elem default_fields	add blank elements and subelements specified in default_fields
core		sort_fields	elem field_orders	sort the elements and subelements in order specified in field_orders
core		_sort_fields	elem orders_dicts	sort the children of elem
core		add_blank_lines	tree blanks_before blanks_between	add blank lines before all elements and subelements specified in blank_before
core		lesk	context_sentence ambiguous_word pos synsets	return a synset for an ambiguous word in a context
core		ancestors	node	returns the list of all nodes dominating the given tree node
core		unique_ancestors	node	returns the list of all nodes dominating the given node where there is only a single path of descent
core		_descendants	node	returns the list of all nodes which are descended from the given tree node in some way
core		_leftmost_descendants	node	returns the set of all nodes descended in some way through left branches from this node
core		_rightmost_descendants	node	returns the set of all nodes descended in some way through right branches from this node
core		_istree	obj	predicate to check whether obj is a nltk tree tree
core		_unique_descendants	node	returns the list of all nodes descended from the given node where there is only a single path of descent
core		_before	node	returns the set of all nodes that are before the given node
core		_immediately_before	node	returns the set of all nodes that are immediately before the given node
core		_after	node	returns the set of all nodes that are after the given node
core		_immediately_after	node	returns the set of all nodes that are immediately after the given node
core		_tgrep_node_literal_value	node	gets the string value of a given parse tree node for comparison using the tgrep node literal predicates
core		_tgrep_macro_use_action	_s _l tokens	builds a lambda function which looks up the macro name used
core		_tgrep_node_action	_s _l tokens	builds a lambda function representing a predicate on a tree node depending on the name of its node
core		_tgrep_parens_action	_s _l tokens	builds a lambda function representing a predicate on a tree node from a parenthetical notation
core		_tgrep_nltk_tree_pos_action	_s _l tokens	builds a lambda function representing a predicate on a tree node which returns true if the node is located at a specific tree
core		_tgrep_relation_action	_s _l tokens	builds a lambda function representing a predicate on a tree node depending on its relation to other nodes in the tree
core		_tgrep_conjunction_action	_s _l tokens join_char	builds a lambda function representing a predicate on a tree node from the conjunction of several other such lambda functions
core		_tgrep_segmented_pattern_action	_s _l tokens	builds a lambda function representing a segmented pattern
core		_tgrep_node_label_use_action	_s _l tokens	returns the node label used to begin a tgrep_expr_labeled see
core		_tgrep_node_label_pred_use_action	_s _l tokens	builds a lambda function representing a predicate on a tree node which describes the use of a previously bound node label
core		_tgrep_bind_node_label_action	_s _l tokens	builds a lambda function representing a predicate on a tree node which can optionally bind a matching node into the tgrep2 string's
core		_tgrep_rel_disjunction_action	_s _l tokens	builds a lambda function representing a predicate on a tree node from the disjunction of several other such lambda functions
core		_macro_defn_action	_s _l tokens	builds a dictionary structure which defines the given macro
core		_tgrep_exprs_action	_s _l tokens	this is the top-lebel node in a tgrep2 search string the predicate function it returns binds together all the state of a
core		_build_tgrep_parser	set_parse_actions	builds a pyparsing-based parser object for tokenizing and interpreting tgrep search strings
core		tgrep_tokenize	tgrep_string	tokenizes a tgrep search string into separate tokens
core		tgrep_compile	tgrep_string	parses and tokenizes if necessary a tgrep search string into a lambda function
core		treepositions_no_leaves	tree	returns all the tree positions in the given tree which are not leaf nodes
core		tgrep_positions	pattern trees search_leaves	return the tree positions in the trees which match the given pattern
core		tgrep_nodes	pattern trees search_leaves	return the tree nodes in the trees which match the given pattern
core	ContextIndex	_default_context	tokens i	one left token and one right token normalized to lowercase
core	ContextIndex	tokens		:rtype list str :return the document that this context index was
core	ContextIndex	word_similarity_dict	word	return a dictionary mapping from words to 'similarity scores ' indicating how often these two words occur in the same
core	ContextIndex	common_contexts	words fail_on_unknown	find contexts where the specified words can all appear and return a frequency distribution mapping each context to the
core	ConcordanceIndex	__init__	tokens key	construct a new concordance index
core	ConcordanceIndex	tokens		:rtype list str :return the document that this concordance index was
core	ConcordanceIndex	offsets	word	:rtype list int :return a list of the offset positions at which the given
core	ConcordanceIndex	print_concordance	word width lines	print a concordance for word with the specified context window
core	TokenSearcher	findall	regexp	find instances of the regular expression in the text
core	Text	__init__	tokens name	create a text object
core	Text	concordance	word width lines	print a concordance for word with the specified context window
core	Text	collocations	num window_size	print collocations derived from the text ignoring stopwords
core	Text	count	word	count the number of times this word appears in the text
core	Text	index	word	find the index of the first occurrence of the word in the text
core	Text	similar	word num	distributional similarity find other words which appear in the same contexts as the specified word list most similar words first
core	Text	common_contexts	words num	find contexts where the specified words appear list most frequent common contexts first
core	Text	dispersion_plot	words	produce a plot showing the distribution of the words through the text
core	Text	generate	words	issues a reminder to users following the book online
core	Text	plot		see documentation for freqdist plot()
core	Text	vocab		:seealso nltk prob freqdist
core	Text	findall	regexp	find instances of the regular expression in the text
core	Text	_context	tokens i	one left & one right token both case-normalized skip over
core	TextCollection	tf	term text	the frequency of the term in text
core	TextCollection	idf	term	the number of texts in the corpus divided by the number of texts that the term appears in
core		register_tag	cls	decorates a class to register it's json tag
core		python_2_unicode_compatible	klass	this decorator defines __unicode__ method and fixes __repr__ and __str__ methods under python 2
core		unicode_repr	obj	for classes that was fixed with @python_2_unicode_compatible unicode_repr returns obj
core	FeatStruct	__new__	cls features	construct and return a new feature structure if this
core	FeatStruct	_keys		return an iterable of the feature identifiers used by this featstruct
core	FeatStruct	_values		return an iterable of the feature values directly defined by this featstruct
core	FeatStruct	_items		return an iterable of fid fval pairs where fid is a feature identifier and fval is the corresponding feature
core	FeatStruct	equal_values	other check_reentrance	return true if self and other assign the same value to to every feature
core	FeatStruct	__eq__	other	return true if self and other are both feature structures assign the same values to all features and contain the same
core	FeatStruct	__hash__		if this feature structure is frozen return its hash value otherwise raise typeerror
core	FeatStruct	_equal	other check_reentrance visited_self visited_other	return true iff self and other have equal values
core	FeatStruct	_calculate_hashvalue	visited	return a hash value for this feature structure
core	FeatStruct	freeze		make this feature structure and any feature structures it contains immutable
core	FeatStruct	frozen		return true if this feature structure is immutable feature
core	FeatStruct	_freeze	visited	make this feature structure and any feature structure it contains immutable
core	FeatStruct	copy	deep	return a new copy of self the new copy will not be frozen
core	FeatStruct	cyclic		return true if this feature structure contains itself
core	FeatStruct	walk		return an iterator that generates this feature structure and each feature structure it contains
core	FeatStruct	_walk	visited	return an iterator that generates this feature structure and each feature structure it contains
core	FeatStruct	_find_reentrances	reentrances	return a dictionary that maps from the id of each feature structure contained in self (including self) to a
core	FeatStruct	substitute_bindings	bindings	:see nltk featstruct substitute_bindings()
core	FeatStruct	retract_bindings	bindings	:see nltk featstruct retract_bindings()
core	FeatStruct	variables		:see nltk featstruct find_variables()
core	FeatStruct	rename_variables	vars used_vars new_vars	:see nltk featstruct rename_variables()
core	FeatStruct	remove_variables		return the feature structure that is obtained by deleting any feature whose value is a variable
core	FeatStruct	subsumes	other	return true if self subsumes other i e return true
core	FeatStruct	__repr__		display a single-line representation of this feature structure suitable for embedding in other representations
core	FeatStruct	_repr	reentrances reentrance_ids	return a string representation of this feature structure
core		_check_frozen	method indent	given a method function return a new method function that first checks if self
core	FeatDict	__init__	features	create a new feature dictionary with the specified features
core	FeatDict	__getitem__	name_or_path	if the feature with the given name or path exists return its value otherwise raise keyerror
core	FeatDict	get	name_or_path default	if the feature with the given name or path exists return its value otherwise return default
core	FeatDict	__contains__	name_or_path	return true if a feature with the given name or path exists
core	FeatDict	has_key	name_or_path	return true if a feature with the given name or path exists
core	FeatDict	__delitem__	name_or_path	if the feature with the given name or path exists delete its value otherwise raise keyerror
core	FeatDict	__setitem__	name_or_path value	set the value for the feature with the given name or path to value
core	FeatDict	__str__		display a multi-line representation of this feature dictionary as an fvm feature value matrix
core	FeatDict	_str	reentrances reentrance_ids	:return a list of lines composing a string representation of this feature dictionary
core	FeatList	__init__	features	create a new feature list with the specified features
core	FeatList	__delitem__	name_or_path	if the feature with the given name or path exists delete its value otherwise raise keyerror
core	FeatList	__setitem__	name_or_path value	set the value for the feature with the given name or path to value
core		substitute_bindings	fstruct bindings fs_class	return the feature structure that is obtained by replacing each variable bound by bindings with its binding
core		retract_bindings	fstruct bindings fs_class	return the feature structure that is obtained by replacing each feature structure value that is bound by bindings with the
core		find_variables	fstruct fs_class	:return the set of variables used by this feature structure
core		rename_variables	fstruct vars used_vars new_vars	return the feature structure that is obtained by replacing any of this feature structure's variables that are in vars
core		remove_variables	fstruct fs_class	:rtype featstruct :return the feature structure that is obtained by deleting
core		unify	fstruct1 fstruct2 bindings trace	unify fstruct1 with fstruct2, and return the resulting feature structure
core		_destructively_unify	fstruct1 fstruct2 bindings forward	attempt to unify fstruct1 and fstruct2 by modifying them in-place
core		_unify_feature_values	fname fval1 fval2 bindings	attempt to unify fval1 and and fval2, and return the resulting unified value
core		_apply_forwards_to_bindings	forward bindings	replace any feature structure that has a forward pointer with the target of its forward pointer to preserve reentrancy
core		_apply_forwards	fstruct forward fs_class visited	replace any feature structure that has a forward pointer with the target of its forward pointer to preserve reentrancy
core		_resolve_aliases	bindings	replace any bound aliased vars with their binding and replace any unbound aliased vars with their representative var
core		subsumes	fstruct1 fstruct2	return true if fstruct1 subsumes fstruct2 i e return
core		conflicts	fstruct1 fstruct2 trace	return a list of the feature paths of all features which are assigned incompatible values by fstruct1 and fstruct2
core		_flatten	lst cls	helper function -- return a copy of list with all elements of type cls spliced in rather than appended in
core	Feature	name		the name of this feature
core	Feature	default		default value for this feature
core	Feature	display		custom display location can be prefix or slash
core	Feature	unify_base_values	fval1 fval2 bindings	if possible return a single value if not return
core	CustomFeatureValue	unify	other	if this base value unifies with other, then return the unified value
core	FeatStructReader	fromstring	s fstruct	convert a string representation of a feature structure as displayed by repr into a featstruct
core	FeatStructReader	read_partial	s position reentrances fstruct	helper function that reads in a feature structure
core	FeatStructReader	_finalize	s pos reentrances fstruct	called when we see the close brace -- checks for a slash feature and adds in default values
core	FeatStructReader	read_app_value	s position reentrances match	mainly included for backwards compat
core	FeatStructReader	_read_seq_value	s position reentrances match	helper function used by read_tuple_value and read_set_value
core		demo	trace	just for testing
core	Tree	_get_node		outdated method to access the node value use the label() method instead
core	Tree	_set_node	value	outdated method to set the node value use the set_label() method instead
core	Tree	label		return the node label of the tree
core	Tree	set_label	label	set the node label of the tree
core	Tree	leaves		return the leaves of the tree
core	Tree	flatten		return a flat version of the tree with all non-root non-terminals removed
core	Tree	height		return the height of the tree
core	Tree	treepositions	order	>>> t = tree fromstring("(s (np d the n dog (vp v chased (np d the n cat )))")
core	Tree	subtrees	filter	generate all the subtrees of this tree optionally restricted to trees matching the filter function
core	Tree	productions		generate the productions that correspond to the non-terminal nodes of the tree
core	Tree	pos		return a sequence of pos-tagged words extracted from the tree
core	Tree	leaf_treeposition	index	:return the tree position of the index-th leaf in this tree
core	Tree	treeposition_spanning_leaves	start end	:return the tree position of the lowest descendant of this tree that dominates self
core	Tree	chomsky_normal_form	factor horzMarkov vertMarkov childChar	this method can modify a tree in three ways 1
core	Tree	un_chomsky_normal_form	expandUnary childChar parentChar unaryChar	this method modifies the tree in three ways 1
core	Tree	collapse_unary	collapsePOS collapseRoot joinChar	collapse subtrees with a single child ie unary productions
core	Tree	convert	cls tree	convert a tree between different subtypes of tree cls determines
core	Tree	fromstring	cls s brackets read_node	read a bracketed tree string and return the resulting tree
core	Tree	_parse_error	cls s match expecting	display a friendly error message when parsing a tree string fails
core	Tree	draw		open a new window containing a graphical diagram of this tree
core	Tree	pretty_print	sentence highlight stream	pretty-print this tree as ascii or unicode art
core	Tree	_repr_png_		draws and outputs in png for ipython
core	Tree	pprint		print a string representation of this tree to 'stream'
core	Tree	pformat	margin indent nodesep parens	:return a pretty-printed string representation of this tree
core	Tree	pformat_latex_qtree		returns a representation of the tree compatible with the latex qtree package
core	ImmutableTree	set_label	value	set the node label this will only succeed the first time the
core	AbstractParentedTree	_setparent	child index dry_run	update the parent pointer of child to point to self this
core	AbstractParentedTree	_delparent	child index	update the parent pointer of child to not point to self this
core	ParentedTree	parent		the parent of this tree or none if it has no parent
core	ParentedTree	parent_index		the index of this tree in its parent i e
core	ParentedTree	left_sibling		the left sibling of this tree or none if it has none
core	ParentedTree	right_sibling		the right sibling of this tree or none if it has none
core	ParentedTree	root		the root of this tree i e the unique ancestor of this tree
core	ParentedTree	treeposition		the tree position of this tree relative to the root of the tree
core	MultiParentedTree	parents		the set of parents of this tree if this tree has no parents
core	MultiParentedTree	left_siblings		a list of all left siblings of this tree in any of its parent trees
core	MultiParentedTree	right_siblings		a list of all right siblings of this tree in any of its parent trees
core	MultiParentedTree	roots		the set of all roots of this tree this set is formed by
core	MultiParentedTree	parent_indices	parent	return a list of the indices where this tree occurs as a child of parent
core	MultiParentedTree	treepositions	root	return a list of all tree positions that can be used to reach this multi-parented tree starting from root
core		bracket_parse	s	use tree read(s remove_empty_top_bracketing=true) instead
core		sinica_parse	s	parse a sinica treebank string and return a tree trees are represented as nested brackettings
core		demo		a demonstration showing how trees and trees can be used
corpus		_make_bound_method	func	magic for creating bound methods (used for _unload)
corpus.reader	XMLCorpusReader	words	fileid	returns all of the words and punctuation symbols in the specified file that were in text nodes -- ie tags are ignored
corpus.reader	XMLCorpusView	__init__	fileid tagspec elt_handler	create a new corpus view based on a specified xml file
corpus.reader	XMLCorpusView	handle_elt	elt context	convert an element into an appropriate value for inclusion in the view
corpus.reader	XMLCorpusView	_read_xml_fragment	stream	read a string from the given stream that does not contain any un-closed tags
corpus.reader	XMLCorpusView	read_block	stream tagspec elt_handler	read from stream until we find at least one element that matches tagspec, and return the result of applying
corpus.reader	OpinionLexiconCorpusReader	words	fileids	return all words in the opinion lexicon note that these words are not
corpus.reader	OpinionLexiconCorpusReader	positive		return all positive words in alphabetical order
corpus.reader	OpinionLexiconCorpusReader	negative		return all negative words in alphabetical order
corpus.reader	TaggedCorpusReader	__init__	root fileids sep word_tokenizer	construct a new tagged corpus reader for a set of documents located at the given root directory
corpus.reader	TaggedCorpusReader	raw	fileids	:return the given file s as a single string
corpus.reader	TaggedCorpusReader	words	fileids	:return the given file s as a list of words and punctuation symbols
corpus.reader	TaggedCorpusReader	sents	fileids	:return the given file s as a list of sentences or utterances each encoded as a list of word
corpus.reader	TaggedCorpusReader	paras	fileids	:return the given file s as a list of paragraphs each encoded as a list of sentences which are
corpus.reader	TaggedCorpusReader	tagged_words	fileids tagset	:return the given file s as a list of tagged words and punctuation symbols encoded as tuples
corpus.reader	TaggedCorpusReader	tagged_sents	fileids tagset	:return the given file s as a list of sentences each encoded as a list of word tag tuples
corpus.reader	TaggedCorpusReader	tagged_paras	fileids tagset	:return the given file s as a list of paragraphs each encoded as a list of sentences which are
corpus.reader	CategorizedTaggedCorpusReader	__init__		initialize the corpus reader categorization arguments
corpus.reader	TaggedCorpusView	read_block	stream	reads one paragraph at a time
corpus.reader	TwitterCorpusReader	__init__	root fileids word_tokenizer encoding	:param root the root directory for this corpus
corpus.reader	TwitterCorpusReader	docs	fileids	returns the full tweet objects as specified by twitter documentation on tweets
corpus.reader	TwitterCorpusReader	strings	fileids	returns only the text content of tweets in the file s :return the given file s as a list of tweets
corpus.reader	TwitterCorpusReader	tokenized	fileids	:return the given file s as a list of the text content of tweets as as a list of words screenanames hashtags urls and punctuation symbols
corpus.reader	TwitterCorpusReader	raw	fileids	return the corpora in their raw form
corpus.reader	TwitterCorpusReader	_read_tweets	stream	assumes that each line in stream is a json-serialised object
corpus.reader	StreamBackedCorpusView	__init__	fileid block_reader startpos encoding	create a new corpus view based on the file fileid, and read with block_reader
corpus.reader	StreamBackedCorpusView	read_block	stream	read a block from the input stream
corpus.reader	StreamBackedCorpusView	_open		open the file stream associated with this corpus view this
corpus.reader	StreamBackedCorpusView	close		close the file stream associated with this corpus view this
corpus.reader		concat	docs	concatenate together the contents of multiple documents from a single corpus using an appropriate concatenation function
corpus.reader	PickleCorpusView	__init__	fileid delete_on_gc	create a new corpus view that reads the pickle corpus fileid
corpus.reader	PickleCorpusView	__del__		if delete_on_gc was set to true when this picklecorpusview was created then delete the corpus view's
corpus.reader	PickleCorpusView	cache_to_tempfile	cls sequence delete_on_gc	write the given sequence to a temporary file as a pickle corpus and then return a picklecorpusview view for that
corpus.reader		read_regexp_block	stream start_re end_re	read a sequence of tokens from a stream where tokens begin with lines that match start_re
corpus.reader		read_sexpr_block	stream block_size comment_char	read a sequence of s-expressions from the stream and leave the stream's file position at the end the last complete s-expression
corpus.reader		_sub_space	m	helper function given a regexp match return a string of spaces that's the same length as the matched string
corpus.reader	BracketParseCorpusReader	__init__	root fileids comment_char detect_blocks	:param root the root directory for this corpus
corpus.reader	CategorizedBracketParseCorpusReader	__init__		initialize the corpus reader categorization arguments
corpus.reader	AlpinoCorpusReader	_normalize	t ordered	normalize the xml sentence element in t
corpus.reader	AlpinoCorpusReader	_word	t	return a correctly ordered list if words
corpus.reader	VerbnetCorpusReader	lemmas	classid	return a list of all verb lemmas that appear in any class or in the classid if specified
corpus.reader	VerbnetCorpusReader	wordnetids	classid	return a list of all wordnet identifiers that appear in any class or in classid if specified
corpus.reader	VerbnetCorpusReader	classids	lemma wordnetid fileid classid	return a list of the verbnet class identifiers if a file
corpus.reader	VerbnetCorpusReader	vnclass	fileid_or_classid	return an elementtree containing the xml for the specified verbnet class
corpus.reader	VerbnetCorpusReader	fileids	vnclass_ids	return a list of fileids that make up this corpus if
corpus.reader	VerbnetCorpusReader	_index		initialize the indexes _lemma_to_class, _wordnet_to_class, and _class_to_fileid by scanning
corpus.reader	VerbnetCorpusReader	_index_helper	xmltree fileid	helper for _index()
corpus.reader	VerbnetCorpusReader	_quick_index		initialize the indexes _lemma_to_class, _wordnet_to_class, and _class_to_fileid by scanning
corpus.reader	VerbnetCorpusReader	longid	shortid	given a short verbnet class identifier (eg '37 10'), map it
corpus.reader	VerbnetCorpusReader	shortid	longid	given a long verbnet class identifier (eg 'confess-37 10'),
corpus.reader	VerbnetCorpusReader	pprint	vnclass	return a string containing a pretty-printed representation of the given verbnet class
corpus.reader	VerbnetCorpusReader	pprint_subclasses	vnclass indent	return a string containing a pretty-printed representation of the given verbnet class's subclasses
corpus.reader	VerbnetCorpusReader	pprint_members	vnclass indent	return a string containing a pretty-printed representation of the given verbnet class's member verbs
corpus.reader	VerbnetCorpusReader	pprint_themroles	vnclass indent	return a string containing a pretty-printed representation of the given verbnet class's thematic roles
corpus.reader	VerbnetCorpusReader	pprint_frame	vnframe indent	return a string containing a pretty-printed representation of the given verbnet frame
corpus.reader	VerbnetCorpusReader	pprint_description	vnframe indent	return a string containing a pretty-printed representation of the given verbnet frame description
corpus.reader	VerbnetCorpusReader	pprint_syntax	vnframe indent	return a string containing a pretty-printed representation of the given verbnet frame syntax
corpus.reader	VerbnetCorpusReader	pprint_semantics	vnframe indent	return a string containing a pretty-printed representation of the given verbnet frame semantics
corpus.reader	CategorizedSentencesCorpusReader	__init__	root fileids word_tokenizer sent_tokenizer	:param root the root directory for the corpus
corpus.reader	CategorizedSentencesCorpusReader	raw	fileids categories	:param fileids a list or regexp specifying the fileids that have to be returned as a raw string
corpus.reader	CategorizedSentencesCorpusReader	readme		return the contents of the corpus readme txt file
corpus.reader	CategorizedSentencesCorpusReader	sents	fileids categories	return all sentences in the corpus or in the specified file s
corpus.reader	CategorizedSentencesCorpusReader	words	fileids categories	return all words and punctuation symbols in the corpus or in the specified file s
corpus.reader	SensevalCorpusReader	raw	fileids	:return the text contents of the given fileids as a single string
corpus.reader		_fixXML	text	fix the various issues with senseval pseudo-xml
corpus.reader	CrubadanCorpusReader	lang_freq	lang	return n-gram freqdist for a specific language
corpus.reader	CrubadanCorpusReader	langs		return a list of supported languages as iso 639-3 codes
corpus.reader	CrubadanCorpusReader	iso_to_crubadan	lang	return internal crubadan code based on iso 639-3 code
corpus.reader	CrubadanCorpusReader	crubadan_to_iso	lang	return iso 639-3 code given internal crubadan code
corpus.reader	CrubadanCorpusReader	_load_lang_mapping_data		load language mappings between codes and description from table txt
corpus.reader	CrubadanCorpusReader	_load_lang_ngrams	lang	load single n-gram language file given the iso 639-3 language code
corpus.reader	StringCategoryCorpusReader	__init__	root fileids delimiter encoding	:param root the root directory for this corpus
corpus.reader	StringCategoryCorpusReader	raw	fileids	:return the text contents of the given fileids as a single string
corpus.reader	SemcorCorpusReader	words	fileids	:return the given file s as a list of words and punctuation symbols
corpus.reader	SemcorCorpusReader	chunks	fileids	:return the given file s as a list of chunks each of which is a list of words and punctuation symbols
corpus.reader	SemcorCorpusReader	tagged_chunks	fileids tag	:return the given file s as a list of tagged chunks represented in tree form
corpus.reader	SemcorCorpusReader	sents	fileids	:return the given file s as a list of sentences each encoded as a list of word strings
corpus.reader	SemcorCorpusReader	chunk_sents	fileids	:return the given file s as a list of sentences each encoded as a list of chunks
corpus.reader	SemcorCorpusReader	tagged_sents	fileids tag	:return the given file s as a list of sentences each sentence
corpus.reader	SemcorCorpusReader	_words	fileid unit bracket_sent pos_tag	helper used to implement the view methods -- returns a list of tokens segmented words chunks or sentences
corpus.reader	SemcorWordView	__init__	fileid unit bracket_sent pos_tag	:param fileid the name of the underlying file
corpus.reader		mimic_wrap	lines wrap_at	wrap the first of 'lines' with textwrap and the remaining lines at exactly the same positions as the first
corpus.reader		_pretty_longstring	defstr prefix wrap_at	helper function for pretty-printing a long string
corpus.reader		_pretty_any	obj	helper function for pretty-printing any attrdict object
corpus.reader		_pretty_semtype	st	helper function for pretty-printing a semantic type
corpus.reader		_pretty_frame_relation_type	freltyp	helper function for pretty-printing a frame relation type
corpus.reader		_pretty_frame_relation	frel	helper function for pretty-printing a frame relation
corpus.reader		_pretty_fe_relation	ferel	helper function for pretty-printing an fe relation
corpus.reader		_pretty_lu	lu	helper function for pretty-printing a lexical unit
corpus.reader		_pretty_exemplars	exemplars lu	helper function for pretty-printing a list of exemplar sentences for a lexical unit
corpus.reader		_pretty_fulltext_sentences	sents	helper function for pretty-printing a list of annotated sentences for a full-text document
corpus.reader		_pretty_fulltext_sentence	sent	helper function for pretty-printing an annotated sentence from a full-text document
corpus.reader		_pretty_pos	aset	helper function for pretty-printing a sentence with its pos tags
corpus.reader		_pretty_annotation	sent aset_level	helper function for pretty-printing an exemplar sentence for a lexical unit
corpus.reader		_annotation_ascii	sent	given a sentence or fe annotation set construct the width-limited string showing an ascii visualization of the sentence's annotations calling either
corpus.reader		_annotation_ascii_frames	sent	ascii string rendering of the sentence along with its targets and frame names
corpus.reader		_annotation_ascii_FE_layer	overt ni feAbbrevs	helper for _annotation_ascii_fes()
corpus.reader		_annotation_ascii_FEs	sent	ascii string rendering of the sentence along with a single target and its fes
corpus.reader		_pretty_fe	fe	helper function for pretty-printing a frame element
corpus.reader		_pretty_frame	frame	helper function for pretty-printing a frame
corpus.reader	Future	__init__	loader	:param loader when called with no arguments returns the value to be stored
corpus.reader	PrettyList	__repr__		return a string representation for this corpus view that is similar to a list's representation but if it would be more
corpus.reader	PrettyLazyMap	__repr__		return a string representation for this corpus view that is similar to a list's representation but if it would be more
corpus.reader	PrettyLazyIteratorList	__repr__		return a string representation for this corpus view that is similar to a list's representation but if it would be more
corpus.reader	PrettyLazyConcatenation	__repr__		return a string representation for this corpus view that is similar to a list's representation but if it would be more
corpus.reader	PrettyLazyConcatenation	__add__	other	return a list concatenating self with other
corpus.reader	PrettyLazyConcatenation	__radd__	other	return a list concatenating other with self
corpus.reader	FramenetCorpusReader	warnings	v	enable or disable warnings of data integrity issues as they are encountered
corpus.reader	FramenetCorpusReader	help	attrname	display help information summarizing the main methods
corpus.reader	FramenetCorpusReader	readme		return the contents of the corpus readme txt or readme file
corpus.reader	FramenetCorpusReader	buildindexes		build the internal indexes to make look-ups faster
corpus.reader	FramenetCorpusReader	doc	fn_docid	returns the annotated document whose id number is fn_docid
corpus.reader	FramenetCorpusReader	frame_by_id	fn_fid ignorekeys	get the details for the specified frame using the frame's id number
corpus.reader	FramenetCorpusReader	frame_by_name	fn_fname ignorekeys check_cache	get the details for the specified frame using the frame's name
corpus.reader	FramenetCorpusReader	frame	fn_fid_or_fname ignorekeys	get the details for the specified frame using the frame's name or id number
corpus.reader	FramenetCorpusReader	frames_by_lemma	pat	returns a list of all frames that contain lus in which the name attribute of the lu matchs the given regular expression
corpus.reader	FramenetCorpusReader	lu_basic	fn_luid	returns basic information about the lu whose id is fn_luid
corpus.reader	FramenetCorpusReader	lu	fn_luid ignorekeys luName frameID	access a lexical unit by its id luname frameid and framename are used
corpus.reader	FramenetCorpusReader	_lu_file	lu ignorekeys	augment the lu information that was loaded from the frame file with additional information from the lu file
corpus.reader	FramenetCorpusReader	_loadsemtypes		create the semantic types index
corpus.reader	FramenetCorpusReader	propagate_semtypes		apply inference rules to distribute semtypes over relations between fes
corpus.reader	FramenetCorpusReader	semtype	key	>>> from nltk corpus import framenet as fn
corpus.reader	FramenetCorpusReader	frames	name	obtain details for a specific frame
corpus.reader	FramenetCorpusReader	frame_ids_and_names	name	uses the frame index which is much faster than looking up each frame definition if only the names and ids are needed
corpus.reader	FramenetCorpusReader	fes	name frame	lists frame element objects if 'name' is provided this is treated as
corpus.reader	FramenetCorpusReader	lus	name frame	obtain details for lexical units
corpus.reader	FramenetCorpusReader	lu_ids_and_names	name	uses the lu index which is much faster than looking up each lu definition if only the names and ids are needed
corpus.reader	FramenetCorpusReader	docs_metadata	name	return an index of the annotated documents in framenet
corpus.reader	FramenetCorpusReader	docs	name	return a list of the annotated full-text documents in framenet optionally filtered by a regex to be matched against the document name
corpus.reader	FramenetCorpusReader	sents	exemplars full_text	annotated sentences matching the specified criteria
corpus.reader	FramenetCorpusReader	annotations	luNamePattern exemplars full_text	frame annotation sets matching the specified criteria
corpus.reader	FramenetCorpusReader	exemplars	luNamePattern frame fe fe2	lexicographic exemplar sentences optionally filtered by lu name and/or 1-2 fes that are realized overtly
corpus.reader	FramenetCorpusReader	_exemplar_of_fes	ex fes	given an exemplar sentence and a set of fe names return the subset of fe names that are realized overtly in the sentence on the fe fe2 or fe3 layer
corpus.reader	FramenetCorpusReader	ft_sents	docNamePattern	full-text annotation sentences optionally filtered by document name
corpus.reader	FramenetCorpusReader	frame_relation_types		obtain a list of frame relation types
corpus.reader	FramenetCorpusReader	frame_relations	frame frame2 type	:param frame optional frame object name or id only relations involving
corpus.reader	FramenetCorpusReader	fe_relations		obtain a list of frame element relations
corpus.reader	FramenetCorpusReader	semtypes		obtain a list of semantic types
corpus.reader	FramenetCorpusReader	_load_xml_attributes	d elt	extracts a subset of the attributes from the given element and returns them in a dictionary
corpus.reader	FramenetCorpusReader	_strip_tags	data	gets rid of all tags and newline characters from the given input
corpus.reader	FramenetCorpusReader	_handle_elt	elt tagspec	extracts and returns the attributes of the given element
corpus.reader	FramenetCorpusReader	_handle_fulltextindex_elt	elt tagspec	extracts corpus/document info from the fulltextindex xml file
corpus.reader	FramenetCorpusReader	_handle_frame_elt	elt ignorekeys	load the info for a frame from a frame xml file
corpus.reader	FramenetCorpusReader	_handle_fecoreset_elt	elt	load fe coreset info from xml
corpus.reader	FramenetCorpusReader	_handle_framerelationtype_elt	elt	load frame-relation element and its child fe-relation elements from frrelation xml
corpus.reader	FramenetCorpusReader	_handle_framerelation_elt	elt	load frame-relation element and its child fe-relation elements from frrelation xml
corpus.reader	FramenetCorpusReader	_handle_fulltextannotation_elt	elt	load full annotation info for a document from its xml file
corpus.reader	FramenetCorpusReader	_handle_fulltext_sentence_elt	elt	load information from the given 'sentence' element each
corpus.reader	FramenetCorpusReader	_handle_fulltextannotationset_elt	elt is_pos	load information from the given 'annotationset' element each
corpus.reader	FramenetCorpusReader	_handle_fulltextlayer_elt	elt	load information from the given 'layer' element each
corpus.reader	FramenetCorpusReader	_handle_framelexunit_elt	elt	load the lexical unit info from an xml element in a frame's xml file
corpus.reader	FramenetCorpusReader	_handle_lexunit_elt	elt ignorekeys	load full info for a lexical unit from its xml file
corpus.reader	FramenetCorpusReader	_handle_lusubcorpus_elt	elt	load a subcorpus of a lexical unit from the given xml
corpus.reader	FramenetCorpusReader	_handle_lusentence_elt	elt	load a sentence from a subcorpus of an lu from xml
corpus.reader	FramenetCorpusReader	_handle_luannotationset_elt	elt is_pos	load an annotation set from a sentence in an subcorpus of an lu
corpus.reader	FramenetCorpusReader	_handle_lulayer_elt	elt	load a layer from an annotation set
corpus.reader		norm	value_string	normalize the string value in an rte pair's value or entailment attribute as an integer 1 0
corpus.reader	RTEPair	__init__	pair challenge id text	:param challenge version of the rte challenge (i e rte1 rte2 or rte3)
corpus.reader	RTECorpusReader	_read_etree	doc	map the xml input into an rtepair
corpus.reader	RTECorpusReader	pairs	fileids	build a list of rtepairs from a rte corpus
corpus.reader	Lemma	count		return the frequency count for this lemma
corpus.reader	Synset	lemma_names	lang	return all the lemma_names associated with the synset
corpus.reader	Synset	lemmas	lang	return all the lemma objects associated with the synset
corpus.reader	Synset	root_hypernyms		get the topmost hypernyms of this synset in wordnet
corpus.reader	Synset	max_depth		:return the length of the longest hypernym path from this synset to the root
corpus.reader	Synset	min_depth		:return the length of the shortest hypernym path from this synset to the root
corpus.reader	Synset	closure	rel depth	return the transitive closure of source under the rel relationship breadth-first
corpus.reader	Synset	hypernym_paths		get the path s from this synset to the root where each path is a list of the synset nodes traversed on the way to the root
corpus.reader	Synset	common_hypernyms	other	find all synsets that are hypernyms of this synset and the other synset
corpus.reader	Synset	lowest_common_hypernyms	other simulate_root use_min_depth	get a list of lowest synset s that both synsets have as a hypernym
corpus.reader	Synset	hypernym_distances	distance simulate_root	get the path s from this synset to the root counting the distance of each node from the initial node on the way
corpus.reader	Synset	shortest_path_distance	other simulate_root	returns the distance of the shortest path linking the two synsets if one exists
corpus.reader	Synset	tree	rel depth cut_mark	>>> from nltk corpus import wordnet as wn
corpus.reader	Synset	path_similarity	other verbose simulate_root	path distance similarity return a score denoting how similar two word senses are based on the
corpus.reader	Synset	lch_similarity	other verbose simulate_root	leacock chodorow similarity return a score denoting how similar two word senses are based on the
corpus.reader	Synset	wup_similarity	other verbose simulate_root	wu-palmer similarity return a score denoting how similar two word senses are based on the
corpus.reader	Synset	res_similarity	other ic verbose	resnik similarity return a score denoting how similar two word senses are based on the
corpus.reader	Synset	jcn_similarity	other ic verbose	jiang-conrath similarity return a score denoting how similar two word senses are based on the
corpus.reader	Synset	lin_similarity	other ic verbose	lin similarity return a score denoting how similar two word senses are based on the
corpus.reader	Synset	_iter_hypernym_lists		:return an iterator over synset objects that are either proper hypernyms or instance of hypernyms of the synset
corpus.reader	WordNetCorpusReader	__init__	root omw_reader	construct a new wordnet corpus reader with the given root directory
corpus.reader	WordNetCorpusReader	of2ss	of	take an id and return the synsets
corpus.reader	WordNetCorpusReader	ss2of	ss	return the id of the synset
corpus.reader	WordNetCorpusReader	_load_lang_data	lang	load the wordnet data of the requested language from the file to
corpus.reader	WordNetCorpusReader	langs		return a list of languages supported by multilingual wordnet
corpus.reader	WordNetCorpusReader	_compute_max_depth	pos simulate_root	compute the max depth for the given part of speech this is
corpus.reader	WordNetCorpusReader	lemma	name lang	return lemma object that matches the name
corpus.reader	WordNetCorpusReader	_data_file	pos	return an open file pointer for the data file for the given part of speech
corpus.reader	WordNetCorpusReader	_synset_from_pos_and_offset		hack to help people like the readers of http //stackoverflow
corpus.reader	WordNetCorpusReader	synsets	lemma pos lang check_exceptions	load all synsets with a given lemma and part of speech tag
corpus.reader	WordNetCorpusReader	lemmas	lemma pos lang	return all lemma objects with a name matching the specified lemma name and part of speech tag
corpus.reader	WordNetCorpusReader	all_lemma_names	pos lang	return all lemma names for all synsets for the given part of speech tag and language or languages
corpus.reader	WordNetCorpusReader	all_synsets	pos	iterate over all synsets with a given part of speech tag
corpus.reader	WordNetCorpusReader	words	lang	return lemmas of the given language as list of words
corpus.reader	WordNetCorpusReader	license	lang	return the contents of license for omw
corpus.reader	WordNetCorpusReader	readme	lang	return the contents of readme for omw
corpus.reader	WordNetCorpusReader	citation	lang	return the contents of citation bib file for omw
corpus.reader	WordNetCorpusReader	lemma_count	lemma	return the frequency count for this lemma
corpus.reader	WordNetCorpusReader	morphy	form pos check_exceptions	find a possible base form for the given form with the given part of speech by checking wordnet's list of exceptional
corpus.reader	WordNetCorpusReader	ic	corpus weight_senses_equally smoothing	creates an information content lookup dictionary from a corpus
corpus.reader	WordNetCorpusReader	custom_lemmas	tab_file lang	reads a custom tab file containing mappings of lemmas in the given language to princeton wordnet 3
corpus.reader	WordNetICCorpusReader	ic	icfile	load an information content file from the wordnet_ic corpus and return a dictionary
corpus.reader		_lcs_ic	synset1 synset2 ic verbose	get the information content of the least common subsumer that has the highest information content value
corpus.reader	LinThesaurusCorpusReader	__defaultdict_factory		factory for creating defaultdict of defaultdict dict s
corpus.reader	LinThesaurusCorpusReader	__init__	root badscore	initialize the thesaurus
corpus.reader	LinThesaurusCorpusReader	similarity	ngram1 ngram2 fileid	returns the similarity score for two ngrams
corpus.reader	LinThesaurusCorpusReader	scored_synonyms	ngram fileid	returns a list of scored synonyms tuples of synonyms and scores for the current ngram
corpus.reader	LinThesaurusCorpusReader	synonyms	ngram fileid	returns a list of synonyms for the current ngram
corpus.reader	LinThesaurusCorpusReader	__contains__	ngram	determines whether or not the given ngram is in the thesaurus
corpus.reader		_parse_args	fun	wraps function arguments if fileids not specified then function set nkjpcorpusreader paths
corpus.reader	NKJPCorpusReader	__init__	root fileids	corpus reader designed to work with national corpus of polish
corpus.reader	NKJPCorpusReader	fileids		returns a list of file identifiers for the fileids that make up this corpus
corpus.reader	NKJPCorpusReader	_view	filename tags	returns a view specialised for use with particular corpus file
corpus.reader	NKJPCorpusReader	add_root	fileid	add root if necessary to specified fileid
corpus.reader	NKJPCorpusReader	header	fileids	returns header s of specified fileids
corpus.reader	NKJPCorpusReader	sents	fileids	returns sentences in specified fileids
corpus.reader	NKJPCorpusReader	words	fileids	returns words in specified fileids
corpus.reader	NKJPCorpusReader	tagged_words	fileids	call with specified tags as a list e g tags=['subst', 'comp']
corpus.reader	NKJPCorpusReader	raw	fileids	returns words in specified fileids
corpus.reader	NKJPCorpus_Header_View	__init__	filename	header_mode a stream backed corpus view specialized for use with
corpus.reader	NKJPCorpus_Text_View	read_block	stream tagspec elt_handler	returns text as a list of sentences
corpus.reader	CorpusReader	__init__	root fileids encoding tagset	:type root pathpointer or str
corpus.reader	CorpusReader	ensure_loaded		load this corpus if it has not already been loaded this is
corpus.reader	CorpusReader	readme		return the contents of the corpus readme file if it exists
corpus.reader	CorpusReader	license		return the contents of the corpus license file if it exists
corpus.reader	CorpusReader	citation		return the contents of the corpus citation bib file if it exists
corpus.reader	CorpusReader	fileids		return a list of file identifiers for the fileids that make up this corpus
corpus.reader	CorpusReader	abspath	fileid	return the absolute path for the given file
corpus.reader	CorpusReader	abspaths	fileids include_encoding include_fileid	return a list of the absolute paths for all fileids in this corpus or for the given list of fileids if specified
corpus.reader	CorpusReader	open	file	return an open stream that can be used to read the given file
corpus.reader	CorpusReader	encoding	file	return the unicode encoding for the given corpus file if known
corpus.reader	CategorizedCorpusReader	__init__	kwargs	initialize this mapping based on keyword arguments as follows
corpus.reader	CategorizedCorpusReader	categories	fileids	return a list of the categories that are defined for this corpus or for the file s if it is given
corpus.reader	CategorizedCorpusReader	fileids	categories	return a list of file identifiers for the files that make up this corpus or that make up the given category s if specified
corpus.reader	TimitCorpusReader	__init__	root encoding	construct a new timit corpus reader in the given directory
corpus.reader	TimitCorpusReader	fileids	filetype	return a list of file identifiers for the files that make up this corpus
corpus.reader	TimitCorpusReader	utteranceids	dialect sex spkrid sent_type	:return a list of the utterance identifiers for all utterances in this corpus or for the given speaker dialect
corpus.reader	TimitCorpusReader	transcription_dict		:return a dictionary giving the 'standard' transcription for each word
corpus.reader	TimitCorpusReader	spkrutteranceids	speaker	:return a list of all utterances associated with a given speaker
corpus.reader	TimitCorpusReader	spkrinfo	speaker	:return a dictionary mapping something
corpus.reader	TimitCorpusReader	phone_times	utterances	offset is represented as a number of 16khz samples!
corpus.reader	TimitCorpusReader	play	utterance start end	play the given audio sample
corpus.reader		read_timit_block	stream	block reader for timit tagged sentences which are preceded by a sentence number that will be ignored
corpus.reader	ChasenCorpusView	read_block	stream	reads one paragraph at a time
corpus.reader	ChunkedCorpusReader	__init__	root fileids extension str2chunktree	:param root the root directory for this corpus
corpus.reader	ChunkedCorpusReader	raw	fileids	:return the given file s as a single string
corpus.reader	ChunkedCorpusReader	words	fileids	:return the given file s as a list of words and punctuation symbols
corpus.reader	ChunkedCorpusReader	sents	fileids	:return the given file s as a list of sentences or utterances each encoded as a list of word
corpus.reader	ChunkedCorpusReader	paras	fileids	:return the given file s as a list of paragraphs each encoded as a list of sentences which are
corpus.reader	ChunkedCorpusReader	tagged_words	fileids tagset	:return the given file s as a list of tagged words and punctuation symbols encoded as tuples
corpus.reader	ChunkedCorpusReader	tagged_sents	fileids tagset	:return the given file s as a list of sentences each encoded as a list of word tag tuples
corpus.reader	ChunkedCorpusReader	tagged_paras	fileids tagset	:return the given file s as a list of paragraphs each encoded as a list of sentences which are
corpus.reader	ChunkedCorpusReader	chunked_words	fileids tagset	:return the given file s as a list of tagged words and chunks
corpus.reader	ChunkedCorpusReader	chunked_sents	fileids tagset	:return the given file s as a list of sentences each encoded as a shallow tree
corpus.reader	ChunkedCorpusReader	chunked_paras	fileids tagset	:return the given file s as a list of paragraphs each encoded as a list of sentences which are
corpus.reader	NombankCorpusReader	__init__	root nomfile framefiles nounsfile	:param root the root directory for this corpus
corpus.reader	NombankCorpusReader	raw	fileids	:return the text contents of the given fileids as a single string
corpus.reader	NombankCorpusReader	instances	baseform	:return a corpus view that acts as a list of nombankinstance objects one for each noun in the corpus
corpus.reader	NombankCorpusReader	lines		:return a corpus view that acts as a list of strings one for each line in the predicate-argument annotation file
corpus.reader	NombankCorpusReader	roleset	roleset_id	:return the xml description for the given roleset
corpus.reader	NombankCorpusReader	rolesets	baseform	:return list of xml descriptions for rolesets
corpus.reader	NombankCorpusReader	nouns		:return a corpus view that acts as a list of all noun lemmas in this corpus (from the nombank
corpus.reader	NombankInstance	roleset		the name of the roleset used by this instance's predicate
corpus.reader	NombankTreePointer	treepos	tree	convert this pointer to a standard 'tree position' pointer given that it points to the given tree
corpus.reader	Pl196xCorpusReader	textids	fileids categories	in the pl196x corpus each category is stored in single file and thus both methods provide identical functionality
corpus.reader	BNCCorpusReader	words	fileids strip_space stem	:return the given file s as a list of words and punctuation symbols
corpus.reader	BNCCorpusReader	tagged_words	fileids c5 strip_space stem	:return the given file s as a list of tagged words and punctuation symbols encoded as tuples
corpus.reader	BNCCorpusReader	sents	fileids strip_space stem	:return the given file s as a list of sentences or utterances each encoded as a list of word
corpus.reader	BNCCorpusReader	tagged_sents	fileids c5 strip_space stem	:return the given file s as a list of sentences each encoded as a list of word tag tuples
corpus.reader	BNCCorpusReader	_views	fileids sent tag strip_space	a helper function that instantiates bncwordviews or the list of words/sentences
corpus.reader	BNCCorpusReader	_words	fileid bracket_sent tag strip_space	helper used to implement the view methods -- returns a list of words or a list of sentences optionally tagged
corpus.reader	BNCWordView	__init__	fileid sent tag strip_space	:param fileid the name of the underlying file
corpus.reader	PropbankCorpusReader	__init__	root propfile framefiles verbsfile	:param root the root directory for this corpus
corpus.reader	PropbankCorpusReader	raw	fileids	:return the text contents of the given fileids as a single string
corpus.reader	PropbankCorpusReader	instances	baseform	:return a corpus view that acts as a list of propbankinstance objects one for each noun in the corpus
corpus.reader	PropbankCorpusReader	lines		:return a corpus view that acts as a list of strings one for each line in the predicate-argument annotation file
corpus.reader	PropbankCorpusReader	roleset	roleset_id	:return the xml description for the given roleset
corpus.reader	PropbankCorpusReader	rolesets	baseform	:return list of xml descriptions for rolesets
corpus.reader	PropbankCorpusReader	verbs		:return a corpus view that acts as a list of all verb lemmas in this corpus (from the verbs
corpus.reader	PropbankInstance	baseform		the baseform of the predicate
corpus.reader	PropbankInstance	sensenumber		the sense number of the predicate
corpus.reader	PropbankInstance	predid		identifier of the predicate
corpus.reader	PropbankTreePointer	treepos	tree	convert this pointer to a standard 'tree position' pointer given that it points to the given tree
corpus.reader	MTETagConverter	msd_to_universal	tag	this function converts the annotation from the multex-east to the universal tagset as described in chapter 5 of the nltk-book
corpus.reader	MTECorpusReader	__init__	root fileids encoding	construct a new mtecorpusreader for a set of documents located at the given root directory
corpus.reader	MTECorpusReader	readme		prints some information about this corpus
corpus.reader	MTECorpusReader	raw	fileids	:param fileids a list specifying the fileids that should be used
corpus.reader	MTECorpusReader	words	fileids	:param fileids a list specifying the fileids that should be used
corpus.reader	MTECorpusReader	sents	fileids	:param fileids a list specifying the fileids that should be used
corpus.reader	MTECorpusReader	paras	fileids	:param fileids a list specifying the fileids that should be used
corpus.reader	MTECorpusReader	lemma_words	fileids	:param fileids a list specifying the fileids that should be used
corpus.reader	MTECorpusReader	tagged_words	fileids tagset tags	:param fileids a list specifying the fileids that should be used
corpus.reader	MTECorpusReader	lemma_sents	fileids	:param fileids a list specifying the fileids that should be used
corpus.reader	MTECorpusReader	tagged_sents	fileids tagset tags	:param fileids a list specifying the fileids that should be used
corpus.reader	MTECorpusReader	lemma_paras	fileids	:param fileids a list specifying the fileids that should be used
corpus.reader	MTECorpusReader	tagged_paras	fileids tagset tags	:param fileids a list specifying the fileids that should be used
corpus.reader	CHILDESCorpusReader	words	fileids speaker stem relation	:return the given file s as a list of words
corpus.reader	CHILDESCorpusReader	tagged_words	fileids speaker stem relation	:return the given file s as a list of tagged words and punctuation symbols encoded as tuples
corpus.reader	CHILDESCorpusReader	sents	fileids speaker stem relation	:return the given file s as a list of sentences or utterances each encoded as a list of word strings
corpus.reader	CHILDESCorpusReader	tagged_sents	fileids speaker stem relation	:return the given file s as a list of sentences each encoded as a list of word tag tuples
corpus.reader	CHILDESCorpusReader	corpus	fileids	:return the given file s as a dict of (corpus_property_key value)
corpus.reader	CHILDESCorpusReader	participants	fileids	:return the given file s as a dict of
corpus.reader	CHILDESCorpusReader	age	fileids speaker month	:return the given file s as string or int
corpus.reader	CHILDESCorpusReader	convert_age	age_year	caclculate age in months from a string in childes format
corpus.reader	CHILDESCorpusReader	MLU	fileids speaker	:return the given file s as a floating number
corpus.reader	CHILDESCorpusReader	webview_file	fileid urlbase	map a corpus file to its web version on the childes website and open it in a web browser
corpus.reader		demo	corpus_root	the childes corpus should be manually downloaded and saved
corpus.reader	Review	__init__	title review_lines	:param title the title of the review
corpus.reader	Review	add_line	review_line	add a line reviewline to the review
corpus.reader	Review	features		return a list of features in the review each feature is a tuple made of
corpus.reader	Review	sents		return all tokenized sentences in the review
corpus.reader	ReviewsCorpusReader	__init__	root fileids word_tokenizer encoding	:param root the root directory for the corpus
corpus.reader	ReviewsCorpusReader	features	fileids	return a list of features each feature is a tuple made of the specific
corpus.reader	ReviewsCorpusReader	raw	fileids	:param fileids a list or regexp specifying the fileids of the files that have to be returned as a raw string
corpus.reader	ReviewsCorpusReader	readme		return the contents of the corpus readme txt file
corpus.reader	ReviewsCorpusReader	reviews	fileids	return all the reviews as a list of review objects if fileids is
corpus.reader	ReviewsCorpusReader	sents	fileids	return all sentences in the corpus or in the specified files
corpus.reader	ReviewsCorpusReader	words	fileids	return all words and punctuation symbols in the corpus or in the specified files
corpus.reader	Comparison	__init__	text comp_type entity_1 entity_2	:param text a string optionally tokenized containing a comparation
corpus.reader	ComparativeSentencesCorpusReader	__init__	root fileids word_tokenizer sent_tokenizer	:param root the root directory for this corpus
corpus.reader	ComparativeSentencesCorpusReader	comparisons	fileids	return all comparisons in the corpus
corpus.reader	ComparativeSentencesCorpusReader	keywords	fileids	return a set of all keywords used in the corpus
corpus.reader	ComparativeSentencesCorpusReader	keywords_readme		return the list of words and constituents considered as clues of a comparison (from listofkeywords
corpus.reader	ComparativeSentencesCorpusReader	raw	fileids	:param fileids a list or regexp specifying the fileids that have to be returned as a raw string
corpus.reader	ComparativeSentencesCorpusReader	readme		return the contents of the corpus readme file
corpus.reader	ComparativeSentencesCorpusReader	sents	fileids	return all sentences in the corpus
corpus.reader	ComparativeSentencesCorpusReader	words	fileids	return all words and punctuation symbols in the corpus
corpus.reader	PanLexLiteCorpusReader	language_varieties	lc	return a list of panlex language varieties
corpus.reader	PanLexLiteCorpusReader	meanings	expr_uid expr_tt	return a list of meanings for an expression
corpus.reader	PanLexLiteCorpusReader	translations	from_uid from_tt to_uid	return a list of translations for an expression into a single language variety
corpus.reader	Meaning	id		:return the meaning's id
corpus.reader	Meaning	quality		:return the meaning's source's quality (0=worst 9=best)
corpus.reader	Meaning	source		:return the meaning's source id
corpus.reader	Meaning	source_group		:return the meaning's source group id
corpus.reader	Meaning	expressions		:return the meaning's expressions as a dictionary whose keys are language variety uniform identifiers and whose values are lists of expression
corpus.reader	ProsConsCorpusReader	__init__	root fileids word_tokenizer encoding	:param root the root directory for the corpus
corpus.reader	ProsConsCorpusReader	sents	fileids categories	return all sentences in the corpus or in the specified files/categories
corpus.reader	ProsConsCorpusReader	words	fileids categories	return all words and punctuation symbols in the corpus or in the specified files/categories
corpus.reader	SentiWordNetCorpusReader	__init__	root fileids encoding	construct a new sentiwordnet corpus reader using data from the specified file
corpus.reader	SentiSynset	__str__		prints just the pos/neg scores for now
corpus.reader	SwadeshCorpusReader	entries	fileids	:return a tuple of words for the specified fileids
corpus.reader	NonbreakingPrefixesCorpusReader	words	lang fileids ignore_lines_startswith	this module returns a list of nonbreaking prefixes for the specified language s
corpus.reader	UnicharsCorpusReader	chars	category fileids	this module returns a list of characters from the perl unicode properties
corpus.reader	MWAPPDBCorpusReader	entries	fileids	:return a tuple of synonym word pairs
corpus.reader	ConllCorpusReader	iob_words	fileids tagset	:return a list of word/tag/iob tuples
corpus.reader	ConllCorpusReader	iob_sents	fileids tagset	:return a list of lists of word/tag/iob tuples
corpus.reader	ConllCorpusReader	_get_srl_spans	grid	list of list of start end tag) tuples
corpus.reader	AlignedCorpusReader	__init__	root fileids sep word_tokenizer	construct a new aligned corpus reader for a set of documents located at the given root directory
corpus.reader	AlignedCorpusReader	raw	fileids	:return the given file s as a single string
corpus.reader	AlignedCorpusReader	words	fileids	:return the given file s as a list of words and punctuation symbols
corpus.reader	AlignedCorpusReader	sents	fileids	:return the given file s as a list of sentences or utterances each encoded as a list of word
corpus.reader	AlignedCorpusReader	aligned_sents	fileids	:return the given file s as a list of alignedsent objects
corpus.reader	DependencyCorpusReader	raw	fileids	:return the given file s as a single string
corpus.reader	PlaintextCorpusReader	__init__	root fileids word_tokenizer sent_tokenizer	construct a new plaintext corpus reader for a set of documents located at the given root directory
corpus.reader	PlaintextCorpusReader	raw	fileids	:return the given file s as a single string
corpus.reader	PlaintextCorpusReader	words	fileids	:return the given file s as a list of words and punctuation symbols
corpus.reader	PlaintextCorpusReader	sents	fileids	:return the given file s as a list of sentences or utterances each encoded as a list of word
corpus.reader	PlaintextCorpusReader	paras	fileids	:return the given file s as a list of paragraphs each encoded as a list of sentences which are
corpus.reader	CategorizedPlaintextCorpusReader	__init__		initialize the corpus reader categorization arguments
corpus.reader	EuroparlCorpusReader	chapters	fileids	:return the given file s as a list of chapters each encoded as a list of sentences which are
corpus.reader	YCOECorpusReader	documents	fileids	return a list of document identifiers for all documents in this corpus or for the documents with the given file s if
corpus.reader	YCOECorpusReader	fileids	documents	return a list of file identifiers for the files that make up this corpus or that store the given document s if specified
corpus.reader	YCOECorpusReader	_getfileids	documents subcorpus	helper that selects the appropriate fileids for a given set of documents from a given subcorpus pos or psd
corpus.reader	CMUDictCorpusReader	entries		:return the cmudict lexicon as a list of entries containing word transcriptions tuples
corpus.reader	CMUDictCorpusReader	raw		:return the cmudict lexicon as a raw string
corpus.reader	CMUDictCorpusReader	words		:return a list of all words defined in the cmudict lexicon
corpus.reader	CMUDictCorpusReader	dict		:return the cmudict lexicon as a dictionary whose keys are lowercase words and whose values are lists of pronunciations
sentiment		negated	input_words include_nt	determine if input contains negation words
sentiment		normalize	score alpha	normalize the score to be between -1 and 1 using an alpha that
sentiment		allcap_differential	words	check whether just some words in the input are all caps
sentiment		scalar_inc_dec	word valence is_cap_diff	check if the preceding words increase decrease or negate/nullify the
sentiment	SentiText	_words_plus_punc		returns mapping of form
sentiment	SentiText	_words_and_emoticons		removes leading and trailing puncutation leaves contractions and most emoticons
sentiment	SentimentIntensityAnalyzer	make_lex_dict		convert lexicon file to a dictionary
sentiment	SentimentIntensityAnalyzer	polarity_scores	text	return a float for sentiment strength based on the input text
sentiment		timer	method	a timer decorator to measure execution performance of methods
sentiment		extract_unigram_feats	document unigrams handle_negation	populate a dictionary of unigram features reflecting the presence/absence in the document of each of the tokens in unigrams
sentiment		extract_bigram_feats	document bigrams	populate a dictionary of bigram features reflecting the presence/absence in the document of each of the tokens in bigrams
sentiment		mark_negation	document double_neg_flip shallow	append _neg suffix to words that appear in the scope between a negation and a punctuation mark
sentiment		output_markdown	filename	write the output of an analysis to a file
sentiment		save_file	content filename	store content in filename can be used to store a sentimentanalyzer
sentiment		split_train_test	all_instances n	randomly split n instances of the dataset into train and test sets
sentiment		json2csv_preprocess	json_file outfile fields encoding	convert json file to csv file preprocessing each row to obtain a suitable dataset for tweets semantic analysis
sentiment		parse_tweets_set	filename label word_tokenizer sent_tokenizer	parse csv file containing tweets and output data a list of text label tuples
sentiment		demo_tweets	trainer n_instances output	train and test naive bayes classifier on 10000 tweets tokenized using tweettokenizer
sentiment		demo_movie_reviews	trainer n_instances output	train classifier on all instances of the movie reviews dataset
sentiment		demo_subjectivity	trainer save_analyzer n_instances output	train and test a classifier on instances of the subjective dataset by pang and lee
sentiment		demo_sent_subjectivity	text	classify a single sentence as subjective or objective using a stored sentimentanalyzer
sentiment		demo_liu_hu_lexicon	sentence plot	basic example of sentiment classification using liu and hu opinion lexicon
sentiment		demo_vader_instance	text	output polarity scores for a text using vader approach
sentiment		demo_vader_tweets	n_instances output	classify 10000 positive and negative tweets using vader approach
sentiment	SentimentAnalyzer	all_words	documents labeled	return all words/tokens from the documents with duplicates
sentiment	SentimentAnalyzer	apply_features	documents labeled	apply all feature extractor functions to the documents this is a wrapper
sentiment	SentimentAnalyzer	unigram_word_feats	words top_n min_freq	return most common top_n word features
sentiment	SentimentAnalyzer	bigram_collocation_feats	documents top_n min_freq assoc_measure	return top_n bigram features (using assoc_measure)
sentiment	SentimentAnalyzer	classify	instance	classify a single instance applying the features that have already been stored in the sentimentanalyzer
sentiment	SentimentAnalyzer	add_feat_extractor	function	add a new function to extract features from a document this function will
sentiment	SentimentAnalyzer	extract_features	document	apply extractor functions and their parameters to the present document
sentiment	SentimentAnalyzer	train	trainer training_set save_classifier	train classifier on the training set optionally saving the output in the file specified by save_classifier
sentiment	SentimentAnalyzer	evaluate	test_set classifier accuracy f_measure	evaluate and print classifier performance on the test set
chunk		accuracy	chunker gold	score the accuracy of the chunker against the gold standard
chunk	ChunkScore	score	correct guessed	given a correctly chunked sentence score another chunked version of the same sentence
chunk	ChunkScore	accuracy		return the overall tag-based accuracy for all text that have been scored by this chunkscore, using the iob conll2000
chunk	ChunkScore	precision		return the overall precision for all texts that have been scored by this chunkscore
chunk	ChunkScore	recall		return the overall recall for all texts that have been scored by this chunkscore
chunk	ChunkScore	f_measure	alpha	return the overall f measure for all texts that have been scored by this chunkscore
chunk	ChunkScore	missed		return the chunks which were included in the correct chunk structures but not in the guessed chunk
chunk	ChunkScore	incorrect		return the chunks which were included in the guessed chunk structures but not in the correct chunk structures listed in input order
chunk	ChunkScore	correct		return the chunks which were included in the correct chunk structures listed in input order
chunk	ChunkScore	guessed		return the chunks which were included in the guessed chunk structures listed in input order
chunk	ChunkScore	__repr__		return a concise representation of this chunkscoring
chunk	ChunkScore	__str__		return a verbose representation of this chunkscoring
chunk		tagstr2tree	s chunk_label root_label sep	divide a string of bracketted tagged text into chunks and unchunked tokens and produce a tree
chunk		conllstr2tree	s chunk_types root_label	return a chunk structure for a single sentence encoded in the given conll 2000 style string
chunk		tree2conlltags	t	return a list of 3-tuples containing word tag iob-tag
chunk		conlltags2tree	sentence chunk_types root_label strict	convert the conll iob format to a tree
chunk		tree2conllstr	t	return a multiline string where each line contains a word tag and iob tag
chunk		ieerstr2tree	s chunk_types root_label	return a chunk structure containing the chunked tagged text that is encoded in the given ieer style string
chunk	ChunkParserI	parse	tokens	return the best chunk structure for the given tokens and return a tree
chunk	ChunkParserI	evaluate	gold	score the accuracy of the chunker against the gold standard
chunk	NEChunkParser	parse	tokens	each token should be a pos-tagged word
chunk	NEChunkParser	_tagged_to_parse	tagged_tokens	convert a list of tagged tokens to a chunk-parse tree
chunk	NEChunkParser	_parse_to_tagged	sent	convert a chunk-parse tree to a list of tagged tokens
chunk		ne_chunk	tagged_tokens binary	use nltk's currently recommended named entity chunker to chunk the given list of tagged tokens
chunk		ne_chunk_sents	tagged_sentences binary	use nltk's currently recommended named entity chunker to chunk the given list of tagged sentences each consisting of a list of tagged tokens
chunk	ChunkString	__init__	chunk_struct debug_level	construct a new chunkstring that encodes the chunking of the text tagged_tokens
chunk	ChunkString	_verify	s verify_tags	check to make sure that s still corresponds to some chunked version of _pieces
chunk	ChunkString	to_chunkstruct	chunk_label	return the chunk structure encoded by this chunkstring
chunk	ChunkString	xform	regexp repl	apply the given transformation to the string encoding of this chunkstring
chunk	ChunkString	__repr__		return a string representation of this chunkstring
chunk	ChunkString	__str__		return a formatted representation of this chunkstring
chunk	RegexpChunkRule	__init__	regexp repl descr	construct a new regexpchunkrule
chunk	RegexpChunkRule	apply	chunkstr	apply this rule to the given chunkstring see the
chunk	RegexpChunkRule	descr		return a short description of the purpose and/or effect of this rule
chunk	RegexpChunkRule	__repr__		return a string representation of this rule it has the form :
chunk	RegexpChunkRule	fromstring	s	create a regexpchunkrule from a string description
chunk	ChunkRule	__init__	tag_pattern descr	construct a new chunkrule
chunk	ChunkRule	__repr__		return a string representation of this rule it has the form :
chunk	ChinkRule	__init__	tag_pattern descr	construct a new chinkrule
chunk	ChinkRule	__repr__		return a string representation of this rule it has the form :
chunk	UnChunkRule	__init__	tag_pattern descr	construct a new unchunkrule
chunk	UnChunkRule	__repr__		return a string representation of this rule it has the form :
chunk	MergeRule	__init__	left_tag_pattern right_tag_pattern descr	construct a new mergerule
chunk	MergeRule	__repr__		return a string representation of this rule it has the form :
chunk	SplitRule	__init__	left_tag_pattern right_tag_pattern descr	construct a new splitrule
chunk	SplitRule	__repr__		return a string representation of this rule it has the form :
chunk	ExpandLeftRule	__init__	left_tag_pattern right_tag_pattern descr	construct a new expandrightrule
chunk	ExpandLeftRule	__repr__		return a string representation of this rule it has the form :
chunk	ExpandRightRule	__init__	left_tag_pattern right_tag_pattern descr	construct a new expandrightrule
chunk	ExpandRightRule	__repr__		return a string representation of this rule it has the form :
chunk	ChunkRuleWithContext	__init__	left_context_tag_pattern chunk_tag_pattern right_context_tag_pattern descr	construct a new chunkrulewithcontext
chunk	ChunkRuleWithContext	__repr__		return a string representation of this rule it has the form :
chunk		tag_pattern2re_pattern	tag_pattern	convert a tag pattern to a regular expression pattern a "tag
chunk	RegexpChunkParser	__init__	rules chunk_label root_label trace	construct a new regexpchunkparser
chunk	RegexpChunkParser	_trace_apply	chunkstr verbose	apply each rule of this regexpchunkparser to chunkstr, in turn
chunk	RegexpChunkParser	_notrace_apply	chunkstr	apply each rule of this regexpchunkparser to chunkstr, in turn
chunk	RegexpChunkParser	parse	chunk_struct trace	:type chunk_struct tree
chunk	RegexpChunkParser	rules		:return the sequence of rules used by regexpchunkparser
chunk	RegexpChunkParser	__repr__		:return a concise string representation of this regexpchunkparser
chunk	RegexpChunkParser	__str__		:return a verbose string representation of this regexpchunkparser
chunk	RegexpParser	__init__	grammar root_label loop trace	create a new chunk parser from the given start state and set of chunk patterns
chunk	RegexpParser	_read_grammar	grammar root_label trace	helper function for __init__: read the grammar if it is a string
chunk	RegexpParser	_add_stage	rules lhs root_label trace	helper function for __init__: add a new stage to the parser
chunk	RegexpParser	parse	chunk_struct trace	apply the chunk parser to this input
chunk	RegexpParser	__repr__		:return a concise string representation of this chunk regexpparser
chunk	RegexpParser	__str__		:return a verbose string representation of this regexpparser
chunk		demo_eval	chunkparser text	demonstration code for evaluating a chunk parser using a chunkscore
chunk		demo		a demonstration for the regexpchunkparser class a single text is
tag		str2tuple	s sep	given the string representation of a tagged token return the corresponding tuple representation
tag		tuple2str	tagged_token sep	given the tuple representation of a tagged token return the corresponding string representation
tag		untag	tagged_sentence	given a tagged sentence return an untagged version of that sentence
tag	AveragedPerceptron	predict	features	dot-product the features and current weights and return the best label
tag	AveragedPerceptron	update	truth guess features	update the feature weights
tag	AveragedPerceptron	average_weights		average weights from all iterations
tag	AveragedPerceptron	save	path	save the pickled model weights
tag	AveragedPerceptron	load	path	load the pickled model weights
tag	PerceptronTagger	__init__	load	:param load load the pickled model upon instantiation
tag	PerceptronTagger	tag	tokens	tag tokenized sentences
tag	PerceptronTagger	train	sentences save_loc nr_iter	train a model from sentences and save it at save_loc nr_iter
tag	PerceptronTagger	load	loc	:param loc load a pickled model at location
tag	PerceptronTagger	normalize	word	normalization used in pre-processing
tag	PerceptronTagger	_get_features	i word context prev	map tokens into a feature representation implemented as a {hashable int} dict
tag	PerceptronTagger	_make_tagdict	sentences	make a tag dictionary for single-tag words
tag	SennaTagger	tag_sents	sentences	applies the tag method over a list of sentences this method will return
tag	SennaChunkTagger	tag_sents	sentences	applies the tag method over a list of sentences this method will return
tag	SennaChunkTagger	bio_to_chunks	tagged_sent chunk_type	extracts the chunks in a bio chunk-tagged sentence
tag	SennaNERTagger	tag_sents	sentences	applies the tag method over a list of sentences this method will return
tag	TaggerI	tag	tokens	determine the most appropriate tag sequence for the given token sequence and return a corresponding list of tagged
tag	TaggerI	tag_sents	sentences	apply self tag() to each element of *sentences* i e :
tag	TaggerI	evaluate	gold	score the accuracy of the tagger against the gold standard
tag	Word	extract_property	tokens index	@return the given token's text
tag	Pos	extract_property	tokens index	@return the given token's tag
tag		nltkdemo18		return 18 templates from the original nltk demo in multi-feature syntax
tag		nltkdemo18plus		return 18 templates from the original nltk demo and additionally a few
tag		fntbl37		return 37 templates taken from the postagging task of the fntbl distribution http //www
tag		brill24		return 24 templates of the seminal tbl paper brill 1995
tag		describe_template_sets		print the available template sets in this demo with a short description"
tag	BrillTagger	__init__	initial_tagger rules training_stats	:param initial_tagger the initial tagger
tag	BrillTagger	rules		return the ordered list of transformation rules that this tagger has learnt
tag	BrillTagger	train_stats	statistic	return a named statistic collected during training or a dictionary of all
tag	BrillTagger	print_template_statistics	test_stats printunused	print a list of all templates ranked according to efficiency
tag	BrillTagger	batch_tag_incremental	sequences gold	tags by applying each rule to the entire corpus rather than all rules to a single sequence
tag		pos_tag	tokens tagset lang	use nltk's currently recommended part of speech tagger to tag the given list of tokens
tag		pos_tag_sents	sentences tagset lang	use nltk's currently recommended part of speech tagger to tag the given list of sentences each consisting of a list of tokens
tag	HiddenMarkovModelTagger	train	cls labeled_sequence test_sequence unlabeled_sequence	train a new hiddenmarkovmodeltagger using the given labeled and unlabeled training instances
tag	HiddenMarkovModelTagger	probability	sequence	returns the probability of the given symbol sequence if the sequence
tag	HiddenMarkovModelTagger	log_probability	sequence	returns the log-probability of the given symbol sequence if the
tag	HiddenMarkovModelTagger	tag	unlabeled_sequence	tags the sequence with the highest probability state sequence this
tag	HiddenMarkovModelTagger	_output_logprob	state symbol	:return the log probability of the symbol being observed in the given
tag	HiddenMarkovModelTagger	_create_cache		the cache is a tuple p o x s where - s maps symbols to integers
tag	HiddenMarkovModelTagger	best_path	unlabeled_sequence	returns the state sequence of the optimal most probable path through the hmm
tag	HiddenMarkovModelTagger	best_path_simple	unlabeled_sequence	returns the state sequence of the optimal most probable path through the hmm
tag	HiddenMarkovModelTagger	random_sample	rng length	randomly sample the hmm to generate a sentence of a given length this
tag	HiddenMarkovModelTagger	entropy	unlabeled_sequence	returns the entropy over labellings of the given sequence this is
tag	HiddenMarkovModelTagger	point_entropy	unlabeled_sequence	returns the pointwise entropy over the possible states at each position in the chain given the observation sequence
tag	HiddenMarkovModelTagger	_transitions_matrix		return a matrix of transition log probabilities
tag	HiddenMarkovModelTagger	_outputs_vector	symbol	return a vector with log probabilities of emitting a symbol when entering states
tag	HiddenMarkovModelTagger	_forward_probability	unlabeled_sequence	return the forward probability matrix a t by n array of log-probabilities where t is the length of the sequence and n is the
tag	HiddenMarkovModelTagger	_backward_probability	unlabeled_sequence	return the backward probability matrix a t by n array of log-probabilities where t is the length of the sequence and n is the
tag	HiddenMarkovModelTagger	test	test_sequence verbose	tests the hiddenmarkovmodeltagger instance
tag	HiddenMarkovModelTrainer	train	labeled_sequences unlabeled_sequences	trains the hmm using both or either of supervised and unsupervised techniques
tag	HiddenMarkovModelTrainer	train_unsupervised	unlabeled_sequences update_outputs	trains the hmm using the baum-welch algorithm to maximise the probability of the data sequence
tag	HiddenMarkovModelTrainer	train_supervised	labelled_sequences estimator	supervised training maximising the joint probability of the symbol and state sequences
tag		_log_add		adds the logged values returning the logarithm of the addition
tag		_market_hmm_example		return an example hmm described at page 381 huang et al
tag		tagset_mapping	source target	retrieve the mapping dictionary between tagsets
tag		map_tag	source target source_tag	maps the tag from the source tagset to the target tagset
tag	SequentialBackoffTagger	backoff		the backoff tagger for this tagger
tag	SequentialBackoffTagger	tag_one	tokens index history	determine an appropriate tag for the specified token and return that tag
tag	SequentialBackoffTagger	choose_tag	tokens index history	decide which tag should be used for the specified token and return that tag
tag	ContextTagger	__init__	context_to_tag backoff	:param context_to_tag a dictionary mapping contexts to tags
tag	ContextTagger	context	tokens index history	:return the context that should be used to look up the tag for the specified token or none if the specified token
tag	ContextTagger	size		:return the number of entries in the table used by this tagger to map from contexts to tags
tag	ContextTagger	_train	tagged_corpus cutoff verbose	initialize this contexttagger's _context_to_tag table based on the given training data
tag	ClassifierBasedTagger	_train	tagged_corpus classifier_builder verbose	build a new classifier based on the given training data *tagged_corpus*
tag	ClassifierBasedTagger	feature_detector	tokens index history	return the feature detector that this tagger uses to generate featuresets for its classifier
tag	ClassifierBasedTagger	classifier		return the classifier that this tagger uses to choose a tag for each word in a sentence
tag	TnT	__init__	unk Trained N C	construct a tnt statistical tagger tagger must be trained
tag	TnT	train	data	uses a set of tagged data to train the tagger
tag	TnT	_compute_lambda		creates lambda values based upon training data
tag	TnT	_safe_div	v1 v2	safe floating point division function does not allow division by 0
tag	TnT	tagdata	data	tags each sentence in a list of sentences
tag	TnT	tag	data	tags a single sentence
tag	TnT	_tagword	sent current_states	:param sent : list of words remaining in the sentence
tag		basic_sent_chop	data raw	basic method for tokenizing input into sentences
tag	CRFTagger	__init__	feature_func verbose training_opt	initialize the crfsuite tagger
tag	CRFTagger	_get_features	tokens idx	extract basic features about this word including - current word
tag	CRFTagger	tag_sents	sents	tag a list of sentences nb before using this function user should specify the mode_file either by
tag	CRFTagger	train	train_data model_file	train the crf tagger using crfsuite
tag	CRFTagger	tag	tokens	tag a sentence using python crfsuite tagger nb before using this function user should specify the mode_file either by
tag	HunposTagger	__init__	path_to_model path_to_bin encoding verbose	starts the hunpos-tag executable and establishes a connection with it
tag	HunposTagger	close		closes the pipe to the hunpos executable
tag	HunposTagger	tag	tokens	tags a single sentence a list of words
tag	BrillTaggerTrainer	__init__	initial_tagger templates trace deterministic	construct a brill tagger from a baseline tagger and a
tag	BrillTaggerTrainer	train	train_sents max_rules min_score min_acc	trains the brill tagger on the corpus *train_sents*, producing at most *max_rules* transformations each of which
tag	BrillTaggerTrainer	_init_mappings	test_sents train_sents	initialize the tag position mapping & the rule related mappings
tag	BrillTaggerTrainer	_find_rules	sent wordnum new_tag	use the templates to find rules that apply at index *wordnum* in the sentence *sent* and generate the tag *new_tag*
tag	BrillTaggerTrainer	_update_rule_applies	rule sentnum wordnum train_sents	update the rule data tables to reflect the fact that *rule* applies at the position * sentnum wordnum *
tag	BrillTaggerTrainer	_update_rule_not_applies	rule sentnum wordnum	update the rule data tables to reflect the fact that *rule* does not apply at the position * sentnum wordnum *
tag	BrillTaggerTrainer	_best_rule	train_sents test_sents min_score min_acc	find the next best rule this is done by repeatedly taking a
tag	BrillTaggerTrainer	_apply_rule	rule test_sents	update *test_sents* by applying *rule* everywhere where its conditions are met
tag	BrillTaggerTrainer	_update_tag_positions	rule	update _tag_positions to reflect the changes to tags that are made by *rule*
tag	BrillTaggerTrainer	_update_rules	rule train_sents test_sents	check if we should add or remove any rules from consideration given the changes made by *rule*
misc		wordfinder	words rows cols attempts	attempt to arrange words into a letter-grid with the specified number of rows and columns
misc		selection	a	selection sort scan the list to find its smallest element then swap it with the first element
misc		bubble	a	bubble sort compare adjacent elements of the list left-to-right and swap them if they are out of order
misc		merge	a	merge sort split the list in half and sort each half then combine the sorted halves
misc	MinimalSet	__init__	parameters	create a new minimal set
misc	MinimalSet	add	context target display	add a new item to the minimal set having the specified context target and display form
misc	MinimalSet	contexts	minimum	determine which contexts occurred with enough distinct targets
cluster	KMeansClusterer	__init__	num_means distance repeats conv_test	:param num_means the number of means to use may use fewer
cluster	KMeansClusterer	means		the means used for clustering
cluster	VectorSpaceClusterer	__init__	normalise svd_dimensions	:param normalise should vectors be normalised to length 1
cluster	VectorSpaceClusterer	cluster_vectorspace	vectors trace	finds the clusters using the given set of vectors
cluster	VectorSpaceClusterer	classify_vectorspace	vector	returns the index of the appropriate cluster for the vector
cluster	VectorSpaceClusterer	likelihood_vectorspace	vector cluster	returns the likelihood of the vector belonging to the cluster
cluster	VectorSpaceClusterer	vector	vector	returns the vector after normalisation and dimensionality reduction
cluster	VectorSpaceClusterer	_normalise	vector	normalises the vector to unit length
cluster		euclidean_distance	u v	returns the euclidean distance between vectors u and v this is equivalent
cluster		cosine_distance	u v	returns 1 minus the cosine of the angle between vectors v and u this is equal to
cluster	Dendrogram	__init__	items	:param items the items at the leaves of the dendrogram
cluster	Dendrogram	merge		merges nodes at given indices in the dendrogram the nodes will be
cluster	Dendrogram	groups	n	finds the n-groups of items leaves reachable from a cut at depth n
cluster	Dendrogram	show	leaf_labels	print the dendrogram in ascii art to standard out
cluster	EMClusterer	__init__	initial_means priors covariance_matrices conv_threshold	creates an em clusterer with the given starting parameters convergence threshold and vector mangling parameters
cluster		demo		non-interactive demonstration of the clusterers with simple 2-d data
cluster	ClusterI	cluster	vectors assign_clusters	assigns the vectors to clusters learning the clustering parameters from the data
cluster	ClusterI	classify	token	classifies the token into a cluster setting the token's cluster parameter to that cluster identifier
cluster	ClusterI	likelihood	vector label	returns the likelihood a float of the token having the corresponding cluster
cluster	ClusterI	classification_probdist	vector	classifies the token into a cluster returning a probability distribution over the cluster identifiers
cluster	ClusterI	num_clusters		returns the number of clusters
cluster	ClusterI	cluster_names		returns the names of the clusters
cluster	ClusterI	cluster_name	index	returns the names of the cluster at index
cluster	GAAClusterer	dendrogram		:return the dendrogram representing the current clustering
cluster		demo		non-interactive demonstration of the clusterers with simple 2-d data
app	ShiftReduceApp	mainloop		enter the tkinter mainloop this function must be called if
app		app		create a shift reduce parser app using a simple grammar and text
app	ChartView	__init__	chart root	construct a new chart display
app	ChartView	_sb_canvas	root expand fill side	helper for __init__: construct a canvas with a scrollbar
app	ChartView	_grow		grow the window if necessary
app	ChartView	_configure	e	the configure callback this is called whenever the window is
app	ChartView	update	chart	draw any edges that have not been drawn this is typically
app	ChartView	_edge_conflict	edge lvl	return true if the given edge overlaps with any edge on the given level
app	ChartView	_analyze_edge	edge	given a new edge recalculate
app	ChartView	_add_edge	edge minlvl	add a single edge to the chartview
app	ChartView	_draw_edge	edge lvl	draw a single edge on the chartview
app	ChartView	_color_edge	edge linecolor textcolor	color in an edge with the given colors
app	ChartView	mark_edge	edge mark	mark an edge
app	ChartView	unmark_edge	edge	unmark an edge or all edges
app	ChartView	_analyze		analyze the sentence string to figure out how big a unit needs to be how big the tree should be etc
app	ChartView	_resize		update the scroll-regions for each canvas this ensures that
app	ChartView	_draw_loclines		draw location lines these are vertical gridlines used to
app	ChartView	_draw_sentence		draw the sentence string
app	ChartView	_draw_treetok	treetok index depth	:param index the index of the first leaf in the tree
app	ChartView	draw		draw everything from scratch
app	ChartParserApp	mainloop		enter the tkinter mainloop this function must be called if
app	ChartParserApp	load_chart		load a chart from a pickle file
app	ChartParserApp	save_chart		save a chart to a pickle file
app	ChartParserApp	load_grammar		load a grammar from a pickle file
app		get_unique_counter_from_url	sp	extract the unique counter from the url if it has one otherwise return
app		wnb	port runBrowser logfilename	run nltk wordnet browser server
app		_pos_match	pos_tuple	this function returns the complete pos tuple for the partial pos tuple given to it
app		get_relations_data	word synset	get synset relations data for a synset note that this doesn't
app		pg	word body	return a html page of nltk browser format constructed from the
app		_abbc	txt	abbc = asterisks breaks bold center
app		_get_synset	synset_key	the synset key is the unique name of the synset this can be retrived via synset
app		_collect_one_synset	word synset synset_relations	returns the html string for one synset or word
app		_collect_all_synsets	word pos synset_relations	return a html unordered list of synsets for the given word and part of speech
app		_synset_relations	word synset synset_relations	builds the html string for the relations of a synset
app	Reference	__init__	word synset_relations	build a reference to a new page
app	Reference	encode		encode this reference into a string to be used in a url
app	Reference	decode	string	decode a reference encoded with reference encode
app	Reference	toggle_synset_relation	synset relation	toggle the display of the relations for the given synset and relation type
app	Reference	toggle_synset	synset	toggle displaying of the relation types for the given synset
app		page_from_word	word	return a html page for the given word
app		page_from_href	href	returns a tuple of the html page built and the new current word
app		page_from_reference	href	returns a tuple of the html page built and the new current word
app		get_static_page_by_path	path	return a static html page from the path given
app		get_static_web_help_page		return the static web help page
app		get_static_welcome_message		get the static welcome page
app		get_static_index_page	with_shutdown	get the static index page
app		get_static_upper_page	with_shutdown	return the upper frame page if with_shutdown is true then a 'shutdown' button is also provided
app		usage		display the command line help message
app	RecursiveDescentApp	mainloop		enter the tkinter mainloop this function must be called if
app	RecursiveDescentApp	_makeroom	treeseg	make sure that no sibling tree bbox's overlap
app		app		create a recursive descent parser demo using a simple grammar and text
app	RegexpChunkApp	__init__	devset_name devset grammar chunk_label	:param devset_name the name of the development set used for display & for save files
app	RegexpChunkApp	_adaptively_modify_eval_chunk	t	modify _eval_chunk to try to keep the amount of time that the eval demon takes between _eval_demon_min and _eval_demon_max
app	RegexpChunkApp	mainloop		enter the tkinter mainloop this function must be called if
parse	GenericStanfordParser	parse_sents	sentences verbose	use stanfordparser to parse multiple sentences takes multiple sentences as a
parse	GenericStanfordParser	raw_parse	sentence verbose	use stanfordparser to parse a sentence takes a sentence as a string
parse	GenericStanfordParser	raw_parse_sents	sentences verbose	use stanfordparser to parse multiple sentences takes multiple sentences as a
parse	GenericStanfordParser	tagged_parse	sentence verbose	use stanfordparser to parse a sentence takes a sentence as a list of
parse	GenericStanfordParser	tagged_parse_sents	sentences verbose	use stanfordparser to parse multiple sentences takes multiple sentences
parse	StanfordNeuralDependencyParser	tagged_parse_sents	sentences verbose	currently unimplemented because the neural dependency parser and the stanfordcorenlp pipeline class doesn't support passing in pre-
parse	EdgeI	span		return a tuple s e , where tokens[s e] is the portion of the sentence that is consistent with this
parse	EdgeI	start		return the start index of this edge's span
parse	EdgeI	end		return the end index of this edge's span
parse	EdgeI	length		return the length of this edge's span
parse	EdgeI	lhs		return this edge's left-hand side which specifies what kind of structure is hypothesized by this edge
parse	EdgeI	rhs		return this edge's right-hand side which specifies the content of the structure hypothesized by this edge
parse	EdgeI	dot		return this edge's dot position which indicates how much of the hypothesized structure is consistent with the
parse	EdgeI	nextsym		return the element of this edge's right-hand side that immediately follows its dot
parse	EdgeI	is_complete		return true if this edge's structure is fully consistent with the text
parse	EdgeI	is_incomplete		return true if this edge's structure is partially consistent with the text
parse	TreeEdge	__init__	span lhs rhs dot	construct a new treeedge
parse	TreeEdge	from_production	production index	return a new treeedge formed from the given production
parse	TreeEdge	move_dot_forward	new_end	return a new treeedge formed from this edge
parse	LeafEdge	__init__	leaf index	construct a new leafedge
parse	Chart	__init__	tokens	construct a new chart the chart is initialized with the
parse	Chart	initialize		clear the chart
parse	Chart	num_leaves		return the number of words in this chart's sentence
parse	Chart	leaf	index	return the leaf value of the word at the given index
parse	Chart	leaves		return a list of the leaf values of each word in the chart's sentence
parse	Chart	edges		return a list of all edges in this chart new edges
parse	Chart	iteredges		return an iterator over the edges in this chart it is
parse	Chart	num_edges		return the number of edges contained in this chart
parse	Chart	select		return an iterator over the edges in this chart any
parse	Chart	_add_index	restr_keys	a helper function for select, which creates a new index for a given set of attributes aka restriction keys
parse	Chart	_register_with_indexes	edge	a helper function for insert, which registers the new edge with all existing indexes
parse	Chart	insert_with_backpointer	new_edge previous_edge child_edge	add a new edge to the chart using a pointer to the previous edge
parse	Chart	insert	edge	add a new edge to the chart and return true if this operation modified the chart
parse	Chart	parses	root tree_class	return an iterator of the complete tree structures that span the entire chart and whose root node is root
parse	Chart	trees	edge tree_class complete	return an iterator of the tree structures that are associated with edge
parse	Chart	_trees	edge complete memo tree_class	a helper function for trees
parse	Chart	child_pointer_lists	edge	return the set of child pointer lists for the given edge
parse	Chart	pretty_format_edge	edge width	return a pretty-printed string representation of a given edge in this chart
parse	Chart	pretty_format_leaves	width	return a pretty-printed string representation of this chart's leaves
parse	Chart	pretty_format	width	return a pretty-printed string representation of this chart
parse	ChartRuleI	apply	chart grammar	return a generator that will add edges licensed by this rule and the given edges to the chart one at a time
parse	ChartRuleI	apply_everywhere	chart grammar	return a generator that will add all edges licensed by this rule given the edges that are currently in the
parse	ChartParser	__init__	grammar strategy trace trace_chart_width	create a new chart parser that uses grammar to parse texts
parse	ChartParser	chart_parse	tokens trace	return the final parse chart from which all possible parse trees can be extracted
parse	SteppingChartParser	initialize	tokens	begin parsing the given tokens
parse	SteppingChartParser	step		return a generator that adds edges to the chart one at a time
parse	SteppingChartParser	_parse		a generator that implements the actual parsing algorithm
parse	SteppingChartParser	strategy		return the strategy used by this parser
parse	SteppingChartParser	grammar		return the grammar used by this parser
parse	SteppingChartParser	chart		return the chart that is used by this parser
parse	SteppingChartParser	current_chartrule		return the chart rule used to generate the most recent edge
parse	SteppingChartParser	parses	tree_class	return the parse trees currently contained in the chart
parse	SteppingChartParser	set_strategy	strategy	change the strategy that the parser uses to decide which edges to add to the chart
parse	SteppingChartParser	set_grammar	grammar	change the grammar used by the parser
parse	SteppingChartParser	set_chart	chart	load a given chart into the chart parser
parse		demo	choice print_times print_grammar print_trees	a demonstration of the chart parsers
parse		load_parser	grammar_url trace parser chart_class	load a grammar from a file and build a parser based on that grammar
parse		taggedsent_to_conll	sentence	a module to convert a single pos tagged sentence into conll format
parse		taggedsents_to_conll	sentences	a module to convert the a pos tagged document stream (i
parse	TestGrammar	run	show_trees	sentences in the test suite are divided into two classes - grammatical (accept) and
parse		extract_test_sentences	string comment_chars encoding	parses a string with one test sentence per line
parse	Configuration	__init__	dep_graph	:param dep_graph the representation of an input in the form of dependency graph
parse	Configuration	_check_informative	feat flag	check whether a feature is informative
parse	Configuration	extract_features		extract the set of features for the current configuration implement standard features as describe in
parse	Transition	__init__	alg_option	:param alg_option the algorithm option of this parser currently support arc-standard and arc-eager algorithm
parse	Transition	left_arc	conf relation	note that the algorithm for left-arc is quite similar except for precondition for both arc-standard and arc-eager
parse	Transition	right_arc	conf relation	note that the algorithm for right-arc is different for arc-standard and arc-eager
parse	Transition	reduce	conf	note that the algorithm for reduce is only available for arc-eager
parse	Transition	shift	conf	note that the algorithm for shift is the same for arc-standard and arc-eager
parse	TransitionParser	__init__	algorithm	:param algorithm the algorithm option of this parser currently support arc-standard and arc-eager algorithm
parse	TransitionParser	_convert_to_binary_features	features	:param features list of feature string which is needed to convert to binary features
parse	TransitionParser	_write_to_file	key binary_features input_file	write the binary features to input file and update the transition dictionary
parse	TransitionParser	_create_training_examples_arc_std	depgraphs input_file	create the training example in the libsvm format and write it to the input_file
parse	TransitionParser	_create_training_examples_arc_eager	depgraphs input_file	create the training example in the libsvm format and write it to the input_file
parse	TransitionParser	train	depgraphs modelfile verbose	:param depgraphs : list of dependencygraph as the training data
parse	TransitionParser	parse	depgraphs modelFile	:param depgraphs the list of test sentence each sentence is represented as a dependency graph where the 'head' information is dummy
parse		demo		>>> from nltk parse import dependencygraph dependencyevaluator
parse	DependencyScorerI	train	graphs	:type graphs list dependencygraph
parse	DependencyScorerI	score	graph	:type graph dependencygraph
parse	NaiveBayesDependencyScorer	train	graphs	trains a naivebayesclassifier using the edges present in graphs list as positive examples the edges not present as
parse	NaiveBayesDependencyScorer	score	graph	converts the graph into a feature-based representation of each edge and then assigns a score to each based on the
parse	ProbabilisticNonprojectiveParser	__init__		creates a new non-projective parser
parse	ProbabilisticNonprojectiveParser	train	graphs dependency_scorer	trains a dependencyscoreri from a set of dependencygraph objects and establishes this as the parser's scorer
parse	ProbabilisticNonprojectiveParser	initialize_edge_scores	graph	assigns a score to every edge in the dependencygraph graph
parse	ProbabilisticNonprojectiveParser	collapse_nodes	new_node cycle_path g_graph b_graph	takes a list of nodes that have been identified to belong to a cycle and collapses them into on larger node
parse	ProbabilisticNonprojectiveParser	update_edge_scores	new_node cycle_path	updates the edge scores to reflect a collapse operation into new_node
parse	ProbabilisticNonprojectiveParser	compute_original_indexes	new_indexes	as nodes are collapsed into others they are replaced by the new node in the graph but it's still necessary
parse	ProbabilisticNonprojectiveParser	compute_max_subtract_score	column_index cycle_indexes	when updating scores the score of the highest-weighted incoming arc is subtracted upon collapse
parse	ProbabilisticNonprojectiveParser	best_incoming_arc	node_index	returns the source of the best incoming arc to the node with address node_index
parse	ProbabilisticNonprojectiveParser	parse	tokens tags	parses a list of tokens in accordance to the mst parsing algorithm for non-projective dependency parses
parse	NonprojectiveDependencyParser	__init__	dependency_grammar	creates a new nonprojectivedependencyparser
parse	NonprojectiveDependencyParser	parse	tokens	parses the input tokens with respect to the parser's grammar parsing
parse	ViterbiParser	__init__	grammar trace	create a new viterbiparser parser that uses grammar to parse texts
parse	ViterbiParser	trace	trace	set the level of tracing output that should be generated when parsing a text
parse	ViterbiParser	_add_constituents_spanning	span constituents tokens	find any constituents that might cover span, and add them to the most likely constituents table
parse	ViterbiParser	_find_instantiations	span constituents	:return a list of the production instantiations that cover a given span of the text
parse	ViterbiParser	_match_rhs	rhs span constituents	:return a set of all the lists of children that cover span and that match rhs
parse	ViterbiParser	_trace_production	production p span width	print trace output indicating that a given production has been applied at a given location
parse		demo		a demonstration of the probabilistic parsers the user is
parse		find_maltparser	parser_dirname	a module to find maltparser jar file and its dependencies
parse		find_malt_model	model_filename	a module to find pre-trained maltparser model
parse	MaltParser	__init__	parser_dirname model_filename tagger additional_java_args	an interface for parsing with the malt parser
parse	MaltParser	parse_tagged_sents	sentences verbose top_relation_label	use maltparser to parse multiple pos tagged sentences takes multiple
parse	MaltParser	parse_sents	sentences verbose top_relation_label	use maltparser to parse multiple sentences
parse	MaltParser	generate_malt_command	inputfilename outputfilename mode	this function generates the maltparser command use at the terminal
parse	MaltParser	train	depgraphs verbose	train maltparser from a list of dependencygraph objects
parse	MaltParser	train_from_file	conll_file verbose	train maltparser from a file
parse	ShiftReduceParser	__init__	grammar trace	create a new shiftreduceparser, that uses grammar to parse texts
parse	ShiftReduceParser	_shift	stack remaining_text	move a token from the beginning of remaining_text to the end of stack
parse	ShiftReduceParser	_match_rhs	rhs rightmost_stack	:rtype bool :return true if the right hand side of a cfg production
parse	ShiftReduceParser	_reduce	stack remaining_text production	find a cfg production whose right hand side matches the rightmost stack elements and combine those stack elements
parse	ShiftReduceParser	trace	trace	set the level of tracing output that should be generated when parsing a text
parse	ShiftReduceParser	_trace_stack	stack remaining_text marker	print trace output displaying the given stack and text
parse	ShiftReduceParser	_trace_shift	stack remaining_text	print trace output displaying that a token has been shifted
parse	ShiftReduceParser	_trace_reduce	stack production remaining_text	print trace output displaying that production was used to reduce stack
parse	ShiftReduceParser	_check_grammar		check to make sure that all of the cfg productions are potentially useful
parse	SteppingShiftReduceParser	stack		:return the parser's stack
parse	SteppingShiftReduceParser	remaining_text		:return the portion of the text that is not yet covered by the stack
parse	SteppingShiftReduceParser	initialize	tokens	start parsing a given text this sets the parser's stack to
parse	SteppingShiftReduceParser	step		perform a single parsing operation if a reduction is
parse	SteppingShiftReduceParser	shift		move a token from the beginning of the remaining text to the end of the stack
parse	SteppingShiftReduceParser	reduce	production	use production to combine the rightmost stack elements into a single tree
parse	SteppingShiftReduceParser	undo		return the parser to its state before the most recent shift or reduce operation
parse	SteppingShiftReduceParser	reducible_productions		:return a list of the productions for which reductions are available for the current parser state
parse	SteppingShiftReduceParser	parses		:return an iterator of the parses that have been found by this parser so far
parse	SteppingShiftReduceParser	set_grammar	grammar	change the grammar used to parse texts
parse		demo		a demonstration of the shift-reduce parser
parse	FeatureTreeEdge	__init__	span lhs rhs dot	construct a new edge if the edge is incomplete (i e if
parse	FeatureTreeEdge	from_production	production index	:return a new treeedge formed from the given production
parse	FeatureTreeEdge	move_dot_forward	new_end bindings	:return a new featuretreeedge formed from this edge
parse	FeatureTreeEdge	bindings		return a copy of this edge's bindings dictionary
parse	FeatureTreeEdge	variables		:return the set of variables used by this edge
parse	FeatureChart	select		returns an iterator over the edges in this chart
parse	FeatureChart	_add_index	restr_keys	a helper function for select, which creates a new index for a given set of attributes aka restriction keys
parse	FeatureChart	_register_with_indexes	edge	a helper function for insert, which registers the new edge with all existing indexes
parse	FeatureChart	_get_type_if_possible	item	helper function which returns the type feature of the item,
parse	InstantiateVarsChart	instantiate_edge	edge	if the edge is a featuretreeedge, and it is complete then instantiate all variables whose names start with '@',
parse	ParserI	grammar		:return the grammar used by this parser
parse	ParserI	parse	sent	:return an iterator that generates parse trees for the sentence
parse	ParserI	parse_sents	sents	apply self parse() to each element of sents
parse	ParserI	parse_all	sent	:rtype list tree
parse	ParserI	parse_one	sent	:rtype tree or none
parse		generate	grammar start depth n	generates an iterator of all sentences from a cfg
parse	DependencyEvaluator	__init__	parsed_sents gold_sents	:param parsed_sents the list of parsed_sents as the output of parser
parse	DependencyEvaluator	_remove_punct	inStr	function to remove punctuation from unicode string
parse	DependencyEvaluator	eval		return the labeled attachment score las and unlabeled attachment score uas
parse	BottomUpProbabilisticChartParser	__init__	grammar beam_size trace	create a new bottomupprobabilisticchartparser, that uses grammar to parse texts
parse	BottomUpProbabilisticChartParser	trace	trace	set the level of tracing output that should be generated when parsing a text
parse	BottomUpProbabilisticChartParser	sort_queue	queue chart	sort the given queue of edge objects placing the edge that should be tried first at the beginning of the queue
parse	BottomUpProbabilisticChartParser	_prune	queue chart	discard items in the queue if the queue is longer than the beam
parse	InsideChartParser	sort_queue	queue chart	sort the given queue of edges in descending order of the inside probabilities of the edges' trees
parse		demo	choice draw_parses print_parses	a demonstration of the probabilistic parsers the user is
parse	DependencyGraph	remove_by_address	address	removes the node with the given address references
parse	DependencyGraph	redirect_arcs	originals redirect	redirects arcs to any of the nodes in the originals list to the redirect node address
parse	DependencyGraph	add_arc	head_address mod_address	adds an arc from the node specified by head_address to the node specified by the mod address
parse	DependencyGraph	connect_graph		fully connects all non-root nodes all nodes are set to be dependents
parse	DependencyGraph	get_by_address	node_address	return the node with the given address
parse	DependencyGraph	contains_address	node_address	returns true if the graph contains a node with the given node address false otherwise
parse	DependencyGraph	to_dot		return a dot representation suitable for using with graphviz
parse	DependencyGraph	_repr_svg_		show svg representation of the transducer ipython magic
parse	DependencyGraph	load	filename zero_based cell_separator top_relation_label	:param filename a name of a file in malt-tab format
parse	DependencyGraph	left_children	node_index	returns the number of left children under the node specified by the given address
parse	DependencyGraph	right_children	node_index	returns the number of right children under the node specified by the given address
parse	DependencyGraph	_parse	input_ cell_extractor zero_based cell_separator	parse a sentence
parse	DependencyGraph	_tree	i	turn dependency graphs into nltk trees
parse	DependencyGraph	tree		starting with the root node build a dependency tree using the nltk tree constructor
parse	DependencyGraph	triples	node	extract dependency triples of the form
parse	DependencyGraph	contains_cycle		check whether there are cycles
parse	DependencyGraph	to_conll	style	the dependency graph in conll format
parse	DependencyGraph	nx_graph		convert the data in a nodelist into a networkx labeled directed graph
parse		malt_demo	nx	a demonstration of the result of reading a dependency version of the first sentence of the penn treebank
parse		conll_demo		a demonstration of how to read a string representation of a conll format dependency tree
parse	DependencySpan	head_index		:return an value indexing the head of the entire dependencyspan
parse	DependencySpan	__repr__		:return a concise string representatino of the dependencyspan
parse	DependencySpan	__str__		:return a verbose string representation of the dependencyspan
parse	DependencySpan	__hash__		:return the hash value of this dependencyspan
parse	ChartCell	__init__	x y	:param x this cell's x coordinate
parse	ChartCell	add	span	appends the given span to the list of spans representing the chart cell's entries
parse	ChartCell	__str__		:return a verbose string representation of this chartcell
parse	ChartCell	__repr__		:return a concise string representation of this chartcell
parse	ProjectiveDependencyParser	__init__	dependency_grammar	create a new projectivedependencyparser from a word-to-word dependency grammar dependencygrammar
parse	ProjectiveDependencyParser	parse	tokens	performs a projective dependency parse on the list of tokens using a chart-based span-concatenation algorithm similar to eisner 1996
parse	ProjectiveDependencyParser	concatenate	span1 span2	concatenates the two spans in whichever way possible this
parse	ProbabilisticProjectiveDependencyParser	__init__		create a new probabilistic dependency parser no additional
parse	ProbabilisticProjectiveDependencyParser	parse	tokens	parses the list of tokens subject to the projectivity constraint and the productions in the parser's grammar
parse	ProbabilisticProjectiveDependencyParser	concatenate	span1 span2	concatenates the two spans in whichever way possible this
parse	ProbabilisticProjectiveDependencyParser	train	graphs	trains a probabilisticdependencygrammar based on the list of input dependencygraphs
parse	ProbabilisticProjectiveDependencyParser	compute_prob	dg	computes the probability of a dependency graph based on the parser's probability model (defined by the parser's
parse		projective_rule_parse_demo		a demonstration showing the creation and use of a dependencygrammar to perform a projective dependency
parse		arity_parse_demo		a demonstration showing the creation of a dependencygrammar in which a specific number of modifiers is listed for a given
parse		projective_prob_parse_demo		a demo showing the training and use of a projective dependency parser
parse	BllipParser	__init__	parser_model reranker_features reranker_weights parser_options	load a bllip parser model from scratch you'll typically want to
parse	BllipParser	parse	sentence	use bllip parser to parse a sentence takes a sentence as a list
parse	BllipParser	tagged_parse	word_and_tag_pairs	use bllip to parse a sentence takes a sentence as a list of
parse	BllipParser	from_unified_model_dir	this_class model_dir parser_options reranker_options	create a bllipparser object from a unified parsing model directory
parse		demo		this assumes the python module bllipparser is installed
parse	IncrementalChartParser	__init__	grammar strategy trace trace_chart_width	create a new earley chart parser that uses grammar to parse texts
parse		demo	print_times print_grammar print_trees trace	a demonstration of the earley parsers
parse	RecursiveDescentParser	__init__	grammar trace	create a new recursivedescentparser, that uses grammar to parse texts
parse	RecursiveDescentParser	_parse	remaining_text tree frontier	recursively expand and match each elements of tree specified by frontier, to cover remaining_text
parse	RecursiveDescentParser	_match	rtext tree frontier	:rtype iter tree :return an iterator of all parses that can be generated by
parse	RecursiveDescentParser	_expand	remaining_text tree frontier production	:rtype iter tree :return an iterator of all parses that can be generated by
parse	RecursiveDescentParser	_production_to_tree	production	:rtype tree :return the tree that is licensed by production
parse	RecursiveDescentParser	trace	trace	set the level of tracing output that should be generated when parsing a text
parse	RecursiveDescentParser	_trace_fringe	tree treeloc	print trace output displaying the fringe of tree the
parse	RecursiveDescentParser	_trace_tree	tree frontier operation	print trace output displaying the parser's current state
parse	SteppingRecursiveDescentParser	initialize	tokens	start parsing a given text this sets the parser's tree to
parse	SteppingRecursiveDescentParser	remaining_text		:return the portion of the text that is not yet covered by the tree
parse	SteppingRecursiveDescentParser	frontier		:return a list of the tree locations of all subtrees that have not yet been expanded and all leaves that have not
parse	SteppingRecursiveDescentParser	tree		:return a partial structure for the text that is currently being parsed
parse	SteppingRecursiveDescentParser	step		perform a single parsing operation if an untried match is
parse	SteppingRecursiveDescentParser	expand	production	expand the first element of the frontier in particular if
parse	SteppingRecursiveDescentParser	match		match the first element of the frontier in particular if
parse	SteppingRecursiveDescentParser	backtrack		return the parser to its state before the most recent match or expand operation
parse	SteppingRecursiveDescentParser	expandable_productions		:return a list of all the productions for which expansions are available for the current parser state
parse	SteppingRecursiveDescentParser	untried_expandable_productions		:return a list of all the untried productions for which expansions are available for the current parser state
parse	SteppingRecursiveDescentParser	untried_match		:return whether the first element of the frontier is a token that has not yet been matched
parse	SteppingRecursiveDescentParser	currently_complete		:return whether the parser's current state represents a complete parse
parse	SteppingRecursiveDescentParser	_parse	remaining_text tree frontier	a stub version of _parse that sets the parsers current state to the given arguments
parse	SteppingRecursiveDescentParser	parses		:return an iterator of the parses that have been found by this parser so far
parse	SteppingRecursiveDescentParser	set_grammar	grammar	change the grammar used to parse texts
parse		demo		a demonstration of the recursive descent parser
ccg	CCGVar	__init__	prim_only	initialize a variable selects a new identifier
ccg	CCGVar	new_id	cls	a class method allowing generation of unique variable identifiers
ccg	CCGVar	substitute	substitutions	if there is a substitution corresponding to this variable return the substituted category
ccg	CCGVar	can_unify	other	if the variable can be replaced with other a substitution is returned
ccg	Direction	restrs		a list of restrictions on the combinators
ccg	CCGLexicon	categories	word	returns all the possible categories for a word
ccg	CCGLexicon	start		return the target category for the parser
ccg	CCGLexicon	__str__		string representation of the lexicon used for debugging
ccg		matchBrackets	string	separate the contents matching the first set of brackets from the rest of the input
ccg		nextCategory	string	separate the string for the next portion of the category from the rest
ccg		parseApplication	app	parse an application operator
ccg		parseSubscripts	subscr	parse the subscripts for a primitive category
ccg		parsePrimitiveCategory	chunks primitives families var	parse a primitive category if the primitive is the special category 'var', replace it with the
ccg		augParseCategory	line primitives families var	parse a string representing a category and returns a tuple with
ccg		fromstring	lex_str include_semantics	convert string representation into a lexicon for ccgs
chat	Chat	__init__	pairs reflections	initialize the chatbot pairs is a list of patterns and responses each
chat	Chat	_substitute	str	substitute words in the string according to the specified reflections e
chat	Chat	respond	str	generate a response to the user input
translate	IBMModel4	__init__	sentence_aligned_corpus iterations source_word_classes target_word_classes	train on sentence_aligned_corpus and create a lexical translation model distortion models a fertility model and a
translate	IBMModel4	set_uniform_probabilities	sentence_aligned_corpus	set distortion probabilities uniformly to
translate	IBMModel4	prob_t_a_given_s	alignment_info	probability of target sentence and an alignment given the
translate		longest_target_sentence_length	sentence_aligned_corpus	:param sentence_aligned_corpus parallel corpus under consideration
translate	IBMModel	set_uniform_probabilities	sentence_aligned_corpus	initialize probability tables to a uniform distribution derived classes should implement this accordingly
translate	IBMModel	sample	sentence_pair	sample the most probable alignments from the entire alignment space
translate	IBMModel	best_model2_alignment	sentence_pair j_pegged i_pegged	finds the best alignment according to ibm model 2
translate	IBMModel	hillclimb	alignment_info j_pegged	starting from the alignment in alignment_info, look at neighboring alignments iteratively for the best one
translate	IBMModel	neighboring	alignment_info j_pegged	determine the neighbors of alignment_info, obtained by
translate	IBMModel	prob_t_a_given_s	alignment_info	probability of target sentence and an alignment given the source sentence
translate	AlignmentInfo	fertility_of_i	i	fertility of word in position i of the source sentence
translate	AlignmentInfo	is_head_word	j	:return whether the word in position j of the target
translate	AlignmentInfo	center_of_cept	i	:return the ceiling of the average positions of the words in
translate	AlignmentInfo	previous_cept	j	:return the previous cept of j, or none if j belongs to
translate	AlignmentInfo	previous_in_tablet	j	:return the position of the previous word that is in the same
translate	AlignmentInfo	zero_indexed_alignment		:return zero-indexed alignment suitable for use in external nltk
translate	StackDecoder	__init__	phrase_table language_model	:param phrase_table table of translations for source language phrases and the log probabilities for those translations
translate	StackDecoder	distortion_factor		float amount of reordering of source phrases
translate	StackDecoder	translate	src_sentence	:param src_sentence sentence to be translated
translate	StackDecoder	find_all_src_phrases	src_sentence	finds all subsequences in src_sentence that have a phrase translation in the translation table
translate	StackDecoder	compute_future_scores	src_sentence	determines the approximate scores for translating every subsequence in src_sentence
translate	StackDecoder	future_score	hypothesis future_score_table sentence_length	determines the approximate score for translating the
translate	StackDecoder	expansion_score	hypothesis translation_option src_phrase_span	calculate the score of expanding hypothesis with
translate	StackDecoder	valid_phrases	all_phrases_from hypothesis	extract phrases from all_phrases_from that contains words
translate	_Hypothesis	__init__	raw_score src_phrase_span trg_phrase previous	:param raw_score likelihood of hypothesis so far
translate	_Hypothesis	score		overall score of hypothesis after accounting for local and
translate	_Hypothesis	untranslated_spans	sentence_length	starting from each untranslated word find the longest
translate	_Hypothesis	translated_positions		list of positions in the source sentence of words already translated
translate	_Stack	__init__	max_size beam_threshold	:param beam_threshold hypotheses that score less than this factor of the best hypothesis are discarded from the stack
translate	_Stack	push	hypothesis	add hypothesis to the stack
translate	_Stack	best		:return hypothesis with the highest score in the stack
translate		extract	f_start f_end e_start e_end	this function checks for alignment point consistency and extracts phrases using the chunk of consistent phrases
translate		phrase_extraction	srctext trgtext alignment max_phrase_length	phrase extraction algorithm extracts all consistent phrase pairs from a word-aligned sentence pair
translate		grow_diag_final_and	srclen trglen e2f f2e	this module symmetrisatizes the source-to-target and target-to-source word alignment output and produces aka
translate		sentence_gleu	references hypothesis min_len max_len	calculates the sentence level gleu google-bleu score described in yonghui wu mike schuster zhifeng chen quoc v
translate		corpus_gleu	list_of_references hypotheses min_len max_len	calculate a single corpus-level gleu score aka system-level gleu for all
translate	IBMModel1	__init__	sentence_aligned_corpus iterations probability_tables	train on sentence_aligned_corpus and create a lexical translation model
translate	IBMModel1	prob_all_alignments	src_sentence trg_sentence	computes the probability of all possible word alignments expressed as a marginal distribution over target words t
translate	IBMModel1	prob_alignment_point	s t	probability that word t in the target sentence is aligned to
translate	IBMModel1	prob_t_a_given_s	alignment_info	probability of target sentence and an alignment given the
translate	IBMModel1	__align	sentence_pair	determines the best word alignment for one sentence pair from the corpus that the model was trained on
translate		sentence_chrf	reference hypothesis min_len max_len	calculates the sentence level chrf character n-gram f-score described in - maja popovic
translate		corpus_chrf	list_of_references hypotheses min_len max_len	calculates the corpus level chrf character n-gram f-score it is the micro-averaged value of the sentence/segment level chrf score
translate	AlignedSent	__repr__		return a string representation for this alignedsent
translate	AlignedSent	_to_dot		dot representation of the aligned sentence
translate	AlignedSent	_repr_svg_		ipython magic : show svg representation of this alignedsent
translate	AlignedSent	__str__		return a human-readable string representation for this alignedsent
translate	AlignedSent	invert		return the aligned sentence pair reversing the directionality
translate	Alignment	fromstring	cls s	read a giza-formatted string and return an alignment object
translate	Alignment	__getitem__	key	look up the alignments that map from a given index or slice
translate	Alignment	invert		return an alignment object being the inverted mapping
translate	Alignment	range	positions	work out the range of the mapping from the given positions
translate	Alignment	__repr__		produce a giza-formatted string representing the alignment
translate	Alignment	__str__		produce a giza-formatted string representing the alignment
translate	Alignment	_build_index		build a list self _index such that self _index[i] is a list
translate		_check_alignment	num_words num_mots alignment	check whether the alignments are legal
translate	PhraseTable	translations_for	src_phrase	get the translations for a source language phrase
translate	PhraseTable	add	src_phrase trg_phrase log_prob	:type src_phrase tuple str
translate		sentence_ribes	references hypothesis alpha beta	the ribes rank-based intuitive bilingual evaluation score from hideki isozaki tsutomu hirao kevin duh katsuhito sudoh and
translate		corpus_ribes	list_of_references hypotheses alpha beta	this function "calculates ribes for a system output hypothesis with multiple references and returns "best" score among multi-references and
translate		position_of_ngram	ngram sentence	this function returns the position of the first instance of the ngram appearing in a sentence
translate		word_rank_alignment	reference hypothesis character_based	this is the word rank alignment algorithm described in the paper to produce the *worder* list i
translate		find_increasing_sequences	worder	given the *worder* list this function groups monotonic +1 sequences
translate		kendall_tau	worder normalize	calculates the kendall's tau correlation coefficient given the *worder* list of word alignments from word_rank_alignment(), using the formula
translate		spearman_rho	worder normalize	calculates the spearman's rho correlation coefficient given the *worder* list of word alignment from word_rank_alignment(), using the formula
translate	IBMModel5	__init__	sentence_aligned_corpus iterations source_word_classes target_word_classes	train on sentence_aligned_corpus and create a lexical translation model vacancy models a fertility model and a
translate	IBMModel5	set_uniform_probabilities	sentence_aligned_corpus	set vacancy probabilities uniformly to
translate	IBMModel5	sample	sentence_pair	sample the most probable alignments from the entire alignment space according to model 4
translate	IBMModel5	prune	alignment_infos	removes alignments from alignment_infos that have
translate	IBMModel5	hillclimb	alignment_info j_pegged	starting from the alignment in alignment_info, look at neighboring alignments iteratively for the best one according
translate	IBMModel5	prob_t_a_given_s	alignment_info	probability of target sentence and an alignment given the
translate	Model5Counts	update_vacancy	count alignment_info i trg_classes	:param count value to add to the vacancy counts
translate	Slots	occupy	position	:return mark slot at position as occupied
translate	Slots	vacancies_at	position	:return number of vacant slots up to and including position
translate	IBMModel2	__init__	sentence_aligned_corpus iterations probability_tables	train on sentence_aligned_corpus and create a lexical translation model and an alignment model
translate	IBMModel2	prob_all_alignments	src_sentence trg_sentence	computes the probability of all possible word alignments expressed as a marginal distribution over target words t
translate	IBMModel2	prob_alignment_point	i j src_sentence trg_sentence	probability that position j in trg_sentence is aligned to
translate	IBMModel2	prob_t_a_given_s	alignment_info	probability of target sentence and an alignment given the
translate	IBMModel2	__align	sentence_pair	determines the best word alignment for one sentence pair from the corpus that the model was trained on
translate		trace	backlinks source_sents_lens target_sents_lens	traverse the alignment cost from the tracebacks and retrieves appropriate sentence pairs
translate		align_log_prob	i j source_sents target_sents	returns the log probability of the two sentences c{source_sents[i]}, c{target_sents[j]} being aligned with a specific c{alignment}
translate		align_blocks	source_sents_lens target_sents_lens params	return the sentence alignment of two text blocks usually paragraphs
translate		align_texts	source_blocks target_blocks params	creates the sentence alignment of two texts
translate		split_at	it split_value	splits an iterator c{it} at values of c{split_value}
translate		parse_token_stream	stream soft_delimiter hard_delimiter	parses a stream of tokens and splits it into sentences (using c{soft_delimiter} tokens) and blocks (using c{hard_delimiter} tokens) for use with the l{align_texts} function
translate	IBMModel3	__init__	sentence_aligned_corpus iterations probability_tables	train on sentence_aligned_corpus and create a lexical translation model a distortion model a fertility model and a
translate	IBMModel3	prob_t_a_given_s	alignment_info	probability of target sentence and an alignment given the
translate		sentence_bleu	references hypothesis weights smoothing_function	calculate bleu score bilingual evaluation understudy from papineni kishore salim roukos todd ward and wei-jing zhu
translate		corpus_bleu	list_of_references hypotheses weights smoothing_function	calculate a single corpus-level bleu score aka system-level bleu for all
translate		modified_precision	references hypothesis n	calculate modified ngram precision
translate		closest_ref_length	references hyp_len	this function finds the reference that is the closest length to the hypothesis
translate		brevity_penalty	closest_ref_len hyp_len	calculate brevity penalty
translate	SmoothingFunction	__init__	epsilon alpha k	this will initialize the parameters required for the various smoothing techniques the default values are set to the numbers used in the
translate	SmoothingFunction	method1	p_n	smoothing method 1 add *epsilon* counts to precision with 0 counts
translate	SmoothingFunction	method2	p_n	smoothing method 2 add 1 to both numerator and denominator from chin-yew lin and franz josef och 2004 automatic evaluation of
translate	SmoothingFunction	method3	p_n	smoothing method 3 nist geometric sequence smoothing the smoothing is computed by taking 1 / 2^k ), instead of 0 for each
translate	SmoothingFunction	method4	p_n references hypothesis hyp_len	smoothing method 4 shorter translations may have inflated precision values due to having
translate	SmoothingFunction	method5	p_n references hypothesis hyp_len	smoothing method 5 the matched counts for similar values of n should be similar
translate	SmoothingFunction	method6	p_n references hypothesis hyp_len	smoothing method 6 interpolates the maximum likelihood estimate of the precision *p_n* with
translate	SmoothingFunction	method7	p_n references hypothesis hyp_len	smoothing method 6 interpolates the maximum likelihood estimate of the precision *p_n* with
translate		alignment_error_rate	reference hypothesis possible	return the alignment error rate aer of an alignment with respect to a "gold standard" reference alignment
tbl		demo		run a demo with defaults see source comments for details
tbl		demo_repr_rule_format		exemplify repr rule (see also str rule and rule format("verbose"))
tbl		demo_str_rule_format		exemplify repr rule (see also str rule and rule format("verbose"))
tbl		demo_verbose_rule_format		exemplify rule format("verbose")
tbl		demo_multiposition_feature		the feature/s of a template takes a list of positions relative to the current word where the feature should be
tbl		demo_multifeature_template		templates can have more than a single feature
tbl		demo_template_statistics		show aggregate statistics per template little used templates are
tbl		demo_generated_templates		template expand and feature expand are class methods facilitating
tbl		demo_learning_curve		plot a learning curve -- the contribution on tagging accuracy of the individual rules
tbl		demo_error_analysis		writes a file with context for each erroneous word after tagging testing data
tbl		demo_serialize_tagger		serializes the learned tagger to a file in pickle format reloads it and validates the process
tbl		demo_high_accuracy_rules		discard rules with low accuracy this may hurt performance a bit
tbl		postag	templates tagged_data num_sents max_rules	brill tagger demonstration
tbl	BrillTemplateI	applicable_rules	tokens i correctTag	return a list of the transformational rules that would correct the *i*th subtoken's tag in the given token
tbl	BrillTemplateI	get_neighborhood	token index	returns the set of indices *i* such that applicable_rules(token i
tbl	Template	__init__		construct a template for generating rules
tbl	Template	_applicable_conditions	tokens index	:returns a set of all conditions for rules that are applicable to c{tokens[index]}
tbl	Template	expand	cls featurelists combinations skipintersecting	factory method to mass generate templates from a list l of lists of features
tbl		error_list	train_sents test_sents	returns a list of human-readable strings indicating the errors in the given tagging of the corpus
tbl	Feature	__init__	positions end	construct a feature which may apply at c{positions}
tbl	Feature	expand	cls starts winlens excludezero	return a list of features one for each start point in starts and for each window length in winlen
tbl	Feature	issuperset	other	return true if this feature always returns true when other does more precisely return true if this feature refers to the same property as other
tbl	Feature	intersects	other	return true if the positions of this feature intersects with those of other more precisely return true if this feature refers to the same property as other
tbl	Feature	extract_property	tokens index	any subclass of feature must define static method extract_property tokens index
tbl	TagRule	apply	tokens positions	apply this rule at every position in positions where it applies to the given sentence
tbl	TagRule	applies	tokens index	:return true if the rule would change the tag of
tbl	Rule	__init__	templateid original_tag replacement_tag conditions	construct a new rule that changes a token's tag from c{original_tag} to c{replacement_tag} if all of the properties
tbl	Rule	format	fmt	return a string representation of this rule
tbl	Rule	_verbose_format		return a wordy human-readable string representation of the given rule
stem		suffix_replace	original old new	replaces the old suffix of the original string by a new suffix
stem	LancasterStemmer	__init__		create an instance of the lancaster stemmer
stem	LancasterStemmer	parseRules	rule_tuple	validate the set of rules used in this stemmer
stem	LancasterStemmer	stem	word	stem a word using the lancaster stemmer
stem	LancasterStemmer	__doStemming	word intact_word	perform the actual word stemming
stem	LancasterStemmer	__getLastLetter	word	get the zero-based index of the last alphabetic character in this string
stem	LancasterStemmer	__isAcceptable	word remove_total	determine if the word is acceptable for stemming
stem	LancasterStemmer	__applyRule	word remove_total append_string	apply the stemming rule to the word
stem	StemmerI	stem	token	strip affixes from the token and return the stem
stem	ISRIStemmer	stem	token	stemming a word token using the isri stemmer
stem	ISRIStemmer	pre32	word	remove length three and length two prefixes in this order
stem	ISRIStemmer	suf32	word	remove length three and length two suffixes in this order
stem	ISRIStemmer	waw	word	remove connective ‘و’ if it precedes a word beginning with ‘و’
stem	ISRIStemmer	pro_w4	word	process length four patterns and extract length three roots
stem	ISRIStemmer	pro_w53	word	process length five patterns and extract length three roots
stem	ISRIStemmer	pro_w54	word	process length five patterns and extract length four roots
stem	ISRIStemmer	end_w5	word	ending step word of length five
stem	ISRIStemmer	pro_w6	word	process length six patterns and extract length three roots
stem	ISRIStemmer	pro_w64	word	process length six patterns and extract length four roots
stem	ISRIStemmer	end_w6	word	ending step word of length six
stem	ISRIStemmer	suf1	word	normalize short sufix
stem	ISRIStemmer	pre1	word	normalize short prefix
stem	PorterStemmer	_is_consonant	word i	returns true if word[i] is a consonant false otherwise a consonant is defined in the paper as follows
stem	PorterStemmer	_measure	stem	returns the 'measure' of stem per definition in the paper from the paper
stem	PorterStemmer	_contains_vowel	stem	returns true if stem contains a vowel else false
stem	PorterStemmer	_ends_double_consonant	word	implements condition *d from the paper
stem	PorterStemmer	_ends_cvc	word	implements condition *o from the paper from the paper
stem	PorterStemmer	_replace_suffix	word suffix replacement	replaces suffix of word with replacement
stem	PorterStemmer	_apply_rule_list	word rules	applies the first applicable suffix-removal rule to the word takes a word and a list of suffix-removal rules represented as
stem	PorterStemmer	_step1a	word	implements step 1a from "an algorithm for suffix stripping"
stem	PorterStemmer	_step1b	word	implements step 1b from "an algorithm for suffix stripping" from the paper
stem	PorterStemmer	_step1c	word	implements step 1c from "an algorithm for suffix stripping"
stem	PorterStemmer	_step2	word	implements step 2 from "an algorithm for suffix stripping"
stem	PorterStemmer	_step3	word	implements step 3 from "an algorithm for suffix stripping"
stem	PorterStemmer	_step4	word	implements step 4 from "an algorithm for suffix stripping" step 4
stem	PorterStemmer	_step5a	word	implements step 5a from "an algorithm for suffix stripping"
stem	PorterStemmer	_step5b	word	implements step 5a from "an algorithm for suffix stripping"
stem		demo		a demonstration of the porter stemmer on a sample from the penn treebank corpus
stem	_LanguageSpecificStemmer	__repr__		print out the string representation of the respective class
stem	_ScandinavianStemmer	_r1_scandinavian	word vowels	return the region r1 that is used by the scandinavian stemmers
stem	_StandardStemmer	_r1r2_standard	word vowels	return the standard interpretations of the string regions r1 and r2
stem	_StandardStemmer	_rv_standard	word vowels	return the standard interpretation of the string region rv
stem	DanishStemmer	stem	word	stem a danish word and return the stemmed form
stem	DutchStemmer	stem	word	stem a dutch word and return the stemmed form
stem	EnglishStemmer	stem	word	stem an english word and return the stemmed form
stem	FinnishStemmer	stem	word	stem a finnish word and return the stemmed form
stem	FrenchStemmer	stem	word	stem a french word and return the stemmed form
stem	FrenchStemmer	__rv_french	word vowels	return the region rv that is used by the french stemmer
stem	GermanStemmer	stem	word	stem a german word and return the stemmed form
stem	HungarianStemmer	stem	word	stem an hungarian word and return the stemmed form
stem	HungarianStemmer	__r1_hungarian	word vowels digraphs	return the region r1 that is used by the hungarian stemmer
stem	ItalianStemmer	stem	word	stem an italian word and return the stemmed form
stem	NorwegianStemmer	stem	word	stem a norwegian word and return the stemmed form
stem	PortugueseStemmer	stem	word	stem a portuguese word and return the stemmed form
stem	RomanianStemmer	stem	word	stem a romanian word and return the stemmed form
stem	RussianStemmer	stem	word	stem a russian word and return the stemmed form
stem	RussianStemmer	__regions_russian	word	return the regions rv and r2 which are used by the russian stemmer
stem	RussianStemmer	__cyrillic_to_roman	word	transliterate a russian word into the roman alphabet
stem	RussianStemmer	__roman_to_cyrillic	word	transliterate a russian word back into the cyrillic alphabet
stem	SpanishStemmer	stem	word	stem a spanish word and return the stemmed form
stem	SpanishStemmer	__replace_accented	word	replaces all accented letters on a word with their non-accented counterparts
stem	SwedishStemmer	stem	word	stem a swedish word and return the stemmed form
stem		demo		this function provides a demonstration of the snowball stemmers
draw	CanvasWidget	__init__	canvas parent	create a new canvas widget this constructor should only be
draw	CanvasWidget	bbox		:return a bounding box for this canvaswidget the bounding
draw	CanvasWidget	width		:return the width of this canvas widget's bounding box in its canvas's coordinate space
draw	CanvasWidget	height		:return the height of this canvas widget's bounding box in its canvas's coordinate space
draw	CanvasWidget	parent		:return the hierarchical parent of this canvas widget
draw	CanvasWidget	child_widgets		:return a list of the hierarchical children of this canvas widget
draw	CanvasWidget	canvas		:return the canvas that this canvas widget is bound to
draw	CanvasWidget	move	dx dy	move this canvas widget by a given distance in particular
draw	CanvasWidget	moveto	x y anchor	move this canvas widget to the given location in particular
draw	CanvasWidget	destroy		remove this canvaswidget from its canvas after a
draw	CanvasWidget	update	child	update the graphical display of this canvas widget and all of its ancestors in response to a change in one of this canvas
draw	CanvasWidget	manage		arrange this canvas widget and all of its descendants
draw	CanvasWidget	tags		:return a list of the canvas tags for all graphical elements managed by this canvas widget including
draw	CanvasWidget	__setitem__	attr value	set the value of the attribute attr to value see the
draw	CanvasWidget	__getitem__	attr	:return the value of the attribute attr see the class
draw	CanvasWidget	__repr__		:return a string representation of this canvas widget
draw	CanvasWidget	hide		temporarily hide this canvas widget
draw	CanvasWidget	show		show a hidden canvas widget
draw	CanvasWidget	hidden		:return true if this canvas widget is hidden
draw	CanvasWidget	bind_click	callback button	register a new callback that will be called whenever this canvaswidget is clicked on
draw	CanvasWidget	bind_drag	callback	register a new callback that will be called after this canvaswidget is dragged
draw	CanvasWidget	unbind_click	button	remove a callback that was registered with bind_click
draw	CanvasWidget	unbind_drag		remove a callback that was registered with bind_drag
draw	CanvasWidget	__press_cb	event	handle a button-press event - record the button press event in self
draw	CanvasWidget	__start_drag	event	begin dragging this object
draw	CanvasWidget	__motion_cb	event	handle a motion event
draw	CanvasWidget	__release_cb	event	handle a release callback - unregister motion & button release callbacks
draw	CanvasWidget	__drag		if this canvaswidget has a drag callback then call it otherwise find the closest ancestor with a drag callback and
draw	CanvasWidget	__click	button	if this canvaswidget has a drag callback then call it otherwise find the closest ancestor with a click callback and
draw	CanvasWidget	_add_child_widget	child	register a hierarchical child widget the child will be
draw	CanvasWidget	_remove_child_widget	child	remove a hierarchical child widget this child will no longer
draw	CanvasWidget	_tags		:return a list of canvas tags for all graphical elements managed by this canvas widget not including graphical
draw	CanvasWidget	_manage		arrange the child widgets of this canvas widget this method
draw	CanvasWidget	_update	child	update this canvas widget in response to a change in one of its children
draw	TextWidget	__init__	canvas text	create a new text widget
draw	TextWidget	text		:return the text displayed by this text widget
draw	TextWidget	set_text	text	change the text that is displayed by this text widget
draw	SymbolWidget	__init__	canvas symbol	create a new symbol widget
draw	SymbolWidget	symbol		:return the name of the symbol that is displayed by this symbol widget
draw	SymbolWidget	set_symbol	symbol	change the symbol that is displayed by this symbol widget
draw	SymbolWidget	symbolsheet	size	open a new tkinter window that displays the entire alphabet for the symbol font
draw	AbstractContainerWidget	__init__	canvas child	create a new container widget this constructor should only
draw	AbstractContainerWidget	child		:return the child widget contained by this container widget
draw	AbstractContainerWidget	set_child	child	change the child widget contained by this container widget
draw	BoxWidget	__init__	canvas child	create a new box widget
draw	OvalWidget	__init__	canvas child	create a new oval widget
draw	ParenWidget	__init__	canvas child	create a new parenthasis widget
draw	BracketWidget	__init__	canvas child	create a new bracket widget
draw	SequenceWidget	__init__	canvas	create a new sequence widget
draw	SequenceWidget	replace_child	oldchild newchild	replace the child canvas widget oldchild with newchild
draw	SequenceWidget	remove_child	child	remove the given child canvas widget child's parent will
draw	SequenceWidget	insert_child	index child	insert a child canvas widget before a given index
draw	StackWidget	__init__	canvas	create a new stack widget
draw	StackWidget	replace_child	oldchild newchild	replace the child canvas widget oldchild with newchild
draw	StackWidget	remove_child	child	remove the given child canvas widget child's parent will
draw	StackWidget	insert_child	index child	insert a child canvas widget before a given index
draw	SpaceWidget	__init__	canvas width height	create a new space widget
draw	SpaceWidget	set_width	width	change the width of this space widget
draw	SpaceWidget	set_height	height	change the height of this space widget
draw	ScrollWatcherWidget	__init__	canvas	create a new scroll-watcher widget
draw	ScrollWatcherWidget	add_child	canvaswidget	add a new canvas widget to the scroll-watcher the
draw	ScrollWatcherWidget	remove_child	canvaswidget	remove a canvas widget from the scroll-watcher the
draw	ScrollWatcherWidget	_adjust_scrollregion		adjust the scrollregion of this scroll-watcher's canvas to include the bounding boxes of all of its children
draw	CanvasFrame	__init__	parent	create a new canvasframe
draw	CanvasFrame	print_to_file	filename	print the contents of this canvasframe to a postscript file
draw	CanvasFrame	scrollregion		:return the current scroll region for the canvas managed by this canvasframe
draw	CanvasFrame	canvas		:return the canvas managed by this canvasframe
draw	CanvasFrame	add_widget	canvaswidget x y	register a canvas widget with this canvasframe the
draw	CanvasFrame	_find_room	widget desired_x desired_y	try to find a space for a given widget
draw	CanvasFrame	destroy_widget	canvaswidget	remove a canvas widget from this canvasframe this
draw	CanvasFrame	pack	cnf	pack this canvasframe see the documentation for
draw	CanvasFrame	destroy		destroy this canvasframe if this canvasframe created a
draw	CanvasFrame	mainloop		enter the tkinter mainloop this function must be called if
draw	ShowText	mainloop		enter the tkinter mainloop this function must be called if
draw	ColorizedList	__init__	parent items	construct a new list
draw	ColorizedList	_init_colortags	textwidget options	set up any colortags that will be used by this colorized list
draw	ColorizedList	_item_repr	item	return a list of text colortag tuples that make up the colorized representation of the item
draw	ColorizedList	get	index	:return a list of the items contained by this list
draw	ColorizedList	set	items	modify the list of items contained by this list
draw	ColorizedList	unmark	item	remove highlighting from the given item or from every item if no item is given
draw	ColorizedList	mark	item	highlight the given item
draw	ColorizedList	markonly	item	remove any current highlighting and mark the given item
draw	ColorizedList	view	item	adjust the view such that the given item is visible if
draw	ColorizedList	add_callback	event func	register a callback function with the list this function
draw	ColorizedList	remove_callback	event func	deregister a callback function if func is none then
draw	MutableOptionMenu	destroy		destroy this widget and the associated menu
draw		demo		a simple demonstration showing how to use canvas widgets
draw	MultiListbox	__init__	master columns column_weights cnf	construct a new multi-column listbox widget
draw	MultiListbox	_resize_column	event	callback used to resize a column of the table return true
draw	MultiListbox	column_names		a tuple containing the names of the columns used by this multi-column listbox
draw	MultiListbox	column_labels		a tuple containing the tkinter label widgets used to
draw	MultiListbox	listboxes		a tuple containing the tkinter listbox widgets used to
draw	MultiListbox	_pagesize		:return the number of rows that makes up one page
draw	MultiListbox	select	index delta see	set the selected row if index is specified then select
draw	MultiListbox	configure	cnf	configure this widget use label_* to configure all
draw	MultiListbox	__setitem__	key val	configure this widget this is equivalent to
draw	MultiListbox	rowconfigure	row_index cnf	configure all table cells in the given row valid keyword
draw	MultiListbox	columnconfigure	col_index cnf	configure all table cells in the given column valid keyword
draw	MultiListbox	itemconfigure	row_index col_index cnf	configure the table cell at the given row and column valid
draw	MultiListbox	insert	index	insert the given row or rows into the table at the given index
draw	MultiListbox	get	first last	return the value s of the specified row s if last is
draw	MultiListbox	bbox	row col	return the bounding box for the given table cell relative to this widget's top-left corner
draw	MultiListbox	hide_column	col_index	hide the given column the column's state is still
draw	MultiListbox	show_column	col_index	display a column that has been hidden using hide_column()
draw	MultiListbox	bind_to_labels	sequence func add	add a binding to each tkinter label widget in this
draw	MultiListbox	bind_to_listboxes	sequence func add	add a binding to each tkinter listbox widget in this
draw	MultiListbox	bind_to_columns	sequence func add	add a binding to each tkinter label and tkinter listbox
draw	Table	__init__	master column_names rows column_weights	construct a new table widget
draw	Table	pack		position this table's main frame widget in its parent widget
draw	Table	grid		position this table's main frame widget in its parent widget
draw	Table	focus		direct keyboard input foxus to this widget
draw	Table	bind	sequence func add	add a binding to this table's main frame that will call func in response to the event sequence
draw	Table	rowconfigure	row_index cnf	:see multilistbox rowconfigure()
draw	Table	columnconfigure	col_index cnf	:see multilistbox columnconfigure()
draw	Table	itemconfigure	row_index col_index cnf	:see multilistbox itemconfigure()
draw	Table	bind_to_labels	sequence func add	:see multilistbox bind_to_labels()
draw	Table	bind_to_listboxes	sequence func add	:see multilistbox bind_to_listboxes()
draw	Table	bind_to_columns	sequence func add	:see multilistbox bind_to_columns()
draw	Table	insert	row_index rowvalue	insert a new row into the table so that its row index will be row_index
draw	Table	extend	rowvalues	add new rows at the end of the table
draw	Table	append	rowvalue	add a new row to the end of the table
draw	Table	clear		delete all rows in this table
draw	Table	__getitem__	index	return the value of a row or a cell in this table if
draw	Table	__setitem__	index val	replace the value of a row or a cell in this table with val
draw	Table	__delitem__	row_index	delete the row_indexth row from this table
draw	Table	__len__		:return the number of rows in this table
draw	Table	_checkrow	rowvalue	helper function check that a given row value has the correct number of elements and if not raise an exception
draw	Table	column_names		a list of the names of the columns in this table
draw	Table	column_index	i	if i is a valid column index integer then return it as is
draw	Table	hide_column	column_index	:see multilistbox hide_column()
draw	Table	show_column	column_index	:see multilistbox show_column()
draw	Table	selected_row		return the index of the currently selected row or none if no row is selected
draw	Table	select	index delta see	:see multilistbox select()
draw	Table	sort_by	column_index order	sort the rows in this table using the specified column's values as a sort key
draw	Table	_sort	event	event handler for clicking on a column label -- sort by that column
draw	Table	_fill_table	save_config	re-draw the table from scratch by clearing out the table's multi-column listbox and then filling it in with values from
draw	Table	_save_config_info	row_indices index_by_id	return a 'cookie' containing information about which row is selected and what color configurations have been applied
draw	Table	_restore_config_info	cookie index_by_id see	restore selection & color configuration information that was saved using _save_config_info
draw	Table	_check_table_vs_mlb		verify that the contents of the table's _rows variable match the contents of its multi-listbox (_mlb)
draw		dispersion_plot	text words ignore_case title	generate a lexical dispersion plot
draw	CFGEditor	_clear_tags	linenum	remove all tags (except arrow and sel) from the given line of the text widget used for editing the productions
draw	CFGEditor	_check_analyze		check if we've moved to a new line if we have then remove
draw	CFGEditor	_replace_arrows		replace any '->' text strings with arrows (char \256 in symbol font)
draw	CFGEditor	_analyze_token	match linenum	given a line number and a regexp match for a token on that line colorize the token
draw	CFGEditor	_analyze_line	linenum	colorize a given line
draw	CFGEditor	_mark_error	linenum line	mark the location of an error in a line
draw	CFGEditor	_analyze		replace -> with arrows and colorize the entire buffer
draw	CFGEditor	_parse_productions		parse the current contents of the textwidget buffer to create a list of productions
draw	TreeSegmentWidget	set_label	label	set the node label to label
draw	TreeSegmentWidget	replace_child	oldchild newchild	replace the child oldchild with newchild
draw		tree_to_treesegment	canvas t make_node make_leaf	convert a tree into a treesegmentwidget
draw	TreeWidget	expanded_tree		return the treesegmentwidget for the specified subtree
draw	TreeWidget	collapsed_tree		return the treesegmentwidget for the specified subtree
draw	TreeWidget	bind_click_trees	callback button	add a binding to all tree segments
draw	TreeWidget	bind_drag_trees	callback button	add a binding to all tree segments
draw	TreeWidget	bind_click_leaves	callback button	add a binding to all leaves
draw	TreeWidget	bind_drag_leaves	callback button	add a binding to all leaves
draw	TreeWidget	bind_click_nodes	callback button	add a binding to all nodes
draw	TreeWidget	bind_drag_nodes	callback button	add a binding to all nodes
draw	TreeWidget	toggle_collapsed	treeseg	collapse/expand a tree
draw	TreeView	mainloop		enter the tkinter mainloop this function must be called if
draw		draw_trees		open a new window containing a graphical diagram of the given trees
tokenize	PunktLanguageVars	_word_tokenizer_re		compiles and returns a regular expression for word tokenization
tokenize	PunktLanguageVars	word_tokenize	s	tokenize a string to split off punctuation other than periods
tokenize	PunktLanguageVars	period_context_re		compiles and returns a regular expression to find contexts including possible sentence boundaries
tokenize		_pair_iter	it	yields pairs of tokens from the given iterator such that each input token will appear as the first element in a yielded tuple
tokenize	PunktToken	_get_type	tok	returns a case-normalized representation of the token
tokenize	PunktToken	type_no_period		the type with its final period removed if it has one
tokenize	PunktToken	type_no_sentperiod		the type with its final period removed if it is marked as a sentence break
tokenize	PunktToken	first_upper		true if the token's first character is uppercase
tokenize	PunktToken	first_lower		true if the token's first character is lowercase
tokenize	PunktToken	is_ellipsis		true if the token text is that of an ellipsis
tokenize	PunktToken	is_number		true if the token text is that of a number
tokenize	PunktToken	is_initial		true if the token text is that of an initial
tokenize	PunktToken	is_alpha		true if the token text is all alphabetic
tokenize	PunktToken	is_non_punct		true if the token is either a number or is alphabetic
tokenize	PunktToken	__repr__		a string representation of the token that can reproduce it with eval(), which lists all the token's non-default
tokenize	PunktToken	__str__		a string representation akin to that used by kiss and strunk
tokenize	PunktBaseClass	_tokenize_words	plaintext	divide the given text into tokens using the punkt word segmentation regular expression and generate the resulting list
tokenize	PunktBaseClass	_annotate_first_pass	tokens	perform the first pass of annotation which makes decisions based purely based on the word type of each word
tokenize	PunktBaseClass	_first_pass_annotation	aug_tok	performs type-based annotation on a single token
tokenize	PunktTrainer	get_params		calculates and returns parameters for sentence boundary detection as derived from training
tokenize	PunktTrainer	train	text verbose finalize	collects training data from a given text if finalize is true it
tokenize	PunktTrainer	train_tokens	tokens verbose finalize	collects training data from a given list of tokens
tokenize	PunktTrainer	finalize_training	verbose	uses data that has been gathered in training to determine likely collocations and sentence starters
tokenize	PunktTrainer	freq_threshold	ortho_thresh type_thresh colloc_thres sentstart_thresh	allows memory use to be reduced after much training by removing data about rare tokens that are unlikely to have a statistical effect with
tokenize	PunktTrainer	_freq_threshold	fdist threshold	returns a freqdist containing only data with counts below a given threshold as well as a mapping (none -> count_removed)
tokenize	PunktTrainer	_get_orthography_data	tokens	collect information about whether each token type occurs with different case patterns i overall ii at
tokenize	PunktTrainer	_reclassify_abbrev_types	types	re classifies each given token if - it is period-final and not a known abbreviation or
tokenize	PunktTrainer	find_abbrev_types		recalculates abbreviations given type frequencies despite no prior determination of abbreviations
tokenize	PunktTrainer	_is_rare_abbrev_type	cur_tok next_tok	a word type is counted as a rare abbreviation if
tokenize	PunktTrainer	_dunning_log_likelihood	count_a count_b count_ab N	a function that calculates the modified dunning log-likelihood ratio scores for abbreviation candidates
tokenize	PunktTrainer	_col_log_likelihood	count_a count_b count_ab N	a function that will just compute log-likelihood estimate in the original paper it's described in algorithm 6 and 7
tokenize	PunktTrainer	_is_potential_collocation	aug_tok1 aug_tok2	returns true if the pair of tokens may form a collocation given log-likelihood statistics
tokenize	PunktTrainer	_find_collocations		generates likely collocations and their log-likelihood
tokenize	PunktTrainer	_is_potential_sent_starter	cur_tok prev_tok	returns true given a token and the token that preceds it if it seems clear that the token is beginning a sentence
tokenize	PunktTrainer	_find_sent_starters		uses collocation heuristics for each candidate token to determine if it frequently starts sentences
tokenize	PunktTrainer	_get_sentbreak_count	tokens	returns the number of sentence breaks marked in a given set of augmented tokens
tokenize	PunktSentenceTokenizer	__init__	train_text verbose lang_vars token_cls	train_text can either be the sole training text for this sentence boundary detector or can be a punktparameters object
tokenize	PunktSentenceTokenizer	train	train_text verbose	derives parameters from a given training text or uses the parameters given
tokenize	PunktSentenceTokenizer	tokenize	text realign_boundaries	given a text returns a list of the sentences in that text
tokenize	PunktSentenceTokenizer	debug_decisions	text	classifies candidate periods as sentence breaks yielding a dict for each that may be used to understand why the decision was made
tokenize	PunktSentenceTokenizer	span_tokenize	text realign_boundaries	given a text returns a list of the start end spans of sentences in the text
tokenize	PunktSentenceTokenizer	sentences_from_text	text realign_boundaries	given a text generates the sentences in that text by only testing candidate sentence breaks
tokenize	PunktSentenceTokenizer	_realign_boundaries	text slices	attempts to realign punctuation that falls after the period but should otherwise be included in the same sentence
tokenize	PunktSentenceTokenizer	text_contains_sentbreak	text	returns true if the given text includes a sentence break
tokenize	PunktSentenceTokenizer	sentences_from_text_legacy	text	given a text generates the sentences in that text annotates all
tokenize	PunktSentenceTokenizer	sentences_from_tokens	tokens	given a sequence of tokens generates lists of tokens each list corresponding to a sentence
tokenize	PunktSentenceTokenizer	_annotate_tokens	tokens	given a set of tokens augmented with markers for line-start and paragraph-start returns an iterator through those tokens with full
tokenize	PunktSentenceTokenizer	_build_sentence_list	text tokens	given the original text and the list of augmented word tokens construct and return a tokenized list of sentence strings
tokenize	PunktSentenceTokenizer	_annotate_second_pass	tokens	performs a token-based classification section 4 over the given tokens making use of the orthographic heuristic (4
tokenize	PunktSentenceTokenizer	_second_pass_annotation	aug_tok1 aug_tok2	performs token-based classification over a pair of contiguous tokens updating the first
tokenize	PunktSentenceTokenizer	_ortho_heuristic	aug_tok	decide whether the given token is the first token in a sentence
tokenize		demo	text tok_cls train_cls	builds a punkt model and applies it to the same text
tokenize	StanfordTokenizer	tokenize	s	use stanford tokenizer's ptbtokenizer to tokenize multiple sentences
tokenize	ReppTokenizer	tokenize	sentence	use repp to tokenize a single sentence
tokenize	ReppTokenizer	tokenize_sents	sentences keep_token_positions	tokenize multiple sentences using repp
tokenize	ReppTokenizer	generate_repp_command	inputfilename	this module generates the repp command to be used at the terminal
tokenize	ReppTokenizer	parse_repp_outputs	repp_output	this module parses the tri-tuple format that repp outputs using the "--format triple" option and returns an generator with tuple of string
tokenize	ReppTokenizer	find_repptokenizer	repp_dirname	a module to find repp tokenizer binary and its *repp set* config file
tokenize		string_span_tokenize	s sep	return the offsets of the tokens in *s*, as a sequence of start end tuples by splitting the string at each occurrence of *sep*
tokenize		regexp_span_tokenize	s regexp	return the offsets of the tokens in *s*, as a sequence of start end tuples by splitting the string at each successive match of *regexp*
tokenize		spans_to_relative	spans	return a sequence of relative spans given a sequence of spans
tokenize		is_cjk	character	python port of moses' code to check for cjk character
tokenize		xml_escape	text	this function transforms the input text into an "escaped" version suitable for well-formed xml formatting
tokenize	MWETokenizer	__init__	mwes separator	initialize the multi-word tokenizer with a list of expressions and a
tokenize	MWETokenizer	add_mwe	mwe	add a multi-word expression to the lexicon stored as a word trie we use util
tokenize	MWETokenizer	tokenize	text	:param text a list containing tokenized text :type text list str
tokenize		_replace_html_entities	text keep remove_illegal encoding	remove entities from text by converting them to their corresponding unicode character
tokenize	TweetTokenizer	tokenize	text	:param text str
tokenize		reduce_lengthening	text	replace repeated character sequences of length 3 or greater with sequences of length 3
tokenize		remove_handles	text	remove twitter username handles from text
tokenize		casual_tokenize	text preserve_case reduce_len strip_handles	convenience function for wrapping the tokenizer
tokenize	TextTilingTokenizer	tokenize	text	return a tokenized copy of *text*, where each "token" represents a separate topic
tokenize	TextTilingTokenizer	_block_comparison	tokseqs token_table	implements the block comparison method
tokenize	TextTilingTokenizer	_smooth_scores	gap_scores	wraps the smooth function from the scipy cookbook
tokenize	TextTilingTokenizer	_mark_paragraph_breaks	text	identifies indented text or line breaks as the beginning of
tokenize	TextTilingTokenizer	_divide_to_tokensequences	text	divides the text into pseudosentences of fixed size
tokenize	TextTilingTokenizer	_create_token_table	token_sequences par_breaks	creates a table of tokentablefields
tokenize	TextTilingTokenizer	_identify_boundaries	depth_scores	identifies boundaries at the peaks of similarity score
tokenize	TextTilingTokenizer	_depth_scores	scores	calculates the depth of each gap i e the average difference
tokenize	TextTilingTokenizer	_normalize_boundaries	text boundaries paragraph_breaks	normalize the boundaries identified to the original text's
tokenize		smooth	x window_len window	smooth the data using a window with requested size
tokenize	SExprTokenizer	tokenize	text	return a list of s-expressions extracted from *text*
tokenize	TokenizerI	tokenize	s	return a tokenized copy of *s*
tokenize	TokenizerI	span_tokenize	s	identify the tokens using integer offsets (start_i end_i), where s[start_i end_i] is the corresponding token
tokenize	TokenizerI	tokenize_sents	strings	apply self tokenize() to each element of strings i e :
tokenize	TokenizerI	span_tokenize_sents	strings	apply self span_tokenize() to each element of strings i e :
tokenize		sent_tokenize	text language	return a sentence-tokenized copy of *text*, using nltk's recommended sentence tokenizer
tokenize		word_tokenize	text language	return a tokenized copy of *text*, using nltk's recommended word tokenizer
tokenize		regexp_tokenize	text pattern gaps discard_empty	return a tokenized copy of *text* see :class regexptokenizer
tokenize	MosesTokenizer	penn_tokenize	text return_str	this is a python port of the penn treebank tokenizer adapted by the moses machine translation community
tokenize	MosesTokenizer	tokenize	text agressive_dash_splits return_str	python port of the moses tokenizer
tokenize	MosesDetokenizer	tokenize	tokens return_str	python port of the moses detokenizer
tokenize	MosesDetokenizer	detokenize	tokens return_str	duck-typing the abstract *tokenize()*
metrics	NgramAssocMeasures	_contingency		calculates values of a contingency table from marginal values
metrics	NgramAssocMeasures	_marginals		calculates values of contingency table marginals from its values
metrics	NgramAssocMeasures	_expected_values	cls cont	calculates expected values for a contingency table
metrics	NgramAssocMeasures	raw_freq		scores ngrams by their frequency
metrics	NgramAssocMeasures	student_t	cls	scores ngrams using student's t test with independence hypothesis for unigrams as in manning and schutze 5
metrics	NgramAssocMeasures	chi_sq	cls	scores ngrams using pearson's chi-square as in manning and schutze 5
metrics	NgramAssocMeasures	mi_like		scores ngrams using a variant of mutual information the keyword
metrics	NgramAssocMeasures	pmi	cls	scores ngrams by pointwise mutual information as in manning and schutze 5
metrics	NgramAssocMeasures	likelihood_ratio	cls	scores ngrams using likelihood ratios as in manning and schutze 5 3 4
metrics	NgramAssocMeasures	poisson_stirling	cls	scores ngrams using the poisson-stirling measure
metrics	NgramAssocMeasures	jaccard	cls	scores ngrams using the jaccard index
metrics	BigramAssocMeasures	_contingency	n_ii n_ix_xi_tuple n_xx	calculates values of a bigram contingency table from marginal values
metrics	BigramAssocMeasures	_marginals	n_ii n_oi n_io n_oo	calculates values of contingency table marginals from its values
metrics	BigramAssocMeasures	_expected_values	cont	calculates expected values for a contingency table
metrics	BigramAssocMeasures	phi_sq	cls	scores bigrams using phi-square the square of the pearson correlation coefficient
metrics	BigramAssocMeasures	chi_sq	cls n_ii n_ix_xi_tuple n_xx	scores bigrams using chi-square i e phi-sq multiplied by the number
metrics	BigramAssocMeasures	fisher	cls	scores bigrams using fisher's exact test pedersen 1996 less
metrics	BigramAssocMeasures	dice	n_ii n_ix_xi_tuple n_xx	scores bigrams using dice's coefficient
metrics	TrigramAssocMeasures	_contingency	n_iii n_iix_tuple n_ixx_tuple n_xxx	calculates values of a trigram contingency table or cube from marginal values
metrics	TrigramAssocMeasures	_marginals		calculates values of contingency table marginals from its values
metrics	QuadgramAssocMeasures	_contingency	n_iiii n_iiix_tuple n_iixx_tuple n_ixxx_tuple	calculates values of a quadgram contingency table from marginal values
metrics	QuadgramAssocMeasures	_marginals		calculates values of contingency table marginals from its values
metrics	ContingencyMeasures	__init__	measures	constructs a contingencymeasures given a ngramassocmeasures class
metrics	ContingencyMeasures	_make_contingency_fn	measures old_fn	from an association measure function produces a new function which accepts contingency table values as its arguments
metrics		accuracy	reference test	given a list of reference values and a corresponding list of test values return the fraction of corresponding values that are
metrics		precision	reference test	given a set of reference values and a set of test values return the fraction of test values that appear in the reference set
metrics		recall	reference test	given a set of reference values and a set of test values return the fraction of reference values that appear in the test set
metrics		f_measure	reference test alpha	given a set of reference values and a set of test values return the f-measure of the test values when compared against the
metrics		log_likelihood	reference test	given a list of reference values and a corresponding list of test probability distributions return the average log likelihood of
metrics		approxrand	a b	returns an approximate significance level between two lists of independently generated test values
metrics		align	str1 str2 epsilon	compute the alignment of two phonetic strings
metrics		_retrieve	i j s S	retrieve the path through the similarity matrix s starting at i j
metrics		sigma_skip	p	returns score of an indel of p
metrics		sigma_sub	p q	returns score of a substitution of p with q
metrics		sigma_exp	p q	returns score of an expansion/compression
metrics		delta	p q	return weighted sum of difference between p and q
metrics		diff	p q f	returns difference between phonetic segments p and q for feature f
metrics		R	p q	return relevant features for segment comparsion
metrics		V	p	return vowel weight if p is vowel
metrics		demo		a demonstration of the result of aligning phonetic sequences used in kondrak's 2002 dissertation
metrics	AnnotationTask	__init__	data distance	initialize an annotation task
metrics	AnnotationTask	load_array	array	load an sequence of annotation results appending to any data already loaded
metrics	AnnotationTask	agr	cA cB i data	agreement between two coders on a given item
metrics	AnnotationTask	N	k i c	implements the "n-notation" used in artstein and poesio 2007
metrics	AnnotationTask	Ao	cA cB	observed agreement between two coders on all items
metrics	AnnotationTask	_pairwise_average	function	calculates the average of function results for each coder pair
metrics	AnnotationTask	avg_Ao		average observed agreement across all coders and items
metrics	AnnotationTask	Do_alpha		the observed disagreement for the alpha coefficient
metrics	AnnotationTask	Do_Kw_pairwise	cA cB max_distance	the observed disagreement for the weighted kappa coefficient
metrics	AnnotationTask	Do_Kw	max_distance	averaged over all labelers
metrics	AnnotationTask	S		bennett albert and goldstein 1954
metrics	AnnotationTask	pi		scott 1955 here multi-pi
metrics	AnnotationTask	kappa		cohen 1960 averages naively over kappas for each coder pair
metrics	AnnotationTask	multi_kappa		davies and fleiss 1982 averages over observed and expected agreements for each coder pair
metrics		get_words_from_dictionary	lemmas	get original set of words used for analysis
metrics		_truncate	words cutlength	group words by stems defined by truncating them at given length
metrics		_count_intersection	l1 l2	count intersection between two line segments defined by coordinate pairs
metrics		_get_derivative	coordinates	get derivative of the line from 0 0 to given coordinates
metrics		_calculate_cut	lemmawords stems	count understemmed and overstemmed pairs for lemma stem pair with common words
metrics		_calculate	lemmas stems	calculate actual and maximum possible amounts of understemmed and overstemmed word pairs
metrics		_indexes	gumt gdmt gwmt gdnt	count understemming index ui overstemming index oi and stemming weight sw
metrics	Paice	__init__	lemmas stems	:param lemmas a dictionary where keys are lemmas and values are sets or lists of words corresponding to that lemma
metrics	Paice	_get_truncation_indexes	words cutlength	count ui oi when stemming is done by truncating words at 'cutlength'
metrics	Paice	_get_truncation_coordinates	cutlength	count ui oi pairs for truncation points until we find the segment where ui oi crosses the truncation line
metrics	Paice	_errt		count error-rate relative to truncation errt
metrics	Paice	update		update statistics after lemmas and stems have been set
metrics		demo		demonstration of the module
metrics		windowdiff	seg1 seg2 k boundary	compute the windowdiff score for a pair of segmentations a
metrics		ghd	ref hyp ins_cost del_cost	compute the generalized hamming distance for a reference and a hypothetical segmentation corresponding to the cost related to the transformation
metrics		pk	ref hyp k boundary	compute the pk metric for a pair of segmentations a segmentation is any sequence over a vocabulary of two items (e
metrics		_rank_dists	ranks1 ranks2	finds the difference between the values in ranks1 and ranks2 for keys present in both dicts
metrics		spearman_correlation	ranks1 ranks2	returns the spearman correlation coefficient for two rankings which should be dicts or sequences of key rank
metrics		ranks_from_sequence	seq	given a sequence yields each element with an increasing rank suitable for use as an argument to spearman_correlation
metrics		ranks_from_scores	scores rank_gap	given a sequence of key score tuples yields each key with an increasing rank tying with previous key's rank if the difference between
metrics	ConfusionMatrix	__init__	reference test sort_by_count	construct a new confusion matrix from a list of reference values and a corresponding list of test values
metrics	ConfusionMatrix	__getitem__	li_lj_tuple	:return the number of times that value li was expected and value lj was given
metrics	ConfusionMatrix	pretty_format	show_percents values_in_chart truncate sort_by_count	:return a multi-line string representation of this confusion matrix
metrics		edit_distance	s1 s2 substitution_cost transpositions	calculate the levenshtein edit-distance between two strings
metrics		binary_distance	label1 label2	simple equality test
metrics		jaccard_distance	label1 label2	distance metric comparing set-similarity
metrics		masi_distance	label1 label2	distance metric that takes into account partial agreement when multiple labels are assigned
metrics		interval_distance	label1 label2	krippendorff's interval distance metric >>> from nltk
metrics		presence	label	higher-order function to test presence of a given label
twitter		credsfromfile	creds_file subdir verbose	convenience function for authentication
twitter	Authenticate	load_creds	creds_file subdir verbose	read oauth credentials from a text file
twitter	Authenticate	_validate_creds_file	verbose	check validity of a credentials file
twitter		add_access_token	creds_file	for oauth 2 retrieve an access token for an app and append it to a credentials file
twitter		guess_path	pth	if the path is not absolute guess that it is a subdirectory of the user's home directory
twitter		extract_fields	tweet fields	extract field values from a full tweet and return them as a list
twitter		json2csv	fp outfile fields encoding	extract selected fields from a file of line-separated json tweets and write to a file in csv format
twitter		outf_writer_compat	outfile encoding errors gzip_compress	identify appropriate csv writer given the python version
twitter		json2csv_entities	tweets_file outfile main_fields entity_type	extract selected fields from a file of line-separated json tweets and write to a file in csv format
twitter		verbose	func	decorator for demo functions
twitter		yesterday		get yesterday's datetime as a 5-tuple
twitter		setup		initialize global variables for the demos
twitter		twitterclass_demo		use the simplified :class twitter class to write some tweets to a file
twitter		sampletoscreen_demo	limit	sample from the streaming api and send output to terminal
twitter		tracktoscreen_demo	track limit	track keywords from the public streaming api and send output to terminal
twitter		search_demo	keywords	use the rest api to search for past tweets containing a given keyword
twitter		tweets_by_user_demo	user count	use the rest api to search for past tweets by a given user
twitter		lookup_by_userid_demo		use the rest api to convert a userid to a screen name
twitter		followtoscreen_demo	limit	using the streaming api select just the tweets from a specified list of userids
twitter		streamtofile_demo	limit	write 20 tweets sampled from the public streaming api to a file
twitter		limit_by_time_demo	keywords	query the rest api for tweets about nltk since yesterday and send the output to terminal
twitter		corpusreader_demo		use :module twittercorpusreader tp read a file of tweets and print out * some full tweets in json format
twitter		expand_tweetids_demo		given a file object containing a list of tweet ids fetch the corresponding full tweets if available
twitter	LocalTimezoneOffsetWithUTC	utcoffset	dt	access the relevant time offset
twitter	BasicTweetHandler	do_continue		returns false if the client should stop fetching tweets
twitter	TweetHandlerI	__init__	limit upper_date_limit lower_date_limit	:param int limit the number of data items to process in the current round of processing
twitter	TweetHandlerI	handle	data	deal appropriately with data returned by the twitter api
twitter	TweetHandlerI	on_finish		actions when the tweet limit has been reached
twitter	TweetHandlerI	check_date_limit	data verbose	validate date limits
twitter	Streamer	register	handler	register a method for handling tweets
twitter	Streamer	on_success	data	:param data response from twitter api
twitter	Streamer	on_error	status_code data	:param status_code the status code returned by the twitter api
twitter	Streamer	sample		wrapper for 'statuses / sample' api call
twitter	Streamer	filter	track follow lang	wrapper for 'statuses / filter' api call
twitter	Query	register	handler	register a method for handling tweets
twitter	Query	expand_tweetids	ids_f verbose	given a file object containing a list of tweet ids fetch the corresponding full tweets from the twitter api
twitter	Query	_search_tweets	keywords limit lang	assumes that the handler has been informed fetches tweets from
twitter	Query	search_tweets	keywords limit lang max_id	call the rest api 'search/tweets' endpoint with some plausible defaults
twitter	Query	user_info_from_id	userids	convert a list of userids into a variety of information about the users
twitter	Query	user_tweets	screen_name limit include_rts	return a collection of the most recent tweets posted by the user
twitter	Twitter	tweets	keywords follow to_screen stream	process some tweets in a simple manner
twitter	TweetViewer	handle	data	direct data to sys stdout
twitter	TweetWriter	__init__	limit upper_date_limit lower_date_limit fprefix	the difference between the upper and lower date limits depends on whether tweets are coming in an ascending date order (i
twitter	TweetWriter	timestamped_file		:return timestamped file name
twitter	TweetWriter	handle	data	write twitter data as line-delimited json into one or more files
classify	TextCat	remove_punctuation	text	get rid of punctuation except apostrophes
classify	TextCat	profile	text	create freqdist of trigrams within text
classify	TextCat	calc_dist	lang trigram text_profile	calculate the "out-of-place" measure between the
classify	TextCat	lang_dists	text	calculate the "out-of-place" measure between
classify	TextCat	guess_language	text	find the language with the min distance
classify	ARFF_Formatter	__init__	labels features	:param labels a list of all class labels that can be generated
classify	ARFF_Formatter	format	tokens	returns a string representation of arff output for the given data
classify	ARFF_Formatter	labels		returns the list of classes
classify	ARFF_Formatter	write	outfile tokens	writes arff data to a file for the given data
classify	ARFF_Formatter	from_train	tokens	constructs an arff_formatter instance with class labels and feature types determined from the given data
classify	ARFF_Formatter	header_section		returns an arff header as a string
classify	ARFF_Formatter	data_section	tokens labeled	returns the arff data section for the given data
classify		apply_features	feature_func toks labeled	use the lazymap class to construct a lazy list-like object that is analogous to map(feature_func toks)
classify		attested_labels	tokens	:return a list of all labels that are attested in the given list of tokens
classify		ne	token	this just assumes that words in all caps or titles are named entities
classify		lemmatize	word	use morphy from wordnet to find the base form of verbs
classify	RTEFeatureExtractor	__init__	rtepair stop lemmatize	:param rtepair a rtepair from which features should be extracted
classify	RTEFeatureExtractor	overlap	toktype debug	compute the overlap between text and hypothesis
classify	RTEFeatureExtractor	hyp_extra	toktype debug	compute the extraneous material in the hypothesis
classify	NaiveBayesClassifier	__init__	label_probdist feature_probdist	:param label_probdist p label the probability distribution over labels
classify	NaiveBayesClassifier	most_informative_features	n	return a list of the 'most informative' features used by this classifier
classify	NaiveBayesClassifier	train	cls labeled_featuresets estimator	:param labeled_featuresets a list of classified featuresets i
classify	PositiveNaiveBayesClassifier	train	positive_featuresets unlabeled_featuresets positive_prob_prior estimator	:param positive_featuresets a list of featuresets that are known as positive examples (i
classify	MaxentClassifier	__init__	encoding weights logarithmic	construct a new maxent classifier model typically new
classify	MaxentClassifier	set_weights	new_weights	set the feature weight vector for this classifier
classify	MaxentClassifier	weights		:return the feature weight vector for this classifier
classify	MaxentClassifier	explain	featureset columns	print a table showing the effect of each of the features in the given feature set and how they combine to determine the
classify	MaxentClassifier	show_most_informative_features	n show	:param show all neg or pos for negative-only or positive-only
classify	MaxentClassifier	train	cls train_toks algorithm trace	train a new maxent classifier based on the given corpus of training samples
classify	MaxentFeatureEncodingI	encode	featureset label	given a featureset label pair return the corresponding vector of joint-feature values
classify	MaxentFeatureEncodingI	length		:return the size of the fixed-length joint-feature vectors that are generated by this encoding
classify	MaxentFeatureEncodingI	labels		:return a list of the "known labels" -- i e all labels
classify	MaxentFeatureEncodingI	describe	fid	:return a string describing the value of the joint-feature whose index in the generated feature vectors is fid
classify	MaxentFeatureEncodingI	train	cls train_toks	construct and return new feature encoding based on a given training corpus train_toks
classify	FunctionBackedMaxentFeatureEncoding	__init__	func length labels	construct a new feature encoding based on the given function
classify	BinaryMaxentFeatureEncoding	__init__	labels mapping unseen_features alwayson_features	:param labels a list of the "known labels" for this encoding
classify	BinaryMaxentFeatureEncoding	train	cls train_toks count_cutoff labels	construct and return new feature encoding based on a given training corpus train_toks
classify	GISEncoding	__init__	labels mapping unseen_features alwayson_features	:param c the correction constant the value of the correction
classify	GISEncoding	C		the non-negative constant that all encoded feature vectors will sum to
classify	TypedMaxentFeatureEncoding	__init__	labels mapping unseen_features alwayson_features	:param labels a list of the "known labels" for this encoding
classify	TypedMaxentFeatureEncoding	train	cls train_toks count_cutoff labels	construct and return new feature encoding based on a given training corpus train_toks
classify		train_maxent_classifier_with_gis	train_toks trace encoding labels	train a new conditionalexponentialclassifier, using the given training samples using the generalized iterative scaling
classify		train_maxent_classifier_with_iis	train_toks trace encoding labels	train a new conditionalexponentialclassifier, using the given training samples using the improved iterative scaling algorithm
classify		calculate_nfmap	train_toks encoding	construct a map that can be used to compress nf which is typically sparse
classify		calculate_deltas	train_toks classifier unattested ffreq_empirical	calculate the update values for the classifier weights for this iteration of iis
classify		train_maxent_classifier_with_megam	train_toks trace encoding labels	train a new conditionalexponentialclassifier, using the given training samples using the external megam library
classify	Senna	executable	base_path	the function that determines the system specific binary that should be used in the pipeline
classify	Senna	_map		a method that calculates the order of the columns that senna pipeline will output the tags into
classify	Senna	tag	tokens	applies the specified operation s on a list of tokens
classify	Senna	tag_sents	sentences	applies the tag method over a list of sentences this method will return a
classify	ClassifierI	labels		:return the list of category labels used by this classifier
classify	ClassifierI	classify	featureset	:return the most appropriate label for the given featureset
classify	ClassifierI	prob_classify	featureset	:return a probability distribution over labels for the given featureset
classify	ClassifierI	classify_many	featuresets	apply self classify() to each element of featuresets i e :
classify	ClassifierI	prob_classify_many	featuresets	apply self prob_classify() to each element of featuresets i e :
classify	MultiClassifierI	labels		:return the list of category labels used by this classifier
classify	MultiClassifierI	classify	featureset	:return the most appropriate set of labels for the given featureset
classify	MultiClassifierI	prob_classify	featureset	:return a probability distribution over sets of labels for the given featureset
classify	MultiClassifierI	classify_many	featuresets	apply self classify() to each element of featuresets i e :
classify	MultiClassifierI	prob_classify_many	featuresets	apply self prob_classify() to each element of featuresets i e :
classify	DecisionTreeClassifier	__init__	label feature_name decisions default	:param label the most likely label for tokens that reach this node in the decision tree
classify	DecisionTreeClassifier	pretty_format	width prefix depth	return a string containing a pretty-printed version of this decision tree
classify	DecisionTreeClassifier	pseudocode	prefix depth	return a string representation of this decision tree that expresses the decisions it makes as a nested set of pseudocode
classify	DecisionTreeClassifier	train	labeled_featuresets entropy_cutoff depth_cutoff support_cutoff	:param binary if true then treat all feature/value pairs as individual binary features rather than using a single n-way
classify	SklearnClassifier	__init__	estimator dtype sparse	:param estimator scikit-learn classifier object
classify	SklearnClassifier	classify_many	featuresets	classify a batch of samples
classify	SklearnClassifier	prob_classify_many	featuresets	compute per-class probabilities for a batch of samples
classify	SklearnClassifier	labels		the class labels used by this classifier
classify	SklearnClassifier	train	labeled_featuresets	train fit the scikit-learn estimator
classify		write_tadm_file	train_toks encoding stream	generate an input file for tadm based on the given corpus of classified tokens
classify		parse_tadm_weights	paramfile	given the stdout output generated by tadm when training a model return a numpy array containing the corresponding weight
classify		call_tadm	args	call the tadm binary with the given arguments
classify		config_megam	bin	configure nltk's interface to the megam maxent optimization package
classify		write_megam_file	train_toks encoding stream bernoulli	generate an input file for megam based on the given corpus of classified tokens
classify		parse_megam_weights	s features_count explicit	given the stdout output generated by megam when training a model return a numpy array containing the corresponding weight
classify		call_megam	args	call the megam binary with the given arguments
inference	TableauProverCommand	__init__	goal assumptions prover	:param goal input expression to prove :type goal sem
inference	Agenda	pop_first		pop the first expression that appears in the agenda
inference	ClosedDomainProver	replace_quants	ex domain	apply the closed domain assumption to the expression - domain = union([e
inference	UniqueNamesProver	assumptions		- domain = union([e free()|e constants() for e in all_expressions])
inference	SetHolder	__getitem__	item	:param item variable
inference	ClosedWorldProver	_make_unique_signature	predHolder	this method figures out how many arguments the predicate takes and returns a tuple containing that number of unique variables
inference	ClosedWorldProver	_make_antecedent	predicate signature	return an application expression with 'predicate' as the predicate and 'signature' as the list of arguments
inference	ClosedWorldProver	_make_predicate_dict	assumptions	create a dictionary of predicates from the assumptions
inference	MaceCommand	__init__	goal assumptions max_models model_builder	:param goal input expression to prove :type goal sem
inference	MaceCommand	_convert2val	valuation_str	transform the output file into an nltk-style valuation
inference	MaceCommand	_make_relation_set	num_entities values	convert a mace4-style relation table into a dictionary
inference	MaceCommand	_make_model_var	value	pick an alphabetic character as identifier for an entity in the model
inference	MaceCommand	_decorate_model	valuation_str format	print out a mace4 model using any mace4 interpformat format
inference	MaceCommand	_transform_output	valuation_str format	transform the output file into any mace4 interpformat format
inference	MaceCommand	_call_interpformat	input_str args verbose	call the interpformat binary with the given input
inference	Mace	_build_model	goal assumptions verbose	use mace4 to build a first order model
inference	Mace	_call_mace4	input_str args verbose	call the mace4 binary with the given input
inference		decode_result	found	decode the result of model_found()
inference		test_model_found	arguments	try some proofs and exhibit the results
inference		test_build_model	arguments	try to build a nltk sem valuation
inference		test_transform_output	argument_pair	transform the model into various mace4 interpformat formats
inference	Prover	prove	goal assumptions verbose	:return whether the proof was successful or not
inference	Prover	_prove	goal assumptions verbose	:return whether the proof was successful or not along with the proof
inference	ModelBuilder	build_model	goal assumptions verbose	perform the actual model building
inference	ModelBuilder	_build_model	goal assumptions verbose	perform the actual model building
inference	TheoremToolCommand	add_assumptions	new_assumptions	add new assumptions to the assumption list
inference	TheoremToolCommand	retract_assumptions	retracted debug	retract assumptions from the assumption list
inference	TheoremToolCommand	assumptions		list the current assumptions
inference	TheoremToolCommand	goal		return the goal
inference	TheoremToolCommand	print_assumptions		print the list of the current assumptions
inference	ProverCommand	prove	verbose	perform the actual proof
inference	ProverCommand	proof	simplify	return the proof string
inference	ProverCommand	get_prover		return the prover object
inference	ModelBuilderCommand	build_model	verbose	perform the actual model building
inference	ModelBuilderCommand	model	format	return a string representation of the model
inference	ModelBuilderCommand	get_model_builder		return the model builder object
inference	BaseTheoremToolCommand	__init__	goal assumptions	:param goal input expression to prove :type goal sem
inference	BaseTheoremToolCommand	add_assumptions	new_assumptions	add new assumptions to the assumption list
inference	BaseTheoremToolCommand	retract_assumptions	retracted debug	retract assumptions from the assumption list
inference	BaseTheoremToolCommand	assumptions		list the current assumptions
inference	BaseTheoremToolCommand	goal		return the goal
inference	BaseTheoremToolCommand	print_assumptions		print the list of the current assumptions
inference	BaseProverCommand	__init__	prover goal assumptions	:param prover the theorem tool to execute with the assumptions
inference	BaseProverCommand	prove	verbose	perform the actual proof store the result to prevent unnecessary
inference	BaseProverCommand	proof	simplify	return the proof string
inference	BaseProverCommand	decorate_proof	proof_string simplify	modify and return the proof string
inference	BaseModelBuilderCommand	__init__	modelbuilder goal assumptions	:param modelbuilder the theorem tool to execute with the assumptions
inference	BaseModelBuilderCommand	build_model	verbose	attempt to build a model store the result to prevent unnecessary
inference	BaseModelBuilderCommand	model	format	return a string representation of the model
inference	BaseModelBuilderCommand	_decorate_model	valuation_str format	:param valuation_str str with the model builder's output
inference	TheoremToolCommandDecorator	__init__	command	:param command theoremtoolcommand to decorate
inference	ProverCommandDecorator	__init__	proverCommand	:param provercommand provercommand to decorate
inference	ProverCommandDecorator	proof	simplify	return the proof string
inference	ProverCommandDecorator	decorate_proof	proof_string simplify	modify and return the proof string
inference	ModelBuilderCommandDecorator	__init__	modelBuilderCommand	:param modelbuildercommand modelbuildercommand to decorate
inference	ModelBuilderCommandDecorator	build_model	verbose	attempt to build a model store the result to prevent unnecessary
inference	ModelBuilderCommandDecorator	model	format	return a string representation of the model
inference	ModelBuilderCommandDecorator	_decorate_model	valuation_str format	modify and return the proof string
inference	ResolutionProver	_prove	goal assumptions verbose	:param goal input expression to prove :type goal sem
inference	ResolutionProverCommand	__init__	goal assumptions prover	:param goal input expression to prove :type goal sem
inference	ResolutionProverCommand	prove	verbose	perform the actual proof store the result to prevent unnecessary
inference	ResolutionProverCommand	_decorate_clauses	clauses	decorate the proof output
inference	Clause	unify	other bindings used skipped	attempt to unify this clause with the other returning a list of resulting unified clauses
inference	Clause	isSubsetOf	other	return true iff every term in 'self' is a term in 'other'
inference	Clause	subsumes	other	return true iff 'self' subsumes 'other', this is if there is a substitution such that every term in 'self' can be unified with a term
inference	Clause	is_tautology		self is a tautology if it contains ground terms p and -p the ground
inference	Clause	replace	variable expression	replace every instance of variable with expression across every atom
inference	Clause	substitute_bindings	bindings	replace every binding
inference		_iterate_first	first second bindings used	this method facilitates movement through the terms of 'self'
inference		_iterate_second	first second bindings used	this method facilitates movement through the terms of 'other'
inference		_unify_terms	a b bindings used	this method attempts to unify two terms two expressions are unifiable
inference		clausify	expression	skolemize clausify and standardize the variables apart
inference		_clausify	expression	:param expression a skolemized expression in cnf
inference	BindingDict	__init__	binding_list	:param binding_list list of (abstractvariableexpression, atomicexpression) to initialize the dictionary
inference	BindingDict	__setitem__	variable binding	a binding is consistent with the dict if its variable is not already bound or if its variable is already bound to its argument
inference	BindingDict	__getitem__	variable	return the expression to which 'variable' is bound
inference	BindingDict	__add__	other	:param other bindingdict the dict with which to combine self
inference		most_general_unification	a b bindings	find the most general unification of the two given expressions
inference	ReadingCommand	parse_to_readings	sentence	:param sentence the sentence to read
inference	ReadingCommand	process_thread	sentence_readings	this method should be used to handle dependencies between readings such as resolving anaphora
inference	ReadingCommand	combine_readings	readings	:param readings readings to combine
inference	ReadingCommand	to_fol	expression	convert this expression into a first-order logic expression
inference	CfgReadingCommand	__init__	gramfile	:param gramfile name of file where grammar can be loaded
inference	CfgReadingCommand	parse_to_readings	sentence	:see readingcommand parse_to_readings()
inference	CfgReadingCommand	combine_readings	readings	:see readingcommand combine_readings()
inference	CfgReadingCommand	to_fol	expression	:see readingcommand to_fol()
inference	DrtGlueReadingCommand	__init__	semtype_file remove_duplicates depparser	:param semtype_file name of file where grammar can be loaded
inference	DrtGlueReadingCommand	parse_to_readings	sentence	:see readingcommand parse_to_readings()
inference	DrtGlueReadingCommand	process_thread	sentence_readings	:see readingcommand process_thread()
inference	DrtGlueReadingCommand	combine_readings	readings	:see readingcommand combine_readings()
inference	DrtGlueReadingCommand	to_fol	expression	:see readingcommand to_fol()
inference	DiscourseTester	__init__	input reading_command background	initialize a discoursetester
inference	DiscourseTester	sentences		display the list of sentences in the current discourse
inference	DiscourseTester	add_sentence	sentence informchk consistchk	add a sentence to the current discourse
inference	DiscourseTester	retract_sentence	sentence verbose	remove a sentence from the current discourse
inference	DiscourseTester	grammar		print out the grammar in use for parsing input sentences
inference	DiscourseTester	_get_readings	sentence	build a list of semantic readings for a sentence
inference	DiscourseTester	_construct_readings		use self _sentences to construct a value for self _readings
inference	DiscourseTester	_construct_threads		use self _readings to construct a value for self _threads
inference	DiscourseTester	_show_readings	sentence	print out the readings for the discourse or a single sentence
inference	DiscourseTester	_show_threads	filter show_thread_readings	print out the value of self _threads or self _filtered_hreads
inference	DiscourseTester	readings	sentence threaded verbose filter	construct and show the readings of the discourse or of a single sentence
inference	DiscourseTester	expand_threads	thread_id threads	given a thread id find the list of logic expression objects corresponding to the reading ids in that thread
inference	DiscourseTester	models	thread_id show verbose	call mace4 to build a model for each current discourse thread
inference	DiscourseTester	add_background	background verbose	add a list of background assumptions for reasoning about the discourse
inference	DiscourseTester	background		show the current background assumptions
inference	DiscourseTester	multiply	discourse readings	multiply every thread in discourse by every reading in readings
inference		load_fol	s	temporarily duplicated from nltk sem util
inference		discourse_demo	reading_command	illustrate the various methods of discoursetester
inference		drt_discourse_demo	reading_command	illustrate the various methods of discoursetester
inference	Prover9CommandParent	print_assumptions	output_format	print the list of the current assumptions
inference	Prover9Command	__init__	goal assumptions timeout prover	:param goal input expression to prove :type goal sem
inference	Prover9Command	decorate_proof	proof_string simplify	:see baseprovercommand decorate_proof()
inference	Prover9Parent	prover9_input	goal assumptions	:return the input string that should be provided to the prover9 binary
inference	Prover9Parent	binary_locations		a list of directories that should be searched for the prover9 executables
inference	Prover9Parent	_call	input_str binary args verbose	call the binary with the given input
inference		convert_to_prover9	input	convert a logic expression to prover9 format
inference		_convert_to_prover9	expression	convert logic expression to prover9 formatted string
inference	Prover9	_prove	goal assumptions verbose	use prover9 to prove a theorem
inference	Prover9	prover9_input	goal assumptions	:see prover9parent prover9_input
inference	Prover9	_call_prover9	input_str args verbose	call the prover9 binary with the given input
inference	Prover9	_call_prooftrans	input_str args verbose	call the prooftrans binary with the given input
inference		test_convert_to_prover9	expr	test that parsing works ok
inference		test_prove	arguments	try some proofs and exhibit the results
sem	FStructure	safeappend	key item	append 'item' to the list at 'key' if no list exists for 'key', then
sem	FStructure	_make_label	value	pick an alphabetic character as identifier for an entity in the model
sem	Boxer	__init__	boxer_drs_interpreter elimeq bin_dir verbose	:param boxer_drs_interpreter a class that converts from the abstractboxerdrs object hierarchy to a different object
sem	Boxer	interpret	input discourse_id question verbose	use boxer to give a first order representation
sem	Boxer	interpret_multi	input discourse_id question verbose	use boxer to give a first order representation
sem	Boxer	interpret_sents	inputs discourse_ids question verbose	use boxer to give a first order representation
sem	Boxer	interpret_multi_sents	inputs discourse_ids question verbose	use boxer to give a first order representation
sem	Boxer	_call_candc	inputs discourse_ids question verbose	call the candc binary with the given input
sem	Boxer	_call_boxer	candc_out verbose	call the boxer binary with the given input
sem	Boxer	_call	input_str binary args verbose	call the binary with the given input
sem	BoxerOutputDrsParser	__init__	discourse_id	this class is used to parse the prolog drs output from boxer into a hierarchy of python objects
sem	BoxerOutputDrsParser	parse_condition	indices	parse a drs condition
sem	BoxerOutputDrsParser	handle_condition	tok indices	handle a drs condition
sem	BoxerOutputDrsParser	_sent_and_word_indices	indices	:return list of (sent_index word_indices) tuples
sem	AbstractBoxerDrs	variables		:return (set<variables>, set<events>, set<propositions>)
sem	AbstractBoxerDrs	_variables		:return (set<variables>, set<events>, set<propositions>)
sem	NltkDrtBoxerDrsInterpreter	interpret	ex	:param ex abstractboxerdrs
sem		_expand	type	expand an ne class name
sem		class_abbrev	type	abbreviate an ne class name
sem		_join	lst sep untag	join a list into a string turning tags tuples into tag strings or just words
sem		descape_entity	m defs	translate one entity to its iso latin value
sem		list2sym	lst	convert a list of strings into a canonical symbol
sem		tree2semi_rel	tree	group a chunk structure into a list of 'semi-relations' of the form (list str tree)
sem		semi_rel2reldict	pairs window trace	converts the pairs generated by tree2semi_rel into a 'reldict': a dictionary which stores information about the subject and object nes plus the filler between them
sem		extract_rels	subjclass objclass doc corpus	filter the output of semi_rel2reldict according to specified ne classes and a filler pattern
sem		rtuple	reldict lcon rcon	pretty print the reldict as an rtuple
sem		clause	reldict relsym	print the relation in clausal form
sem		in_demo	trace sql	select pairs of organizations and locations whose mentions occur with an intervening occurrence of the preposition "in"
sem		conllned	trace	find the copula+'van' relation ('of') in the dutch tagged training corpus from conll 2002
sem		parse_sents	inputs grammar trace	convert input sentences into syntactic trees
sem		root_semrep	syntree semkey	find the semantic representation at the root of a tree
sem		interpret_sents	inputs grammar semkey trace	add the semantic representation to each syntactic parse tree of each input sentence
sem		evaluate_sents	inputs grammar model assignment	add the truth-in-a-model value to each semantic representation for each syntactic parse of each input sentences
sem		demo_legacy_grammar		check that interpret_sents() is compatible with legacy grammars that use a lowercase 'sem' feature
sem	Concept	__init__	prefLabel arity altLabels closures	:param preflabel the preferred label for the concept
sem	Concept	augment	data	add more data to the concept's extension set
sem	Concept	_make_graph	s	convert a set of pairs into an adjacency linked list encoding of a graph
sem	Concept	_transclose	g	compute the transitive closure of a graph represented as a linked list
sem	Concept	_make_pairs	g	convert an adjacency linked list back into a set of pairs
sem	Concept	close		close a binary relation in the concept's extension set
sem		clause2concepts	filename rel_name schema closures	convert a file of prolog clauses into a list of concept objects
sem		cities2table	filename rel_name dbname verbose	convert a file of prolog clauses into a database table
sem		sql_query	dbname query	execute an sql query over a database
sem		_str2records	filename rel	read a file into memory and convert each relation clause into a list
sem		unary_concept	label subj records	make a unary concept out of the primary key in a record
sem		binary_concept	label closures subj obj	make a binary concept out of the primary key and another field in a record
sem		process_bundle	rels	given a list of relation metadata bundles make a corresponding dictionary of concepts indexed by the relation name
sem		make_valuation	concepts read lexicon	convert a list of concept objects into a list of label extension pairs optionally create a valuation object
sem		val_dump	rels db	make a valuation from a list of relation metadata bundles and dump to persistent database
sem		val_load	db	load a valuation from a persistent database
sem		label_indivs	valuation lexicon	assign individual constants to the individuals in the domain of a valuation
sem		make_lex	symbols	create lexical cfg rules for each individual symbol
sem		concepts	items	build a list of concepts corresponding to the relation names in items
sem		sql_demo		print out every row from the 'city db' database
sem		skolemize	expression univ_scope used_variables	skolemize the expression and convert to conjunctive normal form cnf
sem		to_cnf	first second	convert this split disjunction to conjunctive normal form cnf
sem	LinearLogicParser	attempt_ApplicationExpression	expression context	attempt to make an application expression if the next tokens
sem	AtomicExpression	__init__	name dependencies	:param name str for the constant name
sem	AtomicExpression	simplify	bindings	if 'self' is bound by 'bindings', return the atomic to which it is bound
sem	AtomicExpression	compile_pos	index_counter glueFormulaFactory	from iddo lev's phd dissertation p108-109
sem	AtomicExpression	compile_neg	index_counter glueFormulaFactory	from iddo lev's phd dissertation p108-109
sem	ConstantExpression	unify	other bindings	if 'other' is a constant then it must be equal to 'self' if 'other' is a variable
sem	VariableExpression	unify	other bindings	'self' must not be bound to anything other than 'other'
sem	ImpExpression	__init__	antecedent consequent	:param antecedent expression for the antecedent
sem	ImpExpression	unify	other bindings	both the antecedent and consequent of 'self' and 'other' must unify
sem	ImpExpression	compile_pos	index_counter glueFormulaFactory	from iddo lev's phd dissertation p108-109
sem	ImpExpression	compile_neg	index_counter glueFormulaFactory	from iddo lev's phd dissertation p108-109
sem	ApplicationExpression	__init__	function argument argument_indices	:param function expression for the function
sem	ApplicationExpression	simplify	bindings	since function is an implication return its consequent there should be
sem	BindingDict	__setitem__	variable binding	a binding is consistent with the dict if its variable is not already bound or if its variable is already bound to its argument
sem	BindingDict	__getitem__	variable	return the expression to which 'variable' is bound
sem	BindingDict	__add__	other	:param other bindingdict the dict with which to combine self
sem		is_rel	s	check whether a set represents a relation of any arity
sem		set2rel	s	convert a set containing individuals strings or numbers into a set of unary tuples
sem		arity	rel	check the arity of a relation
sem	Valuation	__init__	xs	:param xs a list of symbol value pairs
sem	Valuation	domain		set-theoretic domain of the value-space of a valuation
sem	Valuation	symbols		the non-logical constants which the valuation recognizes
sem		_read_valuation_line	s	read a line in a valuation file
sem		read_valuation	s encoding	convert a valuation string into a valuation
sem	Assignment	purge	var	remove one or all keys i e logic variables from an
sem	Assignment	__str__		pretty printing for assignments {'x', 'u'} appears as 'g[u/x]'
sem	Assignment	_addvariant		create a more pretty-printable version of the assignment
sem	Assignment	add	var val	add a new variable-value pair to the assignment and update self
sem	Model	evaluate	expr g trace	read input expressions and provide a handler for satisfy that blocks further propagation of the undefined error
sem	Model	satisfy	parsed g trace	recursive interpretation function for a formula of first-order logic
sem	Model	i	parsed g trace	an interpretation function
sem	Model	satisfiers	parsed varex g trace	generate the entities from the model's domain that satisfy an open formula
sem		propdemo	trace	example of a propositional model
sem		folmodel	quiet trace	example of a first-order model
sem		foldemo	trace	interpretation of closed expressions in a first-order model
sem		satdemo	trace	satisfiers of an open formula in a first order model
sem		demo	num trace	run exists demos
sem	DrtGlueDemo	mainloop		enter the tkinter mainloop this function must be called if
sem	HoleSemantics	__init__	usr	constructor usr' is a sem expression representing an
sem	HoleSemantics	is_node	x	return true if x is a node label or hole in this semantic representation
sem	HoleSemantics	_break_down	usr	extract holes labels formula fragments and constraints from the hole semantics underspecified representation usr
sem	HoleSemantics	_find_top_most_labels		return the set of labels which are not referenced directly as part of another formula fragment
sem	HoleSemantics	_find_top_hole		return the hole that will be the top of the formula tree
sem	HoleSemantics	pluggings		calculate and return all the legal pluggings mappings of labels to holes of this semantics given the constraints
sem	HoleSemantics	_plug_nodes	queue potential_labels plug_acc record	plug the nodes in queue' with the labels in potential_labels'
sem	HoleSemantics	_plug_hole	hole ancestors0 queue potential_labels0	try all possible ways of plugging a single hole
sem	HoleSemantics	_violates_constraints	label ancestors	return true if the label' cannot be placed underneath the holes given by the set ancestors' because it would violate the constraints imposed
sem	HoleSemantics	_sanity_check_plugging	plugging node ancestors	make sure that a given plugging is legal we recursively go through
sem	HoleSemantics	formula_tree	plugging	return the first-order logic formula tree for this underspecified representation using the plugging given
sem	LogicParser	__init__	type_check	:param type_check bool should type checking be performed? to their types
sem	LogicParser	parse	data signature	parse the expression
sem	LogicParser	process	data	split the data into tokens
sem	LogicParser	get_all_symbols		this method exists to be overridden
sem	LogicParser	inRange	location	return true if the given location is within the buffer
sem	LogicParser	token	location	get the next waiting token if a location is given then
sem	LogicParser	process_next_expression	context	parse the next complete expression from the stream and return it
sem	LogicParser	handle	tok context	this method is intended to be overridden for logics that
sem	LogicParser	get_QuantifiedExpression_factory	tok	this method serves as a hook for other logic parsers that
sem	LogicParser	attempt_EqualityExpression	expression context	attempt to make an equality expression if the next token is an
sem	LogicParser	make_EqualityExpression	first second	this method serves as a hook for other logic parsers that
sem	LogicParser	attempt_BooleanExpression	expression context	attempt to make a boolean expression if the next token is a boolean
sem	LogicParser	get_BooleanExpression_factory	tok	this method serves as a hook for other logic parsers that
sem	LogicParser	attempt_ApplicationExpression	expression context	attempt to make an application expression the next tokens are
sem		read_logic	s logic_parser encoding	convert a file of first order formulas into a list of {expression}s
sem	Variable	__init__	name	:param name the name of the variable
sem		unique_variable	pattern ignore	return a new unique variable
sem		skolem_function	univ_scope	return a skolem function over the variables in univ_scope
sem		typecheck	expressions signature	ensure correct typing across a collection of expression objects
sem	SubstituteBindingsI	substitute_bindings	bindings	:return the object that is obtained by replacing each variable bound by bindings with its values
sem	SubstituteBindingsI	variables		:return a list of all variables in this object
sem	Expression	negate		if this is a negated expression remove the negation
sem	Expression	equiv	other prover	check for logical equivalence
sem	Expression	typecheck	signature	infer and check types raise exceptions if necessary
sem	Expression	findtype	variable	find the type of the given variable as it is used in this expression
sem	Expression	_set_type	other_type signature	set the type of this expression to be the given type raise type
sem	Expression	replace	variable expression replace_bound alpha_convert	replace every instance of 'variable' with 'expression'
sem	Expression	normalize	newvars	rename auto-generated unique variables
sem	Expression	visit	function combinator	recursively visit subexpressions apply 'function' to each
sem	Expression	visit_structured	function combinator	recursively visit subexpressions apply 'function' to each
sem	Expression	variables		return a set of all the variables for binding substitution
sem	Expression	free		return a set of all the free non-bound variables this includes
sem	Expression	constants		return a set of individual constants non-predicates
sem	Expression	predicates		return a set of predicates constants not variables
sem	Expression	simplify		:return beta-converted version of this expression
sem	ApplicationExpression	__init__	function argument	:param function expression, for the function expression
sem	ApplicationExpression	_set_type	other_type signature	:see expression _set_type()
sem	ApplicationExpression	findtype	variable	:see expression findtype()
sem	ApplicationExpression	constants		:see expression constants()
sem	ApplicationExpression	predicates		:see expression predicates()
sem	ApplicationExpression	visit	function combinator	:see expression visit()
sem	ApplicationExpression	uncurry		uncurry this application expression
sem	ApplicationExpression	pred		return uncurried base-function
sem	ApplicationExpression	args		return uncurried arg-list
sem	ApplicationExpression	is_atom		is this expression an atom (as opposed to a lambda expression applied
sem	AbstractVariableExpression	__init__	variable	:param variable variable, for the variable
sem	AbstractVariableExpression	replace	variable expression replace_bound alpha_convert	:see expression replace()
sem	AbstractVariableExpression	_set_type	other_type signature	:see expression _set_type()
sem	AbstractVariableExpression	findtype	variable	:see expression findtype()
sem	AbstractVariableExpression	predicates		:see expression predicates()
sem	AbstractVariableExpression	__eq__	other	allow equality between instances of abstractvariableexpression subtypes
sem	IndividualVariableExpression	_set_type	other_type signature	:see expression _set_type()
sem	IndividualVariableExpression	free		:see expression free()
sem	IndividualVariableExpression	constants		:see expression constants()
sem	FunctionVariableExpression	free		:see expression free()
sem	FunctionVariableExpression	constants		:see expression constants()
sem	ConstantExpression	_set_type	other_type signature	:see expression _set_type()
sem	ConstantExpression	free		:see expression free()
sem	ConstantExpression	constants		:see expression constants()
sem		VariableExpression	variable	this is a factory method that instantiates and returns a subtype of abstractvariableexpression appropriate for the given variable
sem	VariableBinderExpression	__init__	variable term	:param variable variable, for the variable
sem	VariableBinderExpression	replace	variable expression replace_bound alpha_convert	:see expression replace()
sem	VariableBinderExpression	alpha_convert	newvar	rename all occurrences of the variable introduced by this variable binder in the expression to newvar
sem	VariableBinderExpression	free		:see expression free()
sem	VariableBinderExpression	findtype	variable	:see expression findtype()
sem	VariableBinderExpression	visit	function combinator	:see expression visit()
sem	VariableBinderExpression	visit_structured	function combinator	:see expression visit_structured()
sem	VariableBinderExpression	__eq__	other	defines equality modulo alphabetic variance if we are comparing
sem	LambdaExpression	_set_type	other_type signature	:see expression _set_type()
sem	QuantifiedExpression	_set_type	other_type signature	:see expression _set_type()
sem	NegatedExpression	_set_type	other_type signature	:see expression _set_type()
sem	NegatedExpression	visit	function combinator	:see expression visit()
sem	NegatedExpression	negate		:see expression negate()
sem	BinaryExpression	findtype	variable	:see expression findtype()
sem	BinaryExpression	visit	function combinator	:see expression visit()
sem	BooleanExpression	_set_type	other_type signature	:see expression _set_type()
sem	EqualityExpression	_set_type	other_type signature	:see expression _set_type()
sem		is_indvar	expr	an individual variable must be a single lowercase character other than 'e', followed by zero or more digits
sem		is_funcvar	expr	a function variable must be a single uppercase character followed by zero or more digits
sem		is_eventvar	expr	an event variable must be a single lowercase 'e' character followed by zero or more digits
sem	DrtParser	get_all_symbols		this method exists to be overridden
sem	DrtParser	handle	tok context	this method is intended to be overridden for logics that
sem	DrtParser	make_EqualityExpression	first second	this method serves as a hook for other logic parsers that
sem	DrtParser	get_BooleanExpression_factory	tok	this method serves as a hook for other logic parsers that
sem	DrtExpression	equiv	other prover	check for logical equivalence
sem	DrtExpression	get_refs	recursive	return the set of discourse referents in this drs
sem	DrtExpression	is_pronoun_function		is self of the form "pro x "?
sem	DrtExpression	pretty_format		draw the drs
sem	DRS	__init__	refs conds consequent	:param refs list of drtindividualvariableexpression for the
sem	DRS	replace	variable expression replace_bound alpha_convert	replace all instances of variable v with expression e in self where v is free in self
sem	DRS	free		:see expression free()
sem	DRS	get_refs	recursive	:see abstractexpression get_refs()
sem	DRS	visit	function combinator	:see expression visit()
sem	DRS	visit_structured	function combinator	:see expression visit_structured()
sem	DRS	__eq__	other	defines equality modulo alphabetic variance
sem		DrtVariableExpression	variable	this is a factory method that instantiates and returns a subtype of drtabstractvariableexpression appropriate for the given variable
sem	DrtAbstractVariableExpression	get_refs	recursive	:see abstractexpression get_refs()
sem	DrtProposition	visit	function combinator	:see expression visit()
sem	DrtProposition	visit_structured	function combinator	:see expression visit_structured()
sem	DrtNegatedExpression	get_refs	recursive	:see abstractexpression get_refs()
sem	DrtLambdaExpression	alpha_convert	newvar	rename all occurrences of the variable introduced by this variable binder in the expression to newvar
sem	DrtBinaryExpression	get_refs	recursive	:see abstractexpression get_refs()
sem	DrtConcatenation	replace	variable expression replace_bound alpha_convert	replace all instances of variable v with expression e in self where v is free in self
sem	DrtConcatenation	get_refs	recursive	:see abstractexpression get_refs()
sem	DrtConcatenation	__eq__	other	defines equality modulo alphabetic variance
sem	DrtConcatenation	visit	function combinator	:see expression visit()
sem	DrtApplicationExpression	get_refs	recursive	:see abstractexpression get_refs()
sem	PossibleAntecedents	free		set of free variables
sem	PossibleAntecedents	replace	variable expression replace_bound alpha_convert	replace all instances of variable v with expression e in self where v is free in self
sem	DrsDrawer	__init__	drs size_canvas canvas	:param drs drtexpression, the drs to be drawn
sem	DrsDrawer	_get_text_height		get the height of a line of text
sem	DrsDrawer	draw	x y	draw the drs
sem	DrsDrawer	_visit	expression x y	return the bottom-rightmost point without actually drawing the item
sem	DrsDrawer	_draw_command	item x y	draw the given item at the given location
sem	DrsDrawer	_visit_command	item x y	return the bottom-rightmost point without actually drawing the item
sem	DrsDrawer	_handle	expression command x y	:param expression the expression to handle
sem	DrsDrawer	_get_centered_top	top full_height item_height	get the y-coordinate of the point that a figure should start at if its height is 'item_height' and it needs to be centered in an area that
sem	CooperStore	__init__	featstruct	:param featstruct the value of the sem node in a tree from
sem	CooperStore	_permute	lst	:return an iterator over the permutations of the input list
sem	CooperStore	s_retrieve	trace	carry out s-retrieval of binding operators in store if hack=true
sem		parse_with_bindops	sentence grammar trace	use a grammar with binding operators to parse a sentence
sem	GlueFormula	applyto	arg	self = (\x walk x subj -o f
sem	GlueFormula	compile	counter	from iddo lev's phd dissertation p108-109
sem	GlueDict	get_semtypes	node	based on the node return a list of plausible semtypes in order of plausibility
sem	GlueDict	get_meaning_formula	generic word	:param generic a meaning formula string containing the
sem	GlueDict	get_label	node	pick an alphabetic character as identifier for an entity in the model
sem	GlueDict	lookup_unique	rel node depgraph	lookup 'key' there should be exactly one item in the associated relation
sem	Glue	dep_parse	sentence	return a dependency graph for the sentence
