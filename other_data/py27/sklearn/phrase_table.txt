used to fit an estimator within	fit estimator estimator x y	0.071429
logistic loss and gradient	linear_model logistic loss and grad	0.500000
the neighbors	neighbors	0.027027
build a batch of estimators within a	ensemble parallel build estimators n_estimators	0.166667
multiple files	files files n_features dtype	0.500000
discovery rate this uses the benjamini-hochberg procedure	fdr	0.142857
given gradients parameters	updates grads	0.076923
function of the	function x	0.030303
lfw pairs dataset this dataset is a	lfw pairs subset	0.035714
the main classification metrics	metrics classification	0.052632
color	color	1.000000
compute data precision matrix with the generative	precision	0.016667
brier score	metrics brier score loss y_true y_prob sample_weight pos_label	0.333333
perform mean shift clustering	cluster mean shift x	0.500000
the lrd	distances_x neighbors_indices	0.047619
x from y along the first axis	x	0.001692
validate kfold approaches	kfold	0.058824
c such that for c in	c x y	0.030303
for all meta estimators in scikit-learn	meta estimator	0.062500
an estimator on a given test set	estimator x_test y_test scorer	0.500000
the vocabulary dictionary and return term-document	vectorizer fit transform raw_documents y	0.100000
the directory	func dir	0.500000
randomized svd parameters	randomized svd m n_components	0.500000
object	concurrency safe write to_write filename	1.000000
a gaussian distributed dataset	elliptic envelope	0.166667
log odds ratio	log odds	1.000000
each	val predict	0.045455
update the dense dictionary factor in	decomposition update dict dictionary y	0.333333
bytes_limit	joblib	0.007299
outlier factor of	outlier factor decision function	0.500000
the dense dictionary factor in	dictionary y	0.111111
the case of a logistic	linear_model logistic	0.111111
then	transformer mixin	0.500000
return a platform independent representation of	utils shape repr	0.013699
computes y	y	0.002674
decision function of	ensemble ada boost classifier decision function	0.166667
directory corresponding to the	func dir mkdir	0.166667
is the time it take	squeeze time	0.166667
generate	predict estimator	0.045455
to split data into	base shuffle split split x	0.250000
fit all transformers transform the	core feature union fit transform x	0.333333
sparse random projection matrix	random projection fit x	0.333333
the validity of	x metric p metric_params	0.100000
on the grid	grid	0.040000
and return the content as	externals joblib	0.004762
the number of splitting iterations in the cross-validator	kfold get n splits x y	0.111111
two clusterings of a set of points	score labels_true labels_pred	0.047619
raw minimum	min	0.045455
reduction for memmap	memmap	0.066667
inverse label binarization transformation for multiclass	preprocessing inverse binarize multiclass	1.000000
laplacian kernel between x and y	metrics laplacian kernel x y gamma	0.333333
the loss of prediction pred and y	loss function call y pred sample_weight	0.333333
single binary estimator	binary estimator x y	0.500000
func to be run	parallel backend base apply async func	0.250000
two clusterings of a set of	score labels_true labels_pred	0.047619
or stderr depending on verbosity	msg msg_args	0.200000
coefficient	mixin	0.037037
non-negative matrices w h whose product approximates	x w h	0.035714
memmap backed arrays	memmap backed a	0.333333
transform x using one-hot encoding	preprocessing one hot encoder transform x	1.000000
a single binary estimator	core predict binary estimator x	0.200000
decision function	decision function	0.125000
data under each gaussian in the model	mixture gmmbase	0.034483
matrix factorization nmf find	factorization	0.035714
dense dictionary factor in place	dict dictionary y code verbose	0.333333
don't store the timestamp when pickling	externals joblib memory reduce	0.030303
to the mean and component wise scale	scale	0.033333
estimators_features	estimators_features	0.357143
updates terminal regions to median	terminal region tree terminal_regions	0.100000
predict_proba on the estimator with the best found	core base search cv	0.033333
+ fn where tp is	score y_true y_pred labels pos_label	0.027778
row	row	0.466667
each	predict estimator x y	0.045455
paired cosine distances between	paired cosine distances	0.333333
matrices w h whose product approximates	w h	0.031250
lasso path using lars algorithm [1]	linear_model lars path x y	0.100000
each mixture	mixture gmmbase	0.034483
to ensure deterministic output from svd	utils svd	0.166667
the number of splitting iterations in	split get n splits x y groups	0.111111
csc	csc	0.833333
pairwise matrix	parallel pairwise x y func	0.166667
fit an estimator within a job	ensemble parallel fit estimator estimator x y	0.333333
grid_resolution	grid_resolution	1.000000
kernel is stationary	kernel operator is stationary	0.333333
building a cv in a user friendly	cv cv x y classifier	0.031250
chi2	chi2	1.000000
to build a batch	parallel build	0.047619
x relative to y_true	metrics threshold scorer call clf x y sample_weight	0.058824
log messages while keeping track of time	time	0.047619
the timestamp when pickling to avoid the	func reduce	0.050000
cviterable	cviterable	1.000000
platform	utils	0.009709
set the	manifold set	0.500000
the number of splitting iterations in the cross-validator	model_selection cviterable wrapper get n splits	0.111111
is	precision_	0.166667
the grid of alpha values	linear_model alpha grid	0.166667
svmlight / libsvm format	svmlight file f	0.066667
one class versus all others	multiclass	0.076923
of y and	y	0.002674
the cache for the function	joblib memorized func	0.014706
a lower bound on model evidence based on	lower bound	0.071429
the separating hyperplane	svm one class svm decision function	0.333333
curve auc using the trapezoidal rule this is	metrics auc	0.040000
building a cv in a user	check cv cv x y classifier	0.031250
analysis a classifier with	analysis	0.090909
estimator's fit method supports the given parameter	utils has fit parameter estimator parameter	1.000000
the training set according to	factor fit	0.062500
long type introduces	shape repr	0.013699
probabilities for x	proba x	0.444444
a byte string	externals	0.005747
graph of neighbors for points in	neighbors graph	0.066667
a read file object	read file	0.333333
system of equations	b damp atol	0.200000
an arbitrary python object into	dump value filename	0.083333
based on a feature matrix	tree x connectivity n_clusters return_distance	0.250000
predict	predict	0.315068
reorder	reorder	0.357143
output for x relative	metrics threshold scorer call clf x y sample_weight	0.058824
returns whether the kernel is	gaussian_process kernel is	1.000000
and variance along an axix on a csr	variance axis x axis	0.090909
and return the iris	iris	0.111111
we	externals joblib memory	0.016949
and return encoded	transform y	0.023256
don't store the	externals joblib memorized func	0.013158
of a csc matrix	utils csc	1.000000
hopefully pretty robust repr equivalent	joblib safe repr value	1.000000
sample_size	sample_size	1.000000
[rouseeuw1984]_ aiming at computing mcd	n_support remaining_iterations initial_estimates	0.111111
force the execution of	externals joblib memorized	0.013699
non-negative matrix factorization nmf	decomposition non negative factorization x	0.043478
number of splitting iterations in the cross-validator parameters	model_selection leave one out get n splits x	0.111111
given gradients	updates grads	0.076923
two non-negative matrices w h whose	x w h n_components	0.038462
mlp loss function	multilayer perceptron	0.071429
element in the specified row returns	in row row	0.250000
by using the gp prior	return_std return_cov	0.142857
param logic estimators that implement the partial_fit api	utils check partial	0.038462
the number of splitting iterations in	model_selection cviterable wrapper get n splits x y	0.111111
transform documents to document-term matrix	feature_extraction tfidf vectorizer transform raw_documents copy	1.000000
spherical wishart distribution parameters	wishart spherical	0.333333
fit linear	fit x y sample_weight	0.020000
file position	joblib binary zlib file	0.066667
of feature names ordered by their	feature names	0.090909
points on the grid	grid len	0.333333
predict posterior probability of	predict proba	0.250000
compute the gradient	perceptron compute	0.250000
depending from	externals joblib	0.009524
the dual gap convergence criterion the specific definition	dual gap emp_cov precision_ alpha	0.071429
it	joblib	0.014599
gram matrix	omp gram	0.500000
to x return	x	0.001692
calculate the posterior log probability of	multinomial nb joint log likelihood	0.083333
classifier from the training set	classifier fit	0.500000
of the local outlier factor of x	neighbors local outlier factor decision function x	0.100000
the function call with the given arguments	call func	0.100000
get parameters for this estimator	base estimator get params deep	1.000000
a sparse random projection matrix parameters	core base random projection fit x	0.333333
used when memory is inefficient to train all	classes	0.025641
lfw	lfw	0.275862
analysis fa a simple linear generative model with	analysis	0.045455
a grid of	ensemble grid	0.111111
reset	reset	1.000000
cost_matrix	cost_matrix	1.000000
of x and y is	x y	0.002155
scale back the data	inverse transform x copy	0.066667
the process or thread	multiprocessing	0.045455
a memmap instance to reopen on same	joblib reduce memmap a	0.050000
is restricted to the binary classification	y_true	0.043478
the binary classification task	y_true y_score	0.054054
inverse	transform inverse	0.500000
number of splitting iterations in the	group out get n splits	0.111111
back the data to	preprocessing robust scaler inverse transform	0.066667
check initial parameters of	base mixture check parameters	0.200000
returns whether	gaussian_process dot product	0.333333
maximizer of	gaussian_process gaussian process arg max	0.047619
computes the position	mds fit x	0.066667
svmlight / libsvm	svmlight file f n_features dtype multilabel	0.066667
makes sure	copy	0.062500
the euclidean or frobenius norm of	utils norm	0.333333
the given parameter	estimator parameter	0.500000
to capture	check_pickle	0.040000
cache for	joblib memorized	0.015625
random matrix given	random choice	0.166667
warnings without visual nesting	ignore warnings call fn	0.200000
variance along an axix on a csr	variance axis x axis last_mean last_var	0.142857
list of regularization	x y pos_class cs	0.166667
the optimal batch	auto batching mixin compute batch	0.333333
covariance	distribute covar matrix to match covariance	0.250000
the sample weight array	sgd validate sample weight sample_weight n_samples	0.333333
is meant to be cached by a joblib	data_folder_path slice_ color resize	0.033333
to the binary classification task	y_true y_score average	0.076923
back the data to	scaler inverse transform x copy	0.066667
avoid the	joblib memory	0.016949
sample weights by	sample	0.032258
k x y and	gaussian_process exponentiation call x y	0.200000
func to be	joblib pool manager mixin apply async func	0.250000
the maximum likelihood estimator covariance model according	covariance empirical covariance fit x	0.166667
compute class covariance matrix	core class cov	0.250000
based on a feature matrix	x connectivity n_clusters return_distance	0.250000
later scaling	scaler	0.031250
returns a lower bound on	dpgmmbase lower bound	0.071429
data x parameters	fit x y	0.005988
be used when memory is inefficient	classes	0.025641
model	decomposition fast ica	1.000000
meta-information	joblib zndarray wrapper read unpickler	0.043478
the k-neighbors of	kneighbors mixin kneighbors x	0.125000
fit a multi-class classifier by combining binary classifiers	linear_model base sgdclassifier fit	0.076923
image	image	1.000000
x for a spherical	spherical x means	0.500000
a random regression problem with sparse uncorrelated design	make sparse uncorrelated n_samples n_features random_state	0.166667
this function returns posterior probabilities of	cv predict proba x	0.034483
a cv in a user friendly way	cv cv x y	0.031250
ledoit-wolf covariance	covariance ledoit wolf x assume_centered	0.125000
theta as the maximizer of	gaussian_process gaussian process arg max	0.047619
undo the scaling of	preprocessing min max scaler inverse	0.500000
"returns the	multi output classifier score	0.250000
points in the grid	core parameter grid	0.500000
random sample from a given 1-d array	utils choice a size replace	0.250000
for full covariance matrices	full x means covars min_covar	0.166667
get	utils get	1.000000
computes the	covariance empirical	0.200000
the weighted graph of neighbors for points	neighbors graph	0.066667
score with permutations	core permutation test score estimator	0.500000
format	sparse	0.025000
non zero row of x	transform x	0.016949
meant to be cached by a joblib wrapper	data_folder_path slice_ color resize	0.033333
center kernel matrix	preprocessing kernel centerer transform k y	0.500000
under the curve auc using	metrics auc	0.040000
a locally linear embedding analysis on the	locally linear embedding x n_neighbors n_components reg	0.071429
suited for eigenvalue decomposition	value norm_laplacian	0.142857
deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call y pred sample_weight	0.333333
data point	cross val predict estimator	0.045455
process	process	0.727273
a	utils incremental	0.166667
oracle approximating shrinkage covariance model according to the	covariance oas fit x	0.083333
binarize labels in a	label binarize	0.333333
the pairwise matrix	metrics parallel pairwise	0.166667
fit the model by computing truncated svd	fit truncated x n_components	1.000000
the laplacian kernel between x	metrics laplacian kernel x	0.333333
x y and scale if the scale	x y scale	0.500000
covariance estimators	covariance	0.014493
this dataset is described in celeux et	datasets make	0.015625
platform	shape	0.011765
format check x format and	latent dirichlet allocation check	0.062500
faces in the wild lfw pairs dataset	lfw pairs	0.018868
input data point	estimator x y	0.038462
fit label encoder and return encoded labels parameters	label encoder fit transform y	0.200000
fits the graphlasso	graph lasso cv	0.333333
of data with n_zeros additional zeros	data n_zeros	0.500000
corresponding to this	joblib numpy	0.250000
class at each stage	classifier staged	0.333333
the file was opened for writing	externals joblib binary zlib file writable	0.250000
dual gap convergence criterion	dual gap emp_cov	0.071429
neighbors	neighbors lshforest radius neighbors	0.166667
of x to unit norm parameters	preprocessing normalizer transform x y	0.250000
sets fixed random_state parameters for an estimator	estimator	0.014706
the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf x assume_centered block_size	0.125000
matrix factorization nmf find	decomposition non negative factorization x	0.043478
and scaling parameters	y	0.002674
sets of biclusters	cluster consensus	0.250000
base class for forests of trees	base forest	0.333333
logistic loss	linear_model logistic loss w x	0.500000
free energy f v = - log	rbm free energy	0.066667
the posterior log probability of	multinomial nb joint log likelihood	0.083333
fixed random_state parameters for an estimator	estimator	0.014706
aggressive regressor read more in the	aggressive regressor	0.166667
two rows of a csc/csr matrix in-place	utils inplace swap row x m n	0.250000
all meta estimators	meta	0.043478
underlying estimators should be used when	one vs one classifier	0.125000
median absolute error	metrics median absolute error y_true	0.166667
or thread pool	joblib multiprocessing	0.052632
back the data to the original	scaler inverse transform	0.058824
search over	core base search cv fit x y	0.166667
clustering for the subclusters obtained after fitting	clustering	0.050000
persist an arbitrary python object into	value filename compress protocol	0.250000
fit linear model with stochastic gradient descent	linear_model base sgdregressor partial fit x	1.000000
in the wild lfw pairs dataset	datasets fetch lfw pairs subset	0.035714
the submatrix corresponding	submatrix	0.090909
the number of splitting iterations in the cross-validator	model_selection predefined split get n splits	0.111111
zero row of x	transform x y copy	0.142857
calculate true and false positives per binary classification	binary clf curve y_true	0.090909
the isotonic regression model : min sum w[i]	core isotonic regression y	0.066667
of exception types	externals joblib parallel backend	0.029412
a locally linear embedding	locally linear embedding x n_neighbors	0.071429
compute the deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call	0.333333
ensure that x	x	0.001692
the optimal	auto batching mixin compute	0.333333
the parameters for the voting classifier valid parameter	voting classifier set params	0.037037
perform the covariance	mixture covar	0.125000
building a cv in	core check cv cv x y classifier	0.031250
sparse uncorrelated design this dataset is described	datasets make sparse uncorrelated n_samples n_features random_state	0.166667
the model using x y	x y	0.002155
prediction of init	init decision function	0.142857
expression of the dual gap convergence criterion	covariance dual gap emp_cov precision_ alpha	0.071429
two non-negative matrices w h	w h n_components	0.038462
fit a binary classifier	fit binary	0.200000
estimates for each input data point	cross val predict	0.045455
sample weights by class for unbalanced datasets	utils compute sample weight class_weight y indices	0.500000
dense array	sparse coef mixin densify	0.100000
apply the approximate feature map to x	core rbfsampler transform x y	0.333333
binary classification	score y_true y_score average	0.076923
fit ridge regression	ridge fit x y	1.000000
score by cross-validation read more in the	model_selection cross val score estimator x y groups	0.166667
the training set according to the	factor fit predict	0.066667
as training	neighbors local outlier	0.142857
absolute value	abs	0.166667
timestamp when pickling	externals joblib memorized func reduce	0.050000
of exception types to be captured	joblib parallel backend base get exceptions	0.166667
types to	joblib parallel	0.028571
from	size replace p	0.125000
the callable case	callable x	0.083333
fit ridge regression model parameters	linear_model ridge gcv fit x	1.000000
is monotonically correlated with x	core check increasing x	1.000000
extract the coefficients and intercepts from packed_parameters	neural_network base multilayer perceptron unpack packed_parameters	0.250000
timestamp when pickling to avoid	reduce	0.034483
largest k singular values/vectors for a sparse matrix	a k ncv tol	0.166667
each	val	0.037037
graph of neighbors for points	radius neighbors graph	0.066667
of all tokens in the raw documents	feature_extraction count vectorizer	0.125000
building a cv in a user friendly	core check cv cv	0.031250
a locally linear embedding analysis on	locally linear embedding x n_neighbors n_components reg	0.071429
update terminal	update terminal	0.142857
indicate if wrapped estimator is	core one vs one classifier pairwise	0.200000
1 0 if y - pred	y pred	0.250000
run fit on one set	core fit grid point x	0.500000
write array bytes to pickler	write array array pickler	0.333333
the gradient	gradient w x	0.500000
boolean mask indicating which features are	support mask	0.125000
under the	score x	0.033333
log probability for full covariance	mixture log multivariate normal density full x	0.333333
data_home	data_home	0.277778
incremental mean and variance	incr mean variance	0.333333
transforms features	feature_range axis copy	1.000000
predict posterior probability of data under each gaussian	mixture gmmbase predict proba	1.000000
utility for building a cv in a user	core check cv cv x y	0.031250
extract	extract	0.857143
scale back	preprocessing robust scaler inverse transform x	0.066667
a transform function to portion of selected features	preprocessing transform selected x transform selected	0.333333
for indices increasingly apart the distance depending	verbosity filter index	0.055556
non-negative matrix factorization nmf find two non-negative matrices	decomposition non negative factorization x	0.043478
probability calibration with sigmoid method platt 2000 parameters	core sigmoid calibration df y sample_weight	0.500000
range approximates the range of	range	0.058824
for the precision matrix	precision	0.016667
predict_proba on the estimator with the best found	search cv predict	0.074074
cross-validated estimates for each	y cv	0.050000
get the parameters of	ensemble voting classifier get	0.200000
with the generative	base pca get	0.076923
compute non-negative matrix factorization	decomposition non negative factorization	0.043478
x which should contain a	x	0.001692
cohen's kappa	metrics cohen kappa	0.250000
huber loss	huber loss	0.333333
mostly low rank matrix with bell-shaped singular values	low rank matrix	0.083333
the density model	density	0.043478
last_mean	last_mean	1.000000
and last element of numpy array or sparse	and last element arr	0.166667
a score by cross-validation read more in the	model_selection cross val score estimator x y groups	0.166667
on the estimator with the best found parameters	search cv predict	0.074074
we can also predict based on an unfitted	predict x	0.011765
return the directory in	output dir	0.047619
compute the weighted	base mixture score	0.200000
of regularization parameters	pos_class cs	0.166667
back the data to the	inverse transform x copy	0.066667
the kernel k	gaussian_process constant kernel	0.333333
calculate true and false positives per binary	binary clf curve y_true y_score	0.090909
compute area under the curve auc	metrics auc	0.040000
score function	score y_true y_pred sample_weight	0.062500
precision vector is	precision positivity precision	0.250000
for each boosting iteration	ensemble ada boost classifier staged	1.000000
as the maximizer of the	process arg max	0.047619
to a projection to the normalized laplacian	n_components eigen_solver	0.166667
inefficient to	x y classes	0.027778
specified layer	grad layer n_samples	0.166667
to avoid	externals joblib memorized func	0.013158
the best	cv predict proba x	0.068966
of the data	data	0.038462
a parallelbackend which will execute all batches sequentially	sequential backend	0.500000
class with a metaclass	externals joblib with metaclass	1.000000
a	utils lsqr a	0.037037
the mean squared error between two	error norm comp_cov norm scaling squared	0.166667
input checker utility for building a cv	core check cv cv	0.031250
fit	fit x y	0.191617
the free energy f v	bernoulli rbm free energy	0.066667
given cache	cache	0.111111
checker utility for building a cv	cv cv x y classifier	0.031250
convert coefficient matrix	coef mixin	0.090909
check values of the basic parameters	mixture base mixture check initial parameters x	1.000000
biclusters	biclusters	1.000000
each input data	estimator	0.014706
to build a batch of estimators within	build estimators	0.166667
out of bag predictions and score	ensemble base forest set oob score x y	0.250000
check a precision vector is	mixture check precision positivity precision	0.500000
parameters	parameters	0.444444
number of patches that will be extracted in	n patches i_h i_w p_h p_w	0.333333
mixin class for all	estimator mixin	0.250000
apply clustering to a projection to the	spectral clustering affinity n_clusters	0.166667
using	utils shape repr	0.013699
predict using the trained model	multilayer perceptron predict x	0.333333
y = tanh(gamma <x y> + coef0)	y gamma coef0	1.000000
evaluate decision function output for x	x y	0.002155
mkdir	mkdir	0.625000
force the execution	joblib memorized	0.015625
function opening the right fileobject from	externals joblib read fileobject fileobj	0.100000
named estimators	base composition	1.000000
the median and component wise scale	preprocessing robust scale	0.125000
along an axix on a	axis x axis last_mean last_var	0.200000
computes the	covariance empirical covariance	0.071429
x which should	x	0.001692
number of splitting iterations in	base kfold get n splits x y groups	0.111111
path length from source to all reachable	path length graph source	0.200000
swaps two rows of a csc matrix in-place	utils inplace swap row csc x m	0.250000
an array	utils check array array	0.250000
the number of splitting iterations in the	kfold get n splits	0.111111
the lfw people dataset	fetch lfw people	0.040000
transform data to polynomial features parameters	preprocessing polynomial features transform x y	0.500000
negative value	negative x	0.200000
fprime	fprime	1.000000
number of splitting iterations in the cross-validator	cross validator get n splits	0.125000
python object	dump value filename	0.083333
split data into	model_selection base shuffle split split	0.250000
estimates for each input	core cross	0.045455
of all tokens in the raw documents	feature_extraction count vectorizer fit	0.125000
for full covariance	normal density full x means covars min_covar	0.166667
initialize the model parameters of the derived	mixture base mixture initialize x resp	0.500000
type introduces an 'l' suffix when using	utils shape	0.013699
computes the weighted graph of neighbors for	neighbors radius neighbors mixin radius neighbors graph	0.066667
fastmcd algorithm	cov	0.100000
rescale	rescale	1.000000
spherical wishart distribution	wishart spherical	0.333333
values for a given dataset split	train	0.117647
the backend	backend	0.016949
fit estimator and predict	model_selection fit and predict estimator x y	1.000000
vectors rows of u	flip u	0.047619
the initial centroids parameters	cluster init centroids x k init random_state	0.166667
to an array	quadratic discriminant	0.500000
in x	x	0.020305
derived class	resp	0.090909
display the message on	parallel print	0.142857
get the boolean mask indicating which features	get support mask	0.333333
compute the gradient of	perceptron compute	0.250000
the callable case	pairwise callable x	0.083333
matching pursuit problems	x y n_nonzero_coefs	0.500000
data precision matrix with	precision	0.016667
helper function to	helper alpha y v u	0.333333
a one-vs-all fashion several regression	classes neg_label pos_label	0.333333
boolean thresholding of array-like or scipy	preprocessing binarize x threshold	0.083333
an array using numpy	numpy array wrapper	1.000000
function used to build	parallel build	0.047619
the squared loss for regression	neural_network squared loss y_true y_pred	0.500000
process or	multiprocessing backend	0.038462
remove	memory	0.015625
and breiman [2]	make friedman3	0.166667
the best found parameters	base search cv predict	0.076923
exception types to	backend base get	0.066667
divergence	divergence	0.545455
classification metrics read more in the	metrics classification	0.052632
compute the laplacian kernel between	laplacian kernel	0.166667
to a sparse	sparse	0.025000
a transformed real-valued array into a hash	projection to hash	0.333333
grad	grad	0.833333
rank matrix with bell-shaped singular values	rank matrix	0.166667
size_threshold	size_threshold	1.000000
the number of splitting iterations in the	base kfold get n splits x y groups	0.111111
the timestamp when pickling to avoid the hash	memorized func reduce	0.050000
the number of splitting iterations in	model_selection cviterable wrapper get n splits x	0.111111
pipeline after transforms	pipeline	0.083333
neighbors for points in	neighbors	0.054054
the hash depending from it	memorized	0.015873
the logistic loss	linear_model logistic loss w x	0.500000
estimate model parameters with	mixture base mixture fit	0.200000
input validation for standard estimators	x y x y accept_sparse dtype	0.250000
the validity of the input	params x metric p metric_params	0.100000
the pairwise matrix in n_jobs	metrics parallel pairwise x y func n_jobs	0.111111
check	base mixture check	1.000000
any negative value	non negative x whom	0.200000
be used for scaling	scaler	0.031250
train estimator on	estimator estimator x	0.181818
update and a youngs	utils	0.009709
data matrix x and target y	base multilayer perceptron partial	0.166667
with the generative model	decomposition base pca	0.071429
first prime	prime	0.111111
precision the precision	precision	0.016667
active	active	0.833333
estimates for each	estimator x	0.030303
meta estimators in	meta	0.043478
input checker utility for building a cv	check cv cv	0.031250
partial dependence	partial dependence	1.000000
log-det of the cholesky decomposition	det cholesky matrix_chol	0.500000
long type	repr	0.012500
output for x	x	0.001692
the isotonic regression model : min sum w[i]	core isotonic regression	0.055556
a general function given points on a	y reorder	0.111111
for building a cv	cv cv x y classifier	0.031250
autocorrelation parameters theta as the maximizer of	gaussian_process gaussian process arg max	0.047619
estimates for	val predict estimator x y	0.045455
dual gap convergence criterion the specific definition	dual gap	0.071429
ledoit	ledoit	1.000000
with the	pca get	0.076923
with all sets	cv	0.009009
each input data point	core cross val	0.043478
the content of the data home cache	datasets clear data home data_home	0.076923
c in (l1_min_c infinity)	c x y	0.030303
perform standardization by centering and	standard scaler transform x y copy	0.333333
optimal	auto batching mixin compute	0.333333
the training	factor fit predict	0.066667
search	base search	0.100000
avoid the hash depending from it	joblib	0.014599
break the pairwise matrix in n_jobs even	metrics parallel pairwise x y func n_jobs	0.111111
callable	callable	0.294118
finds indices in	neighbors find matching indices	0.250000
indices to split data into	predefined split split	0.250000
the dual gap convergence criterion the	dual gap	0.071429
empty the	memorized func clear warn	0.250000
the l1 distances between	metrics paired manhattan distances	0.083333
class	gradient boosting classifier	0.666667
for c in	c x y	0.030303
for the labeled faces in the wild lfw	lfw	0.034483
for elastic net parameter search	l1_ratio	0.030303
loader for the labeled faces in the	data_home	0.055556
full covariance matrices	density full x	0.166667
lfw pairs dataset this operation	fetch lfw pairs	0.018868
along any axis center to	x axis	0.030769
can also predict based on an unfitted model	predict x	0.011765
features are selected returns	feature_selection selector mixin	0.142857
to	to	1.000000
cache folders to make cache size fit in	externals joblib memory reduce size	0.083333
apply clustering to a projection to	clustering affinity n_clusters n_components	0.166667
build a batch of estimators within a	ensemble parallel build estimators n_estimators ensemble	0.166667
w to minimize the	w	0.035714
best possible score is	score y_true y_pred	0.038462
run fit with	grid search cv fit x	0.333333
for building a cv in	cv cv x y classifier	0.031250
the autocorrelation parameters theta as the maximizer	arg max	0.047619
that whenever scale	scale scale	0.250000
first_line	first_line	1.000000
coefficient score the	score y_true y_pred	0.038462
rank matrix with bell-shaped	rank matrix	0.166667
operation is meant to	index_file_path data_folder_path slice_ color	0.033333
low rank matrix with bell-shaped	make low rank matrix	0.083333
labels	classifier	0.013699
matrix factorization nmf find two	non negative factorization	0.043478
dense dictionary	dict dictionary	0.111111
check x format check	decomposition latent dirichlet allocation check	0.062500
representation of an array shape under python 2	shape repr shape	0.166667
function used to partition	partition	0.100000
jobs	jobs	0.388889
return whether the file was opened for writing	zlib file writable	0.250000
maximize the variational	mixture dpgmmbase	0.083333
returns the number of splitting iterations in	model_selection base kfold get n splits	0.111111
returns the number of splitting iterations in the	kfold get n splits	0.111111
em update for	dirichlet allocation em step	0.500000
a cv in a	core check cv cv x	0.031250
representation of an array shape under python 2	utils shape repr shape	0.166667
a precision vector is positive-definite	precision positivity precision covariance_type	0.500000
least squares solver	core linear discriminant analysis solve lsqr	1.000000
return a tolerance which is independent of	cluster tolerance x	0.058824
input	core cross val	0.043478
:ref user guide <mean_absolute_error>	y_true y_pred sample_weight multioutput	0.100000
reverse	feature_selection selector mixin inverse	1.000000
ledoit-wolf covariance	covariance ledoit wolf shrinkage x assume_centered block_size	0.125000
score by cross-validation read more in the	model_selection cross val score estimator x y	0.166667
in the wikipedia page	utils step1 state	0.142857
exception types	joblib parallel backend	0.045455
signature from the given list of parameter objects	signature	0.047619
on the estimator with the best found parameters	core base search cv	0.066667
dot w h	w h	0.031250
jobs that can actually run in	jobs	0.111111
predict	decision	0.027778
to make cache size fit in	reduce size	0.083333
a single tree	trees tree forest x y	0.142857
the number of	len	0.038462
k x	product call x	0.200000
main classification metrics read more in the	metrics classification	0.052632
number of splitting iterations in the	pgroups out get n splits x y groups	0.111111
of	of	1.000000
z-file	zfile file_handle	0.666667
resp	resp	0.454545
grid of alpha values	alpha grid x y	0.166667
permutation_test_score	core permutation test score estimator x	0.500000
list of exception	base get	0.066667
splits	splits	0.500000
updates	updates	1.000000
learn the vocabulary dictionary and return term-document	transform raw_documents y	0.100000
setting the parameters for the voting classifier	ensemble voting classifier	0.031250
scale if the scale	scale	0.033333
reconfigure the backend	backend base configure n_jobs parallel	0.500000
with joblib dump	joblib	0.007299
by scaling each feature to a given	scale x	0.043478
split data into training and test set	model_selection base kfold split x y groups	0.200000
loader for the labeled faces	data_home	0.055556
x from y along the	x	0.001692
checking for random matrix generation	check input size n_components n_features	0.200000
precision matrix with	pca get precision	0.066667
fit the model using x as training	tsne fit x skip_num_points	0.500000
false positives per binary classification threshold	metrics binary clf curve y_true y_score pos_label	0.090909
for the california housing dataset	datasets fetch california housing	0.083333
a large sparse linear	utils lsqr a	0.037037
x relative to y_true	scorer call estimator x y_true sample_weight	0.200000
estimate model parameters with the em algorithm	mixture gmmbase fit	0.250000
fit ridge regression	ridge classifier fit x	1.000000
function called with the given arguments	externals joblib memorized func get	0.125000
is meant to be	data_folder_path slice_ color resize	0.033333
for x using	x y	0.002155
with the best found	base search cv predict proba x	0.076923
check that predict	utils check	0.023810
data onto the	ridge_alpha	0.052632
calculate true and false positives per binary classification	metrics binary clf curve y_true	0.090909
a score by cross-validation read more in the	cross val score estimator x y groups	0.166667
to the binary classification task	precision recall curve y_true	0.142857
fit a single tree	build trees tree forest x	0.142857
regression problem with sparse uncorrelated design	sparse uncorrelated n_samples	0.166667
return a reference	and shelve	0.200000
opposite of the local outlier	neighbors local outlier	0.142857
number of splitting iterations in the cross-validator parameters	cviterable wrapper get n splits x y	0.111111
for	val predict estimator x y	0.045455
data point	y	0.002674
an 'l' suffix when	shape	0.011765
an unfitted estimator	unfitted name estimator	0.142857
the log-likelihood	score	0.010101
return whether the file was opened for writing	binary zlib file writable	0.250000
convert	mixin	0.037037
center	cross_decomposition center scale xy	1.000000
and return that	y	0.002674
detection	detection	0.833333
confidences	confidences	1.000000
data home	data home	0.076923
check if vocabulary is empty or missing	mixin check vocabulary	0.250000
the pixel-to-pixel gradient connections edges are weighted	img mask return_as dtype	0.166667
estimate the spherical wishart distribution parameters	gaussian mixture estimate wishart spherical nk xk	0.333333
get the	linear_model base randomized linear model get	0.500000
curve auc using the	auc x	0.040000
there to implement the usual api and	fit x y	0.017964
the long type introduces an 'l' suffix when	shape repr	0.013699
it as a zipped pickle	target_dir cache_path	0.142857
method for updating terminal	update terminal	0.142857
the derivative of the logistic sigmoid	neural_network inplace logistic derivative z delta	0.166667
select	select	0.700000
number of splitting iterations in the cross-validator parameters	kfold get n splits x y groups	0.111111
sizes of training subsets and	sizes	0.050000
thread pool	backend	0.033898
implicit data conversions happening in the	data conversion	0.333333
used to fit a single	build trees	0.142857
of a single sample	sample	0.032258
of any n-dimensional array in place using strides	arr patch_shape extraction_step	0.166667
the median and component wise scale	scale x	0.043478
and return the number	externals joblib	0.014286
finder	finder	1.000000
if suitable step length is not found	search	0.019231
a filename	filename	0.050000
determination regression score	r2 score y_true y_pred sample_weight	0.125000
"returns the mean	multi output classifier score x	0.250000
generate a random multilabel classification	make multilabel classification n_samples n_features n_classes	0.500000
the validity	metric p metric_params	0.100000
for reproducibility flips the sign	utils deterministic vector sign	0.066667
the right fileobject	fileobject fileobj	0.200000
directory in which are persisted the	dir	0.038462
check initial parameters of the derived	check parameters	0.200000
find two non-negative matrices w h whose	w h	0.031250
model by computing full svd	full	0.055556
the actual data loading for the lfw people	lfw people	0.040000
accept precomputed	precomp distr	0.250000
any axis center to the mean	x axis	0.015385
1 0 if y	y	0.002674
for the california	datasets fetch california	0.333333
random projection p only changes	core johnson lindenstrauss min dim n_samples	0.142857
the exponential chi-squared kernel	metrics chi2 kernel	0.333333
binary classifier on x	binary x	0.500000
we don't store	externals	0.011494
aggressive regressor read more in the :ref	aggressive regressor	0.166667
the laplacian kernel between x	laplacian kernel x	0.333333
introduces	shape	0.011765
normalize rows and columns of x	normalize x	0.076923
in hastie et al	hastie 10 2 n_samples random_state	0.166667
utility for building a cv in	check cv cv x y classifier	0.031250
implement the usual api and hence	patch extractor fit x y	0.142857
graph	graph	0.255319
reconfigure the	configure n_jobs parallel	0.200000
if suitable step length is not found and	utils line search	0.029412
unbalanced datasets	weight class_weight	0.200000
predict regression target at each stage for x	gradient boosting regressor staged predict x	1.000000
graph of the	feature_extraction img to graph	0.333333
spherical wishart distribution	wishart spherical nk xk	0.333333
thresholding of array-like or scipy	binarize x threshold	0.083333
the vocabulary dictionary and return term-document	count vectorizer fit transform raw_documents y	0.166667
w	x w ht	0.250000
to polynomial	preprocessing polynomial	1.000000
shrunk ledoit-wolf covariance	covariance ledoit wolf x assume_centered	0.125000
svd	svd m n_components	0.500000
k x y and optionally its gradient	rbf call x y eval_gradient	0.333333
a wishart the	mixture wishart	0.125000
<mean_squared_log_error>	sample_weight multioutput	0.500000
classification dataset is constructed by taking a	datasets	0.015152
a given radius of	x radius	0.058824
mixin class for	estimator mixin	0.250000
whose range approximates the range of a	randomized range finder a	0.166667
ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage	0.125000
the number of splitting iterations in	pgroups out get n splits x y	0.111111
of y	y	0.002674
folders to make cache size fit in bytes_limit	memory reduce size	0.083333
a tolerance which is	tolerance	0.045455
predict based on an unfitted model by using	predict	0.006849
compute the deviance	deviance call y pred	0.333333
timestamp when pickling to	memorized func reduce	0.050000
handle the callable case for pairwise_{distances kernels}	callable x	0.083333
k x y and optionally its gradient	call x y eval_gradient	0.200000
avoid	externals joblib memorized func	0.013158
download the 20 newsgroups data and stored it	datasets download 20newsgroups	0.200000
predict_log_proba of the	predict log proba	0.029412
descriptors of a	a	0.018182
undo the scaling of x according	preprocessing min max scaler inverse transform x	0.250000
k x y and optionally its gradient	matern call x y eval_gradient	0.333333
returns the number of splitting iterations in	group out get n splits x	0.111111
prediction scores note	metrics roc	0.040000
lad updates terminal regions to median	ensemble least absolute error update terminal region	0.200000
measure the similarity of two clusterings of	fowlkes mallows score labels_true labels_pred	0.333333
contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true	0.200000
to	backend base get	0.066667
private function used to fit an estimator	fit estimator estimator x y sample_weight	0.071429
dimensionality reduction	decomposition factor	1.000000
transform the data	transform x	0.016949
the kl divergence	manifold kl divergence	0.083333
range of a	range finder a	0.166667
estimates for each input data	core cross val predict	0.045455
probability calibration with sigmoid method platt 2000 parameters	sigmoid calibration df	0.500000
all the covariance	to match covariance	0.250000
the trained model parameters	multilayer perceptron	0.071429
point	core cross val predict estimator x	0.045455
the callable case	pairwise callable x y	0.083333
convert a collection of text documents	vectorizer	0.022222
compute decision	decision	0.055556
function used to fit an estimator	fit estimator estimator x	0.055556
equal to the average path	ensemble average path	0.142857
generate indices to split data into	base shuffle split split	0.250000
kappa a statistic that measures inter-annotator agreement	cohen kappa score y1 y2 labels weights	0.500000
validation on an array	array array	0.166667
the sample weight array	base sgd validate sample weight	0.333333
when memory is inefficient to train all	classes	0.025641
a list of feature name -> indices mappings	dict vectorizer	0.250000
for mono and multi-outputs	y eps n_alphas	0.250000
the local outlier	neighbors local outlier	0.142857
matching pursuit omp solves n_targets orthogonal	linear_model orthogonal mp x y	0.250000
array from the meta-information and the	joblib zndarray wrapper read unpickler	0.043478
filters the given args and kwargs using	ignore_lst args kwargs	0.333333
x from y along the first axis we	x z	0.050000
sparse	sparse	0.325000
func to be run	sequential backend apply async func	0.250000
x	x y sample_weight	0.025974
the log-likelihood of a	covariance score	0.071429
output for x relative to	metrics threshold scorer call clf x y sample_weight	0.058824
a decision tree regressor from the training set	tree decision tree regressor fit	0.250000
evaluates the reduced likelihood function for	reduced likelihood function	0.041667
function for	function	0.021277
covariance estimator read more in the	covariance	0.014493
subclass	subclass	1.000000
logistic regression	logistic regression path x	0.333333
the scaler	preprocessing min max scaler	0.200000
callable case for	callable x y metric	0.083333
finds seeds for	get bin seeds x	0.250000
rescale data so as to support	linear_model rescale data	1.000000
decision tree regressor from the	tree decision tree regressor	0.166667
binary classification used in hastie	datasets make hastie	0.125000
mean and variance along an axix on	mean variance axis x axis last_mean last_var	0.333333
estimates for each input data point	estimator x y	0.038462
compute minimum and	min	0.045455
the number of patches	n patches	0.500000
underlying estimators should	one vs one classifier	0.125000
parallel	joblib parallel backend	0.045455
perform a locally linear embedding analysis	manifold locally linear embedding x n_neighbors n_components reg	0.071429
columns	x columns	0.250000
folder	externals joblib delete folder	1.000000
get the weights from an array of distances	neighbors get	0.125000
procedure described in [rouseeuw1984]_ aiming at computing	step x n_support remaining_iterations initial_estimates	0.111111
actual fitting performing the search	search cv fit x y parameter_iterable	0.333333
0 if y	y	0.002674
closest cluster	cluster	0.021277
perform dbscan clustering from features or distance matrix	cluster dbscan fit x y sample_weight	0.200000
x for a spherical model	spherical x means	0.500000
returns the number of splitting iterations in	model_selection base cross validator get n splits	0.125000
each input data point	estimator	0.014706
the estimator with the best found parameters	search cv predict proba	0.076923
validate x whenever one tries to predict	decision tree validate x predict x check_input	0.500000
compute elastic net path with	path	0.025641
lasso_stability_path	linear_model lasso stability path x y mask weights	1.000000
homogeneity metric of a cluster labeling given	metrics cluster homogeneity score labels_true	0.500000
cache for the function	joblib memorized	0.015625
pairwise matrix in	parallel pairwise	0.166667
graph of k-neighbors for points in x parameters	kneighbors mixin kneighbors graph x n_neighbors mode	0.333333
return the query	query	0.125000
assumes x	transform x	0.016949
raw documents	feature_extraction count vectorizer fit	0.125000
fit estimator and predict	fit and predict estimator x y	1.000000
update nmf	update h	0.500000
a score by cross-validation read more in	model_selection cross val score estimator x	0.166667
creates an affinity matrix for x using the	x	0.001692
an array array of	array	0.076923
of moved objects in six moves urllib_parse	module six moves urllib parse	0.333333
found and raise an exception if a	utils line search	0.029412
log of probability estimates	regression predict log proba x	1.000000
boost	boosting boost	1.000000
coefficient of determination regression score	r2 score y_true	0.125000
compute	base pca get	0.076923
log probability for full covariance	log multivariate normal density full x means	0.333333
callable case for	metrics pairwise callable	0.083333
each input data	core cross	0.045455
copy	copy	0.312500
shrunk ledoit-wolf covariance matrix	covariance ledoit wolf	0.125000
performs clustering on x	fit predict x	0.250000
lfw pairs dataset	datasets fetch lfw pairs	0.037736
content of the data home	datasets clear data home	0.076923
the autocorrelation parameters theta as the maximizer of	arg max	0.047619
to the mean and component wise scale	scale x	0.043478
the best found parameters	model_selection base search cv predict proba x	0.076923
assume_centered	assume_centered	1.000000
each input	x	0.001692
embedding read	embedding	0.040000
return a platform independent	utils	0.009709
score corresponds to	score y_true y_score	0.025000
position of	mds fit x	0.066667
to split data into training and test	model_selection time series split split x y groups	0.200000
validation and conversion	dtype csr_output	0.166667
all the content of the data	clear data	0.142857
grid of alpha values for	alpha grid x y xy	0.166667
version of	fobj	0.125000
to compute log	log	0.018868
pairs dataset this dataset is	pairs	0.055556
the process of the parallel	parallel	0.019231
computation of max absolute value of	max abs	0.047619
compute the grid of alpha values for	linear_model alpha grid x y	0.166667
the leaf	base decision tree apply	0.166667
run in parallel n_jobs is the	n_jobs	0.023256
and a name for	name	0.033333
ward	ward tree x	1.000000
squares projection of the data onto	transform x ridge_alpha	0.071429
pairwise matrix in	parallel pairwise x	0.166667
an 'l' suffix when using the	utils	0.009709
classification on an array of test	gaussian_process gaussian process classifier	0.500000
by scaling each feature to a given	preprocessing minmax scale x	0.142857
based on a feature matrix	tree x connectivity n_clusters	0.250000
fit a binary classifier on x	sgdclassifier fit binary x	1.000000
omp solves n_targets orthogonal	linear_model orthogonal mp x y	0.250000
for random matrix generation	check input size n_components n_features	0.200000
check input and compute prediction of init	base gradient boosting init decision function x	0.142857
set of points	metric	0.071429
parallel execution only	externals joblib parallel	0.014085
noise	noise	1.000000
build a contingency matrix describing the relationship	cluster contingency matrix labels_true labels_pred eps	0.333333
a random sample from a given 1-d array	utils choice a size replace p	0.250000
between x and y	x y	0.002155
axis center to	axis	0.028169
randomized	randomized	0.818182
force the execution of the	joblib memorized	0.015625
on	fit predict	0.055556
predict class labels for samples in x	linear_model linear classifier mixin predict x	1.000000
precisions parameters	precisions nk xk	0.166667
in hastie	datasets make hastie	0.125000
number of splitting iterations in the cross-validator	kfold get n splits x y	0.111111
transform a sequence of instances to	feature_extraction feature hasher transform raw_x y	0.333333
tp / tp + fn where tp is	score y_true y_pred labels pos_label	0.027778
the data x which should	x y	0.002155
we don't store the	joblib memory	0.016949
with	pca	0.047619
factorization nmf find two non-negative	decomposition non negative factorization	0.043478
predict if a	predict	0.006849
model parameters with	fit x y	0.005988
calculate true and false positives per	clf curve y_true	0.250000
factorizing common	first call clf	0.200000
predict class log-probabilities for	ensemble ada boost classifier predict log proba	1.000000
perform a locally linear embedding analysis on	manifold locally linear embedding x n_neighbors	0.071429
cv in a user friendly way	cv cv x y	0.031250
list of exception	base	0.014286
precision the precision is the ratio tp /	precision	0.016667
compute minimum distances between one point and a	metrics pairwise distances argmin x y	0.500000
path with coordinate descent the	enet path x	0.050000
argument checking for random	n_features	0.083333
the timestamp when pickling to	memory reduce	0.030303
points into	manifold	0.100000
prediction of init	boosting init decision function	0.142857
from 0 to n	n	0.050000
and 2 in the wikipedia page	utils step1 state	0.142857
one group out cross-validator provides	one group out	0.166667
kernel density estimation read more in the	kernel density	0.083333
transform data	transform x	0.016949
precision the precision	metrics precision	0.033333
hash depending from it	externals joblib memorized	0.013699
batch of estimators within a job	estimators n_estimators ensemble x	0.083333
class covariance matrix	core class cov x y	0.250000
search over	search cv	0.018182
full	full x means	0.166667
the array corresponding to this	externals joblib numpy array	0.250000
estimate model parameters with the em	mixture base mixture fit x	0.200000
fit the hierarchical clustering	cluster feature agglomeration fit	0.250000
does nothing this transformer is stateless	feature_extraction hashing vectorizer partial fit x y	1.000000
transformed data	h	0.041667
sets of biclusters	consensus score	0.250000
to check the test_size and train_size at init	split init test_size train_size	0.250000
to	joblib parallel	0.028571
single tree in parallel	ensemble parallel build trees tree forest x	0.200000
type	shape repr	0.013699
submatrix corresponding to	submatrix	0.090909
lfw pairs dataset this	lfw pairs	0.037736
as the maximizer of	process arg max	0.047619
load the kddcup99	datasets fetch brute kddcup99	0.166667
the leaves of the cf	birch get leaves	0.333333
kappa a	cohen kappa	0.250000
predict using the trained model parameters	base multilayer perceptron predict x	0.333333
binary classification task	y_true y_score pos_label sample_weight	0.066667
average of the decision functions	decision function x	0.018868
arbitrary python object	joblib dump value filename	0.083333
write	write data	1.000000
filters	joblib filter args func ignore_lst	0.500000
the maximum absolute value to	preprocessing max abs	0.050000
for each	cross val predict estimator x	0.045455
negative value in	negative x	0.200000
group out cross-validator provides train/test indices to split	group out	0.142857
updates terminal regions	terminal region tree terminal_regions leaf x	0.066667
kernel k	white kernel	0.250000
element wise squaring of array-likes and sparse matrices	utils safe sqr x copy	0.125000
the logistic loss	logistic loss w	0.500000
used to compute log probabilities within a job	predict log proba estimators estimators_features	0.250000
squared euclidean or frobenius norm	utils squared norm	0.500000
metric for multilabel classification parameters	binary_metric y_true y_score	1.000000
each	x	0.001692
memmap instance to reopen	reduce memmap	0.166667
clustering for	clustering	0.050000
batch of estimators within a job	estimators n_estimators ensemble	0.083333
process	externals joblib multiprocessing	0.052632
within a job	parallel	0.019231
project to the distortion introduced by a random	n_samples eps	0.125000
estimate the precisions parameters of the precision distribution	gaussian mixture estimate precisions nk	0.166667
return the kernel	kernel	0.062500
the boolean mask indicating which	support mask	0.125000
function call with the given	externals joblib format call func	0.100000
estimate sample	utils compute sample	0.100000
learning prevents rebuilding of cftree from scratch	cluster birch	0.090909
blobs for	datasets make blobs	0.333333
cluster is worthy enough to	cluster cfsubcluster	0.250000
cache folders to	externals	0.005747
estimate mutual	mutual	0.181818
the kernel is stationary	kernel mixin is stationary	0.333333
explained variance regression score	explained variance	0.166667
paired cosine distances between x and y	metrics paired cosine distances x y	0.333333
weighted graph of neighbors for points	neighbors radius neighbors graph	0.066667
a tolerance which is independent of the	tolerance	0.045455
there to implement the usual api and hence	preprocessing binarizer fit x y	0.142857
stacklevel is the depth	memorized func check previous func code stacklevel	1.000000
the ardregression model according to the	linear_model ardregression	0.100000
remove a subcluster	subcluster new_subcluster1 new_subcluster2	0.166667
logistic regression cv aka logit maxent classifier	logistic regression cv	0.200000
of k-neighbors	kneighbors	0.125000
fit the	chi2sampler fit	0.250000
length is not found	utils line search	0.029412
fit estimator and predict values for a	fit and predict estimator x y train	0.250000
be persisted instead	array wrapper	0.166667
train estimator on training subsets incrementally and compute	core incremental fit estimator estimator x y	0.200000
fit the model	base randomized linear model fit	1.000000
terminal regions to	terminal region tree terminal_regions leaf	0.066667
a precision vector	precision positivity precision	0.250000
active default backend	active backend	1.000000
johnson	johnson	1.000000
return a tolerance which is independent	tolerance x	0.058824
the trained model	neural_network base multilayer perceptron	0.083333
memory is inefficient to	y classes	0.027778
dispatch table	reduce_func	0.125000
fit the gradient boosting model	base gradient boosting fit x y sample_weight	1.000000
updates terminal	terminal region tree terminal_regions leaf	0.066667
of the dual gap convergence criterion the	dual gap emp_cov precision_	0.071429
split data into	predefined split split x	0.250000
spherical	spherical resp x	1.000000
model with passive aggressive algorithm	passive aggressive classifier	0.125000
in n_jobs even	n_jobs	0.023256
n_samples itree which is equal to the average	average	0.066667
with new iteration	j est	1.000000
binomial deviance loss function for	binomial deviance	0.250000
compute	get	0.012048
init	ensemble base gradient boosting init	0.142857
reconfigure the backend	parallel backend base configure n_jobs parallel	0.500000
name	func name	0.047619
perform classification	nearest centroid predict	0.142857
step for diagonal cases	mstep diag gmm x responsibilities weighted_x_sum	0.250000
dual gap convergence criterion	dual gap emp_cov precision_ alpha	0.071429
samples in x and y	x y	0.002155
f-beta score is the weighted harmonic mean of	metrics fbeta score y_true	0.333333
the usual api and hence	preprocessing binarizer fit x y	0.142857
calculate approximate perplexity for data x	decomposition latent dirichlet allocation perplexity x	1.000000
k	gaussian_process constant	1.000000
fits the oracle approximating shrinkage covariance model according	covariance oas fit	0.083333
gaussian process regression model we	gaussian_process gaussian process regressor	0.058824
do nothing and return the estimator unchanged this	feature_extraction	0.037037
the given arguments	memorized func	0.016949
input validation for standard	y accept_sparse dtype	0.250000
neighbors within a	lshforest radius neighbors	0.166667
iteration did not converge attributes	no convergence	1.000000
gp prior	x return_std return_cov	0.142857
incremental fit on a batch of samples	discrete nb partial fit x y classes sample_weight	0.166667
implementation is restricted to the binary	y_score average sample_weight	0.142857
gaussian random	core gaussian random	1.000000
model and transform with the	transform x	0.016949
get a signature object for	externals signature	0.050000
don't store	externals	0.011494
exponential	exponential	1.000000
median of data with n_zeros additional zeros	get median data n_zeros	0.500000
estimate the precisions parameters of	gaussian mixture estimate precisions	0.166667
exception types	get	0.012048
number of splitting iterations in the	out get n splits	0.111111
multilabel classification problem	datasets make multilabel classification	0.166667
build a batch of estimators within a job	parallel build estimators n_estimators ensemble x y	0.166667
with the generative	decomposition base pca	0.071429
of a read	read	0.052632
using	linear_model base randomized linear	1.000000
compute	pca get	0.076923
logic estimators that implement the	utils check partial fit	0.038462
maximizer of the reduced likelihood function	gaussian process arg max reduced likelihood function	0.333333
parallel n_jobs is the is	n_jobs	0.023256
i_h	i_h	1.000000
assign ranks to data dealing with ties appropriately	utils rankdata a method	1.000000
update it	update	0.035714
the best found	model_selection base search cv	0.040000
mean squared	norm comp_cov norm scaling squared	0.500000
a which this function is	externals joblib memorized func check	0.125000
generate a grid of points based on	ensemble grid from	0.166667
getter	covariance empirical covariance get	0.166667
given test	y_test scorer	0.333333
with the best found	base search cv	0.052632
called with the given arguments	memorized func	0.016949
fit the hierarchical clustering on	cluster agglomerative clustering fit x	0.250000
and return the breast	breast	0.111111
parallel	externals joblib parallel	0.028169
function call with the	call func	0.100000
the directory in which are persisted the result	output dir	0.047619
by scaling each feature to a given	scale	0.033333
pickle the descriptors of a memmap instance	joblib reduce memmap a	0.050000
data and concatenate results	y	0.002674
compute class	class	0.071429
lad updates terminal regions	least absolute error update terminal region tree terminal_regions	0.200000
the covariance m step for diagonal cases	mixture covar mstep diag gmm x responsibilities weighted_x_sum	0.500000
base class for mlp classification and regression	base multilayer perceptron	0.142857
cd	cd	1.000000
type introduces an 'l'	shape repr	0.013699
precision	metrics precision	0.033333
computes the weighted graph of neighbors for	radius neighbors mixin radius neighbors graph	0.066667
the number of splitting iterations in the	split get n splits x y	0.111111
of determination regression score function	r2 score y_true	0.125000
return the	externals joblib	0.028571
affinity matrix for x using the	x	0.001692
identify uniquely python objects containing	obj hash_name coerce_mmap	0.200000
lad updates terminal regions	least absolute error update terminal region tree	0.200000
cf tree for the	cluster birch fit x y	0.200000
store the	joblib	0.014599
decision function output for x relative to	metrics threshold scorer call clf x	0.058824
print verbose message on initialization	print verbose msg init beg n_init	1.000000
perform the covariance m	mixture covar	0.125000
normalize x by scaling rows and	scale normalize x	0.142857
number of splitting iterations in	model_selection predefined split get n splits x	0.111111
scale back	robust scaler inverse transform	0.066667
the precision the precision	metrics precision	0.033333
for c in (l1_min_c	c x y loss fit_intercept	0.030303
check the test_size and train_size at init	init test_size train_size	0.250000
row of x to unit norm parameters	preprocessing normalizer transform x y	0.250000
a cv in a user friendly	core check cv cv x	0.031250
used when memory is inefficient to train	x y classes	0.027778
the default backend used by parallel	parallel backend backend	0.166667
maximizer of the reduced	gaussian process arg max reduced	0.200000
or	externals joblib	0.004762
returns the number of splitting iterations in	pgroups out get n splits x y groups	0.111111
workaround python 2 limitations of pickling	obj methodname	0.111111
false positives per binary classification threshold	binary clf curve y_true y_score pos_label sample_weight	0.090909
get the weights from an array	neighbors get	0.125000
allocate a new ndarray with aligned memory	utils aligned zeros shape dtype order align	0.500000
generate the random projection matrix parameters	core sparse random projection make random matrix n_components	1.000000
evaluate the density model on the data	kernel density score samples	0.250000
n_jobs even	func n_jobs	0.166667
fraction of time controlled by self	progress	0.100000
weighted graph of neighbors for points in x	mixin radius neighbors graph x	0.500000
rows of a csr matrix in-place	utils inplace swap row csr x m	0.250000
set the diagonal of	manifold set diag	0.333333
the function called with the given arguments	memorized func	0.016949
factorization nmf find two non-negative matrices w	non negative factorization x w	0.500000
building a cv in a user friendly way	core check cv cv x y	0.031250
the number of splitting iterations in the	model_selection base kfold get n splits x	0.111111
is restricted to the binary classification task	y_true	0.021739
check the	check params x	0.500000
apply clustering to a	clustering affinity n_clusters	0.166667
dual gap convergence criterion the	covariance dual gap emp_cov precision_	0.071429
total log probability under the	neighbors kernel density score x y	0.333333
size=none replace=true p=none) generates a random sample from	size replace	0.125000
exponential loss function for binary classification	exponential loss	1.000000
from it	joblib memory	0.016949
x return	x	0.001692
get the directory	func get func dir	1.000000
private function used to compute decisions within	decision function estimators estimators_features x	0.500000
scale back the	scaler inverse transform x copy	0.066667
least squares projection of the data onto	ridge_alpha	0.052632
on sparse	sparse	0.025000
array corresponding to this wrapper	externals joblib numpy array wrapper	0.333333
the	backend base	0.064516
coefficient score	score	0.010101
with built-in cross-validation	cv	0.036036
transforms and predict_log_proba of the	predict log proba x	0.045455
average of the decision functions of the base	decision function	0.025000
get a list of all	utils all	0.500000
label propagation classifier read more in the :ref	label propagation	0.200000
filters	func ignore_lst	0.500000
linkage agglomerative	linkage tree	1.000000
absolute sizes of training subsets	train sizes	0.066667
nans	nans	1.000000
patches of any n-dimensional	extract patches	0.083333
the score for a fit	fit	0.003257
unit norm vector length	preprocessing normalize x norm axis copy	0.200000
predict_proba on the estimator with the best found	search cv predict proba	0.076923
x as training	x	0.003384
of possible outcomes for samples in x	svm base svc predict	0.222222
the search	search cv fit	0.111111
the number of splitting iterations in	one out get n splits x y groups	0.111111
whether the kernel is stationary	kernel is stationary	0.200000
and compute prediction of init	gradient boosting init decision function x	0.142857
fit the model	pca fit	0.222222
long	utils shape repr	0.013699
squaring of array-likes and sparse	utils safe sqr x copy	0.125000
predict multi-class targets using	core output code classifier predict x	0.250000
extent the local structure is retained	trustworthiness x x_embedded n_neighbors precomputed	0.200000
load datasets in the svmlight / libsvm format	datasets load svmlight file f n_features dtype multilabel	0.500000
perform a locally linear embedding	locally linear embedding x n_neighbors n_components reg	0.071429
the similarity of two clusterings of a	score labels_true labels_pred	0.047619
sure centering is not enabled	robust scaler check array	0.250000
computes the position of the points	mds fit x y init	0.066667
a single binary estimator	binary estimator x y classes	0.500000
a cv in	cv cv x y	0.031250
single boost using the	classifier boost	0.100000
patches that will be extracted in	patches i_h i_w p_h p_w	0.250000
the cache	memorized func	0.016949
check if vocabulary is empty	feature_extraction vectorizer mixin check vocabulary	0.250000
the free energy f v	rbm free energy	0.066667
download the 20 newsgroups data	datasets download 20newsgroups	0.200000
is not found and raise an	search	0.019231
function output for x relative	metrics threshold scorer call clf x	0.058824
leaves of the	leaves	0.071429
point	x y	0.002155
"news" format strip the headers by	strip newsgroup	0.090909
dual gap convergence criterion	covariance dual gap emp_cov	0.071429
fit the model	output estimator fit	0.200000
gaussian and label samples by	gaussian	0.029412
restricted to the binary classification	y_true y_score pos_label sample_weight	0.066667
cache for the	memorized	0.015873
the california	datasets fetch california	0.333333
cross-validated choice of	cv	0.009009
that would be indth in iteration	core parameter grid getitem ind	0.333333
model parameters with	fit x	0.006410
reproducibility flips the sign	deterministic vector sign	0.066667
partially fit underlying estimators should be	one vs one classifier partial fit	0.166667
fetch the effective stop words list	feature_extraction vectorizer mixin get stop words	0.200000
of exception types	joblib parallel backend base get	0.066667
coding mixin	coding mixin	1.000000
calibration curve	calibration curve y_true	0.142857
from source to all	source cutoff	0.200000
the shrunk ledoit-wolf covariance	covariance ledoit wolf x assume_centered block_size	0.125000
computes the log-likelihood of	empirical covariance score	0.166667
long type introduces an 'l' suffix when using	repr	0.012500
of a	utils	0.009709
inplace column scaling	utils inplace column scale x scale	0.166667
the image samples in x into a	x	0.001692
shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x assume_centered	0.125000
that for c in (l1_min_c	c x y loss fit_intercept	0.030303
estimator and predict	and predict estimator x	1.000000
to update	update	0.035714
from data	manifold spectral	0.111111
make sure no	non neg array	0.250000
matrices from a given template	type tied_cv covariance_type n_components	1.000000
of csgraph	validate graph csgraph	0.250000
error of	error	0.020000
random projection p only changes the	core johnson lindenstrauss min dim n_samples	0.142857
check x	latent dirichlet allocation check	0.062500
each input data point	predict estimator x y	0.045455
path with	enet path	0.050000
return a platform independent representation	shape	0.011765
we can also predict based	predict	0.006849
get the	neighbors get	0.125000
fit ridge regression model parameters	linear_model base ridge cv fit x y sample_weight	1.000000
computes the position of	mds	0.050000
median absolute error regression loss read	metrics median absolute error	0.166667
modified weiszfeld	linear_model modified weiszfeld	1.000000
the rcv1 multilabel dataset	datasets fetch rcv1	0.333333
generate an array with constant block	make	0.041667
number of splitting iterations in	leave one out get n splits x y	0.111111
lrd	distances_x neighbors_indices	0.047619
transform function to portion of selected features	transform selected	0.333333
friedman [1] and breiman [2]	friedman3 n_samples noise random_state	0.166667
the test/test sizes are meaningful wrt to	model_selection validate shuffle split n_samples test_size train_size	0.111111
the parameters for the voting classifier valid parameter	ensemble voting classifier set	0.037037
continuous target variable	x y discrete_features n_neighbors	0.500000
of module names and a name for the	func name	0.047619
fit a single tree in parallel	ensemble parallel build trees tree	0.200000
mstep	do mstep x	0.500000
number of splitting iterations in the	leave one group out get n splits x	0.111111
of x and dot	divergence x	0.250000
validate the provided	n_components	0.083333
to the cache for the	memorized func	0.016949
data and concatenate results	x y	0.002155
score by cross-validation read more in the	cross val score	0.166667
underlying estimators should be	core one vs one classifier	0.111111
y and scale if the scale	y scale	0.500000
classifier valid parameter keys can be listed with	classifier	0.013699
factorization nmf	negative factorization x	0.043478
given	scorer	0.136364
x for a diagonal model	diag x means covars	0.500000
single boost using the	ensemble ada boost classifier boost	0.100000
a mostly low rank matrix with bell-shaped singular	datasets make low rank matrix	0.083333
the cache	joblib memorized	0.015625
check that predict raises	utils check	0.023810
trained model	neural_network base multilayer perceptron	0.083333
and predicted probabilities for a calibration curve	calibration curve y_true y_prob normalize	0.142857
matrix m	m k k_skip eigen_solver	1.000000
to binary labels the output of transform	transform y	0.023256
regression or lasso path using lars algorithm	linear_model lars path x	0.100000
decorator for tests involving both blas calls and	with blas func	0.333333
given cache key	externals joblib cache key	0.250000
array from the meta-information and	joblib zndarray wrapper read unpickler	0.043478
to	get	0.012048
fit to data then transform it	transformer mixin fit transform x	0.500000
don't store the timestamp when pickling	externals joblib memorized func reduce	0.050000
scale back the data to the original representation	scaler inverse transform x copy	0.066667
in x as a mini-batch	mini batch dictionary learning partial fit x y	1.000000
to the normalized laplacian	n_components eigen_solver	0.166667
a locally linear embedding analysis on	locally linear embedding x	0.071429
sign	sign	0.300000
transform feature->value dicts to array	feature_extraction dict vectorizer transform x	0.200000
grid of alpha values for	alpha grid x y	0.166667
count	multinomial nb count	1.000000
return the shortest	shortest	0.125000
the array	array	0.076923
test	shuffle	0.083333
and configure	estimator append random_state	0.142857
convert coefficient matrix to	linear_model	0.025641
creates a biclustering for	cluster base spectral fit	0.250000
fit and then predict labels for	mixture gmmbase fit predict x y	0.333333
kernel k x	gaussian_process sum call x	1.000000
for updating terminal regions (=leaves)	function update terminal region tree terminal_regions leaf	0.200000
linear model with stochastic gradient descent	y coef_init intercept_init	0.333333
store the timestamp when pickling to	memory reduce	0.030303
avoid	memory	0.015625
the wild lfw pairs dataset this dataset	lfw pairs subset	0.035714
generate cross-validated estimates for each input data point	core cross val predict estimator x y cv	0.071429
fit a binary classifier on x and y	fit binary x y	1.000000
fit estimator and compute scores for a given	core fit and score estimator x y scorer	1.000000
of module names and a name for the	get func name	0.047619
cv	check cv cv x y classifier	0.031250
two clusterings of	score labels_true labels_pred	0.047619
regression or lasso path using lars algorithm	linear_model lars path	0.100000
template method for updating terminal regions (=leaves)	update terminal region tree terminal_regions leaf x	0.200000
for validation and conversion	directed dtype csr_output	0.166667
with_scaling	with_scaling	1.000000
project	biclustering project	1.000000
cv in a user	check cv cv	0.031250
load the kddcup99	brute kddcup99	0.166667
transform is sometimes referred to by	transform y	0.023256
compute data	get	0.012048
a score by cross-validation read more in the	cross val score	0.166667
the model according to	x y sample_weight	0.025974
fetch an	datasets fetch	0.200000
estimate the diagonal covariance vectors	mixture estimate gaussian covariances diag resp x	1.000000
decision function of x	ada boost classifier decision function x	0.333333
in multiplicative update	decomposition multiplicative update h	0.500000
input	cross val predict estimator x	0.045455
terminal regions	terminal regions	1.000000
uncorrelated design	uncorrelated	0.166667
a cv in a user friendly way	cv cv x y classifier	0.031250
estimates	predict estimator	0.045455
reconfigure	base configure n_jobs parallel	0.500000
generate train	shuffle split iter	0.166667
regression score function best possible score is	score y_true y_pred sample_weight	0.062500
for validation and conversion of	directed dtype csr_output	0.166667
lars using bic or aic for model	lars ic	0.250000
reconstruct the array from the meta-information and the	externals joblib zndarray wrapper read unpickler	0.043478
dimension of a dataset of shape (n_samples n_features)	decomposition infer dimension	1.000000
a matrix of patch data	feature_extraction patch extractor transform	0.200000
lowest bound for c such that for c	c x y loss	0.030303
each input	estimator x y	0.038462
terminal regions to median estimates	terminal	0.047619
class representing an arbitrary value	not memorized result	0.500000
the paired cosine	metrics paired cosine	0.333333
is equal to the average	ensemble average	0.125000
a classifier	classifier	0.013699
return the shortest path length	single source shortest path length graph	0.333333
cv in a user friendly	check cv cv	0.031250
a subclass of the unpickler	unpickler	0.090909
context of the memory	joblib memory	0.016949
compute the residual (= negative gradient)	ensemble binomial deviance negative gradient	0.333333
evaluate	call x	0.035714
callable case	pairwise callable	0.083333
the kernel k x y and	dot product call x y	0.200000
full	full x	0.166667
an arbitrary python object	value filename	0.083333
of a single sample image parameters	sample image image_name	0.166667
for eigenvalue decomposition	value norm_laplacian	0.142857
of the data onto	transform x ridge_alpha	0.071429
is not found and raise an	line search	0.029412
evaluate the	score samples	0.500000
list of edges for a 3d image	feature_extraction make edges 3d n_x n_y n_z	0.250000
position of the points in	mds fit x y init	0.066667
for each input data point	y	0.002674
global clustering	cluster birch global clustering x	0.142857
validation of	base lib svm validate	0.500000
regression or lasso path using lars algorithm [1]	linear_model lars path x y	0.100000
types to	externals joblib parallel	0.014085
get a list of all estimators	utils all estimators	0.500000
split data into	model_selection cviterable wrapper split	0.250000
the least-squares solution to a large	utils lsqr a	0.037037
svmlight format this function is equivalent to mapping	svmlight	0.050000
learn vocabulary and idf return term-document	raw_documents y	0.250000
maximum likelihood estimator covariance model according to the	covariance empirical covariance fit x	0.166667
log-likelihood of a gaussian data set with self	score x_test	1.000000
the graphlasso	graph lasso cv	0.333333
objective function the absolute error of the	error	0.020000
svmlight / libsvm file format	svmlight file x	1.000000
elastic net parameter search	l1_ratio	0.030303
absolute sizes	sizes	0.050000
sparse and dense inputs	y sample_weight	0.017857
tokens in the raw documents	feature_extraction count vectorizer fit raw_documents y	0.125000
performs division and handles divide-by-zero	metrics prf divide numerator denominator metric modifier	0.250000
matplotlib	matplotlib	1.000000
write array bytes to	wrapper write array array	0.500000
convert	linear_model sparse coef mixin	0.090909
median absolute error regression	median absolute error y_true y_pred	0.166667
terminal regions (=leaves)	terminal region tree terminal_regions leaf	0.066667
reducer function to a given type in	externals joblib customizable pickler register type	0.083333
of exception types to	joblib	0.007299
0	0	1.000000
gaussian process	gaussian_process gaussian process	0.333333
for fit subclasses should implement this method!	decomposition base pca fit x	0.333333
implementation is restricted to the binary classification task	curve y_true	0.125000
spherical	spherical nk xk	1.000000
binary classifier on x and y	binary x y alpha c	0.500000
path of the scikit-learn data dir	data home data_home	0.055556
abort	abort	1.000000
absolute error of the kl divergence of	kl divergence error	0.100000
data precision matrix with the generative	precision	0.016667
regression score	score y_true y_pred	0.038462
scale input vectors individually to unit	copy	0.062500
a given dataset	scorer	0.045455
based on a feature	connectivity n_clusters return_distance	0.250000
predict multi-class targets using underlying estimators	output code classifier predict	0.250000
used to update params with given gradients parameters	updates grads	0.076923
sparse random matrix	random choice	0.166667
object	externals joblib concurrency safe write to_write filename	1.000000
factorizing common classes	fit first call clf classes	0.058824
reconfigure the backend and	backend base configure n_jobs parallel	0.500000
for each input	cross val predict estimator x y	0.045455
to update terminal regions	update terminal regions tree	0.500000
list of exception types	externals	0.005747
cleanup a temporary folder if still existing	utils delete folder folder_path warn	0.250000
variance along an axix on a csr or	variance axis x axis last_mean last_var	0.142857
memmap instance to reopen on same	joblib reduce memmap	0.166667
breiman [2]	make friedman3 n_samples	0.166667
pos_class	pos_class	1.000000
factorizing common classes param logic	fit first call clf classes	0.058824
set with self	x_test	0.083333
run fit on one set of parameters	core fit grid point x y estimator parameters	1.000000
hessian in the case of a logistic loss	linear_model logistic grad hess w x	1.000000
predict is invariant of compute_labels	predict name clusterer	1.000000
in bytes_limit	externals	0.005747
product with	projection transform x y	0.333333
fit	output estimator fit x	0.200000
the number of splitting iterations in the cross-validator	model_selection base cross validator get n splits	0.125000
x y and optionally its gradient	exp sine squared call x y eval_gradient	1.000000
for indices increasingly apart	joblib verbosity filter index	0.055556
avoid the hash depending from	func	0.011364
from distances using just nearest neighbors	nn distances neighbors	0.500000
stable implementation of givens rotation	utils sym ortho a b	0.250000
linear embedding	linear embedding x	0.200000
generate indices to split data into	kfold split	0.250000
to split data according	split	0.027778
score by cross-validation read more in	model_selection cross val score estimator	0.166667
project data to vectors and cluster the result	biclustering project and cluster data vectors n_clusters	0.333333
we don't	externals joblib memorized func	0.013158
base class for classification loss functions	classification loss function	1.000000
error regression	error y_true y_pred	0.125000
predict multi-output variable using a model trained for	core multi output estimator predict	0.166667
to build a batch of estimators within a	parallel build estimators n_estimators ensemble	0.166667
handle the callable case for pairwise_{distances kernels}	metrics pairwise callable x	0.083333
ignore	ignore	1.000000
the number of splitting iterations in	cviterable wrapper get n splits	0.111111
provided precisions	mixture check precisions precisions covariance_type	0.250000
estimates for each input data	estimator x y	0.038462
matrices w h whose	w h	0.031250
matrix factorization nmf find two non-negative matrices	factorization	0.035714
the density model	neighbors kernel density	0.090909
covariance matrix shrunk on the diagonal read	shrunk covariance	0.090909
shortest path length	utils single source shortest path length graph	0.333333
gradient	gradient w x y	0.500000
lfw pairs dataset this operation is	lfw pairs	0.018868
the search over	base search cv	0.026316
for elastic net parameter search parameters	y xy l1_ratio	0.250000
of the set of samples x	x y	0.002155
distances between x and y read more in	distances x y	0.142857
a sparse random projection matrix	random projection fit x	0.333333
coefficient of determination regression score	metrics r2 score y_true	0.125000
inertia	inertia	1.000000
actual fitting performing the search over parameters	core base search cv fit x y parameter_iterable	0.333333
non-negative matrix factorization nmf	non negative factorization	0.043478
callable case for pairwise_{distances kernels}	pairwise callable x y metric	0.083333
curve auc using	metrics auc	0.040000
one after the other and transforms	x y	0.002155
linear model with passive	passive	0.117647
or	multiprocessing backend	0.038462
fit the	svm linear svc fit x y	0.333333
patch	patch	1.000000
mean and	y	0.002674
decorate	decorate	1.000000
the reduced likelihood	gaussian_process gaussian process reduced likelihood	0.142857
nothing	feature_extraction	0.037037
the depth a which	externals joblib memorized func check previous func code	0.055556
transformer from an arbitrary callable	transformer	0.100000
the maximizer	gaussian_process gaussian process arg max	0.047619
loss for classification	loss y_true	0.250000
soft boundary of the set of samples x	x y sample_weight	0.012987
conditional property using the descriptor protocol	iff has attr descriptor	0.083333
in the wild lfw pairs dataset this	datasets fetch lfw pairs	0.018868
list of feature names ordered by their	dict vectorizer get feature names	0.142857
multiprocessing	if safe multiprocessing	1.000000
update w in multiplicative update	multiplicative update w x w	0.500000
the local structure is retained	x_embedded n_neighbors precomputed	0.200000
computes	empirical covariance	0.125000
transform data x according to fitted model	decomposition latent dirichlet allocation unnormalized transform x	1.000000
regression model we can also predict based on	regressor predict x	0.166667
reduced likelihood	reduced likelihood	0.200000
data point	cross val predict estimator x y	0.045455
the best found parameters	model_selection base search cv predict	0.076923
for full	normal density full x means covars min_covar	0.166667
make	make estimator	1.000000
and returns the	y w	0.500000
compute the initial centroids	cluster init centroids x k init random_state	0.166667
restricted to the binary classification task	score y_true y_score	0.025000
csgraph	graph csgraph directed	0.250000
the descriptors of a memmap instance	joblib reduce memmap a	0.050000
type introduces an 'l' suffix when	utils shape repr	0.013699
the smacof	smacof	0.100000
voting classifier valid parameter keys	voting classifier set params	0.037037
the number of splitting iterations in	predefined split get n splits	0.111111
private function used to compute decisions within	ensemble parallel decision function estimators estimators_features	0.500000
minval	minval	1.000000
space	space	1.000000
compute the laplacian kernel between	metrics laplacian kernel	0.166667
of exception types	externals joblib parallel backend base	0.034483
decisions within a	estimators_features x	0.111111
rand index adjusted	metrics cluster adjusted rand score labels_true	0.333333
cv	cv cv x	0.031250
dataset along any axis center	axis	0.028169
relative	predict scorer call	1.000000
transforms one after the other and	x y	0.002155
the barycenter	barycenter	0.090909
last step in pipeline after transforms	core pipeline	0.076923
label propagation classifier read more in the	label propagation	0.200000
model	fit x y do_prediction	0.166667
from source to all reachable	graph source	0.200000
list of exception types to be captured	base get exceptions	0.166667
attach a reducer function to a	externals joblib customizable pickler register	0.200000
independent representation	utils shape repr	0.013699
distance of the samples x	decision function x	0.018868
compute the median of	median	0.066667
closest cluster each	cluster	0.021277
base class for all estimators	base	0.014286
for a given dataset	scorer	0.045455
indices	find matching indices	0.250000
write	joblib numpy array wrapper write	1.000000
the breast cancer wisconsin	breast cancer return_x_y	0.250000
weighted graph of k-neighbors for	kneighbors mixin kneighbors graph	0.250000
attach a reducer function to a given type	externals joblib customizable pickler register type	0.083333
list of exception	parallel backend	0.030303
function used to fit a single tree	trees tree forest x y	0.142857
function used to compute log probabilities within a	parallel predict log proba	0.058824
found and raise	line search	0.029412
the number of splitting iterations in the cross-validator	pgroups out get n splits	0.111111
get parameters	get params	0.500000
estimates for each input	core cross val	0.043478
logistic loss and gradient	logistic loss and grad	0.500000
download the 20 newsgroups data and stored	download 20newsgroups	0.200000
does nothing this transformer is stateless	feature_extraction hashing vectorizer partial fit x	1.000000
handle the callable case	metrics pairwise callable x	0.083333
generate random samples from a gaussian distribution	mixture sample gaussian mean covar covariance_type n_samples	1.000000
prediction	y sample_weight	0.017857
sigmoid	sigmoid	0.777778
of the logistic	neural_network inplace logistic	0.333333
the posterior log probability	core multinomial nb joint log likelihood	0.083333
full covariance matrices	density full x means covars min_covar	0.166667
hierarchical	agglomerative	0.142857
like d iteritems but accepts any collections mapping	feature_extraction iteritems d	1.000000
hash depending	joblib memorized	0.015625
two rows of a csc/csr matrix in-place	utils inplace swap row x	0.250000
the least-squares solution to a large sparse linear	lsqr a	0.037037
along an axis on a csr or csc	axis x axis	0.083333
indices increasingly apart the distance depending on the	externals joblib verbosity filter index	0.055556
input checker utility for building a cv in	core check cv cv x	0.031250
normalized	normalized	1.000000
fit linear model	linear_model base sgdclassifier fit	0.076923
residual (= negative gradient)	ensemble binomial deviance negative gradient y pred	0.333333
svd	svd m	0.500000
multi-class	threshold	0.076923
coefficient of determination regression score	metrics r2 score y_true y_pred sample_weight	0.125000
patches of any n-dimensional array	feature_extraction extract patches	0.083333
perform a locally linear embedding	locally linear embedding x n_neighbors n_components	0.071429
generate	core base shuffle split	0.166667
gaussian process model fitting method	gaussian_process gaussian process fit	0.250000
model and transform	transform	0.011236
from it	func	0.011364
used to build a batch of	ensemble parallel build	0.047619
report showing	report	0.047619
get parameters of this kernel	get params deep	0.200000
sparse combination of the dictionary atoms	decomposition sparse coding mixin transform x y	0.333333
find the least-squares solution to a	utils lsqr a	0.037037
check if vocabulary is empty or missing not	feature_extraction vectorizer mixin check vocabulary	0.250000
with sparse uncorrelated design this dataset is described	datasets make sparse uncorrelated n_samples	0.166667
transform function to portion of selected features	preprocessing transform selected x transform selected copy	0.333333
build a batch of	ensemble parallel build	0.047619
boosted classifier	ensemble ada boost classifier	0.200000
a random projection p only changes	johnson lindenstrauss min dim n_samples eps	0.142857
generate names	core name	0.250000
v = - log sum_h exp(-e v	v	0.052632
get feature names from all transformers	union get feature names	1.000000
data loading for the lfw people dataset this	lfw people	0.040000
scale back the	preprocessing robust scaler inverse transform x	0.066667
edges for	feature_extraction make edges	0.066667
methodname	methodname	1.000000
distance depending on the value of verbose	verbose	0.062500
to build a batch of estimators within	parallel build estimators n_estimators ensemble x	0.166667
the function call with the given arguments	externals joblib format call func	0.100000
validation	svm validate	0.500000
fit the model using x y as training	isotonic regression fit x y sample_weight	1.000000
be captured	joblib parallel backend base get exceptions	0.166667
classification used in hastie	datasets make hastie	0.125000
transform x	transform x	0.050847
for the one-vs-one multi	one vs one	0.050000
fit a multi-class	fit	0.003257
warning class used to notify the user of	warning	0.083333
a which	externals joblib memorized func check	0.125000
under each gaussian	mixture gmmbase	0.034483
first prime element in	prime in	0.166667
estimate model parameters with	fit	0.003257
return a platform independent representation of	repr	0.012500
estimates the minimum covariance determinant matrix	covariance fast mcd x support_fraction cov_computation_method	0.250000
private function used to fit an estimator within	fit estimator estimator x y	0.071429
callable case	pairwise callable x y metric	0.083333
each class with a	classifier	0.013699
estimate the parameters of the gaussian distribution	mixture bayesian gaussian mixture estimate means nk xk	1.000000
random multilabel	datasets make multilabel	0.333333
home cache	home	0.142857
the file	binary zlib file	0.187500
partially fit a single binary estimator one-vs-one	core partial fit ovo binary estimator	1.000000
the number of splitting iterations in the cross-validator	predefined split get n splits x y groups	0.111111
estimator	estimator x y i	1.000000
that can actually run in parallel	joblib parallel backend	0.045455
a memmap instance to reopen on same	memmap a	0.050000
standardize a dataset along any axis	axis with_centering with_scaling	0.333333
x from y	x	0.001692
covariance estimator read more in	covariance	0.014493
gaussian and label	gaussian	0.029412
fit	linear_model base sgdclassifier fit	0.230769
finds the k-neighbors of a point	neighbors kneighbors mixin kneighbors x n_neighbors return_distance	0.250000
for each input data point	core	0.015385
cross-validated estimates for each input data point	cross val predict estimator x y cv	0.071429
filters the given args and kwargs	ignore_lst args kwargs	0.333333
the 20 newsgroups data and stored it	20newsgroups	0.055556
return the kernel k x	rbf call x	0.200000
returns distinct binary samples of length dimensions	datasets generate hypercube samples dimensions	1.000000
for a fit	fit	0.003257
check a precision vector is	check precision positivity precision	0.500000
kernel k	gaussian_process compound kernel	0.333333
the reduced likelihood function for the	reduced likelihood function	0.041667
factorization nmf find two non-negative	decomposition non negative factorization x	0.043478
to the	neural_network bernoulli rbm	0.333333
log probability for full covariance	log multivariate normal density full x means covars	0.333333
each	predict	0.006849
compute labels and inertia using a	labels inertia precompute dense x x_squared_norms centers distances	0.250000
x according to the fitted	x	0.001692
under the curve auc	auc x	0.040000
hash depending	func	0.011364
hash depending	joblib	0.014599
along an axis on a csr or	axis	0.014085
ledoit-wolf covariance matrix	covariance ledoit wolf	0.125000
returns the number of splitting iterations in	pgroups out get n splits	0.111111
a batch of estimators within a job	estimators	0.052632
compute the laplacian kernel between x	metrics laplacian kernel x	0.333333
apply clustering	clustering affinity n_clusters n_components	0.166667
the meta-information	zndarray wrapper read unpickler	0.043478
aggressive algorithm	aggressive classifier partial	1.000000
used to fit a single	trees	0.083333
factorization nmf find two non-negative matrices w h	negative factorization x w h	1.000000
format check x format and make sure no	check non neg array	0.500000
compute scores for a	score	0.010101
the estimator with the best found	core base search cv predict	0.076923
of the decision functions of the	decision function x	0.018868
compressor matching fileobj	detect compressor fileobj	1.000000
generate	estimator x y	0.038462
fit all transformers transform the	union fit transform	0.333333
a grid of	grid	0.040000
estimator is	core	0.015385
along any axis center to	axis	0.028169
params with given gradients	updates grads	0.076923
back the data to the original representation parameters	preprocessing robust scaler inverse transform x	0.066667
inplace row scaling of a csr or	inplace row scale x	0.142857
back the data to the original representation	robust scaler inverse transform x	0.066667
k-fold cross	kfold	0.058824
display the message on	externals joblib parallel print	0.125000
compute data precision matrix with the	get precision	0.052632
cubic	cubic	1.000000
a list of all estimators	utils all estimators	0.500000
naive bayes classifier	nb	0.222222
estimates for each	val predict estimator	0.045455
for parallel processing this method is meant to	joblib parallel	0.028571
the time it take to	externals joblib squeeze time	0.200000
the end of	end ll	0.166667
predict using	predict x	0.035294
wild lfw pairs	datasets fetch lfw pairs	0.018868
em	gmmbase	0.062500
multi-output variable using	core multi output estimator	0.142857
posterior log probability	multinomial nb joint log likelihood	0.083333
negative gradient)	deviance negative gradient	1.000000
number of samples in array-like	utils num samples	0.250000
for a fit	fit rfe estimator	0.166667
the process of the parallel execution only	externals joblib parallel	0.014085
of the dual gap convergence criterion	covariance dual gap emp_cov precision_	0.071429
full covariance matrices	normal density full x means covars	0.166667
returns the number of splitting iterations in the	cviterable wrapper get n splits x y	0.111111
project data to vectors and cluster	biclustering project and cluster data vectors	0.333333
gradient and	x y	0.004310
fit all transformers	union fit	1.000000
of the scikit-learn data dir	data home data_home	0.055556
lfw people dataset this operation is	lfw people	0.040000
the maximizer of the	gaussian process arg max	0.047619
of methods for outliers detection with covariance estimators	detection mixin	0.500000
update terminal regions	error update terminal regions tree	0.500000
the best class label for each sample in	one vs one classifier predict	0.500000
sure that an estimator implements	check estimator estimator	0.142857
array from the meta-information and	externals joblib zndarray wrapper read unpickler	0.043478
of neighbors for	radius neighbors	0.086957
x and	divergence x	0.250000
to fit an estimator within a job	parallel fit estimator estimator x	0.333333
contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true labels_pred eps	0.200000
to a sparse	decomposition sparse	0.111111
in x into a	x	0.001692
checking for random matrix generation	input size n_components n_features	0.200000
logistic loss and gradient	linear_model logistic loss and grad w x	0.500000
a covariance matrix shrunk on the diagonal	shrunk covariance	0.090909
fit all transformers	feature union fit	1.000000
fit an estimator within a job	parallel fit estimator estimator x y sample_weight	0.333333
such that for c in (l1_min_c infinity) the	c x y loss	0.030303
vectors x	dummy regressor predict x	1.000000
determine absolute sizes of training subsets	train sizes	0.066667
least-squares solution to a large sparse linear	a	0.018182
estimates	cross val predict estimator x	0.045455
apply the approximate feature map to	core rbfsampler transform	0.333333
transform function to portion of selected features	transform selected x transform selected copy	0.333333
cache folders to make cache size fit	externals joblib memory reduce size	0.083333
a random multilabel	make multilabel	0.333333
matrix whose range approximates the range of a	range finder a	0.166667
opposite of the local outlier factor	neighbors local outlier factor	0.125000
random regression problem with sparse	make sparse	0.125000
model with stochastic gradient descent	linear_model base sgdregressor	0.250000
binary classifier predicts one class versus all others	multiclass x y alpha c	0.166667
reduced likelihood function for	gaussian_process gaussian process reduced likelihood function	0.047619
multilabel	make multilabel	0.333333
to	externals joblib parallel backend	0.029412
corresponds to the area	y_true y_score	0.027027
building a cv in a	core check cv cv x	0.031250
a covariance matrix shrunk	covariance shrunk covariance	0.090909
in x and y	x y	0.008621
a locally linear	manifold locally linear	0.250000
the data samples in x	x y	0.002155
a grid of points based on	grid from	0.166667
finds indices in	matching indices	0.250000
of x (as	function x	0.030303
data and parameters	y	0.013369
update w	x w	0.083333
array from the meta-information and the z-file	zndarray wrapper read unpickler	0.043478
fit the kernel density model	neighbors kernel density fit x y	0.250000
true and predicted probabilities for a calibration curve	calibration curve y_true y_prob	0.142857
inplace column scaling of a csc/csr matrix	utils inplace column scale x scale	0.166667
x from y	x z	0.050000
private function used to build a batch	parallel build	0.047619
for validation and conversion of csgraph inputs	validate graph csgraph directed dtype csr_output	0.166667
accuracy	accuracy	1.000000
returns the number of splitting iterations in	model_selection predefined split get n splits x y	0.111111
provided precisions	mixture check precisions precisions covariance_type n_components n_features	0.250000
a given radius of a point or points	radius	0.045455
predict the closest	predict	0.006849
and class probabilities	x y	0.002155
score by cross-validation read more in	cross val score estimator	0.166667
convert coefficient	linear_model sparse coef mixin	0.090909
x y and	exponentiation call x y	0.200000
reference	and shelve	0.200000
given format	format spmatrix accept_sparse dtype	1.000000
sort	sort	1.000000
k-neighbors of a	neighbors kneighbors mixin kneighbors	0.100000
terminal regions (=leaves)	terminal region tree terminal_regions leaf x	0.066667
an	utils shape	0.013699
the backend	backend base	0.032258
a list of edges for a 3d image	make edges 3d n_x n_y n_z	0.250000
coverage error measure compute how far	coverage error y_true y_score	0.166667
pickling reduction for memmap backed arrays	externals joblib reduce memmap backed	1.000000
problem with sparse	make sparse	0.125000
an estimator implements the	estimator estimator	0.052632
true and predicted probabilities for a calibration curve	core calibration curve y_true y_prob normalize n_bins	0.142857
of the dual gap convergence criterion	dual gap emp_cov precision_	0.071429
determine the optimal batch size	joblib auto batching mixin compute batch size	0.333333
func to be	parallel backend base apply async func	0.250000
to retrieve a reliable function code hash	externals joblib get func code func	0.250000
init	boosting init	0.142857
descriptors of a memmap instance	externals joblib reduce memmap a	0.050000
a single binary estimator	binary estimator x y i	0.500000
finds the k-neighbors of	kneighbors mixin kneighbors x n_neighbors return_distance	0.250000
the density model on the	density	0.043478
function opening the right fileobject from a filename	read fileobject fileobj filename mmap_mode	0.250000
a logistic regression	logistic regression	0.200000
handle the callable case for pairwise_{distances kernels}	metrics pairwise callable	0.083333
a single binary	predict binary	0.200000
data precision matrix	base pca get precision	0.066667
initialization of the mixture parameters	bayesian gaussian mixture initialize x	1.000000
the recall the recall	metrics recall	0.033333
input validation for	x y x y accept_sparse dtype	0.250000
scale back the data to the original representation	standard scaler inverse transform x	0.066667
by the callers	externals joblib effective	0.200000
mapping from feature integer indices to feature name	get feature names	0.090909
number of splitting iterations in the cross-validator parameters	model_selection leave pgroups out get n splits	0.111111
compute elastic net path with coordinate descent	linear_model enet path	0.050000
exception	externals joblib parallel	0.014085
file was opened for writing	externals joblib binary zlib file writable	0.250000
getter for the precision matrix	get precision	0.052632
a cv in a user friendly way	core check cv cv	0.031250
inefficient	classes	0.025641
compute prediction of init	ensemble base gradient boosting init decision function	0.142857
exception types to	externals	0.005747
compute area under the curve auc	auc x	0.040000
the ardregression model according to the given	linear_model ardregression	0.100000
normalize x according to kluger's	log normalize x	0.200000
learn and	fit transform x y	0.250000
the function called with the given arguments	externals joblib memorized func get output	0.125000
checker utility for building a cv in	check cv cv x y	0.031250
of the scaler	scaler	0.093750
median absolute error	median absolute error y_true y_pred	0.166667
to run in parallel	externals joblib	0.004762
labels	classifier mixin score x	1.000000
char	char	1.000000
the l1 distances between the	metrics paired manhattan distances	0.083333
coverage error measure compute	metrics coverage error y_true	0.166667
list	backend base	0.032258
optimal batch	externals joblib auto batching mixin compute batch	0.333333
the range of a	range finder a	0.166667
of the dual gap convergence criterion	dual gap emp_cov precision_ alpha	0.071429
remove cache folders to	joblib memory reduce	0.030303
with respect to coefs and intercept for	n_samples activations deltas	0.166667
input checker utility for building a cv in	cv cv x	0.031250
"news" format strip the headers by removing	strip newsgroup	0.090909
fit the model and transform with the	fit transform x	0.166667
aggressive algorithm	aggressive regressor partial	1.000000
check that the parameters are well defined	mixture bayesian gaussian mixture check parameters	1.000000
for x relative	predict scorer call estimator x	0.166667
function	function x	0.121212
checker utility for building a cv in	core check cv cv x	0.031250
outlier on the	outlier factor	0.166667
of the dual gap convergence criterion	dual gap	0.071429
this classification dataset is	datasets	0.015152
of points that will be sampled	core parameter sampler len	0.333333
locally linear embedding analysis on	manifold locally linear embedding x n_neighbors n_components	0.071429
c in (l1_min_c infinity) the	c	0.022222
get the values used to update params with	get	0.012048
arbitrary python object into	dump value filename	0.083333
of this	params	0.085714
factorization nmf find two non-negative matrices w	factorization x w	0.500000
fit the hierarchical clustering on	cluster feature agglomeration fit x y	0.250000
spatial independence correlation model pure nugget	gaussian_process pure nugget theta	1.000000
run fit on one set of	core fit grid point x	0.500000
using a single binary estimator	core predict binary estimator x	0.200000
hash	externals joblib	0.009524
features are selected	feature_selection selector mixin	0.142857
whether the kernel	kernel mixin	0.166667
weighted_x_sum	weighted_x_sum	1.000000
multioutput	multioutput	1.000000
fit the kernel density model on the	neighbors kernel density fit x	0.250000
hungarian	hungarian	1.000000
regression score function best possible score is 1	score y_true y_pred sample_weight	0.062500
private function used to build	parallel build	0.047619
x from y along	x z reg	0.066667
check input and compute prediction of init	boosting init decision function x	0.142857
the log of the determinant of	log	0.018868
kernel k x y and optionally its gradient	gaussian_process exponentiation call x y eval_gradient	0.333333
density estimation read more	density	0.043478
of the set of samples x	x y sample_weight	0.012987
compute labels and inertia using a full distance	labels inertia precompute dense x x_squared_norms centers distances	0.250000
the cache	memorized	0.015873
a byte	externals joblib binary	0.200000
model according to the	sample_weight	0.037037
wrapped function cache result and return a	externals joblib memorized func	0.013158
matching pursuit	x y n_nonzero_coefs tol	0.250000
the variational lower bound for the concentration parameter	mixture dpgmmbase bound concentration	1.000000
outlyingness of observations in x according to	covariance outlier detection mixin predict x	0.250000
find	state find	0.500000
apply decision function to	analysis decision function x	0.500000
update terminal regions	update terminal regions tree	0.500000
that can actually run in parallel	externals joblib parallel	0.014085
configure a copy of	append	0.083333
the mean and	x y	0.002155
the significance of a cross-validated	x y cv	0.050000
a reducer function to a	externals joblib customizable pickler	0.200000
validate user provided precisions	precisions precisions	0.250000
text before tokenization	feature_extraction vectorizer mixin build preprocessor	0.142857
in the given file	compress	0.100000
to be used	preprocessing	0.105263
matrix to evaluate the accuracy of a classification	y_true y_pred	0.037037
multilabel classification problem	make multilabel classification	0.166667
multivariate	multivariate	1.000000
building a cv in a user	cv cv x y classifier	0.031250
computes the weighted graph	graph	0.063830
implementation is restricted to the binary classification task	y_true y_score average	0.076923
and smooth	x y	0.004310
of estimators within a job	estimators n_estimators ensemble x	0.083333
getter for the	covariance empirical covariance	0.071429
and perform dimensionality	y	0.002674
number of splitting iterations in the cross-validator	kfold get n splits	0.111111
function for parameter value indexing	model_selection index param value x v	0.200000
of exception types to	get	0.012048
lfw	fetch lfw	0.125000
logistic loss	linear_model logistic loss	0.500000
grid of alpha values for	linear_model alpha grid x y xy	0.166667
indices to split data into	split	0.027778
implement a single boost	boosting boost iboost x y sample_weight	1.000000
sign of elements of all the vectors rows	sign	0.050000
cv and	y	0.002674
function to a given type in	externals joblib customizable pickler register type	0.083333
of array-like or scipy	binarize x threshold	0.083333
calculates a covariance	covariance emp_cov shrinkage	0.500000
rate and potentially other states at the	neural_network base optimizer iteration ends time_step	0.142857
a function	delayed function	0.200000
is inefficient to train all data	x y classes	0.027778
compute the average log-likelihood	decomposition factor analysis score	0.333333
windows cannot encode some characters	externals joblib clean win chars string	0.333333
loading for the lfw pairs dataset this	datasets fetch lfw pairs	0.018868
described in [rouseeuw1984]_ aiming at computing mcd	x n_support remaining_iterations initial_estimates	0.111111
log probability for full covariance matrices	mixture log multivariate normal density full	0.333333
matrix shrunk	shrunk	0.043478
input checker utility for building a cv in	check cv cv	0.031250
check that predict raises	utils check estimators	0.142857
all meta estimators	meta estimator	0.062500
factorize argument checking for random matrix generation	check input size n_components n_features	0.200000
of the decision functions of the base	decision function x	0.018868
the meta-information	externals joblib zndarray wrapper read unpickler	0.043478
random regression problem with sparse uncorrelated design	make sparse uncorrelated	0.166667
return the shortest path	shortest path	0.333333
in the	in	0.090909
of samples x	x	0.001692
x y and	scale xy x y	1.000000
coverage error measure compute	metrics coverage error y_true y_score	0.166667
global clustering for	cluster birch global clustering	0.142857
n_jobs is the is the	n_jobs	0.023256
process	multiprocessing backend	0.038462
gradient	gradient w	0.500000
fit the model according to the given training	svr fit x y sample_weight	0.250000
fit_predict of last step in pipeline after transforms	core pipeline fit predict x y	0.166667
of x	function x	0.030303
to	joblib parallel backend	0.045455
shuffle-labels-out cross-validation iterator	label shuffle split	1.000000
the rfe	rfe	0.100000
neighbors for points	radius neighbors mixin radius neighbors	0.125000
global clustering for the	global clustering x	0.142857
content of the data home cache	data home	0.076923
locally linear embedding analysis	manifold locally linear embedding x n_neighbors	0.071429
isotonic regression model : min sum	core isotonic regression y	0.066667
case of a logistic	linear_model logistic	0.111111
to	preprocessing	0.210526
building a cv in a user friendly way	cv cv	0.031250
generate	core cross val predict estimator	0.045455
biclustering kluger 2003	biclustering	0.166667
and binary classification	y	0.002674
actual data loading for the lfw pairs	fetch lfw pairs	0.018868
fit a single binary estimator	fit binary estimator x y	1.000000
with the em algorithm	mixture gmmbase	0.034483
the recall is the ratio tp /	metrics recall	0.033333
param logic estimators that implement	utils check partial fit	0.038462
the voting classifier	ensemble voting classifier	0.031250
elastic net path with coordinate descent	enet path x	0.050000
utils	utils	0.048544
sequence	feature_extraction vectorizer mixin build tokenizer	1.000000
fit	svc fit	0.333333
reduced likelihood function for	process reduced likelihood function	0.047619
cross-validated estimates for each input data point	cv	0.009009
the brier score	metrics brier score loss y_true y_prob sample_weight pos_label	0.333333
for building a cv in	check cv cv	0.031250
lfw pairs dataset this dataset is	fetch lfw pairs	0.018868
for each input data	predict estimator x	0.045455
an adaboost classifier	ada boost classifier	0.333333
a process or thread pool	backend	0.016949
along any axis center to the median and	x axis	0.015385
mean and variance along an axix on	mean variance axis x axis	0.142857
the median and component wise scale	robust scale x	0.125000
each input data	cross val predict estimator	0.045455
the generative model	pca get	0.076923
call predict	predict	0.006849
abstract base class for gradient boosting	base gradient boosting	0.100000
stop_words	stop_words	1.000000
with the final estimator parameters	core pipeline	0.076923
factorization nmf find two	negative factorization x	0.043478
sparse coding mixin	sparse coding mixin	1.000000
used to capture the arguments of a function	function check_pickle	0.333333
mlp classification and regression	multilayer perceptron	0.071429
long type introduces an 'l' suffix	utils	0.009709
center and scale	robust scaler transform x y	0.200000
\#3" regression problem this dataset is described	datasets make	0.015625
coverage error measure compute how far	metrics coverage error y_true y_score sample_weight	0.166667
used by logistic regression and cv and linearsvc	liblinear x y c fit_intercept	0.142857
regression score function best possible score is 1	score y_true y_pred	0.038462
input checker utility for building a cv	check cv cv x y	0.031250
compute prediction of init	boosting init decision function	0.142857
a mostly low	low	0.125000
for a sparse	utils svds a	0.166667
* np dot x w	linear_model intercept dot w x	1.000000
normalize	cluster scale normalize	1.000000
the blup parameters and evaluates the reduced likelihood	gaussian process reduced likelihood	0.142857
sparse random	random	0.058824
descent the elastic net	l1_ratio	0.030303
median and	y	0.002674
initialize the model parameters of the	base mixture initialize	0.333333
the cache for the	memorized	0.015873
the solution to a sparse coding problem	sparse encode x dictionary gram cov	0.333333
cv in a user friendly way	core check cv cv x y classifier	0.031250
for a sparse matrix	a	0.018182
query based on include_self param	neighbors query include self x include_self	0.333333
a full lars path	linear_model omp path	0.100000
full lars path parameters	linear_model omp path	0.100000
given observations	covariance outlier detection mixin	0.250000
finds the neighbors within a given radius of	neighbors x radius return_distance	0.500000
fit	base sgdclassifier fit x	0.333333
component wise scale to unit	preprocessing scale x	0.090909
multi-class targets using underlying	output code	0.200000
feature names ordered by their indices	feature_extraction dict vectorizer get feature names	0.142857
private function used to compute	parallel decision function estimators	0.333333
we don't store	externals joblib memorized func	0.013158
matrix to	coef mixin	0.090909
provided	n_components	0.083333
interpret	feature_selection calculate threshold estimator	1.000000
compute the median of data	utils get median data	0.333333
score by cross-validation	core cross val score	0.333333
for ridge and	y	0.002674
range of	utils randomized range	0.083333
the mean and	y	0.002674
of biclusters	cluster consensus score	0.250000
compute the mean silhouette coefficient	metrics cluster silhouette score x labels metric	0.250000
estimates the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x assume_centered block_size	0.125000
run fit on	core fit grid	1.000000
solve the linear assignment problem using	utils linear assignment	0.090909
back the data to the original representation	inverse transform x	0.051282
loading for the lfw pairs	lfw pairs	0.018868
(= 2 * negative log-likelihood)	ensemble binomial	0.250000
returns the number of splitting iterations in the	one group out get n splits	0.111111
opening the right fileobject from	externals joblib read fileobject	0.100000
used to build a batch of	build	0.037037
the local outlier factor	neighbors local outlier factor	0.125000
a dataset along any axis center to the	axis	0.028169
is zero we handle it correctly	preprocessing handle zeros in	1.000000
diagonal	diag resp x nk means	1.000000
compute minimum and maximum along an axis on	min max axis x axis	0.333333
dataname	dataname	1.000000
loading for the lfw pairs dataset this	fetch lfw pairs	0.018868
make and configure a copy of the base_estimator_	ensemble base ensemble make estimator append random_state	0.166667
function for factorizing common classes param logic	first call clf classes	0.058824
and then the underlying estimator	y	0.002674
returns whether the	gaussian_process stationary	0.333333
to the cache	joblib memorized	0.015625
locally linear	locally linear	0.400000
the free energy f v =	bernoulli rbm free energy	0.066667
compute elastic net path	path	0.025641
reporter	reporter	1.000000
:ref user guide	y_true y_pred	0.111111
of the samples x to	x	0.003384
base class for regression loss functions	regression loss function	1.000000
folder_path	folder_path	1.000000
replace=true p=none) generates a random sample from a	a size replace p	0.142857
list of exception types to	joblib	0.007299
belongs	mean shift	0.125000
lower bound on model evidence based	mixture dpgmmbase lower bound	0.071429
n_components	cluster base spectral svd array n_components	1.000000
best found parameters	model_selection base search cv predict	0.076923
kernel k	compound kernel	0.333333
log-probabilities for	log proba	0.181818
graph of k-neighbors for points in x	neighbors kneighbors mixin kneighbors graph x n_neighbors mode	0.333333
true and false positives per binary classification	metrics binary clf curve y_true	0.090909
the data and concatenate	x y	0.002155
x relative	predict scorer call estimator x	0.166667
meant to be cached by a	data_folder_path slice_ color resize	0.033333
vectors in x and y	x y sum_over_features	0.250000
a binary classifier on x and y	binary x y	0.500000
independent representation of	shape repr	0.013699
predict is invariant of compute_labels	labels predict name clusterer	1.000000
data to	data	0.076923
the process	externals joblib multiprocessing backend	0.035714
r^2 coefficient of determination regression	r2	0.076923
contingency matrix describing the	contingency matrix	0.166667
the long type introduces	utils	0.009709
a large	utils lsqr a	0.037037
scale back the data to	scaler inverse transform x	0.052632
for factorizing common classes param	first call clf classes	0.058824
workers	parallel initialize	0.333333
set the parameters of this	core pipeline set params	0.500000
scaling features of	scaler	0.031250
read an array using numpy memmap	externals joblib numpy array wrapper read mmap unpickler	1.000000
shrunk ledoit-wolf	ledoit wolf x	0.250000
h	x w h beta_loss	0.500000
the content of the data	data	0.038462
get a signature object for	externals signature obj	0.200000
set the parameters of this	pipeline set params	0.500000
count and	core multinomial nb count x y	0.250000
array is	array	0.076923
for data x with ability to accept precomputed	precomp distr x	0.333333
standardize a dataset along	with_centering with_scaling	0.200000
estimators that implement the partial_fit api	utils check partial fit	0.038462
filename	fileobj filename	0.500000
fit ridge regression model	linear_model ridge classifier fit	1.000000
the cf node	cluster birch	0.090909
to avoid the hash depending from	joblib	0.014599
estimate the spherical variance values	mixture estimate gaussian covariances spherical resp x	1.000000
and maximum	max	0.071429
workers requested by the	base	0.014286
generate a cartesian product of input arrays	utils cartesian arrays out	1.000000
x y and optionally its gradient	gaussian_process exponentiation call x y eval_gradient	0.333333
leaf that each sample is predicted as	base decision tree apply x check_input	0.500000
a decision	decision	0.027778
kernel k x y and	exponentiation call x y	0.200000
of a memmap instance to reopen	joblib reduce memmap a	0.050000
h in multiplicative update nmf	multiplicative update h x w h	0.250000
for creating a class with a	externals add	0.142857
actually run in parallel	parallel	0.019231
remove cache folders to make cache size fit	size	0.032258
and dispatch	dispatch	0.111111
function to portion of selected features	selected	0.142857
perform dbscan clustering from vector array	cluster dbscan x eps	0.200000
kernel is stationary	is stationary	0.142857
fit the model to	fit x	0.012821
parallel execution only a fraction	externals joblib parallel	0.014085
list of edges for a	make edges	0.066667
and evaluates the reduced likelihood	gaussian process reduced likelihood	0.142857
fit the model to	multi output estimator fit x	0.200000
an arbitrary python object into one file	joblib dump value filename	0.083333
adjusted for	cluster adjusted	0.333333
generate primal coefficients from dual coefficients	coef dual_coef n_support support_vectors	0.333333
one group out cross-validator provides train/test	one group out	0.166667
break the pairwise matrix in	pairwise x y	0.166667
spectrum spectrum	spectrum n_samples	0.166667
then return the	rfe	0.100000
ridge regression	ridge	0.285714
**kwargs, in the context of the memory	externals joblib memory	0.016949
model evidence based on x and membership	x z	0.050000
the likelihood of the data	x	0.003384
building a cv in a user friendly way	cv cv x y	0.031250
reports verbose output to stdout	verbose reporter	0.500000
data and	score x y sample_weight	0.250000
x to the separating hyperplane	lib svm decision function x	0.250000
compute area under the curve auc using	metrics auc	0.040000
callable that handles preprocessing and tokenization	build analyzer	0.333333
using the gaussian process regression	gaussian process regressor	0.055556
matrices w h	x w h n_components	0.038462
free energy f v =	free energy	0.066667
generate a random multilabel classification	multilabel classification n_samples n_features n_classes n_labels	0.500000
sample from the decision boundary for each class	core one vs rest classifier decision function	0.250000
classes	classes	0.153846
people	people	0.600000
implementation is restricted to the binary classification task	score y_true y_score average	0.076923
different probability thresholds note this	probas_pred pos_label sample_weight	0.066667
two covariance estimators	covariance	0.014493
sample weights by class for unbalanced datasets	sample weight class_weight	0.500000
thresholding of array-like or scipy	binarize x threshold copy	0.083333
a contingency matrix describing	contingency matrix labels_true labels_pred eps	0.166667
estimator on training subsets incrementally and	incremental fit estimator estimator x y	0.200000
output from svd	utils svd	0.166667
and false positives per binary	binary clf curve y_true	0.090909
boolean thresholding of array-like or scipy sparse matrix	preprocessing binarize x threshold	0.083333
boost	base weight boosting boost	1.000000
fit a multi-class classifier by combining binary	sgdclassifier fit	0.076923
memory is inefficient to train	classes	0.025641
estimate the precisions parameters	gaussian mixture estimate precisions nk	0.166667
kernel k x y and	gaussian_process rbf call x y	0.200000
callable case for pairwise_{distances	metrics pairwise callable x	0.083333
train test	split iter	0.166667
fit linear model with passive aggressive algorithm	linear_model passive aggressive regressor fit x	1.000000
explained variance regression	metrics explained variance	0.166667
and hence	patch extractor fit x y	0.142857
func to be run	backend apply async func	0.250000
long type introduces an 'l' suffix when using	shape repr	0.013699
of np dot	dot	0.125000
the lfw people dataset this operation	lfw people	0.040000
estimate the precisions parameters of	mixture bayesian gaussian mixture estimate precisions	0.166667
a spherical	spherical	0.090909
generate	val predict estimator x y	0.045455
and return the number of	externals joblib	0.014286
persist an arbitrary python object into one	dump value filename compress protocol	0.250000
does not need to	x y residual	0.500000
to avoid the hash depending from	joblib memorized func	0.014706
x	rbf call x	0.200000
data precision matrix with the generative model	pca get precision	0.066667
a tolerance which is independent of	cluster tolerance x	0.058824
predict on the estimator	predict x	0.011765
callable case for pairwise_{distances kernels}	metrics pairwise callable x y	0.083333
to the data x which should	x y	0.002155
the estimator with the best found	base search cv predict proba	0.076923
factorize density check according to	check density density n_features	0.166667
the bound	mixture bound state	0.500000
indices corresponding to test sets	test indices	0.333333
fit the model	svr fit	0.333333
process or thread	externals joblib multiprocessing backend	0.035714
indices to split data into	predefined split split x	0.250000
the similarity of two clusterings of	score labels_true labels_pred sparse	0.047619
incrementally fit the model to data	core multi output regressor partial fit x	0.200000
determinant with the	det fit x	0.333333
estimate class weights for unbalanced datasets	class weight class_weight classes y	0.500000
vectors for reproducibility flips the sign of	deterministic vector sign	0.066667
are going to run in	joblib multiprocessing	0.052632
the position of	mds fit x	0.066667
can actually run in parallel n_jobs	n_jobs	0.023256
step3	step3	1.000000
[rouseeuw1984]_ aiming at computing	c step x n_support remaining_iterations initial_estimates	0.111111
for c in (l1_min_c infinity) the model	c x y loss	0.030303
the case method='lasso' is	x y xy gram	0.090909
split data into training and test	split split x y groups	0.200000
predict multi-output variable using a model trained for	core multi output estimator predict x	0.166667
compute non-negative matrix factorization nmf find	non negative factorization	0.043478
iterate over columns	feature_selection iterate columns x columns	0.250000
estimate model parameters with the em algorithm	base mixture fit x y	0.200000
perform classification on an array of test	nearest centroid predict	0.142857
or thread	joblib multiprocessing	0.052632
boost	ensemble base weight boosting boost	1.000000
false positives per binary	binary clf curve y_true y_score	0.090909
boolean mask indicating which features	support mask	0.125000
hash	func	0.011364
random_state and sets them	random_state	0.076923
estimates for each input	val predict estimator	0.045455
building a cv	core check cv cv	0.031250
private helper function for factorizing common classes	first call clf classes	0.058824
returns the number of splitting iterations in the	leave one out get n splits	0.111111
type introduces an 'l' suffix	shape repr	0.013699
of the samples x to the separating hyperplane	svm one class svm decision function x	0.250000
fit linear model with stochastic gradient descent	base sgdregressor fit x y coef_init	1.000000
configure	append	0.083333
shortest path length from source to all	source shortest path length graph source cutoff	0.111111
from the meta-information and the z-file	joblib zndarray wrapper read unpickler	0.043478
inplace	inplace	0.833333
get the directory corresponding to	get func dir mkdir	0.333333
length is not found and raise an	utils line search	0.029412
backend	backend name	1.000000
a bell-shaped curve of width	effective_rank tail_strength	0.125000
with the generative model	decomposition base	0.076923
fit the model with x	chi2sampler fit x y	1.000000
this implementation is restricted to the binary classification	score y_true y_score	0.025000
generate cross-validated estimates	x y cv	0.050000
partially fit	partial fit x y	0.500000
sigmoid kernel between x and y : k	sigmoid kernel	1.000000
the samples x	decision function x	0.018868
coverage error measure compute how	coverage error	0.166667
arbitrary python object into one	externals joblib dump value filename	0.083333
blobs	make blobs	0.333333
used to compute log probabilities within	predict log proba	0.029412
fit the model to the data x which	fit x y	0.005988
coverage error measure compute how far	coverage error y_true y_score sample_weight	0.166667
preprocessing	preprocessing	0.263158
private function used to compute decisions within	function estimators estimators_features x	0.500000
cluster each sample	cluster	0.021277
the given training data and parameters	x y	0.008621
of the data	datasets clear data	0.142857
l1 distances between the vectors in	paired manhattan distances	0.083333
estimate model parameters with the em algorithm	base mixture fit x	0.200000
sag	sag	1.000000
the reduced likelihood	reduced likelihood	0.200000
private function used to fit a single tree	build trees tree forest x	0.142857
name est weight tuples excluding none transformers	core feature union iter	0.333333
the least-squares solution to a large sparse linear	utils lsqr a	0.037037
with iterative fitting along a regularization path	cv	0.018018
the function with the given arguments and	func call	0.047619
neighbors within a	radius neighbors	0.043478
elastic net model with iterative fitting along a	elastic net cv	0.333333
to be used for later scaling	scaler fit	0.076923
each input data	core cross val	0.043478
time it take to	externals joblib squeeze time t	0.200000
all the content of the data home	datasets clear data home	0.076923
evaluate a score by cross-validation read more in	cross val score	0.166667
return the shortest	source shortest	0.333333
parallel processing	joblib parallel	0.028571
init	validate shuffle split init	1.000000
returns the number of splitting iterations in the	base kfold get n splits x y groups	0.111111
the number of splitting iterations in the cross-validator	cviterable wrapper get n splits x	0.111111
predict class or regression value for x	predict x check_input	1.000000
a function to preprocess the text before tokenization	feature_extraction vectorizer mixin build preprocessor	0.142857
propagation classifier read more in the :ref user	propagation	0.076923
x and returns	fit transform x y w	0.500000
closest cluster each sample in	cluster	0.021277
input checker utility for building a cv	cv cv	0.031250
to cleanup a temporary folder if still existing	utils delete folder folder_path warn	0.250000
non-negative matrix factorization nmf	factorization x	0.043478
the shortest path length from source	single source shortest path length graph source	0.111111
with a given cache	externals joblib cache	0.250000
imputer	imputer	1.000000
checker utility for building a cv in	check cv cv x y classifier	0.031250
inverse covariance w/ cross-validated choice of the	cv	0.009009
one class versus all others	multiclass x y	0.166667
the hash	joblib memorized func	0.014706
prediction scores note this implementation	roc	0.033333
gram	gram	0.916667
lfw people dataset this operation is	datasets fetch lfw people	0.040000
returns the number of splitting iterations in the	group out get n splits x y groups	0.111111
error-correcting output-code multiclass strategy output-code based strategies consist	output code	0.200000
matrix whose range approximates the range	range	0.058824
is the time it take to	externals joblib squeeze time	0.200000
blup parameters and evaluates the reduced	reduced	0.062500
back to line_search_wolfe2 if suitable step	wolfe12 f fprime xk pk	0.028571
parallelbackend must implement	parallel backend base	0.037037
shutdown the process or	joblib multiprocessing backend terminate	0.166667
hash objects that won't normally pickle	my hash	0.333333
constructor store the useful information	externals joblib zndarray wrapper init filename init_args state	0.200000
remove cache folders to	memory	0.015625
for unbalanced datasets	weight class_weight classes y	0.333333
the model using x as training	x skip_num_points	0.166667
scale back the data to	inverse transform x copy	0.066667
python object	externals joblib dump value filename	0.083333
pixel-to-pixel gradient connections edges are weighted with	img mask return_as dtype	0.166667
values for a given dataset	y train	0.166667
for building a cv in a	cv cv x y	0.031250
matrix to dense array	mixin densify	0.100000
and scale the data parameters	x y	0.002155
classification by definition a confusion matrix :math c	metrics confusion matrix y_true y_pred labels	1.000000
a cv in a	core check cv cv x y classifier	0.031250
sigmoid regression model	sigmoid calibration	0.500000
to compute log probabilities within	parallel predict log proba	0.058824
the median and component wise scale	scale	0.033333
in the :ref user guide <classification_report>	y_true y_pred labels target_names	0.200000
estimator and set the base_estimator_ attribute	ensemble ada boost regressor validate estimator	0.333333
in	one	0.142857
transform feature->value dicts to	feature_extraction dict vectorizer transform x y	0.200000
names for	core name	0.250000
compute the decision function of the	decision function x raw_values	0.083333
compute the minimum	min	0.045455
minimum distances between one point and a	pairwise distances argmin x y	0.333333
boosting for	boosting	0.111111
labeled faces in the wild lfw pairs dataset	datasets fetch lfw pairs subset	0.035714
classifier valid parameter	classifier	0.013699
sparse linear system of equations	b damp atol	0.200000
splits for	splits	0.083333
apply a mask	mask	0.071429
the other and transforms	x y	0.002155
estimates for each	core cross val	0.043478
list	base get	0.066667
theilsenregressor class	linear_model lstsq x y	1.000000
of transform is sometimes	transform	0.011236
actual fitting performing the search	base search cv fit x y parameter_iterable	0.333333
predict the target of	cv predict	0.041667
the wild lfw pairs dataset	lfw pairs	0.018868
format	linear_model sparse coef mixin	0.090909
check x format and make sure no	dirichlet allocation check non neg array	0.500000
cross-validated estimates for each input	x y cv	0.050000
list of	externals joblib parallel backend base	0.034483
building a cv in a user friendly	core check cv cv x y	0.031250
of the given data	y	0.002674
lfw pairs	fetch lfw pairs subset	0.035714
each input data point	estimator x	0.030303
lad updates terminal regions to median estimates	ensemble least absolute error update terminal region	0.200000
subcluster from	subcluster new_subcluster1 new_subcluster2	0.166667
number of splitting iterations in the	base cross validator get n splits	0.125000
identity	identity	1.000000
the lfw pairs	fetch lfw pairs	0.018868
<classification_report>	labels target_names	1.000000
predict_proba on the estimator with the best	cv	0.018018
we don't store the timestamp when pickling to	func reduce	0.050000
as	copy_x	0.125000
of the cholesky decomposition	log det cholesky	0.166667
of	utils shape repr	0.013699
for x	x y sample_weight	0.012987
cross-validated	estimator x y cv	0.100000
arbitrary python object	externals joblib dump value filename	0.083333
posterior probability	proba	0.029412
workers	configure n_jobs parallel	0.200000
for later scaling	scaler fit x	0.153846
the shortest path length from source to all	single source shortest path length graph source	0.111111
back the	robust scaler inverse transform	0.066667
types	base get	0.066667
computes the position of	mds fit	0.066667
function opening the right fileobject from a filename	externals joblib read fileobject fileobj filename	0.250000
decision	ensemble ada boost classifier decision	0.333333
the median of data with n_zeros additional zeros	median data n_zeros	0.500000
utility for building a cv in a user	cv cv	0.031250
compute	utils	0.009709
net path	linear_model enet path x	0.050000
neighbors for points	neighbors radius neighbors	0.100000
of a memmap instance	reduce memmap a	0.050000
check validity of parameters	check params	0.200000
a precision vector is	precision positivity precision	0.250000
linear assignment problem using	linear assignment	0.090909
type suitable for scipy	int	0.100000
on an array	utils check array array	0.250000
return the current file position	externals joblib binary zlib file tell	0.333333
confidence scores for samples	linear_model linear classifier mixin decision function	0.500000
current file position	joblib binary zlib file tell	0.333333
input data point	cross val	0.038462
deviance loss function for multi-class	deviance	0.062500
area under the curve auc using the trapezoidal	metrics auc	0.040000
handle	handle	1.000000
compute the score	model_selection score	0.166667
tolerance which is independent of the dataset	tolerance x	0.058824
constructs signature	signature	0.047619
supervised	supervised	1.000000
length is not found and	utils line search	0.029412
parameters for the voting classifier valid parameter keys	ensemble voting classifier	0.031250
scale back the data	inverse transform x	0.051282
scaling	scaler inverse	0.250000
the data onto the sparse components	sparse pca transform x ridge_alpha	0.200000
gaussian mixture	gaussian mixture	0.400000
sample weights by class	compute sample	0.100000
neighbors from	neighbors	0.027027
to avoid	joblib memory	0.016949
actual fitting performing the search over parameters	search cv fit x y parameter_iterable	0.333333
the covariance matrices from a given template	to match covariance type tied_cv covariance_type	0.333333
handle the callable case for	metrics pairwise callable	0.083333
param logic estimators that implement the	utils check partial	0.038462
number of splitting iterations in the	cross validator get n splits x	0.125000
number of splitting iterations in the cross-validator	base cross validator get n splits x y	0.125000
to avoid the	memory	0.015625
directory	dir	0.038462
full covariance	density full x means covars min_covar	0.166667
cache_path	cache_path	1.000000
precision matrix with	base pca get precision	0.066667
x (as	decision function x	0.018868
for the lfw people dataset this	lfw people	0.040000
meant to	data_folder_path slice_ color resize	0.033333
exception types to be captured	get exceptions	0.166667
the descriptors of a memmap instance	externals joblib reduce memmap a	0.050000
predict using the multi-layer perceptron classifier parameters	neural_network mlpclassifier predict x	0.500000
row of x	x	0.001692
are going to run in	externals joblib multiprocessing	0.052632
to raw minimum	min	0.045455
sure centering is not enabled for	preprocessing robust scaler check array x	0.250000
by a random projection p only changes	johnson lindenstrauss min dim n_samples eps	0.142857
the decision function of	decision function	0.025000
shortest path length from source	single source shortest path length graph source	0.111111
a contingency matrix describing the relationship	cluster contingency matrix labels_true labels_pred	0.333333
theta as the maximizer of	gaussian process arg max	0.047619
by a random projection p only changes the	johnson lindenstrauss min dim n_samples eps	0.142857
full	density full x means covars min_covar	0.166667
matrix for label	label	0.045455
number of splitting iterations in the cross-validator parameters	model_selection leave one group out get n splits	0.111111
args	args	0.714286
omp solves n_targets orthogonal matching pursuit problems	linear_model orthogonal mp x y n_nonzero_coefs	0.200000
a sparse	decomposition sparse	0.222222
itree which is equal to the average path	average path	0.142857
perform a locally linear embedding	manifold locally linear embedding	0.062500
to avoid the	externals joblib	0.009524
csgraph inputs	utils sparsetools validate graph csgraph directed	0.250000
data	decomposition base pca	0.071429
theta	theta eval_gradient	0.500000
lars path	linear_model omp path	0.100000
provided precisions	precisions precisions covariance_type n_components n_features	0.250000
or	joblib multiprocessing	0.052632
w to minimize the	w ht	0.250000
estimate the precisions parameters of the precision distribution	bayesian gaussian mixture estimate precisions nk xk sk	0.166667
individually to unit	copy	0.062500
search	base search cv	0.026316
reproducibility flips the	deterministic vector	0.076923
according to the given training data and parameters	x y	0.002155
for building a cv in	cv cv x	0.031250
factor of x (as bigger is better	factor decision function x	0.166667
initialization of the mixture parameters	bayesian gaussian mixture initialize	1.000000
a sparse random matrix	utils random choice	0.333333
measure	cluster fowlkes mallows	0.250000
predict class probabilities at each stage for	gradient boosting classifier staged predict proba	0.500000
run in parallel	parallel	0.019231
logistic regression and cv and	x y	0.002155
search over	search cv fit	0.111111
median absolute error regression loss read	metrics median absolute error y_true y_pred	0.166667
whether the file supports seeking	joblib binary zlib file seekable	0.250000
sizes of training subsets and validate 'train_sizes'	translate train sizes train_sizes n_max_training_samples	0.500000
regions	regions	1.000000
k-neighbors for	kneighbors mixin kneighbors	0.100000
voting classifier valid parameter keys	ensemble voting classifier	0.031250
each class with	classifier	0.013699
determine absolute sizes of	core translate train sizes	0.066667
returns the number of splitting iterations in the	cviterable wrapper get n splits x	0.111111
lower bound for the	bound	0.083333
display the process	print	0.076923
the curve auc from prediction scores note	roc auc score	0.166667
the number of splitting iterations in the cross-validator	group out get n splits	0.111111
process or thread	joblib multiprocessing backend	0.052632
compute labels and inertia using a full	labels inertia precompute dense x x_squared_norms centers	0.250000
parameters and evaluates the reduced	reduced	0.062500
the hierarchical clustering	cluster agglomerative clustering	0.500000
lasso path using lars algorithm [1] the optimization	linear_model lars path	0.100000
or lasso path using lars algorithm	linear_model lars path x	0.100000
descriptors of a memmap instance	joblib reduce memmap a	0.050000
with the given arguments	memorized func	0.016949
classification this function returns posterior probabilities	cv predict proba	0.034483
posterior log probability of the	core bernoulli nb joint log likelihood	0.083333
a list of edges	make edges	0.066667
types	parallel backend	0.030303
em update for	allocation em step	0.500000
between two covariance	covariance	0.014493
random projection p only changes	johnson lindenstrauss min dim n_samples	0.142857
convert a sparse matrix to a	utils ensure sparse	1.000000
svd_solver	svd_solver	0.833333
precision is the ratio tp / tp	precision	0.016667
for full	multivariate normal density full x means covars	0.166667
computes the position of the points in	mds	0.050000
net path with	linear_model enet path	0.050000
neighbors within a given radius	lshforest radius neighbors x radius	0.142857
utility for building a cv in a user	core check cv cv	0.031250
y is in a multilabel format	is multilabel y	0.333333
sign of vectors for reproducibility flips the sign	deterministic vector sign	0.066667
to avoid the hash depending from	memorized func	0.016949
loading for the lfw people	lfw people	0.040000
of the function called with the given arguments	func	0.011364
back the data to	scaler inverse transform x	0.052632
private method don't use directly	signature bind args kwargs partial	1.000000
repeated splits for an arbitrary randomized cv splitter	repeated splits	0.125000
path with	linear_model enet path	0.050000
solution to a	a	0.018182
loads data	datasets load data	0.500000
the weighted graph of neighbors for	neighbors mixin radius neighbors graph	0.066667
multilabel classification problem	multilabel classification	0.166667
precision matrix	covariance empirical covariance get precision	0.250000
note this implementation	roc	0.033333
perform the actual data loading for the lfw	lfw	0.068966
predict_proba on the estimator with the best found	core base search cv predict	0.076923
the voting classifier valid	ensemble voting classifier	0.031250
function used to fit an estimator	fit estimator estimator x y sample_weight	0.071429
x y and	exp sine squared call x y	1.000000
a	externals joblib	0.061905
solve the isotonic regression model : min sum	core isotonic regression y	0.066667
thread	externals joblib multiprocessing	0.052632
process regression model we can	process regressor	0.166667
restricted to the binary classification task	y_true	0.021739
return posterior probabilities of	core quadratic discriminant analysis predict proba x	0.333333
the timestamp when pickling to avoid the	joblib memorized func reduce	0.050000
false positives per binary	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
w in multiplicative update nmf	decomposition multiplicative update w x w h beta_loss	0.500000
types to	joblib parallel backend	0.045455
error of the kl divergence of p_ijs	kl divergence error	0.100000
full distance matrix	cluster	0.021277
for the labeled faces in the wild lfw	datasets fetch lfw	0.041667
length from source	length graph source	0.200000
tolerance which is independent of the dataset	cluster tolerance x tol	0.058824
generate train test indices	indices	0.055556
coefs and intercept for specified layer	layer	0.090909
multilabel	datasets make multilabel	0.333333
module names and a name for	func name	0.047619
and	y sample_weight random_state	0.166667
and intercept for specified layer	layer n_samples	0.166667
depending	memory	0.015625
measure the	fowlkes mallows	0.250000
binary labels the output of transform is sometimes	transform y	0.023256
number of splitting iterations in the cross-validator parameters	out get n splits	0.111111
output for x relative	metrics threshold scorer call clf x y	0.058824
or thread pool	externals joblib multiprocessing backend	0.035714
the closest cluster	cluster	0.021277
sample weight array	base sgd validate sample weight sample_weight n_samples	0.333333
grads	grads	0.833333
the linear assignment problem using	utils linear assignment	0.090909
which are going to run in	externals joblib multiprocessing backend effective	0.250000
median absolute error regression loss read more in	median absolute error	0.166667
estimates for each input	y	0.002674
loading for the lfw people dataset this	lfw people	0.040000
based on a	x connectivity n_clusters	0.250000
evaluate the density model on	density score samples	0.250000
of classification	classifier	0.013699
the pairwise matrix	parallel pairwise x y	0.166667
computes multidimensional scaling using the smacof algorithm	manifold smacof dissimilarities metric n_components init	1.000000
kddcup99	kddcup99	0.545455
posterior log probability of the samples x	core multinomial nb joint log likelihood x	0.500000
[rouseeuw1984]_ aiming at computing	step x n_support remaining_iterations initial_estimates	0.111111
between labels	metrics	0.043478
neighbors within a	neighbors	0.027027
apply transforms and	x	0.001692
set the sample weight array	base sgd validate sample weight	0.333333
handle the callable case	pairwise callable	0.083333
a calibration	core calibration	0.125000
function code hash	func code func	1.000000
fit the model to the	neural_network bernoulli rbm fit	1.000000
a locally linear embedding analysis on the data	manifold locally linear embedding x n_neighbors n_components	0.071429
lad updates terminal regions to median estimates	ensemble least absolute error update terminal region tree	0.200000
float32 then dtype	dtype	0.062500
directory in which are persisted	output dir	0.047619
score	score y_true y_pred	0.076923
finds indices in sorted array of	neighbors find matching indices tree bin_x left_mask right_mask	0.166667
generate cross-validated estimates for each input data point	cv	0.009009
is the time it take	squeeze time t	0.166667
fn where tp is	score y_true y_pred labels pos_label	0.027778
data home	datasets clear data home data_home	0.076923
regression model	regressor	0.027027
contingency matrix describing the relationship	cluster contingency matrix	0.333333
rows of u such	flip u	0.047619
back the data to the	standard scaler inverse transform x copy	0.066667
the number of splitting iterations in the	cviterable wrapper get n splits x y	0.111111
compute the number of	feature_extraction compute n	1.000000
hessian in the case of a logistic loss	logistic grad hess	1.000000
trained	multilayer perceptron	0.071429
hierarchical clustering	cluster agglomerative clustering	0.500000
precision the precision is	metrics precision	0.033333
mean absolute error regression loss read	metrics mean absolute error	0.166667
don't store	memorized	0.015873
number of splitting iterations in the	cross validator get n splits x y groups	0.125000
log probabilities within a	predict log proba	0.029412
an arbitrary python object	filename	0.050000
wise scale	robust scale x	0.125000
scale back the data to the	preprocessing robust scaler inverse transform	0.066667
tolerance which is independent of	tolerance	0.045455
a large sparse linear	lsqr a	0.037037
for a sparse	a	0.018182
and the	x y	0.004310
number of splitting iterations in the	model_selection cviterable wrapper get n splits x y	0.111111
compute data precision matrix with	decomposition base pca get precision	0.066667
to a large sparse	a	0.018182
the vectors in x and y	x y sum_over_features size_threshold	0.250000
mutual information	feature_selection mutual info regression x y	0.500000
estimator on training subsets incrementally and compute scores	incremental fit estimator estimator x y	0.200000
svmlight / libsvm format	svmlight file f n_features	0.066667
generate a random regression problem	make regression n_samples n_features n_informative n_targets	1.000000
the lfw people dataset this operation	fetch lfw people	0.040000
the unnormalized posterior log probability of x i	nb joint log likelihood x	0.111111
finds the neighbors within a given radius of	radius neighbors x radius return_distance	0.500000
according to a percentile	percentile	0.100000
values for	train	0.117647
be used when memory is inefficient to	y classes	0.027778
generate train test	shuffle split	0.142857
convert	coef	0.058824
for each	cross val predict	0.045455
transform binary labels back to multi-class	preprocessing label binarizer inverse transform y threshold	0.333333
initialize	initialize x	0.200000
compute class covariance matrix	core class cov x y priors shrinkage	0.250000
least-squares solution to a large sparse linear system	a	0.018182
c such that for c	c x	0.030303
don't	joblib memorized func	0.014706
for c such that for c	c x	0.030303
scale back the data to the	scaler inverse transform x copy	0.066667
check a precision vector is positive-definite	mixture check precision positivity precision covariance_type	1.000000
laplacian matrix and	laplacian	0.034483
estimator	estimator estimator x	0.181818
of patch	patch extractor	0.090909
score by cross-validation read more in the	cross val score estimator x	0.166667
sample weights by class for	compute sample	0.100000
each input data point	core cross val predict estimator x	0.045455
perform mean shift clustering	cluster mean shift	0.500000
regression	regression path	1.000000
logic estimators that implement the partial_fit api	utils check partial	0.038462
display the	print	0.076923
initialize the model parameters	base mixture initialize parameters x random_state	1.000000
the data under the model	mixture vbgmm score samples x	0.200000
hessian in the case of a multinomial loss	multinomial grad hess w x	1.000000
importances the higher the more important	importances	0.100000
uncompressed bytes from the file	externals joblib binary zlib file	0.083333
template method for updating terminal regions (=leaves)	update terminal region tree terminal_regions leaf	0.200000
a	a w	1.000000
callable case for pairwise_{distances kernels}	callable x	0.083333
embedding read more	embedding	0.040000
function	joblib delayed function	0.200000
and **kwargs, in the context of the memory	memory	0.015625
faces in the wild lfw pairs	datasets fetch lfw pairs	0.018868
method for updating terminal regions (=leaves)	function update terminal region tree terminal_regions leaf	0.200000
call with the given arguments	joblib format call	0.200000
the shrunk ledoit-wolf	ledoit wolf shrinkage x	0.250000
modified	linear_model modified	1.000000
compute precision-recall pairs for different probability thresholds note	probas_pred pos_label sample_weight	0.066667
func to be	apply async func	0.250000
returns the number of splitting iterations in the	model_selection cviterable wrapper get n splits	0.111111
for x	call estimator x	0.166667
samples can be different from	calibrated classifier cv	0.071429
fit estimator and predict values for a given	core fit and predict estimator x y train	0.250000
compute the gradient of loss	compute loss grad	1.000000
importances the higher the more	importances	0.100000
the search over	search cv	0.018182
normalized laplacian	n_components eigen_solver	0.166667
and	x y sample_weight	0.038961
exception types	externals joblib parallel backend base get	0.066667
a list of edges for a 3d image	edges 3d n_x n_y n_z	0.250000
make cache size fit	reduce size	0.083333
emulate function total_seconds introduced in python2	utils total seconds delta	0.200000
mu	mu	1.000000
concatenates results	feature union	0.142857
classification on test	core	0.015385
mean shift clustering	cluster mean shift x bandwidth seeds bin_seeding	0.500000
estimators	ensemble voting classifier	0.031250
absolute error of the kl divergence of p_ijs	kl divergence error	0.100000
random matrix given	random choice csc	0.166667
for full covariance	full x means covars min_covar	0.166667
func	parallel backend base apply async func	0.250000
building a cv in a user	cv cv	0.031250
implements a conditional property using the descriptor protocol	iff has attr descriptor	0.083333
means	means x z	0.250000
non-negative matrix factorization nmf	factorization	0.035714
computes the barycenter	barycenter	0.090909
model to the data x which should contain	x y	0.002155
is the depth	previous func code	0.333333
neighbors within	radius neighbors x	0.166667
and compute	x y	0.004310
a which this function	externals joblib memorized func	0.013158
the kddcup99 dataset	brute kddcup99	0.166667
model with stochastic gradient descent	base sgdregressor partial	0.333333
parametergrid instance for the given param_grid	get param iterator	0.166667
for	cross val predict estimator x	0.045455
embedding read more in the :ref user	embedding	0.040000
score of	model_selection score	0.166667
used when memory is inefficient	y classes	0.027778
curve auc using the trapezoidal rule	metrics auc x	0.040000
values	x y train	0.166667
lfw pairs dataset this dataset is a collection	datasets fetch lfw pairs subset	0.035714
data point	core cross val predict estimator x y	0.045455
compute the euclidean or frobenius norm of	utils norm	0.333333
data covariance	covariance	0.014493
center and scale the	preprocessing robust scaler transform x y	0.200000
and make sure no	non neg array	0.250000
linear embedding	linear embedding x n_neighbors n_components reg	0.200000
from module_path/data/data_file_name	module_path data_file_name	1.000000
predictions using a single binary	binary	0.031250
the deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call y	0.333333
estimators that implement the partial_fit api need to	utils check	0.023810
compute data	pca	0.047619
makes sure centering is not enabled	robust scaler check array x copy	0.333333
the shrunk covariance model according to the given	covariance shrunk covariance fit	0.083333
fit a single tree in parallel	ensemble parallel build trees tree forest x	0.200000
the actual data loading for the lfw	fetch lfw	0.083333
format check x format and	decomposition latent dirichlet allocation check	0.062500
timestamp when pickling to avoid the hash depending	func reduce	0.050000
total	total	1.000000
procedure described in [rouseeuw1984]_ aiming at computing mcd	step x n_support remaining_iterations initial_estimates	0.111111
the sparse components	decomposition sparse pca	0.500000
actual fitting performing	fit x y parameter_iterable	0.500000
when memory is inefficient to train	y classes	0.027778
the log-likelihood of a gaussian	score	0.010101
with ability to accept precomputed	precomp distr	0.250000
determine the	backend base	0.032258
of alpha	alpha	0.058824
private function used to compute log probabilities within	predict log proba	0.029412
fit a binary classifier on x and y	fit binary x y alpha c	1.000000
list	externals	0.005747
bound for c such that for c in	c	0.022222
by a random	n_samples eps	0.125000
test	core base shuffle split iter	0.166667
init	init decision function x	0.142857
with respect to coefs and intercept	activations deltas	0.032258
is meant to	data_folder_path slice_ color resize	0.033333
transform on the estimator with the best found	search cv transform	1.000000
chunking it into mini-batches	cluster mini batch kmeans	1.000000
for parallel processing this method is meant to	parallel	0.019231
gaussian process model	gaussian process predict	0.500000
inside a with	externals joblib	0.004762
task	curve	0.142857
a nicely formatted statement displaying	args kwargs object_name	0.166667
of exception types	externals	0.005747
the derivative	derivative z delta	0.333333
models for feature selection this implements	model	0.058824
check if	utils check	0.023810
least-squares solution to a large sparse	a	0.018182
avoid the hash depending from it	memorized	0.015873
and a	utils	0.009709
locked pipe implementation	pickling queue	1.000000
rcv1 multilabel dataset downloading it if necessary	datasets fetch rcv1 data_home subset download_if_missing random_state	0.500000
compute directory associated with a given cache key	cache key to dir cachedir func argument_hash	1.000000
check initial parameters of	mixture base mixture check parameters	0.200000
the maximum absolute value to be used for	preprocessing max abs	0.050000
draw randomly sampled indices	indices random_state bootstrap n_population n_samples	1.000000
fit with all	cv fit x	0.250000
this classification dataset is constructed	datasets	0.015152
fit x	fit transform x	0.166667
of exception types to be captured	get exceptions	0.166667
back to line_search_wolfe2 if suitable	wolfe12 f fprime xk pk	0.028571
sign correction to ensure deterministic output from svd	svd flip u v u_based_decision	1.000000
store the timestamp when pickling to	func reduce	0.050000
for species	fetch species	0.500000
estimate model parameters	mixture base mixture fit x	0.200000
a contingency matrix describing the relationship between labels	metrics cluster contingency matrix labels_true labels_pred eps	0.200000
cache	externals	0.005747
sure that an estimator implements	core check estimator estimator	0.142857
of last step in pipeline after transforms	core pipeline	0.076923
on an array	check array array accept_sparse	0.250000
restricted to the binary classification task	score y_true y_score average	0.076923
reduced likelihood function for the given autocorrelation	process reduced likelihood function	0.047619
find the least-squares solution to a	lsqr a	0.037037
restricted to the binary classification task	score y_true y_score average sample_weight	0.076923
the leaf	tree apply x	0.166667
mask to edges weighted	edges weights mask edges weights	0.333333
return the disk usage in a directory	externals joblib disk used path	0.250000
return the number of	externals joblib	0.014286
implement a single boost	boost classifier boost iboost x	1.000000
curve	curve y_true	0.125000
list	externals joblib parallel backend	0.029412
process classification based on	process	0.090909
data and	y	0.013369
groups	group	0.083333
from prediction scores note this implementation	metrics roc	0.040000
sparse and	x y sample_weight	0.012987
parameters and evaluates the reduced likelihood function for	gaussian process reduced likelihood function	0.047619
returns the number of splitting iterations in	base kfold get n splits	0.111111
objective function the absolute error of	error	0.020000
factorization nmf find two non-negative matrices	decomposition non negative factorization x	0.043478
hyper parameter search with cross-validation	search cv	0.018182
variance along an axix on a	variance axis x axis	0.090909
validation of y and class_weight	validate targets y	1.000000
prediction scores note this implementation is restricted to	roc	0.033333
compute non-negative matrix factorization nmf find two non-negative	decomposition non negative factorization x	0.043478
the lfw pairs dataset this	lfw pairs	0.018868
a name for the	name	0.033333
calculate mean update and	mean	0.035714
partition estimators	partition estimators	0.200000
compute the score	score	0.020202
x for	x y	0.002155
for each	predict	0.006849
estimator and	y	0.002674
precisions parameters	precisions	0.066667
error regression loss read more	error	0.060000
the value of verbose	verbose	0.062500
absolute error regression loss read	absolute error y_true y_pred	0.142857
compute the per-sample average log-likelihood	mixture base mixture score	0.111111
the weighted graph of neighbors for points	radius neighbors graph	0.066667
function to a given type in the	externals joblib customizable pickler register type	0.083333
the test_size and train_size at init	shuffle split init test_size train_size	0.250000
computes the paired cosine distances between x	paired cosine distances x	0.333333
in multiplicative update	multiplicative update	0.666667
private helper function for factorizing common	fit first call clf	0.200000
types	externals joblib	0.004762
factorization nmf find two	factorization x	0.043478
predict class for x	ensemble gradient boosting classifier predict x	1.000000
the main classification metrics read more in the	metrics classification	0.052632
function used to fit an estimator within a	fit estimator estimator x y sample_weight	0.071429
fit	regression cv fit x	1.000000
values of the basic	initial	0.166667
find the	utils hungarian state find	0.500000
outlier factor of	outlier factor	0.166667
according to a percentile of	percentile	0.100000
the search	core base search cv fit	0.166667
logistic loss and gradient	logistic loss and grad w	0.500000
an array shape under python 2 the long	repr shape	0.166667
score by cross-validation read more in the :ref	cross val score estimator x y	0.166667
sets the flattened log-transformed non-fixed hyperparameters	gaussian_process compound kernel theta theta	0.333333
array of a single sample image parameters	sample image image_name	0.166667
helper to workaround python 2 limitations of	helper obj methodname	0.333333
for	core cross val predict estimator x y	0.045455
this is the time it take	squeeze time	0.166667
optimization objective for the case method='lasso' is :	x y xy gram	0.090909
depending from	externals joblib memorized func	0.013158
compute class covariance matrix	core class cov x y priors	0.250000
evaluate a score by cross-validation	core cross val score estimator x y	0.333333
to predict apply predict_proba	predict	0.013699
to the training set x	x	0.001692
data to	data x y fit_intercept normalize	1.000000
loader for the california housing dataset from	california housing data_home download_if_missing	0.250000
and inertia using a	inertia precompute dense x x_squared_norms centers distances	0.250000
covariance w/ cross-validated choice	cv	0.009009
loading for the lfw people dataset this operation	fetch lfw people	0.040000
the bound term related to proportions	mixture dpgmmbase bound proportions z	0.333333
the beta-divergence of	beta	0.090909
an extra-trees	extra trees	1.000000
gradient boosting for	gradient boosting	0.333333
handle the callable case	pairwise callable x y	0.083333
m step for full cases	mstep full gmm x responsibilities weighted_x_sum	0.250000
for the lfw pairs dataset this	lfw pairs	0.018868
outlyingness	covariance outlier detection mixin predict	0.250000
apply decision function to an array	quadratic discriminant analysis decision function x	1.000000
one	one	0.857143
onto the	ridge_alpha	0.052632
orthogonal matching pursuit omp solves n_targets orthogonal	linear_model orthogonal mp	0.250000
of array-like or scipy sparse	preprocessing binarize	0.083333
a projection to the normalized laplacian	eigen_solver	0.090909
element wise squaring of array-likes and sparse	utils safe sqr x copy	0.125000
the score on	score x	0.033333
classifier on x and y	x y alpha	0.090909
to fit an estimator within a job	parallel fit estimator estimator x y sample_weight	0.333333
the test/test sizes are meaningful wrt	model_selection validate shuffle split n_samples test_size train_size	0.111111
the shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage	0.125000
points in the grid	parameter grid	0.200000
when memory is inefficient to train all data	classes	0.025641
on x and	fit predict x y	0.250000
compute class covariance matrix	core class cov x	0.250000
to x using	transform x	0.016949
score corresponds to the	score y_true y_score	0.025000
parameters and evaluates the reduced likelihood	gaussian process reduced likelihood	0.142857
reducing the	items root_path	0.066667
free energy f v = - log	neural_network bernoulli rbm free energy	0.066667
attach a reducer function to a	externals joblib customizable pickler	0.200000
oracle approximating shrinkage covariance model	covariance oas	0.083333
to a projection	spectral	0.026316
data loading for the lfw people	lfw people	0.040000
objective function the objective	covariance objective	0.125000
init	boosting init decision	0.142857
lower bound for	bound means	0.250000
the labeled faces in the wild lfw	fetch lfw	0.041667
number of splitting iterations in the cross-validator parameters	leave one out get n splits	0.111111
negative value in x	x whom	1.000000
private function used to fit a single tree	trees tree forest x y	0.142857
stochastic gradient descent optimizer	optimizer	0.142857
the decision function	ensemble ada boost classifier decision function	0.166667
estimator and	x y	0.002155
the curve auc using the	metrics auc x	0.040000
m step for diagonal	diag	0.031250
the process or thread pool	externals joblib multiprocessing backend	0.035714
private function used	decision function estimators	0.333333
kernel k x	gaussian_process exponentiation call x	0.200000
perform a locally linear embedding analysis on	manifold locally linear embedding x	0.071429
the range of a	randomized range finder a	0.166667
on left-out data for	x_train y_train x_test y_test	0.200000
generate a sparse random projection matrix parameters	core base random projection fit	0.333333
dot product-based euclidean norm implementation see http //fseoane	decomposition norm	1.000000
type introduces an	shape	0.011765
predict class at each stage for x	gradient boosting classifier staged predict x	1.000000
to compute decisions within a job	estimators_features x	0.111111
fit the	core skewed chi2sampler fit	0.250000
cv in	cv cv x y classifier	0.031250
estimate the spherical wishart distribution	mixture bayesian gaussian mixture estimate wishart spherical	0.333333
csgraph	csgraph directed	0.250000
residues on left-out data for a	residues x_train y_train x_test y_test	0.083333
sparse uncorrelated design this dataset is	datasets make sparse uncorrelated n_samples n_features	0.166667
classification	datasets make classification	0.500000
covariance	covar	0.153846
n_negative	n_negative	1.000000
shortest path length	shortest path length	0.333333
kernel k x y and	gaussian_process dot product call x y	0.200000
true and false positives per binary classification threshold	binary clf curve y_true	0.090909
computes the position of the points in	mds fit x	0.066667
display the	joblib parallel print	0.166667
finds seeds	bin seeds	0.250000
is in a multilabel format	utils is multilabel	1.000000
for each	estimator x y	0.038462
finds seeds	cluster get bin seeds	0.250000
warning used when the	warning	0.083333
probabilities of possible outcomes for samples in	svm base svc predict proba	0.333333
the multi-layer perceptron classifier	neural_network mlpclassifier	0.333333
the weighted graph of k-neighbors for	kneighbors mixin kneighbors graph	0.250000
fit	svm linear svr fit	0.333333
sample weights	compute sample	0.100000
fit the hierarchical clustering on the data	cluster feature agglomeration fit x y	0.250000
main classification metrics	metrics classification	0.052632
x	decision function x	0.037736
of regularization parameters	y pos_class cs	0.166667
the free energy f v = - log	bernoulli rbm free energy	0.066667
to the	spectral	0.026316
computes the paired cosine distances between x	metrics paired cosine distances x	0.333333
given arguments	func get	0.100000
laplacian matrix and convert it to a sparse	laplacian	0.034483
fit	tsne fit	0.333333
lasso path using lars algorithm [1]	linear_model lars path x	0.100000
rbfsampler	rbfsampler	1.000000
perform the mstep	do mstep x responsibilities params	1.000000
number of splitting iterations in the cross-validator	pgroups out get n splits x y	0.111111
don't store	externals joblib memorized func	0.013158
sparse and dense	y sample_weight	0.017857
score by cross-validation read more in the :ref	model_selection cross val score estimator x y	0.166667
compute the loss	loss	0.027027
private function used to build a batch of	build	0.037037
to learning rate and potentially other states at	neural_network base optimizer iteration ends time_step	0.142857
a tolerance which is independent	cluster tolerance x tol	0.058824
chi2sampler	chi2sampler	1.000000
a platform independent representation of	utils shape repr	0.013699
if the estimator has been refit	base search cv	0.026316
kernel k	gaussian_process compound kernel call	0.333333
the significance of a cross-validated	y cv	0.050000
to the binary	y_score average	0.111111
computes	alpha	0.235294
axis center to	x axis	0.030769
cv in a user	core check cv cv x	0.031250
final estimator fits all	core pipeline	0.076923
stochastic gradient descent optimizer with momentum parameters	sgdoptimizer	0.166667
reproducibility flips the sign of elements of all	utils deterministic vector sign flip	0.066667
in hastie	hastie	0.076923
to	transform	0.033708
for indices increasingly apart the	externals joblib verbosity filter index	0.055556
private marker - used in parameter & signature	void	1.000000
which are going to run in parallel	joblib multiprocessing backend effective	0.250000
learn vocabulary and idf from training set	feature_extraction tfidf vectorizer fit raw_documents y	1.000000
to x	x	0.003384
fit a single binary estimator	core fit binary estimator	1.000000
actual fitting performing	x y parameter_iterable	0.500000
repeated k-fold cross validator	repeated kfold	1.000000
subset of dataset and properly handle kernels	safe split estimator x y indices	0.200000
independent	utils shape repr	0.013699
center and scale the	robust scaler transform x y	0.200000
dataset along any axis center to the mean	axis	0.014085
matrix shrunk on the diagonal	covariance shrunk	0.066667
estimates for each	cross	0.037037
list of exception types	joblib parallel backend base	0.058824
then return	rfe	0.100000
back	robust scaler inverse transform	0.066667
when	utils shape	0.013699
found and raise an exception	search	0.019231
scores note this implementation is restricted to the	metrics roc	0.040000
of last step in pipeline after transforms	pipeline fit	0.166667
ensure x min() >= min_value	utils make nonnegative x min_value	1.000000
cache	memory reduce	0.030303
calculates a covariance matrix	covariance emp_cov shrinkage	0.500000
linear embedding analysis	linear embedding x	0.200000
avoid the hash depending	joblib	0.014599
utility for building a cv in a	check cv cv	0.031250
precisions	precisions x	0.250000
x by scaling rows	x	0.001692
anova f-value for the provided	feature_selection f classif x	0.200000
training set according to	fit	0.003257
transforms features by scaling each feature to	minmax scale x feature_range axis copy	0.200000
make and configure a copy of	make estimator append random_state	0.166667
do_prediction	do_prediction	1.000000
of a classification	y_true	0.021739
hash to identify uniquely python objects	joblib hash obj hash_name coerce_mmap	0.250000
building a cv in a	check cv cv x y classifier	0.031250
update w in multiplicative update nmf	multiplicative update w x w h beta_loss	0.500000
apply the derivative of the logistic sigmoid function	neural_network inplace logistic derivative z delta	0.166667
the curve auc	auc	0.040816
kddcup99 dataset downloading it if	datasets fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
thresholding of array-like or scipy sparse	preprocessing binarize x threshold copy	0.083333
feature name -> indices mappings	feature_extraction dict vectorizer fit x	0.250000
w/ cross-validated choice	cv	0.009009
precisions parameters of the precision	precisions	0.066667
output_dir	output_dir	1.000000
fit linear model with stochastic gradient descent	base sgdclassifier fit x y coef_init intercept_init	1.000000
model by using the gp prior	return_std return_cov	0.142857
compute the residual (=	ensemble binomial	0.250000
expression of the dual gap convergence criterion	dual gap emp_cov precision_ alpha	0.071429
elastic net	l1_ratio	0.030303
documents	feature_extraction count vectorizer	0.125000
fit kernel ridge regression model	kernel ridge fit x y sample_weight	1.000000
absolute sizes	train sizes	0.066667
reconstruct the array from the meta-information	joblib zndarray wrapper read unpickler	0.043478
compute the boolean mask x == missing_values	preprocessing get mask x value_to_mask	0.333333
classification	svc	0.111111
data and	y sample_weight	0.017857
timestamp when pickling to avoid the	func reduce	0.050000
in the :ref user guide <univariate_feature_selection>	fwe	0.200000
return a tolerance which is independent of the	tolerance x	0.058824
raise_exception	raise_exception	1.000000
graph of	grid to graph	0.333333
compute non-negative matrix factorization nmf find two non-negative	factorization x	0.043478
computes the weighted graph of neighbors	neighbors graph	0.066667
linear assignment problem using the hungarian	linear assignment x	0.090909
the hash	memory	0.015625
the number of splitting iterations in the cross-validator	model_selection predefined split get n splits x y	0.111111
predict multi-class targets using	output code classifier predict x	0.250000
the cache	joblib memorized func	0.014706
eigenvalues and eigenvectors	m sigma	0.250000
voting classifier valid parameter	ensemble voting classifier set	0.037037
don't store the timestamp when pickling	joblib memory reduce	0.030303
filters	filter args func ignore_lst	0.500000
introduces	shape repr	0.013699
over	feature_selection	0.066667
format check x format and make sure no	latent dirichlet allocation check non neg array	0.500000
or too common	feature_extraction count vectorizer limit	0.250000
curve auc from	auc	0.020408
staged	staged	0.416667
and false positives per binary classification threshold	metrics binary clf curve	0.090909
detects the soft boundary of the set	one class svm fit	0.125000
not found and raise an exception if a	line search	0.029412
shortest path length from source to	single source shortest path length graph source cutoff	0.111111
list of	joblib parallel	0.028571
validation and conversion of csgraph inputs	csgraph directed dtype csr_output	0.166667
of the kl divergence of	manifold kl divergence	0.083333
a transformer from an arbitrary callable	function transformer	0.333333
weighted graph of neighbors for points	neighbors mixin radius neighbors graph	0.066667
compute the initial centroids	cluster init centroids x k init	0.166667
and y read more in	y	0.002674
estimates the shrunk ledoit-wolf	ledoit wolf x assume_centered	0.250000
return the shortest path length	shortest path length graph	0.333333
indices in sorted array of	find matching indices tree bin_x left_mask right_mask	0.166667
training set according	factor fit predict	0.066667
to the median and component wise scale	preprocessing robust scale	0.125000
a job	ensemble parallel	0.250000
edges for a	make edges	0.066667
points on the grid	model_selection parameter grid len	0.333333
voting classifier valid	ensemble voting classifier	0.031250
values for a given	x y train	0.166667
generate indices to split data into	base kfold split	0.250000
split data into	shuffle split split	0.250000
to multi-class labels	y threshold	0.166667
the hasher	hasher	0.111111
split data into training and test set	model_selection base shuffle split split x y groups	0.200000
calculate true and false positives per binary	metrics binary clf curve	0.090909
euclidean or frobenius norm	norm	0.125000
of determination regression score function	metrics r2 score y_true y_pred sample_weight multioutput	0.125000
onto	x ridge_alpha	0.071429
model by computing truncated svd	truncated x n_components	0.200000
kernel k	gaussian_process product call	1.000000
submatrix corresponding to bicluster	bicluster mixin get submatrix	0.333333
x format check x format and	check	0.017857
values for x relative	scorer call estimator x	0.166667
the number of splitting iterations in the cross-validator	leave one group out get n splits	0.111111
the covariance matrices from a given template	matrix to match covariance type tied_cv covariance_type n_components	0.333333
a contingency matrix describing the relationship between labels	metrics cluster contingency matrix	0.200000
lfw pairs	datasets fetch lfw pairs subset	0.035714
parameters with the em algorithm	mixture gmmbase	0.034483
the given autocorrelation parameters theta	theta	0.076923
the boolean mask x == missing_values	mask x value_to_mask	0.333333
lasso path using lars algorithm	linear_model lars path x y	0.100000
parallel	parallel backend	0.030303
graph	feature_extraction img to graph	0.333333
a batch of estimators within	estimators n_estimators ensemble x	0.083333
regularization parameters	path x y pos_class cs	0.166667
unbalanced datasets	weight class_weight y indices	0.200000
scale back the data to the original	preprocessing standard scaler inverse transform	0.066667
lad updates terminal regions to	least absolute error update terminal region tree	0.200000
the posterior log probability of	bernoulli nb joint log likelihood	0.083333
type introduces an 'l'	utils shape repr	0.013699
data to vectors and cluster the	and cluster data vectors	0.333333
actually run in parallel n_jobs is the	n_jobs	0.023256
enough to be merged if	merge subcluster nominee_cluster threshold	0.250000
estimator with the best found	base search cv predict proba x	0.076923
the density model on the data	kernel density	0.083333
used to fit an estimator within a	fit estimator estimator x y	0.071429
disk usage in a	joblib disk used path	0.250000
median and	x y	0.002155
the decision boundary for each class	core one vs rest classifier decision function x	0.250000
estimator on training subsets incrementally and compute scores	model_selection incremental fit estimator estimator x y	0.200000
execution of the function with the given arguments	func	0.011364
category	category	1.000000
bistochastic	bistochastic	1.000000
the number of splitting iterations in the	leave pgroups out get n splits x y	0.111111
integer indices	indices	0.055556
with a given mapping	class_mapping	0.125000
to capture the	check_pickle	0.040000
function called with the given arguments	func	0.011364
the first	first	0.125000
of two clusterings of a set of points	score labels_true labels_pred sparse	0.047619
the 20 newsgroups data and stored	20newsgroups	0.055556
continuous target variable	discrete_features n_neighbors	0.500000
on the estimator with the best found	core base search cv predict	0.076923
inplace column scaling of a	utils inplace column scale	0.166667
estimate the precisions parameters	mixture bayesian gaussian mixture estimate precisions nk xk	0.166667
predict class probabilities at each stage	ensemble gradient boosting classifier staged predict proba	0.500000
store the timestamp when pickling to avoid the	memorized func reduce	0.050000
as	isotonic	0.166667
list of exception types to	backend	0.016949
for c such that for c in (l1_min_c	c x y	0.030303
graph of k-neighbors for points in x parameters	neighbors kneighbors mixin kneighbors graph x n_neighbors mode	0.333333
linear embedding analysis on	linear embedding	0.083333
disk	joblib disk	1.000000
place	code verbose	0.333333
took to run	completed batch_size duration	0.333333
regression problem this dataset is	datasets make	0.015625
a 3d image	3d n_x n_y n_z	0.333333
multiple files in	files files	0.500000
or thread pool	externals	0.005747
thresholding of array-like or scipy sparse	preprocessing binarize x threshold	0.083333
number of splitting iterations in the cross-validator	leave one out get n splits x y	0.111111
estimators within a	estimators n_estimators ensemble x	0.083333
a parallelbackend must implement	parallel backend	0.030303
predicted target values for x relative to y_true	scorer call estimator x y_true sample_weight	0.200000
false positives per binary classification	metrics binary clf curve y_true y_score	0.090909
last_var	last_var	1.000000
matrix shrunk on the diagonal read more	covariance shrunk	0.066667
call with the	format call	0.200000
a diagonal	diag	0.031250
in place using strides	arr patch_shape extraction_step	0.166667
for full covariance matrices	multivariate normal density full x means covars	0.166667
fit linear model with	fit x y coef_init	0.250000
multi-output variable using a model trained for	core multi output estimator	0.142857
each sample from the decision	decision	0.027778
determination regression score	metrics r2 score y_true y_pred	0.125000
problem with sparse	sparse	0.025000
two rows of a csc/csr matrix in-place	utils inplace swap row x m	0.250000
rows of a csc/csr matrix in-place	utils inplace swap row	0.250000
mean	incr mean	0.166667
hash depending from it	externals joblib memorized func	0.013158
all the content of the data home cache	clear data home data_home	0.076923
reconfigure the backend and return the number	externals joblib parallel backend base configure n_jobs	0.333333
for an estimator finds all parameters ending	ensemble set random states estimator	0.333333
estimators within a job	estimators n_estimators	0.083333
inplace column	inplace column scale x	0.166667
the voting classifier valid parameter keys can be	voting classifier set params	0.037037
range	randomized range finder	0.083333
the kernel	kernel operator	0.142857
the estimator with the best found parameters	model_selection base search cv predict proba	0.076923
a locally linear embedding analysis on the	manifold locally linear embedding	0.062500
each input	x y	0.002155
meta-information and the	externals joblib zndarray wrapper read unpickler	0.043478
wild lfw pairs dataset this	lfw pairs subset	0.035714
of moved objects in six moves urllib_request	module six moves urllib request	0.333333
fit with	cv fit	0.200000
compute the l1 distances between the	metrics paired manhattan distances	0.083333
a mostly low rank matrix with bell-shaped	datasets make low rank matrix	0.083333
squared loss for regression	squared loss y_true y_pred	1.000000
get parameter names	get param names cls	1.000000
density lrd the lrd of a sample is	density distances_x neighbors_indices	0.200000
is not found and raise an exception if	line search	0.029412
perform a locally linear embedding	manifold locally linear embedding x	0.071429
fit on the estimator with randomly drawn parameters	core randomized search cv fit x	1.000000
cache folders	externals joblib	0.004762
return the shortest path length from source	single source shortest path length graph source	0.111111
log of the determinant of a wishart the	mixture wishart log	1.000000
predicted target values for x relative to y_true	predict scorer call estimator x y_true sample_weight	0.200000
estimates the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf x	0.125000
data	core	0.015385
swaps two columns of a csc/csr matrix in-place	utils inplace swap column x	0.250000
private helper function for parameter value indexing	model_selection index param value	0.200000
minimum	preprocessing min	0.166667
the estimator with the best found parameters	base search cv	0.052632
energy	energy	1.000000
step	step x	0.333333
back the data to the	preprocessing standard scaler inverse transform x	0.066667
number of splitting iterations in	one out get n splits	0.111111
expected value of the log of the determinant	log	0.018868
neighbors within a given radius	neighbors x radius	0.142857
voting classifier	voting classifier set params	0.037037
predict posterior probability of	predict proba x	0.333333
x	dot product call x	0.200000
given format	format spmatrix accept_sparse dtype copy	1.000000
density estimation read more in the :ref	density	0.043478
build a contingency matrix describing the	contingency matrix	0.166667
the score	core score	0.166667
updating terminal regions (=leaves)	terminal region tree terminal_regions leaf x	0.066667
of exception	parallel	0.019231
class for automagically batching jobs	auto batching mixin	0.333333
main classification	classification	0.071429
to fit a single tree	trees tree forest x	0.142857
build a contingency matrix describing	contingency matrix	0.166667
a single binary estimator	core predict binary estimator	0.200000
the kernel is stationary	kernel operator is stationary	0.333333
the estimator with the best found	core base search cv	0.033333
the validity of	params x metric p metric_params	0.100000
a decision tree regressor	tree decision tree regressor	0.166667
iter	iter	0.250000
depending from	externals joblib memorized	0.013699
train	shuffle	0.083333
type introduces	shape repr	0.013699
convert coefficient matrix	sparse coef	0.071429
of neighbors	neighbors radius neighbors mixin radius neighbors	0.125000
platform independent representation of	utils shape repr	0.013699
inplace row scaling of a csr or csc	utils inplace row scale	0.142857
input validation for standard	utils check x y x y accept_sparse dtype	0.250000
of all hyperparameter specifications	kernel hyperparameters	0.333333
the blup parameters and evaluates the reduced likelihood	process reduced likelihood	0.142857
predict class probabilities for	classifier predict proba	0.250000
a score	score estimator x	0.250000
shortest path length	single source shortest path length	0.333333
label	preprocessing	0.105263
dataset along any axis center to	x axis	0.030769
shutdown the process	externals joblib multiprocessing backend terminate	0.166667
load	load	0.464286
model from	manifold spectral embedding	0.111111
returns first and last element of numpy array	core first and last element arr	0.250000
returns the number of splitting iterations in the	model_selection predefined split get n splits x y	0.111111
estimates for each input	x	0.001692
x (as bigger	decision function x	0.018868
types	parallel backend base	0.037037
compute the boolean mask x == missing_values	mask x value_to_mask	0.333333
implements a conditional property using the descriptor	iff has attr descriptor	0.083333
single tree in parallel	ensemble parallel build trees tree forest x y	0.200000
graph of neighbors for points	neighbors graph	0.066667
for each boosting iteration	ada boost classifier staged	1.000000
the shortest path length from source to	source shortest path length graph source cutoff	0.111111
wise scale	preprocessing robust scale	0.125000
batch and dispatch them	joblib parallel dispatch one batch	0.500000
the data home cache	data home data_home	0.055556
checker utility for building a cv in	cv cv x	0.031250
we don't store the	externals joblib memorized	0.013699
filters	ignore_lst	0.142857
initial centroids parameters	cluster init centroids x k init	0.166667
number of splitting iterations in the cross-validator	model_selection leave pgroups out get n splits x	0.111111
the beta-divergence of x and dot w h	beta divergence x w h beta	0.500000
spherical	spherical resp x nk means	1.000000
parameters theta as the maximizer	arg max	0.047619
passive aggressive algorithm	passive aggressive classifier	0.125000
fn where tp is the number of	score y_true y_pred labels pos_label	0.027778
a calibration	calibration	0.071429
tree	tree forest x y	1.000000
return	datasets get	0.200000
read value from cache	joblib memorized result	1.000000
is equal to the average path length	ensemble average path length	0.090909
the backend	parallel backend base	0.037037
pretty print the dictionary 'params' parameters	core pprint params offset printer	0.250000
introduces an 'l' suffix when	utils shape	0.013699
of	backend base get	0.066667
scale back	inverse transform x	0.051282
k x y and	gaussian_process dot product call x y	0.200000
normalize x by scaling rows and columns	normalize x	0.076923
outliers in a gaussian distributed dataset	elliptic envelope	0.166667
and smooth feature	y	0.005348
return a platform independent representation	repr	0.012500
learn vocabulary and idf return term-document matrix	vectorizer fit transform raw_documents y	0.100000
probability calibration with isotonic regression or sigmoid	calibrated classifier cv	0.071429
the decision function of the	decision function x raw_values	0.083333
net path with	enet path	0.050000
function call with the given	call func	0.100000
the content of the data home	datasets clear data home	0.076923
compute	decomposition	0.047619
integer features using a one-hot aka one-of-k scheme	one hot encoder	0.200000
perform a locally linear	manifold locally linear	0.250000
estimates the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf	0.125000
for scaling	scaler	0.031250
iterate over the points in the grid	parameter grid iter	1.000000
data	val predict estimator x y	0.045455
x relative to	metrics threshold scorer call clf x	0.058824
and evaluates the reduced	process reduced	0.125000
x and y read more	x y	0.002155
it	externals	0.011494
inner fit for	neural_network bernoulli rbm fit v_pos rng	0.250000
best found parameters	base search cv predict proba	0.076923
the validity	x metric p metric_params	0.100000
call wrapped	call	0.052632
nicely formatted statement displaying the function call with	format call func args kwargs object_name	0.333333
range of	range	0.058824
a mostly low rank matrix with bell-shaped	make low rank matrix	0.083333
a score	score estimator x y	0.250000
the right fileobject from a filename	externals joblib read fileobject fileobj filename	0.250000
fit the model to	core multi output estimator fit	0.200000
getter for	covariance empirical covariance	0.071429
file supports seeking	binary zlib file seekable	0.250000
sum_over_features	sum_over_features	1.000000
inplace column	utils inplace column scale x scale	0.166667
transforms one after the other and transforms	x y	0.002155
kddcup99 dataset downloading it if	kddcup99 subset data_home download_if_missing random_state	0.111111
meta estimators in scikit-learn	meta	0.043478
on an array	utils check array array accept_sparse	0.250000
loader for the california housing dataset	fetch california housing data_home download_if_missing	0.250000
fit the rfe model and	rfe fit x y	0.250000
classification by definition a confusion matrix :math c	confusion matrix y_true	1.000000
predict is	labels predict	0.250000
user provided 'weights'	weights weights n_components	1.000000
returns whether the kernel is	gaussian_process pairwise kernel is	1.000000
such that for c in (l1_min_c infinity)	c x	0.030303
a cv	core check cv cv x	0.031250
the data and concatenate	y	0.002674
check input and compute prediction of init	init	0.076923
remove a subcluster	subcluster new_subcluster1	0.166667
for unbalanced datasets	weight class_weight	0.200000
whether the	pairwise	0.066667
target	regressor	0.054054
boosted regressor from	ensemble ada boost regressor	0.333333
scale back the	robust scaler inverse transform	0.066667
compute	decomposition base	0.076923
when memory is inefficient	x y classes	0.027778
to split data into training and test	series split split x y groups	0.200000
the function call with	externals joblib format call func	0.100000
locally linear embedding analysis	manifold locally linear embedding x n_neighbors n_components	0.071429
fetch an mldata org data set	datasets fetch mldata dataname target_name data_name transpose_data	1.000000
net path with coordinate descent the	enet path	0.050000
returns posterior probabilities	cv predict proba	0.034483
compute the laplacian kernel	laplacian kernel	0.166667
for creating a class with a metaclass	externals add metaclass metaclass	0.166667
sample	compute sample	0.100000
y	y sample_weight	0.089286
matching pursuit	n_nonzero_coefs tol	0.250000
low rank matrix with bell-shaped singular values	low rank matrix	0.083333
fit the hierarchical clustering on	cluster feature agglomeration fit x	0.250000
func to	backend apply async func	0.250000
grid of points based on	grid from	0.166667
restricted to the binary classification task	metrics precision recall curve y_true	0.142857
to avoid the hash depending from it	memorized func	0.016949
dataset in	datasets	0.015152
number of splitting iterations in	group out get n splits	0.111111
we don't	memory	0.015625
best found	core base search cv predict	0.076923
euclidean or frobenius norm of	norm	0.125000
shutdown the process or thread pool	backend base terminate	0.500000
returns the number of splitting iterations in	cviterable wrapper get n splits	0.111111
area under the curve auc from prediction	auc	0.020408
of csgraph inputs	graph csgraph directed	0.250000
return the shortest path length from source	source shortest path length graph source cutoff	0.111111
free energy f v = - log sum_h	rbm free energy	0.066667
the weighted graph of neighbors for points in	neighbors mixin radius neighbors graph	0.066667
that implement the partial_fit api need to be	utils check partial	0.038462
avoid the hash depending from	joblib memorized	0.015625
zero_based	zero_based	1.000000
the search over	search cv fit x y	0.111111
passive aggressive algorithm	linear_model passive aggressive classifier	0.250000
score	score x y	0.060606
reduction for memmap backed arrays	memmap backed a	0.333333
for parallel processing this method is meant	joblib parallel	0.028571
format check x format and make sure no	decomposition latent dirichlet allocation check non neg array	0.500000
compute mean and variance	mean variance	0.166667
pickle-protocol - return state of the estimator	core isotonic regression getstate	0.250000
the paired distances between x	metrics paired distances x	0.500000
terminal regions (=leaves)	loss function update terminal region tree terminal_regions leaf	0.200000
in data	data	0.038462
inverse the	transform inverse	0.500000
unfitted model by using the gp prior	return_std return_cov	0.142857
perform dbscan clustering from	cluster dbscan x	0.200000
for each	x y	0.002155
call transform on the estimator with	transform	0.011236
this dataset is described in celeux et al	datasets	0.015152
whose range approximates the range of	utils randomized range finder	0.083333
in multiplicative update nmf	decomposition multiplicative update h x w	0.500000
fit the gradient boosting model	base gradient boosting fit	1.000000
c_step procedure described in [rouseeuw1984]_ aiming at computing	covariance c step x n_support remaining_iterations initial_estimates	0.111111
random regression problem with sparse uncorrelated design	sparse uncorrelated n_samples	0.166667
classifier from the training set x y	classifier fit x y	1.000000
compute area under the curve auc	metrics auc x y	0.040000
perform a locally linear embedding	manifold locally linear embedding x n_neighbors	0.071429
nmf initialization	decomposition initialize nmf x	1.000000
wild lfw pairs dataset this dataset is	lfw pairs subset	0.035714
the	covariance empirical	0.100000
set	model set	0.500000
force the execution of the function with	externals joblib memorized	0.013699
data	datasets load data	0.500000
the directory in which	output dir	0.047619
the range of	randomized range	0.083333
of the parallel execution only a fraction	externals joblib parallel	0.014085
step length is not found and raise	search	0.019231
a	externals joblib parallel backend base	0.068966
truncated randomized svd parameters	randomized svd m n_components n_oversamples n_iter	0.500000
loss and gradient	loss and grad w x y	1.000000
cross-validated estimates	estimator x y cv	0.050000
l1 distances between the vectors in	metrics manhattan distances	0.083333
matrix shrunk	covariance shrunk	0.066667
the file supports seeking	externals joblib binary zlib file seekable	0.250000
or	externals joblib multiprocessing	0.052632
loading of moved objects in six moves urllib_robotparser	module six moves urllib robotparser	0.333333
regression model we can also predict based	regressor predict x	0.166667
kernel is	kernel mixin is	0.333333
of init	boosting init	0.142857
samples can be different from the	calibrated classifier cv	0.071429
called with the given arguments	memorized func get output	0.125000
with the best found parameters	base search cv predict proba x	0.076923
the model parameters	mixture base mixture	0.111111
univariate feature selector with configurable strategy	generic univariate select	1.000000
and return the wine	wine	0.111111
data point	core cross	0.045455
is restricted to the binary classification task	y_true y_score average sample_weight	0.076923
the wishart distribution	mixture wishart	0.125000
log probability	log multivariate normal density	0.250000
perform a locally linear embedding analysis	manifold locally linear embedding	0.062500
add an item to six moves	externals add move move	0.333333
function with the given arguments and persist	func call	0.047619
step in pipeline after transforms	core pipeline fit predict	0.166667
model parameters	fit x	0.012821
of neighbors for	neighbors radius neighbors mixin radius neighbors	0.125000
store the	memorized	0.015873
used to build a batch of estimators within	parallel build estimators n_estimators ensemble x	0.166667
breast	breast	0.666667
class weights for	compute class	0.166667
dense	dense	1.000000
rows of a csr matrix in-place	utils inplace swap row csr x	0.250000
absolute value of	abs	0.083333
compute the decision function of x	ensemble ada boost classifier decision function x	0.333333
dataset	ensemble random trees	1.000000
of exception types to be captured	base get exceptions	0.166667
matrix whose range approximates the range of	utils randomized range	0.083333
checker utility for building a cv	check cv cv	0.031250
boosted classifier/regressor	ensemble base weight boosting	0.333333
all the vectors rows of u such	u	0.032258
any axis center to	axis	0.028169
function used to fit a single tree	build trees tree	0.142857
out	out	0.333333
fit the model by computing full svd	pca fit full	1.000000
makes sure centering is	scaler check array x copy	1.000000
training set x and returns the labels	x y	0.002155
in	memory	0.015625
perform the covariance m step for	mixture covar	0.125000
a size limit	externals joblib	0.004762
cholesky decomposition of	cholesky	0.083333
pairwise matrix	pairwise x	0.166667
back the	scaler inverse transform	0.058824
arguments of a function	externals joblib delayed function	0.200000
nothing and return the estimator unchanged this method	feature_extraction	0.037037
actual data loading for the lfw people dataset	lfw people	0.040000
avoid the hash depending from	externals	0.011494
embedding space parameters	manifold	0.100000
subsets incrementally	model_selection incremental	1.000000
apply clustering to a projection to the	clustering	0.050000
when using the	shape	0.011765
different probability thresholds note this implementation is	probas_pred pos_label sample_weight	0.066667
main classification metrics read more in	metrics classification	0.052632
is zero we handle it correctly	handle zeros in	1.000000
avoid the hash depending	externals joblib memorized	0.013699
and persist the output values	call	0.052632
transforms one after the other and transforms	y	0.002674
on x and y	x y alpha c	0.333333
sure centering is	scaler check array	1.000000
it take to	externals joblib squeeze	1.000000
build a batch of	parallel build	0.047619
files	files	0.700000
incrementally fit the model	core multi output regressor partial fit x	0.200000
approximates the range of a	utils randomized range finder a	0.166667
diagonal structure for	biclusters shape n_clusters noise minval	0.058824
type introduces an 'l' suffix when using the	utils	0.009709
the	gaussian mixture	0.200000
init	model_selection validate shuffle split init	1.000000
hash	joblib memorized func	0.014706
finite	finite	1.000000
right fileobject from a	externals joblib read fileobject	0.100000
for x relative	metrics threshold scorer call clf x y sample_weight	0.058824
the curve auc	metrics auc x y	0.040000
of estimators within a	estimators n_estimators ensemble x y	0.083333
the decision function of the given	decision function x raw_values	0.083333
get the	func get	0.100000
hash depending	externals joblib memorized func	0.013158
a grid of points	grid	0.040000
the decision functions	decision function x	0.018868
support_fraction	support_fraction	1.000000
the oracle approximating shrinkage covariance	covariance oas fit	0.083333
the local	neighbors local outlier factor local	0.500000
for each input data	estimator x	0.030303
the	externals joblib memorized	0.027397
set the sample	sample	0.032258
area under the curve auc using	metrics auc	0.040000
paired cosine	metrics paired cosine	0.333333
list of	get	0.012048
project data to vectors and cluster the result	project and cluster data vectors n_clusters	0.333333
generate	core cross	0.045455
[rouseeuw1984]_ aiming at computing	x n_support remaining_iterations initial_estimates	0.111111
estimate class weights for unbalanced datasets	utils compute class weight class_weight classes	0.500000
to	linear_model sparse coef mixin	0.090909
persist an arbitrary python object into one file	dump value filename compress protocol	0.250000
error regression loss	log error	0.142857
implementation is restricted to the binary classification task	score y_true y_score average sample_weight	0.076923
with non-overlapping	group	0.083333
compute the log-det of the cholesky decomposition	mixture compute log det cholesky matrix_chol	0.500000
to fit an estimator within a job	ensemble parallel fit estimator estimator x y	0.333333
count and smooth feature occurrences	core multinomial nb count x y	0.250000
function to emulate function total_seconds introduced in python2	utils total seconds delta	0.200000
outcomes for	ensemble voting classifier	0.031250
generate a sparse random projection matrix parameters	core base random projection fit x y	0.333333
data set with self	x_test y	0.142857
array from the meta-information and the	externals joblib zndarray wrapper read unpickler	0.043478
is not found and	line search	0.029412
platform independent	utils shape	0.013699
a batch of estimators within a	estimators n_estimators ensemble x y	0.083333
given mapping	y class_mapping	0.333333
estimate model parameters with the em	mixture gmmbase fit x y	0.250000
number of points on the grid	model_selection parameter grid len	0.333333
the svmlight / libsvm format into sparse	svmlight file f n_features dtype multilabel	0.066667
platform independent	shape	0.011765
don't store the timestamp when pickling	memorized func reduce	0.050000
returns whether the kernel	gaussian_process kernel	0.333333
a cv in a user friendly	check cv cv x y	0.031250
to split data into training and test	model_selection cviterable wrapper split x y groups	0.200000
of edges	edges	0.047619
for	core cross val predict estimator	0.045455
descriptors of a memmap instance to	memmap a	0.050000
the samples x to	x	0.003384
classification dataset is	datasets	0.015152
non-negative matrix factorization nmf	decomposition non negative factorization	0.043478
memory is inefficient to train all data	x y classes	0.027778
arbitrary python object into one	dump value filename	0.083333
number of splitting iterations in	leave one group out get n splits	0.111111
generate indices to split data into	model_selection base kfold split x	0.250000
classification this function returns posterior probabilities of classification	core calibrated classifier cv predict proba	0.200000
absolute value to	abs	0.083333
avoid the hash	memory	0.015625
factorize density check according to li et	core check density density n_features	0.166667
minimum covariance determinant matrix	covariance fast mcd x support_fraction cov_computation_method random_state	0.250000
fit linear model	fit x y coef_init	0.250000
number of splitting iterations in the cross-validator parameters	get n splits x	0.111111
the diagonal of the laplacian matrix and	diag laplacian	0.111111
transforms features by scaling each	scale x feature_range axis copy	0.200000
module names and a name	func name	0.047619
returns a list of feature	feature_extraction dict vectorizer get feature	0.200000
compute the initial centroids	cluster init centroids x	0.166667
solve the isotonic regression model	isotonic regression	0.055556
test indices	iter indices	0.250000
score of the underlying estimator	score	0.010101
for x relative	scorer call estimator x	0.166667
transform	transform y	0.069767
cv	check cv cv x y	0.031250
wrapped function cache	externals joblib memorized func	0.013158
subset of dataset and	y	0.002674
of transform is sometimes referred to	transform y	0.023256
run fit with all	grid search cv fit x y	0.333333
input and compute prediction of init	init	0.076923
load the kddcup99 dataset downloading it if necessary	brute kddcup99 subset data_home download_if_missing random_state	0.111111
prediction of init	boosting init decision function x	0.142857
too common features	count vectorizer limit features x vocabulary	1.000000
an estimator within a	estimator estimator x y	0.333333
a locally linear embedding	locally linear embedding x	0.071429
input data point	x y	0.002155
weighted graph of k-neighbors for points in x	kneighbors mixin kneighbors graph x n_neighbors mode	0.333333
swaps two rows of a csc/csr matrix	utils inplace	0.250000
fit_predict of last step in pipeline after transforms	core pipeline fit predict x	0.166667
loads data	load data	0.500000
update	update dict	0.333333
find the first prime	state find prime	0.500000
the grid	grid	0.120000
check values of the basic parameters	base mixture check initial parameters	1.000000
of exception	parallel backend base	0.037037
content of the data	clear data	0.142857
the dual gap convergence criterion the specific	covariance dual gap emp_cov precision_	0.071429
boolean thresholding of array-like or scipy sparse matrix	binarize x threshold	0.083333
estimates for each input data point	val predict estimator	0.045455
shrunk ledoit-wolf covariance matrix	covariance ledoit wolf x	0.125000
inplace column scaling	inplace column scale x scale	0.166667
the training set	fit	0.016287
bicluster	bicluster mixin	0.333333
persist an arbitrary python object	value filename compress protocol	0.250000
estimators	estimators estimators	0.500000
fit linear model with stochastic gradient descent	linear_model base sgdregressor fit	1.000000
to be used for later	fit	0.003257
determine the optimal batch	parallel backend base compute batch	1.000000
laplacian matrix and convert it to	laplacian	0.034483
fit the model	skewed chi2sampler fit	0.250000
fit the model by	decomposition pca fit	0.125000
random multilabel classification problem	multilabel classification	0.166667
transform is sometimes referred	transform	0.011236
helper	helper alpha y v u	0.333333
the decision functions	decision function	0.025000
c in	c x y loss fit_intercept	0.030303
types	parallel	0.019231
later	fit x y	0.005988
distances between	distances	0.250000
the right compressor file object in write mode	externals joblib write fileobject filename compress	0.500000
a single tree in parallel	ensemble parallel build trees tree	0.200000
find the first prime element in the specified	utils hungarian state find prime in	0.333333
coder	coder	1.000000
indices to split data according to a	split	0.027778
number of patches that will be extracted	n patches i_h i_w p_h p_w	0.333333
of an array shape under python 2	utils shape repr shape	0.166667
the number of splitting iterations in the cross-validator	one out get n splits x	0.111111
helper to workaround python 2 limitations of	parallel helper obj methodname	0.333333
create subset of dataset and properly handle kernels	utils safe split estimator x y indices	0.200000
return whether the file supports seeking	file seekable	0.250000
compute elastic net path with coordinate descent	linear_model enet path x	0.050000
neighbors within a	neighbors lshforest radius neighbors	0.166667
undo the scaling of x according to feature_range	min max scaler inverse transform x	0.250000
of vectors for reproducibility flips the sign	utils deterministic vector sign flip	0.066667
feature names ordered by their	dict vectorizer get feature names	0.142857
component wise scale to unit variance	preprocessing scale x	0.090909
types to	externals joblib	0.004762
argmin	argmin	1.000000
an array shape under python 2 the	utils shape repr shape	0.166667
process or	externals joblib	0.004762
be used when memory is inefficient	x y classes	0.027778
center to the mean and component wise scale	scale x	0.043478
range of	utils randomized range finder	0.083333
by the callers	effective	0.090909
image from	from	0.045455
fp where tp is the number	score y_true y_pred labels pos_label	0.027778
the set of samples x	x	0.001692
update for	step	0.125000
helper function for factorizing common classes param logic	fit first call clf classes	0.058824
for each input data point	estimator	0.014706
opposite of the local outlier factor of	neighbors local outlier factor decision	0.125000
transforms and	y sample_weight	0.017857
global	birch global	1.000000
step length is not found and raise an	utils line search	0.029412
the neighbors within	lshforest radius neighbors x	0.166667
estimate the spherical variance values	mixture estimate gaussian covariances spherical resp	1.000000
list of	backend	0.016949
the	pca	0.047619
load datasets in the svmlight / libsvm	datasets load svmlight file f	0.500000
by its spectrum spectrum	spectrum	0.090909
size=none replace=true p=none) generates a random sample from	size	0.032258
x and dot w	divergence x w	0.500000
finds seeds	bin seeds x	0.250000
estimate the	bayesian gaussian mixture estimate	1.000000
the nonzero componentwise l1 cross-distances between the vectors	gaussian_process l1 cross distances	0.111111
call predict_log_proba	predict log proba	0.029412
rand index adjusted for	cluster adjusted rand score labels_true labels_pred	0.333333
compute the maximum absolute value to be used	preprocessing max abs	0.050000
loss	ensemble loss	0.166667
sparse and	x y	0.002155
operation is meant to be cached	data_folder_path slice_ color resize	0.033333
build from the c and cpp files	build from c and cpp files	0.500000
an estimator	estimator estimator x y sample_weight	0.333333
compute area under the curve auc using the	auc x y	0.040000
shortest	single source shortest	0.333333
predict using the multi-layer perceptron classifier	neural_network mlpclassifier predict x	0.500000
the hash depending from it	func	0.011364
list of regularization parameters	pos_class cs	0.166667
number of splitting iterations in the	kfold get n splits x	0.111111
x and returns	transform x y w	0.500000
process or thread pool	backend	0.033898
fit linear model	base sgdclassifier fit	0.076923
unnormalized posterior log probability of	nb joint log likelihood	0.033333
recall is the ratio	recall	0.028571
independent representation of	repr	0.012500
model parameters with the em algorithm	mixture gmmbase fit	0.250000
used when memory is inefficient to train all	x y classes	0.027778
binary labels back	preprocessing label binarizer inverse	0.166667
factorization	decomposition non negative factorization	0.043478
the kddcup99 dataset	kddcup99	0.090909
the california housing dataset from statlib	datasets fetch california housing	0.083333
select features according to a percentile of the	select percentile	0.333333
a buffered	joblib buffered	0.333333
create subset of dataset and properly handle kernels	safe split estimator x y indices	0.200000
a	externals joblib pool manager	1.000000
the wild lfw pairs dataset	fetch lfw pairs	0.018868
returns the score on	score	0.010101
sparse combination of the dictionary atoms	decomposition sparse coding mixin transform x	0.333333
a transform function to portion of selected features	x transform selected copy	0.333333
compute the per-sample average log-likelihood of the given	mixture base mixture score	0.111111
sample from a given 1-d array	utils choice a size replace	0.250000
list of edges for	feature_extraction make edges	0.066667
back the data to the	robust scaler inverse transform x	0.066667
pipeline after transforms	core pipeline fit predict	0.166667
helper function for factorizing common classes	fit first call clf classes	0.058824
isotonic regression model : min sum w[i] (y[i]	isotonic regression y	0.066667
dual gap convergence criterion the specific	dual gap	0.071429
of x for	x	0.001692
building a cv in	check cv cv x y classifier	0.031250
kl divergence of p_ijs	kl divergence	0.083333
scaler	max scaler	0.333333
estimate the spherical wishart distribution	bayesian gaussian mixture estimate wishart spherical nk	0.333333
linear model with passive aggressive algorithm	passive aggressive regressor	0.125000
functions of the	function	0.021277
the data and concatenate results	x y	0.002155
weighted graph of neighbors for points	neighbors graph	0.066667
split data into training and test	predefined split split x y groups	0.200000
fit the model	core skewed chi2sampler fit	0.250000
suitable step length is not found and	line search	0.029412
fit ridge regression	ridge cv fit x y sample_weight	1.000000
data point	estimator x	0.030303
the l1	metrics manhattan	0.333333
the linear assignment problem using the hungarian	linear assignment x	0.090909
validation of y and class_weight	lib svm validate targets y	1.000000
ledoit-wolf covariance matrix	covariance ledoit wolf x	0.125000
loss and the gradient	loss and gradient w x y	1.000000
min	min	0.272727
svmlight format this function is equivalent to	svmlight	0.050000
for the precisions	precisions	0.066667
fit multitaskelasticnet	multi task elastic net fit	1.000000
we don't store	externals joblib memory	0.016949
csc matrix	csc	0.166667
fit estimator and compute scores for	core fit and score estimator x y	0.333333
the lfw people dataset this operation is meant	datasets fetch lfw people data_folder_path slice_ color resize	0.333333
cache folders	memory reduce	0.030303
the process	joblib multiprocessing backend	0.052632
shutdown	externals joblib multiprocessing backend terminate	0.166667
is restricted to the binary classification task	y_true y_score pos_label	0.066667
usual api and hence	patch extractor fit x y	0.142857
a binary	metrics average binary	0.500000
compute data precision matrix with the generative	pca get precision	0.066667
relative	scorer	0.045455
true and false positives per	clf curve y_true	0.250000
x y	x y max_samples	1.000000
estimates the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf x assume_centered block_size	0.125000
a binary	binary	0.062500
data	transform	0.011236
remove cache folders to	externals joblib	0.004762
precision matrix with the generative model	precision	0.016667
fit the model	estimator fit x y	0.200000
the best found	search cv predict proba x	0.076923
delete all the content of the data home	data home	0.076923
of givens rotation	utils sym ortho a b	0.250000
fit the model	core multi output estimator fit x y	0.200000
python object	joblib dump value filename	0.083333
each	estimator	0.014706
the scaling	scaler inverse	0.250000
input validation on an array	array array	0.166667
n_init	n_init	1.000000
random regression problem with sparse uncorrelated design	make sparse uncorrelated n_samples	0.166667
write array bytes	wrapper write array array	0.500000
cache for the function	memorized func	0.016949
elements of all the vectors rows of u	u	0.032258
such that for c	c x y	0.030303
the image from	from	0.045455
number of splitting iterations in	split get n splits	0.111111
dataset and	y	0.002674
predict class probabilities at each stage for	ensemble gradient boosting classifier staged predict proba	0.500000
check that	metrics cluster check	0.500000
to split data into training and test	shuffle split split x y groups	0.200000
in the context of the memory	memory	0.015625
protocol	protocol	1.000000
don't store the timestamp when pickling to avoid	externals joblib memory reduce	0.030303
dispatch	parallel dispatch	0.250000
a biclustering for x	x	0.001692
make cache size fit	memory reduce size	0.083333
max_iter	max_iter	1.000000
return the directory in which are	output dir	0.047619
isotonic regression model :	core isotonic regression y	0.066667
terminate	terminate	0.454545
predict class at each stage for x	classifier staged predict x	0.500000
dual gap convergence criterion the specific definition	dual gap emp_cov precision_ alpha	0.071429
computes the weighted graph of neighbors for points	neighbors radius neighbors graph	0.066667
the kddcup99 dataset downloading it if	fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
all the content of the data home cache	datasets clear data home	0.076923
estimate model parameters with the em algorithm	mixture gmmbase fit x y do_prediction	0.250000
curve auc using the	metrics auc x y	0.040000
class used to hash	hash	0.083333
compute	compute log	1.000000
from source to	graph source cutoff	0.200000
the local	neighbors local	0.250000
indices to split data into	shuffle split split x	0.250000
multi-class targets using underlying estimators	core output code classifier	0.250000
for label spreading computes the graph	semi_supervised label spreading build graph	0.142857
the shrunk covariance model according to	covariance shrunk covariance fit x	0.083333
blup parameters and evaluates the reduced	process reduced	0.125000
the grid of alpha values for	alpha grid	0.166667
samples by quantile this classification dataset is constructed	datasets	0.015152
returns the number of splitting iterations in the	out get n splits x y	0.111111
set the diagonal of the	manifold set diag	0.333333
remove cache folders to make cache size fit	memory reduce size	0.083333
the validity of the	metric p metric_params	0.100000
store the timestamp when pickling to avoid the	func reduce	0.050000
biclustering for x	x	0.001692
mixin class for all density estimators in scikit-learn	density mixin	1.000000
faces in the wild lfw pairs dataset	lfw pairs subset	0.035714
a cv in a user friendly	cv cv	0.031250
exceptions	utils assert raise message exceptions	1.000000
the scaling of	scaler	0.031250
maximum absolute value to	max abs	0.047619
along any axis center to the median	axis	0.014085
score function best possible score is	score y_true y_pred sample_weight multioutput	0.062500
provided precisions	precisions precisions covariance_type n_components	0.250000
predict multi-output variable using	core multi output estimator predict	0.166667
returns the number of splitting iterations in the	kfold get n splits x y groups	0.111111
a cv in a user friendly way	cv cv x	0.031250
estimate model parameters	base mixture fit	0.200000
and compute prediction of init	init decision function	0.142857
later scaling	scaler fit x y	0.200000
the shortest path length from source to all	single source shortest path length graph source cutoff	0.111111
bound term related to proportions	mixture dpgmmbase bound proportions z	0.333333
batch and dispatch	externals joblib parallel dispatch one batch	0.500000
compute mean	mean	0.035714
input data point	predict estimator	0.045455
check according to li et al	check	0.017857
the laplacian kernel between x and y	laplacian kernel x y gamma	0.333333
of points	len	0.038462
this dataset is described in friedman [1]	datasets make	0.015625
with sparse	make sparse	0.125000
position of	mds fit x y	0.066667
the beta-divergence	beta	0.090909
a score by cross-validation read more in	model_selection cross val score	0.166667
the voting classifier valid parameter keys can	voting classifier	0.035714
wild lfw pairs dataset this	datasets fetch lfw pairs	0.018868
the feature importances	tree base decision tree feature importances	0.333333
load datasets	datasets load	0.083333
private helper function for parameter value indexing	model_selection index param value x v	0.200000
meta-information and the	joblib zndarray wrapper read unpickler	0.043478
cv	cv cv x y classifier	0.031250
decision function of	decision function x raw_values	0.083333
return the directory in	dir	0.038462
predicts one class versus all others	multiclass x y alpha	0.166667
the kernel k x	gaussian_process dot product call x	0.200000
estimates the shrunk ledoit-wolf covariance	covariance ledoit wolf x assume_centered	0.125000
calculate	calculate	1.000000
description	description	1.000000
list of exception	externals joblib parallel backend	0.029412
determination regression score function	r2 score y_true y_pred	0.125000
to unit norm parameters	preprocessing normalizer transform	1.000000
the backend	parallel backend	0.030303
with the given arguments	func	0.022727
number of splitting iterations in	one group out get n splits	0.111111
number of splitting iterations in the	model_selection base kfold get n splits x	0.111111
zero row of x	transform x y	0.031250
reconstruct the array from the meta-information and	zndarray wrapper read unpickler	0.043478
building a cv	core check cv cv x y	0.031250
multinomial	multinomial	0.666667
capture the arguments of a function	function check_pickle	0.333333
dataset	random trees	1.000000
inplace column scaling of	utils inplace column scale x	0.166667
from	joblib	0.014599
inside a	externals	0.005747
a	shape repr	0.013699
input data point	core cross val predict estimator x y	0.045455
remove too rare or	feature_extraction	0.037037
c_step procedure described in [rouseeuw1984]_ aiming at computing	c step x n_support remaining_iterations initial_estimates	0.111111
kernel k x y and optionally its gradient	x y eval_gradient	0.210526
cv in a user friendly way	core check cv cv	0.031250
data matrix x	x	0.001692
covariance	mixture distribute covar matrix to match covariance	0.250000
for building a cv in a	core check cv cv x	0.031250
of the determinant	det	0.071429
to compute log probabilities within a job	parallel predict log proba estimators estimators_features x	0.250000
lad updates terminal regions to median estimates	ensemble least absolute error update terminal	0.200000
a score by cross-validation read more in	cross val score estimator	0.166667
clusterer	clusterer	1.000000
estimates for each	x	0.001692
the maximizer	arg max	0.047619
of x and dot w	decomposition beta divergence x w	0.500000
from the	from	0.045455
with the	decomposition	0.047619
a batch of estimators within a job	estimators n_estimators ensemble x	0.083333
check the user provided 'weights'	mixture check weights weights n_components	1.000000
online computation of	partial	0.086957
cv in a user friendly way	check cv cv x y classifier	0.031250
reproducibility flips the sign of elements of all	deterministic vector sign flip	0.066667
sign of vectors for reproducibility flips the sign	deterministic vector sign flip	0.066667
for label	label	0.045455
california housing dataset from	california housing	0.083333
generate train	base shuffle	0.166667
k-fold iterator variant	label kfold	0.250000
the data as a sparse	sparse	0.025000
for parameter value indexing	model_selection index param value x v indices	0.200000
shutdown the process or thread	externals joblib parallel backend base terminate	0.500000
classification dataset is	datasets make	0.015625
cluster the	cluster	0.021277
log-likelihood of a gaussian data set with	covariance score	0.071429
mean squared logarithmic	metrics mean squared log	1.000000
elastic net optimization function varies for mono and	y l1_ratio	1.000000
the training set according	fit predict	0.055556
and compute prediction of init	init decision function x	0.142857
in x according to the	x	0.001692
under the curve auc from prediction	auc	0.020408
data covariance with	covariance	0.028986
cache folders to make cache size fit in	memory reduce size	0.083333
check if estimator adheres to scikit-learn	utils check estimator estimator	0.250000
false positives per binary	binary clf curve	0.090909
to bicluster	bicluster mixin	0.333333
and cv and	x y	0.002155
test	shuffle split	0.142857
random sample from a given 1-d array	utils choice a size	0.250000
load the covertype dataset downloading it if necessary	datasets fetch covtype data_home download_if_missing random_state shuffle	0.333333
used to fit a single tree in parallel	ensemble parallel build trees tree forest x	0.200000
elastic net path with	linear_model enet path x	0.050000
the local	factor local	0.500000
target_dir	target_dir	1.000000
iteritems but accepts any collections mapping	feature_extraction iteritems	1.000000
compute minimum and maximum	min max axis	0.500000
ensure that x	predict x	0.011765
dense array	coef mixin densify	0.100000
full covariance matrices	normal density full x	0.166667
pairwise matrix	metrics parallel pairwise x	0.166667
output for x relative to y_true	metrics threshold scorer call clf x y	0.058824
cache folders to	externals joblib	0.004762
cut	cut	1.000000
density model on	kernel density	0.083333
fit the rfe model and then	rfe fit x y	0.250000
model to the training set x and returns	predict x y	0.043478
returns the number of splitting iterations in the	out get n splits	0.111111
stochastic average gradient the gradient of the loss	loss	0.027027
the sign of vectors for reproducibility flips the	utils deterministic vector	0.076923
store	externals joblib memory	0.016949
uses the benjamini-hochberg procedure	fdr	0.142857
fit linear model with stochastic gradient descent	base sgdregressor partial fit x y sample_weight	1.000000
fit the model according to the	svc fit x y sample_weight	0.250000
the wild lfw pairs dataset this	fetch lfw pairs subset	0.035714
dump	dump	1.000000
precision matrix with the generative model	get precision	0.052632
check the input data x	mixture check x x n_components n_features	1.000000
two	cc	0.166667
update the dense dictionary factor in	update dict dictionary	0.333333
covariance determinant matrix	covariance fast mcd	0.250000
number of splitting iterations in the	leave pgroups out get n splits x y	0.111111
fit	manifold tsne fit	0.333333
in svmlight format	svmlight	0.050000
the long type introduces an	shape repr	0.013699
store the timestamp when pickling	reduce	0.034483
distributions for the precisions	precisions x z	0.250000
an 'l'	utils shape repr	0.013699
used by parallel inside a with block	externals joblib parallel	0.014085
em update	dirichlet allocation em step	0.500000
spectral co-clustering algorithm dhillon 2001	spectral coclustering	1.000000
a locally linear embedding analysis on	manifold locally linear embedding x	0.071429
estimates for each input data point	cross val	0.038462
compute data precision matrix	pca get precision	0.066667
the svmlight / libsvm format into sparse csr	svmlight file f n_features	0.066667
check	check	0.339286
breiman [2]	friedman3 n_samples noise	0.166667
compute non-negative matrix factorization nmf	decomposition non negative factorization x	0.043478
each input data	x	0.001692
spectral biclustering kluger 2003	spectral biclustering	1.000000
the number of splitting iterations in	one group out get n splits x y	0.111111
function for factorizing common classes	first call clf classes	0.058824
cholesky	cholesky	0.416667
estimates for each input	cross	0.037037
regression cv aka logit maxent classifier	regression cv	0.200000
target of new samples can be different from	core calibrated classifier	0.083333
thread pool and return the number	externals joblib multiprocessing backend	0.035714
l1 distances between the vectors	metrics manhattan distances	0.083333
folders to make cache size fit in	memory reduce size	0.083333
non-negative matrix factorization nmf	negative factorization	0.043478
n_jobs	x y func n_jobs	0.166667
covariance	cov x y priors	0.500000
compute the loss of prediction pred and y	loss function call y pred sample_weight	0.333333
component wise scale	scale	0.066667
check x format check x	check	0.017857
loader for the california housing dataset from	california housing data_home	0.250000
generate	cross val predict	0.045455
of x	x y	0.004310
matrix product with the random matrix	random projection	0.125000
a	externals joblib sequential backend apply async	1.000000
[1] and breiman [2]	friedman3 n_samples noise random_state	0.166667
data to	data x y fit_intercept	1.000000
under windows this is the time it take	squeeze time t	0.166667
thread pool and return the	externals joblib multiprocessing backend	0.035714
the number of splitting iterations in the cross-validator	cviterable wrapper get n splits x y groups	0.111111
one-hot aka one-of-k scheme	one hot encoder	0.200000
function call	format call func	0.100000
for building a cv in a user friendly	cv cv x y	0.031250
approximate nearest	lshforest	0.125000
contingency matrix describing the relationship between labels	metrics cluster contingency matrix labels_true labels_pred	0.200000
minimum	utils min	0.250000
a tolerance which is	cluster tolerance x tol	0.058824
the sigmoid	sigmoid	0.111111
dets	dets	1.000000
generates a random sample from	size replace	0.125000
in pipeline after transforms	pipeline fit predict x y	0.166667
or thread pool	joblib multiprocessing backend	0.052632
column	utils column	1.000000
curve auc	auc x	0.040000
of module names and a name for the	name	0.033333
computes the log-likelihood of a	empirical covariance score	0.166667
a tolerance which is independent of the dataset	tolerance x tol	0.058824
fit	fit x y classes	1.000000
of parameters	x y estimator parameters	0.500000
of csgraph	sparsetools validate graph csgraph directed	0.250000
locally linear embedding	manifold locally linear embedding x n_neighbors n_components	0.071429
decision functions of the	decision function x	0.018868
matrix generation	core check input size n_components	1.000000
predict_proba on the estimator with the best found	core base search cv predict proba x	0.076923
bag predictions	ensemble base forest set oob	0.250000
the relationship between	metrics cluster	0.142857
return the active default backend	externals joblib get active backend	1.000000
c such that for c	c	0.022222
pipeline after transforms	pipeline fit	0.166667
non-overlapping groups	group	0.083333
generates integer indices corresponding to test sets	partition iterator iter test indices	0.333333
multiprocessing	multiprocessing	0.272727
meant to be cached	index_file_path data_folder_path slice_ color	0.033333
python object into	value filename	0.083333
fit the model according to the	svr fit x y sample_weight	0.250000
using the gaussian	gaussian	0.029412
scaler	preprocessing standard scaler	0.333333
theil-sen estimator robust multivariate regression model	theil sen regressor	1.000000
dataset this operation is meant	index_file_path data_folder_path slice_ color	0.033333
checker utility for building a cv in a	cv cv x y	0.031250
csgraph inputs	sparsetools validate graph csgraph	0.250000
from it	externals	0.011494
according to the given training data	sample_weight	0.037037
row of x	transform x y copy	0.142857
fit gaussian process classification model parameters	gaussian_process gaussian process classifier fit x	0.500000
single boost	boost	0.062500
w to	x w	0.083333
and false positives per binary	binary clf curve	0.090909
fitted model parameters	fit	0.003257
the cholesky decomposition of	det cholesky	0.166667
initial_bound	initial_bound	1.000000
covariance matrix	cov	0.100000
from prediction scores note this implementation	roc	0.033333
the function code and the	func code	0.200000
the model parameters	base mixture	0.111111
sparse	make sparse	0.125000
a transform	selected x transform	0.333333
full covariance	normal density full x means	0.166667
w to minimize	w ht	0.250000
the dense dictionary factor in place	dictionary y code verbose	0.333333
finds seeds for mean_shift	get bin seeds x bin_size min_bin_freq	0.500000
csgraph inputs	graph csgraph directed	0.250000
density model on the data	kernel density	0.083333
cache	externals joblib memorized func	0.013158
such that for c in (l1_min_c	c	0.022222
remove a subcluster from	subcluster new_subcluster1	0.166667
train test indices	core base shuffle split iter indices	0.250000
score	score estimator x y groups	0.333333
perform	responsibilities params min_covar	0.500000
generate name est weight tuples excluding none transformers	core feature union iter	0.333333
callable case	callable x y metric	0.083333
of	externals joblib	0.004762
distribution p(h|v)	hiddens v rng	0.500000
to the separating hyperplane	svm one class svm decision function	0.333333
pk	pk	1.000000
to a	externals	0.005747
factorize density check according	check density density n_features	0.166667
and inertia using a full	inertia precompute dense x x_squared_norms centers	0.250000
h in multiplicative update nmf	decomposition multiplicative update h x w h beta_loss	0.250000
update nmf	update h x	0.500000
estimates for	val predict	0.045455
check input and compute prediction of init	base gradient boosting init	0.142857
set the parameters of	set	0.160000
c_step procedure described in [rouseeuw1984]_ aiming at computing	x n_support remaining_iterations initial_estimates	0.111111
fit the model to data	core multi output estimator fit x y	0.200000
model and	x y	0.002155
factor	factor	0.800000
estimate the	mixture estimate gaussian	1.000000
of workers	configure n_jobs parallel	0.200000
func to be run	apply async func	0.250000
is	y_true y_pred	0.037037
indices to split data into training and test	model_selection base shuffle split split x y groups	0.200000
the shortest path length	shortest path length	0.333333
arg	arg	1.000000
the directory in which	dir	0.038462
c	c x y loss fit_intercept	0.030303
meant to be cached by	data_folder_path slice_ color resize	0.033333
shrunk covariance model according to the given	covariance shrunk covariance fit	0.083333
return the	externals	0.005747
r^2 coefficient of determination regression score	metrics r2 score y_true	0.125000
the residues on left-out data for a	residues x_train y_train x_test y_test	0.083333
transform function to portion of selected features	selected x transform selected	0.333333
the local	outlier factor local	0.500000
cache for	memorized	0.015873
test vectors	core dummy regressor predict	0.250000
of x from y along the first axis	x z reg	0.066667
and then	x y	0.002155
of this kernel	deep	0.076923
by scaling each feature to a	minmax scale x	0.142857
axis center to the	axis	0.028169
housing dataset from	housing	0.111111
back the data	standard scaler inverse transform x	0.066667
compute the decision function of the given	decision function x raw_values	0.083333
em update for	decomposition latent dirichlet allocation em step	0.500000
to capture the arguments of a function	joblib delayed function check_pickle	0.333333
normalize x according	cluster log normalize x	0.200000
private function used to compute log probabilities within	parallel predict log proba	0.058824
execution	externals joblib	0.004762
false positives per binary classification	binary clf curve y_true y_score pos_label	0.090909
the decision functions of the base	decision function	0.025000
data point	cross	0.037037
print	print	0.615385
patches of any	feature_extraction extract patches	0.083333
generate a sparse random projection matrix	random projection fit x	0.333333
of x from y	x z	0.050000
the gradient and the	y	0.005348
buffered version of a read file object	externals joblib buffered read file fobj	0.500000
will execute all batches sequentially	sequential	0.166667
validate x whenever one tries to predict	decision tree validate x predict x	0.500000
each	estimator x	0.030303
return the disk usage in	externals joblib disk used path	0.250000
computation of max absolute value	max abs	0.047619
covariance matrix shrunk on the diagonal	shrunk covariance	0.090909
described by its spectrum spectrum	spectrum n_samples n_features	0.166667
build a batch of estimators within a	ensemble parallel build estimators n_estimators ensemble x	0.166667
checker utility for building a cv	core check cv cv x y classifier	0.031250
prediction scores this score	score	0.010101
model parameters with	fit x y do_prediction	0.166667
the gradient of loss	loss	0.027027
the one-vs-one multi class libsvm	one vs one	0.050000
pairwise matrix in	metrics parallel pairwise x y func	0.166667
this estimator	params	0.028571
container_path	container_path	1.000000
a dataset along any axis	x axis	0.030769
training	fit	0.029316
perform a locally linear embedding analysis	manifold locally linear embedding x	0.071429
train test	core base shuffle	0.166667
a given template	type tied_cv	0.333333
the optimal	base compute	1.000000
the graphlasso covariance model to x	covariance graph lasso cv fit x	0.333333
fit the model	multi output estimator fit x	0.200000
transform	selected x transform	0.333333
is worthy	cfsubcluster	0.111111
of array-like or scipy	preprocessing binarize x threshold	0.083333
fit all transformers transform	union fit transform	0.333333
lfw pairs dataset this dataset is a collection	datasets fetch lfw pairs	0.018868
data precision matrix with the	pca get precision	0.066667
function output for x relative	metrics threshold scorer call clf x y	0.058824
residuals	residuals	1.000000
return a tolerance which is independent of the	cluster tolerance	0.058824
est	est	1.000000
kind	kind	1.000000
the long type introduces an	shape	0.011765
types to	parallel backend	0.030303
0 if y -	y	0.002674
evaluate a score	score estimator x y	0.250000
number of splitting iterations in	base kfold get n splits	0.111111
call wrapped function cache result and return	externals joblib memorized func call	0.200000
the precision the precision is the	metrics precision	0.033333
decision function output for x relative	metrics threshold scorer call clf x y sample_weight	0.058824
and potentially other states at the	neural_network base optimizer iteration ends time_step	0.142857
check that predict raises an exception in an	utils check	0.023810
for factorizing common classes param logic	partial fit first call clf classes	0.058824
coefficient of determination r^2	multi output regressor score x	0.200000
cross-validated estimates for each input data point	y cv	0.050000
estimate the precisions parameters of the precision	gaussian mixture estimate precisions nk xk	0.166667
for building a cv in a	core check cv cv x y classifier	0.031250
parameters and raise valueerror if not valid	ensemble base gradient boosting	0.111111
provided precisions	mixture check precisions precisions	0.250000
initialize	initialize	0.545455
input data point	estimator x	0.030303
input checker utility for building a cv in	core check cv cv	0.031250
backend and return the number	externals joblib parallel backend	0.029412
introduces an	utils shape repr	0.013699
f-beta score is the weighted harmonic mean	metrics fbeta score y_true y_pred	0.333333
cleanup a temporary folder if still existing	joblib delete folder folder_path	0.250000
the isotonic regression model : min	core isotonic regression	0.055556
scale back the data to the original	preprocessing robust scaler inverse transform x	0.066667
determination regression score function	r2 score y_true y_pred sample_weight multioutput	0.125000
recall the recall	metrics recall	0.033333
graphviz	graphviz	1.000000
configure	configure	0.714286
count and smooth	core bernoulli nb count x y	0.250000
each input data	core cross val predict estimator	0.045455
compute the gradient	neural_network base multilayer perceptron compute	0.250000
"news" format strip	datasets strip newsgroup	0.181818
list of exception	parallel backend base	0.037037
also predict	predict	0.006849
binary classification task	y_true y_score average sample_weight	0.076923
fit	svc fit x	0.333333
a tolerance which is independent of the dataset	tolerance x	0.058824
posterior log probability of the samples x	multinomial nb joint log likelihood x	0.500000
convert coefficient	coef	0.058824
from an open file object	f header_length dtype	0.333333
fit the model using x as training	neighbors local outlier factor fit x	0.333333
representing each class	classifier	0.013699
get the directory corresponding	func get func dir mkdir	0.333333
generate a dense gaussian random matrix	gaussian random matrix n_components n_features random_state	1.000000
distributions for the means	means	0.076923
learn vocabulary and idf	tfidf vectorizer fit transform raw_documents y	0.250000
evaluate the density model on	neighbors kernel density score samples x	0.250000
gradient of loss	loss grad	0.250000
parameters and evaluates the reduced likelihood function	reduced likelihood function	0.041667
import	import	1.000000
transform function to portion of selected features parameters	selected x transform selected	0.333333
the k-neighbors of a	neighbors kneighbors mixin kneighbors x	0.125000
is a	externals joblib is	0.500000
the largest k singular values/vectors	k ncv tol	0.166667
dissimilarities	dissimilarities	1.000000
default backend used by parallel inside	parallel backend backend n_jobs	0.166667
routine for validation and conversion of csgraph	graph csgraph directed dtype csr_output	0.166667
number of splitting iterations in	cross validator get n splits x y	0.125000
return the kernel k x y and	gaussian_process compound kernel call x y	0.333333
a single boost using the	boost classifier boost	0.100000
decision boundary for each class	core one vs rest classifier decision function	0.250000
to split data into training and test set	base shuffle split split x y groups	0.200000
note this implementation is restricted to	sample_weight	0.018519
to catch and hide warnings	ignore warnings	0.166667
a score by cross-validation read more in the	model_selection cross val score estimator x	0.166667
gaussian	gaussian	0.647059
fit the model	rbfsampler fit	0.250000
update the variational distributions for the	mixture dpgmmbase update	0.250000
an exception in an unfitted	unfitted	0.083333
the leaf	decision tree apply	0.166667
back the data	scaler inverse transform x	0.052632
mean and	utils incr mean	0.166667
use gaussianrandomprojection to produce a cosine lsh fingerprint	gaussian random projection hash	0.166667
content of the data	datasets clear data	0.142857
computes	x y alpha	0.181818
fit ridge regression	ridge cv fit	1.000000
one-vs-one multi class libsvm in	one vs one	0.050000
determinant with the	det	0.071429
of dataset and	x y	0.002155
return the kernel k	gaussian_process white kernel call	0.333333
and false positives per binary	metrics binary clf curve y_true y_score	0.090909
reconfigure the backend and	backend base configure	0.500000
estimates for each input data point	core cross val predict estimator x y	0.045455
the variational lower bound	mixture dpgmmbase bound	0.166667
get the parameters of the	ensemble voting classifier get	0.200000
power_iteration_normalizer	power_iteration_normalizer	1.000000
data_name	data_name	1.000000
elements of all the vectors rows of u	flip u	0.047619
classification problem	make classification	0.500000
the gaussian process model fitting method	gaussian_process gaussian process fit	0.250000
metrics	metrics	0.304348
remove cache folders	externals joblib memory	0.016949
for memmap backed arrays	memmap backed a	0.333333
update w in multiplicative update	decomposition multiplicative update w x w	0.500000
log of	log	0.056604
fit the model by computing truncated	fit truncated	1.000000
number of splitting iterations in	model_selection predefined split get n splits	0.111111
to predict	predict	0.013699
then	core transformer mixin	1.000000
n_samples	n_samples	0.294118
compute l1 and l2 regularization coefficients	compute regularization alpha l1_ratio regularization	0.333333
the arguments of a function	function	0.021277
indices increasingly apart the distance depending on	externals joblib verbosity filter index	0.055556
depending from it	externals joblib	0.009524
dbscan	dbscan	0.833333
random sample from a	a size replace	0.142857
sparse and dense	x y sample_weight	0.012987
classification by definition a confusion matrix :math c	confusion matrix y_true y_pred	1.000000
return probability estimates	predict proba	0.250000
the oracle approximating shrinkage covariance model	covariance oas fit	0.083333
with given gradients parameters	updates grads	0.076923
kernel k x	gaussian_process matern call x	0.200000
initialize the model parameters	mixture base mixture initialize	0.333333
the search over	base search cv fit x	0.166667
classifier with built-in cross-validation	classifier cv	1.000000
objective for the case method='lasso' is :	x y xy gram	0.090909
optimization function varies for mono and	x y	0.002155
the position	mds	0.050000
fit an estimator	fit estimator estimator	0.055556
the derivative of the logistic sigmoid function	logistic derivative z delta	0.166667
data x	x x	0.333333
training set according	fit	0.003257
predict based on	predict x	0.011765
depending from it	externals	0.011494
file position	externals joblib binary zlib file	0.083333
the parameters for the voting classifier valid parameter	voting classifier	0.035714
gradient and	w x y	0.133333
estimator with the best found	search cv predict proba	0.076923
under the curve auc	auc x y	0.040000
generate indices to split data into	kfold split x	0.250000
updating terminal	terminal	0.047619
fit linear model with passive aggressive algorithm	linear_model passive aggressive classifier partial fit	1.000000
do basic checks on matrix covariance	mixture validate covars covars covariance_type n_components	0.250000
makes sure centering is not enabled for	preprocessing robust scaler check array x copy	0.333333
break the pairwise matrix in	pairwise x	0.166667
cache	cache	0.666667
gaussian distribution	gaussian mean covar covariance_type	0.500000
curve auc using the trapezoidal	auc x	0.040000
to fit a single tree in parallel	ensemble parallel build trees tree forest x	0.200000
the callable case for	metrics pairwise callable x	0.083333
get the weights from	get	0.012048
step in pipeline after transforms	pipeline fit	0.166667
compute the laplacian kernel between x and y	laplacian kernel x y gamma	0.333333
distances between x and y read	distances x y	0.142857
largest k singular values/vectors for a	utils svds a k ncv tol	0.166667
input and compute prediction of init	base gradient boosting init decision function x	0.142857
remove cache	reduce	0.017241
is the depth a which this function is	externals joblib memorized func check previous func code	0.055556
number of splitting iterations in the	pgroups out get n splits x y	0.111111
normalized mutual information between two clusterings	metrics cluster normalized mutual info score labels_true labels_pred	1.000000
the function call	call func	0.100000
cosine distances between x and y read more	cosine distances x y	0.333333
of the dual gap convergence criterion the specific	dual gap emp_cov	0.071429
mean squared logarithmic error regression loss read	mean squared log error	0.200000
returns a list of feature	feature	0.055556
mean absolute error regression loss	metrics mean absolute error	0.166667
seeds for	cluster get bin seeds x	0.250000
compute mean and variance along an axix on	utils mean variance axis x axis	0.142857
returns posterior probabilities of classification	core calibrated classifier cv predict proba	0.200000
score is 1	score y_true y_pred	0.038462
the best found parameters	core base search cv	0.066667
and concatenate	x y	0.002155
compute the loss of	loss	0.027027
label propagation classifier read more	label propagation	0.200000
out cross-validator provides train/test	out	0.095238
func to be run	joblib sequential backend apply async func	0.250000
the elastic net optimization function varies for mono	l1_ratio	0.030303
fit to data then transform	core transformer mixin fit transform	0.500000
scale back the data	preprocessing robust scaler inverse transform x	0.066667
to make cache size fit	joblib memory reduce size	0.083333
scale back the data	standard scaler inverse transform x copy	0.066667
the backend and	backend	0.016949
with the fastmcd algorithm	cov	0.100000
ardregression model according to the	linear_model ardregression	0.100000
back the data	inverse transform x copy	0.066667
returns n_neighbors of	n_neighbors return_distance	0.250000
predict_proba on the estimator with the best	cv predict proba	0.068966
estimates for each input data	val predict estimator x y	0.045455
points based on the	from	0.045455
generative model	decomposition base pca get	0.142857
compute minimum and maximum	min max	0.250000
inplace row scaling	inplace row scale x	0.142857
get the weights from an	neighbors get	0.125000
matrix factorization nmf find two non-negative	non negative factorization x	0.043478
the long type introduces an	utils shape repr	0.013699
the sign of elements of all	sign	0.050000
determination regression score	metrics r2 score y_true y_pred sample_weight	0.125000
of the data home	datasets clear data home	0.076923
return a tolerance which is independent of the	cluster tolerance x tol	0.058824
used to compute log	log	0.018868
number of splitting iterations in the	cross validator get n splits	0.125000
the output of transform	transform y	0.023256
to avoid the hash	externals joblib memory	0.016949
we don't store the	externals joblib	0.009524
the gradient of the loss is	loss	0.027027
print verbose message on initialization	mixture print verbose msg init beg n_init	1.000000
evaluate a score by cross-validation read more in	model_selection cross val score estimator	0.166667
axis center to the mean and component wise	axis	0.014085
decision function to	decision function	0.025000
in an unfitted	unfitted	0.083333
compute minimum and maximum	utils min max axis x	0.500000
pipeline after transforms	core pipeline	0.076923
shutdown the process or thread	parallel backend base terminate	0.500000
the dense dictionary factor in	dictionary	0.071429
main classification metrics read	metrics classification	0.052632
return a	shape	0.011765
the vectors rows of u	flip u	0.047619
shrunk covariance model according to	covariance shrunk covariance fit x	0.083333
the callable case for pairwise_{distances	metrics pairwise callable x y	0.083333
the data home cache	data home	0.076923
cf	birch get	0.333333
of neighbors for points in	mixin radius neighbors	0.125000
apply clustering to a projection to the normalized	clustering	0.050000
feature names for	feature names	0.090909
feature	vectorizer get feature	0.200000
cv in a user friendly way	cv cv x	0.031250
memmap instance to reopen	memmap	0.066667
row-wise squared euclidean norm of x	row norms x squared	1.000000
scores note	metrics roc	0.040000
area under the curve auc	auc x	0.040000
to the normalized	spectral	0.026316
fit	linear svc fit	0.333333
this operation is meant to be	data_folder_path slice_ color resize	0.033333
the l1 distances between the vectors in	metrics manhattan distances	0.083333
principal component analysis pca using randomized svd	randomized pca	1.000000
under the curve auc using the	auc x y	0.040000
estimate the spherical wishart distribution	gaussian mixture estimate wishart spherical nk xk	0.333333
fit with	search cv fit	0.111111
and smooth feature occurrences	x y	0.004310
a score by cross-validation read more in the	model_selection cross val score	0.166667
given cache	externals joblib cache	0.250000
x to unit norm parameters	preprocessing normalizer transform x y copy	0.250000
and its corresponding derivatives with respect to each	activations deltas	0.032258
tests involving both blas calls and	with blas func	0.333333
of points based on	from	0.045455
e	e	1.000000
cache folders	joblib	0.007299
regression	regressor	0.081081
the function call with the	call func	0.100000
of the votingclassifier	params deep	0.111111
predict multi-output variable using a	core multi output estimator predict	0.166667
compute the initial centroids	cluster init centroids x k	0.166667
prediction scores note this	roc	0.033333
in friedman [1] and breiman [2]	make friedman3	0.166667
collection of text documents into a	vectorizer	0.022222
using matrix product with the random matrix	random projection transform x y	0.333333
fit the model with x	fit x	0.012821
capture the arguments of a function	externals joblib delayed function check_pickle	0.333333
number of splitting iterations in	model_selection leave pgroups out get n splits	0.111111
gaussian and label samples by quantile this	make gaussian	0.125000
the backend and	parallel backend	0.030303
of the memory	memory	0.015625
the reduced likelihood function for the given	process reduced likelihood function	0.047619
in the proper format	ensemble base weight boosting validate	1.000000
lsi	truncated svd	0.500000
function_name	function_name	1.000000
display the message	parallel print	0.142857
multiple files in svmlight	svmlight files files	0.200000
shrunk ledoit-wolf covariance	covariance ledoit wolf	0.125000
returns posterior probabilities of	cv predict proba x	0.034483
for fit subclasses should implement this method!	decomposition base pca fit x y	0.333333
the meta-information and the	zndarray wrapper read unpickler	0.043478
transform is sometimes referred to	transform y	0.023256
fit estimator and compute scores	core fit and score estimator x	0.333333
a score by cross-validation read more in the	cross val score estimator	0.166667
path with coordinate descent	enet path	0.050000
for	cross val predict estimator	0.045455
cond	cond	1.000000
\#3" regression problem this dataset is described in	datasets make	0.015625
list of all hyperparameter specifications	kernel hyperparameters	0.333333
the curve auc using	metrics auc x	0.040000
binary metric for multilabel classification parameters	average binary score binary_metric y_true y_score	0.500000
compute the weighted	weighted	0.125000
pickler that accepts custom reducers	customizable pickler	1.000000
input checker utility for building a cv in	check cv cv x y classifier	0.031250
hasher	hasher	0.666667
point	predict	0.006849
to a large sparse	utils lsqr a	0.037037
any axis center to the	axis	0.028169
preprocessor	preprocessor	1.000000
for different probability thresholds	probas_pred pos_label	0.200000
hash	memory	0.015625
of samples	samples	0.052632
apply the derivative of the logistic sigmoid function	logistic derivative z delta	0.166667
too rare or too common features	feature_extraction count vectorizer limit features x vocabulary	0.250000
tol	tol	1.000000
of methods for outliers detection with covariance estimators	outlier detection mixin	0.500000
rand	rand	0.833333
schedule	callback	0.375000
the curve auc using the	auc x y	0.040000
sparse components	sparse pca	0.500000
number of splitting iterations in the	predefined split get n splits	0.111111
utility function opening the right fileobject from a	joblib read fileobject fileobj	0.100000
that can actually run in parallel	externals joblib parallel backend	0.029412
pairwise matrix in	metrics parallel pairwise x	0.166667
store the	externals joblib	0.009524
the number of splitting iterations in the cross-validator	model_selection leave pgroups out get n splits x	0.111111
binary classification task	precision recall curve y_true	0.142857
curve auc using the trapezoidal rule this	auc x y	0.040000
input and compute prediction of init	init decision	0.142857
number of splitting iterations in the cross-validator parameters	pgroups out get n splits x y	0.111111
process or thread	externals	0.005747
predict the target of new samples	predict	0.006849
a file object	file	0.035714
according to	y sample_weight	0.035714
number of splitting iterations in the cross-validator	split get n splits x y	0.111111
all the content of the data home	data home data_home	0.055556
fit the model	fit	0.071661
python object into one file	filename	0.050000
the right fileobject from	joblib read fileobject fileobj	0.100000
error_template	error_template	1.000000
the free energy f v =	rbm free energy	0.066667
is	operator is	1.000000
a cv in a user friendly	cv cv x y classifier	0.031250
initial parameters of the derived class	parameters	0.055556
classification	make classification	0.500000
compute the deviance	deviance call y pred sample_weight	0.333333
compute elastic net path with	enet path x	0.050000
meta-information and the	zndarray wrapper read unpickler	0.043478
function opening the right fileobject	fileobject fileobj	0.200000
compute the	mixture base mixture score	0.111111
possible outcomes for samples	ensemble voting classifier predict	0.100000
compute binary logistic loss for classification	neural_network binary log loss y_true y_prob	0.500000
apply clustering to	clustering affinity n_clusters n_components	0.166667
of a csc/csr matrix in-place	utils inplace swap column x m	0.250000
x_old	x_old	1.000000
the california housing dataset	california housing	0.083333
patches that will be extracted in an	patches i_h i_w p_h p_w	0.250000
update	update w x	1.000000
tree	tree forest x	1.000000
range approximates the range of	range finder	0.083333
is inefficient to	y classes	0.027778
mixin	estimator mixin	0.250000
the weighted log	mixture base mixture	0.111111
compute log probabilities within a job	predict log proba estimators estimators_features x n_classes	0.250000
metric for multilabel classification parameters	score binary_metric y_true	1.000000
split data into training and test set	model_selection predefined split split x y groups	0.200000
return the shortest path	utils single source shortest path	0.333333
coverage error measure compute how	coverage error y_true	0.166667
the gaussian process model fitting	gaussian_process gaussian process fit	0.250000
given type	register type	0.333333
the optimization objective for the case method='lasso' is	xy gram	0.090909
function for parameter value indexing	model_selection index param value x	0.200000
lars path parameters	omp path	0.100000
indices to split data into	series split split x	0.250000
estimator on training subsets incrementally and	model_selection incremental fit estimator estimator x y	0.200000
normalizer	normalizer	1.000000
classif	classif	1.000000
in parallel	joblib parallel backend	0.045455
compute mutual information between continuous and discrete variables	feature_selection compute mi cd c d n_neighbors	1.000000
x from y along the first	x z reg	0.066667
fit label encoder and	label encoder fit transform y	0.200000
timestamp when pickling to avoid the hash depending	joblib memory reduce	0.030303
the labeled faces in the wild lfw pairs	lfw pairs	0.018868
private helper function for factorizing common classes	partial fit first call clf classes	0.058824
values	train	0.117647
stout or stderr depending on verbosity	msg msg_args	0.200000
labels the output of transform is sometimes referred	transform y	0.023256
long type introduces an 'l' suffix when using	shape	0.011765
training set x and returns	x y	0.002155
compute gaussian log-density at x for a diagonal	log multivariate normal density diag x means covars	1.000000
predict on	predict x	0.011765
computes a	utils	0.009709
calculate true and false positives per	clf curve y_true y_score	0.250000
used to build a batch of estimators within	ensemble parallel build estimators n_estimators ensemble	0.166667
false positives per binary classification	binary clf curve	0.090909
evaluate the density model on the	neighbors kernel density score samples x	0.250000
memmap instance to reopen on same file	reduce memmap	0.166667
fit the	estimator fit x y	0.200000
get number of jobs	utils get n jobs n_jobs	0.250000
predicted probabilities for a calibration curve	core calibration curve y_true y_prob	0.142857
inplace column scaling of a csc/csr matrix	inplace column scale x	0.166667
input data	core	0.015385
score with	score estimator x	0.125000
'l' suffix when using the	utils shape	0.013699
factorization nmf find	negative factorization	0.043478
under the curve auc using the trapezoidal	auc x y	0.040000
check initial parameters of the	mixture base mixture check parameters x	0.200000
the training	factor fit	0.062500
predict	predict x	0.152941
x relative	metrics threshold scorer call clf x	0.058824
for each input data point	core cross val predict estimator x y	0.045455
computes	y alpha	0.222222
fit the model according to the given training	linear svc fit x y sample_weight	0.250000
array from the meta-information	zndarray wrapper read unpickler	0.043478
function output for x relative to	metrics threshold scorer call clf x y sample_weight	0.058824
returns the number of splitting iterations in	one group out get n splits	0.111111
graph	feature_extraction grid to graph	0.333333
compute receiver operating characteristic roc note this	metrics roc curve	0.142857
the estimator	estimator	0.044118
similarity of two sets of biclusters	cluster consensus score a b similarity	0.500000
from a given 1-d array	utils choice a size replace	0.250000
arpack	arpack	1.000000
true and predicted probabilities for a calibration curve	core calibration curve y_true	0.142857
returns the huber loss and the	huber loss and	0.166667
create dataset abstraction	linear_model make dataset x	1.000000
and **kwargs, in the context of the memory	joblib memory	0.016949
solve the isotonic regression model :	isotonic regression y	0.066667
data x with ability	x	0.001692
returns the number of splitting iterations in the	model_selection leave one out get n splits x	0.111111
diabetes	diabetes	1.000000
process classification based	process	0.090909
of the function called with the given arguments	joblib memorized func	0.014706
model we can also predict	predict x	0.011765
the hash depending from	joblib	0.014599
the number of splitting iterations in the	cross validator get n splits	0.125000
the number of splitting iterations in the cross-validator	base kfold get n splits	0.111111
kernel k x	product call x	0.200000
update the variational distributions for the means	mixture dpgmmbase update means x z	1.000000
centroids on x by	fit x y	0.005988
generate	n_samples n_components	0.500000
the similarity of	a b similarity	0.125000
problem with sparse uncorrelated design this dataset is	datasets make sparse uncorrelated n_samples n_features	0.166667
the parameters for the voting classifier	voting classifier set params	0.037037
the weighted graph of neighbors for	radius neighbors mixin radius neighbors graph	0.066667
we	joblib	0.014599
matrix to	sparse coef	0.071429
training subsets incrementally	core incremental fit	0.500000
function cache result and return	externals joblib memorized func	0.013158
coefficient of determination regression score	r2 score y_true y_pred sample_weight	0.125000
the search over parameters	search cv	0.018182
implementation is restricted to the binary classification	score y_true y_score	0.025000
a cv in a user friendly	core check cv cv x y	0.031250
to x return leaf indices	x	0.001692
fit the kernel density model on the	neighbors kernel density fit	0.250000
the kernel k x y and	product call x y	0.200000
leaf	decision tree apply x	0.166667
the specified row	row row	0.166667
the derivative	derivative	0.125000
a calibration curve	calibration curve y_true y_prob	0.142857
covariance matrix	cov x y priors	0.500000
be captured	parallel backend base get exceptions	0.166667
== missing_values	value_to_mask	0.166667
cleanup a temporary folder if still existing	folder folder_path warn	0.250000
hessian in the case of a logistic loss	linear_model logistic grad hess w	1.000000
the paired	metrics paired	0.400000
curve auc from prediction scores note this	metrics roc auc	0.166667
folders to make cache size fit	joblib memory reduce size	0.083333
an unsuccessful bst search since the	n_samples_leaf	0.111111
the average log-likelihood	decomposition factor analysis score x y	0.333333
cache for	externals joblib memorized	0.013699
false positives per binary classification	binary clf curve y_true	0.090909
return whether the file supports seeking	joblib binary zlib file seekable	0.250000
the metric	metric	0.071429
samples x	svc decision function x	0.200000
chars	chars	1.000000
fit the model using x y as training	isotonic regression fit x y	1.000000
metaclass	metaclass	1.000000
don't store the timestamp when pickling to	externals joblib memorized func reduce	0.050000
like assert_all_finite but only for ndarray	utils assert all finite	0.333333
given type in the dispatch table	pickler register type reduce_func	1.000000
loader for the	subset data_home	0.125000
nicely formatted statement displaying the function call with	externals joblib format call func args kwargs object_name	0.333333
x y as training	x y xy	0.333333
whose range approximates the range	range finder	0.083333
an 'l' suffix when using the	utils shape repr	0.013699
repeated	repeated	1.000000
generate a random regression	make regression n_samples n_features	1.000000
delete all the content of the data	clear data	0.142857
don't store the timestamp when pickling	reduce	0.034483
the callable case for	metrics pairwise callable	0.083333
neighbors	radius neighbors x	0.166667
make cache size fit in bytes_limit	externals joblib memory reduce size	0.083333
classification this function returns posterior probabilities of classification	core calibrated classifier cv predict proba x	0.200000
for c	c x y loss	0.030303
the binary classification task	y_true	0.021739
range	randomized range	0.083333
generate indices to split data into	series split split x	0.250000
meant to be cached by a joblib	data_folder_path slice_ color resize	0.033333
range of	randomized range finder	0.083333
transforms features by scaling each feature	preprocessing minmax scale x feature_range axis copy	0.200000
constructor store the useful information	externals joblib ndarray wrapper init filename subclass allow_mmap	0.200000
regression score function best possible score is 1	score y_true y_pred sample_weight multioutput	0.062500
a cv in a	cv cv x y classifier	0.031250
used to fit a single tree	build trees tree forest x	0.142857
average a binary metric for multilabel classification parameters	binary score binary_metric y_true y_score average	1.000000
convert coefficient matrix to dense array	linear_model sparse coef mixin densify	0.100000
factorization nmf	negative factorization	0.043478
to capture the arguments of a function	delayed function check_pickle	0.333333
to the binary classification task	score y_true y_score average	0.076923
true and predicted probabilities for a calibration curve	core calibration curve y_true y_prob	0.142857
remove	remove	1.000000
exception types to be captured	parallel backend base get exceptions	0.166667
and predict_log_proba	predict log proba x	0.045455
binary labels the output of transform	transform	0.011236
for a given	y scorer	0.111111
the optimization objective for the case method='lasso' is	x y xy gram	0.090909
factor analysis fa a simple linear generative model	factor analysis	0.166667
predictions using a single binary estimator	core predict binary estimator x	0.200000
convert coefficient matrix to	linear_model sparse coef mixin	0.090909
problem with sparse uncorrelated design this dataset is	datasets make sparse uncorrelated	0.166667
x and returns the transformed data	transform x y w h	0.500000
for each input data	y	0.002674
of the log of the	log	0.018868
make predictions using a single binary estimator	predict binary estimator	0.200000
the scaling of x according to feature_range	scaler inverse transform x	0.026316
x (as bigger is better	x	0.001692
multi-output variable using a model	core multi output estimator	0.142857
modify the sign of vectors for reproducibility flips	utils deterministic vector	0.076923
with the best found	core base search cv	0.033333
largest k singular values/vectors for a sparse matrix	utils svds a k ncv tol	0.166667
list	parallel backend	0.030303
the long type introduces an 'l' suffix	utils	0.009709
estimate model parameters with the	base mixture fit	0.200000
from the meta-information and the z-file	externals joblib zndarray wrapper read unpickler	0.043478
covariance w/ cross-validated	cv	0.009009
the iris	iris	0.111111
out cross-validator provides train/test indices to	out	0.095238
an arbitrary python object into one	filename	0.050000
generate	shuffle	0.083333
back the data	preprocessing standard scaler inverse transform x	0.066667
the number of splitting iterations in the cross-validator	base cross validator get n splits x	0.125000
values for a given dataset split	x y train	0.166667
calculate the posterior log probability	multinomial nb joint log likelihood	0.083333
the range	randomized range	0.083333
classification	y_true y_score pos_label	0.066667
the laplacian	laplacian	0.068966
libsvm-based classifiers	svc	0.111111
a given cache key	externals joblib cache key	0.250000
to split data into training and test set	split split x y groups	0.200000
x for later	fit x y	0.005988
housing dataset	housing	0.111111
the blup parameters and evaluates the reduced	gaussian_process gaussian process reduced	0.125000
transform new points into embedding space	manifold locally linear embedding transform	0.500000
classes param logic	classes	0.025641
scale back	preprocessing standard scaler inverse transform	0.066667
of the function called with the given arguments	joblib memorized func get output	0.125000
eps	eps	0.833333
compute the boolean mask x	mask x	0.333333
stopping	stopping	1.000000
factorization	non negative factorization x	0.043478
anova f-value for	feature_selection f classif x	0.200000
or	feature_extraction mask edges weights	0.500000
not found and raise	utils line search	0.029412
w h whose product approximates the non-	x w h n_components	0.038462
abs	abs	0.416667
measure the similarity	cluster fowlkes mallows	0.250000
a memmap instance to	joblib reduce memmap a	0.050000
multi-task l1/l2 lasso	multi task lasso	1.000000
finds radius neighbors from the candidates obtained	get radius neighbors query max_depth bin_queries radius	1.000000
building a cv	core check cv cv x	0.031250
preprocess the text before tokenization	feature_extraction vectorizer mixin build preprocessor	0.142857
shortest path length from source to	utils single source shortest path length graph source	0.111111
dense_output	dense_output	1.000000
the process	joblib multiprocessing	0.052632
returns the number of	len	0.038462
regression problem this dataset is described in	datasets make	0.015625
and variance along an axix on	variance axis x axis last_mean last_var	0.142857
usage in	used path	0.250000
and apply	y	0.005348
check that the	mixture bayesian gaussian mixture check	1.000000
routine for validation and conversion of csgraph	csgraph directed dtype csr_output	0.166667
to be used for later	fit x y	0.005988
folders to make cache size fit in bytes_limit	reduce size	0.083333
fit linear model with passive aggressive algorithm	linear_model passive aggressive classifier fit	1.000000
to avoid the hash depending	memory	0.015625
non-negative matrix factorization nmf find two	non negative factorization x	0.043478
estimates for	val predict estimator x	0.045455
the main classification metrics read more in	metrics classification	0.052632
store	memorized	0.015873
the decision boundary for each class	one vs rest classifier decision function	0.250000
solve the isotonic regression model : min	isotonic regression y	0.066667
n_support	n_support	0.833333
with	decomposition	0.047619
to a large sparse linear system	lsqr a	0.037037
the free energy f v = -	bernoulli rbm free energy	0.066667
the number of splitting iterations in	leave pgroups out get n splits x	0.111111
sorted array of	tree bin_x left_mask right_mask	0.166667
the kddcup99 dataset downloading it if necessary	fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
a score	score estimator x y scoring	0.333333
vector x	x	0.003384
forests	forest	0.076923
posterior probabilities of classification	classifier cv predict proba	0.200000
stratified shufflesplit cross validation iterator	stratified shuffle split	1.000000
global clustering for	birch global clustering x	0.142857
fit the model according to the given	svc fit x y sample_weight	0.250000
from it	joblib	0.014599
confidence scores for	linear_model linear classifier mixin	0.500000
search over parameters	base search cv fit	0.166667
capture the arguments of a function	joblib delayed function check_pickle	0.333333
the covariance matrices from a given template	match covariance type tied_cv covariance_type	0.333333
sizes of training subsets and	translate train sizes	0.066667
generate indices to split data into	time series split split x	0.250000
leaf	base decision tree apply	0.166667
fit label encoder parameters	preprocessing label encoder fit y	0.500000
of all the vectors rows of u such	flip u	0.047619
class covariance	core class cov x y	0.250000
compute non-negative matrix factorization nmf find two	non negative factorization x	0.043478
return the kernel k x y and	gaussian_process exponentiation call x y	0.200000
generate cross-validated estimates for each input data point	y cv	0.050000
to avoid the hash depending	memorized func	0.016949
a single boost	boost	0.062500
normalize x by scaling	cluster scale normalize x	0.142857
shortest path length from source to all	shortest path length graph source cutoff	0.111111
returns the number of splitting iterations in the	base kfold get n splits	0.111111
a sparse	utils	0.009709
the hyperbolic tanh	neural_network inplace tanh	0.500000
compute gaussian log-density at x for a spherical	mixture log multivariate normal density spherical x means	1.000000
the kernel is stationary	stationary kernel mixin is stationary	0.333333
hastie et al	hastie 10 2 n_samples random_state	0.166667
seeds for	cluster get bin seeds	0.250000
z	z	0.909091
the timestamp when pickling to avoid	func reduce	0.050000
check the	neighbors check params x	0.500000
list of feature names ordered by their indices	feature names	0.090909
write the	func write	0.500000
stacklevel is the depth	previous func code stacklevel	1.000000
calculates a covariance matrix shrunk on	shrunk covariance emp_cov shrinkage	0.250000
transforms one after the other and transforms the	y	0.002674
of edges for a 3d image	make edges 3d n_x n_y n_z	0.250000
neighbors within a	radius neighbors x	0.166667
derivative	derivative z	0.333333
the dimension	dimension	0.050000
ovr	ovr	1.000000
readinto	readinto	1.000000
the number of splitting iterations in	out get n splits	0.111111
dense dictionary factor	dictionary	0.071429
return the path of	datasets get	0.200000
fit	lasso lars ic fit	1.000000
predict class at each stage for	ensemble gradient boosting classifier staged predict	0.500000
iteration_idx	iteration_idx	1.000000
list of exception types to be captured	get exceptions	0.166667
inplace row scaling of a csr or csc	inplace row scale x	0.142857
diagonal	diag resp x	1.000000
or	feature_extraction mask edges	0.500000
estimator has been refit	core base search cv	0.033333
such that for c in (l1_min_c infinity) the	c x y loss fit_intercept	0.030303
format check x	dirichlet allocation check	0.062500
in parallel	externals joblib parallel	0.014085
data point	core	0.015385
the meta-information and the z-file	zndarray wrapper read unpickler	0.043478
predict class labels for samples in x	linear classifier mixin predict x	1.000000
by cross-validation read more in	model_selection cross val	0.250000
load dataset from multiple files in svmlight	datasets load svmlight files files n_features dtype	0.500000
check that the	metrics cluster check	0.500000
and returns the transformed	y w h	0.500000
run in parallel	externals joblib parallel	0.014085
variance regression score function best possible score is	variance score y_true y_pred	1.000000
decorator for creating a class with a metaclass	externals add metaclass metaclass	0.166667
for full covariance matrices	density full	0.166667
check initial parameters of the derived class	base mixture check parameters x	0.200000
sample in x	x	0.001692
log probability for	log multivariate normal	0.500000
strip the headers by removing	datasets strip	0.076923
data	x y	0.002155
'l' suffix	shape	0.011765
list of exception types to	parallel backend base get	0.066667
indices to split data into training and test	base shuffle split split x y groups	0.200000
lasso linear model	lasso	0.125000
a lower bound on model evidence based on	dpgmmbase lower bound	0.071429
temporary folder if still existing	folder folder_path warn	0.250000
long type introduces an 'l'	shape	0.011765
the number of splitting iterations in the cross-validator	cviterable wrapper get n splits	0.111111
the svmlight / libsvm	svmlight file f n_features	0.066667
cross-validated estimates for each	core cross val predict estimator x y cv	0.071429
to build a batch of estimators within a	build estimators n_estimators ensemble	0.166667
versus all others	multiclass x y alpha	0.166667
hash depending from it	joblib memory	0.016949
decorator to catch and hide warnings	ignore warnings	0.166667
model to the data	y	0.002674
on	fit	0.009772
each input	predict estimator	0.045455
predict if	predict x	0.011765
to build a batch of estimators within a	ensemble parallel build estimators n_estimators ensemble x	0.166667
with the generative model	base pca get	0.076923
back	standard scaler inverse transform x	0.066667
derivatives with respect to each parameter weights	activations deltas	0.032258
check initial parameters of the derived	mixture base mixture check parameters x	0.200000
from the decision boundary for each class	one vs rest classifier decision function	0.250000
'l'	utils	0.009709
the initial centroids	cluster init centroids	0.166667
reduced likelihood function	process reduced likelihood function	0.047619
dense dictionary factor	dictionary y	0.111111
solve the linear assignment problem using the hungarian	linear assignment x	0.090909
number of splitting iterations in the	leave one out get n splits x	0.111111
output of transform is sometimes referred to by	transform	0.011236
beg	beg	1.000000
the recall the recall is the	metrics recall	0.033333
solve the isotonic regression model :	core isotonic regression y	0.066667
a cv	check cv cv x y classifier	0.031250
coverage error measure compute how far we	coverage error	0.166667
returns the number of splitting iterations in the	cross validator get n splits x y groups	0.125000
predict class at each stage for	classifier staged predict	0.333333
for full	full x means covars	0.166667
generate cross-validated estimates	y cv	0.050000
with all	search cv	0.018182
diagonal of the kernel k x x	exponentiation diag x	1.000000
estimates for each	cross val predict	0.045455
search on hyper parameters	search cv	0.018182
returns a list of edges	feature_extraction make edges	0.066667
and a set of	x y axis	0.250000
terminal	terminal region tree	0.100000
non-negative matrix factorization nmf find two non-negative matrices	non negative factorization x	0.043478
project data to vectors and cluster	project and cluster data vectors n_clusters	0.333333
for tests involving both blas calls and multiprocessing	utils if safe multiprocessing with blas func	0.500000
function code hash	code func	1.000000
instance for the given param_grid	cv get param iterator	0.166667
filename	fileobj filename mmap_mode	0.500000
the sample weight array	linear_model base sgd validate sample weight sample_weight	0.333333
compute log probabilities within a job	ensemble parallel predict log proba estimators estimators_features	0.250000
the number of splitting iterations in	kfold get n splits x y	0.111111
split data into	time series split split	0.250000
return an iterator over the key value pairs	externals iteritems d	1.000000
returns the number of splitting iterations in the	model_selection predefined split get n splits	0.111111
from the decision boundary for each class	one vs rest classifier decision function x	0.250000
data under the model	mixture dpgmmbase score samples x	0.200000
get the directory corresponding to	func get func dir mkdir	0.333333
read array from unpickler file handle	wrapper read array unpickler	1.000000
score by cross-validation	core cross val score estimator x y scoring	0.333333
data in x and transform x	transform x y	0.062500
patch data	feature_extraction patch extractor transform	0.200000
and false positives per binary classification threshold	binary clf curve y_true y_score pos_label sample_weight	0.090909
opening the right fileobject from	joblib read fileobject fileobj	0.100000
new samples can be different from the	core calibrated classifier	0.083333
run fit on	fit x y	0.005988
cachedir	cachedir	1.000000
process or thread pool and return the number	externals joblib multiprocessing backend	0.035714
restricted to the binary classification	score y_true y_score average	0.076923
validate x whenever one tries to predict apply	forest validate x predict x	0.500000
check x format and make sure no	check non neg array	0.500000
a random sample from a	a size	0.142857
wrapped function cache result and	externals joblib memorized func	0.013158
inplace row scaling of a csr	utils inplace row scale x	0.142857
avoid	externals joblib memorized	0.013699
at each stage	staged	0.250000
randomized linear models for feature selection this implements	randomized linear model	0.076923
a random projection p only changes the	core johnson lindenstrauss min dim n_samples eps	0.142857
step6	step6	1.000000
a transformed real-valued array into	projection to	0.166667
an estimator within	estimator estimator	0.052632
the breast	breast	0.111111
function used to fit a single tree	build trees tree forest x y	0.142857
neighbors for points in	radius neighbors mixin radius neighbors	0.125000
build a batch of estimators within a	build estimators n_estimators ensemble	0.166667
ardregression	ardregression	0.833333
the file supports seeking	file seekable	0.250000
sizes of	core translate train sizes	0.066667
as the maximizer of the reduced likelihood	process arg max reduced likelihood	0.250000
the gradient	gradient	0.090909
predict class log-probabilities for x	ensemble bagging classifier predict log proba x	1.000000
the array corresponding to	joblib numpy array	0.250000
the function call	externals joblib format call func	0.100000
inplace column scaling of a csc/csr	utils inplace column scale	0.166667
block diagonal structure	biclusters shape n_clusters noise minval	0.058824
fit the rfe model and automatically tune	feature_selection rfecv fit x y	1.000000
is positive-definite	covariance_type	0.083333
a decision tree	tree decision tree	0.250000
and	fit transform x y	0.250000
fit is on	fit x	0.006410
importances	importances	0.400000
fit linear	fit x	0.032051
return the shortest	single source shortest	0.333333
theta as the maximizer of the reduced	gaussian process arg max reduced	0.200000
divergence of	divergence	0.090909
the parameters for the voting	ensemble voting	0.142857
of array-likes and sparse	utils safe sqr x copy	0.125000
distances between x	distances x	0.500000
function for factorizing common	partial fit first call clf	0.200000
check input and compute prediction of init	init decision	0.142857
maximizer of	arg max	0.047619
compute probabilities of possible outcomes for samples in	ensemble voting classifier predict proba	0.200000
number of splitting iterations in the cross-validator	cviterable wrapper get n splits x y groups	0.111111
any negative value in an array	non negative x whom	0.200000
configure a copy	estimator append	0.142857
to avoid	externals joblib memorized	0.013699
the test/test sizes are meaningful wrt to the	validate shuffle split n_samples test_size train_size	0.111111
all transformers	feature union	0.285714
boolean thresholding of array-like or scipy sparse	binarize x threshold	0.083333
factorization nmf find two non-negative	negative factorization	0.043478
linear assignment problem using the hungarian	utils linear assignment x	0.090909
each non zero row of x	transform x y copy	0.142857
a process or thread pool	multiprocessing backend	0.038462
memmap instance to	memmap	0.066667
partially fit underlying estimators should be used when	one vs one classifier partial fit x	0.166667
generate	predict estimator x y	0.045455
by scaling each feature to a given range	minmax scale x	0.142857
a score by cross-validation read more in the	cross val score estimator x	0.166667
the scaler	scaler	0.093750
prediction scores note this implementation is	roc	0.033333
sign of elements of all	sign flip	0.066667
input	val predict estimator x	0.045455
initialize the model parameters of	base mixture initialize x	0.333333
is the time	time t	0.125000
true and false positives per binary classification threshold	binary clf curve y_true y_score pos_label sample_weight	0.090909
similarity of two clusterings of a set of	score labels_true labels_pred	0.047619
the lrd of a sample is	distances_x neighbors_indices	0.047619
used for later scaling	scaler fit x	0.153846
of length dimensions	dimensions	0.125000
func to	mixin apply async func	0.250000
of a csc/csr matrix	utils inplace	0.250000
sample weight array	sgd validate sample weight	0.333333
the given arguments and persist the	func call	0.047619
coefficient matrix to dense array format	mixin densify	0.100000
on the training set	fit	0.003257
the percentiles of	percentiles grid_resolution	0.250000
and	random_state	0.076923
the timestamp when pickling to avoid the hash	externals joblib memory reduce	0.030303
search over parameters	base search cv fit x y	0.166667
generate cross-validated estimates for	cross val predict estimator x y cv	0.071429
in multiplicative update	multiplicative update w	1.000000
weighted graph of neighbors for points	neighbors radius neighbors mixin radius neighbors graph	0.066667
the number	len	0.038462
the oracle approximating shrinkage	oas fit x	0.333333
a	externals joblib parallel backend base apply	1.000000
shrunk on the diagonal read	covariance shrunk	0.066667
seeds	cluster get bin seeds x	0.250000
parameters and raise valueerror if not	base gradient boosting	0.100000
ovr decision	ovr decision	1.000000
returns the submatrix corresponding	submatrix	0.090909
independent representation of	shape	0.011765
xk	xk	1.000000
text report	report	0.047619
of a multinomial	multinomial	0.083333
load and return the diabetes dataset regression	datasets load diabetes return_x_y	1.000000
a list of edges for	edges	0.047619
fit ridge regression	ridge cv fit x	1.000000
step for full cases	mstep full gmm x responsibilities weighted_x_sum	0.250000
data loading for the lfw pairs	lfw pairs	0.018868
net path with	path x	0.045455
transform	transform x	0.084746
autocorrelation parameters theta as the maximizer of	gaussian process arg max	0.047619
introduces an 'l'	utils shape	0.013699
partially fit	partial fit	0.333333
problem this dataset is described	datasets	0.015152
the hash depending from it	memory	0.015625
compute data precision matrix with the generative model	base pca get precision	0.066667
each sample	vs	0.166667
timestamp when pickling to avoid the	externals joblib memorized func reduce	0.050000
the gaussian process model	gaussian_process gaussian process predict	0.500000
used to fit a single tree	build trees tree forest x y	0.142857
the average hamming loss	metrics hamming loss y_true y_pred labels sample_weight	0.333333
such that for c in (l1_min_c	c x y loss	0.030303
each parameter weights and bias vectors	backprop x y	0.200000
of an array shape under python 2	shape repr shape	0.166667
normalize x by scaling rows and	normalize x	0.076923
estimator's fit method supports	utils has fit parameter	0.500000
y as training	y	0.005348
partially fit underlying estimators should be	core one vs one classifier partial fit x	0.166667
introduced by a random projection p only changes	johnson lindenstrauss min dim n_samples	0.142857
for full covariance matrices	full x means	0.166667
dataset this operation is meant to	data_folder_path slice_ color resize	0.033333
dataset is constructed by	datasets	0.015152
the recall is the ratio tp / tp	metrics recall	0.033333
boosted classifier/regressor from	ensemble base weight boosting	0.333333
the autocorrelation parameters theta as the maximizer of	gaussian_process gaussian process arg max	0.047619
free energy f v = - log sum_h	free energy	0.066667
csgraph inputs	sparsetools validate graph csgraph directed	0.250000
mixin class for all meta estimators in scikit-learn	meta estimator mixin	0.250000
the callable case for pairwise_{distances	callable	0.058824
the shrunk ledoit-wolf	ledoit wolf x assume_centered block_size	0.250000
with	label	0.045455
the deviance (= 2 * negative log-likelihood)	ensemble binomial deviance	0.333333
given type in the dispatch table	register type reduce_func	1.000000
with	joblib	0.007299
class weights	compute class	0.166667
x y and	gaussian_process dot product call x y	0.200000
based on a feature matrix	x connectivity n_components n_clusters	1.000000
of its patches	patches 2d patches image_size	0.333333
data loading for the lfw pairs	fetch lfw pairs	0.018868
divergence of p_ijs and	divergence	0.090909
column scaling of a csc/csr matrix	column	0.083333
of points on	len	0.038462
compute minimum distances between one point and a	metrics pairwise distances argmin x y axis	0.500000
for the lfw pairs	datasets fetch lfw pairs	0.018868
list of feature names ordered by their indices	dict vectorizer get feature names	0.142857
return	repr	0.012500
one group out cross-validator	one group out	0.166667
of max absolute value	preprocessing max abs	0.050000
of the kl	kl	0.125000
x according to	transform x	0.016949
scores this score corresponds to the area	score y_true y_score	0.025000
that will be extracted	i_h i_w p_h p_w	0.250000
median of	utils get median	0.166667
point	val	0.037037
posterior probabilities of	cv predict proba	0.034483
data matrix x and target y	multilayer perceptron partial	0.166667
estimates for each input data	predict estimator	0.045455
learn the vocabulary dictionary and return term-document matrix	vectorizer fit transform raw_documents y	0.100000
for	cross val predict estimator x y	0.045455
avoid the hash depending	externals joblib	0.009524
binarize labels in	label binarize	0.333333
covariance with	covariance	0.043478
predictions using a single binary estimator	core predict binary estimator	0.200000
prediction	x y sample_weight	0.012987
for building a cv	core check cv cv x y	0.031250
to a large sparse	lsqr a	0.037037
one-vs-one multi	svm one vs one	0.050000
compute directory associated with a given cache key	joblib cache key to dir cachedir func argument_hash	1.000000
extracts patches of any n-dimensional	patches	0.055556
call wrapped function cache	externals joblib memorized func call	0.200000
the number of splitting iterations in the cross-validator	kfold get n splits x	0.111111
for	covariance get	0.166667
process	multiprocessing	0.045455
check initial parameters of the	mixture check parameters	0.166667
absolute error regression loss read more in	absolute error y_true	0.142857
the data as a sparse	decomposition sparse	0.111111
function and cache result or	func	0.011364
and compute scores	y classes	0.055556
measure	fowlkes mallows	0.250000
loss of prediction pred and y	loss function call y pred sample_weight	0.333333
names for estimators	name estimators estimators	0.500000
data	pca get	0.076923
fit	linear svc fit x	0.333333
the function called with the given arguments	externals joblib memorized func get	0.125000
helper function for factorizing common classes param logic	first call clf classes	0.058824
fit on	fit	0.003257
the decision functions of the base classifiers	ensemble bagging classifier decision function x	0.333333
based on a	connectivity n_clusters return_distance	0.250000
output for x	x y	0.002155
axis center to the median and component wise	x axis	0.015385
estimates for	estimator x	0.030303
the lfw pairs dataset this operation is	datasets fetch lfw pairs	0.018868
n_leaves	n_leaves	1.000000
oracle approximating shrinkage covariance model	covariance oas fit x	0.083333
paired cosine	paired cosine	0.333333
extraction_step	extraction_step	1.000000
fit an estimator within a	fit estimator estimator x y sample_weight	0.071429
ledoit-wolf covariance matrix	covariance ledoit wolf x assume_centered block_size	0.125000
is a general function given points on a	y reorder	0.111111
factorization	decomposition non negative factorization x	0.043478
and a	y	0.002674
helper class for managing pool	pool manager mixin	0.500000
and return	y	0.008021
features	features x vocabulary	1.000000
a lower bound on model	mixture dpgmmbase lower bound	0.071429
the huber loss and the gradient	linear_model huber loss and gradient w x y	0.333333
true and false positives per binary classification threshold	binary clf curve y_true y_score pos_label	0.090909
minimum covariance determinant with the fastmcd algorithm	covariance min cov det	0.500000
ledoitwolf estimator ledoit-wolf is a particular	ledoit wolf	0.111111
computes the free energy f	neural_network bernoulli rbm free energy	0.066667
an array of distances and a parameter weights	weights dist weights	0.142857
generate train	core base	0.083333
class for all meta estimators in	meta	0.043478
maximum absolute value to be used for	max abs	0.047619
voting classifier valid parameter keys can be	voting classifier set params	0.037037
load and return the boston house-prices dataset regression	datasets load boston return_x_y	0.500000
on x and y	x y alpha	0.090909
solution to a large sparse linear system	lsqr a	0.037037
mean and variance along an axix	mean variance axis x axis	0.142857
apply clustering to a	cluster spectral clustering affinity n_clusters n_components	0.166667
list of feature name -> indices mappings	dict vectorizer	0.250000
ensemble	ensemble base ensemble	1.000000
to fit a single tree	trees tree forest x y	0.142857
generate train test indices	shuffle split iter indices	0.250000
error of the kl divergence	manifold kl divergence error	0.100000
actually run in parallel	joblib parallel backend	0.045455
by cross-validation read more in the :ref	model_selection cross val	0.250000
dictionary factor	dict dictionary y	0.111111
an estimator within	estimator estimator x	0.090909
fit the model according to the	svm linear svc fit x y sample_weight	0.250000
clustering on the	cluster	0.021277
the	joblib	0.036496
nans	feature_selection clean nans	1.000000
split data into training and test	base shuffle split split x y groups	0.200000
for elastic net parameter search parameters	x y xy l1_ratio	0.250000
validate user provided precisions	precisions precisions covariance_type n_components n_features	0.250000
mask	weights mask	0.333333
the default backend	backend backend n_jobs	0.333333
from it	memory	0.015625
call	externals joblib format call	0.200000
point	cross	0.037037
a which this function is called	externals joblib memorized	0.013699
c in (l1_min_c	c x	0.030303
computes the free energy f v = -	bernoulli rbm free energy	0.066667
of exception	joblib parallel backend base	0.058824
the kernel k x	x	0.006768
of the normalization constant	logz v s dets	0.200000
dictionary learning finds a dictionary a	batch dictionary learning	0.142857
of a memmap instance to reopen	memmap a	0.050000
of parameters	y estimator parameters	0.500000
measure the	cluster fowlkes mallows	0.250000
the score on the given data if	score x	0.033333
the unnormalized posterior log probability of x	core base nb joint log likelihood x	0.200000
the significance of a cross-validated	cv	0.009009
a binary metric for multilabel classification parameters	average binary score binary_metric y_true y_score	0.500000
function used to fit a single tree	trees tree	0.142857
training set	factor fit predict	0.066667
each	core cross val predict estimator x	0.045455
count	core multinomial nb count	1.000000
fit linear model with stochastic gradient descent	linear_model base sgdregressor fit x	1.000000
hash	hash	0.666667
cholesky decomposition	det cholesky	0.166667
generate a random multilabel classification	datasets make multilabel classification n_samples n_features n_classes n_labels	0.500000
to build	build	0.074074
is meant to be cached by a joblib	index_file_path data_folder_path slice_ color	0.033333
huber loss function for robust regression	huber loss function	1.000000
with the given arguments	joblib memorized func	0.014706
the laplacian matrix and	laplacian	0.034483
points on the	len	0.038462
mlp loss function and	multilayer perceptron	0.071429
then dtype float32 is returned	metrics return float dtype	0.250000
fit the model by computing truncated	decomposition pca fit truncated x	1.000000
in the proper format	ensemble base weight boosting validate x predict	1.000000
log probability for full covariance matrices	log multivariate normal density full x	0.333333
init	ensemble base gradient boosting init decision	0.142857
in x into a matrix of patch data	feature_extraction patch extractor transform x	0.500000
each input data point	cross val predict	0.045455
the context of the memory	externals joblib memory	0.016949
regressor from	regressor	0.054054
as training data and y	y	0.005348
the timestamp when pickling	joblib memory reduce	0.030303
data and parameters	fit x y	0.005988
perform a locally linear embedding analysis on the	manifold locally linear embedding x	0.071429
data loading for the lfw people dataset	fetch lfw people	0.040000
run fit with all sets	grid search cv fit x y	0.333333
predict probability for each possible outcome	semi_supervised base label propagation predict proba x	1.000000
matching pursuit	y n_nonzero_coefs tol	0.250000
performs inductive inference across	semi_supervised base label propagation predict x	1.000000
absolute value to be used for later scaling	abs scaler fit x	1.000000
clustering for the subclusters obtained after fitting	clustering x	0.142857
of array-like or scipy sparse matrix	binarize x threshold	0.083333
used to fit an estimator within a	fit estimator estimator x y sample_weight	0.071429
x y and optionally its gradient	sine squared call x y eval_gradient	1.000000
absolute error of the kl divergence of p_ijs	manifold kl divergence error	0.100000
erase the complete cache directory	joblib memory clear warn	0.333333
an exception in an unfitted	estimators unfitted name	0.142857
x == missing_values	x value_to_mask	1.000000
computes the logistic loss	linear_model logistic loss w x y alpha	1.000000
spectral embedding for non-linear dimensionality reduction	spectral embedding	0.200000
estimate the precisions parameters of the precision	bayesian gaussian mixture estimate precisions nk xk sk	0.166667
don't	externals joblib memorized	0.013699
indices to split data into	base shuffle split split	0.250000
apply a mask to edges weighted	weights mask edges weights	0.333333
perform dbscan clustering from features	cluster dbscan fit x y sample_weight	0.200000
cross-validated estimates for each input data	estimator x y cv	0.050000
last	last	1.000000
in representing each class with	classifier	0.013699
the process or	joblib multiprocessing backend	0.052632
precision is the ratio tp / tp +	metrics precision	0.033333
raised when configuration should fallback to another backend	fallback to backend	1.000000
numpy array of a single sample image parameters	sample image image_name	0.166667
w in multiplicative update nmf	multiplicative update w x w h beta_loss	0.500000
apply clustering to	cluster spectral clustering affinity n_clusters	0.166667
leaf	apply	0.083333
finds seeds for	seeds	0.111111
fit a binary classifier	sgdclassifier fit binary	0.333333
returns a lower bound on model	lower bound	0.071429
decision function of the given	decision function x raw_values	0.083333
of transform	transform	0.011236
cv	cv cv x y	0.031250
find the	hungarian state find	0.500000
position of	mds fit x y init	0.066667
convergence_iter	convergence_iter	1.000000
with respect to coefs and intercept	n_samples activations deltas	0.166667
a platform independent representation	utils shape	0.013699
least-squares solution to a large sparse linear system	utils lsqr a	0.037037
an mldata org data set	mldata dataname target_name data_name transpose_data	0.500000
a calibration curve	calibration curve y_true y_prob normalize n_bins	0.142857
e-step in em update	latent dirichlet allocation e step x	1.000000
returns the number of splitting iterations in the	one group out get n splits x y	0.111111
no-op object decorating	not memorized func	1.000000
compute the weighted percentile of array with sample_weight	utils weighted percentile array sample_weight percentile	0.333333
graph of neighbors for points in x	neighbors mixin radius neighbors graph x	0.500000
generate cross-validated estimates for	cv	0.009009
likelihood function for the given	likelihood function	0.142857
display the message on stout	joblib parallel print	0.166667
reduced likelihood function for the	gaussian_process gaussian process reduced likelihood function	0.047619
false for indices increasingly apart the distance	externals joblib verbosity filter index	0.055556
return the path	datasets get	0.200000
the least-squares solution to a large	lsqr a	0.037037
store the	externals joblib memorized func	0.013158
of biclusters	consensus	0.111111
y_[i]) ** 2	sample_weight y_min y_max	0.166667
sparse combination of the dictionary atoms	sparse coding mixin transform	0.333333
an 'l'	utils	0.009709
classification on an array of test	process classifier	0.500000
update and a	utils incremental	0.166667
given data	y	0.002674
remove cache folders to make cache size fit	reduce size	0.083333
whether the	dot product	0.250000
function func with arguments	func	0.011364
function to avoid code duplication between self _errors_svd	linear_model ridge gcv errors and values svd	0.333333
y is in a multilabel format	utils is multilabel y	0.333333
connectivity matrix	connectivity n_components affinity	1.000000
metrics should use this function first to	metrics	0.043478
any negative value in an	non negative x whom	0.200000
of two clusterings of a set	score labels_true labels_pred sparse	0.047619
dpgmmbase	dpgmmbase	1.000000
length from source to all reachable nodes	length graph source cutoff	0.200000
string to the file	zlib file	0.076923
function	parallel decision function	1.000000
the number of splitting iterations in the cross-validator	leave pgroups out get n splits x	0.111111
x_offset	x_offset	1.000000
each input data	cross	0.037037
position of the points	mds	0.050000
rows in x	x	0.001692
is meant to be cached	index_file_path data_folder_path slice_ color	0.033333
best found parameters	model_selection base search cv predict proba	0.076923
estimate mutual information	feature_selection mutual info classif	0.500000
estimate the precisions	gaussian mixture estimate precisions nk xk	0.166667
the number of splitting iterations in the cross-validator	model_selection leave pgroups out get n splits	0.111111
p_ij from	desired_perplexity verbose	0.500000
sparse uncorrelated design	sparse uncorrelated n_samples n_features	0.166667
indices to split data according to	split	0.027778
search over	core base search cv	0.033333
of points on the grid	parameter grid len	0.333333
label spreading	semi_supervised label spreading build	0.250000
unnormalized posterior log probability of x i	core base nb joint log likelihood x	0.200000
set x and returns	predict x y	0.043478
factor of x (as bigger	factor decision function x	0.166667
the test_size and train_size at init	split init test_size train_size	0.250000
the position of the points	mds fit x y	0.066667
squeeze	squeeze	0.833333
x from y along the	x z	0.050000
the process or thread	externals joblib multiprocessing backend	0.035714
run fit on one set of	model_selection fit grid point x	0.500000
for full covariance matrices	density full x	0.166667
estimates	core	0.015385
mean shift clustering	cluster mean shift x	0.500000
output	called str function_name	0.250000
incremental mean and variance along an axix on	mean variance axis x axis last_mean last_var	0.333333
precision the precision is the ratio	precision	0.016667
predict multi-class targets	core output code classifier predict	0.250000
the rcv1 multilabel	rcv1	0.125000
to fit a single tree in parallel	ensemble parallel build trees tree forest	0.200000
return	utils shape	0.013699
sparse uncorrelated design	make sparse uncorrelated n_samples n_features	0.166667
mixin class for all meta estimators	meta estimator mixin	0.250000
fit	regression cv fit	1.000000
parameters	params	0.142857
computes multidimensional scaling using smacof algorithm parameters	smacof single dissimilarities metric n_components init	0.333333
number of estimators in	len	0.038462
columns of a csc/csr matrix in-place	utils inplace swap column x m	0.250000
a cv in a user friendly way	core check cv cv x y classifier	0.031250
for data x	x	0.003384
predict using the gaussian process	gaussian process	0.083333
calculate the posterior log probability of the	bernoulli nb joint log likelihood	0.083333
measure the similarity of two clusterings of	fowlkes mallows score labels_true labels_pred sparse	0.333333
supports seeking	seekable	0.166667
whether the kernel	stationary kernel mixin	0.333333
write the	write	0.181818
new samples can be different from	calibrated classifier cv	0.071429
of exception types to be captured	exceptions	0.083333
sparse format	linear_model sparse coef	0.076923
and evaluates the reduced	gaussian_process gaussian process reduced	0.125000
value in data	data	0.038462
to the binary classification	y_true	0.043478
to avoid the hash	externals	0.011494
ledoit-wolf covariance	covariance ledoit wolf x	0.125000
mixture models	mixture	0.041667
model	base	0.014286
the callable case for	callable	0.058824
compute the largest k singular values/vectors for	k ncv tol	0.166667
with all sets	search cv	0.018182
the timestamp when pickling	externals joblib memory reduce	0.030303
path	omp path	0.100000
kernel k x	dot product call x	0.200000
estimator on training subsets incrementally and compute	incremental fit estimator estimator x y	0.200000
in the wild lfw pairs dataset this	datasets fetch lfw pairs subset	0.035714
em algorithm and return the cluster	gmmbase	0.062500
format	format	0.833333
x as a	x	0.001692
a which this	externals joblib memorized	0.013699
rfe model and	y	0.002674
type introduces an 'l' suffix	repr	0.012500
is stationary	mixin is stationary	1.000000
search over parameters	search cv fit x	0.111111
to avoid the	memorized	0.015873
of the local outlier factor	neighbors local outlier factor	0.125000
and set the base_estimator_ attribute	ensemble ada boost classifier validate	1.000000
the training	fit	0.022801
data loading for the lfw pairs dataset	fetch lfw pairs	0.018868
fit linear model with stochastic gradient descent	base sgdregressor fit	1.000000
of rows in x	x	0.001692
and compute scores for	and score	0.333333
the number of splitting iterations in the cross-validator	one group out get n splits x	0.111111
svmlight format	svmlight	0.050000
abort	base abort	1.000000
by computing full svd	full	0.055556
priors from multioutput-multiclass target data parameters	distribution y	1.000000
for indices increasingly apart the distance depending on	joblib verbosity filter index	0.055556
lasso path using lars algorithm	linear_model lars path	0.100000
but fall back to line_search_wolfe2 if suitable step	wolfe12 f fprime xk pk	0.028571
is equal to the average path	average path	0.142857
sparse matrix x	x dict_type	0.200000
omp solves n_targets orthogonal matching pursuit problems	linear_model orthogonal mp x y n_nonzero_coefs tol	0.200000
inplace column scaling of a csc/csr	inplace column scale	0.166667
pairs dataset this dataset is a collection	pairs subset	0.125000
orthogonal matching pursuit step on	n_nonzero_coefs	0.090909
svmlight / libsvm	svmlight file f n_features	0.066667
data	get	0.012048
low rank matrix with bell-shaped	datasets make low rank matrix	0.083333
score by cross-validation read more in the	model_selection cross val score estimator	0.166667
estimator on	estimator estimator	0.105263
in the :ref user guide <mini_batch_kmeans>	mini batch kmeans	0.166667
propagation classifier read more	propagation	0.076923
of the graph-lasso objective function the objective	objective	0.076923
cv in a user	cv cv x y classifier	0.031250
factor in place	y code verbose	0.333333
of the local outlier factor	neighbors local outlier factor decision	0.125000
pairwise matrix in n_jobs even	metrics parallel pairwise x y func n_jobs	0.111111
transform is sometimes	transform	0.011236
estimate the precisions parameters of the precision distribution	bayesian gaussian mixture estimate precisions nk	0.166667
svds	svds	1.000000
that for c	c x	0.030303
all the content of the data	datasets clear data	0.142857
we don't	externals joblib	0.009524
set the diagonal of the laplacian	manifold set diag laplacian	0.333333
uncorrelated design	uncorrelated n_samples n_features random_state	1.000000
check according	check	0.017857
the leaf	apply x	0.166667
generate the random projection matrix	sparse random projection make random matrix n_components n_features	1.000000
k-fold iterator variant with non-overlapping labels	kfold	0.058824
data loading for the lfw people dataset	lfw people	0.040000
sign of elements of all the	sign flip	0.066667
number of splitting iterations in the cross-validator	predefined split get n splits x y	0.111111
isotonic regression or sigmoid	calibrated classifier	0.083333
the leaf that each sample is predicted as	tree apply x check_input	0.500000
shutdown the	joblib multiprocessing backend terminate	0.166667
matrix for x using the	x y	0.002155
fit	cv fit x	0.250000
apply the derivative	derivative z	0.333333
compute density of a sparse	utils density	1.000000
pickling reduction for memmap backed arrays	externals joblib reduce memmap backed a m	1.000000
return the score for a fit	fit	0.003257
read from file-like object until size bytes are	bytes fp size error_template	0.333333
estimates	estimator	0.014706
returns whether the	gaussian_process dot product	0.333333
validate input	base sgd validate	1.000000
the search over	core base search	0.111111
directory in which are persisted the result	output dir	0.047619
row of x to unit norm parameters	preprocessing normalizer transform x	0.250000
dataset is described by its spectrum spectrum	spectrum	0.090909
non-negative matrices w h whose product	w h	0.031250
maximizer of	gaussian process arg max	0.047619
init	gradient boosting init decision function x	0.142857
number of splitting iterations in	base cross validator get n splits x y	0.125000
calculates a covariance matrix shrunk on the	shrunk covariance emp_cov shrinkage	0.250000
nonzero entries	feature_extraction count vectorizer inverse transform	0.166667
passive aggressive classifier read more	passive aggressive classifier	0.125000
gaussian_process	gaussian_process	0.156250
binarizer parameters	binarizer	0.142857
number of splitting iterations in	predefined split get n splits	0.111111
store the timestamp when pickling to avoid	func reduce	0.050000
within a given radius of a point or	x radius	0.058824
train test indices	shuffle split iter indices	0.250000
split data into	base kfold split x	0.250000
or	feature_extraction	0.074074
clusterings	clusterings	1.000000
partially fit a single binary estimator one-vs-one	partial fit ovo binary estimator	1.000000
long type introduces an 'l' suffix when	shape	0.011765
conditional property using	iff has attr descriptor	0.083333
computes an orthonormal matrix whose	size n_iter power_iteration_normalizer	0.166667
the unnormalized posterior log probability of	core base nb joint log likelihood	0.166667
generate cross-validated estimates for	predict estimator x y cv	0.071429
list	externals joblib parallel backend base get	0.066667
not found and raise an	line search	0.029412
jaccard similarity coefficient score	score y_true y_pred	0.038462
fit to data then transform it	core transformer mixin fit transform	0.500000
returns the number of splitting iterations in	split get n splits x	0.111111
weiszfeld step	weiszfeld step x	1.000000
to build a batch of estimators within a	build estimators n_estimators	0.166667
generate train	split iter	0.166667
y_prob	y_prob	0.857143
check that all arrays have consistent first dimensions	utils check consistent length	1.000000
evaluate	call	0.052632
the vocabulary dictionary and return term-document matrix	transform raw_documents y	0.100000
evaluates the reduced likelihood function	gaussian process reduced likelihood function	0.047619
number of splitting iterations in	split get n splits x y	0.111111
l1 distances between the vectors in x	metrics manhattan distances x	0.500000
dictionary factor in place	dictionary y code verbose	0.333333
average log-likelihood	decomposition factor analysis score	0.333333
with non-overlapping groups	group	0.083333
run fit on the estimator	fit x	0.006410
fit estimator and predict values	core fit and predict estimator x y train	0.250000
ncv	ncv	1.000000
func_code	func_code	1.000000
score on the given data if	score x y	0.030303
compute elastic net path with coordinate descent	enet path	0.050000
pprint	pprint	1.000000
clustering on x and returns cluster labels	cluster dbscan fit predict x y sample_weight	0.166667
x relative	metrics threshold scorer call clf x y	0.058824
for a calibration curve	core calibration curve y_true y_prob normalize n_bins	0.142857
the maximum	max	0.071429
which are going to run in parallel	externals joblib multiprocessing backend effective	0.250000
cluster is worthy	cluster cfsubcluster	0.250000
elastic net path with	enet path x	0.050000
center x y and	center scale xy x y	1.000000
get the directory corresponding to the cache	memorized func get func dir mkdir	0.500000
case method='lasso' is	y xy gram	0.090909
on the estimator with the best found parameters	model_selection base search cv	0.120000
transform is	transform y	0.023256
the search	core base search cv fit x y	0.166667
probabilities for a calibration curve	calibration curve y_true y_prob normalize	0.142857
from source	source	0.100000
parameters for each mixture	mixture gmmbase	0.034483
referred to by some authors as	preprocessing label binarizer	0.071429
full covariance matrices	full x	0.166667
and evaluates the reduced likelihood function	gaussian_process gaussian process reduced likelihood function	0.047619
in n_jobs even slices	y func n_jobs	0.166667
fit the model using x as training	neighbors unsupervised mixin fit x y	0.500000
fit a multi-class classifier by	sgdclassifier fit	0.076923
w	w ht l1_reg	0.250000
fit the model using x y	fit x y sample_weight	0.020000
placeholder for fit subclasses should implement this method!	decomposition base pca fit	0.333333
strip the headers by	datasets strip	0.076923
of x to unit norm parameters	preprocessing normalizer transform x	0.250000
report showing the main	report	0.047619
implicit data conversions happening in the code	data conversion	0.333333
determine whether y is monotonically correlated with x	core check increasing x y	0.333333
to make cache size fit in	memory reduce size	0.083333
predict_proba on the estimator with the best found	model_selection base search cv	0.040000
similarity coefficient score the	score y_true y_pred normalize sample_weight	0.125000
one class versus all others	multiclass x y alpha c	0.166667
fit the model and transform with the	fit transform	0.100000
array-like or scipy sparse matrix	binarize x	0.083333
em	em	1.000000
end of	end ll	0.166667
linear	linear	0.875000
fit is on	fit	0.003257
leave-one-out cross validation iterator	leave one out	1.000000
and train_size	train_size	0.142857
file was opened for writing	binary zlib file writable	0.250000
filters the given args and kwargs	args func ignore_lst args kwargs	0.333333
swaps two rows of a csc/csr matrix in-place	utils inplace swap row x m n	0.250000
ward clustering based on a feature matrix	cluster ward tree x connectivity n_clusters return_distance	0.250000
private function used to compute decisions within a	function estimators estimators_features	0.500000
binary labels back to	label binarizer inverse	0.166667
fit multitaskelasticnet model with coordinate descent parameters	linear_model multi task elastic net fit	1.000000
indicating which features are	support	0.125000
callable case	callable x	0.083333
the l1 distances between the vectors in	metrics paired manhattan distances	0.083333
depth	memorized func check previous func code	0.333333
return the kernel k x	matern call x	0.200000
boolean thresholding of array-like or scipy	binarize x threshold	0.083333
loss of	ensemble loss	0.166667
depending	joblib memorized	0.015625
a tolerance which is independent of	tolerance	0.045455
checking for random matrix generation	core check input size n_components n_features	0.200000
sample from	neural_network bernoulli rbm sample	0.500000
outcomes for samples in	ensemble voting classifier predict	0.100000
fit linear model with passive aggressive algorithm	linear_model passive aggressive classifier partial fit x	1.000000
of module names and a name	name	0.033333
friedman [1] and breiman [2]	make friedman3	0.166667
cfsubcluster	cfsubcluster	0.555556
the dual gap convergence criterion	dual gap emp_cov precision_	0.071429
logic estimators that implement the partial_fit	utils check partial	0.038462
extracts patches of any n-dimensional array	patches	0.055556
remove cache	joblib memory	0.016949
set	core pipeline set	0.250000
implementation is restricted to the binary classification	y_true y_score pos_label	0.066667
a buffered version of a read file object	joblib buffered read file fobj	0.500000
precision the precision is the ratio tp	metrics precision	0.033333
generate a random multilabel classification problem	datasets make multilabel classification n_samples n_features n_classes n_labels	0.500000
the k-neighbors of	kneighbors mixin kneighbors	0.100000
the neighbors within a given radius of	radius neighbors x radius	0.142857
class covariance	core class cov	0.250000
the given arguments	joblib memorized func get output	0.125000
isolation forest algorithm return the anomaly score of	isolation forest	0.200000
when using	repr	0.012500
from	externals joblib	0.009524
get the values used to update params	sgdoptimizer get	0.125000
avoid code duplication between self _errors_svd	linear_model ridge gcv errors and values svd	0.333333
delete all the content of the data home	data home data_home	0.055556
opposite of the local outlier factor	local outlier factor decision function	0.125000
vectors rows of u such that	flip u	0.047619
compute gaussian log-density at x for a diagonal	log multivariate normal density diag x means	1.000000
the best found	base search cv predict proba x	0.076923
perform a locally linear embedding	manifold locally linear embedding x n_neighbors n_components	0.071429
matrix shrunk on the diagonal	shrunk	0.043478
of a memmap instance to reopen	reduce memmap a	0.050000
learn vocabulary and idf return	fit transform raw_documents y	0.100000
check if y	y	0.002674
in the svmlight / libsvm format into	svmlight file f	0.066667
outlier on the training set	outlier factor fit predict	0.200000
the number of splitting iterations in the cross-validator	group out get n splits x	0.111111
of a csr matrix in-place	utils inplace swap row csr x m	0.250000
list of module names and a name for	name	0.033333
random multilabel classification problem	datasets make multilabel classification	0.166667
prepare	prepare	1.000000
the score on the given data if the	score x	0.033333
sample images for image manipulation	sample images	0.250000
inefficient	x y classes	0.027778
divergence of p_ijs	divergence	0.090909
matrix representing a fully connected graph	graph	0.021277
coordinate descent the elastic net optimization function	l1_ratio	0.030303
model according to the given training	x y sample_weight	0.025974
for x relative	metrics predict scorer call estimator x	0.166667
generate random samples from the model	mixture gmmbase sample n_samples random_state	1.000000
estimates for each input	core cross val predict estimator x	0.045455
spherical	spherical resp	1.000000
a random sample from a	a size replace p	0.142857
lad updates terminal regions to median	least absolute error update terminal region	0.200000
generate a random multilabel classification problem	make multilabel classification n_samples n_features n_classes n_labels	0.500000
fit the model to data	output estimator fit x	0.200000
the pairwise matrix	parallel pairwise	0.166667
prediction of init	base gradient boosting init decision	0.142857
the weighted log probabilities for	base mixture	0.111111
median absolute error regression	metrics median absolute error	0.166667
for the voting classifier valid	voting classifier set	0.037037
remove cache folders to	reduce	0.017241
to workaround python 2 limitations	obj methodname	0.111111
multiple files in svmlight format this function is	svmlight files files n_features dtype	0.200000
api and hence	patch extractor fit x y	0.142857
don't store	externals joblib	0.009524
apply clustering to a projection	clustering affinity n_clusters	0.166667
array corresponding	numpy array	0.500000
the callable case for	pairwise callable	0.083333
log probabilities within a job	parallel predict log proba estimators estimators_features x	0.250000
estimator on training subsets incrementally	core incremental fit estimator estimator x	0.500000
the model to the data x which should	x y	0.002155
and	x	0.005076
for the voting classifier valid parameter keys can	ensemble voting classifier set	0.037037
classification	y_true y_pred labels sample_weight	0.125000
change the default backend used by parallel	parallel backend backend	0.166667
return terms per document with nonzero entries	feature_extraction count vectorizer inverse transform	0.166667
predict using the trained model parameters	base multilayer perceptron predict	0.333333
on x and returns cluster	cluster dbscan fit predict x y sample_weight	0.166667
'l' suffix when using the	utils	0.009709
the actual data loading for the lfw people	datasets fetch lfw people	0.040000
a which	externals joblib	0.004762
indices	core base shuffle split iter indices	0.250000
the wild lfw pairs dataset this dataset	fetch lfw pairs	0.018868
mean shift clustering using a	mean shift	0.125000
and last element of	and last element arr	0.166667
score on the given	score x y	0.030303
fit estimator and transform dataset	ensemble random trees embedding fit transform x y	1.000000
a classification	y_true y_pred	0.037037
x into a matrix of patch data	feature_extraction patch extractor transform x	0.500000
cleanup a temporary folder if still existing	delete folder folder_path	0.250000
a single binary	core predict binary	0.200000
to build from the c	build from c	0.500000
train	shuffle split	0.142857
initial centroids	cluster init centroids x k init	0.166667
call predict on the estimator with the	predict x	0.011765
find the first prime element in	find prime in	0.333333
symmetric decorrelation i	decomposition sym decorrelation w	1.000000
solution to a sparse coding problem	decomposition sparse encode	0.333333
loading for the lfw people dataset this	fetch lfw people	0.040000
implementation is restricted to the binary classification	score y_true y_score average	0.076923
class weights for unbalanced datasets	class weight class_weight classes y	0.500000
a cv in	core check cv cv x y classifier	0.031250
log of probability estimates	linear_model sgdclassifier predict log proba	1.000000
given arguments	memorized func	0.016949
estimate the precisions parameters of the precision distribution	gaussian mixture estimate precisions nk xk	0.166667
underlying estimators should be used	one vs one classifier	0.125000
on x	x	0.001692
generate a mostly low rank matrix with	datasets make low rank matrix n_samples n_features	0.500000
ledoit-wolf covariance	covariance ledoit wolf shrinkage	0.125000
the curve auc	auc x	0.040000
regularization parameters	x y pos_class cs	0.166667
return whether the file	zlib file	0.153846
function to portion of selected features	selected copy	0.500000
not found	utils line search	0.029412
class covariance	class cov x	0.250000
voting classifier valid	ensemble voting classifier set params	0.037037
outlier factor of x (as bigger is	outlier factor decision function x	0.200000
count	core multinomial nb count x	1.000000
when	shape	0.011765
a given radius	radius	0.045455
the	covariance empirical covariance	0.071429
generate a random n-class classification problem	datasets make classification n_samples n_features n_informative n_redundant	0.500000
and breiman [2]	friedman3 n_samples noise	0.166667
predict the target	predict x	0.011765
recall the recall is the ratio	metrics recall	0.033333
the parameters for the voting classifier valid	ensemble voting classifier set params	0.037037
scores note this implementation is restricted to the	roc	0.033333
estimate the precisions parameters of the precision	bayesian gaussian mixture estimate precisions	0.166667
compute_sources	compute_sources	0.833333
compute the grid of alpha values	alpha grid	0.166667
estimates for each input	estimator	0.014706
avoid	func	0.011364
breiman [2]	make friedman3 n_samples noise	0.166667
uncompressed bytes	joblib binary zlib	0.333333
returns the number of splitting iterations in the	leave pgroups out get n splits	0.111111
bytes_limit	externals	0.005747
predict_log_proba on the	predict log proba	0.029412
the gp prior	x return_std return_cov	0.142857
under the model	mixture dpgmmbase score samples	1.000000
write array bytes to pickler file	array wrapper write array array pickler	0.333333
squares projection of the data onto	x ridge_alpha	0.071429
train test	shuffle split	0.142857
update parameters with given gradients parameters	optimizer update params grads	1.000000
log-det of the cholesky decomposition of matrices	det cholesky matrix_chol covariance_type	1.000000
of the decision functions of	decision function	0.025000
store the	joblib memorized	0.015625
number of splitting iterations in the cross-validator	leave one out get n splits x	0.111111
ardregression model according to the given training	linear_model ardregression	0.100000
from data in	manifold spectral embedding	0.111111
the number of splitting iterations in the	predefined split get n splits x	0.111111
using	linear_model	0.179487
a func to	externals joblib parallel backend base apply async func	0.250000
exception	externals joblib parallel backend	0.029412
class covariance matrix	core class cov x y priors	0.250000
training set x and	x y	0.002155
linear model with stochastic gradient descent	x y coef_init intercept_init	0.333333
list of	parallel	0.019231
normalize x by scaling rows and	cluster scale normalize x	0.142857
for validation and conversion of csgraph	sparsetools validate graph csgraph directed dtype csr_output	0.166667
and scaling	transform x y	0.031250
is the depth a which	externals joblib memorized func check previous func code	0.055556
the pairwise matrix in	pairwise x	0.166667
inplace column scaling of a csc/csr	utils inplace column scale x scale	0.166667
the barycenter weighted	barycenter	0.090909
for the	covariance	0.028986
samples can be different from	core calibrated classifier cv	0.111111
list of exception	joblib parallel backend	0.045455
determinant matrix	fast mcd x	1.000000
x according to the	x	0.001692
initialize the model parameters	base mixture initialize	0.333333
on	model_selection	0.166667
compute the squared	neural_network squared	0.500000
compute data precision matrix with the	base pca get precision	0.066667
the voting classifier valid parameter keys	ensemble voting classifier set	0.037037
faces in the wild lfw pairs	lfw pairs subset	0.035714
return a tolerance which is	tolerance	0.045455
splits for an arbitrary randomized	splits	0.083333
solver	solver	1.000000
x relative to y_true	metrics threshold scorer call clf x y	0.058824
load dataset from multiple files in svmlight	datasets load svmlight files files n_features dtype multilabel	0.500000
true and predicted probabilities for a calibration curve	calibration curve y_true	0.142857
maximum absolute value	preprocessing max abs	0.050000
a list of edges for a	feature_extraction make edges	0.066667
build a batch	build	0.037037
data	clear data	0.142857
dataset and properly handle kernels	utils safe split estimator x y indices	0.200000
with the best found	model_selection base search cv	0.040000
model using x as	x	0.003384
computes the paired cosine	paired cosine	0.333333
the binary classification task	score y_true y_score	0.025000
step in pipeline after transforms	pipeline fit predict x y	0.166667
for parallel processing this method is meant to	externals joblib parallel	0.014085
update the dense dictionary factor in place	update dict dictionary y code verbose	1.000000
dictionary learning finds a dictionary a	dictionary learning	0.142857
with block checkerboard structure for biclustering	checkerboard shape n_clusters noise minval	0.066667
locally linear embedding analysis on the data	locally linear embedding x n_neighbors n_components	0.071429
returns the number of splitting iterations in	predefined split get n splits x	0.111111
x y as training data	x y	0.004310
largest k singular values/vectors for	k ncv tol	0.166667
estimator with the best found parameters	base search cv predict proba	0.076923
indices in	matching indices	0.250000
a	utils	0.048544
point	cross val predict estimator	0.045455
on the estimator with the best found parameters	base search cv predict	0.076923
empty	memorized func clear warn	0.250000
learn and	transform x y	0.031250
operation is meant to be	data_folder_path slice_ color resize	0.033333
the dense dictionary factor in	dict dictionary	0.111111
predict class log-probabilities	ensemble forest classifier predict log proba	0.500000
covariance	covariance	0.246377
fit a single binary	core fit binary	1.000000
neighbors for	radius neighbors mixin radius neighbors	0.125000
cluster is worthy enough	cluster cfsubcluster	0.250000
estimate sample weights by class for unbalanced datasets	utils compute sample weight class_weight y indices	0.500000
an	shape repr	0.013699
latent	latent	1.000000
kddcup99 dataset	fetch brute kddcup99	0.166667
compute the precision the precision is the ratio	metrics precision	0.033333
the process or thread	externals joblib	0.004762
constructs signature from the given list of parameter	externals signature	0.050000
model with stochastic gradient descent	base sgdregressor	0.100000
cancer	cancer	1.000000
lower bound on model	dpgmmbase lower bound	0.071429
name est weight tuples excluding none	iter	0.050000
compute non-negative matrix factorization	factorization x	0.043478
kernel k x	x	0.006768
binary labels back to	preprocessing label binarizer inverse	0.166667
estimates for each input data point	val predict	0.045455
the pairwise matrix	pairwise x y	0.166667
function output for x	x	0.001692
average of the decision functions of	decision function	0.025000
fit linear model with passive aggressive algorithm	passive aggressive regressor fit x y	1.000000
private function used to fit a single tree	trees tree	0.142857
operation is meant to be cached	index_file_path data_folder_path slice_ color	0.033333
the shortest path length from source to all	source shortest path length graph source	0.111111
stacklevel	stacklevel	0.750000
build from the c	build from c	0.500000
regression	y_pred	0.142857
initialization of	initialize x	0.400000
metric for multilabel classification parameters	binary_metric y_true	1.000000
function returns posterior probabilities of classification	classifier cv predict proba x	0.200000
true and predicted probabilities for a calibration curve	calibration curve	0.142857
the kddcup99 dataset	datasets fetch brute kddcup99	0.166667
scaler	min max scaler	0.083333
estimators	estimators	0.368421
depending from it	joblib memorized func	0.014706
grid of alpha values	alpha grid	0.166667
matrix to dense array format	sparse coef mixin densify	0.100000
thresholding of array-like or scipy sparse matrix	binarize x threshold copy	0.083333
or lasso path using lars algorithm [1]	linear_model lars path x	0.100000
also predict based on an unfitted model	predict x	0.011765
fit	sgdclassifier fit x y	0.333333
reconfigure the backend and return the number of	externals joblib parallel backend base configure n_jobs parallel	0.333333
data point	core cross val predict	0.045455
the score on the given data if the	score x y	0.030303
build a batch of estimators within a	parallel build estimators n_estimators ensemble	0.166667
return the	externals joblib get	0.285714
random matrix	random choice csc	0.166667
input validation for standard	accept_sparse dtype	0.200000
estimator with the best found parameters	core base search cv predict proba	0.076923
matrices w h whose product approximates	w h n_components	0.038462
theta as the maximizer of the	process arg max	0.047619
to be merged if	merge subcluster nominee_cluster threshold	0.250000
of x according	inverse transform x	0.025641
the index of the leaf	tree base decision tree apply x	0.166667
return the directory in which	dir	0.038462
recall is the ratio tp / tp +	metrics recall	0.033333
make and configure	ensemble base ensemble make estimator append random_state	0.166667
predict regression target at each stage for x	regressor staged predict x	1.000000
labels the output of transform is sometimes	transform	0.011236
coefficient	linear_model sparse coef	0.076923
least-squares solution to a large	a	0.018182
w h whose product approximates	w h	0.031250
matrix x and target y	base multilayer perceptron partial	0.166667
return a callable that handles preprocessing and tokenization	vectorizer mixin build analyzer	0.333333
matrix given column class distributions	classes class_probability	0.166667
randomized lasso	randomized lasso	1.000000
train estimator on training subsets incrementally	model_selection incremental fit estimator estimator	0.500000
partially fit a single binary estimator	partial fit binary estimator x y	1.000000
predict on	predict	0.006849
biclusters	metrics cluster consensus	0.250000
the kernel k x y and	matern call x y	0.200000
return the compressor matching fileobj	externals joblib detect compressor fileobj	1.000000
depending from it	func	0.011364
estimate sample weights by class for unbalanced datasets	utils compute sample weight class_weight y	0.500000
to x	x y	0.002155
that implement	utils check partial	0.038462
cache folders to	joblib	0.007299
trained model parameters	neural_network base multilayer perceptron	0.083333
exception types	parallel backend	0.030303
compute the decision function of the given observations	covariance outlier detection mixin decision function x raw_values	0.333333
return staged predictions for	boost classifier staged predict	1.000000
covariance matrix shrunk	shrunk covariance	0.090909
right fileobject from a filename	externals joblib read fileobject fileobj filename mmap_mode	0.250000
kernel k x	gaussian_process white kernel call x	0.333333
rand index adjusted for	cluster adjusted rand	0.333333
log probabilities within a	parallel predict log proba	0.058824
optimal	compute	0.058824
svmlight / libsvm file format	svmlight file x y	1.000000
fit	fit y	1.000000
with the	decomposition base pca	0.071429
the model by computing full svd on x	full x n_components	0.333333
to catch and hide warnings	utils ignore warnings	0.142857
for validation and conversion of csgraph inputs	csgraph directed dtype csr_output	0.166667
initialize the model parameters of the	mixture base mixture initialize x	0.333333
a nicely formatted statement displaying the function call	call func args kwargs object_name	0.333333
set the parameters of	pipeline set	0.250000
median absolute error regression loss read more in	median absolute error y_true y_pred	0.166667
parallel backend	parallel backend name	1.000000
fixes	cluster fix	1.000000
name for	name	0.033333
the model to the training set x and	predict x y	0.043478
a	externals joblib memorized func check	0.125000
with the generative model	decomposition base pca get	0.071429
orthogonal matching pursuit step	n_nonzero_coefs	0.090909
exception in an unfitted	estimators unfitted	0.142857
inplace row scaling of a csr matrix	utils inplace csr row scale x	1.000000
points	axis metric	0.250000
write array bytes to pickler file	write array array pickler	0.333333
standardize a dataset along any axis center	axis with_centering with_scaling	0.333333
in the given	compress	0.100000
back to line_search_wolfe2	wolfe12 f fprime xk pk	0.028571
hastie	make hastie	0.125000
c in (l1_min_c infinity) the model	c x y	0.030303
usual api and	x y	0.006466
low rank matrix with bell-shaped singular values most	datasets make low rank matrix	0.083333
evaluate the density model on the data	neighbors kernel density score samples x	0.250000
compute the weighted log probabilities for	base mixture score	0.200000
the number of splitting iterations in the cross-validator	base cross validator get n splits x y	0.125000
model according to the given training	sample_weight	0.037037
mostly low rank matrix with bell-shaped singular	make low rank matrix	0.083333
length	length graph	1.000000
shutdown the process or	externals joblib multiprocessing backend terminate	0.166667
gaussian naive bayes gaussiannb can perform online updates	gaussian nb	0.500000
avoid the hash depending from it	func	0.011364
along an axix on a csr or csc	axis x axis	0.083333
to the distortion introduced by a random	n_samples	0.058824
mean squared logarithmic error	metrics mean squared log error	0.200000
change	seek offset whence	1.000000
trigger	trigger	1.000000
thread pool	externals joblib multiprocessing backend	0.035714
compute class covariance matrix	class cov x y priors	0.250000
to pickler	pickler	0.083333
align	align	1.000000
compute data	decomposition base	0.076923
number of splitting iterations in the cross-validator parameters	base kfold get n splits x y groups	0.111111
from a	a size	0.142857
computes the log-likelihood of a gaussian data set	empirical covariance score	0.166667
class for all meta estimators	meta	0.043478
onto the sparse components	sparse pca transform x ridge_alpha	0.200000
fit	multi output estimator fit x y	0.200000
the raw documents	feature_extraction count vectorizer	0.125000
local structure is retained	trustworthiness x x_embedded n_neighbors precomputed	0.200000
2	2	1.000000
resample	resample	1.000000
the shortest	shortest	0.125000
nothing and return the estimator unchanged this	feature_extraction	0.037037
object_name	object_name	1.000000
given training data and parameters	fit x y	0.005988
return a buffered	externals joblib buffered	0.333333
reconstruct the array	externals joblib ndarray wrapper read unpickler	0.333333
the dense dictionary	dict dictionary	0.111111
dataset is constructed	datasets make	0.015625
estimate the spherical wishart	bayesian gaussian mixture estimate wishart spherical nk xk	0.333333
downloading it if necessary	data_home download_if_missing random_state	1.000000
covariance determinant mcd : robust estimator of covariance	cov det	0.200000
points in x	x	0.003384
gradient	grad w	1.000000
shrinkage	shrinkage	1.000000
the isotonic regression model :	isotonic regression y	0.066667
neighbors within a given radius of a point	lshforest radius neighbors x radius	0.142857
rand index adjusted for	metrics cluster adjusted rand score labels_true labels_pred	0.333333
list of exception types	backend	0.016949
the number of splitting iterations in the	base kfold get n splits	0.111111
the number of splitting iterations in the cross-validator	cross validator get n splits	0.125000
sign of vectors for reproducibility flips the	utils deterministic vector	0.076923
to the given training data and parameters	y	0.010695
evaluates the reduced likelihood function for the given	reduced likelihood function	0.041667
by scaling each feature to a given range	preprocessing minmax scale	0.142857
function call with the given arguments	call func	0.100000
fit linear	fit x y	0.029940
the precision the precision is	metrics precision	0.033333
estimator	estimator x	0.151515
true and false positives per binary classification threshold	metrics binary clf curve y_true	0.090909
label sets with a given mapping	preprocessing multi label binarizer transform y class_mapping	1.000000
hash depending from	externals	0.011494
coefficient score	score y_true y_pred normalize	0.125000
by scaling	preprocessing minmax scale	0.142857
'l' suffix when using	utils shape	0.013699
number of splitting iterations in the cross-validator parameters	pgroups out get n splits x y groups	0.111111
the number of splitting iterations in the	group out get n splits x y groups	0.111111
non-negative matrices w h whose product approximates the	w h n_components	0.038462
the california	california	0.125000
mask	mask edges weights mask	0.333333
density model	kernel density	0.083333
compute data	decomposition	0.047619
list of exception types to	joblib parallel backend	0.045455
in svmlight format this function is equivalent	svmlight	0.050000
compute minimum	min	0.045455
implementation is restricted to the binary classification task	precision recall curve y_true	0.142857
kddcup99 dataset downloading it	brute kddcup99 subset data_home download_if_missing random_state	0.111111
for x	x	0.021997
reducing the size of the	items root_path	0.066667
update the variational distributions	mixture dpgmmbase update	0.250000
to fit a single tree	trees tree	0.142857
a new	utils	0.009709
posterior log probability of	core bernoulli nb joint log likelihood	0.083333
case of a logistic	logistic	0.047619
non-negative matrices w h whose product	w h n_components	0.038462
format check x format and make sure no	allocation check non neg array	0.500000
return a tolerance which is independent of the	tolerance x tol	0.058824
infers the dimension of a dataset	dimension	0.050000
descriptors of a memmap instance to	externals joblib reduce memmap a	0.050000
computes the free energy f v =	rbm free energy	0.066667
maximum absolute value to	preprocessing max abs	0.050000
nystroem	nystroem	1.000000
introduced by a random	n_samples	0.058824
swaps two rows of a csc/csr matrix in-place	utils inplace swap row	0.250000
indices in sorted array of integers	find matching indices tree bin_x left_mask right_mask	0.166667
fit linear model	linear_model linear regression fit x y	1.000000
given arguments	joblib memorized func get output	0.125000
introduces	repr	0.012500
is float32 then dtype	dtype	0.062500
break the pairwise matrix in	pairwise	0.066667
run fit on	fit x	0.006410
returns the number of splitting iterations in the	base cross validator get n splits x	0.125000
normalize x according	normalize x	0.076923
init	gradient boosting init	0.142857
any axis center to the mean and component	x axis	0.015385
the number of splitting iterations in the	one out get n splits x y groups	0.111111
the directory in which are persisted the	dir	0.038462
load datasets in the svmlight / libsvm format	datasets load svmlight file f	0.500000
iterate over the	iter	0.050000
or thread pool and return the number	externals joblib multiprocessing backend	0.035714
exp(-e v h	v	0.052632
coverage error measure	coverage error y_true y_score	0.166667
this dataset is described in friedman [1]	datasets	0.015152
scale	preprocessing max abs scaler transform	1.000000
compute data	base pca get	0.076923
number of splitting iterations in	predefined split get n splits x	0.111111
instance for the given param_grid	grid search cv get param iterator	0.166667
labels to binary labels the output of transform	transform y	0.023256
new samples can be different from	core calibrated classifier	0.083333
check initial parameters of the	mixture check parameters x	0.166667
and predicted probabilities for a calibration curve	calibration curve y_true	0.142857
of exception types	parallel backend	0.030303
prediction	linear_model ransacregressor	0.500000
samples x to the separating hyperplane	svm one class svm decision function x	0.250000
a single boost using the	ensemble ada boost classifier boost	0.100000
and a name for the	name	0.033333
y as training data	y xy	0.333333
note this implementation is restricted	roc	0.033333
generate a sparse random matrix	utils random choice csc n_samples	0.333333
based on	connectivity n_clusters	0.250000
batch of estimators within a	estimators	0.052632
embedding analysis	embedding x n_neighbors	0.200000
c_step procedure described in [rouseeuw1984]_ aiming at computing	n_support remaining_iterations initial_estimates	0.111111
global clustering for the subclusters	global clustering x	0.142857
compute mutual information between two variables	compute mi x	1.000000
update h in multiplicative update nmf	multiplicative update h x w h	0.250000
a sparse matrix	svds a	0.166667
shortest path length from source	source shortest path length graph source cutoff	0.111111
the trained model parameters	base multilayer perceptron	0.142857
scale back the data to the original	standard scaler inverse transform	0.066667
fit an estimator within	fit estimator estimator x y	0.071429
isotonic regression model : min	isotonic regression	0.055556
the decision functions of the base	decision function x	0.018868
the reduced likelihood function for	gaussian process reduced likelihood function	0.047619
inplace row scaling	inplace row scale x scale	0.142857
estimates for each	cross val	0.038462
dense dictionary factor in	dict dictionary y	0.111111
minmax	minmax	1.000000
cache	joblib memory reduce	0.030303
training set x and returns the	predict x y	0.043478
t	t	1.000000
kernel k x y and	gaussian_process exp sine squared call x y	1.000000
compute the moore-penrose pseudo-inverse of a hermetian matrix	utils pinvh a cond rcond lower	0.200000
data precision matrix	precision	0.016667
return	return	1.000000
vectors for reproducibility flips the sign of	utils deterministic vector sign flip	0.066667
fit the model according to the given	linear svr fit x y sample_weight	0.250000
by computing full svd on x	full x n_components	0.333333
cal_sstats	cal_sstats	1.000000
training set	factor fit	0.062500
boolean thresholding of array-like or scipy	binarize	0.045455
of the local	neighbors local	0.250000
or	joblib	0.007299
build a process or thread pool	backend	0.016949
cache folders to	reduce	0.017241
include_self param	include self x include_self	1.000000
update the dense dictionary factor	update dict dictionary y	0.333333
the deviance	deviance call y pred sample_weight	0.333333
information for reducing the size of	items root_path	0.066667
boosted regressor	ensemble ada boost regressor	0.333333
dataset downloading it if	subset data_home download_if_missing random_state	0.166667
total log probability under	neighbors kernel density score x y	0.333333
sparsetools	sparsetools	1.000000
tp + fn where tp is	score y_true y_pred labels pos_label	0.027778
constructs signature from the given list of parameter	signature	0.047619
model according to the	y sample_weight	0.035714
estimate the spherical wishart	gaussian mixture estimate wishart spherical nk	0.333333
too rare or too common features	feature_extraction count vectorizer limit features	0.250000
fit the model according to the	linear svc fit x y sample_weight	0.250000
seeds for mean_shift	seeds x bin_size min_bin_freq	0.500000
distributions for the means	means x z	0.250000
remove cache folders	externals joblib memory reduce	0.030303
to the cache	externals joblib memorized	0.013699
number of jobs	effective n jobs n_jobs	0.333333
fit the model	chi2sampler fit	0.250000
convert coefficient	sparse coef mixin	0.083333
score with the final estimator parameters	core pipeline score x	1.000000
probabilities for a calibration curve	core calibration curve y_true	0.142857
model using x as training	x	0.003384
data in	data compress	0.100000
the output of transform is	transform y	0.023256
restricted to the binary	y_score	0.083333
a temporary folder if still existing	utils delete folder folder_path warn	0.250000
according to	sample_weight	0.074074
path	path	0.282051
within a job	estimators estimators_features	0.333333
a sparse combination of the dictionary atoms	sparse coding mixin transform x	0.333333
terminal regions to median estimates	terminal region tree terminal_regions leaf	0.066667
depth a which this	externals joblib memorized func check previous func code	0.055556
kernel is stationary	kernel mixin is stationary	0.333333
normalized mutual information	normalized mutual info	1.000000
depending from it	externals joblib memory	0.016949
to fit an estimator within a job	ensemble parallel fit estimator estimator x y sample_weight	0.333333
a mostly low rank matrix with bell-shaped	low rank matrix	0.083333
eigenvalues and eigenvectors of the	m sigma	0.250000
multiple files in svmlight format	svmlight files files n_features dtype	0.200000
the number of splitting iterations in	pgroups out get n splits	0.111111
matrices w h whose product approximates the	x w h n_components	0.038462
reconstruct the array	wrapper read unpickler	0.333333
replace=true p=none) generates a random sample from	size	0.032258
with the generative model	base pca	0.071429
to the normalized laplacian	eigen_solver	0.090909
of the cf	birch	0.125000
values of the basic parameters	initial parameters x	1.000000
of feature	vectorizer get feature	0.200000
private function used to compute decisions within a	function estimators estimators_features x	0.500000
fit_predict of last step in pipeline after transforms	pipeline fit	0.166667
initialization	initialize x resp	0.500000
approximation of the breakdown point	linear_model breakdown point n_samples n_subsamples	1.000000
the position of	mds fit x y init	0.066667
perform dbscan clustering	cluster dbscan	0.125000
labels in	preprocessing label	0.166667
the number of splitting iterations in the cross-validator	predefined split get n splits x y	0.111111
factorizing common classes param logic	partial fit first call clf classes	0.058824
returns the submatrix corresponding to bicluster i	core bicluster mixin get submatrix i data	0.333333
bytes	bytes	1.000000
score_func	score_func	1.000000
of exception types to	externals joblib parallel backend base	0.034483
call wrapped function cache result	externals joblib memorized func call	0.200000
return the shortest path length	utils single source shortest path length	0.333333
exception in an unfitted estimator	estimators unfitted name estimator	0.142857
a covariance matrix shrunk on the diagonal	covariance shrunk covariance	0.090909
the number of splitting iterations in the	pgroups out get n splits x	0.111111
returns the number of splitting iterations in the	model_selection cviterable wrapper get n splits x	0.111111
columns of	x columns	0.250000
expression of the dual gap convergence criterion the	covariance dual gap	0.071429
estimates	cross val	0.038462
an 'l' suffix	utils	0.009709
faces in the wild lfw pairs dataset this	datasets fetch lfw pairs subset	0.035714
mutual	mutual	0.818182
plssvd	plssvd	1.000000
predict_log_proba on the estimator with the best	cv predict log proba x	0.500000
to check the test_size and train_size at	test_size train_size	0.200000
reduced likelihood function for the	process reduced likelihood function	0.047619
perform classification	neighbors nearest centroid predict	0.142857
precision matrix with the	base pca get precision	0.066667
an exception in an unfitted	estimators unfitted	0.142857
returns the submatrix corresponding to bicluster	core bicluster mixin get submatrix	0.333333
the best class label for each sample in	vs one classifier predict	0.500000
a given cache key	joblib cache key	0.250000
estimates	core cross	0.045455
set x y	x y sample_weight	0.038961
found parameters	model_selection base search	0.750000
fit the model with x	skewed chi2sampler fit x y	1.000000
fit ridge regression	ridge gcv fit x	1.000000
according	quadratic discriminant analysis	1.000000
initial parameters of	parameters	0.055556
coverage error measure compute how far	metrics coverage error	0.166667
fit on	core fit	0.333333
implement the usual api and hence	decomposition sparse coder fit x y	0.142857
the file supports seeking	joblib binary zlib file seekable	0.250000
laplace	laplace	1.000000
constructs a	function	0.021277
the laplacian	metrics laplacian	0.333333
path with coordinate descent	path	0.025641
delete all the content of the data home	clear data home	0.076923
loader	data_home download_if_missing	0.400000
versus all others	multiclass x	0.166667
"news" format strip lines beginning	strip newsgroup	0.090909
the paired distances between	metrics paired distances	0.500000
run fit on the estimator with randomly drawn	randomized search cv fit	0.500000
normalized laplacian	eigen_solver	0.090909
x y	x y sample_weight	0.064935
group out cross-validator provides train/test indices	group out	0.142857
sparse inverse covariance w/ cross-validated choice	cv	0.009009
apply dimensionality reduction to x using the model	decomposition factor analysis transform x	0.250000
loss is	loss	0.027027
matrix	sparse coef	0.071429
is not found and raise an	utils line search	0.029412
a covariance matrix shrunk on the diagonal read	covariance shrunk covariance	0.090909
dictionary factor in	dictionary	0.071429
neighbors within	lshforest radius neighbors	0.166667
read up to size	read size	1.000000
analysis fa a simple linear generative	analysis	0.045455
initialize the model parameters	mixture base mixture initialize parameters x	1.000000
true and false positives per binary	binary clf curve y_true y_score pos_label sample_weight	0.090909
shrunk covariance model according to the given training	covariance shrunk covariance fit x	0.083333
split data into	model_selection cviterable wrapper split x	0.250000
utility for building a cv in a	cv cv	0.031250
a random regression problem with sparse uncorrelated design	sparse uncorrelated n_samples n_features	0.166667
x as a	x y	0.002155
moore-penrose pseudo-inverse of a hermetian matrix	utils pinvh a cond rcond lower	0.200000
generate train test	core base shuffle split iter	0.166667
workaround python 2 limitations of	obj methodname	0.111111
non-negative matrix factorization nmf find	decomposition non negative factorization	0.043478
default backend used by parallel inside a with	externals joblib parallel backend backend	1.000000
predict raises an exception in an unfitted estimator	unfitted name estimator	0.142857
output of transform is sometimes referred to	transform y	0.023256
clustering	cluster	0.191489
return whether the file supports seeking	externals joblib binary zlib file seekable	0.250000
linear embedding analysis	linear embedding x n_neighbors	0.200000
deviance loss function for	deviance	0.125000
write array bytes to pickler file handle	numpy array wrapper write array array pickler	0.333333
dataset along any axis	x axis	0.030769
shuffle	shuffle	0.500000
found and raise an exception if	search	0.019231
precision is the	precision	0.016667
inplace column scaling of	inplace column scale	0.166667
updating terminal	function update terminal	0.200000
posterior probability of data	proba x	0.111111
fit the hierarchical clustering on the data parameters	cluster agglomerative clustering fit x y	0.250000
group	group	0.500000
zero row of x	x	0.001692
the logistic loss and gradient	logistic loss and grad w x y	0.500000
a large sparse linear system	lsqr a	0.037037
to what extent the local structure is retained	manifold trustworthiness x x_embedded n_neighbors precomputed	0.200000
multinomial loss	multinomial loss	1.000000
grid of points	grid	0.040000
number of splitting iterations in the	one out get n splits x	0.111111
function and its corresponding derivatives with respect	activations deltas	0.032258
array from the meta-information and the z-file	externals joblib zndarray wrapper read unpickler	0.043478
class with a	classifier	0.013699
returns the coefficient of determination r^2	multi output regressor score x y	0.200000
the bound	mixture bound	0.500000
function best possible score is 1	score y_true y_pred sample_weight multioutput	0.062500
w	w ht	0.250000
of the function called with the given arguments	func get	0.100000
matrix to dense array format	mixin densify	0.100000
on an array	check array array	0.250000
this score	score	0.010101
"news" format strip the headers by removing everything	datasets strip newsgroup	0.090909
ensure deterministic output from svd	svd	0.071429
parallel processing this method is meant to	parallel	0.019231
for multiclass	multiclass	0.076923
apply a mask	edges weights mask	0.333333
mean shift clustering using a flat kernel	mean shift	0.125000
jobs that can actually run in parallel	jobs	0.055556
inplace column scaling	utils inplace column scale x	0.166667
for	x y	0.002155
expression of the dual gap convergence criterion the	dual gap emp_cov	0.071429
fits the graphlasso covariance model to x	covariance graph lasso cv fit x	0.333333
type introduces an 'l' suffix when using the	repr	0.012500
build a batch of estimators within a job	parallel build estimators n_estimators ensemble x	0.166667
score	score y_true y_pred normalize	0.125000
with passive aggressive algorithm	linear_model passive aggressive regressor	0.250000
local	neighbors local outlier factor local	0.500000
gradient boosting for classification	gradient boosting classifier	0.333333
patches of any n-dimensional array	patches	0.055556
a size limit	externals	0.005747
mstep	mstep	1.000000
train	split	0.027778
locally linear embedding read	locally linear embedding	0.050000
note this implementation	metrics roc	0.040000
the isotonic regression model	isotonic regression y	0.066667
terminal regions	terminal regions tree	1.000000
a platform independent representation of	utils	0.009709
helper function for parameter value indexing	model_selection index param value x	0.200000
the given args and kwargs using	args kwargs	0.100000
fall back to line_search_wolfe2	wolfe12 f fprime xk pk	0.028571
"news" format strip the headers by removing	datasets strip newsgroup	0.090909
compute	score	0.020202
the precision is the ratio tp	precision	0.016667
x	estimator x	0.030303
we don't store the timestamp when pickling to	reduce	0.034483
function for factorizing common classes	partial fit first call clf classes	0.058824
and dispatch them	dispatch one	0.250000
do basic checks on matrix covariance sizes and	mixture validate covars covars covariance_type n_components	0.250000
we can also predict based	predict x	0.011765
the long type introduces an 'l'	repr	0.012500
squares ls estimation	squares error	0.500000
for each input data point	cross val predict estimator x	0.045455
em update for	dirichlet allocation em step x	0.500000
shrunk ledoit-wolf	ledoit wolf shrinkage x	0.250000
mean update	incremental mean	0.166667
import path as	resolv_alias win_characters	0.166667
a file object providing transparent gzip de compression	binary gzip file	0.500000
don't store the timestamp when pickling to	memorized func reduce	0.050000
to update terminal regions	error update terminal regions	0.500000
x y and	xy x y	1.000000
score on	score x y	0.030303
and	x y sample_weight random_state	0.166667
the hash depending from	memorized func	0.016949
for the voting classifier valid parameter	ensemble voting classifier set	0.037037
compute non-negative matrix factorization nmf	non negative factorization	0.043478
back the data to the original representation	standard scaler inverse transform	0.066667
scale back the data to the original representation	preprocessing standard scaler inverse transform x	0.066667
eval	eval	1.000000
fit	estimator fit x	0.200000
each	cross val predict	0.045455
do nothing and return the estimator	feature_extraction	0.037037
based on the percentiles of x	from x x percentiles grid_resolution	0.333333
step	step x x_old	1.000000
circles	circles	1.000000
pairs	pairs	0.388889
calculate the posterior log probability of the	nb joint log likelihood	0.066667
data precision matrix with the generative	get precision	0.052632
the long type introduces	utils shape repr	0.013699
maximum likelihood covariance estimator	covariance empirical covariance x assume_centered	0.166667
process	externals joblib multiprocessing backend	0.035714
downloading it if necessary	subset data_home download_if_missing random_state	0.166667
the gaussian mixture	mixture gaussian mixture	0.333333
model according to the given training data	x y sample_weight	0.025974
possible score is	score y_true y_pred	0.038462
the number of splitting iterations in the	one out get n splits	0.111111
randomized linear models for feature selection	randomized linear model	0.076923
that x	predict x	0.011765
compute the gradient	base multilayer perceptron compute	0.250000
corresponding to	mkdir	0.125000
the shrunk ledoit-wolf	ledoit wolf shrinkage	0.250000
boolean thresholding of array-like or scipy sparse	binarize x threshold copy	0.083333
the isotonic regression model : min sum	isotonic regression	0.055556
for fit	fit x y	0.005988
fit the model according to the given training	fit x y sample_weight	0.040000
score corresponds to the area under the	score y_true y_score	0.025000
dataset is constructed by taking	datasets make	0.015625
number of splitting iterations in the	cross validator get n splits x y	0.125000
sparse uncorrelated design this dataset is described in	datasets make sparse uncorrelated	0.166667
standardize a dataset along any axis center to	axis with_mean with_std	0.333333
em	dirichlet allocation em	1.000000
to compute log probabilities	ensemble parallel predict log proba	0.058824
the paired distances between	paired distances	0.500000
building a cv in	core check cv cv	0.031250
compute the decision function of	decision function x raw_values	0.083333
check x format and make sure no	allocation check non neg array	0.500000
predict class log-probabilities for	ensemble gradient boosting classifier predict log proba	0.500000
f-beta score is the weighted harmonic mean	metrics fbeta score y_true y_pred beta labels	0.333333
callable case for pairwise_{distances	pairwise callable x y metric	0.083333
is the	y_true	0.021739
parameter weights	weights dist weights	0.142857
a multinomial	multinomial	0.083333
and a name	get func name	0.047619
set the diagonal of the	set diag	0.333333
transforms and predict_log_proba of the final estimator parameters	core pipeline predict log proba x	0.500000
after the other and transforms	y	0.002674
the boolean mask x == missing_values	get mask x value_to_mask	0.333333
absolute sizes of training	translate train sizes	0.066667
split data into training and test set	shuffle split split x y groups	0.200000
a lower bound on model evidence	dpgmmbase lower bound	0.071429
a single tree	build trees tree forest x y	0.142857
used to fit an estimator within a job	parallel fit estimator estimator x y sample_weight	0.333333
sign of vectors for reproducibility flips	deterministic vector	0.076923
x relative to y_true	metrics predict scorer call estimator x y_true sample_weight	0.200000
generate	core cross val predict	0.045455
of the leaf	tree apply x	0.166667
returns the number of splitting iterations in the	model_selection cviterable wrapper get n splits x y	0.111111
finds the k-neighbors of	neighbors kneighbors mixin kneighbors x n_neighbors return_distance	0.250000
estimate sample weights by class for	compute sample	0.100000
the sparse components	sparse pca	0.500000
for	means covars	0.500000
to build a batch of estimators within	ensemble parallel build estimators n_estimators ensemble	0.166667
implementation is restricted to the binary classification task	y_true y_score	0.054054
return log-probability estimates	base nb predict log proba	0.500000
leaf	tree apply x	0.166667
sign of vectors for reproducibility flips the	deterministic vector	0.076923
predict class probabilities at each stage	boosting classifier staged predict proba	0.500000
number of splitting iterations in	group out get n splits x y	0.111111
n_redundant	n_redundant	1.000000
the graphlasso covariance model	covariance graph lasso cv	0.111111
the shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage x assume_centered	0.125000
introduces an 'l' suffix when using the	repr	0.012500
error regression loss read	log error	0.142857
number of splitting iterations in the	out get n splits x y groups	0.111111
non-negative matrix factorization nmf find two	decomposition non negative factorization	0.043478
indices to split data into	base kfold split	0.250000
check x format and	decomposition latent dirichlet allocation check	0.062500
pool	backend	0.016949
is restricted to the binary classification task	score y_true y_score average	0.076923
a lower bound on model evidence	mixture dpgmmbase lower bound	0.071429
the	base pca	0.071429
w to	x w ht l1_reg	0.250000
initialize the model parameters of	base mixture initialize	0.333333
fully connected graph	graph	0.021277
for the data samples in x	x y	0.002155
with joblib dump	externals joblib	0.004762
the dual gap convergence criterion the	dual gap emp_cov precision_	0.071429
x	exp sine squared call x	1.000000
call transform on the estimator with the best	cv transform x	0.500000
predict multi-output variable using a model	core multi output estimator predict x	0.166667
for validation and conversion of csgraph inputs	graph csgraph directed dtype csr_output	0.166667
best found	base search cv predict proba x	0.076923
the lfw people dataset this operation is meant	fetch lfw people data_folder_path slice_ color resize	0.333333
x	x doc_topic_distr	0.333333
by logistic regression and cv and linearsvc	svm fit liblinear x y c fit_intercept	0.142857
ward clustering based on a	cluster ward tree x connectivity n_clusters	0.250000
diagonal model	diag	0.031250
x y and scale if the scale parameter==true	xy x y scale	0.500000
and potentially other states at	neural_network base optimizer iteration ends time_step	0.142857
task	metrics precision recall curve	1.000000
isotropic gaussian	centers cluster_std	1.000000
lars	lars	0.454545
right fileobject from a	joblib read fileobject	0.100000
k-neighbors	neighbors kneighbors mixin kneighbors x	0.125000
persist an arbitrary python object into one	value filename compress protocol	0.250000
that will be extracted in an	i_h i_w p_h p_w	0.250000
a binary classifier on x and y	binary x y alpha c	0.500000
compute non-negative matrix factorization nmf find two	decomposition non negative factorization	0.043478
sample_weight	y sample_weight	0.017857
a tolerance which is independent of	cluster tolerance	0.058824
class for all meta	meta estimator	0.062500
compute the boolean mask	preprocessing get mask	0.333333
cv in a user	check cv cv x y classifier	0.031250
with the	get	0.012048
cache folders to	externals joblib memory	0.016949
dual_coef	dual_coef	1.000000
output for x relative to	metrics threshold scorer call clf x y	0.058824
itree which is equal to the average path	ensemble average path	0.142857
and scale the	y	0.002674
perform the covariance m step for full cases	mixture covar mstep full gmm x responsibilities weighted_x_sum	0.500000
computes the position of	mds fit x y init	0.066667
species distribution dataset from phillips et al 2006	fetch species distributions	1.000000
make cache size fit in	size	0.032258
csgraph	graph csgraph	0.250000
matrix whose range approximates the range	randomized range finder	0.083333
variance along an axix on a csr or	variance axis x axis	0.090909
helper to workaround python 2 limitations	helper obj methodname	0.333333
the number of splitting iterations in the	model_selection predefined split get n splits	0.111111
return the shortest path length	source shortest path length	0.333333
possible score is	score y_true y_pred sample_weight	0.062500
updates terminal regions to median	terminal region tree terminal_regions leaf x	0.066667
a byte string	externals joblib	0.004762
1d	1d y warn	1.000000
using the gaussian process regression model we	gaussian_process gaussian process regressor	0.058824
fit the model using x	factor fit x y	1.000000
the scaling of x according	scaler inverse transform x	0.026316
by a bell-shaped curve of width	effective_rank tail_strength	0.125000
label	preprocessing label	0.500000
list of feature	feature_extraction dict vectorizer get feature	0.200000
scaling of x according to feature_range	scaler inverse transform x	0.026316
the parameters of this estimator	params	0.028571
h whose product	h n_components	0.166667
described in [rouseeuw1984]_ aiming at computing	n_support remaining_iterations initial_estimates	0.111111
estimate the spherical wishart	bayesian gaussian mixture estimate wishart spherical	0.333333
x from y along the first axis we	x z reg	0.066667
for parallel	externals joblib parallel	0.014085
fit x	fit x	0.006410
error regression loss read more in the :ref	error y_true y_pred	0.125000
suffix	shape	0.011765
training set x y	fit x y	0.023952
number of splitting iterations in	group out get n splits x	0.111111
directory in which are persisted the result	dir	0.038462
classification metrics read more in	metrics classification	0.052632
convert a sparse matrix to a given format	utils ensure sparse format spmatrix accept_sparse dtype copy	1.000000
helper function for parameter value indexing	model_selection index param value x v indices	0.200000
similarity coefficient score the	score y_true y_pred normalize	0.125000
f-beta score is the weighted harmonic mean of	metrics fbeta score y_true y_pred beta labels	0.333333
weighted graph of neighbors for points	radius neighbors mixin radius neighbors graph	0.066667
of a memmap instance to reopen on	joblib reduce memmap a	0.050000
given text in	text	0.222222
perform standardization by centering and scaling parameters	standard scaler transform x y copy	0.333333
generate cross-validated estimates	cv	0.009009
from features or distance	y sample_weight	0.017857
to build a batch of estimators within	parallel build estimators n_estimators ensemble x y	0.166667
check x format	decomposition latent dirichlet allocation check	0.062500
back	scaler inverse transform x copy	0.066667
fit the model using x y as training	linear_model base randomized linear model fit x y	0.333333
probabilities for a calibration curve	calibration curve y_true	0.142857
compute the l1 distances between	metrics paired manhattan distances	0.083333
hopefully pretty robust repr equivalent	externals joblib safe repr value	1.000000
points in	core parameter	0.250000
and scaling	y	0.002674
this is the time	time	0.047619
generate	core	0.030769
descriptors of a memmap instance to	joblib reduce memmap a	0.050000
exception	joblib parallel backend	0.045455
in the :ref user guide	y_true y_pred	0.111111
utility for building a cv	core check cv cv x y	0.031250
input checker utility for building a cv	cv cv x y	0.031250
function cache	externals joblib memorized func	0.013158
indices to split data into	model_selection time series split split	0.250000
squared euclidean or frobenius norm of	squared norm	0.500000
to avoid the hash	joblib memory	0.016949
of the cholesky decomposition	det cholesky	0.166667
deterministic	deterministic	1.000000
inside a with block	externals joblib	0.004762
contains valid probabilities	probabilistic predictions	0.500000
the range	range	0.058824
class for regression loss	regression loss	1.000000
for a full lars path	omp path	0.100000
w h whose product approximates the non-	w h	0.031250
with the best found parameters	search cv predict	0.074074
data home	data home data_home	0.055556
generate	cross val predict estimator x y	0.045455
the number of splitting iterations in the	model_selection base cross validator get n splits x	0.125000
initialize the model parameters	base mixture initialize parameters	1.000000
fit label encoder	preprocessing label encoder fit transform	1.000000
update	step	0.125000
scale back the data to the	scaler inverse transform	0.058824
log sum_h exp(-e v	v	0.052632
python object into one	externals joblib dump value filename	0.083333
vectors individually to unit	axis copy	0.166667
matrix	mixin	0.037037
display	parallel print	0.142857
the long type introduces an 'l' suffix	repr	0.012500
a platform	utils	0.009709
return a tolerance which is	cluster tolerance	0.058824
this function returns posterior probabilities	cv predict proba x	0.034483
given param_grid	model_selection grid search cv get param iterator	0.166667
dictionary of all tokens in the raw documents	feature_extraction count vectorizer fit raw_documents	0.125000
prediction scores note this implementation is restricted to	metrics roc	0.040000
matrix factorization	factorization x	0.043478
compute the median	utils get median	0.166667
covariance m step for diagonal cases	covar mstep diag gmm x responsibilities weighted_x_sum	1.000000
average	average	0.466667
convert	sparse coef	0.071429
the local outlier factor of	neighbors local outlier factor decision	0.125000
given radius	radius	0.045455
unpickle	unpickle	1.000000
store the	func	0.011364
given estimator	estimator	0.014706
normalize x by scaling rows and columns	scale normalize x	0.142857
the curve auc using	auc	0.020408
weighted graph of neighbors for	neighbors graph	0.066667
the cache for the function	memorized	0.015873
calculate true and false positives per	clf curve y_true y_score pos_label	0.250000
returns distinct binary samples of length	datasets generate hypercube samples	0.333333
k x	call x	0.142857
prediction of init	gradient boosting init	0.142857
is inefficient to	classes	0.025641
apply	affinity	0.142857
compute the deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call y pred	0.333333
the logistic loss and gradient	logistic loss and grad	0.500000
from features or distance matrix	x y sample_weight	0.012987
for each input data	core	0.015385
check initial parameters of the	check parameters x	0.200000
a contingency matrix describing the relationship	cluster contingency matrix labels_true labels_pred eps	0.333333
samples from a	sample	0.032258
maximum likelihood estimator covariance model according	covariance empirical covariance fit x	0.166667
and evaluates the reduced likelihood function for the	process reduced likelihood function	0.047619
of x	x x	0.333333
the process or	multiprocessing backend	0.038462
columns of a	x columns	0.250000
partition	partition	0.600000
mcd from	n_support	0.166667
return precisions as a full matrix	mixture dpgmmbase get precisions	0.250000
kappa a	kappa	0.111111
coefficient matrix to dense array format	linear_model sparse coef mixin densify	0.100000
to build a	ensemble parallel build	0.047619
indices in sorted array	neighbors find matching indices tree bin_x left_mask right_mask	0.166667
of	utils shape	0.013699
search over parameters	base search cv fit x	0.166667
of exception	parallel backend base get	0.066667
from file-like object until size bytes are read	externals joblib read bytes fp size error_template	0.500000
return a platform independent representation of	shape	0.011765
dirichlet allocation with online variational bayes algorithm	dirichlet allocation	1.000000
as the maximizer of	gaussian_process gaussian process arg max	0.047619
of points	metric	0.071429
expression of the dual gap convergence criterion	dual gap	0.071429
the lfw people	fetch lfw people	0.040000
compute log probabilities of	log proba	0.090909
fit the ardregression model according to	linear_model ardregression fit	0.250000
huber	huber	0.750000
indices to split data in train/test sets	split	0.027778
mixin class for all meta	meta estimator mixin	0.250000
from it	externals joblib	0.009524
log of probability estimates	regression predict log proba	1.000000
of array-like or scipy	binarize	0.045455
vectors in x and y	x y sum_over_features size_threshold	0.250000
the residues	residues	0.125000
a random projection p only changes the	johnson lindenstrauss min dim n_samples eps	0.142857
on x and returns cluster	cluster dbscan fit predict x y	0.166667
function	function	0.468085
skip test if	skip	0.142857
decisions within a	estimators_features	0.071429
getter for the	covariance	0.028986
write a byte string to the file	externals joblib binary zlib file write	1.000000
training set according to	factor fit predict	0.066667
matrices w h	w h	0.031250
project data to vectors and cluster the result	biclustering project and cluster data vectors	0.333333
compute the log-det of the cholesky decomposition of	mixture compute log det cholesky matrix_chol	0.500000
estimators that implement the partial_fit api need	utils check partial fit	0.038462
calculate true and false positives per binary classification	metrics binary clf curve	0.090909
false positives per	clf curve y_true y_score pos_label	0.250000
transform	fit transform	0.100000
getter for	covariance	0.028986
find the first prime element in	hungarian state find prime in	0.333333
fit a multi-class classifier by combining binary	base sgdclassifier fit	0.076923
returns the number of splitting iterations in the	pgroups out get n splits	0.111111
generate cross-validated estimates for each	cv	0.009009
update h in multiplicative update	decomposition multiplicative update h x w h beta_loss	0.250000
outlier on	outlier factor	0.166667
clustering on	dbscan fit predict	0.333333
lfw people dataset this operation is meant	fetch lfw people data_folder_path slice_ color resize	0.333333
estimate the precisions	mixture bayesian gaussian mixture estimate precisions nk xk	0.166667
pickling reduction for	externals joblib reduce	0.333333
of jobs for the computation	jobs n_jobs	0.100000
compute decisions within a job	estimators_features	0.071429
a contingency matrix describing the	contingency matrix labels_true labels_pred eps sparse	0.166667
gaussian process model fitting method	gaussian_process gaussian process fit x	0.250000
not found and	utils line search	0.029412
back the data to the original	inverse transform x copy	0.066667
msg	msg	1.000000
fit	learning fit	1.000000
coefficient of determination regression score	r2 score y_true y_pred	0.125000
in multiplicative	decomposition multiplicative	1.000000
cache folders to	externals joblib memory reduce	0.030303
single binary estimator	binary estimator x	0.363636
compute data precision matrix with	precision	0.016667
returns the number of splitting iterations in	out get n splits x y	0.111111
call wrapped function cache result and	externals joblib memorized func call	0.200000
suitable step length is not found and	utils line search	0.029412
catch and hide warnings	ignore warnings	0.166667
and return encoded labels parameters	transform y	0.023256
or thread	externals joblib multiprocessing	0.052632
update the	update dict	0.333333
types to	base	0.014286
bicluster	core bicluster mixin	0.500000
compute out-of-bag scores	ensemble forest regressor set oob score x	1.000000
the hierarchical clustering on the	cluster feature agglomeration	0.125000
fit the model	tsne fit	0.333333
filters the given args and kwargs using a	joblib filter args func ignore_lst args kwargs	0.333333
norm vector length	norm	0.125000
avoid	joblib memory	0.016949
check x format check x	allocation check	0.062500
loader for the california housing dataset	datasets fetch california housing data_home download_if_missing	0.250000
for	val predict estimator x	0.045455
hash depending from it	func	0.011364
class for all meta estimators in	meta estimator	0.062500
on the grid	model_selection parameter grid	0.250000
train estimator on training	fit estimator estimator x	0.111111
estimates for each	core cross	0.045455
best class label for each sample in x	core one vs one classifier predict x	1.000000
returns a list of edges for	edges	0.047619
of the derived class	resp	0.090909
train estimator on training subsets incrementally and	core incremental fit estimator estimator x y	0.200000
element in the specified	in	0.090909
lasso linear model with iterative fitting along a	lasso cv	0.333333
for the voting classifier valid parameter keys can	voting classifier set params	0.037037
computation of max absolute value	preprocessing max abs	0.050000
fits the maximum likelihood estimator covariance model according	covariance empirical covariance fit	0.166667
prediction of init	init decision	0.142857
compute mutual information between two variables	feature_selection compute mi x y x_discrete	1.000000
for reducing the	items root_path	0.066667
for the	covariance empirical	0.100000
returns the number of splitting iterations in the	get n splits x	0.111111
sample weights	utils compute sample	0.100000
checker utility for building a cv in a	cv cv x y classifier	0.031250
long type introduces	utils	0.009709
for each input data point	predict estimator	0.045455
number of splitting iterations in the cross-validator	base kfold get n splits x y groups	0.111111
estimates for each input data	x	0.001692
the curve auc from prediction scores note this	roc auc score	0.166667
repr	safe repr	1.000000
lfw people dataset this operation	lfw people	0.040000
for reproducibility flips the	utils deterministic vector	0.076923
the cache for the	externals joblib memorized func	0.013158
platform	shape repr	0.013699
the training set x	fit x	0.025641
the usual api and	y	0.008021
from it	joblib memorized func	0.014706
a memmap instance to reopen	reduce memmap a	0.050000
diag	diag	0.156250
generate a	n_samples n_components n_features n_nonzero_coefs	0.500000
huber loss and the gradient	huber loss and gradient	0.333333
computes the maximum likelihood covariance estimator	covariance empirical covariance x	0.166667
generate a random multilabel classification	multilabel classification n_samples n_features	0.500000
path length from source to all reachable nodes	path length graph source	0.200000
process of the parallel execution only	externals joblib parallel	0.014085
boost using the	boost classifier boost	0.100000
data point	core cross val	0.043478
local outlier factor of x (as	local outlier factor decision function x	0.100000
back the data to the original representation parameters	preprocessing standard scaler inverse transform x	0.066667
matrix	cluster	0.021277
data to vectors	data vectors	1.000000
prediction of init	base gradient boosting init	0.142857
the rfe model and	y	0.002674
implement a single boost	ensemble base weight boosting boost iboost x y	1.000000
laplacian matrix and convert it	laplacian	0.034483
extract the coefficients and intercepts from	neural_network base multilayer perceptron unpack	0.250000
area under the curve auc using the	metrics auc	0.040000
types	joblib parallel backend	0.045455
the kernel	kernel call	0.333333
class for unbalanced datasets	weight class_weight	0.200000
of the data home	clear data home data_home	0.076923
log probabilities within a job	predict log proba estimators estimators_features x n_classes	0.250000
recall is the ratio tp	metrics recall	0.033333
bernoulli restricted boltzmann machine rbm	bernoulli rbm	1.000000
thresholding of array-like or scipy sparse matrix	binarize x threshold	0.083333
on x and returns cluster labels	cluster dbscan fit predict x y sample_weight	0.166667
folders	externals joblib memory	0.016949
initial parameters of the derived	parameters x	0.125000
coverage error measure compute how	coverage error y_true y_score	0.166667
reconfigure the backend	backend base configure	0.500000
tolerance which is independent	tolerance x	0.058824
and component wise scale	preprocessing robust scale	0.125000
spectral	spectral	0.236842
from prediction scores note this	roc	0.033333
hence	decomposition sparse coder fit x	1.000000
func to be	joblib parallel backend base apply async func	0.250000
fit ridge regression model parameters	linear_model base ridge cv fit x y	1.000000
update the dense	update	0.035714
the sample weight array	linear_model base sgd validate sample weight sample_weight n_samples	0.333333
for parallel processing this method is	parallel	0.019231
of the unpickler to unpickle our numpy pickles	numpy unpickler	0.333333
a contingency matrix describing	contingency matrix	0.166667
objective for the case method='lasso' is	xy gram	0.090909
angle regression or lasso path using lars algorithm	linear_model lars path x	0.100000
estimates	estimator x y	0.038462
to the binary classification	y_true y_score pos_label sample_weight	0.066667
factor analysis fa a simple linear	factor analysis	0.166667
list of exception types	externals joblib parallel	0.014085
implements a conditional property using the	iff has attr descriptor	0.083333
the bound	mixture bound state log lik	0.500000
unsupervised outlier detection	one class svm	1.000000
search over	core base search cv fit	0.166667
data to vectors	data vectors n_clusters	1.000000
the estimator and set the base_estimator_ attribute	ensemble bagging classifier validate estimator	0.333333
input data point	val	0.037037
hash depending from	memorized func	0.016949
meta-information and the z-file	joblib zndarray wrapper read unpickler	0.043478
row-wise	utils row norms	1.000000
partially fit underlying estimators should be used when	one vs one classifier partial fit x y	0.166667
used to compute log probabilities within a	ensemble parallel predict log proba	0.058824
to partition	partition	0.100000
on the estimator with the best found	base search cv predict	0.076923
cross-validated estimates for	predict estimator x y cv	0.071429
samples in	svm	0.142857
shortest path length from source to	source shortest path length graph source cutoff	0.111111
the model to the training set x and	x y	0.002155
get the weights from an array of	get	0.012048
the means	means x	0.250000
'l' suffix	utils shape repr	0.013699
the logistic loss and gradient	logistic loss and grad w x	0.500000
check initial parameters	mixture check parameters	0.166667
the weighted graph of neighbors for points in	radius neighbors mixin radius neighbors graph	0.066667
l1 distances between	metrics manhattan distances	0.083333
bytes_limit	memory reduce	0.030303
cross-validated estimates	x y cv	0.050000
model parameters	fit	0.006515
matrix	matrix labels_true labels_pred	1.000000
predict_proba on the estimator with the best found	base search cv predict proba x	0.076923
length from source to all	length graph source cutoff	0.200000
compute area under the curve auc	auc	0.040816
long type	utils shape	0.013699
this dataset is described in celeux	datasets	0.015152
cohen's kappa	cohen kappa	0.250000
transform function to portion of selected features	x transform selected copy	0.333333
position	mds fit x	0.066667
results from clf predict calls	probas	0.166667
fit the model	svm linear svc fit	0.333333
the logistic loss and gradient	linear_model logistic loss and grad	0.500000
em update for 1 iteration	dirichlet allocation em step x total_samples batch_update parallel	1.000000
for the california	california	0.125000
from the c and cpp files	from c and cpp files	1.000000
finds the neighbors within a given radius	radius neighbors x radius return_distance	0.500000
the parameters for the voting classifier valid parameter	voting classifier set	0.037037
the decision function of the	decision function x	0.018868
to a projection to the normalized	spectral	0.026316
for creating a class with	add	0.071429
the data home	data home data_home	0.055556
clear all covered	utils hungarian state clear	1.000000
back the	inverse transform x	0.051282
predefined split cross validation iterator	predefined split	1.000000
or lasso path using lars algorithm [1]	linear_model lars path	0.100000
a memmap instance to reopen on same file	joblib reduce memmap a	0.050000
log-likelihood of a gaussian	score	0.010101
for full	full x means covars min_covar	0.166667
iterate over	feature_selection iterate	1.000000
generate	shuffle split iter	0.166667
test_size	test_size	1.000000
using the gaussian process regression model we can	gaussian_process gaussian process regressor	0.058824
the given args and kwargs using a	args kwargs	0.100000
a score by cross-validation	core cross val score estimator x y	0.333333
private helper function for factorizing common	first call clf	0.200000
for each	core cross val predict estimator x	0.045455
download the	datasets download	0.500000
seeds for	get bin seeds	0.250000
"friedman \#3" regression problem this dataset is	datasets	0.015152
sets of biclusters	metrics cluster consensus	0.250000
number of splitting iterations in the	pgroups out get n splits x	0.111111
estimates the shrunk ledoit-wolf	ledoit wolf shrinkage x assume_centered block_size	0.250000
samples can be different from the	core calibrated classifier cv	0.111111
split data into training and test set	time series split split x y groups	0.200000
gaussian process model fitting	gaussian_process gaussian process fit	0.250000
the number of splitting iterations in the cross-validator	split get n splits x y groups	0.111111
a dataset of shape (n_samples n_features)	decomposition infer	0.200000
seeds	cluster get bin seeds	0.250000
using matrix product with the random matrix	random projection	0.125000
x and returns the	x y	0.002155
reproducibility flips the sign of	utils deterministic vector sign flip	0.066667
theta as the maximizer of the	arg max	0.047619
compute non-negative matrix factorization nmf find	negative factorization	0.043478
fit the model by computing	decomposition pca fit	0.125000
decision function of x	gradient boosting classifier decision function x	0.333333
class representing an arbitrary	not memorized result	0.500000
loader for the california housing dataset from	fetch california housing data_home download_if_missing	0.250000
in x into a matrix of patch data	patch extractor transform x	0.500000
minimum distances between one point and	pairwise distances argmin x y	0.333333
neighbors for points in	neighbors radius neighbors mixin radius neighbors	0.125000
two clusterings of	score labels_true labels_pred sparse	0.047619
w[i] (y[i] - y_[i]) ** 2	y sample_weight y_min y_max	0.166667
to the binary classification	score y_true y_score	0.025000
parameters that would be indth in iteration	core parameter grid getitem ind	0.333333
inplace row scaling	utils inplace row scale x scale	0.142857
the precisions	precisions	0.066667
finds indices in sorted array	find matching indices tree bin_x left_mask right_mask	0.166667
jaccard similarity coefficient score the	score y_true	0.058824
indices increasingly apart the distance	verbosity filter index	0.055556
the neighbors within	neighbors x	0.166667
voting classifier valid parameter	voting classifier set params	0.037037
collection of text documents to a matrix	vectorizer	0.022222
initialize the model parameters of the derived class	base mixture initialize x resp	0.500000
a random projection p only changes	core johnson lindenstrauss min dim n_samples	0.142857
curve auc using	auc	0.020408
of a csc/csr matrix in-place	utils inplace swap column x m n	0.250000
of the dual gap convergence criterion the specific	dual gap	0.071429
helper	parallel helper	0.500000
and y	y sum_over_features	1.000000
estimator with the best	cv	0.045045
function of the given	function x	0.030303
an array shape under python 2 the long	utils shape repr shape	0.166667
wild lfw pairs dataset this	datasets fetch lfw pairs subset	0.035714
task	task	1.000000
dictionary factor in	dict dictionary	0.111111
the backend and return the number	externals joblib parallel backend base	0.034483
learn vocabulary and idf	raw_documents y	0.250000
the lfw people dataset this	fetch lfw people	0.040000
weighted graph of neighbors	neighbors graph	0.066667
back the data to the original	scaler inverse transform x	0.052632
finds seeds	get bin seeds	0.250000
signature from the	signature	0.047619
subset of dataset and	x y	0.002155
private function used to compute log probabilities	ensemble parallel predict log proba	0.058824
each input data point	val predict estimator x	0.045455
load sample images for image manipulation	load sample images	0.250000
implements a conditional property using	iff has attr descriptor	0.083333
multinomial deviance loss function	multinomial deviance	0.250000
two non-negative matrices w h whose	w h n_components	0.038462
is the depth	check previous func code	0.333333
stochastic gradient descent optimizer parameters	optimizer	0.142857
factorization nmf find two non-negative matrices w h	non negative factorization x w h n_components	1.000000
coverage error measure compute how	metrics coverage error y_true y_score	0.166667
the wild lfw pairs	datasets fetch lfw pairs subset	0.035714
probability calibration with sigmoid method platt 2000	sigmoid calibration df y	0.500000
generative	decomposition base pca	0.071429
is not found and raise an exception if	utils line search	0.029412
reduced likelihood function for the given autocorrelation parameters	process reduced likelihood function	0.047619
undo the scaling	preprocessing min max scaler	0.200000
transform feature->value dicts to array or	feature_extraction dict vectorizer transform	0.200000
a memmap instance to reopen	externals joblib reduce memmap a	0.050000
cross-validated estimates for each input	core cross val predict estimator x y cv	0.071429
parallel	joblib parallel	0.057143
matrix	k k_skip eigen_solver	1.000000
learn the vocabulary dictionary and return term-document matrix	fit transform raw_documents y	0.100000
given radius of a point or points	x radius	0.058824
apply clustering to	cluster spectral clustering affinity n_clusters n_components	0.166667
the isotonic regression model : min sum	core isotonic regression	0.055556
loads data from module_path/data/data_file_name	data module_path data_file_name	0.500000
weighted graph of neighbors	neighbors mixin radius neighbors graph	0.066667
evaluate the density model on the	density score samples	0.250000
list of exception types to	joblib parallel	0.028571
probabilities	proba	0.264706
it	memory	0.015625
compute	multilayer perceptron compute	0.250000
which is equal to the average path	ensemble average path	0.142857
given dataset split	x y scorer	0.111111
iris	iris	0.666667
mldata org data set	mldata dataname target_name data_name transpose_data	0.500000
tolerance which is independent of the dataset	tolerance	0.045455
process	externals	0.005747
returns the index of the leaf	decision tree apply	0.166667
error	error y_true y_score	1.000000
or thread	externals joblib	0.004762
write array bytes to	array wrapper write array array	0.500000
count and	count x y	0.250000
fit a single binary estimator	core fit binary estimator x	1.000000
absolute error regression	absolute error y_true y_pred	0.142857
theta	theta	0.538462
the voting classifier valid	voting classifier set	0.037037
true and false positives per binary classification threshold	metrics binary clf curve	0.090909
lfw pairs dataset this	lfw pairs subset	0.035714
the kernel k	gaussian_process compound kernel	0.333333
number of splitting iterations in	cross validator get n splits x	0.125000
of edges	make edges	0.066667
x relative	scorer call estimator x	0.166667
of a csc/csr matrix in-place	utils inplace swap row x m n	0.250000
pairwise matrix	metrics parallel pairwise x y	0.166667
filters	externals joblib filter args func ignore_lst	0.500000
check initial parameters of the derived	base mixture check parameters x	0.200000
lower bound on	mixture dpgmmbase lower bound	0.071429
ndarray with aligned	aligned	0.076923
returns the huber loss and the gradient	linear_model huber loss and gradient	0.333333
train estimator	estimator estimator	0.105263
perform classification on test	core	0.015385
this is a general function given points on	y reorder	0.111111
it take	squeeze	0.166667
the voting classifier valid parameter keys can be	voting classifier	0.035714
as target values parameters	neighbors supervised float mixin	0.500000
predict class probabilities for	ensemble ada boost classifier predict proba	1.000000
median of data	median data	0.333333
transform data back to its original space	decomposition randomized pca inverse transform x y	1.000000
load text files	datasets load files	0.500000
sample from the decision boundary for each class	one vs rest classifier decision function x	0.250000
the	memorized func	0.033898
reproducibility flips the sign of elements	utils deterministic vector sign	0.066667
pursuit model	pursuit	0.181818
elastic net optimization function varies	l1_ratio	0.030303
data point	predict estimator x y	0.045455
of exception	get	0.012048
whether the file supports seeking	file seekable	0.250000
classification problem	datasets make classification	0.500000
lower bound on model evidence	lower bound	0.071429
the variational distributions for the	mixture dpgmmbase	0.166667
x as a	x y iter_offset	0.333333
a job	estimators estimators_features	0.333333
classifier using ridge regression	ridge classifier	0.333333
whether the file	joblib binary zlib file	0.133333
the timestamp when pickling to	externals joblib memory reduce	0.030303
function used to compute log probabilities within a	predict log proba	0.029412
param logic estimators that	utils check	0.023810
the range	utils randomized range finder	0.083333
fit all transformers transform the data	union fit transform	0.333333
fit the calibrated model parameters	core calibrated classifier cv fit x y	1.000000
of the	base	0.014286
sign of elements of all the	sign	0.050000
a large sparse linear system	utils lsqr a	0.037037
in	externals joblib memory	0.016949
the optimal batch	externals joblib auto batching mixin compute batch	0.333333
remove cache	externals joblib	0.004762
used by parallel inside a with	externals joblib parallel	0.014085
sample weights by class for unbalanced datasets	utils compute sample weight class_weight	0.500000
be used when memory is inefficient to	x y classes	0.027778
multi-class targets using	core output code	0.250000
outlier on the training set according to	outlier factor fit predict	0.200000
length is not found and raise	search	0.019231
the posterior log probability of the	multinomial nb joint log likelihood	0.083333
reconstruct the array from the meta-information and	externals joblib zndarray wrapper read unpickler	0.043478
used to build a batch of estimators within	ensemble parallel build estimators	0.166667
augment dataset with an additional dummy feature	preprocessing add dummy feature x	1.000000
neighbors within a given radius of	lshforest radius neighbors x radius	0.142857
x to unit norm parameters	preprocessing normalizer transform x	0.250000
elastic net path with coordinate descent	path	0.025641
evaluate a score by cross-validation read more in	cross val score estimator x y groups	0.166667
generate isotropic gaussian blobs for	make blobs n_samples n_features centers cluster_std	0.333333
lfw pairs dataset this operation is meant to	fetch lfw pairs index_file_path data_folder_path slice_ color	0.333333
estimate sample weights	sample	0.032258
fit linear model	sgdclassifier fit x	0.333333
compute area under the curve auc from prediction	auc	0.020408
fit the model	linear svc fit	0.333333
explained	metrics explained	1.000000
placeholder for fit subclasses should implement this method!	decomposition base pca fit x	0.333333
mask to edges weighted or not	feature_extraction mask edges weights mask edges weights	0.166667
make and configure a copy	ensemble base ensemble make estimator append random_state	0.166667
generate	datasets make	0.031250
and a set of	y	0.002674
thresholding of array-like or scipy	preprocessing binarize x threshold copy	0.083333
such that for c in (l1_min_c infinity) the	c	0.022222
force	memorized	0.015873
variance along an axix on	variance axis x axis	0.090909
types to be captured	get exceptions	0.166667
parameters theta as the maximizer of the reduced	process arg max reduced	0.200000
names and a name	name	0.033333
find the null space	manifold null space	0.333333
absolute sizes of training subsets and validate 'train_sizes'	train sizes train_sizes n_max_training_samples	0.500000
we don't store the timestamp when pickling	externals joblib memory reduce	0.030303
matrix to	sparse	0.025000
compute non-negative matrix factorization	factorization	0.035714
fit all transformers transform the data	feature union fit transform x	0.333333
perform the covariance m step for diagonal cases	mixture covar mstep diag gmm x responsibilities weighted_x_sum	0.500000
generate cross-validated estimates for each	estimator x y cv	0.050000
the number of splitting iterations in the	model_selection base kfold get n splits x y	0.111111
return the kernel k x y and	call x y	0.142857
validate x whenever one tries to predict apply	base decision tree validate x predict x	0.500000
to compute decisions within	estimators_features	0.071429
feature_range	feature_range	1.000000
principal components analysis	pca	0.047619
the maximizer of the reduced likelihood function	gaussian_process gaussian process arg max reduced likelihood function	0.333333
predict multi-class targets using underlying estimators	output code classifier predict x	0.250000
x with	x	0.001692
buffered version of a read file object	buffered read file fobj	0.500000
update nmf	update	0.035714
cache folders	externals joblib memory reduce	0.030303
hash depending from	joblib memorized func	0.014706
svmlight / libsvm format into sparse csr matrix	svmlight file f n_features dtype multilabel	0.066667
first prime element in the specified	prime in	0.166667
returns the score on the	score	0.010101
a batch of estimators within	estimators n_estimators	0.083333
exception	backend base get	0.066667
binary metric for multilabel classification parameters	binary score binary_metric y_true	0.500000
return the score for a fit	fit rfe	0.166667
exception types to	externals joblib parallel backend base	0.034483
to build a batch of estimators within a	parallel build estimators n_estimators	0.166667
whether the file	zlib file	0.153846
hessian in the case of a multinomial loss	linear_model multinomial grad hess w x	1.000000
apply clustering to a	clustering	0.050000
the k-neighbors of	neighbors kneighbors mixin kneighbors	0.100000
the voting classifier valid	ensemble voting classifier set params	0.037037
back the data to	robust scaler inverse transform x	0.066667
input checker utility for building a cv in	core check cv cv x y	0.031250
for kernels	kernel	0.015625
used to build a	build	0.037037
self	self	1.000000
of feature name -> indices mappings	dict vectorizer	0.250000
nmf find two non-negative matrices w	w	0.035714
each input	val	0.037037
for random matrix generation	input size n_components n_features	0.200000
one class versus all others	multiclass x y alpha	0.166667
compute the decision function	decision function x raw_values	0.083333
as training	copy_x	0.125000
false positives per	clf curve y_true y_score	0.250000
largest k singular values/vectors for a	a k ncv tol	0.166667
y -	y	0.002674
with likelihood terms for standard covariance types	initial_bound precs means	1.000000
the descriptors of a	a	0.018182
multitaskelasticnet	multi task elastic net	0.250000
finds indices in sorted array of	matching indices tree bin_x left_mask right_mask	0.166667
such that for c in (l1_min_c infinity)	c x y loss fit_intercept	0.030303
matrix for x using	x y	0.002155
get the weights from an array	get	0.012048
undo the scaling	min max scaler	0.083333
utility function opening the right fileobject from	joblib read fileobject fileobj	0.100000
error between two covariance	covariance error	0.500000
in pipeline after transforms	pipeline	0.083333
a large sparse linear	a	0.018182
the lfw pairs	datasets fetch lfw pairs	0.018868
of an array shape under python 2	repr shape	0.166667
exponential chi-squared kernel x and y	metrics chi2 kernel x y	0.333333
the long type	utils	0.009709
vectors for reproducibility flips the sign	utils deterministic vector sign	0.066667
list of	joblib	0.007299
on the training set according to the	factor fit predict	0.066667
fit across one fold	feature_selection rfe single fit	0.200000
a list of feature names ordered by their	dict vectorizer get feature names	0.142857
generate a sparse random projection matrix parameters	core base random projection	0.200000
the descriptors of a memmap instance to	joblib reduce memmap a	0.050000
the score on the given	score	0.010101
a tolerance which is independent	tolerance x	0.058824
returns the number of splitting iterations in	split get n splits x y groups	0.111111
gaussian and label samples by quantile this classification	gaussian	0.029412
metrics should use	metrics	0.043478
in bytes_limit	joblib memory reduce	0.030303
given radius of a	x radius	0.058824
union	union	0.625000
each	val predict estimator x	0.045455
features by scaling each feature to a	min max scaler	0.083333
strip the headers	datasets strip	0.076923
to the cache for the function	externals joblib memorized	0.013699
each input	estimator	0.014706
nn	nn	0.833333
check that	check	0.053571
matrix to evaluate the accuracy of a classification	y_true	0.021739
a sparse random matrix	utils random choice csc	0.333333
to check the test_size and train_size at init	init test_size train_size	0.250000
the wild lfw pairs dataset this dataset	datasets fetch lfw pairs	0.018868
scores this score corresponds to the	score y_true y_score	0.025000
calculate mean update and a	utils incremental mean	0.250000
configure	estimator append	0.142857
utility for building a cv in	check cv cv	0.031250
given type in	type	0.125000
w to	w ht l1_reg	0.250000
thread pool	multiprocessing	0.045455
be used for scaling	scaler fit x	0.076923
a size	externals joblib	0.004762
in x and y	x y sum_over_features size_threshold	0.250000
a random sample from	size	0.032258
for c such that for c in (l1_min_c	c x y loss	0.030303
mutual information between two clusterings	metrics cluster mutual info score labels_true labels_pred	1.000000
standardize a dataset along any axis center to	x axis with_centering with_scaling	0.333333
and evaluates the reduced	gaussian process reduced	0.125000
the number of splitting iterations in the	split get n splits	0.111111
squared loss for	squared loss y_true	1.000000
number of splitting iterations in the cross-validator	model_selection predefined split get n splits x y	0.111111
methods a parallelbackend must implement	parallel backend base	0.037037
in x and transform x	transform x y	0.062500
on	core	0.030769
for different probability thresholds note this implementation is	probas_pred pos_label sample_weight	0.066667
scale back	inverse transform	0.062500
updating terminal	loss function update terminal	0.200000
the kernel is	kernel mixin is	0.333333
true and predicted probabilities for a calibration curve	core calibration curve y_true y_prob normalize	0.142857
for reproducibility flips the sign of elements of	utils deterministic vector sign	0.066667
log probabilities within a job	parallel predict log proba estimators estimators_features x n_classes	0.250000
the position of the points	mds fit	0.066667
vectors	vectors n_clusters	1.000000
count and smooth feature	multinomial nb count x y	0.250000
the vectors in x and y	x y sum_over_features	0.250000
list of	base get	0.066667
a buffered version of a read file object	buffered read file fobj	0.500000
to split data into	model_selection time series split split x	0.250000
coefficient of determination regression score function	r2 score y_true y_pred sample_weight	0.125000
of vectors for reproducibility flips the	deterministic vector	0.076923
each input	cross val predict estimator x y	0.045455
data point	cross val predict	0.045455
in the wild lfw pairs dataset this	lfw pairs	0.018868
n_jobs	n_jobs	0.186047
the feature importances	base decision tree feature importances	0.333333
anova f-value for the provided sample	feature_selection f classif x	0.200000
clustering on x and returns cluster labels	cluster dbscan fit predict x y	0.166667
sometimes referred to by some authors as	preprocessing label binarizer	0.071429
generate a cartesian product of input arrays	utils cartesian arrays	1.000000
bound for c such that for c	c	0.022222
kernel between x and y : k	kernel	0.031250
process	externals joblib	0.004762
private helper function for factorizing common classes param	fit first call clf classes	0.058824
helper to workaround python 2 limitations of pickling	parallel helper obj methodname	0.333333
generate	split	0.027778
of np dot x	dot x	0.500000
log of the	log	0.018868
of x for later	fit x	0.006410
of the gradient	ensemble base gradient	0.333333
diagonal of the kernel k x x	kernel mixin diag x	1.000000
two non-negative matrices w h	w h	0.031250
given arguments	joblib memorized func	0.014706
normalize x by scaling rows	normalize x	0.076923
which	backend	0.016949
configure a copy of the base_estimator_	append	0.083333
bytes_limit	bytes_limit	1.000000
skip test if	check skip	0.500000
score	score y_true y_pred sample_weight multioutput	0.062500
parameters and raise valueerror if not	ensemble base gradient boosting	0.111111
predict_log_proba of	predict log proba	0.029412
thresholding of array-like or scipy sparse matrix	preprocessing binarize	0.083333
check that the parameters are well defined	bayesian gaussian mixture check parameters	1.000000
depth a	externals joblib memorized func check previous func code	0.055556
the decision	gradient boosting classifier decision	0.333333
a tolerance which is independent of the	tolerance x tol	0.058824
decision function output for x relative to y_true	metrics threshold scorer call clf x y sample_weight	0.058824
input validation	utils check x y x y accept_sparse dtype	0.250000
the file descriptor for the underlying file	binary zlib file fileno	0.333333
abort	externals joblib parallel backend base abort	1.000000
sure centering is not enabled	robust scaler check array x	0.250000
determinant with the fastmcd algorithm	cov det	0.200000
transforms features by scaling each feature	minmax scale x feature_range axis copy	0.200000
of estimators within	estimators n_estimators ensemble x y	0.083333
exception types to be captured	exceptions	0.083333
"friedman \#3" regression problem this dataset is	datasets make	0.015625
can be different from	calibrated classifier	0.083333
the maximum absolute value	max abs	0.047619
weighted graph of neighbors	neighbors radius neighbors graph	0.066667
what extent the local structure is retained	x x_embedded n_neighbors precomputed	0.200000
min_samples	min_samples	1.000000
n	n	0.450000
took	completed batch_size duration	0.333333
factorize density check according	core check density density n_features	0.166667
by logistic regression and cv and	x y	0.002155
folder	folder	1.000000
predict the target	predict	0.006849
compute joint probabilities	manifold joint probabilities	1.000000
along any axis center to the mean and	x axis	0.015385
pairs dataset this dataset is a collection	pairs	0.055556
going up	even	0.166667
axis center to the mean	x axis	0.015385
svmlight / libsvm format	svmlight file f n_features dtype multilabel	0.066667
data precision matrix with	get precision	0.052632
with stochastic gradient descent	base sgdregressor partial	0.333333
for diagonal cases	mstep diag gmm x responsibilities weighted_x_sum	0.250000
retrieve a	externals	0.005747
matrix product with the random matrix parameters	core base random projection	0.200000
function opening the right fileobject from a filename	joblib read fileobject fileobj filename mmap_mode	0.250000
customized copy	replace parameters return_annotation	1.000000
number of splitting iterations in the cross-validator parameters	leave one group out get n splits	0.111111
replace=true p=none) generates a random sample from	size replace	0.125000
error regression loss read more	log error	0.142857
kddcup99 dataset downloading it	fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
returns the score on the given data if	score x y	0.030303
computes the graph	graph	0.021277
makes sure centering is not enabled for sparse	robust scaler check array x copy	0.333333
the em	mixture gmmbase	0.034483
print verbose message on the	mixture print verbose msg init	0.333333
the given arguments	externals joblib memorized func get output	0.125000
introduces	utils	0.009709
splits for an arbitrary randomized cv	splits	0.083333
names and a name	func name	0.047619
based on a	tree x connectivity n_clusters	0.250000
graph	grid to graph	0.333333
predict class probabilities at each stage for x	ensemble gradient boosting classifier staged predict proba x	1.000000
with passive aggressive algorithm	linear_model passive aggressive classifier	0.250000
array-like or scipy sparse	preprocessing binarize x threshold	0.083333
x as a	fit x y iter_offset	0.333333
the k-neighbors	neighbors kneighbors mixin kneighbors	0.100000
of neighbors	radius neighbors	0.086957
get number of jobs for the computation	utils get n jobs n_jobs	0.250000
for	ensemble ada boost classifier	0.400000
the number of jobs which	backend effective n jobs n_jobs	0.333333
for _fit_coordinate_descent update	update	0.035714
the maximizer of	gaussian_process gaussian process arg max	0.047619
a cv in a user friendly way	check cv cv x y classifier	0.031250
maximum	max	0.214286
a random sample from a given 1-d array	utils choice a size replace	0.250000
of bag predictions and score	ensemble base forest set oob score x y	0.250000
to what extent the local structure is retained	x x_embedded n_neighbors precomputed	0.200000
number of splitting iterations in the	model_selection cviterable wrapper get n splits x	0.111111
updates terminal regions to	terminal region tree terminal_regions leaf x	0.066667
the wild lfw pairs dataset this	lfw pairs	0.018868
number of splitting iterations in the	split get n splits	0.111111
home	home	0.857143
uncorrelated	uncorrelated	0.833333
the neighbors within	neighbors lshforest radius neighbors	0.166667
return parametergrid instance for the given param_grid	param iterator	0.166667
ica	ica	1.000000
display the message	externals joblib parallel print	0.125000
the wild lfw pairs	lfw pairs subset	0.035714
the recall is the ratio tp / tp	recall	0.028571
feature	get feature	0.250000
the callable case for pairwise_{distances kernels}	callable x	0.083333
and scale the	scaler transform x y	0.200000
non-negative matrix factorization nmf find	non negative factorization x	0.043478
classification metrics read	metrics classification	0.052632
the timestamp when pickling to avoid	memorized func reduce	0.050000
and false positives per	clf curve y_true y_score pos_label sample_weight	0.250000
this function returns posterior probabilities of classification	core calibrated classifier cv predict proba	0.200000
data and	mixin score x y	1.000000
mcd from	x n_support	1.000000
returns the score on the given data	score	0.010101
predicts one class versus all others	multiclass x y	0.166667
varies for mono and multi-outputs	y eps n_alphas	0.250000
set the intercept_	linear model set intercept x_offset y_offset x_scale	1.000000
responsibilities	responsibilities	0.714286
determine absolute sizes of training subsets and	core translate train sizes	0.066667
predict multi-class targets using underlying estimators	core output code classifier predict	0.250000
hide warnings	warnings	0.076923
private function used to compute	function estimators	0.333333
the pairwise matrix	parallel pairwise x	0.166667
rand index adjusted for chance	adjusted rand score labels_true labels_pred	0.333333
file descriptor for the underlying file	joblib binary zlib file fileno	0.333333
samples of length dimensions	samples dimensions rng	1.000000
parallel n_jobs is the	n_jobs	0.023256
of the laplacian matrix and convert it to	laplacian	0.034483
class	boosting classifier	0.500000
the model to the training set x	x	0.001692
object	safe write to_write filename	1.000000
x	x dict_type	0.200000
normalize x by scaling	normalize x	0.076923
the estimator and set the base_estimator_ attribute	ensemble ada boost regressor validate estimator	0.333333
also predict based on an unfitted model	predict	0.006849
a calibration curve	core calibration curve y_true y_prob normalize n_bins	0.142857
number of jobs which are going to	externals joblib multiprocessing backend effective n jobs n_jobs	0.333333
the binary classification task	y_true y_score average	0.076923
inplace row scaling of a csr or	utils inplace row scale	0.142857
new	locally linear	0.200000
mini-batch dictionary learning finds a dictionary a set	dictionary learning	0.142857
the boston house-prices	boston return_x_y	0.250000
inplace row scaling of a csr	utils inplace row scale	0.142857
in multiplicative update nmf	decomposition multiplicative update h	0.500000
that for c in (l1_min_c infinity)	c x y	0.030303
each input	core cross val predict estimator x	0.045455
best possible score is 1	score y_true y_pred sample_weight multioutput	0.062500
to cleanup a temporary folder if still existing	joblib delete folder folder_path	0.250000
from prediction scores note this implementation is restricted	roc	0.033333
update parameters with given gradients parameters	update params grads	1.000000
of all the vectors rows of u	u	0.032258
edges for a	feature_extraction make edges	0.066667
multinomial loss	multinomial loss w	1.000000
avoid the hash	joblib	0.014599
the number of splitting iterations in	get n splits x y	0.111111
label encoder parameters	preprocessing label encoder	0.333333
loading for the lfw people dataset this	datasets fetch lfw people	0.040000
curve auc using the trapezoidal rule	auc	0.020408
precision matrix with	precision	0.016667
under a size	externals	0.005747
of the function called with the given arguments	memorized func get	0.125000
private function used to fit a single tree	build trees tree forest	0.142857
svmlight / libsvm format into sparse	svmlight file f n_features dtype multilabel	0.066667
l1 distances between the vectors	metrics paired manhattan distances	0.083333
callable case for pairwise_{distances kernels}	metrics pairwise callable	0.083333
returns the number of splitting iterations in the	group out get n splits x y	0.111111
tolerance which is independent of the	tolerance x	0.058824
check x format and	latent dirichlet allocation check	0.062500
array-like or scipy	preprocessing binarize x threshold copy	0.083333
dual gap convergence criterion the specific	dual gap emp_cov	0.071429
standardize a dataset along any axis	x axis with_mean with_std	0.333333
lshforest	lshforest	0.625000
function call with the given arguments	externals joblib format call func	0.100000
a name for the	get func name	0.047619
orthogonal matching pursuit step on a	n_nonzero_coefs	0.090909
generate cross-validated estimates for each input data	cross val predict estimator x y cv	0.071429
right_mask	right_mask	1.000000
fit the hierarchical clustering on	cluster agglomerative clustering fit	0.250000
image_name	image_name	1.000000
rand index adjusted for	cluster adjusted rand score labels_true	0.333333
scale	robust scale x	0.125000
embedding	embedding x n_neighbors n_components reg	0.200000
depth a which	externals joblib memorized func check previous func code	0.055556
classification metrics read more	metrics classification	0.052632
reduced likelihood	gaussian_process gaussian process reduced likelihood	0.142857
predict using the trained model parameters	neural_network base multilayer perceptron predict	0.333333
k x	gaussian_process dot product call x	0.200000
of	parallel backend base get	0.066667
the posterior log probability of the samples x	nb joint log likelihood x	0.222222
an array	array array	0.166667
a byte	externals	0.005747
significance of a cross-validated	cv	0.009009
score is	score y_true y_pred sample_weight	0.062500
perform classification on an array of test vectors	gaussian process classifier predict	1.000000
clear all	clear	0.166667
to fit an estimator	fit estimator estimator	0.055556
points on the grid	parameter grid len	0.333333
the precision is the ratio	metrics precision	0.033333
raise	raise	1.000000
memory is inefficient	classes	0.025641
derivatives with respect to each parameter	activations deltas	0.032258
an array shape under python 2 the long	shape repr shape	0.166667
implement a single boost	boost classifier boost iboost	1.000000
global clustering for the	global clustering	0.142857
onehotencoder to	preprocessing one hot encoder	0.500000
inverse label	preprocessing inverse	1.000000
graph of	feature_extraction grid to graph	0.333333
inefficient to train all	y classes	0.027778
for building a cv in a user friendly	core check cv cv	0.031250
return a platform	repr	0.012500
the number of splitting iterations in the cross-validator	cross validator get n splits x	0.125000
scale is zero we handle it correctly	preprocessing handle zeros in scale scale	0.500000
california housing dataset from	datasets fetch california housing	0.083333
shutdown the process or thread	multiprocessing backend terminate	0.166667
decision tree regressor from the training set	tree decision tree regressor fit	0.250000
names and a name for	name	0.033333
train	core base	0.083333
fit estimator and predict values for	model_selection fit and predict estimator x y train	0.250000
not enabled for sparse	preprocessing robust	0.111111
version	fobj	0.125000
parameters	parameters x	0.125000
of the reduced likelihood function	reduced likelihood function	0.041667
fit the	output estimator fit x	0.200000
people dataset this operation is	people	0.100000
using thresholding	thresholding y output_type classes	0.500000
whose range approximates the range of	randomized range	0.083333
descent the elastic net optimization function varies	l1_ratio	0.030303
functions of the	function x	0.030303
byte string to the file	binary zlib file	0.062500
class for all meta estimators in scikit-learn	meta	0.043478
n_population	n_population	1.000000
in the wild lfw pairs	fetch lfw pairs subset	0.035714
the univariate feature selection	base filter	0.333333
estimates the shrunk ledoit-wolf covariance	covariance ledoit wolf x	0.125000
jaccard similarity coefficient score	score y_true y_pred normalize sample_weight	0.125000
step using	linear_model	0.025641
curve auc using the	metrics auc	0.040000
lowest bound for c such that for c	c	0.022222
dual gap convergence criterion	covariance dual gap emp_cov precision_ alpha	0.071429
a sparse random projection matrix	random projection	0.125000
precisions	precisions nk xk	0.166667
detect	detect	1.000000
each input data	y	0.002674
ledoit-wolf	ledoit wolf	0.111111
to a	externals joblib customizable pickler register	0.200000
linkage agglomerative clustering	cluster linkage	1.000000
generate a sparse random projection matrix parameters	core base random projection fit x	0.333333
print verbose message on the end	base mixture print verbose msg init end ll	0.333333
compute prediction of init	base gradient boosting init	0.142857
for full covariance	density full x means	0.166667
input validation on an array	utils check array array accept_sparse	0.250000
of data under each gaussian in the model	mixture gmmbase	0.034483
iteritems	iteritems	1.000000
force the execution of the function with	joblib memorized	0.015625
computes the paired	metrics paired	0.200000
neighbors	neighbors	0.378378
negative value in	negative	0.090909
matrix to evaluate the accuracy of a classification	y_true y_pred labels sample_weight	0.125000
or	multiprocessing	0.045455
fit	fit x y sample_weight	0.260000
compute the unnormalized posterior log probability	base nb joint log likelihood	0.166667
directory corresponding	func dir mkdir	0.166667
elasticnet model trained	elastic net	0.111111
fit linear model	fit x y coef_init intercept_init	0.230769
based on a feature	connectivity n_clusters	0.250000
underlying estimators should be used when memory	one vs one classifier	0.125000
iterate over columns of	feature_selection iterate columns x columns	0.250000
image_size	image_size	1.000000
validation of	lib svm validate	0.500000
call transform on the estimator with the	transform	0.011236
run fit with all sets	grid search cv fit x	0.333333
indices increasingly apart the distance depending on	joblib verbosity filter index	0.055556
of the laplacian matrix and convert	laplacian	0.034483
random projection the components of the random matrix	random projection	0.125000
to	mixin	0.037037
of min	preprocessing min	0.166667
input checker utility for building a cv	core check cv cv x y	0.031250
ledoit-wolf covariance	covariance ledoit wolf shrinkage x assume_centered	0.125000
into a matrix of patch	feature_extraction patch extractor	0.200000
and the gradient	and gradient w x y epsilon	1.000000
predict the target of new samples	cv predict	0.041667
kernel density estimation read more	kernel density	0.083333
actual fitting performing the search over	base search cv fit x y parameter_iterable	0.333333
an extra-trees regressor	extra trees regressor	1.000000
previous	previous	1.000000
out cross-validator provides	out	0.095238
predict class probabilities at each stage for x	classifier staged predict proba x	1.000000
set the sample weight array	base sgd validate sample weight sample_weight	0.333333
the bound term related to proportions	dpgmmbase bound proportions z	0.333333
curve auc using the trapezoidal rule this is	auc	0.020408
and scaling parameters	x y	0.002155
distributions for the precisions	precisions	0.066667
image samples in x into	x	0.001692
and scale	scaler transform x y	0.200000
the k-neighbors of a	kneighbors mixin kneighbors	0.100000
brier	brier	1.000000
of exception types	backend	0.016949
the number of splitting iterations in the cross-validator	get n splits x y	0.111111
sample from the decision boundary for each class	one vs rest classifier decision function	0.250000
predict using the multi-layer perceptron classifier parameters	neural_network mlpclassifier predict	0.500000
list of	parallel backend base get	0.066667
the local	local outlier factor local	0.500000
centroids on x by chunking it into mini-batches	cluster mini batch kmeans fit x y	0.500000
input vectors individually to unit	axis copy	0.166667
complete cache directory	joblib memory clear warn	0.333333
the number of splitting iterations in	group out get n splits	0.111111
of feature names ordered by their indices	feature names	0.090909
l1 distances between the vectors	manhattan distances	0.083333
check that the	bayesian gaussian mixture check	1.000000
split data into training and test	base kfold split x y groups	0.200000
folders	joblib memory reduce	0.030303
c such that for c in (l1_min_c	c x	0.030303
graphlasso covariance	covariance graph lasso cv fit	0.111111
list	backend	0.016949
from the c and cpp	from c and cpp	1.000000
nothing and return	feature_extraction	0.037037
the neighbors	neighbors x	0.166667
generate	csc n_samples	1.000000
updates terminal regions to	terminal region	0.100000
measure the similarity of two clusterings of	metrics cluster fowlkes mallows score labels_true labels_pred	0.333333
cache	externals joblib memory	0.016949
up to size	size	0.032258
learn vocabulary and idf return	tfidf vectorizer fit transform raw_documents y	0.250000
connected graph	graph	0.021277
of estimators within a	estimators	0.052632
a single boost using	ada boost classifier boost	0.100000
y	y max_samples	1.000000
optimization objective for the case method='lasso' is :	xy gram	0.090909
scale back	scaler inverse transform x copy	0.066667
:ref user guide <mini_batch_kmeans>	mini batch kmeans	0.166667
of transform is sometimes referred	transform	0.011236
fit the model and transform	fit transform x	0.166667
a grid of points based on the	ensemble grid from	0.166667
sort features by name	feature_extraction count vectorizer sort features	1.000000
inefficient to train	y classes	0.027778
within a job	estimators estimators_features x n_classes	0.333333
return a platform independent	shape repr	0.013699
checker utility for building a cv in	core check cv cv	0.031250
requested by the callers	joblib effective	0.200000
the estimator with the best found parameters	core base search cv predict proba x	0.076923
compute prediction of init	ensemble base gradient boosting init decision function x	0.142857
accuracy of a classification	y_true y_pred	0.037037
for label spreading	semi_supervised label spreading build	0.250000
data in the	data compress	0.100000
multi-task l1/l2 elasticnet	multi task elastic net	0.250000
we don't store the timestamp when pickling to	externals joblib memory reduce	0.030303
norm_laplacian	norm_laplacian	1.000000
timestamp when pickling to avoid the hash depending	memorized func reduce	0.050000
embedding	embedding x n_neighbors n_components	0.200000
reconfigure the backend and return the number of	externals joblib parallel backend base configure n_jobs	0.333333
the number of splitting iterations in the cross-validator	predefined split get n splits x	0.111111
the hyperbolic tanh function	neural_network inplace tanh	0.500000
the number of splitting iterations in the cross-validator	model_selection base kfold get n splits x	0.111111
true and false positives per binary classification	metrics binary clf curve	0.090909
the decision function of x	ensemble ada boost classifier decision function x	0.333333
later scaling	scaler fit x	0.153846
allocation	allocation	1.000000
coefficient matrix to	mixin	0.037037
classification this function returns posterior probabilities of	cv predict proba	0.034483
parameters for the voting	voting	0.066667
the grid of alpha values for	alpha grid x y xy	0.166667
diagonal of the kernel k x x	dot product diag x	1.000000
an array using	array wrapper	0.166667
the long	shape	0.011765
best found	core base search cv	0.033333
mean absolute error regression loss read	mean absolute error	0.166667
of new samples can be different from the	calibrated classifier cv	0.071429
fit linear model with passive aggressive algorithm	linear_model passive aggressive regressor partial fit	1.000000
calculate true and false positives per binary classification	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
paired distances between	paired distances	0.500000
bicluster	bicluster mixin get	0.500000
x and	fit predict x y	0.250000
a list of feature name -> indices mappings	dict vectorizer fit x y	0.250000
determine the optimal batch size	externals joblib auto batching mixin compute batch size	0.333333
the precision the precision is the ratio	metrics precision	0.033333
the neighbors within	neighbors lshforest radius neighbors x	0.166667
precision matrix	decomposition base pca get precision	0.066667
convert coefficient matrix	linear_model	0.025641
learn vocabulary and idf return term-document matrix	tfidf vectorizer fit transform raw_documents y	0.250000
features are selected returns	feature_selection selector	0.142857
param logic estimators that implement the partial_fit api	utils check	0.023810
finds indices in sorted array of integers	find matching indices tree bin_x left_mask right_mask	0.166667
output for x relative	metrics threshold scorer call clf x	0.058824
single binary estimator	binary estimator x y i	0.500000
compute gaussian log-density at	mixture log multivariate normal density	0.500000
does not need	x y residual	0.500000
for each input	x y	0.002155
factorization nmf find	non negative factorization	0.043478
estimators that implement the partial_fit api need	utils check	0.023810
return a tolerance which is independent of	tolerance x	0.058824
feature name -> indices mappings	feature_extraction dict vectorizer fit	0.250000
clustering	clustering x	0.142857
fit linear model with stochastic gradient descent	base sgdregressor fit x y coef_init intercept_init	1.000000
note this implementation is	roc	0.033333
predict_proba on the estimator with the best	cv predict proba x	0.068966
compute the gradient of	compute	0.058824
for	estimator x y	0.038462
the voting classifier valid parameter keys can	voting classifier set	0.037037
matrices w h	x w h	0.035714
swaps two rows of a csc matrix in-place	utils inplace swap row csc x	0.250000
dispatch them	joblib parallel dispatch one	0.250000
covariance	covar matrix to match covariance	0.250000
return the kernel k x	call x	0.142857
predict multi-class targets	output code classifier predict x	0.250000
store	memory	0.015625
of x for	fit x y	0.005988
return the shortest path length from source	utils single source shortest path length graph source	0.111111
to fit an estimator	fit estimator estimator x	0.055556
sure that	check	0.017857
for updating terminal regions (=leaves)	update terminal region tree terminal_regions leaf x	0.200000
possible outcomes for samples in x	svm base svc predict	0.222222
for each input	core cross val predict estimator	0.045455
data	predict estimator x	0.045455
restricted to the binary classification task	y_true y_score pos_label sample_weight	0.066667
max absolute value	preprocessing max abs	0.050000
reporter	ensemble verbose reporter	1.000000
that will be extracted in	i_h i_w p_h p_w	0.250000
helper function	helper alpha y v u	0.333333
the graph is connected true or not false	graph is connected graph	0.500000
the test/test sizes are meaningful wrt to the	shuffle split n_samples test_size train_size	0.111111
is meant to be cached by	index_file_path data_folder_path slice_ color	0.033333
computes the gradient	alpha	0.117647
error of the kl divergence of p_ijs and	kl divergence error	0.100000
value of the log	log	0.018868
the trained	base multilayer perceptron	0.142857
with likelihood terms for standard covariance types	lik x initial_bound precs means	1.000000
median absolute error regression	median absolute error	0.166667
fit the	output estimator fit x y	0.200000
fit a single binary estimator	core fit binary estimator x y	1.000000
support	support	0.625000
free parameters	parameters	0.055556
coverage error measure compute how far we	metrics coverage error	0.166667
directory	func dir	0.500000
validation and	dtype	0.062500
lrd of a sample is the inverse	distances_x neighbors_indices	0.047619
each	x y	0.002155
center	center scale	1.000000
solution to a large sparse linear	a	0.018182
build a batch of estimators within a	build estimators	0.166667
compute joint probabilities p_ij from distances	manifold joint probabilities distances desired_perplexity verbose	1.000000
transform on the estimator with the best	cv transform	0.500000
weighted	weighted	0.750000
function for parameter value indexing	model_selection index param value	0.200000
remove cache folders to	joblib	0.007299
transforms features by scaling each feature	scale x feature_range axis copy	0.200000
estimate sample weights by class for unbalanced datasets	sample weight class_weight	0.500000
dual gap convergence criterion	covariance dual gap	0.071429
with the best found parameters	core base search cv predict proba	0.076923
helper function for factorizing common classes	partial fit first call clf classes	0.058824
a logistic	logistic	0.095238
votingclassifier	params deep	0.111111
the sign of elements of all	sign flip	0.066667
returns distinct binary	datasets generate hypercube	1.000000
the other and	x y	0.002155
helper to	helper	0.100000
when memory is inefficient	y classes	0.027778
of exception	backend base get	0.066667
returns first and	core first and	1.000000
the hash	joblib	0.014599
strip	datasets strip	0.153846
data	val predict	0.045455
fit the model to	multi output estimator fit x y	0.200000
x	matern call x	0.200000
log	linear_model sgdclassifier predict log	0.500000
number of points on the grid	grid len	0.333333
print verbose	mixture base mixture print verbose	1.000000
mean and variance	mean variance	0.500000
hash depending from	joblib	0.014599
seeds	seeds	0.666667
base class for spectral biclustering	base spectral	1.000000
generate a	n_samples n_features n_classes	0.333333
contingency matrix describing the relationship between	metrics cluster contingency matrix	0.200000
regressor from the	regressor	0.054054
the vectors in x and y	x y	0.004310
np dot x	dot x	0.500000
orthogonal matching pursuit model omp	orthogonal matching pursuit	0.250000
can be different from the	core calibrated classifier	0.083333
the voting classifier	voting classifier set	0.037037
estimator is using a precomputed	core	0.015385
compute data	decomposition base pca	0.071429
whether the file supports seeking	externals joblib binary zlib file seekable	0.250000
type_filter	type_filter	1.000000
note this implementation is restricted to	metrics roc	0.040000
generate a random	n_samples n_features n_classes	0.333333
signature	signature	0.333333
logistic loss and gradient	logistic loss and grad w x y	0.500000
log probabilities within	predict log proba	0.029412
number of points on the	len	0.038462
check input and compute prediction of init	ensemble base gradient boosting init decision function x	0.142857
user provided precisions	mixture check precisions precisions covariance_type	0.250000
reconstruct the array from the meta-information	externals joblib zndarray wrapper read unpickler	0.043478
check x format check x format	decomposition latent dirichlet allocation check	0.062500
of x	x z	0.050000
perplexity	perplexity	0.636364
the content of the data home cache	clear data home data_home	0.076923
of parameters and raise valueerror if not	ensemble base gradient boosting	0.111111
for the voting	voting	0.066667
compute directory associated with a given	to dir cachedir func argument_hash	0.250000
pairs dataset this dataset is a	pairs	0.055556
project data to vectors and cluster	spectral biclustering project and cluster data vectors	0.333333
descriptors of a memmap instance to reopen on	externals joblib reduce memmap a	0.050000
generative	decomposition	0.047619
graphlasso covariance model	covariance graph lasso cv fit	0.111111
perform the actual data loading for the lfw	datasets fetch lfw	0.083333
returns whether the kernel is stationary	gaussian_process kernel operator is stationary	1.000000
precision matrix with the generative	base pca get precision	0.066667
calibration curve	calibration curve	0.142857
based on a feature	tree x connectivity n_clusters return_distance	0.250000
local outlier factor	neighbors local outlier factor	0.125000
compute the weighted log probabilities	base mixture score	0.200000
friedman [1] and breiman [2]	make friedman3 n_samples noise random_state	0.166667
mono and	x y	0.002155
tolerance which	cluster tolerance x tol	0.058824
fit an estimator within a job	parallel fit estimator estimator x y	0.333333
the best found parameters	search cv predict proba x	0.076923
read up to len b bytes into b	externals joblib binary zlib file readinto b	1.000000
estimator with the best found	core base search cv predict proba	0.076923
index_file_path	index_file_path	1.000000
rows of a csc matrix in-place	utils inplace swap row csc x m	0.250000
input	cross val predict estimator	0.045455
a random regression problem with sparse uncorrelated design	make sparse uncorrelated	0.166667
fit the model to data	multi output estimator fit x y sample_weight	0.200000
fit linear	linear_model base sgdclassifier fit x y	0.333333
predicted probabilities for a calibration curve	core calibration curve y_true	0.142857
generate a	n_samples n_features n_classes n_labels	0.333333
to delete to keep	to delete root_path bytes_limit	0.500000
'l'	utils shape repr	0.013699
of x and dot w h	decomposition beta divergence x w h	0.500000
in friedman [1] and breiman [2]	friedman3	0.090909
the number of splitting iterations in	leave one group out get n splits	0.111111
an unfitted	estimators unfitted name	0.142857
u	u	0.354839
global clustering for the subclusters obtained	global clustering	0.142857
lad updates terminal	least absolute error update terminal region tree terminal_regions	0.200000
report showing the	report	0.047619
returns whether the kernel	gaussian_process stationary kernel	1.000000
the linear assignment problem using	linear assignment	0.090909
check initial parameters	base mixture check parameters	0.200000
list of feature	get feature	0.125000
the best found parameters	search cv predict proba	0.076923
to predict apply	predict	0.013699
generate indices to split data into	model_selection predefined split split	0.250000
don't store the timestamp when pickling to	joblib memory reduce	0.030303
such that for c in	c x y loss	0.030303
values for x	call estimator x	0.166667
reconfigure the backend and return the	externals joblib parallel backend base configure	0.333333
deviance	deviance call y pred	0.333333
the kernel k x	gaussian_process matern call x	0.200000
func to be	func	0.034091
svmlight	svmlight	0.300000
x	regressor predict x	0.166667
bind	bind	1.000000
projection p only changes	core johnson lindenstrauss min dim	0.500000
the model and transform	transform x	0.016949
the depth	func check previous func code	0.333333
estimates	core cross val predict estimator	0.045455
pickling reduction for memmap	externals joblib reduce memmap	0.142857
the weighted graph of neighbors for points in	neighbors radius neighbors mixin radius neighbors graph	0.066667
the	joblib parallel backend base	0.058824
the huber loss	huber loss	0.333333
points on	len	0.038462
coefs and intercept for specified layer	grad layer	0.166667
apply a transform	selected x transform	0.333333
compute non-negative matrix factorization	negative factorization	0.043478
to a large	a	0.018182
sample weights by class for	sample	0.032258
mutual information	mutual info	1.000000
the hash depending from	externals	0.011494
homogeneity	homogeneity	1.000000
number of splitting iterations in	base cross validator get n splits x	0.125000
local outlier factor of x (as bigger is	local outlier factor decision function x	0.100000
function opening the right fileobject from a filename	joblib read fileobject fileobj filename	0.250000
process or thread pool	joblib multiprocessing	0.052632
faces in the wild lfw pairs dataset	datasets fetch lfw pairs	0.018868
the wild lfw pairs dataset this dataset is	lfw pairs	0.018868
attach a reducer function to a	externals joblib customizable	0.200000
the long type introduces an 'l'	shape repr	0.013699
train	train	0.294118
the local	local	0.181818
l1 distances between the vectors in x	manhattan distances x	0.500000
inplace column scaling of a csc/csr	inplace column scale x	0.166667
iterator over	iter	0.050000
extensions to	extensions	0.142857
convert coefficient matrix	sparse coef mixin	0.083333
load datasets in the	datasets load	0.083333
loading for the lfw pairs dataset this operation	fetch lfw pairs	0.018868
cv aka	cv	0.009009
init	base gradient boosting init decision	0.142857
grid of alpha values for	linear_model alpha grid	0.166667
estimates the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x assume_centered	0.125000
the already fitted lsh forest	neighbors lshforest partial fit	0.200000
randomized logistic regression works by subsampling	randomized logistic regression	0.166667
fits the graphlasso covariance model to	covariance graph lasso cv fit	0.111111
fit ridge regression model parameters	linear_model ridge fit x y sample_weight	1.000000
evaluate the density model on the data	kernel density score samples x	0.250000
of all the vectors rows of u	flip u	0.047619
get a signature object for the	externals signature obj	0.200000
a single binary estimator	binary estimator x y	0.500000
cov_init	cov_init	1.000000
implement a single	iboost	0.285714
used to compute log probabilities within a	predict log proba	0.029412
r^2 coefficient of determination regression score function	metrics r2 score y_true y_pred sample_weight	0.125000
perform a locally linear embedding	locally linear embedding	0.050000
manhattan	manhattan	0.625000
all the vectors rows of u such that	flip u	0.047619
estimate class weights	utils compute class	0.166667
fit with all sets	cv fit x y	0.250000
indices increasingly apart the distance depending	externals joblib verbosity filter index	0.055556
median of data	utils get median data	0.333333
remove	joblib memory reduce	0.030303
neighbors within	lshforest radius neighbors x	0.166667
input	input	1.000000
the parameters for the voting classifier	voting classifier set	0.037037
compute the mean and	y	0.002674
based on a feature	x connectivity n_clusters return_distance	0.250000
based on a	connectivity n_clusters	0.250000
find two non-negative matrices w h whose product	x w h	0.035714
one set	point x y estimator	0.500000
for full covariance	density full	0.166667
add documentation to a	externals add doc func doc	0.500000
the mean squared error between two covariance estimators	covariance error norm comp_cov norm scaling squared	0.250000
transformer	preprocessing imputer	0.333333
accuracy of a classification	y_true	0.021739
the votingclassifier	params deep	0.111111
cv in a user friendly way	check cv cv x	0.031250
opposite of the local outlier factor of x	neighbors local outlier factor decision function x	0.100000
ovo	ovo	0.833333
:ref user guide <image_feature_extraction>	patch extractor	0.090909
the mean silhouette coefficient	metrics cluster silhouette score x	0.250000
the distortion introduced by a random	n_samples eps	0.125000
cv in a user	cv cv x	0.031250
kl	manifold kl	0.333333
voting	voting	0.400000
of x (as bigger	x	0.001692
to run in	joblib	0.007299
mean	mean	0.464286
two non-negative matrices w h whose product approximates	w h n_components	0.038462
largest graph connected components that contains one	manifold graph connected component graph node_id	1.000000
array-like or scipy	preprocessing binarize	0.083333
a score by cross-validation read more in the	model_selection cross val score estimator	0.166667
log probabilities	predict log proba	0.029412
lfw people dataset this operation is meant to	datasets fetch lfw people data_folder_path slice_ color resize	0.333333
out cross-validator provides train/test indices to split	out	0.095238
label	gmmbase	0.062500
ward clustering	cluster ward tree	1.000000
sparse matrix x	x	0.001692
estimator and predict values for a given dataset	and predict estimator x y train	0.200000
fit the model	linear model fit	1.000000
full covariance matrices	normal density full x means	0.166667
function cache result and return a	joblib memorized func	0.014706
handle the callable case	callable x	0.083333
suffix when using the	shape repr	0.013699
to	sparse	0.025000
procedure described in [rouseeuw1984]_ aiming at computing	covariance c step x n_support remaining_iterations initial_estimates	0.111111
of points that will be sampled	parameter sampler len	0.333333
transform a sequence of instances to a scipy	feature hasher transform raw_x	0.333333
apply clustering to	spectral clustering	0.142857
voting classifier valid parameter keys	ensemble voting classifier set	0.037037
terminal	terminal region tree terminal_regions	0.100000
neighbors for points	neighbors radius neighbors mixin radius neighbors	0.125000
the leaves of	leaves	0.071429
expression of the dual gap convergence criterion	covariance dual gap	0.071429
paired cosine distances between	metrics paired cosine distances	0.333333
if y	y	0.005348
force	externals joblib memorized	0.013699
the number of splitting iterations in the	cross validator get n splits x y	0.125000
the content of the data home	clear data home data_home	0.076923
lfw people dataset this	lfw people	0.040000
store	externals joblib memorized	0.013699
loader for the california housing dataset from statlib	california housing data_home	0.250000
the model according to the	y sample_weight	0.035714
function output for x relative to y_true	metrics threshold scorer call clf x	0.058824
independent representation	repr	0.012500
that for c in (l1_min_c infinity) the model	c	0.022222
timestamp when pickling to avoid the	reduce	0.034483
retrieve or aggregate feature importances from estimator	get feature importances estimator norm_order	1.000000
= - log sum_h exp(-e v	v	0.052632
number of splitting iterations in the cross-validator parameters	split get n splits x	0.111111
for memmap backed arrays	memmap backed	0.333333
input checker utility for building a cv	core check cv cv x y classifier	0.031250
list of feature names ordered by their	vectorizer get feature names	0.125000
the maximizer of the reduced likelihood	process arg max reduced likelihood	0.250000
samples x to the separating hyperplane	svm base lib svm decision function x	0.250000
fit the model	svm linear svc fit x y	0.333333
we	memorized	0.015873
least-squares solution to a	lsqr a	0.037037
fit_predict of last step in pipeline after transforms	core pipeline fit	0.166667
area under the curve auc using the trapezoidal	metrics auc x	0.040000
sparse uncorrelated design this dataset is	datasets make sparse uncorrelated	0.166667
list of	externals joblib parallel backend base get	0.066667
forest classifier	forest classifier	1.000000
perform the	responsibilities params min_covar	0.500000
weight	weight	1.000000
convert coefficient	coef mixin	0.090909
the pairwise matrix in n_jobs even slices	metrics parallel pairwise x y func n_jobs	0.111111
using randomized svd	randomized	0.090909
input data point	x	0.001692
the timestamp when pickling to	joblib memorized func reduce	0.050000
reconstruct the array	joblib ndarray wrapper read unpickler	0.333333
metrics should	metrics	0.043478
if	if	1.000000
parameters for the voting classifier valid parameter keys	voting classifier set params	0.037037
graph of the	grid to graph	0.333333
actual fitting performing the	parameter_iterable	0.142857
check input and compute prediction of init	gradient boosting init decision	0.142857
representation	shape repr	0.013699
paired cosine distances between x and y read	paired cosine distances x y	0.333333
break the pairwise matrix in	pairwise x y func	0.166667
check values of the basic parameters	check initial parameters	1.000000
and compute prediction of init	gradient boosting init decision	0.142857
precision is the ratio	metrics precision	0.033333
to binary labels the output of transform is	transform y	0.023256
the search over	base search	0.100000
for full covariance matrices	full	0.055556
calculate mean update	incremental mean	0.166667
class_weight	class_weight	1.000000
to the median and component wise scale	robust scale x	0.125000
each input	val predict estimator x y	0.045455
zero row of x	x y copy	0.142857
the file	externals joblib binary zlib file	0.166667
list of exception types	parallel	0.019231
a	externals joblib sequential backend	1.000000
calculate out of bag predictions and score	ensemble base bagging set oob score x y	0.250000
the logistic loss and gradient	logistic loss and grad w	0.500000
matching pursuit omp solves n_targets orthogonal	linear_model orthogonal mp	0.250000
mean and variance along an axix on	incr mean variance axis x axis last_mean last_var	0.333333
iterinv helper class to repeatedly solve m*x=b	iter inv	0.200000
returns the	w	0.035714
the cache for	joblib memorized func	0.014706
memory	externals joblib memory	0.016949
"news" format strip lines beginning	datasets strip newsgroup	0.090909
projection to the normalized laplacian	n_components eigen_solver	0.166667
fit	svr fit x	0.333333
single boost	boost classifier boost	0.100000
used when memory is inefficient to	classes	0.025641
used to fit an estimator within a job	parallel fit estimator estimator x y	0.333333
of bag predictions	ensemble base bagging set oob	0.250000
compute the mlp loss	neural_network base multilayer perceptron	0.083333
a logistic	linear_model logistic	0.111111
solve the isotonic regression model : min sum	isotonic regression y	0.066667
edges	feature_extraction make edges	0.066667
print verbose message on the	base mixture print verbose msg init	0.333333
of x and y is float32	x y	0.002155
validity of the input	params x metric p metric_params	0.100000
probabilities of classification this function returns posterior probabilities	cv predict proba	0.034483
net path with coordinate descent	linear_model enet path x	0.050000
the	shape repr	0.027397
a classification	y_true y_pred labels sample_weight	0.125000
fit	base sgdclassifier fit x y	0.333333
lad updates terminal regions to	ensemble least absolute error update terminal region tree	0.200000
convert a sparse matrix to a given format	utils ensure sparse format spmatrix accept_sparse dtype	1.000000
transform function to portion of selected features parameters	transform selected x transform selected copy	0.333333
binary metric for multilabel classification parameters	binary score binary_metric y_true y_score	0.500000
fit	ic fit	1.000000
used to compute log probabilities within a job	predict log proba estimators estimators_features x n_classes	0.250000
compute the boolean mask x == missing_values	get mask x value_to_mask	0.333333
pickling reduction for memmap backed arrays	externals joblib reduce memmap backed a	1.000000
given type in the	register type	0.333333
train	shuffle split iter	0.166667
folders to make cache size fit	size	0.032258
sets the flattened log-transformed non-fixed hyperparameters	gaussian_process kernel operator theta theta	0.333333
along an axis on a csr or csc	axis	0.014085
the number of splitting iterations in	cross validator get n splits x	0.125000
single tree in parallel	ensemble parallel build trees tree forest	0.200000
get the values used to	get	0.012048
any axis center to the mean	axis	0.014085
on the training set according	fit predict	0.055556
with the generative	decomposition base	0.076923
skip_num_points	skip_num_points	1.000000
coefficient of determination r^2	multi output regressor score	0.200000
handle the callable case for pairwise_{distances	callable x y	0.083333
loader for the california housing	california housing data_home	0.250000
last element of numpy array or sparse	last element arr	0.142857
parameters for the voting classifier	ensemble voting classifier set params	0.037037
type	utils shape repr	0.013699
isolation forest algorithm return the anomaly score	isolation forest	0.200000
of x from y along the first	x z	0.050000
omp solves n_targets orthogonal	linear_model orthogonal mp	0.250000
a contingency matrix describing the	contingency matrix labels_true labels_pred	0.166667
transforms and predict_log_proba	predict log proba x	0.045455
partially fit underlying estimators	one vs one classifier partial fit	0.166667
robust	robust	0.454545
list of exception types	joblib	0.007299
long type introduces	utils shape repr	0.013699
predict using the gaussian process regression model	gaussian process regressor	0.055556
positivity	positivity	0.714286
lasso path using lars algorithm [1]	linear_model lars path	0.100000
matrix factorization nmf find two non-negative matrices w	non negative factorization x w	0.500000
gaussian naive bayes	gaussian nb	0.500000
relative	scorer call	1.000000
given arguments	externals joblib memorized func get	0.125000
binary metric for multilabel classification parameters	average binary score binary_metric y_true	0.500000
reducing	items root_path	0.066667
on training	fit	0.006515
kernel k x y and	gaussian_process compound kernel call x y	0.333333
score	score	0.202020
binary classifier on x and y	binary x y	0.500000
return the feature importances	decision tree feature importances	0.333333
of x	x y copy	0.142857
c in (l1_min_c infinity) the	c x y	0.030303
for scaling	scaler fit x	0.076923
the isotonic regression model	core isotonic regression	0.055556
with sparse uncorrelated design	sparse uncorrelated n_samples	0.166667
repeated splits for	repeated splits	0.125000
full	multivariate normal density full	0.166667
the graphlasso covariance model	covariance graph lasso cv fit	0.111111
scale back the data to the original	robust scaler inverse transform x	0.066667
compute data	decomposition base pca get	0.071429
x and returns	x y	0.002155
the search	base search cv fit x y	0.166667
for updating terminal	function update terminal	0.200000
group out cross-validator provides train/test indices to	group out	0.142857
normalization constant	logz v s dets	0.200000
matrix factorization nmf find two	decomposition non negative factorization	0.043478
the diagonal	diag	0.031250
similarity of two clusterings of a	score labels_true labels_pred sparse	0.047619
based on a feature	tree x connectivity n_clusters	0.250000
shelve	shelve	1.000000
list of module names and a name for	get func name	0.047619
classification on test vectors x	core dummy regressor predict x	0.250000
function to portion of selected features parameters	selected	0.142857
a list of edges	edges	0.047619
linear assignment problem using the hungarian algorithm	linear assignment x	0.090909
vectors in x	x	0.005076
on samples in	svm base	1.000000
update h	x w h	0.035714
to compute log probabilities within a	predict log proba	0.029412
perform dbscan clustering from features or	cluster dbscan fit x y sample_weight	0.200000
the estimator with the best found parameters	model_selection base search cv predict	0.076923
read value from	result get	1.000000
parameters theta as the maximizer of the	gaussian_process gaussian process arg max	0.047619
create a base class with a metaclass	externals joblib with metaclass meta	1.000000
path with coordinate descent	linear_model enet path x	0.050000
identify uniquely python	obj hash_name coerce_mmap	0.200000
generate train	iter	0.050000
along an axix on	axis x axis	0.083333
matrix shrunk on the diagonal read	shrunk	0.043478
the given data x	x y	0.002155
the estimator with the best found parameters	core base search cv predict	0.076923
identify uniquely python objects containing numpy	obj hash_name coerce_mmap	0.200000
norm vector length	normalize x norm axis	1.000000
precision matrix with	get precision	0.052632
meta-information	zndarray wrapper read unpickler	0.043478
compute the pseudo-likelihood	neural_network bernoulli rbm score samples	1.000000
given args and kwargs using	args kwargs	0.100000
generate a random multilabel classification problem	datasets make multilabel classification n_samples n_features n_classes	0.500000
implementation is restricted to the binary classification	score y_true y_score average sample_weight	0.076923
a tolerance which is independent of	tolerance x tol	0.058824
csv file	csv f	1.000000
ll	ll	1.000000
an	repr	0.012500
coverage error measure compute how far we need	metrics coverage error y_true	0.166667
the lfw pairs dataset this	fetch lfw pairs	0.018868
for building a cv in a user friendly	check cv cv x y classifier	0.031250
model	pca	0.047619
transform on the estimator with	transform x	0.016949
k-fold iterator variant with non-overlapping	label kfold	0.250000
cache	joblib memorized	0.046875
persist an arbitrary python object	dump value filename compress protocol	0.250000
the pairwise matrix in n_jobs	pairwise x y func n_jobs	0.111111
whenever scale is zero we handle it correctly	handle zeros in scale scale	0.500000
the graph is connected true or not false	manifold graph is connected graph	0.500000
the directory corresponding	func dir mkdir	0.166667
data and labels	classifier mixin score x y sample_weight	1.000000
the reduced likelihood function for	reduced likelihood function	0.041667
dispatch them	externals joblib parallel dispatch one	0.250000
x for a diagonal	diag x means covars	0.500000
model from data	decomposition	0.047619
mask	mask	0.428571
return whether the file was opened for writing	joblib binary zlib file writable	0.250000
all tokens in the raw documents	feature_extraction count vectorizer fit	0.125000
dense	random_state	0.076923
error regression loss	error y_true	0.111111
regression model we can also	regressor	0.027027
creates a	cluster base spectral fit	0.250000
function and cache result or read	func	0.011364
a cv in a user	core check cv cv	0.031250
the callable case for pairwise_{distances	pairwise callable x	0.083333
detects	one class svm fit	0.125000
children	children	1.000000
dataset downloading it if necessary	subset data_home download_if_missing random_state	0.166667
fit	fit x	0.230769
cf node	cluster birch get	0.333333
function used to build a batch of	ensemble parallel build	0.047619
median across axis 0	median axis 0	0.333333
h whose product approximates	h	0.041667
with nonzero entries	feature_extraction count vectorizer inverse transform	0.166667
out cross-validator provides train/test indices to split data	out	0.095238
function	delayed function	0.200000
relationship between	metrics cluster	0.142857
with sparse uncorrelated design	sparse uncorrelated n_samples n_features	0.166667
faces in the wild lfw pairs dataset this	datasets fetch lfw pairs	0.018868
new_subcluster1	new_subcluster1	1.000000
lad updates terminal regions to median	least absolute error update terminal region tree	0.200000
kernel k x y and optionally its gradient	call x y eval_gradient	0.200000
sign correction to	flip u v u_based_decision	0.166667
std to be used for later scaling	preprocessing standard scaler fit x	1.000000
labels	classifier mixin score	1.000000
estimate the precisions parameters	bayesian gaussian mixture estimate precisions nk xk sk	0.166667
the raw documents	feature_extraction count vectorizer fit raw_documents	0.125000
feature name -> indices mappings	dict vectorizer	0.250000
number of splitting iterations in	base kfold get n splits x y	0.111111
actual fitting performing the	cv fit x y parameter_iterable	0.500000
unpickler to unpickle our numpy pickles	numpy unpickler	0.333333
metric is invalid	undefined metric	0.333333
the decision	ada boost classifier decision	0.333333
that implement the partial_fit api need to be	utils check partial fit	0.038462
input data	predict	0.006849
interpret	feature_selection calculate	1.000000
for species	species	0.142857
k x y and optionally its gradient	product call x y eval_gradient	0.333333
for each input	cross	0.037037
check according	core check	0.111111
target values for x	x	0.001692
compute non-negative matrix factorization nmf	non negative factorization x	0.043478
'l' suffix when using the	repr	0.012500
gp prior	return_std return_cov	0.142857
check that the parameters are well defined	mixture bayesian gaussian mixture check parameters x	1.000000
return the number of workers	externals joblib parallel	0.014085
input data	cross val	0.038462
list of module names and a name	name	0.033333
of x (as bigger is	function x	0.030303
lfw people	fetch lfw people	0.040000
locally linear embedding analysis on	locally linear embedding x n_neighbors n_components reg	0.071429
back the data	standard scaler inverse transform x copy	0.066667
the covariance m step for full cases	mixture covar mstep full gmm x responsibilities weighted_x_sum	0.500000
[1] and breiman [2]	make friedman3 n_samples	0.166667
return the directory	get output dir	0.047619
the models computed by 'path' parameters	linear_model path residuals	0.250000
dual gap convergence criterion the specific definition	covariance dual gap	0.071429
distances between the vectors in x	distances x	0.500000
graph is connected true or not false	graph is connected graph	0.500000
does not need	tree x y residual	0.500000
memmap instance to reopen on	memmap	0.066667
factorization nmf find two non-negative matrices	factorization	0.035714
compute the initial centroids	cluster init centroids	0.166667
the voting classifier valid parameter keys can be	ensemble voting classifier	0.031250
finds seeds for	seeds x	0.250000
an array using numpy	joblib numpy array wrapper	0.333333
cache and return it	externals joblib memorized	0.013699
called with the given arguments	joblib memorized func get output	0.125000
the kernel k x	rbf call x	0.200000
split data into	base kfold split	0.250000
the weighted	mixture base mixture	0.111111
scale each non zero row of x	x	0.001692
for each	core cross val predict estimator x y	0.045455
'l' suffix when using the	utils shape repr	0.013699
grid of alpha values	linear_model alpha grid x	0.166667
gaussian process regression model we can also predict	gaussian process regressor predict x	1.000000
classifier from the training set x y	classifier fit x y sample_weight	1.000000
of csgraph inputs	utils sparsetools validate graph csgraph	0.250000
back the data to the	scaler inverse transform x	0.052632
number of samples	samples	0.052632
the scaling of x	scaler inverse transform x	0.026316
locally linear embedding analysis	locally linear embedding x n_neighbors n_components	0.071429
distribution	bayesian	0.142857
impute all missing values	preprocessing imputer transform	0.500000
include_other	include_other	1.000000
and breiman [2]	friedman3	0.090909
exception types to	base	0.014286
for each	val	0.037037
score for a fit	fit rfe estimator x y	0.166667
parametergrid instance for the given param_grid	cv get param iterator	0.166667
dataset this operation is meant	data_folder_path slice_ color resize	0.033333
total log probability under the model	neighbors kernel density score x	0.333333
and a set	y	0.002674
used in hastie	hastie	0.076923
with joblib	externals joblib	0.004762
the curve auc using the trapezoidal	auc x y	0.040000
compute the	neural_network	0.142857
timestamp when pickling to avoid the	memory reduce	0.030303
creates a biclustering for x	spectral fit x	1.000000
for full	multivariate normal density full x means	0.166667
output from svd	svd	0.071429
a sparse random projection matrix parameters	core base random projection	0.200000
a cv in	cv cv	0.031250
fit the model and	fit	0.003257
helper function for parameter value indexing	model_selection index param value	0.200000
skip test	utils check skip	0.500000
path with coordinate descent	path x	0.045455
project data to vectors and cluster the result	project and cluster data vectors	0.333333
matrices w h whose product approximates the non-	x w h n_components	0.038462
best found	core base search cv predict proba x	0.076923
the variational lower bound for the	mixture dpgmmbase bound means	0.250000
linear models for feature selection this	linear model	0.090909
being run on travis	travis	0.142857
and dense inputs	y	0.002674
of x from y along	x	0.001692
is stationary	operator is stationary	1.000000
locally linear embedding analysis	manifold locally linear embedding x n_neighbors n_components reg	0.071429
file object providing transparent zlib de compression	binary zlib file	0.062500
precisions parameters of	precisions	0.066667
identity function	preprocessing identity x	1.000000
of a csr matrix in-place	utils inplace swap row csr x m n	0.250000
to dense array format	mixin densify	0.100000
ridge regression with built-in	ridge	0.071429
of neighbors for points in	neighbors radius neighbors	0.100000
shrunk on the diagonal	shrunk	0.043478
computes the free energy f v =	bernoulli rbm free energy	0.066667
the cache for	memorized func	0.016949
in pipeline after transforms	core pipeline	0.076923
pairwise matrix in	metrics parallel pairwise x y	0.166667
lasso path using lars algorithm [1] the	linear_model lars path	0.100000
the recall the recall is the	recall	0.028571
all the covariance	covar matrix to match covariance	0.250000
the range of a	utils randomized range finder a	0.166667
using x as training	x	0.003384
is the depth	memorized func check previous func code	0.333333
indices to split data into training and test	predefined split split x y groups	0.200000
class at each stage	gradient boosting classifier staged	0.500000
local outlier factor of x (as bigger	neighbors local outlier factor decision function x	0.100000
non-negative matrices w	x w	0.083333
csr_output	csr_output	0.555556
logic estimators that implement the	utils check partial	0.038462
for x	x y	0.002155
fit the model using x y	fit x y	0.005988
configure a copy	append	0.083333
block checkerboard	checkerboard	0.100000
update the dense dictionary	decomposition update dict dictionary	0.333333
returns the number of splitting iterations in the	pgroups out get n splits x y groups	0.111111
the parallel execution only a fraction	externals joblib parallel	0.014085
sizes of training	core translate train sizes	0.066667
the process or thread	joblib multiprocessing backend	0.052632
general function given points on a	x y reorder	0.111111
determination regression score function	metrics r2 score y_true y_pred sample_weight	0.125000
check the	check params	0.200000
the sign of vectors for reproducibility flips	deterministic vector	0.076923
the dense dictionary factor	dictionary y	0.111111
regression target at each stage	regressor staged	0.500000
write array bytes to pickler file handle	write array array pickler	0.333333
calibration curve	calibration curve y_true y_prob normalize n_bins	0.142857
undo the scaling	preprocessing min max scaler inverse	0.500000
parameter value indexing	model_selection index param value x v indices	0.200000
one-vs-one	svm one vs one	0.050000
we don't store	joblib memorized func	0.014706
transforms features by scaling each feature	min max scaler	0.083333
and a parameter weights	weights dist weights	0.142857
y_test	y_test	0.714286
calibrated	calibrated	1.000000
and y	y sum_over_features size_threshold	1.000000
constant block diagonal structure for	biclusters shape n_clusters noise minval	0.058824
for parallel	parallel	0.019231
process or	joblib multiprocessing	0.052632
coding	coding	1.000000
turn a transformed real-valued array into a	projection to	0.166667
curve auc using the trapezoidal rule this is	metrics auc x	0.040000
to avoid the	externals joblib memorized func	0.013158
diagonal cases	mstep diag gmm x responsibilities weighted_x_sum	0.250000
locally linear embedding analysis	locally linear embedding x n_neighbors	0.071429
absolute error of	error	0.020000
free energy f v = - log sum_h	neural_network bernoulli rbm free energy	0.066667
multiclass	multiclass y classes	0.500000
expression of the dual gap convergence criterion the	dual gap	0.071429
names and a name for the	name	0.033333
net path with coordinate	linear_model enet path	0.050000
parallelbackend which	backend	0.033898
returns the number of splitting iterations in the	leave pgroups out get n splits x y	0.111111
get a signature object for the	externals signature	0.050000
validity of the input parameters	x metric p metric_params	0.100000
compute the maximum absolute value to	preprocessing max abs	0.050000
model to x	fit x y	0.005988
that accepts custom reducers	customizable	0.166667
is the	y_true y_pred beta labels	0.500000
to multi-class labels parameters	y threshold	0.166667
fit with all sets	search cv fit	0.111111
back	preprocessing robust scaler inverse transform x	0.066667
covariance_type	covariance_type	0.416667
transform binary labels back	preprocessing label binarizer inverse transform	0.500000
area under the curve auc using	auc x y	0.040000
pairs for different probability thresholds note this	probas_pred pos_label sample_weight	0.066667
before the first blank line	header	0.090909
the long type introduces an 'l' suffix when	utils shape	0.013699
mcd from	select candidates x n_support	1.000000
learn vocabulary and idf	feature_extraction tfidf vectorizer fit transform raw_documents y	0.250000
ridge regression model parameters	linear_model ridge gcv	1.000000
a given radius of a	x radius	0.058824
a dataset along any axis center to the	x axis	0.030769
detb	detb	1.000000
whether the file supports seeking	binary zlib file seekable	0.250000
lad updates terminal regions to median	least absolute error update terminal	0.200000
as the maximizer of the reduced likelihood function	gaussian process arg max reduced likelihood function	0.333333
performs clustering on	dbscan fit predict	0.333333
return the shortest path length from source to	shortest path length graph source	0.111111
check input and compute prediction of init	init decision function x	0.142857
apply a transform	transform	0.011236
set the	model set	0.500000
free energy f v	rbm free energy	0.066667
mean squared logarithmic error regression loss	metrics mean squared log error	0.200000
this operation is meant to	data_folder_path slice_ color resize	0.033333
model to the training set x and	x y	0.002155
indices to split data into training and test	model_selection base kfold split x y groups	0.200000
when using	utils shape	0.013699
values of the basic parameters	initial parameters	1.000000
not found and raise an exception if a	utils line search	0.029412
actual data loading for the lfw pairs dataset	fetch lfw pairs	0.018868
normalize x according to kluger's log-interactions	log normalize x	0.200000
to split data in train/test	split	0.027778
set the parameters	pipeline set	0.250000
opening the right fileobject from	read fileobject	0.100000
inplace row scaling of a csr or csc	utils inplace row scale x	0.142857
using the gaussian process	gaussian process	0.083333
fit estimator and compute scores for	core fit and score estimator x	0.333333
opening the right fileobject from a	externals joblib read fileobject	0.100000
shutdown	backend terminate	0.166667
the shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage x	0.125000
node_id	node_id	1.000000
mean absolute error regression loss read more in	mean absolute error	0.166667
input	cross	0.037037
a lower bound on model evidence based	mixture dpgmmbase lower bound	0.071429
neighbors within a given radius of	radius neighbors x radius	0.142857
score corresponds to the area under	score y_true y_score	0.025000
data precision matrix with the	base pca get precision	0.066667
the score on the given data	score	0.010101
predicted target values for x relative	metrics predict scorer call estimator x	0.166667
is restricted to the binary	y_score	0.083333
function for factorizing common classes param logic estimators	first call clf classes	0.058824
absolute error regression loss read more	absolute error y_true	0.142857
the array from the meta-information and the z-file	zndarray wrapper read unpickler	0.043478
given column class distributions	classes class_probability	0.166667
weights by class for unbalanced datasets	weight class_weight	0.200000
partially fit underlying	one vs one classifier partial fit x y	0.166667
estimate covariance with	covariance	0.014493
generate cross-validated estimates for each	predict estimator x y cv	0.071429
in n_jobs even slices	func n_jobs	0.166667
compute mutual information between two variables	compute mi x y x_discrete	1.000000
gaussian process classification based on laplace approximation	gaussian process classifier laplace	1.000000
randomized search on hyper parameters	randomized search cv	0.166667
l1 and l2 regularization coefficients for w and	regularization alpha l1_ratio regularization	0.250000
the timestamp when pickling to avoid the hash	memory reduce	0.030303
nicely formatted statement displaying the function	func args kwargs object_name	0.333333
fit linear model with stochastic gradient descent	base sgdregressor fit x	1.000000
em	latent dirichlet allocation em	1.000000
returns whether	gaussian_process	0.093750
squared euclidean or frobenius norm of x	squared norm x	1.000000
the wikipedia page	utils step1 state	0.142857
graph matrix for label	label	0.045455
a tolerance which	tolerance x tol	0.058824
weighted log probabilities for each sample	samples	0.052632
data and	x y	0.010776
coefficient of determination	r2	0.076923
regression and cv and	y	0.002674
a sparse random projection matrix parameters	core base random projection fit x y	0.333333
number of splitting iterations in	get n splits x y	0.111111
the vocabulary dictionary and return term-document matrix	count vectorizer fit transform raw_documents y	0.166667
process regression model we can also predict	process regressor predict	0.500000
number of jobs which are going to	joblib multiprocessing backend effective n jobs n_jobs	0.333333
kernel x	kernel x	0.250000
b	b	1.000000
timestamp when pickling to avoid the hash	externals joblib memorized func reduce	0.050000
x parameters	x	0.003384
row	row row	0.166667
the samples x to the	x	0.003384
call predict on the estimator with	predict	0.006849
avoid the hash depending from it	externals joblib memory	0.016949
scale back the data to the original representation	robust scaler inverse transform x	0.066667
x using the model	x	0.001692
whether the kernel is stationary	pairwise kernel is stationary	1.000000
number of splitting iterations in the	leave one group out get n splits	0.111111
we don't store the timestamp when pickling to	externals joblib memorized func reduce	0.050000
for parallel processing this	joblib parallel	0.028571
matrix to dense array format	coef mixin densify	0.100000
single binary	binary	0.125000
determination regression	metrics r2	0.125000
learn the vocabulary dictionary and return term-document	vectorizer fit transform raw_documents y	0.100000
the backend and return the number	externals joblib parallel backend	0.029412
generates boolean masks corresponding to test sets	model_selection base cross validator iter test masks x	1.000000
input and compute prediction of init	ensemble base gradient boosting init decision function x	0.142857
depending on the value of verbose	verbose	0.062500
for the models computed by 'path' parameters	linear_model path residuals	0.250000
full lars path	path	0.025641
the image samples in x	x	0.001692
columns	columns x columns	0.250000
fit the model according to	linear svc fit x y sample_weight	0.250000
perform the	mixture	0.083333
axis center to the median and component	x axis	0.015385
random multilabel	make multilabel	0.333333
timestamp when pickling to avoid the hash depending	reduce	0.034483
h in multiplicative update	decomposition multiplicative update h x w h	0.250000
calibrate	core calibrated classifier	0.083333
vectors x	x	0.001692
we don't store the timestamp when pickling to	joblib memorized func reduce	0.050000
this	deep	0.230769
a minimum	min	0.045455
factorize argument checking for random	n_features	0.083333
cf tree for the input	cluster birch fit x y	0.200000
path with coordinate	enet path	0.050000
x and y is float32 then dtype	dtype x y	0.500000
a locally linear embedding analysis on the	manifold locally linear embedding x n_neighbors n_components	0.071429
input checker utility for building a cv	cv cv x	0.031250
finds seeds for mean_shift	cluster get bin seeds x bin_size min_bin_freq	0.500000
the main classification	classification	0.071429
the number of splitting iterations in the cross-validator	out get n splits x y	0.111111
the oracle approximating shrinkage covariance	covariance oas	0.083333
compute mutual information between two variables	feature_selection compute mi	1.000000
trace	decomposition trace	0.500000
class decorator for creating a class with	add	0.071429
estimate the	mixture bayesian gaussian mixture estimate	1.000000
set the sample weight array	sgd validate sample weight sample_weight n_samples	0.333333
the :ref user guide <mean_absolute_error>	y_true y_pred sample_weight multioutput	0.100000
to make cache size fit in bytes_limit	size	0.032258
of a csc matrix in-place	utils inplace swap row csc x	0.250000
number of points that will be sampled	model_selection parameter sampler len	0.333333
computing truncated svd by arpack or randomized	truncated x n_components svd_solver	0.333333
signature from the given list of parameter objects	externals signature	0.050000
wild lfw pairs dataset this	fetch lfw pairs subset	0.035714
input and compute prediction of init	boosting init decision function	0.142857
split data in	split	0.027778
and scale the data	transform x y	0.031250
graph of neighbors for points in x	neighbors radius neighbors graph x	0.500000
the covariance matrices from a given template	covariance type tied_cv covariance_type	0.333333
terminal regions	terminal	0.047619
data point	val predict estimator	0.045455
but fall back to line_search_wolfe2	wolfe12 f fprime xk pk	0.028571
display the message on stout or	joblib parallel print	0.166667
input and compute prediction of init	ensemble base gradient boosting init decision	0.142857
model according to the given	x y sample_weight	0.025974
matrix factorization	non negative factorization	0.043478
svd	n_components	0.083333
fit on the estimator with randomly drawn	randomized search cv fit x	0.500000
curve auc from prediction scores	auc score	0.052632
cv in	check cv cv x	0.031250
return the directory in which are persisted the	get output dir	0.047619
introduced by a random projection p only changes	core johnson lindenstrauss min dim n_samples	0.142857
add an	add	0.071429
check the estimator and set the base_estimator_ attribute	ensemble ada boost classifier validate estimator	0.333333
return	shape	0.011765
x and returns the labels	predict x y	0.043478
for full covariance	multivariate normal density full	0.166667
loss and gradient	loss and grad	1.000000
points in	manifold	0.100000
projection of the data onto	ridge_alpha	0.052632
generate indices to split data into	model_selection base shuffle split split x	0.250000
to a given type in	externals joblib customizable pickler register type	0.083333
search	core base search cv fit x	0.166667
the given arguments and persist	func call	0.047619
of approximate nearest	lshforest	0.125000
the maximum likelihood covariance estimator	covariance empirical covariance	0.071429
the lfw pairs dataset this operation is meant	datasets fetch lfw pairs index_file_path data_folder_path slice_ color	0.333333
class covariance	core class cov x y priors shrinkage	0.250000
depending from it	memory	0.015625
fit all transformers using	core feature union fit	0.500000
train test	core base	0.083333
compute the gradient of loss	base multilayer perceptron compute loss grad	1.000000
regression problem this dataset is described in friedman	datasets make	0.015625
the kernel k x y and	call x y	0.142857
w h whose	w h n_components	0.038462
force the execution of the function with the	memorized	0.015873
that	check	0.017857
intercept for specified layer	grad layer n_samples	0.166667
the number of splitting iterations in the	model_selection leave one out get n splits x	0.111111
lfw pairs dataset this dataset	datasets fetch lfw pairs subset	0.035714
compute minimum distances between one point and	metrics pairwise distances argmin x y	0.500000
the residues on left-out data for a full	residues x_train y_train x_test y_test	0.083333
gradient and the	x y	0.004310
first prime element in the	prime in	0.166667
perform a locally linear embedding analysis on	manifold locally linear embedding	0.062500
encoder parameters	encoder	0.125000
of exception types to	joblib parallel backend base get	0.066667
of the data onto	x ridge_alpha	0.071429
fit a binary classifier on x and y	linear_model base sgdclassifier fit binary x y alpha	1.000000
support vector classification	svc	0.111111
onto	ridge_alpha	0.052632
parallel processing this	joblib parallel	0.028571
an 'l' suffix	utils shape repr	0.013699
the lfw people dataset	datasets fetch lfw people	0.040000
the search	core base search cv	0.033333
fit linear	fit x y coef_init	0.250000
indices to split data into training and test	series split split x y groups	0.200000
list of feature	vectorizer get feature	0.200000
normalize x by	scale normalize x	0.142857
estimate the spherical wishart distribution parameters	gaussian mixture estimate wishart spherical	0.333333
and hide warnings	warnings	0.076923
log-likelihood of a	score	0.010101
given data if the estimator has been refit	base search cv	0.026316
dual gap convergence criterion the specific	dual gap emp_cov precision_ alpha	0.071429
rcv1	rcv1	0.625000
sample images for image	sample images	0.250000
mean	metrics mean	1.000000
reconfigure	configure n_jobs	0.500000
the cache for	externals joblib memorized func	0.013158
to run a	externals	0.005747
tolerance which is independent of the	tolerance x tol	0.058824
silhouette coefficient	metrics cluster silhouette	0.333333
score function	score y_true y_pred sample_weight multioutput	0.062500
lfw pairs dataset this dataset is	datasets fetch lfw pairs	0.018868
single binary	core predict binary	0.200000
initial_estimates	initial_estimates	1.000000
compute the decision	gradient boosting classifier decision	0.333333
right fileobject from	read fileobject fileobj	0.100000
full covariance matrices	normal density full	0.166667
the logistic loss and gradient	linear_model logistic loss and grad w x	0.500000
compress	compress	0.500000
the kernel k x y and	gaussian_process rbf call x y	0.200000
path length	path length graph	1.000000
matrix of patch data	feature_extraction patch extractor transform	0.200000
objective function iterating once over all	coordinate descent x	0.333333
the generative model	base pca	0.071429
returns a lower bound on model evidence	mixture dpgmmbase lower bound	0.071429
an 'l' suffix when using	utils	0.009709
for full covariance	normal density full x	0.166667
effective stop words list	feature_extraction vectorizer mixin get stop words	0.200000
along an axis on a csr	axis	0.014085
regions	regions tree	1.000000
the residual (=	ensemble binomial	0.250000
number of splitting iterations in the cross-validator parameters	kfold get n splits x y	0.111111
pickle the descriptors of a memmap instance to	joblib reduce memmap a	0.050000
and the gradient	and gradient w	1.000000
generate train test	core	0.015385
function called with the given arguments	func get output	0.125000
get the values used to update	neural_network sgdoptimizer get	0.125000
squares projection of the data onto the	x ridge_alpha	0.071429
used when memory is inefficient to	x y classes	0.027778
download the 20 newsgroups data and stored	datasets download 20newsgroups	0.200000
matrix factorization nmf find	decomposition non negative factorization	0.043478
score by cross-validation read more in the	cross val score estimator	0.166667
cache	joblib memory	0.016949
the best found	base search cv predict proba	0.076923
the linear assignment problem using the hungarian	utils linear assignment x	0.090909
estimate sample weights by class	sample	0.032258
minimum and maximum along an axis on a	utils min max axis x axis	0.333333
and compute scores for a	and score	0.333333
the score on the given data	score x	0.033333
check that the parameters are well defined	mixture check parameters x	0.166667
any axis center to the mean and	x axis	0.015385
fit the model according to the given training	linear svr fit x y sample_weight	0.250000
regression or lasso path using lars algorithm	linear_model lars path x y	0.100000
approximate nearest	lshforest kneighbors x	0.500000
an 'l' suffix when using the	shape	0.011765
actual data loading for the lfw people	lfw people	0.040000
density model on the	kernel density	0.083333
sure centering is not enabled for sparse matrices	preprocessing robust scaler check array x	0.250000
don't store the timestamp when pickling	memory reduce	0.030303
shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage	0.125000
inplace row scaling of a	utils inplace row scale x	0.142857
patches of any	patches	0.055556
n_targets	n_targets	1.000000
estimator with the best found parameters	core base search cv predict proba x	0.076923
a sparse	datasets make sparse	1.000000
check if vocabulary is empty or missing	check vocabulary	0.250000
on the estimator with the best found	model_selection base search cv predict proba	0.076923
whether the kernel is	stationary kernel mixin is	0.333333
execution only a	externals joblib	0.004762
ovr decision function	ovr decision function	1.000000
as	core isotonic	1.000000
of	parallel backend base	0.037037
of the scaler	max scaler	0.333333
setstate	setstate	1.000000
compute class covariance	core class cov x	0.250000
estimator is using a	core	0.015385
silhouette coefficient	metrics cluster silhouette score	0.250000
a platform	shape repr	0.013699
fits the shrunk	shrunk	0.043478
to avoid the hash depending from	memorized	0.015873
the content of the data home	data home data_home	0.055556
of vectors for reproducibility flips the sign of	deterministic vector sign flip	0.066667
max absolute value of	max abs	0.047619
extracts patches of	extract patches	0.083333
initial parameters	parameters x	0.125000
fit kernel ridge regression model	kernel ridge fit x	1.000000
isotonic regression model : min sum w[i]	core isotonic regression	0.055556
introduces an 'l' suffix when	shape repr	0.013699
count and smooth feature occurrences	bernoulli nb count x y	0.250000
a score by cross-validation read more in the	model_selection cross val score estimator x y	0.166667
all the vectors rows of u	flip u	0.047619
should be used when memory is inefficient to	classes	0.025641
fit a binary classifier on	base sgdclassifier fit binary	0.333333
predict_proba on the estimator with the best	cv predict	0.083333
fit on	model_selection fit	0.500000
score by cross-validation read more in	cross val score estimator x y groups	0.166667
an array with constant block diagonal structure	biclusters shape n_clusters noise minval	0.058824
build a batch of estimators within a job	build estimators n_estimators ensemble	0.166667
clear	clear	0.833333
number of splitting iterations in	model_selection base cross validator get n splits	0.125000
make sure that	utils check	0.023810
kernel k	gaussian_process constant kernel call	0.333333
product	projection	0.071429
blup parameters and evaluates the reduced likelihood function	gaussian process reduced likelihood function	0.047619
configure a copy of the	estimator append	0.142857
validate x whenever one tries to predict	forest validate x predict x	0.500000
scale back	preprocessing standard scaler inverse transform x	0.066667
cross-validated	core cross val predict estimator x y cv	0.071429
infers the dimension of	dimension	0.050000
the reduced likelihood function for the given autocorrelation	gaussian process reduced likelihood function	0.047619
density lrd the lrd	density distances_x neighbors_indices	0.200000
memmap	mmap	0.166667
x with ability to	x	0.001692
values for x relative to y_true	metrics predict scorer call estimator x y_true sample_weight	0.200000
x for	fit x y	0.005988
the position of the points in	mds fit x	0.066667
target of new samples can be different from	calibrated classifier cv	0.071429
content of the data home cache	clear data home	0.076923
of classification	core calibrated classifier	0.083333
in multiplicative update nmf	decomposition multiplicative update h x	0.500000
x_train	x_train	1.000000
derivative	derivative z delta	0.333333
wild lfw pairs dataset this dataset is a	datasets fetch lfw pairs subset	0.035714
with a given mapping	y class_mapping	0.333333
predict new data by linear interpolation	sigmoid calibration predict	1.000000
apply a mask to edges weighted	edges weights mask edges weights	0.333333
of the kernel	normalized kernel mixin	0.333333
of estimators within	estimators	0.052632
inplace row scaling of a csr or csc	utils inplace row scale x scale	0.142857
apply clustering to a projection to the	clustering affinity n_clusters n_components	0.166667
curve auc from	auc score	0.052632
assignment problem using	assignment	0.111111
generate	val predict estimator x	0.045455
introduces an 'l'	repr	0.012500
elastic net path with coordinate descent	path x	0.045455
the parameters for the voting classifier	voting classifier	0.035714
leaves	leaves	0.428571
data under each gaussian in the	mixture gmmbase	0.034483
introduces	utils shape repr	0.013699
export a	export graphviz decision_tree out_file max_depth feature_names	0.333333
perform mean	mean	0.035714
wild lfw pairs dataset this dataset	datasets fetch lfw pairs	0.018868
multiple files in svmlight format this	svmlight files files n_features dtype	0.200000
of init	base gradient boosting init decision function	0.142857
individually to unit norm vector length	x norm axis copy	0.200000
distance matrix	cluster	0.021277
cluster_std	cluster_std	1.000000
linear regression with combined l1	elastic net	0.111111
estimates for each	core cross val predict estimator x y	0.045455
pipeline after transforms	core pipeline fit	0.166667
factorization nmf find two non-negative matrices w h	non negative factorization x w h	1.000000
model to the	neural_network bernoulli rbm	0.333333
logistic	inplace logistic	0.333333
the number of splitting iterations in the	model_selection leave pgroups out get n splits	0.111111
estimates for each input data point	core cross val predict estimator	0.045455
of estimators within a	estimators n_estimators ensemble	0.083333
compute non-negative matrix factorization nmf find	factorization	0.035714
locally	manifold locally	0.333333
compute data precision matrix with the generative model	precision	0.016667
k	constant	0.142857
its spectrum spectrum	spectrum n_samples	0.166667
determinant with the fastmcd algorithm	cov det fit x y	1.000000
return posterior probabilities of classification	core quadratic discriminant analysis predict proba x	0.333333
onevsoneclassifier	core one vs one	1.000000
to	numpy	0.083333
for each input data point	core cross val	0.043478
the meta-information and the	joblib zndarray wrapper read unpickler	0.043478
coefficient matrix	coef mixin	0.090909
fit	chi2sampler fit	0.250000
predict based on an unfitted model	predict x	0.011765
data	core cross val predict estimator x	0.045455
used to compute log probabilities within a job	predict log proba estimators estimators_features x	0.250000
cv aka logit maxent classifier	cv	0.009009
the gaussian process model fitting method	gaussian_process gaussian process fit x	0.250000
random multilabel classification problem	make multilabel classification	0.166667
fit the model to data	fit x	0.012821
y as training data	y	0.005348
the given arguments	memorized func get	0.125000
text files	files	0.100000
train estimator on training subsets incrementally and	incremental fit estimator estimator x y	0.200000
distance of the samples x to	x	0.003384
on left-out data	x_train y_train x_test y_test	0.200000
error regression loss read more in the	error y_true	0.111111
fit_intercept	fit_intercept	1.000000
the model and transform	transform	0.011236
to	memory reduce	0.030303
directory	get output dir	0.047619
score on the	score x	0.033333
matrix factorization nmf find two	factorization	0.035714
dot	dot	0.625000
function used to compute log probabilities	parallel predict log proba	0.058824
the long type introduces an 'l' suffix	shape repr	0.013699
stochastic gradient descent	linear_model base sgdregressor	0.250000
of array-like or scipy	preprocessing binarize x threshold copy	0.083333
subset of dataset and properly handle kernels	utils safe split estimator x y indices	0.200000
outlyingness of observations in x according to the	outlier detection mixin predict x	0.250000
score corresponds to the area	score y_true y_score	0.025000
em update for 1 iteration	allocation em step x total_samples batch_update parallel	1.000000
scale back	scaler inverse transform	0.058824
input and compute prediction of init	ensemble base gradient boosting init decision function	0.142857
flattened log-transformed non-fixed hyperparameters	exponentiation theta	1.000000
set the intercept_	model set intercept x_offset y_offset x_scale	1.000000
prediction of init	boosting init decision	0.142857
for training	classifier	0.013699
compute mutual information between two variables	compute mi x y	1.000000
return the shortest path length from source to	single source shortest path length graph source	0.111111
force the execution of	joblib memorized	0.015625
suitable step length is not found	utils line search	0.029412
get	ensemble voting classifier get	0.200000
20 newsgroups dataset and	datasets fetch 20newsgroups	0.333333
annotation	annotation	1.000000
finds seeds for	bin seeds	0.250000
categories	categories load_content	0.500000
the number of splitting iterations in the	out get n splits	0.111111
data loading for the lfw pairs dataset	datasets fetch lfw pairs	0.018868
an estimator	estimator	0.088235
compute class	core class	0.500000
to the normalized laplacian	spectral	0.026316
a contingency matrix describing the relationship	cluster contingency matrix	0.333333
generate a mostly low rank matrix with bell-shaped	low rank matrix n_samples n_features	0.500000
and dispatch	externals joblib parallel dispatch one	0.250000
compute minimum	utils min	0.250000
building a cv	check cv cv	0.031250
in the raw documents	feature_extraction count vectorizer	0.125000
r^2 coefficient of determination regression score function	metrics r2 score y_true y_pred	0.125000
cache for the	externals joblib memorized func	0.013158
random sample from a	a size replace p	0.142857
the isotonic regression model :	core isotonic regression	0.055556
perform dbscan clustering from features or distance	cluster dbscan fit x y sample_weight	0.200000
under the curve auc	auc score	0.052632
mixin class for all cluster estimators in scikit-learn	cluster mixin	1.000000
with respect to each parameter	activations deltas	0.032258
setting the parameters for the voting classifier	voting classifier set params	0.037037
pairs dataset this operation is meant	pairs index_file_path data_folder_path slice_ color	0.333333
store the	externals joblib memory	0.016949
of array-likes and sparse matrices	utils safe sqr x copy	0.125000
fit the model using x as training	fit x skip_num_points	0.500000
check initial parameters of the derived class	check parameters	0.200000
the paired distances between x	paired distances x	0.500000
we don't store	joblib	0.014599
of the kl	manifold kl	0.333333
x	svc decision function x	0.200000
separating hyperplane	base lib svm decision function	0.333333
the dual gap convergence criterion	covariance dual gap emp_cov precision_ alpha	0.071429
features by scaling each feature to a given	min max scaler	0.083333
estimator with the best found	core base search cv	0.033333
null	null	0.750000
the number of splitting iterations in the cross-validator	group out get n splits x y	0.111111
non-negative matrices w h whose product approximates the	x w h n_components	0.038462
outlyingness of observations	covariance outlier detection mixin predict	0.250000
proba	proba	0.147059
precision the precision is the	precision	0.016667
to the cache	externals joblib memorized func	0.013158
the pairwise matrix in	parallel pairwise x y	0.166667
generate	val predict	0.045455
x and dot w h	beta divergence x w h	0.500000
check values of the basic	check initial	1.000000
remove cache folders	joblib	0.007299
grid	parameter grid	0.200000
loss function and its corresponding derivatives with respect	activations deltas	0.032258
convert coefficient matrix	sparse	0.025000
dispatch	externals joblib parallel dispatch	0.250000
warnings without visual nesting	utils ignore warnings call fn	0.200000
:ref user guide <univariate_feature_selection>	fwe	0.200000
minimum	min	0.227273
timestamp when pickling to avoid the hash depending	joblib memorized func reduce	0.050000
generate an array with	datasets make	0.031250
ordinary least squares linear regression	linear regression	1.000000
the local outlier factor of	local outlier factor decision	0.125000
list of edges for a 3d image	make edges 3d n_x n_y n_z	0.250000
an 'l' suffix when using	repr	0.012500
the similarity of two clusterings	score labels_true labels_pred sparse	0.047619
predict class labels for	ensemble voting classifier predict	0.100000
compute the boolean mask	mask	0.071429
some authors	preprocessing label binarizer	0.071429
found and raise an	utils line search	0.029412
mostly low	low	0.125000
print verbose message on the end	mixture print verbose msg init end ll	0.333333
handle	preprocessing handle	1.000000
l1-penalized covariance estimator	covariance graph lasso	0.166667
connected	connected	1.000000
initialization	beg n_init	1.000000
voting	ensemble voting	0.142857
by logistic regression and cv and linearsvc	fit liblinear x y c fit_intercept	0.142857
the curve auc using the trapezoidal	metrics auc	0.040000
documentation	doc func doc	1.000000
fit the calibrated model	calibrated classifier cv fit x y	1.000000
of the gradient	base gradient	0.250000
ridge classifier	ridge classifier	0.333333
"fits the model to the training set x	x	0.001692
all the covariance	mixture distribute covar matrix to match covariance	0.250000
and then the	x y	0.002155
loading of moved objects in six moves urllib_error	module six moves urllib error	0.333333
the hash depending	func	0.011364
the hash depending	joblib	0.014599
that can actually run in parallel	parallel	0.019231
analysis fa a	analysis	0.045455
returns the number of splitting iterations in the	predefined split get n splits x y groups	0.111111
this implementation is restricted to the binary classification	score y_true y_score average	0.076923
with a given cache key	joblib cache key	0.250000
the deviance	deviance	0.062500
fit a binary classifier on	sgdclassifier fit binary	0.333333
return_cov	return_cov	1.000000
one-vs-all fashion several regression and	y classes neg_label pos_label	0.111111
template method for updating terminal regions (=leaves)	function update terminal region tree terminal_regions leaf	0.200000
for x using the	x	0.001692
scale back	preprocessing standard scaler inverse transform x copy	0.066667
in a gaussian distributed dataset	elliptic envelope	0.166667
linear model with passive aggressive algorithm	linear_model passive aggressive classifier	0.250000
other and transforms the	y	0.002674
to avoid the hash depending	joblib memory	0.016949
type introduces an 'l' suffix when using	shape	0.011765
count and smooth feature occurrences	multinomial nb count x y	0.250000
evaluate a score	score estimator x	0.250000
returns the number of splitting iterations in the	cross validator get n splits x	0.125000
single binary estimator one-vs-one	ovo binary estimator x	0.500000
callable case for	metrics pairwise callable x	0.083333
lasso_stability_path	linear_model lasso stability path	1.000000
back	scaler inverse transform x	0.052632
the derived	x resp	0.166667
sparse uncorrelated design this dataset is described in	datasets make sparse uncorrelated n_samples n_features	0.166667
whose range approximates the range of	utils randomized range	0.083333
decision function	ensemble ada boost classifier decision function	0.166667
given args and kwargs using a list of	args kwargs	0.100000
transform feature->value dicts to array or sparse	feature_extraction dict vectorizer transform x y	0.200000
x from y	x z reg	0.066667
we don't store	joblib memorized	0.015625
an estimator implements	estimator estimator	0.052632
reconfigure the backend and	parallel backend base configure n_jobs parallel	0.500000
run fit on one set	model_selection fit grid point x y estimator	0.500000
shutdown the process or thread pool	parallel backend base terminate	0.500000
estimates for	cross val predict	0.045455
elastic net path with coordinate descent the	enet path x	0.050000
shrunk on	shrunk	0.043478
coefficient score the	score y_true	0.058824
helper to	parallel helper	0.500000
from the meta-information and the z-file	zndarray wrapper read unpickler	0.043478
function used to compute log probabilities	predict log proba	0.029412
pairs dataset this	pairs	0.111111
sure centering is not enabled for	robust scaler check array x	0.250000
log probability	log	0.018868
and predicted probabilities for a calibration curve	core calibration curve	0.142857
the curve auc using the trapezoidal rule	auc x y	0.040000
average of the decision functions of the	decision function x	0.018868
the california housing	california housing	0.083333
multilabel	multilabel	0.666667
of an unsuccessful bst search since the	n_samples_leaf	0.111111
get the parameters of the votingclassifier	get params deep	0.200000
maximizer of the reduced	gaussian_process gaussian process arg max reduced	0.200000
get the values used to update params with	sgdoptimizer get	0.125000
estimate the precisions parameters	bayesian gaussian mixture estimate precisions nk xk	0.166667
lfw pairs dataset this dataset	fetch lfw pairs	0.018868
modify	_build_utils	0.166667
generate a random	n_samples n_features n_classes n_labels	0.333333
arbitrary python object into one	value filename	0.083333
component wise scale	robust scale	0.125000
of x according to feature_range	transform x	0.016949
full covariance	normal density full x means covars min_covar	0.166667
hash depending	externals joblib	0.009524
samples in array-like x	utils num samples x	0.250000
negative	negative x	0.200000
a list of edges for a 3d image	feature_extraction make edges 3d n_x n_y n_z	0.250000
a given 1-d array	utils choice a	1.000000
with sparse uncorrelated design	make sparse uncorrelated n_samples n_features random_state	0.166667
the function call with the given	joblib format call func	0.100000
in the svmlight / libsvm format into	svmlight file f n_features	0.066667
compute area under the curve auc	metrics auc x	0.040000
c in (l1_min_c	c x y loss fit_intercept	0.030303
exponential chi-squared kernel x and y	metrics chi2 kernel x y gamma	0.333333
the hash	externals joblib memorized	0.013699
concatenates results of multiple transformer objects	feature union	0.142857
the generative	base pca	0.071429
of an estimator on a given test set	estimator x_test y_test scorer	0.500000
each input data point	predict estimator x	0.045455
compute the initial centroids parameters	cluster init centroids x k init	0.166667
inverse label binarization transformation using	preprocessing inverse binarize	0.250000
the index of the leaf	decision tree apply x	0.166667
a list of feature name -> indices mappings	feature_extraction dict vectorizer fit	0.250000
a gaussian data set with self	x_test y	0.142857
fit a binary classifier on x	base sgdclassifier fit binary x	1.000000
names and a name for	func name	0.047619
the given param_grid	cv get param iterator	0.166667
reduce x	x	0.003384
with iterative fitting along a regularization	cv	0.018018
verbose message on the end of	verbose msg init end ll	0.333333
in parallel n_jobs	n_jobs	0.023256
bytes_limit	externals joblib memory	0.016949
iteropinv	iter op inv	1.000000
fit label encoder and return encoded	label encoder fit transform y	0.200000
score on the given	score x	0.033333
k-means clustering	cluster k means x n_clusters init precompute_distances	0.500000
get the boolean mask indicating which features	selector mixin get support mask	0.333333
predict	decision function	0.025000
fit the hierarchical clustering on the	cluster agglomerative clustering fit x	0.250000
sparse uncorrelated design	make sparse uncorrelated n_samples n_features random_state	0.166667
based on a feature matrix	tree x connectivity n_components n_clusters	1.000000
random regressor	core dummy regressor	0.200000
and configure a copy of the base_estimator_ attribute	append random_state	0.142857
for the lfw pairs dataset	fetch lfw pairs	0.018868
observations	covariance outlier detection mixin	0.250000
binary metric for multilabel classification parameters	metrics average binary score binary_metric y_true y_score	0.500000
the best found parameters	base search cv	0.052632
a which this function	externals joblib memorized	0.013699
matrices	covariance_type n_features	0.500000
in the svmlight / libsvm format	svmlight file f n_features	0.066667
back the data to the original representation	robust scaler inverse transform	0.066667
nmf find two non-negative matrices w h whose	x w h	0.035714
get the values used	sgdoptimizer get	0.125000
for building a cv	cv cv x y	0.031250
by quantile this classification dataset is	datasets make	0.015625
initialized	initialized	1.000000
by cross-validation read more in the :ref user	model_selection cross val	0.250000
building a cv in a user	cv cv x y	0.031250
returns the number of splitting iterations in the	split get n splits x	0.111111
store	externals	0.011494
the sign of elements of all the	sign flip	0.066667
read the z-file	read zfile file_handle	1.000000
using x	x y	0.002155
the array corresponding to this wrapper	joblib numpy array wrapper	0.333333
using the gaussian	gaussian_process gaussian	0.250000
directory in	get output dir	0.047619
of determination regression score	r2 score y_true y_pred sample_weight	0.125000
row of x	x y	0.002155
getter	covariance	0.028986
the hash	externals joblib	0.009524
error regression loss read more in the	log error	0.142857
the logistic loss and	logistic loss and	0.500000
convert coefficient	sparse	0.025000
a with block	externals joblib	0.004762
private helper function for factorizing common	partial fit first call clf	0.200000
performs clustering on x and returns cluster labels	cluster dbscan fit predict x y sample_weight	0.166667
each input data	cross val predict estimator x	0.045455
the timestamp when pickling to	externals joblib memorized func reduce	0.050000
lad updates terminal regions	ensemble least absolute error update terminal region	0.200000
used to fit an estimator within a job	ensemble parallel fit estimator estimator x	0.333333
input	core cross	0.045455
terminal regions	terminal region	0.100000
empty the function's	memorized func clear warn	0.250000
liblinear	liblinear	1.000000
the linear assignment problem using the hungarian algorithm	utils linear assignment x	0.090909
a locally linear embedding analysis	locally linear embedding x n_neighbors	0.071429
x and returns the transformed	transform x y w h	0.500000
or thread pool	multiprocessing	0.045455
parameters	x compute_sources	1.000000
compute data precision matrix with the generative model	decomposition base pca get precision	0.066667
factorization nmf find two	non negative factorization x	0.043478
area under the curve auc using the	metrics auc x y	0.040000
bell-shaped curve of width	effective_rank tail_strength	0.125000
mse for the models computed by 'path' parameters	linear_model path residuals	0.250000
low rank matrix with bell-shaped singular values	datasets make low rank matrix	0.083333
a cv in a	check cv cv	0.031250
timestamp when pickling to avoid the hash	joblib memory reduce	0.030303
check according to li	check	0.017857
line_search_wolfe2 if suitable step length is not found	utils line search wolfe12 f fprime xk pk	1.000000
the binary classification task	score y_true y_score average sample_weight	0.076923
to the	neural_network bernoulli	0.333333
the isotonic regression model :	core isotonic regression y	0.066667
the output of transform is sometimes	transform	0.011236
calculate approximate perplexity	decomposition latent dirichlet allocation perplexity	0.333333
outlyingness of observations in x	outlier detection mixin predict x	0.250000
returns the index of the leaf	tree base decision tree apply x	0.166667
the search over parameters	search cv fit x y	0.111111
all the content of the data home cache	datasets clear data home data_home	0.076923
recompute log probabilities	core bernoulli nb update feature log prob	1.000000
performs clustering on x and returns cluster labels	core cluster mixin fit predict x y	0.500000
c in (l1_min_c	c x y	0.030303
with aligned	aligned	0.076923
is not found and	utils line search	0.029412
of classification this function returns posterior probabilities	cv predict proba	0.034483
functions of	function	0.021277
x y = (gamma <x y> + coef0)^degree	x y degree gamma	1.000000
non-negative matrix factorization nmf find two non-negative	non negative factorization x	0.043478
the scaling of	scaler inverse transform	0.029412
the search over	search	0.019231
shortest path length from source to	shortest path length graph source cutoff	0.111111
homogeneity metric of a cluster labeling given a	metrics cluster homogeneity	0.500000
fit linear model with stochastic gradient descent	linear_model base sgdregressor fit x y coef_init intercept_init	1.000000
determine the optimal batch size	parallel backend base compute batch size	1.000000
reduced likelihood function for the	reduced likelihood function	0.041667
euclidean norm of x	x	0.001692
an estimator within	estimator estimator x y	0.333333
the curve auc using the	metrics auc	0.040000
attempts to retrieve a reliable	externals joblib	0.004762
the model	model	0.058824
getter for the	empirical covariance get	0.166667
1d	1d	1.000000
a which this function is called to	externals joblib memorized func check	0.125000
partially fit underlying	one vs one classifier partial fit	0.166667
predict using the trained	neural_network base multilayer perceptron predict x	0.333333
rand index	rand	0.166667
seeds for mean_shift	cluster get bin seeds x bin_size min_bin_freq	0.500000
predicted probabilities for a calibration curve	calibration curve	0.142857
target values for x	estimator x	0.030303
the decision function	ensemble gradient boosting classifier decision function	0.166667
false positives per binary	metrics binary clf curve	0.090909
raw documents	feature_extraction count vectorizer fit raw_documents	0.125000
set the parameters of this estimator	core pipeline set params	0.500000
return parametergrid instance for the given param_grid	model_selection grid search cv get param iterator	0.166667
the dual gap convergence criterion	dual gap emp_cov precision_ alpha	0.071429
called with the given arguments	externals joblib memorized func get	0.125000
is 2d square and	tol raise_warning raise_exception	0.500000
random projection p only changes the	core johnson lindenstrauss min dim n_samples eps	0.142857
input validation for	y accept_sparse dtype	0.250000
with sparse	sparse	0.025000
private helper function for factorizing common classes param	first call clf classes	0.058824
a locally linear embedding analysis on the data	manifold locally linear embedding x n_neighbors	0.071429
from a	a size replace	0.142857
average path length of	ensemble average path length	0.090909
class weights for unbalanced datasets	class weight class_weight classes	0.500000
selected	feature_selection	0.066667
sample weights by	utils compute sample	0.100000
the number of splitting iterations in the cross-validator	one group out get n splits x y	0.111111
api and	x y	0.006466
model parameters	linear_model	0.051282
usual api and	y	0.008021
is inefficient to train all	x y classes	0.027778
one set	point	0.200000
draw randomly sampled	random_state bootstrap n_population n_samples	1.000000
process regression	process regressor	0.166667
the kddcup99	brute kddcup99	0.166667
estimates for each	y	0.002674
test_size and train_size at init	init test_size train_size	0.250000
equal to the average path	average path	0.142857
based on a feature matrix	connectivity n_components n_clusters	1.000000
the voting classifier valid parameter keys	voting classifier set	0.037037
from source to all	graph source cutoff	0.200000
perform dbscan clustering from	cluster dbscan x eps	0.200000
similarity of two sets	a b similarity	0.125000
returns a list of edges for a	feature_extraction make edges	0.066667
binary classification	y_true y_score pos_label sample_weight	0.066667
correction to raw minimum covariance determinant estimates	covariance min cov det correct covariance data	0.500000
of a csc/csr matrix in-place	utils inplace swap row	0.250000
the linear assignment problem using the	linear assignment x	0.090909
gaussian and label samples by quantile	make gaussian	0.125000
or thread	externals	0.005747
the kernel k x y and	gaussian_process matern call x y	0.200000
from features or distance matrix	y sample_weight	0.017857
decision function output for x relative to	metrics threshold scorer call clf x y sample_weight	0.058824
boolean thresholding of array-like or scipy sparse	preprocessing binarize x	0.083333
or thread	externals joblib multiprocessing backend	0.035714
estimator with the best found	search cv predict	0.074074
one-vs-one multi class libsvm in	svm one vs one	0.050000
detects the	class svm fit	0.125000
that implement the partial_fit api	utils check partial	0.038462
the pairwise matrix in	metrics parallel pairwise x y	0.166667
continuous	x y n_neighbors	1.000000
matrix factorization nmf find two	non negative factorization x	0.043478
p_ij from distances using just nearest neighbors	nn distances neighbors desired_perplexity verbose	1.000000
the precision the precision	precision	0.016667
by scaling	minmax scale x	0.142857
time under windows this is the time	time	0.047619
this implementation is restricted to the binary classification	y_true y_score average	0.076923
estimate the precisions parameters of	bayesian gaussian mixture estimate precisions nk	0.166667
using the gaussian process regression model we can	gaussian process regressor	0.055556
the model according to the given training data	sample_weight	0.037037
and false positives per binary classification	binary clf curve y_true y_score pos_label sample_weight	0.090909
detects	class svm fit	0.125000
predict confidence scores for samples	linear_model linear classifier mixin decision	0.500000
input checker utility for building a cv in	cv cv x y	0.031250
target_variables	target_variables grid x	1.000000
compute a logistic regression model	linear_model logistic regression path	0.333333
aggressive classifier read more in the :ref user	aggressive classifier	0.166667
linear model parameters	linear_model elastic net	1.000000
class at each stage for	gradient boosting classifier staged	0.500000
area under the curve auc using	auc	0.020408
of max absolute value of	max abs	0.047619
x according	x	0.003384
read value from cache and return it	externals joblib memorized result	0.500000
implementation is restricted to the binary classification	y_true y_score	0.054054
probabilities p_ij from distances using just nearest neighbors	probabilities nn distances neighbors desired_perplexity verbose	1.000000
avoid the hash depending	externals	0.011494
check that	cluster check	0.500000
projection p only changes the	core johnson lindenstrauss min dim	0.500000
the flattened log-transformed non-fixed hyperparameters	gaussian_process exponentiation theta theta	0.333333
embedding analysis	embedding	0.040000
of x	transform x y copy	0.142857
class at each stage for	ensemble gradient boosting classifier staged	0.333333
shutdown the process or thread	terminate	0.090909
shrunk ledoit-wolf covariance matrix	covariance ledoit wolf x assume_centered block_size	0.125000
convert a collection of text documents to a	vectorizer	0.022222
a reference	and shelve	0.200000
batch of estimators within a	estimators n_estimators ensemble	0.083333
compute non-negative matrix factorization nmf find	decomposition non negative factorization x	0.043478
x with ability to accept precomputed doc_topic_distr	precomp distr x doc_topic_distr	0.500000
c-support vector classification	svc	0.111111
transformed real-valued array into a hash	projection to hash	0.333333
estimator and set the base_estimator_ attribute	ensemble bagging classifier validate estimator	0.333333
evaluates the reduced likelihood function for	process reduced likelihood function	0.047619
reconfigure the	base configure n_jobs	0.500000
number of splitting iterations in the cross-validator parameters	leave pgroups out get n splits x y	0.111111
building a cv in a user	core check cv cv x y	0.031250
vocabulary dictionary and return term-document matrix	vectorizer fit transform raw_documents y	0.100000
and evaluates the reduced likelihood	gaussian_process gaussian process reduced likelihood	0.142857
for each input data	core cross val predict estimator x	0.045455
an array list sparse matrix or similar	array array accept_sparse dtype order	0.500000
a cv in	check cv cv x y	0.031250
net path with coordinate	enet path	0.050000
and compute prediction of init	gradient boosting init	0.142857
learn the vocabulary dictionary and return term-document	feature_extraction count vectorizer fit transform raw_documents y	0.166667
fit the hierarchical clustering on	cluster agglomerative clustering fit x y	0.250000
featureunion	union	0.125000
the long type introduces an 'l' suffix	utils shape repr	0.013699
from the meta-information	zndarray wrapper read unpickler	0.043478
samples by quantile this classification dataset is	datasets make	0.015625
a shuffled copy of y	y	0.002674
outlier on the training set according to	outlier factor fit	0.200000
returns the number of splitting iterations in the	base cross validator get n splits	0.125000
a	externals joblib memorized func	0.013158
class with	add	0.071429
matching pursuit	x y n_nonzero_coefs	0.500000
under the curve auc using	metrics auc x	0.040000
timestamp when pickling to avoid the	joblib memory reduce	0.030303
mkdirp	mkdirp	1.000000
measure the similarity of two clusterings	metrics cluster fowlkes mallows score labels_true labels_pred	0.333333
build a batch of estimators within	build estimators	0.166667
the logarithm of the normalization constant	logz v s dets n_features	0.200000
the score of	model_selection score	0.166667
exception	joblib parallel	0.028571
is not found and raise	search	0.019231
on training subsets incrementally	incremental fit	0.500000
orthogonal matching pursuit step	x y n_nonzero_coefs tol	0.250000
of determination	r2	0.076923
the precision is	metrics precision	0.033333
on x and membership	x z	0.050000
load datasets in the svmlight / libsvm	datasets load svmlight file f n_features dtype multilabel	0.500000
new data into the already fitted lsh forest	neighbors lshforest partial fit	0.200000
that	utils check partial fit	0.038462
the models computed by 'path' parameters	linear_model path residuals x	0.250000
score by cross-validation read more in	cross val score estimator x y	0.166667
edges for a 3d image	feature_extraction make edges 3d n_x n_y n_z	0.250000
check that predict raises an exception	utils check	0.023810
training data	regression	0.066667
call predict_log_proba on the estimator	predict log proba	0.029412
back the data to the original	standard scaler inverse transform	0.066667
truncated	n_components n_oversamples n_iter	1.000000
where tp is the	score y_true y_pred labels pos_label	0.055556
gradient	grad w x y	1.000000
everything	everything	1.000000
the residues on left-out data for	residues x_train y_train x_test y_test	0.083333
multi-class targets	core output code	0.250000
gaussian process model	gaussian_process gaussian process predict	0.500000
of all tokens in the raw documents	feature_extraction count vectorizer fit raw_documents	0.125000
fit the model to	fit	0.016287
check values of the basic parameters	mixture base mixture check initial parameters	1.000000
the timestamp when pickling to avoid the	memorized func reduce	0.050000
fit with all sets	search cv fit x y	0.111111
test vector	core	0.030769
files with categories as subfolder names	files container_path description categories	0.500000
kernel is	kernel is	0.400000
generate cross-validated estimates for each input data	x y cv	0.050000
the gaussian process regression	gaussian process regressor	0.055556
terminal regions	terminal region tree terminal_regions leaf x	0.066667
the estimator with the best found parameters	base search cv predict proba x	0.076923
with the	mixture	0.041667
each sample in x	x	0.001692
the kddcup99	datasets fetch brute kddcup99	0.166667
estimates for each input data	val predict	0.045455
samples in array-like	utils num samples	0.250000
sizes of training subsets	translate train sizes	0.066667
false positives per binary classification	metrics binary clf curve y_true y_score pos_label	0.090909
a which	externals	0.005747
covariance	match covariance	0.250000
generate a random multilabel classification	datasets make multilabel classification n_samples n_features	0.500000
gaussian process	gaussian process	0.166667
the unnormalized posterior log probability	nb joint log likelihood	0.033333
long	shape	0.011765
fit the model using x as	neighbors local outlier factor fit x	0.333333
compute the residues on left-out data for a	residues x_train y_train x_test y_test	0.083333
function to a	externals joblib customizable	0.200000
its spectrum spectrum	spectrum	0.090909
row scaling of a csr or csc	row	0.066667
absolute sizes of	translate train sizes	0.066667
don't store the	externals joblib memorized	0.013699
for each	core	0.015385
hyperparameters	kernel	0.015625
using	shape	0.011765
the decision function	ada boost classifier decision function	0.166667
estimators that implement the partial_fit api need to	utils check partial	0.038462
the weighted graph of neighbors for points in	neighbors graph	0.066667
object	write to_write filename	1.000000
timestamp when pickling	joblib memorized func reduce	0.050000
the number of splitting iterations in the	model_selection base cross validator get n splits	0.125000
an array shape under python 2	utils shape repr shape	0.166667
perform classification on samples in	svm one class svm predict	1.000000
predict using	predict	0.020548
of possible outcomes	voting	0.066667
building a cv in a user friendly way	check cv cv x y classifier	0.031250
lower bound for the	bound means	0.250000
neighbors for	neighbors	0.054054
when using the	utils	0.009709
orthogonal matching pursuit step	omp x y n_nonzero_coefs tol	0.500000
restricted to the binary classification task	recall curve y_true	0.142857
generate an array with block checkerboard structure for	datasets make checkerboard shape n_clusters noise minval	0.500000
memory is inefficient to train all	classes	0.025641
score by cross-validation read more in the :ref	cross val score	0.166667
error of the	error	0.020000
list of regularization	y pos_class cs	0.166667
diagonal of	diag	0.156250
for c in	c x	0.030303
fit a multi-class classifier by	linear_model base sgdclassifier fit	0.076923
right fileobject from	joblib read fileobject	0.100000
training and test set	x y groups	0.500000
that for c in (l1_min_c	c x	0.030303
predict multi-class targets using underlying	core output code classifier predict	0.250000
find two non-negative matrices w h whose product	x w h n_components	0.038462
memmap instance to reopen on same	reduce memmap	0.166667
regression cv	regression cv	0.200000
computes the gradient and	w x y alpha	0.250000
computes the logistic loss and gradient	logistic loss and grad w x y alpha	1.000000
x into a matrix of patch data	patch extractor transform x	0.500000
rows of a csc matrix in-place	utils inplace swap row csc	0.250000
cpus	cpu count	0.333333
update the dense dictionary	update dict dictionary	0.333333
compute class covariance matrix	class cov x y priors shrinkage	0.250000
set the sample weight array	linear_model base sgd validate sample weight sample_weight n_samples	0.333333
svmlight / libsvm format into sparse csr	svmlight file f n_features dtype	0.066667
precisions	precisions precisions	0.250000
the shrunk ledoit-wolf covariance	covariance ledoit wolf	0.125000
exception	externals joblib	0.004762
a func	externals joblib sequential backend apply async func	0.250000
y_train	y_train	1.000000
reconfigure	configure	0.142857
read more in the :ref user guide <classification_report>	y_true y_pred labels target_names	0.200000
true and false positives per binary classification	metrics binary clf curve y_true y_score	0.090909
density model on the	neighbors kernel density	0.090909
outlyingness of observations in x	covariance outlier detection mixin predict x	0.250000
kernel k x	matern call x	0.200000
cache folders	memory	0.015625
normalize x by	cluster scale normalize x	0.142857
the given observations	covariance outlier detection mixin	0.250000
under the curve auc	metrics auc x y	0.040000
fit the hierarchical clustering	cluster feature agglomeration fit x	0.250000
to split data into	base kfold split	0.250000
edges weighted	edges	0.047619
and false positives per	clf curve y_true	0.250000
boosted classifier/regressor from the training set x y	ensemble base weight boosting fit x y	1.000000
datasets in	datasets	0.015152
prf	prf	1.000000
windows this is the time it take to	externals joblib squeeze time t	0.200000
forests of trees	forest	0.076923
images for	images	0.111111
predict the class labels for	classifier predict	0.200000
returns the number of splitting iterations in	model_selection cviterable wrapper get n splits x	0.111111
input validation	x y accept_sparse dtype	0.250000
compute the median and	fit x y	0.005988
of edges for a 3d image	feature_extraction make edges 3d n_x n_y n_z	0.250000
state of the	state	0.066667
draw randomly sampled indices	generate indices random_state bootstrap n_population n_samples	1.000000
lad updates terminal regions to median	least absolute error update terminal region tree terminal_regions	0.200000
print verbose message on initialization	base mixture print verbose msg init beg n_init	1.000000
of determination regression score function	r2 score y_true y_pred sample_weight multioutput	0.125000
reducing the size of	items root_path	0.066667
shrunk covariance model according to the given	covariance shrunk covariance fit x	0.083333
for each input	cross val	0.038462
samples in	predict	0.006849
each	core	0.015385
the gradient	ensemble base gradient	0.666667
backend	parallel backend	0.030303
logistic loss and	linear_model logistic loss and	0.500000
of csgraph inputs	csgraph	0.111111
estimates for each input data	core cross val predict estimator x y	0.045455
model according to	sample_weight	0.037037
fit estimator and predict	fit and predict estimator	1.000000
predict based on an unfitted model by	predict x	0.011765
list	joblib parallel backend base	0.058824
exception types to be captured	joblib parallel backend base get exceptions	0.166667
python object into one	joblib dump value filename	0.083333
maximizer of the	process arg max	0.047619
a single tree	trees tree forest	0.142857
the kernel k x y and	gaussian_process white kernel call x y	0.333333
rand index adjusted for chance	cluster adjusted rand score	0.333333
leaf	tree apply	0.166667
compute logistic loss for classification	neural_network log loss y_true y_prob	1.000000
to	externals joblib parallel backend base get	0.066667
to	base	0.014286
transformed real-valued array into	projection to	0.166667
feature ranking with recursive feature elimination	rfe	0.100000
possible score is 1	score y_true y_pred sample_weight	0.062500
run in parallel n_jobs is the is the	n_jobs	0.023256
fit the calibrated model parameters	core calibrated classifier cv fit x y sample_weight	1.000000
initialization of the mixture parameters	gaussian mixture initialize x resp	1.000000
get the	model get	0.500000
repeated splits for an arbitrary randomized cv	repeated splits	0.125000
as the maximizer of the reduced likelihood	arg max reduced likelihood	0.250000
generate a random n-class classification	make classification n_samples n_features n_informative n_redundant	0.500000
for building a cv in a user	cv cv x y	0.031250
orthogonal matching pursuit step	y n_nonzero_coefs tol	0.250000
lrd the lrd of a sample is	distances_x neighbors_indices	0.047619
a	shape	0.011765
the wild lfw pairs dataset this	datasets fetch lfw pairs subset	0.035714
c in (l1_min_c infinity)	c	0.022222
the voting classifier valid parameter keys can be	ensemble voting classifier set params	0.037037
aggressive algorithm	aggressive classifier	0.166667
input and compute prediction of init	base gradient boosting init decision function	0.142857
for x relative to	metrics threshold scorer call clf x y	0.058824
the cache for	memorized	0.015873
number of workers	configure n_jobs parallel	0.200000
w h whose product	w h	0.031250
number of splitting iterations in	out get n splits x y	0.111111
spmatrix	spmatrix	1.000000
detects the soft boundary of the	class svm fit	0.125000
estimator on training subsets incrementally and compute	incremental fit estimator estimator x y classes	0.200000
used when memory is inefficient to	y classes	0.027778
neighbors	radius neighbors	0.130435
scaler	max abs scaler	0.250000
get number	utils get n	0.500000
n_neighbors	n_neighbors	0.625000
training set x	predict x	0.011765
return a tolerance which	cluster tolerance x tol	0.058824
fit the calibrated model parameters	core calibrated classifier cv fit	1.000000
we don't store the	memorized	0.015873
print verbose message on the end of	mixture print verbose msg init end ll	0.333333
with constant block diagonal structure for	biclusters shape n_clusters noise minval	0.058824
write array bytes to pickler file handle	wrapper write array array pickler	0.333333
platform independent representation	utils	0.009709
fit the model using	factor fit	0.062500
coefficient matrix to	linear_model	0.025641
underlying estimators should	core one vs one classifier	0.111111
cache	reduce	0.017241
the kernel k	gaussian_process white kernel	0.333333
rows of a csc/csr matrix	utils inplace	0.250000
signal	signal	0.857143
x y as	x y xy	0.333333
a full lars path parameters	path	0.025641
x and	x y sample_weight	0.012987
pickling	externals joblib reduce	0.333333
the maximum likelihood covariance estimator	covariance empirical covariance x	0.166667
constructor store the useful information for later	externals joblib zndarray wrapper init filename init_args state	0.200000
find the first prime element	hungarian state find prime	0.500000
of new samples can be different from the	calibrated classifier	0.083333
the curve auc using the trapezoidal rule this	auc	0.020408
in the svmlight / libsvm format into	svmlight file f n_features dtype multilabel	0.066667
estimates for each input data point	core cross	0.045455
the curve auc from prediction scores note this	metrics roc auc score	0.166667
x and returns the	predict x y	0.043478
target variable	y discrete_features	1.000000
distance	distance	1.000000
of the function called with the given arguments	joblib memorized func get	0.125000
convert coefficient matrix	mixin	0.037037
gcv	gcv	0.833333
function cache	joblib memorized func	0.014706
exception	joblib parallel backend base get	0.066667
to sparse format	linear_model sparse coef	0.076923
by cross-validation read more in the :ref user	cross val	0.038462
check initial parameters of	check parameters	0.200000
this dataset is described in friedman [1] and	datasets	0.015152
boost using	boost	0.062500
main classification metrics read more	metrics classification	0.052632
the output of transform is sometimes referred	transform	0.011236
back the data to the	standard scaler inverse transform	0.066667
path with coordinate descent	enet path x	0.050000
split data into	model_selection time series split split x	0.250000
for each input data point	predict	0.006849
function used to partition	ensemble partition	0.200000
estimate sample weights by	utils compute sample	0.100000
gradient boosting for regression	gradient boosting regressor	0.500000
a transform function to portion of selected features	transform selected x transform selected copy	0.333333
returns the submatrix corresponding to	mixin get submatrix	0.166667
returns the number of splitting iterations in	model_selection cviterable wrapper get n splits	0.111111
the feature	feature	0.055556
column class distributions parameters	classes class_probability	0.166667
neighbors for points in	mixin radius neighbors	0.125000
warning used when the metric is invalid	undefined metric warning	1.000000
assignment problem using the hungarian	assignment x	0.250000
people dataset	people	0.100000
returns the score on the given	score x	0.033333
lsqr	lsqr	1.000000
the depth	previous func code	0.333333
split data according to a	split	0.027778
lower bound on model evidence	mixture dpgmmbase lower bound	0.071429
remove cache	joblib memory reduce	0.030303
get	func get	0.100000
true and predicted probabilities for a calibration curve	calibration curve y_true y_prob normalize	0.142857
the neighbors within a given radius of	lshforest radius neighbors x radius	0.142857
vectors	dummy regressor	0.333333
eval function func with arguments *args	eval func	0.166667
incrementally fit	core multi output regressor partial fit	0.200000
returns the number of splitting iterations in	base kfold get n splits x y	0.111111
type introduces an 'l' suffix when using	shape repr	0.013699
gaussian random matrix	gaussian random matrix n_components	1.000000
a cv in	check cv cv	0.031250
introduces an 'l' suffix	shape	0.011765
path length from source to	path length graph source	0.200000
in the specified row	in row row	0.250000
fit linear model with passive aggressive algorithm	linear_model passive aggressive classifier fit x y coef_init	1.000000
check a	mixture check	0.142857
nmf	h beta_loss	0.333333
input	sgd	0.166667
partially fit a single binary estimator	core partial fit binary estimator	1.000000
already fitted lsh forest	neighbors lshforest partial fit x y	0.200000
regression score	score y_true y_pred sample_weight	0.062500
predicted target values for x relative	predict scorer call estimator x	0.166667
bytes_limit	joblib memory	0.016949
j	j	1.000000
fit	fit parameter estimator	1.000000
persist an arbitrary python object into	joblib dump value filename compress protocol	0.250000
minimum covariance	covariance	0.014493
of the kl divergence	kl divergence	0.083333
read array from unpickler file handle	externals joblib numpy array wrapper read array unpickler	1.000000
factorization nmf	factorization x	0.043478
of the kernel k	white kernel	0.250000
whose range approximates the range	randomized range finder	0.083333
neighbors within a given radius of a point	radius neighbors x radius	0.142857
callable that handles preprocessing and tokenization	feature_extraction vectorizer mixin build analyzer	0.333333
returns a list of feature	dict vectorizer get feature	0.200000
in pipeline after transforms	pipeline fit	0.166667
the test_size and train_size at init	model_selection validate shuffle split init test_size train_size	0.250000
function cache result and return	joblib memorized func	0.014706
compute l1 and l2 regularization coefficients	decomposition compute regularization alpha l1_ratio regularization	0.333333
mean shift clustering using a flat	mean shift	0.125000
"returns the mean accuracy	multi output classifier score	0.250000
submatrix corresponding to bicluster	core bicluster mixin get submatrix	0.333333
in	compress	0.100000
the cholesky decomposition of	cholesky	0.083333
in parallel	ensemble parallel	0.250000
value	value	1.000000
case method='lasso' is	xy gram	0.090909
the average	average	0.066667
build a batch of estimators within a	build estimators n_estimators	0.166667
a mask to edges weighted	edges weights mask edges weights	0.333333
function of the	function x raw_values	0.250000
sample weights by class for unbalanced datasets	compute sample weight class_weight y	0.500000
the reduced	process reduced	0.125000
linear assignment problem using the hungarian	utils linear assignment	0.090909
i_w	i_w	1.000000
test indices	core base shuffle split iter indices	0.250000
fit for one mini-batch	fit	0.003257
random sample from	size replace p	0.125000
binarization transformation using thresholding	binarize thresholding y	1.000000
on the estimator with the best	cv predict proba x	0.068966
fit label encoder and return encoded labels parameters	preprocessing label encoder fit transform y	0.200000
back the data to the original representation parameters	robust scaler inverse transform x	0.066667
of x and	beta divergence x	0.250000
clustering on	cluster	0.021277
check x format and make sure no	latent dirichlet allocation check non neg array	0.500000
from source to all	graph source	0.200000
the data onto the	ridge_alpha	0.052632
train test	split	0.027778
all meta	meta estimator	0.062500
getter for the precision matrix	precision	0.016667
pairwise matrix	pairwise x y func	0.166667
private function used to build a	ensemble parallel build	0.047619
set x and returns the labels	x y	0.002155
we	externals joblib memorized	0.013699
the kernel is	is	0.066667
zero row of x to unit norm parameters	preprocessing normalizer transform x y	0.250000
update the dense	decomposition update	0.125000
lfw pairs dataset	fetch lfw pairs	0.037736
sizes	train sizes	0.066667
generate cross-validated estimates for each input	core cross val predict estimator x y cv	0.071429
perform dbscan clustering from vector array or	cluster dbscan x eps min_samples	0.200000
of the derived	resp	0.090909
of new samples can be different from the	core calibrated classifier	0.083333
return the shortest path length from source	source shortest path length graph source	0.111111
a locally linear embedding	locally linear embedding x n_neighbors n_components	0.071429
the average path	average path	0.142857
don't store the timestamp when pickling to	externals joblib memory reduce	0.030303
fit x into an embedded space and	manifold tsne fit transform x y	0.500000
cs	cs	1.000000
minimum covariance determinant matrix	covariance fast mcd x support_fraction cov_computation_method	0.250000
fit the kernel density model	neighbors kernel density fit	0.250000
checker utility for building a cv	cv cv x y	0.031250
download the 20 newsgroups data and	download 20newsgroups	0.200000
returns the number of splitting iterations in	kfold get n splits	0.111111
fit the model using x as training	neighbors unsupervised mixin fit x	0.500000
generate random	n_samples random_state	1.000000
std to be	preprocessing standard	0.250000
model and transform with the	transform	0.011236
module_path	module_path	1.000000
compute the deviance	deviance call	0.333333
list of edges for a 3d image	edges 3d n_x n_y n_z	0.250000
scaling	scaling	1.000000
the isotonic regression model : min sum w[i]	isotonic regression	0.055556
strip the headers by removing everything before	datasets strip	0.076923
count and smooth feature occurrences	core bernoulli nb count x y	0.250000
function of x	function x	0.121212
fit linear model	linear_model base sgdclassifier fit x y	0.333333
helper function to	helper alpha y	0.333333
format	linear_model	0.025641
algorithms	n_components init eps	1.000000
fit the model using x as training data	neighbors local outlier factor fit x y	0.333333
posterior probabilities of classification	calibrated classifier cv predict proba x	0.200000
indices to split data in	split	0.027778
compute the boolean mask x	get mask x	0.333333
check x format and	check	0.017857
fit a multi-class	sgdclassifier fit	0.076923
x and y read	x y	0.002155
the normalization constant	logz v s dets n_features	0.200000
x from y along	x z	0.050000
format check	dirichlet allocation check	0.062500
check if the test/test sizes are meaningful wrt	validate shuffle split n_samples test_size train_size	0.111111
in multiplicative update	multiplicative update w x	1.000000
filters the given args and kwargs using a	externals joblib filter args func ignore_lst args kwargs	0.333333
svc	svc	0.555556
returns the number of splitting iterations in	base kfold get n splits x y groups	0.111111
elastic net optimization	l1_ratio	0.030303
transform the	transform x	0.016949
global clustering for the	birch global clustering	0.142857
data_folder_path	data_folder_path	1.000000
nmf find two non-negative matrices w	x w	0.083333
cross-validated score with	score estimator x y cv	0.083333
graphlasso covariance model to	covariance graph lasso cv	0.111111
the long	utils	0.009709
indices to split time series data samples	time series split	0.250000
reconstruct the array	read unpickler	0.200000
checker utility for building a cv in	core check cv cv x y	0.031250
scale back the data to the	robust scaler inverse transform x	0.066667
by logistic regression and cv and	y	0.002674
raw file object e g created with open	raw file	0.200000
tsne	tsne	0.833333
a batch of estimators within a	estimators	0.052632
boosted classifier from the training set	ensemble ada boost classifier fit	0.500000
the shortest path length from source	source shortest path length graph source cutoff	0.111111
objective function iterating once over all	coordinate descent	0.333333
search	base search cv fit x y	0.166667
for the labeled faces in the wild lfw	fetch lfw	0.041667
finds the k-neighbors	neighbors kneighbors mixin kneighbors x n_neighbors return_distance	0.250000
batch and dispatch	parallel dispatch one batch	0.500000
receiver operating characteristic roc note this implementation	metrics roc curve	0.142857
arguments	externals joblib memorized func	0.013158
to avoid the hash depending from it	memory	0.015625
generate train test	base shuffle	0.166667
loading for the lfw pairs	datasets fetch lfw pairs	0.018868
the residual (= negative gradient)	ensemble binomial deviance negative gradient y pred	0.333333
for each	cross val predict estimator x y	0.045455
a given cache	externals joblib cache	0.250000
gram matrix	gram omp gram	0.500000
for each input	core cross val predict estimator x y	0.045455
where tp is the number	score y_true y_pred labels pos_label	0.055556
the k-neighbors of a	kneighbors mixin kneighbors x	0.125000
limit	limit	1.000000
estimate class	compute class	0.166667
curve auc using the trapezoidal rule this	metrics auc x y	0.040000
pursuit omp solves n_targets orthogonal	linear_model orthogonal mp	0.250000
data	val predict estimator x	0.045455
this classification dataset is constructed by	datasets make	0.015625
exception	externals joblib parallel backend base	0.034483
the process or thread pool	multiprocessing	0.045455
the k-neighbors of a point	kneighbors mixin kneighbors x n_neighbors	0.125000
for the data samples in x	x	0.001692
the data onto	x ridge_alpha	0.071429
the mean silhouette coefficient	metrics cluster silhouette score	0.250000
the l1 distances between the	metrics manhattan distances	0.083333
of transform	transform y	0.023256
this classification dataset is	datasets make	0.015625
all arrays have consistent first dimensions	consistent length	1.000000
get the boolean mask indicating which features	mixin get support mask	0.333333
maximum likelihood estimator covariance model according	covariance empirical covariance fit	0.166667
generate cross-validated estimates	val predict estimator x y cv	0.071429
is equal to the average path length	average path length	0.090909
check initial parameters	check parameters x	0.200000
cache for the function	joblib memorized func	0.014706
learn vocabulary and idf return term-document matrix	raw_documents y	0.250000
computes multidimensional scaling using smacof algorithm parameters	manifold smacof single dissimilarities metric n_components init	0.333333
indices in sorted array of	neighbors find matching indices tree bin_x left_mask right_mask	0.166667
data x which should	x y	0.002155
csgraph	utils sparsetools validate graph csgraph	0.250000
and scale the data	scaler transform x y	0.200000
maximize class separation	core linear discriminant analysis transform	0.250000
transform function to portion of selected features parameters	transform selected	0.333333
back the data to	standard scaler inverse transform x copy	0.066667
inplace row scaling of	inplace row scale x scale	0.142857
loading of moved objects in six moves urllib_parse	module six moves urllib parse	0.333333
graph of neighbors for	neighbors radius neighbors graph	0.066667
determination	r2	0.076923
do nothing	feature_extraction	0.037037
the derivative of the logistic sigmoid function	neural_network inplace logistic derivative z delta	0.166667
for indices increasingly apart	externals joblib verbosity filter index	0.055556
compute class covariance	class cov x y	0.250000
inplace column scaling of a csc/csr	utils inplace column scale x	0.166667
apply transforms and predict_log_proba of the	predict log proba x	0.045455
used to compute log probabilities within a	parallel predict log proba	0.058824
input validation for standard estimators	x y accept_sparse dtype	0.250000
mapping from feature integer indices to feature name	feature_extraction count vectorizer get feature names	1.000000
norms	norms	1.000000
nonzero componentwise l1 cross-distances between the	gaussian_process l1 cross distances	0.111111
predict multi-output variable using a model trained	core multi output estimator predict x	0.166667
apply clustering	spectral clustering affinity n_clusters n_components	0.166667
additive	additive	1.000000
given type in the	pickler register type	0.333333
estimate the precisions parameters of the	mixture bayesian gaussian mixture estimate precisions nk xk	0.166667
the validity	params x metric p metric_params	0.100000
classifier valid parameter keys can	classifier set	0.125000
the query based on	neighbors query	0.333333
prediction of init	ensemble base gradient boosting init decision function	0.142857
class for all kernel operators	kernel operator	0.142857
score is	score y_true y_pred sample_weight multioutput	0.062500
long	utils shape	0.013699
one-vs-one multi class libsvm in the case	one vs one	0.050000
sizes of training	translate train sizes	0.066667
one-vs-one multi class libsvm in the	svm one vs one	0.050000
in svmlight format this function	svmlight	0.050000
fit	linear svc fit x y	0.333333
the variational	mixture dpgmmbase	0.416667
to partition estimators between jobs	ensemble partition estimators n_estimators n_jobs	0.200000
validation on an array	check array array	0.250000
org data set	dataname target_name data_name transpose_data	1.000000
handle the callable case for pairwise_{distances	callable x y metric	0.083333
cross-validated estimates for each input data	val predict estimator x y cv	0.071429
check input and compute prediction of init	ensemble base gradient boosting init decision	0.142857
recall the recall is the	recall	0.028571
normalize x according to kluger's	normalize x	0.076923
list of	joblib parallel backend base	0.058824
a cv in a user friendly	check cv cv x y classifier	0.031250
build a contingency matrix describing the relationship between	metrics cluster contingency matrix	0.200000
a lower bound on model	lower bound	0.071429
internally used	imgs file_paths slice_ color resize	1.000000
break the pairwise matrix	pairwise x	0.166667
number of splitting iterations in the	model_selection leave one out get n splits	0.111111
parameters theta as the maximizer of the reduced	arg max reduced	0.200000
the grid	parameter grid	0.200000
force the	externals joblib memorized	0.013699
call with the given arguments	externals joblib format call	0.200000
fit the hierarchical clustering	cluster feature agglomeration fit x y	0.250000
determine absolute sizes	train sizes	0.066667
indicating which	support	0.125000
return a shuffled copy of y	y	0.002674
the paired cosine	paired cosine	0.333333
to fit an estimator	fit estimator estimator x y	0.071429
for the lfw people dataset	lfw people	0.040000
this function returns posterior probabilities of	cv predict proba	0.034483
split data into training and test	model_selection base kfold split x y groups	0.200000
building a cv in a	core check cv cv x y classifier	0.031250
true and false positives per binary classification	binary clf curve y_true y_score	0.090909
that simply predicts zero	zero	0.166667
point	cross val predict estimator x y	0.045455
determine the optimal batch	joblib parallel backend base compute batch	1.000000
deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call y	0.333333
to avoid the hash	joblib memorized	0.015625
predict multi-output variable using a model	core multi output estimator predict	0.166667
parameters of this kernel	params deep	0.111111
a random multilabel classification problem	make multilabel classification	0.166667
x	call estimator x	0.166667
file-like object until size bytes are read	read bytes fp size error_template	0.500000
scaling features	scaler transform	0.500000
outlier factor of	outlier factor decision	0.500000
in hastie et al	datasets make hastie 10 2 n_samples random_state	0.166667
types to be captured	exceptions	0.083333
rank matrix with bell-shaped singular values most of	rank matrix	0.166667
in	joblib memory reduce	0.030303
embedding space	manifold	0.100000
avoid the hash	joblib memorized func	0.014706
problem this dataset is described in friedman	datasets	0.015152
we don't store the timestamp when pickling	joblib memory reduce	0.030303
sample weight array	sgd validate sample weight sample_weight	0.333333
return staged predictions for x	ensemble ada boost classifier staged predict x	1.000000
this score corresponds to	score y_true y_score	0.025000
predict multi-class targets	core output code classifier predict x	0.250000
with passive aggressive algorithm	passive aggressive regressor partial	1.000000
median absolute error regression loss read more	median absolute error	0.166667
break the pairwise matrix	parallel pairwise x y func	0.166667
mono and multi-outputs	x y eps n_alphas	0.250000
covariance determinant matrix	covariance fast mcd x	0.250000
shuffled copy of y	y	0.002674
and return a reference	and shelve	0.200000
fit a	base sgdclassifier fit	0.153846
model and transform	transform x	0.016949
call with the	joblib format call	0.200000
number of splitting iterations in the	one group out get n splits	0.111111
load the kddcup99 dataset downloading it if	kddcup99 subset data_home download_if_missing random_state	0.111111
subclusters	subclusters	1.000000
compute area under the curve auc using	metrics auc x y	0.040000
on an array of test vectors x	x	0.001692
given column class distributions	classes class_probability random_state	0.166667
function first to assert that	check	0.017857
get parameters of this kernel	gaussian_process compound kernel get params deep	1.000000
fit estimator and predict values	model_selection fit and predict estimator x y train	0.250000
fits the maximum likelihood estimator covariance model according	covariance empirical covariance fit x	0.166667
of the log of the determinant of a	log	0.018868
estimator on training subsets incrementally and compute scores	incremental fit estimator estimator x y classes	0.200000
a signal	signal	0.142857
finds seeds for	cluster get bin seeds	0.250000
estimates for each input data point	predict	0.006849
a reliable	externals joblib get	0.142857
for updating terminal	loss function update terminal	0.200000
initial parameters of the derived class	parameters x	0.125000
gaussian process classification	gaussian_process gaussian process classifier	0.500000
and transforms the	x y	0.002155
don't store	joblib memory	0.016949
estimates	core cross val	0.043478
of csgraph	sparsetools validate graph csgraph	0.250000
fit the model	core multi output estimator fit x	0.200000
under the curve auc using the	metrics auc x	0.040000
return a platform independent representation	utils	0.009709
covariance matrix shrunk on	covariance shrunk covariance	0.090909
estimate the	gaussian mixture estimate	1.000000
parameters for each mixture component	mixture gmmbase get covars	0.250000
type introduces an	utils shape	0.013699
shutdown the process or thread pool	multiprocessing backend terminate	0.166667
subsets and validate 'train_sizes'	train_sizes n_max_training_samples	0.200000
mapping from feature integer indices to feature name	feature names	0.090909
non-negative matrix factorization nmf find two	non negative factorization	0.043478
validation	svm base lib svm validate	0.500000
the maximum likelihood estimator covariance model according to	covariance empirical covariance fit	0.166667
predict class probabilities at each stage	classifier staged predict proba	0.500000
of the cf node	cluster birch	0.090909
p_ij from distances	distances desired_perplexity verbose	1.000000
make sure that an estimator implements the necessary	core check estimator estimator	0.142857
construct an array array of a type suitable	make int array	1.000000
log of the determinant	log	0.018868
in n_jobs	x y func n_jobs	0.166667
maximum along an axis on a csr	max axis x axis	0.142857
linear model with stochastic gradient descent	base sgdregressor	0.100000
as the maximizer of the reduced	gaussian process arg max reduced	0.200000
selected returns	feature_selection selector mixin	0.142857
fit a single binary estimator one-vs-one	fit ovo binary estimator x y	1.000000
the data onto the sparse components	decomposition sparse pca transform x ridge_alpha	0.200000
for the case method='lasso' is :	x y xy gram	0.090909
is inefficient to train all data	classes	0.025641
exponential chi-squared	metrics chi2	0.333333
logic estimators that	utils check partial	0.038462
dual gap convergence criterion the specific definition is	covariance dual gap	0.071429
load and return the breast cancer wisconsin	load breast cancer return_x_y	1.000000
autocorrelation parameters theta as the maximizer	gaussian process arg max	0.047619
compute prediction of init	init decision function	0.142857
scale back the data to the original	preprocessing robust scaler inverse transform	0.066667
from	externals joblib read	0.333333
function func with arguments *args	func	0.011364
one after the other and transforms	y	0.002674
bayesian ard regression	ardregression	0.166667
reduced	reduced	0.437500
backed	backed	0.833333
fit a multi-class classifier by combining binary	linear_model base sgdclassifier fit	0.076923
to size	size	0.032258
along an axix on a csr	axis x axis	0.083333
compute log probabilities within a job	parallel predict log proba estimators estimators_features x	0.250000
net	net	0.833333
new samples can be different from the	calibrated classifier cv	0.071429
x and perform dimensionality reduction on x	x y	0.002155
the isotonic regression model	isotonic regression	0.055556
estimate model	fit x y	0.017964
implement a single boost	classifier boost iboost x y sample_weight	1.000000
input validation for standard estimators	accept_sparse dtype	0.200000
perform mean shift	mean shift x bandwidth	0.500000
disk usage	joblib disk used path	0.250000
each	cross val predict estimator	0.045455
predict class log-probabilities for x	ensemble ada boost classifier predict log proba x	1.000000
evaluate the density model	kernel density score samples	0.250000
back the data to the original representation	inverse transform	0.062500
problem with sparse uncorrelated design	sparse uncorrelated	0.166667
check that predict	utils check estimators	0.142857
sgdclassifier	sgdclassifier	1.000000
fit to data then transform it	transformer mixin fit transform	0.500000
faces in the wild lfw pairs dataset this	fetch lfw pairs	0.018868
two rows of a csc matrix in-place	utils inplace swap row csc	0.250000
modified	modified	1.000000
bound for c such that for c in	c x	0.030303
long type introduces an 'l' suffix	utils shape	0.013699
return parametergrid instance for the given param_grid	cv get param iterator	0.166667
parameters and evaluates the reduced	gaussian_process gaussian process reduced	0.125000
mean squared error	error norm comp_cov norm scaling squared	0.166667
compute precision-recall pairs for different probability thresholds	probas_pred pos_label	0.200000
apply clustering to a projection	spectral clustering	0.142857
make_default	make_default	0.833333
a memmap instance to reopen	joblib reduce memmap a	0.050000
the callable case	metrics pairwise callable x y metric	0.083333
oracle approximating shrinkage algorithm	oas x	0.500000
set the parameters of this kernel	gaussian_process kernel set params	1.000000
the vectors rows of u such that	flip u	0.047619
from source to all reachable	source cutoff	0.200000
output	output	0.857143
pursuit omp solves n_targets orthogonal	linear_model orthogonal mp x	0.250000
and return	externals	0.005747
apply a mask to edges weighted or	feature_extraction mask edges weights mask edges weights	0.166667
projection of the data onto the	transform x ridge_alpha	0.071429
cv	check cv cv x	0.031250
set the parameters of this	set params	0.750000
the directory in which are persisted the result	get output dir	0.047619
model	fit	0.026059
with respect to coefs and	activations deltas	0.032258
we can also predict based on an	predict x	0.011765
a lower bound on	lower bound	0.071429
param logic estimators that implement the partial_fit	utils check partial fit	0.038462
to avoid the hash depending	externals joblib memorized	0.013699
wild lfw pairs dataset this dataset is	lfw pairs	0.018868
compute the	perceptron compute	0.250000
centroids on x by chunking it into mini-batches	mini batch kmeans fit x y	0.500000
w h whose	w h	0.031250
graphlasso	graph lasso cv fit	0.333333
concatenates results of multiple	feature union	0.142857
the covertype	covtype	0.125000
set x y	x y	0.008621
implementation is restricted to the binary classification task	metrics precision recall curve y_true	0.142857
full covariance matrices	full	0.055556
create a base class with a metaclass	externals with metaclass meta	1.000000
list of feature name -> indices mappings	feature_extraction dict vectorizer fit x	0.250000
partially fit underlying estimators	one vs one classifier partial fit x y	0.166667
x to the separating hyperplane	base lib svm decision function x	0.250000
of the kernel	kernel mixin	0.166667
objects that won't normally pickle	my	1.000000
requested by the	base	0.014286
check initial parameters of the	check parameters	0.200000
polynomial features parameters	polynomial features	0.333333
type introduces an 'l' suffix when	shape repr	0.013699
list of exception types	backend base get	0.066667
to run a	externals joblib parallel backend base	0.034483
construct a pipeline from the given	core make pipeline	0.250000
to build a batch of estimators within	ensemble parallel build estimators n_estimators ensemble x	0.166667
cv in	check cv cv x y	0.031250
corrcoef	corrcoef	1.000000
format check x format	allocation check	0.062500
the samples x	x	0.008460
function called with the given arguments	joblib memorized func	0.014706
with_std	with_std	1.000000
recall the recall is the ratio tp /	recall	0.028571
build a contingency matrix describing	contingency matrix labels_true labels_pred eps	0.166667
the callable case for pairwise_{distances kernels}	callable	0.058824
store the	memory	0.015625
the hash depending from	memory	0.015625
scaling features of x according to feature_range	preprocessing min max scaler transform x	1.000000
return the disk usage in a	externals joblib disk used path	0.250000
back the data	inverse transform x	0.051282
models	model	0.058824
the score for a fit across one fold	feature_selection rfe single fit rfe estimator	0.200000
breakdown	breakdown	0.750000
modified weiszfeld step	linear_model modified weiszfeld step x	1.000000
the one-vs-one multi class libsvm in	one vs one	0.050000
posterior probabilities	cv predict proba x	0.034483
orthogonal matching pursuit step on a precomputed gram	linear_model gram omp gram xy n_nonzero_coefs tol_0	1.000000
all the covariance matrices from a given template	match covariance type tied_cv covariance_type	0.333333
update terminal regions	error update terminal regions tree x	0.500000
mini-batch dictionary learning finds a dictionary	batch dictionary learning	0.142857
predict class	base decision tree predict	0.500000
remove	reduce	0.017241
there to implement the usual api and hence	decomposition sparse coder fit x y	0.142857
fit the model by computing full svd	fit full	1.000000
provided precisions	check precisions precisions covariance_type n_components	0.250000
boosting for regression	boosting regressor	0.500000
validation of y and class_weight	base lib svm validate targets y	1.000000
from prediction scores note	metrics roc	0.040000
agglomerative	agglomerative	0.714286
coefficient score	score y_true y_pred	0.038462
the lfw people dataset this operation	datasets fetch lfw people	0.040000
x and y read more in the :ref	x y	0.002155
a given radius of a point or	x radius	0.058824
area under the curve auc using the	auc x y	0.040000
classification on samples in	svm base svc	1.000000
with iterative fitting along a regularization path the	cv	0.018018
maximum along an axis on	max axis x axis	0.142857
force the execution of the function	memorized	0.015873
suffix	repr	0.012500
apply clustering to a projection	clustering affinity n_clusters n_components	0.166667
all transformers using	core feature union	0.250000
register	register	1.000000
predict_log_proba on the estimator	predict log proba	0.029412
for updating terminal regions (=leaves)	terminal region tree terminal_regions leaf	0.066667
number of splitting iterations in the cross-validator parameters	predefined split get n splits	0.111111
strategy	strategy	1.000000
under the model	mixture vbgmm score samples	1.000000
inplace row scaling	utils inplace row scale	0.142857
depth a which this function	externals joblib memorized func check previous func code	0.055556
the process or thread pool	joblib multiprocessing backend	0.052632
from	size replace	0.125000
of the function called with the given arguments	externals joblib memorized func	0.013158
transform binary labels back to	inverse transform	0.031250
evaluates the reduced	gaussian process reduced	0.125000
of module names and a name for	name	0.033333
subclass of the unpickler	unpickler	0.090909
the number of splitting iterations in the	model_selection predefined split get n splits x	0.111111
this uses the benjamini-hochberg procedure	fdr	0.142857
under a size limit	externals	0.005747
predict on the estimator with the best	cv predict	0.041667
the training set according to	fit	0.003257
unfitted	unfitted name	0.142857
thresholding of array-like or scipy sparse matrix	binarize	0.045455
l1 distances between the	metrics manhattan distances	0.083333
on an array list sparse matrix or similar	utils check array array accept_sparse dtype order	0.500000
to split data into training and test	base kfold split x y groups	0.200000
the data samples in x	x	0.001692
sizes of	translate train sizes	0.066667
kernel k x y and optionally its gradient	gaussian_process matern call x y eval_gradient	0.333333
for each input data point	core cross	0.045455
cross-validated estimates for	cv	0.009009
on the training set according	factor fit	0.062500
make	make	0.250000
of two clusterings of	score labels_true labels_pred sparse	0.047619
solve the linear	utils linear	0.333333
be captured	backend base get exceptions	0.166667
x y and	dot product call x y	0.200000
as target values	neighbors supervised float mixin	0.500000
fit the model using x	fit x	0.019231
estimates for each	predict estimator	0.045455
predict using the trained	multilayer perceptron predict	0.333333
avoid the hash depending	func	0.011364
too rare or too common features	feature_extraction count vectorizer limit features x	0.250000
the shortest path length from source to	utils single source shortest path length graph source	0.111111
the huber loss and	linear_model huber loss and	0.166667
pairs	pairs subset	0.125000
the right fileobject from	externals joblib read fileobject fileobj	0.100000
mapping from feature integer indices to feature name	vectorizer get feature names	0.125000
number of splitting iterations in the	get n splits x y groups	0.111111
k-neighbors of a point	neighbors kneighbors mixin kneighbors x	0.125000
false positives per binary classification threshold	binary clf curve y_true y_score	0.090909
on the training set	factor fit predict	0.066667
for the lfw people dataset	fetch lfw people	0.040000
full	density full x	0.166667
full covariance matrices	full x means covars min_covar	0.166667
data point	cross val predict estimator x	0.045455
on the training set according	factor fit predict	0.066667
a locally linear embedding analysis on the data	locally linear embedding x n_neighbors n_components	0.071429
store	memorized func	0.016949
and evaluates the reduced likelihood function	reduced likelihood function	0.041667
handle the callable case for	pairwise callable x y metric	0.083333
partially fit underlying estimators should be	one vs one classifier partial fit x y	0.166667
file	joblib binary zlib file	0.200000
the dual gap convergence criterion the specific	dual gap emp_cov precision_	0.071429
validate x whenever one tries to predict	base decision tree validate x predict x	0.500000
to avoid	joblib	0.014599
for tests involving both blas calls and multiprocessing	multiprocessing with blas func	0.500000
thresholding of array-like or scipy sparse matrix	preprocessing binarize x threshold copy	0.083333
inverse covariance w/ cross-validated	cv	0.009009
california	fetch california	0.333333
fit a binary classifier on x and y	base sgdclassifier fit binary x y alpha c	1.000000
return parametergrid instance for the given param_grid	search cv get param iterator	0.166667
computes the weighted graph of k-neighbors for	kneighbors mixin kneighbors graph	0.250000
undo the scaling of	preprocessing min max scaler	0.200000
load the kddcup99 dataset	datasets fetch brute kddcup99	0.166667
tokens in the raw documents	feature_extraction count vectorizer fit raw_documents	0.125000
construct a featureunion from the given	core make union	0.250000
exception types to be captured	backend base get exceptions	0.166667
two rows of a csc matrix in-place	utils inplace swap row csc x m	0.250000
opening the right fileobject from	joblib read fileobject	0.100000
fit estimator and transform dataset	random trees embedding fit transform x y sample_weight	1.000000
and a name for	get func name	0.047619
the	memorized	0.031746
logistic regression randomized logistic regression works by subsampling	randomized logistic regression	0.166667
callable case for pairwise_{distances	metrics pairwise callable x y	0.083333
for x using	x	0.001692
the recall is the	metrics recall	0.033333
data point	predict estimator	0.045455
transform on the	transform	0.011236
mean shift	mean shift x bandwidth seeds bin_seeding	0.500000
list of feature name -> indices mappings	dict vectorizer fit x	0.250000
weights and bias vectors	backprop x y	0.200000
an array array	array	0.076923
centroid classifier	centroid	0.166667
a tolerance which	tolerance	0.045455
coefficient of determination regression score function	r2 score y_true y_pred sample_weight multioutput	0.125000
the deviance	deviance call y pred	0.333333
under the curve auc using the trapezoidal	auc	0.020408
return the directory in which are persisted the	output dir	0.047619
x into a	x	0.001692
regression and cv and linearsvc	fit liblinear x y c fit_intercept	0.142857
maximum absolute value	max abs	0.047619
norm vector length	preprocessing normalize x norm	1.000000
the	base	0.100000
hash	memorized func	0.016949
compute the gradient of loss	neural_network base multilayer perceptron compute loss grad	1.000000
false for indices increasingly apart	joblib verbosity filter index	0.055556
and y is	y	0.002674
compute probabilities of possible	proba	0.029412
returns the number of splitting iterations in the	one out get n splits x	0.111111
mean	incremental mean	0.166667
predict based on an	predict	0.006849
a zipped pickle	target_dir cache_path	0.142857
check x format	latent dirichlet allocation check	0.062500
coefficient matrix to	linear_model sparse coef	0.076923
tfidf	tfidf	1.000000
update w in multiplicative update nmf	decomposition multiplicative update w x w h	0.500000
variance regression score function best	variance	0.058824
convert a collection of	vectorizer	0.044444
mean and variance along an axix on a	mean variance axis x axis	0.142857
load the kddcup99 dataset downloading it if necessary	datasets fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
from svd	utils svd	0.166667
build	sample_weight check_input	0.500000
helper function for factorizing common classes	first call clf classes	0.058824
the function call with	call func	0.100000
the model by computing truncated svd	truncated x n_components	0.200000
steps 1 and 2 in the wikipedia page	utils step1 state	0.142857
generate cross-validated estimates for	val predict estimator x y cv	0.071429
compute the decision function of x	ada boost classifier decision function x	0.333333
should be used when memory is inefficient	x y classes	0.027778
check the test_size and train_size at init	model_selection validate shuffle split init test_size train_size	0.250000
transform feature->value dicts to	feature_extraction dict vectorizer transform x	0.200000
and intercept for specified layer	grad layer n_samples	0.166667
with the best found parameters	base search cv predict proba	0.076923
the given arguments and persist the output	func call	0.047619
avoid the	memorized	0.015873
kl divergence of p_ijs and	kl divergence	0.083333
transform binary labels back	binarizer inverse transform y	0.500000
transform the data and concatenate	transform x y	0.031250
coefficient matrix	sparse coef	0.071429
model	decomposition base pca get	0.071429
generate names for	core name	0.250000
a score by cross-validation read more in	model_selection cross val score estimator x y groups	0.166667
linear assignment problem using	linear assignment x	0.090909
number of splitting iterations in the cross-validator	cviterable wrapper get n splits x y	0.111111
partially fit underlying estimators should be	one vs one classifier partial fit x	0.166667
number of splitting iterations in the cross-validator	group out get n splits x y groups	0.111111
the	joblib memorized func	0.029412
labeled faces in the wild lfw pairs dataset	lfw pairs subset	0.035714
log-likelihood of a gaussian data set	covariance score	0.071429
fit estimator and predict	core fit and predict estimator	1.000000
the weighted	weighted	0.125000
introduces an 'l' suffix	utils shape repr	0.013699
number of splitting iterations in	pgroups out get n splits x	0.111111
each input data point	core	0.015385
incrementally fit the model	core multi output regressor partial fit x y	0.200000
each input data point	core cross val predict	0.045455
a nicely formatted statement displaying the function call	format call func args kwargs object_name	0.333333
a mask	mask edges weights mask	0.333333
precisions parameters of the precision distribution	precisions nk xk	0.166667
partial	partial	0.347826
to	externals joblib memorized	0.013699
of vectors for reproducibility flips	utils deterministic vector	0.076923
x y and scale if the scale	xy x y scale	0.500000
of observations in x	x	0.001692
the c and cpp	c and cpp	0.500000
matrix factorization nmf find two non-negative matrices w	negative factorization x w	0.500000
outlier on the training set according to the	outlier factor fit predict	0.200000
the dispatch table	reduce_func	0.125000
a locally linear embedding	manifold locally linear embedding x n_neighbors n_components reg	0.071429
a single tree in parallel	ensemble parallel build trees tree forest x	0.200000
white kernel	white kernel	0.250000
the log-likelihood of a gaussian data	covariance score	0.071429
checker utility for building a cv in a	core check cv cv x y classifier	0.031250
depending	externals joblib	0.009524
two columns of a csc/csr matrix in-place	utils inplace swap column x m n	0.250000
clone	clone	1.000000
fit	fit transform	0.300000
rand index adjusted	cluster adjusted rand score labels_true labels_pred	0.333333
functions	function x	0.030303
of feature name -> indices mappings	feature_extraction dict vectorizer	0.200000
rare or too common	feature_extraction count vectorizer limit	0.250000
make and configure	make estimator append random_state	0.166667
multi-output variable using a model trained for each	core multi output estimator	0.142857
of	externals joblib parallel	0.014085
note this implementation is restricted	sample_weight	0.018519
score function best possible score is 1	score y_true y_pred	0.038462
data home cache	clear data home	0.076923
on x	fit x	0.019231
file as a	externals joblib	0.004762
compute the laplacian kernel	metrics laplacian kernel	0.166667
encoder	encoder	0.875000
can actually run in parallel n_jobs is	n_jobs	0.023256
label sets	preprocessing multi label binarizer	0.200000
element	element	0.500000
partially fit underlying estimators should	core one vs one classifier partial fit	0.166667
estimate sample weights by class for unbalanced datasets	utils compute sample weight class_weight	0.500000
transform is sometimes referred to by some authors	preprocessing label binarizer transform	1.000000
predict using the trained	base multilayer perceptron predict	0.333333
for	covariance empirical covariance get	0.166667
the curve auc using the	auc	0.020408
generate train test	shuffle split iter	0.166667
voting classifier	ensemble voting classifier set	0.037037
class	core class	0.500000
predict class	ensemble bagging classifier predict	1.000000
for each input	val predict estimator x y	0.045455
regularization	x y pos_class cs	0.166667
by removing everything before the first blank line	header	0.090909
x to the separating hyperplane	svm base lib svm decision function x	0.250000
data point	val predict estimator x y	0.045455
linear	utils linear	0.333333
absolute sizes of	sizes	0.050000
weights for unbalanced datasets	weight class_weight classes y	0.333333
gibbs	gibbs	1.000000
linear models for feature selection this implements	linear model	0.090909
the callable case for	pairwise callable x y metric	0.083333
path with	path x	0.045455
descriptors of a memmap instance to	reduce memmap a	0.050000
generate an array with block checkerboard structure	datasets make checkerboard shape n_clusters noise minval	0.500000
n_components	n_components	0.500000
build a contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true labels_pred eps	0.200000
problem	n_informative n_targets	1.000000
length dimensions	dimensions rng	0.333333
for c	c x y	0.030303
run fit on the estimator	fit	0.003257
the kernel is stationary	kernel is stationary	0.400000
long	repr	0.012500
callable case for	callable x	0.083333
using ridge regression	ridge	0.071429
trees	trees	0.416667
feature names ordered by their indices	dict vectorizer get feature names	0.142857
uncompressed bytes from	joblib binary zlib	0.333333
graph of neighbors	neighbors graph	0.066667
get number of jobs	get n jobs n_jobs	0.250000
and dispatch them	parallel dispatch	0.250000
in friedman [1] and breiman [2]	make friedman3 n_samples	0.166667
exception types to	joblib parallel backend	0.045455
list of	joblib parallel backend base get	0.066667
recall is the	metrics recall	0.033333
global clustering for the subclusters	birch global clustering x	0.142857
significance of a cross-validated score with permutations	core permutation test score estimator x y cv	0.166667
general function given points on	reorder	0.071429
for reproducibility flips the sign of	utils deterministic vector sign	0.066667
lfw people dataset this	fetch lfw people	0.040000
the function called with the given arguments	func get output	0.125000
the first prime element in the	prime in	0.166667
multi-layer perceptron classifier	mlpclassifier	0.166667
the number of splitting iterations in	leave one group out get n splits x	0.111111
dual gap convergence criterion the specific definition is	dual gap emp_cov precision_	0.071429
with the given arguments	memorized func get	0.125000
shutdown the process or thread pool	externals joblib parallel backend base terminate	0.500000
the curve auc	metrics auc x	0.040000
least-squares solution to a large sparse	lsqr a	0.037037
doc	doc	1.000000
fit the model	linear svr fit	0.333333
the kernel ridge model parameters	core kernel ridge	0.500000
the given file as a	externals joblib	0.004762
the shortest path length	single source shortest path length	0.333333
matrix of patch	patch extractor	0.090909
number of splitting iterations in the cross-validator parameters	model_selection leave pgroups out get n splits x	0.111111
bound	mixture dpgmmbase bound	0.166667
and dense	x y sample_weight random_state	0.166667
evaluate the density model	neighbors kernel density score samples x	0.250000
train test	core base shuffle split iter	0.166667
posterior log probability of the	bernoulli nb joint log likelihood	0.083333
estimate the parameters of the gaussian	gaussian mixture estimate means nk	1.000000
data point	val	0.037037
invariant of compute_labels	name clusterer	0.250000
windows cannot encode some characters in filename	joblib clean win chars string	0.333333
found	core base search	0.111111
the memory	externals joblib memory	0.016949
back the	inverse transform	0.062500
estimates for each input data	cross val predict estimator x	0.045455
the	base mixture	0.111111
terminal regions to median	terminal region tree terminal_regions leaf x	0.066667
tolerance which is	cluster tolerance x tol	0.058824
distances using just nearest neighbors	nn distances neighbors	0.500000
the number of splitting iterations in the cross-validator	pgroups out get n splits x y groups	0.111111
mean squared logarithmic error	mean squared log error	0.200000
folders to	joblib memory	0.016949
an object to be persisted instead	array wrapper	0.166667
non	non	1.000000
each input data	core cross val predict estimator x y	0.045455
step4	step4	1.000000
column	column	0.583333
callable case for pairwise_{distances	metrics pairwise callable	0.083333
predict the target of new	predict	0.006849
x	xy x	1.000000
rare or too common features	feature_extraction count vectorizer limit features x	0.250000
cache folders to	joblib memory	0.016949
bounds	bounds	1.000000
split data into	series split split x	0.250000
make predictions using a single binary estimator	core predict binary estimator x	0.200000
matrix to dense array format	densify	0.066667
on the training set according to	factor fit predict	0.066667
list of feature names ordered by their	feature_extraction dict vectorizer get feature names	0.142857
for full covariance	full x means	0.166667
transforms and	x	0.001692
shrunk covariance model according	covariance shrunk covariance fit x	0.083333
naive bayes classifier	discrete nb	1.000000
least-squares solution to a large	lsqr a	0.037037
a grid of points	ensemble grid	0.111111
transform a sequence of instances to	feature_extraction feature hasher transform raw_x	0.333333
the mstep	do mstep x	0.500000
the curve auc using the trapezoidal rule	auc	0.020408
function call with	joblib format call func	0.100000
fit the model	linear svc fit x	0.333333
return a callable that handles preprocessing and tokenization	feature_extraction vectorizer mixin build analyzer	0.333333
transforms and	x y sample_weight	0.012987
of x	fit x	0.006410
lars path parameters	path	0.025641
array-like or scipy sparse	preprocessing binarize x	0.083333
forests of	forest	0.076923
a list of all	utils all	0.500000
the estimator with the best found parameters	base search cv predict	0.076923
the timestamp when pickling to	joblib memory reduce	0.030303
w	w	0.535714
x y and get	x y	0.002155
the timestamp when pickling to avoid	externals joblib memory reduce	0.030303
gradient boosting	gradient boosting	0.666667
finds seeds for	get bin seeds	0.250000
store the timestamp when pickling to avoid the	externals joblib memory reduce	0.030303
linkage agglomerative clustering	cluster linkage tree x	1.000000
weighted graph of neighbors for points in x	neighbors radius neighbors graph x	0.500000
array-like or scipy sparse matrix	preprocessing binarize x	0.083333
in n_jobs even	x y func n_jobs	0.166667
transform function to portion of selected features	transform selected copy	0.333333
folders	memory	0.015625
gaussian process regression gpr	gaussian process regressor	0.055556
disk usage in	disk used path	0.250000
observations in x according to the fitted model	x	0.001692
in the wild lfw pairs dataset this dataset	fetch lfw pairs subset	0.035714
and transforms	y	0.002674
update reporter	verbose reporter update	1.000000
model to the	neural_network bernoulli	0.333333
generative model	base pca	0.071429
the content of the data home cache	data home data_home	0.055556
global clustering for the subclusters obtained after fitting	global clustering	0.142857
feature importances	tree base decision tree feature importances	0.333333
the image from all of	from	0.045455
rand index adjusted for chance	adjusted rand score labels_true	0.333333
the directory in which are persisted	output dir	0.047619
of neighbors for	neighbors mixin radius neighbors	0.125000
estimates for	core cross val predict estimator x	0.045455
the grid	model_selection parameter grid	0.250000
a covariance matrix shrunk on the	shrunk covariance	0.090909
for each input data point	estimator x	0.030303
pairwise matrix in n_jobs even	parallel pairwise x y func n_jobs	0.111111
find the first prime element in	state find prime in	0.333333
for updating terminal regions (=leaves)	terminal region tree terminal_regions leaf x	0.066667
or thread pool and return the	externals joblib multiprocessing backend	0.035714
each input data	estimator x	0.030303
sample from the distribution p(h|v)	rbm sample hiddens v rng	1.000000
a function	externals joblib delayed function	0.200000
fit is on grid of	fit x	0.006410
creates a biclustering	base spectral fit	0.250000
c and	c and	0.500000
fit a binary	sgdclassifier fit binary	0.333333
split data into training and test set	kfold split x y groups	0.200000
indices to split data into	time series split split	0.250000
to avoid the hash	memorized func	0.016949
factorization nmf find two	decomposition non negative factorization x	0.043478
the given label sets parameters	preprocessing multi label binarizer	0.200000
fit_predict of last step in pipeline after transforms	core pipeline	0.076923
coefficient of determination regression score function	r2 score y_true	0.125000
with x	x y	0.004310
global clustering	birch global clustering x	0.142857
generate isotropic gaussian	n_samples n_features centers cluster_std	1.000000
similarity coefficient score	score y_true y_pred normalize	0.125000
number of splitting iterations in the cross-validator parameters	group out get n splits	0.111111
match	match	1.000000
compute prediction of init	init decision	0.142857
with	decomposition base pca get	0.071429
data loading for the lfw people	fetch lfw people	0.040000
a list of feature name -> indices mappings	feature_extraction dict vectorizer fit x y	0.250000
of the local outlier	neighbors local outlier	0.142857
splits for an	splits	0.083333
approximates the range of	utils randomized range finder	0.083333
validation and conversion of csgraph	sparsetools validate graph csgraph directed dtype csr_output	0.166667
x format check	decomposition latent dirichlet allocation check	0.062500
avoid the	externals joblib memorized func	0.013158
code	func code	0.200000
sets of biclusters	cluster consensus score	0.250000
output of transform is sometimes referred	transform	0.011236
find the first prime	utils hungarian state find prime	0.500000
the dual gap convergence criterion the specific definition	covariance dual gap	0.071429
assignment problem using	assignment x	0.250000
monitor	monitor	1.000000
on an array	array array accept_sparse	0.250000
an unfitted model by using the gp prior	return_std return_cov	0.142857
x and membership	x z	0.050000
the derived class	x resp	0.166667
non-negative matrices w h whose product approximates the	x w h	0.035714
estimate the spherical wishart	bayesian gaussian mixture estimate wishart spherical nk	0.333333
matching pursuit model omp parameters	matching pursuit	0.333333
for building a cv in	check cv cv x y	0.031250
attempts to retrieve a	externals	0.005747
representation of an array shape under python 2	repr shape	0.166667
to bicluster	core bicluster	0.500000
measure the similarity of two clusterings of a	cluster fowlkes mallows score labels_true labels_pred sparse	0.333333
with passive	linear_model passive	0.200000
that implement the partial_fit api need to	utils check	0.023810
orthogonal matching pursuit	n_nonzero_coefs	0.090909
used to fit an estimator within a	fit estimator estimator x	0.055556
cluster	cluster	0.212766
a random regression problem with sparse uncorrelated design	sparse uncorrelated	0.166667
project data to vectors and cluster the	spectral biclustering project and cluster data vectors	0.333333
get the	randomized linear model get	0.500000
pairs dataset this operation is meant to	pairs index_file_path data_folder_path slice_ color	0.333333
and score	score x y sample_weight	0.250000
compute gaussian log-density at	log multivariate normal density	0.500000
and false positives per binary	metrics binary clf curve y_true y_score pos_label	0.090909
list of	externals	0.005747
predict based on an unfitted	predict x	0.011765
repeated splits for an	repeated splits	0.125000
task	precision recall curve	1.000000
the model with x	x	0.003384
the	externals	0.028736
sgdregressor	sgdregressor	0.454545
a cross-validated	y cv	0.050000
the parameters that would be indth in iteration	core parameter grid getitem ind	0.333333
initial parameters of the	parameters x	0.125000
and a name for	func name	0.047619
thresholding of array-like or scipy sparse	preprocessing binarize x	0.083333
by scaling each feature to a	scale	0.033333
for the voting classifier valid parameter keys can	ensemble voting classifier set params	0.037037
a nicely formatted statement displaying the function	func args kwargs object_name	0.333333
shortest path length	utils single source shortest path length	0.333333
logsumexp	logsumexp	1.000000
graph of neighbors for points in	neighbors radius neighbors mixin radius neighbors graph	0.066667
cv in a user friendly	cv cv x y	0.031250
for full	multivariate normal density full x means covars min_covar	0.166667
multi-class targets using underlying	core output code classifier	0.250000
with	group	0.083333
function to a	externals joblib	0.004762
multilabel classification	datasets make multilabel classification	0.166667
the process	externals joblib	0.004762
not enabled	robust	0.090909
helper function to test error messages in exceptions	assert raise message exceptions message function	1.000000
a tolerance which is independent of the	cluster tolerance x	0.058824
mle	mle	1.000000
update the dense dictionary	decomposition update dict dictionary y	0.333333
function for factorizing common classes param	fit first call clf classes	0.058824
net path with coordinate descent	path	0.025641
and compute prediction of init	boosting init decision function x	0.142857
graph of neighbors	radius neighbors graph	0.066667
a reducer function to a	externals joblib customizable pickler register	0.200000
a list of edges for a	edges	0.047619
polynomial features	polynomial features	0.333333
estimate sample weights by class for	sample	0.032258
such that for c in	c x y loss fit_intercept	0.030303
extract the	externals joblib extract	0.500000
estimates for each input data	estimator x	0.030303
decorator to catch and hide warnings	utils ignore warnings	0.142857
derivative of the logistic sigmoid function	logistic derivative z delta	0.166667
cholesky decomposition	cholesky omp x	1.000000
that for c in (l1_min_c infinity) the	c	0.022222
quoting	quoting	1.000000
checker utility for building a cv in	cv cv	0.031250
jobs	jobs n_jobs	0.200000
returns the number of splitting iterations in the	kfold get n splits x	0.111111
of an array shape under python 2 the	shape repr shape	0.166667
number of splitting iterations in	model_selection base kfold get n splits	0.111111
fit the	linear svr fit x y	0.333333
with respect to each	activations deltas	0.032258
sets of biclusters	metrics cluster consensus score	0.250000
compute the grid of alpha values for	alpha grid x y	0.166667
fit kernel ridge regression model parameters	core kernel ridge fit	1.000000
estimate the precisions parameters of	gaussian mixture estimate precisions nk xk sk	0.166667
introduces	utils shape	0.013699
returns the number of splitting iterations in	model_selection leave pgroups out get n splits	0.111111
indices to split data into	base kfold split x	0.250000
shortest path	source shortest path	0.333333
model fitting	fit x y	0.005988
neighbors for	radius neighbors	0.086957
returns the number of splitting iterations in	one out get n splits	0.111111
iter_offset	iter_offset	1.000000
labeled faces in the wild lfw pairs	lfw pairs subset	0.035714
return a tolerance which is independent of the	cluster tolerance x	0.058824
of the dual gap convergence criterion	covariance dual gap emp_cov precision_ alpha	0.071429
silhouette	silhouette	1.000000
module	module	1.000000
score by cross-validation read more in the	model_selection cross val score estimator x	0.166667
an estimator within a	estimator estimator	0.052632
'l' suffix	utils shape	0.013699
folders to	joblib	0.007299
locally linear embedding analysis on the	manifold locally linear embedding x n_neighbors n_components reg	0.071429
mean shift clustering	cluster mean shift x bandwidth seeds	0.500000
windows cannot encode some characters	clean win chars string	0.333333
the	func	0.022727
empty	externals joblib memorized func clear warn	0.250000
the maximizer of the reduced likelihood	arg max reduced likelihood	0.250000
building a cv in	cv cv x	0.031250
find the least-squares solution to a large sparse	utils lsqr a	0.037037
neighbors within a given radius of a	radius neighbors x radius	0.142857
the estimator with the best found parameters	search cv predict	0.074074
determine the number of	n	0.050000
estimator	estimator x y	0.153846
tolerance which is independent of the	cluster tolerance	0.058824
more in the :ref user guide	y_true y_pred	0.111111
to polynomial features parameters	preprocessing polynomial features	0.500000
compute the decision function of	decision function x	0.018868
transformed	h	0.041667
lfw pairs dataset this operation is	fetch lfw pairs	0.018868
the dictionary 'params' parameters	core pprint params offset printer	0.250000
rand index adjusted	metrics cluster adjusted rand score	0.333333
x into a matrix	x	0.001692
perform a locally linear embedding analysis on	manifold locally linear embedding x n_neighbors n_components	0.071429
for building a cv in a	cv cv x y classifier	0.031250
precision matrix with the generative	get precision	0.052632
the process or thread pool	externals	0.005747
split data into training and test set	base kfold split x y groups	0.200000
returns the number of splitting iterations in the	out get n splits x y groups	0.111111
collection	vectorizer	0.066667
metrics should use this	metrics	0.043478
loss for	loss	0.054054
for the voting classifier valid	ensemble voting classifier set params	0.037037
a given dataset split	y scorer	0.111111
x format check x format and	decomposition latent dirichlet allocation check	0.062500
in n_jobs even	func n_jobs	0.166667
validation on an array	array array accept_sparse	0.250000
on left-out data for a full lars	x_train y_train x_test y_test	0.200000
compute data precision matrix with the generative model	get precision	0.052632
handle the callable case for pairwise_{distances	pairwise callable x y	0.083333
logic estimators that implement	utils check	0.023810
build a batch of estimators within	parallel build estimators n_estimators ensemble x y	0.166667
computes the nonzero componentwise l1 cross-distances between	gaussian_process l1 cross distances	0.111111
from	manifold spectral embedding	0.111111
the solution to a sparse coding problem	sparse encode x	0.333333
batch size	batch size	1.000000
sign of elements of all the vectors	sign flip	0.066667
sparse uncorrelated design this dataset is described in	datasets make sparse uncorrelated n_samples	0.166667
types to	get	0.012048
template method for updating terminal regions (=leaves)	function update terminal region tree terminal_regions leaf x	0.200000
process or	joblib multiprocessing backend	0.052632
set the parameters of this estimator	base estimator set params	1.000000
terminal	loss function update terminal	0.200000
eval function func with arguments *args and **kwargs,	eval func	0.166667
precision-recall pairs for different probability thresholds	probas_pred pos_label	0.200000
of x and y is float32 then	x y	0.002155
a contingency matrix describing	contingency matrix labels_true labels_pred eps sparse	0.166667
score on the	score x y	0.030303
estimates	val predict	0.045455
when using	utils shape repr	0.013699
elastic net parameter search parameters	xy l1_ratio	0.250000
matrix whose range approximates the range of	randomized range finder	0.083333
negative value in an array	negative	0.090909
elastic net path with coordinate descent the	linear_model enet path x	0.050000
for x	estimator x	0.030303
func	apply async func	0.250000
timestamp when pickling to avoid the	memorized func reduce	0.050000
minimum distances between one point and a	pairwise distances argmin x y axis	0.333333
lfw pairs dataset this operation	datasets fetch lfw pairs	0.018868
find the first prime element in the specified	state find prime in	0.333333
validate user provided precisions	check precisions precisions	0.250000
note this implementation is restricted to the	sample_weight	0.018519
check that the	check	0.017857
fit the ardregression model according to the	linear_model ardregression fit	0.250000
indices to split data into training and test	base kfold split x y groups	0.200000
compute the l1 distances between the	manhattan distances	0.083333
single	trees	0.083333
cross-validated lasso using the lars algorithm	lars cv	0.333333
the graphlasso covariance model to	covariance graph lasso cv	0.111111
raw documents	feature_extraction count vectorizer	0.125000
a locally linear embedding analysis	manifold locally linear embedding x n_neighbors n_components	0.071429
prediction	sample_weight	0.018519
for	ada boost classifier	0.666667
shortest path length from source to all	utils single source shortest path length graph source	0.111111
imputer	preprocessing imputer	0.333333
returns the number of splitting iterations in the	leave pgroups out get n splits x	0.111111
a list of all estimators from sklearn	utils all estimators include_meta_estimators include_other type_filter include_dont_test	0.500000
incrementally fit the model to	core multi output regressor partial fit	0.200000
init	gradient boosting init decision	0.142857
transform data to polynomial features	preprocessing polynomial features transform x y	0.500000
axis center	axis	0.028169
multiprocessing	safe multiprocessing	1.000000
to workaround python 2 limitations of pickling instance	obj methodname	0.111111
matrix factorization nmf find two non-negative matrices	negative factorization	0.043478
to dense array	linear_model sparse coef mixin densify	0.100000
wild lfw pairs dataset this dataset is a	fetch lfw pairs	0.018868
return a platform independent representation of	utils	0.009709
fit on the estimator with	fit x	0.006410
the number of splitting iterations in the cross-validator	pgroups out get n splits x	0.111111
content of the data home	data home	0.076923
whose range approximates the range of a	utils randomized range finder a	0.166667
kddcup99 dataset downloading it if necessary	fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
a cv in a user friendly way	check cv cv	0.031250
kernel k x y and	gaussian_process matern call x y	0.200000
the log of the determinant	log	0.018868
best found	core base search cv predict proba	0.076923
from features or distance matrix	fit x y sample_weight	0.020000
usual api and hence	decomposition sparse coder fit x y	0.142857
fall back to line_search_wolfe2 if suitable step	wolfe12 f fprime xk pk	0.028571
thresholding of array-like or scipy sparse	binarize x	0.083333
a sparse	svds a	0.166667
param	include self	1.000000
lrd the lrd of a sample	distances_x neighbors_indices	0.047619
template method for updating terminal	terminal	0.047619
check x format check	check	0.017857
objective for the case method='lasso' is :	y xy gram	0.090909
for reproducibility flips the sign	deterministic vector sign flip	0.066667
as training	xy	0.076923
corresponds to	y_true y_score	0.027027
perform classification on an	neighbors nearest centroid predict	0.142857
scale back the data	standard scaler inverse transform x	0.066667
disk usage in a directory	disk used path	0.250000
absolute error regression loss	absolute error y_true y_pred	0.142857
for full	full x	0.166667
to a large	utils lsqr a	0.037037
transform data back to its original space	decomposition randomized pca inverse transform x	1.000000
skip test if	utils check skip	0.500000
of the local outlier factor	local outlier factor decision function	0.125000
exception	base	0.014286
data and labels	classifier mixin score x y	1.000000
generate a random multilabel classification	make multilabel classification n_samples n_features n_classes n_labels	0.500000
a function	joblib delayed function	0.200000
logistic regression and cv and linearsvc	svm fit liblinear x y c fit_intercept	0.142857
approximate feature map to x	core rbfsampler transform x	0.333333
setting the parameters for the voting classifier valid	ensemble voting classifier set	0.037037
k x y and	gaussian_process matern call x y	0.200000
absolute value to be	abs	0.083333
by scaling each feature to a	preprocessing minmax scale x	0.142857
right fileobject	fileobject	0.100000
precision matrix with the	precision	0.016667
scale back the data to the original representation	preprocessing robust scaler inverse transform	0.066667
the isotonic regression model : min	isotonic regression y	0.066667
weights by class for unbalanced datasets	weight class_weight y indices	0.200000
the number of splitting iterations in the cross-validator	base cross validator get n splits	0.125000
the	empirical covariance	0.125000
factorization nmf find two non-negative matrices	negative factorization x	0.043478
of the determinant of a wishart the	mixture wishart	0.125000
models for	model	0.058824
class covariance matrix	class cov x y priors shrinkage	0.250000
scale back the data to the	preprocessing standard scaler inverse transform	0.066667
the	bayesian gaussian mixture	0.333333
rand index adjusted for chance	metrics cluster adjusted rand score labels_true	0.333333
for a given dataset split	x y scorer	0.111111
it	joblib memorized func	0.014706
convergence	convergence	1.000000
the svmlight / libsvm format into	svmlight file f n_features dtype	0.066667
predict_log_proba on the estimator with the best found	search cv predict log proba	1.000000
the california housing dataset	fetch california housing	0.083333
ensemble	ensemble base	0.333333
of init	ensemble base gradient boosting init decision function x	0.142857
file	file x	1.000000
from the decision boundary for each class	core one vs rest classifier decision function x	0.250000
from	from	0.318182
estimate class	class	0.071429
\#3" regression problem this dataset is	datasets	0.015152
outlier	outlier factor	0.166667
the model	linear model	0.090909
function is made of a shifted scaled version	mle precision_	1.000000
the test/test sizes are meaningful wrt to	shuffle split n_samples test_size train_size	0.111111
columns of a matrix	x columns	0.250000
the kernel k x y and	gaussian_process constant kernel call x y	0.333333
name for the	get func name	0.047619
wrapped function cache result and return	externals joblib memorized func	0.013158
calculate the posterior log probability	core bernoulli nb joint log likelihood	0.083333
voting classifier	ensemble voting classifier set params	0.037037
graph of neighbors for points in	mixin radius neighbors graph	0.066667
sample weights by class for unbalanced datasets	sample weight class_weight y	0.500000
implement a single boost	ada boost classifier boost iboost x y sample_weight	1.000000
the number of splitting iterations in the	kfold get n splits x y	0.111111
implement a single boost	base weight boosting boost iboost	1.000000
number of splitting iterations in the cross-validator parameters	leave one group out get n splits x	0.111111
train estimator on training	fit estimator estimator	0.111111
test	base shuffle split	0.142857
net path with coordinate descent the	enet path x	0.050000
to	externals joblib	0.033333
rand index	rand score labels_true labels_pred	1.000000
x y and	gaussian_process rbf call x y	0.200000
possible score is 1	score y_true y_pred sample_weight multioutput	0.062500
curve auc from prediction scores note	metrics roc auc	0.166667
the neighbors within a	neighbors x	0.166667
projection of the data onto the	ridge_alpha	0.052632
estimates the minimum covariance determinant matrix	covariance fast mcd	0.250000
back	inverse transform x	0.051282
or not	feature_extraction	0.037037
pursuit omp solves n_targets orthogonal matching pursuit problems	linear_model orthogonal mp x y n_nonzero_coefs tol	0.200000
hence	patch extractor	0.090909
compute the largest k singular values/vectors for a	a k ncv tol	0.166667
calculate true and false positives per binary classification	metrics binary clf curve y_true y_score pos_label	0.090909
the kddcup99 dataset downloading it if	datasets fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
used by logistic regression and cv and	y	0.002674
kernel k	constant kernel	0.250000
the data in the given file	data compress	0.100000
of the scaler	preprocessing min max scaler	0.200000
returns the score on the	score x y	0.030303
x from y along the first axis	x z	0.050000
dual gap convergence criterion the specific definition	covariance dual gap emp_cov precision_ alpha	0.071429
checker utility for building a cv in a	check cv cv x	0.031250
timestamp when pickling to avoid the hash	reduce	0.034483
lfw pairs dataset this dataset is a collection	fetch lfw pairs subset	0.035714
absolute error regression	absolute error	0.142857
python object into	externals joblib dump value filename	0.083333
the kddcup99	kddcup99	0.090909
l1-penalized covariance	covariance graph lasso	0.166667
decision boundary for each class	one vs rest classifier decision function	0.250000
reduced likelihood	gaussian process reduced likelihood	0.142857
recall is	recall	0.028571
func to	func	0.034091
func	sequential backend apply async func	0.250000
a python object from a file persisted	load filename mmap_mode	0.333333
gaussian process model	gaussian process	0.083333
generates a random sample from a	a size	0.142857
list of feature	dict vectorizer get feature	0.200000
load the kddcup99	kddcup99	0.090909
y	y max_samples max_depth	1.000000
of exception types	externals joblib parallel	0.014085
the pairwise matrix in n_jobs even	metrics parallel pairwise x y func n_jobs	0.111111
edges	edges	0.285714
'l' suffix when	utils	0.009709
generate names for estimators	name estimators estimators	0.500000
a locally linear embedding analysis on the	locally linear embedding x	0.071429
pipeline	pipeline	0.500000
predict regression target at each stage for	boosting regressor staged predict	0.500000
number of splitting iterations in the	kfold get n splits	0.111111
deterministic output from svd	svd	0.071429
updates terminal regions to median estimates	terminal region tree terminal_regions leaf x	0.066667
matrix whose range approximates the range	utils randomized range finder	0.083333
building a cv in	check cv cv x	0.031250
schedule a func to be	externals joblib sequential backend apply async func callback	0.250000
compute receiver operating characteristic roc note this implementation	metrics roc curve	0.142857
x and target y	multilayer perceptron partial	0.166667
score function	score y_true y_pred	0.038462
wild lfw pairs	fetch lfw pairs subset	0.035714
score is 1	score y_true y_pred sample_weight multioutput	0.062500
check values of the basic	mixture check initial	1.000000
kernel k x y and	dot product call x y	0.200000
relative	predict scorer call estimator	1.000000
the	decomposition base pca	0.071429
from source	graph source cutoff	0.200000
of exception types	get	0.012048
called	called	1.000000
reproducibility flips the sign of elements	utils deterministic vector sign flip	0.066667
remove	externals joblib memory reduce	0.030303
loader for the labeled faces in the wild	subset data_home	0.125000
the model according to	sample_weight	0.037037
reconstruct	feature_extraction reconstruct	0.333333
schedule a func to	externals joblib sequential backend apply async func callback	0.250000
each input data point	cross val predict estimator x	0.045455
the beta-divergence of x and dot w h	decomposition beta divergence x w h beta	0.500000
and false positives per binary classification threshold	binary clf curve y_true	0.090909
rcond	rcond	1.000000
batch_size	batch_size	0.600000
binary	binary	0.343750
the weighted graph	graph	0.063830
validation and conversion of csgraph inputs	validate graph csgraph directed dtype csr_output	0.166667
randomly drawn	randomized search cv	0.166667
optimization objective for the case method='lasso' is	x y xy gram	0.090909
to a projection to the	spectral	0.026316
get the	linear model get	0.500000
the dual gap convergence criterion	covariance dual gap	0.071429
tied covariance matrix	covariances tied resp	1.000000
compute area under the curve auc using	auc x	0.040000
density estimation read more in the	density	0.043478
infers the dimension	dimension	0.050000
and class	x y	0.002155
r^2 coefficient of determination regression score	metrics r2 score y_true y_pred sample_weight	0.125000
dense array format	sparse coef mixin densify	0.100000
for indices increasingly apart the distance depending	joblib verbosity filter index	0.055556
that for c in (l1_min_c	c	0.022222
a logistic regression model	linear_model logistic regression path x y	0.333333
bound for c such that for c	c x	0.030303
all transformers	union	0.250000
of x and	divergence x	0.250000
helper function to output	called str function_name	0.250000
remove too rare or too common features	feature_extraction count vectorizer limit features x	0.250000
the posterior log probability of the samples x	core bernoulli nb joint log likelihood x	0.500000
an 'l' suffix when using	shape repr	0.013699
generate cross-validated estimates	cross val predict estimator x y cv	0.071429
locally linear embedding analysis on the data	manifold locally linear embedding x n_neighbors n_components	0.071429
estimate the parameters of the gaussian	gaussian mixture estimate means nk xk	1.000000
log probability for	log multivariate normal density	0.250000
from	func	0.011364
log-probabilities	log proba	0.272727
recall is the ratio tp / tp +	recall	0.028571
number of splitting iterations in	predefined split get n splits x y groups	0.111111
for building a cv	core check cv cv	0.031250
a buffered	buffered	0.125000
nicely formatted statement displaying the	args kwargs object_name	0.166667
in x into a matrix of	x	0.001692
to unit norm vector length	norm axis copy	0.200000
project	project	1.000000
product with the	projection transform	0.333333
along any axis center to the median and	axis	0.014085
and binary	y	0.002674
for updating terminal	ensemble loss function update terminal	0.200000
minimum covariance determinant with the fastmcd algorithm	covariance min cov det fit x y	0.500000
of feature names ordered by their indices	feature_extraction dict vectorizer get feature names	0.142857
opposite of the local	local	0.090909
for the voting classifier valid	voting classifier	0.035714
h in multiplicative update	decomposition multiplicative update h x w h beta_loss	0.250000
center	robust scaler transform x	1.000000
global clustering for the subclusters obtained after fitting	cluster birch global clustering x	0.142857
number of splitting iterations in	leave one out get n splits x	0.111111
of array-like or scipy	binarize x	0.083333
cancer wisconsin	cancer return_x_y	1.000000
false positives per binary classification threshold	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
scale back the	standard scaler inverse transform x copy	0.066667
sparse combination of the dictionary atoms	sparse coding mixin transform x	0.333333
this implementation is restricted to the binary classification	score y_true y_score average sample_weight	0.076923
cache for	memorized func	0.016949
to avoid the	memorized func	0.016949
augment dataset with an additional dummy feature	preprocessing add dummy feature	1.000000
returns the number of splitting iterations in the	model_selection base kfold get n splits x	0.111111
and dense inputs	y sample_weight	0.017857
whether the kernel	kernel operator	0.142857
count and	multinomial nb count x y	0.250000
type introduces an 'l' suffix when using	utils	0.009709
evaluates the reduced likelihood	process reduced likelihood	0.142857
bytes_limit	reduce	0.017241
factory	factory	1.000000
range	utils randomized range	0.083333
the distribution p(h|v)	hiddens v rng	0.500000
fit estimator and predict values for a	core fit and predict estimator x y train	0.250000
anova f-value for the	feature_selection f classif x y	0.200000
the sign of elements of	sign flip	0.066667
the hash depending from it	externals joblib memorized	0.013699
the accuracy of a classification	y_true y_pred labels sample_weight	0.125000
classification used in hastie et al	make hastie 10 2 n_samples random_state	0.166667
for each	x	0.001692
radius	radius	0.227273
extract	externals joblib extract	0.500000
number of splitting iterations in the cross-validator parameters	base kfold get n splits x y	0.111111
batch and dispatch	dispatch one batch	0.500000
split data into training and test set	predefined split split x y groups	0.200000
average gradient the gradient of the loss is	loss	0.027027
read	read unpickler	0.200000
parallel	parallel build	0.047619
utility for building a cv in a	core check cv cv	0.031250
particular sample is an outlier or not	ensemble isolation forest	0.500000
the number of splitting iterations in the cross-validator	cviterable wrapper get n splits x y	0.111111
partially fit a single binary estimator one-vs-one	core partial fit ovo binary estimator x	1.000000
isolation forest algorithm return the anomaly	isolation forest	0.200000
log-det of the cholesky decomposition of matrices	log det cholesky matrix_chol covariance_type	1.000000
and dense	y sample_weight	0.017857
for full covariance	normal density full x means	0.166667
[1] and breiman [2]	friedman3 n_samples	0.166667
checker utility for building a cv in	core check cv cv x y classifier	0.031250
and false positives per binary classification	metrics binary clf curve y_true	0.090909
the number of splitting iterations in the cross-validator	cross validator get n splits x y groups	0.125000
find the first prime element	find prime	0.500000
to a file	externals joblib	0.004762
solve the linear assignment problem using the	linear assignment	0.090909
a six moves urllib namespace that resembles	module six moves urllib	0.333333
break the pairwise matrix in n_jobs even	pairwise x y func n_jobs	0.111111
a random multilabel classification	multilabel classification	0.166667
in	memory reduce	0.030303
perform mean shift	mean shift x bandwidth seeds bin_seeding	0.500000
fit the model according	quadratic discriminant analysis fit x	1.000000
_build_utils	_build_utils	0.833333
note this implementation is restricted to the	metrics roc	0.040000
median absolute error regression loss read more	metrics median absolute error	0.166667
squares	squares error	0.500000
the solution to a sparse coding problem	decomposition sparse encode	0.333333
returns the number of splitting iterations in	predefined split get n splits x y	0.111111
path with coordinate	linear_model enet path x	0.050000
incremental mean and variance along an axix on	incr mean variance axis x axis last_mean last_var	0.333333
absolute error regression loss read more	absolute error	0.142857
avoid the hash depending from	externals joblib	0.009524
input	core cross val predict estimator	0.045455
compute the decision	ada boost classifier decision	0.333333
incremental fit on a batch	gaussian nb partial fit x y classes sample_weight	0.200000
the training set x and returns the	x y	0.002155
of the cholesky decomposition	cholesky	0.083333
factorization nmf find two non-negative matrices w h	factorization x w h	1.000000
the hierarchical	agglomerative	0.142857
checkerboard structure	checkerboard shape n_clusters noise minval	0.066667
score on the given data if	score	0.010101
the samples x to the separating hyperplane	base lib svm decision function x	0.250000
for each	estimator	0.014706
the estimator with the best	cv predict	0.083333
subcluster from	subcluster new_subcluster1	0.166667
for building a cv in	check cv cv x y classifier	0.031250
lfw pairs	lfw pairs	0.037736
returns first	core first	1.000000
standardize a dataset	with_centering with_scaling	0.200000
provided precisions	precisions precisions covariance_type	0.250000
randomized linear models	randomized linear model	0.076923
matrix for label spreading computes the graph	semi_supervised label spreading build graph	0.142857
do basic checks on matrix	mixture validate covars covars covariance_type n_components	0.250000
the number of jobs	n jobs n_jobs	0.142857
platform independent	utils	0.009709
fits the shrunk covariance model according to the	covariance shrunk covariance fit x	0.083333
fit with all	search cv fit x	0.111111
the residues on left-out data	residues x_train y_train x_test y_test	0.083333
import path as a list of module	resolv_alias win_characters	0.166667
each input	core cross val predict estimator	0.045455
and	y classes	0.055556
probabilities for a calibration curve	calibration curve	0.142857
platform	utils shape	0.013699
loss and the	loss and	0.333333
performs approximate nearest neighbor search using lsh forest	lshforest	0.125000
path parameters	linear_model omp path	0.100000
with stochastic gradient descent	x y coef_init intercept_init	0.333333
the number of splitting iterations in	base cross validator get n splits x y	0.125000
gram	linear_model gram omp gram	0.500000
type introduces an 'l' suffix when	repr	0.012500
train test	base	0.014286
set the diagonal	manifold set diag	0.333333
for each input	val	0.037037
linear embedding analysis on the data	linear embedding x n_neighbors n_components	0.200000
for	predict estimator x	0.045455
for full	multivariate normal density full x	0.166667
the parameters for the voting classifier valid	voting classifier	0.035714
coverage error measure compute how far	metrics coverage error y_true y_score	0.166667
for c in (l1_min_c infinity) the model	c x	0.030303
of the normalization constant	logz v s dets n_features	0.200000
path	path x y	1.000000
memmap instance to	externals joblib reduce memmap	0.142857
the callable case for pairwise_{distances kernels}	metrics pairwise callable x y	0.083333
computes the logistic loss	logistic loss w x y alpha	1.000000
under each gaussian in	mixture gmmbase	0.034483
receiver operating characteristic roc note	metrics roc curve	0.142857
of the scaler	abs scaler	0.333333
allow_mmap	allow_mmap	1.000000
of signature bind call holds	bound arguments	0.333333
regression score function	score y_true y_pred sample_weight multioutput	0.062500
each	core cross	0.045455
a contingency matrix describing the relationship between labels	metrics cluster contingency matrix labels_true labels_pred eps sparse	0.200000
the leaves of	get leaves	0.111111
backend and return the	externals joblib parallel backend base	0.034483
pairs dataset this dataset	pairs	0.055556
solution to a sparse coding problem	sparse encode x	0.333333
standardize a dataset along any	with_mean with_std	0.200000
fit a single binary	fit binary	0.400000
timestamp when pickling to	joblib memorized func reduce	0.050000
median of	get median	0.166667
computes the exponential chi-squared kernel x and y	metrics chi2 kernel x y gamma	0.333333
cache folders	externals	0.005747
search over parameters	search cv fit	0.111111
density model on the data	density	0.043478
set the diagonal of	set diag	0.333333
pkl	pkl	1.000000
return probability estimates	nb predict proba	1.000000
full	normal density full	0.166667
version of a	fobj	0.125000
fit the model from	manifold isomap fit	0.333333
a locally linear embedding analysis on the data	manifold locally linear embedding x	0.071429
fit estimator and predict values for	fit and predict estimator x y train	0.250000
feature names ordered by their indices	vectorizer get feature names	0.125000
ridge regression	ridge classifier	0.333333
folders to	joblib memory reduce	0.030303
func to	pool manager mixin apply async func	0.250000
svmlight / libsvm format into sparse csr matrix	svmlight file f n_features dtype	0.066667
false positives per	clf curve y_true y_score pos_label sample_weight	0.250000
relative to	metrics threshold scorer call clf	0.333333
fit	mixin fit	1.000000
and inertia using a full distance	inertia precompute dense x x_squared_norms centers	0.250000
calculate true and false positives per binary	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
the weighted graph of neighbors for	radius neighbors graph	0.066667
huber loss and the gradient	linear_model huber loss and gradient	0.333333
count	bernoulli nb count	1.000000
general function given points on a	y reorder	0.111111
x according	transform x	0.016949
h in multiplicative update	multiplicative update h x w h beta_loss	0.250000
log-likelihood of a gaussian data set with self	score x_test y	1.000000
and class	w x y	0.066667
used for scaling	scaler fit x	0.076923
one-vs-one multi class libsvm in the case	svm one vs one	0.050000
precision the precision is the ratio	metrics precision	0.033333
binarization transformation using thresholding	binarize thresholding y output_type	1.000000
break the pairwise matrix in	parallel pairwise	0.166667
each non zero row of x	transform x	0.016949
back the data to	preprocessing standard scaler inverse transform x copy	0.066667
the validity of the input parameters	params x metric p metric_params	0.100000
the gaussian process model	gaussian process	0.083333
the submatrix corresponding to	submatrix	0.090909
/ tp + fp where tp is the	score y_true y_pred labels pos_label	0.027778
samples of	samples	0.052632
of	externals joblib parallel backend	0.029412
of the local outlier factor of x (as	neighbors local outlier factor decision function x	0.100000
search	core base search cv fit x y	0.166667
shrunk covariance model according to the	covariance shrunk covariance fit	0.083333
is	precision_ alpha	1.000000
full covariance matrices	density full x means covars	0.166667
net path with	enet path x	0.050000
has	has	1.000000
representing	memorized result	0.500000
python object from a file persisted	load filename mmap_mode	0.333333
sqr	sqr	1.000000
validity of the input parameters	metric p metric_params	0.100000
each input data point	core cross	0.045455
reduced	gaussian process reduced	0.125000
avoid the hash depending from it	externals joblib	0.009524
an exception in an unfitted estimator	unfitted name estimator	0.142857
test indices	indices	0.055556
objective function the objective	objective	0.076923
mean and component wise scale to unit	preprocessing scale x	0.090909
a random regression problem with sparse	make sparse	0.125000
full covariance	density full x means	0.166667
undo the scaling of	min max scaler inverse	0.500000
selected returns	feature_selection selector	0.142857
detects the soft boundary of	class svm fit	0.125000
smacof algorithm parameters	manifold smacof	0.200000
that implement	utils check partial fit	0.038462
classifier that makes predictions using simple rules	dummy classifier	0.500000
isotonic regression model	isotonic regression y	0.066667
interpret	feature_selection calculate threshold estimator importances	1.000000
compute area under the curve auc from	auc score	0.052632
expression of the dual gap convergence criterion	covariance dual gap emp_cov	0.071429
distances between the vectors in x and y	distances x y sum_over_features	1.000000
whether	pairwise	0.066667
and a set of	x y	0.002155
the	decomposition base pca get	0.071429
compute the deviance	deviance call y	0.333333
mean squared logarithmic error regression loss read	metrics mean squared log error	0.200000
estimate the diagonal covariance vectors	mixture estimate gaussian covariances diag	1.000000
collect results from clf predict calls	collect probas x	1.000000
lfw pairs dataset this dataset is	fetch lfw pairs subset	0.035714
calculate the posterior log probability of the samples	core bernoulli nb joint log likelihood	0.083333
the kernel k x	gaussian_process compound kernel call x	0.333333
cache folders to	joblib memory reduce	0.030303
the shortest	single source shortest	0.333333
the :ref user guide <image_feature_extraction>	patch extractor	0.090909
number of splitting iterations in	one out get n splits x y	0.111111
fit linear	linear_model base sgdclassifier fit	0.076923
and predicted probabilities for a calibration curve	core calibration curve y_true	0.142857
boolean mask x	mask x	0.333333
the kl divergence	kl divergence	0.083333
cross-validated score	score estimator x y cv	0.083333
a minimum covariance determinant with the fastmcd algorithm	covariance min cov det fit	0.500000
class_mapping	class_mapping	0.625000
regression problem this dataset is described in	datasets	0.015152
classifiers	ensemble bagging classifier	0.200000
position of the points	mds fit x	0.066667
point	predict estimator x	0.045455
format	mixin	0.037037
get number of jobs for the computation	get n jobs n_jobs	0.250000
tol_0	tol_0	1.000000
matrix	linear_model sparse coef mixin	0.090909
class versus all others	multiclass x y alpha c	0.166667
memory is inefficient to train all	x y classes	0.027778
2 in the wikipedia page	utils step1 state	0.142857
attach a reducer function to a	externals joblib	0.004762
a	externals joblib pool	1.000000
for full covariance matrices	density full x means covars	0.166667
retrieve a reliable function code hash	externals joblib get func code func	0.250000
for later scaling	scaler	0.031250
the lfw people dataset this operation is meant	lfw people data_folder_path slice_ color resize	0.333333
of neighbors for points	neighbors mixin radius neighbors	0.125000
or lasso path using lars algorithm	linear_model lars path	0.100000
suffix	utils shape	0.013699
make sure that	core check	0.111111
arrays	arrays	1.000000
fit an estimator within a	fit estimator estimator x y	0.071429
the best found parameters	base search cv predict proba x	0.076923
register a new parallel backend factory	externals joblib register parallel backend name factory make_default	1.000000
product with	projection transform x	0.333333
precision the precision is	precision	0.016667
standardize a dataset along any axis	x axis with_centering with_scaling	0.333333
returns whether the	gaussian_process pairwise	0.333333
n_components	spectral svd array n_components	1.000000
negative gradient)	deviance negative gradient y	1.000000
cluster each	cluster	0.021277
predict class or regression value for x	tree base decision tree predict x check_input	1.000000
fit the model	svc fit x y	0.333333
load and return the iris dataset classification	datasets load iris return_x_y	1.000000
we don't store the timestamp when pickling	memorized func reduce	0.050000
of estimators within a job	estimators n_estimators ensemble	0.083333
x	gaussian_process rbf call x	0.200000
the number of splitting iterations in the	model_selection base kfold get n splits	0.111111
transform data to polynomial features parameters	preprocessing polynomial features transform	0.500000
parallel processing this method is	joblib parallel	0.028571
batch of estimators within a	estimators n_estimators ensemble x y	0.083333
the posterior log probability	core bernoulli nb joint log likelihood	0.083333
fit linear model with passive aggressive algorithm	linear_model passive aggressive regressor fit x y	1.000000
ridge regression model parameters	linear_model ridge	1.000000
fit the model	svr fit x y	0.333333
number of splitting iterations in the cross-validator	base kfold get n splits	0.111111
compute the score of	model_selection score	0.166667
the median of	utils get median	0.166667
inplace column scaling of	utils inplace column scale	0.166667
in x	x y	0.002155
model with x	x y	0.004310
and a youngs	utils incremental	0.166667
groups	groups	1.000000
number of splitting iterations in the cross-validator parameters	one group out get n splits x y	0.111111
jobs for the	jobs n_jobs	0.100000
reconfigure the backend	parallel backend base configure n_jobs	0.500000
by the callers	base effective	0.250000
threshold value	threshold	0.076923
bic	bic	1.000000
remove cache	memory reduce	0.030303
the logistic loss and	linear_model logistic loss and	0.500000
list of edges for a	feature_extraction make edges	0.066667
the kernel k x y and	gaussian_process exponentiation call x y	0.200000
thread pool and	multiprocessing backend	0.038462
kernel is	stationary kernel mixin is	0.333333
predict is invariant of compute_labels	clusterer compute labels predict name clusterer	1.000000
clustering on x and	dbscan fit predict x y sample_weight	0.333333
a minimum covariance determinant with the fastmcd algorithm	covariance min cov det fit x y	0.500000
calculate mean update	mean	0.035714
flattened log-transformed non-fixed hyperparameters	kernel operator theta	1.000000
sparse uncorrelated design this dataset is described	datasets make sparse uncorrelated n_samples n_features	0.166667
incremental fit on	gaussian nb partial fit x y classes sample_weight	0.200000
we	func	0.011364
on the estimator with the best found	model_selection base search cv	0.040000
bagging classifier	bagging classifier	1.000000
suitable step length is not found	line search	0.029412
matrix shrunk on the	shrunk	0.043478
meant to be	data_folder_path slice_ color resize	0.033333
an array using numpy	externals joblib numpy array wrapper	0.333333
inplace column scaling of a	utils inplace column scale x	0.166667
from prediction scores this score corresponds to	score y_true y_score	0.025000
reconfigure	base configure n_jobs	0.500000
precisions parameters of the precision	precisions nk	0.166667
to build from	build from	0.250000
error regression loss read more in the :ref	error y_true	0.111111
approximate nearest	lshforest kneighbors	0.500000
time	time	0.380952
of np dot x y t	dot x y	0.250000
scale back the data to the	standard scaler inverse transform x	0.066667
multi	multi	1.000000
timestamp when pickling to avoid	memory reduce	0.030303
length is not found	line search	0.029412
values	values	1.000000
provided precisions	check precisions precisions covariance_type n_components n_features	0.250000
to partition	ensemble partition	0.200000
specified layer	grad layer	0.166667
the neighbors within	lshforest radius neighbors	0.166667
1 0 if y -	y	0.002674
terminal	terminal	0.380952
kappa a statistic that measures inter-annotator agreement	metrics cohen kappa score y1 y2 labels weights	0.500000
or thread pool	externals joblib	0.004762
split data	split	0.055556
with given gradients	grads	0.166667
raw_documents	raw_documents	1.000000
min sum w[i] (y[i] - y_[i]) ** 2	y sample_weight y_min y_max	0.166667
given arguments and persist the output	func call	0.047619
mostly low rank matrix with bell-shaped	make low rank matrix	0.083333
an estimator within a	estimator estimator x y sample_weight	0.333333
the score on the given data	score x y	0.030303
two rows of a csr matrix in-place	utils inplace swap row csr x m n	0.250000
sample from	size replace p	0.125000
classification problem	classification	0.142857
batches	batches	0.833333
empty the function's	externals joblib memorized func clear warn	0.250000
return staged predictions	staged predict	1.000000
compute the maximum absolute value	max abs	0.047619
a new subcluster into	subcluster subcluster	0.500000
fit linear model with stochastic gradient descent	linear_model base sgdregressor partial fit x y sample_weight	1.000000
information for reducing the size of the	items root_path	0.066667
the centroids on x	fit x	0.006410
check	neighbors check params x	0.500000
score	score estimator x y	0.375000
k x	rbf call x	0.200000
sparse uncorrelated design this dataset is described	datasets make sparse uncorrelated n_samples	0.166667
lad updates terminal regions to	ensemble least absolute error update terminal	0.200000
feature_names	feature_names	1.000000
on the estimator with the best	cv predict proba	0.068966
implementation with customizable pickling reducers	pickling	1.000000
squared logarithmic error	squared log error	0.166667
all the covariance matrices from a given template	covariance type tied_cv covariance_type n_components	0.333333
fit a single tree	build trees tree forest	0.142857
with the	base pca	0.071429
to	externals joblib memorized func	0.013158
by scaling each feature to a	scale x	0.043478
transform a sequence of instances to a	feature hasher transform raw_x y	0.333333
a tolerance which	cluster tolerance x tol	0.058824
string to the file	binary zlib file	0.062500
deprecated	deprecated	1.000000
utility for building a cv in a	check cv cv x y classifier	0.031250
utility for building a cv	core check cv cv	0.031250
on the estimator with the best found parameters	core base search cv predict proba x	0.076923
with the best	cv predict proba	0.068966
within a given radius of a point	radius	0.045455
to retrieve a reliable	externals joblib	0.004762
the given arguments	externals joblib memorized func	0.013158
described in friedman [1] and breiman [2]	make friedman3 n_samples	0.166667
for data x with	x	0.001692
on left-out data for a	x_train y_train x_test y_test	0.200000
embedding analysis on the data	embedding x n_neighbors n_components reg	0.200000
log-transformed bounds on	exponentiation bounds	0.333333
the wild lfw pairs dataset	fetch lfw pairs subset	0.035714
return the directory in which	get output dir	0.047619
init	init	0.538462
fit linear model	fit x	0.032051
solution to a sparse coding problem	sparse encode	0.333333
number of splitting iterations in the	model_selection base kfold get n splits	0.111111
estimate model parameters	fit	0.003257
of min	min	0.045455
for random	n_features	0.083333
folders to	externals joblib	0.004762
then return the score of the underlying estimator	rfe score	0.200000
display the message on stout	externals joblib parallel print	0.125000
parameters for the voting classifier valid	voting classifier set params	0.037037
factorization nmf find two non-negative	factorization x	0.043478
used to fit an estimator	fit estimator estimator x y	0.071429
inplace row scaling of a csr or	inplace row scale	0.142857
data loading for the lfw pairs dataset this	lfw pairs	0.018868
of the gradient boosting	base gradient boosting	0.100000
get	exponentiation get	1.000000
average of the decision functions of	decision function x	0.018868
unit	copy	0.062500
nmf	nmf	1.000000
run in parallel	joblib parallel	0.028571
a lower bound on model	dpgmmbase lower bound	0.071429
an 'l'	repr	0.012500
actual data loading for the lfw	fetch lfw	0.083333
compute log probabilities within	predict log proba	0.029412
store	externals joblib	0.009524
and maximum along an axis on a csr	max axis x axis	0.142857
call predict_log_proba on the estimator with the	predict log proba	0.029412
types to	backend	0.016949
each input data	core cross val predict	0.045455
for each input	val predict	0.045455
build a contingency matrix describing the	contingency matrix labels_true labels_pred	0.166667
a 1-way anova	feature_selection f oneway	0.333333
the :ref user guide <mean_squared_log_error>	y_true y_pred sample_weight multioutput	0.100000
graph of k-neighbors for	neighbors kneighbors mixin kneighbors graph	0.250000
shutdown the process or thread pool	externals joblib multiprocessing backend terminate	0.166667
the maximizer of the	gaussian_process gaussian process arg max	0.047619
corresponding to test sets	partition iterator iter test	1.000000
input	estimator	0.014706
of x	fit x y	0.005988
score with permutations	core permutation test score estimator x y	0.500000
inverse label binarization transformation for multiclass	preprocessing inverse binarize multiclass y	1.000000
to compute log probabilities within a job	predict log proba estimators estimators_features x n_classes	0.250000
constructs a new estimator with the same parameters	core clone estimator safe	0.333333
predicted target values for x relative	scorer call estimator x	0.166667
of last step in pipeline after transforms	pipeline	0.083333
estimate the parameters of the gaussian distribution	bayesian gaussian mixture estimate means nk xk	1.000000
factorize density check according to li et al	core check density density n_features	0.166667
fit linear model with passive aggressive algorithm	linear_model passive aggressive regressor partial fit x	1.000000
elastic net optimization function varies for mono	l1_ratio	0.030303
partially fit a	core partial fit	0.500000
outlyingness	outlier detection mixin predict	0.250000
parametergrid instance for the given param_grid	model_selection grid search cv get param iterator	0.166667
reconstruct	joblib ndarray wrapper read unpickler	0.333333
matrix factorization nmf	decomposition non negative factorization x	0.043478
fit the model with x	rbfsampler fit x	1.000000
cv in a user friendly way	check cv cv	0.031250
factorize density check according to	core check density density n_features	0.166667
depending	joblib	0.014599
squared logarithmic error regression	squared log error	0.166667
a which this function is	externals joblib memorized	0.013699
white	white	0.857143
analysis	analysis	0.318182
the	mixture	0.500000
determination regression score function	metrics r2 score y_true y_pred	0.125000
squares projection of the data onto	ridge_alpha	0.052632
the maximizer of the reduced	arg max reduced	0.200000
used to fit a single tree	trees tree forest x	0.142857
each parameter weights and	y	0.002674
parameters theta as the maximizer of	gaussian process arg max	0.047619
ridge and	y	0.002674
data x which should contain a	x y	0.002155
described in [rouseeuw1984]_ aiming at computing	x n_support remaining_iterations initial_estimates	0.111111
with the best found parameters	model_selection base search cv predict	0.076923
and evaluates the reduced	reduced	0.062500
message on stout or stderr depending on verbosity	msg msg_args	0.200000
sparse random matrix given	random	0.058824
positive-definite	covariance_type	0.083333
platform independent representation	repr	0.012500
load the kddcup99 dataset downloading it if necessary	fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
a minimum covariance determinant with the fastmcd algorithm	covariance min cov det fit x	0.500000
and its corresponding derivatives with respect to	activations deltas	0.032258
learn vocabulary and idf	vectorizer fit transform raw_documents y	0.100000
maximum likelihood estimator covariance model according to	covariance empirical covariance fit	0.166667
a memmap instance to reopen	memmap a	0.050000
the minimum	preprocessing min	0.166667
k-neighbors of	neighbors kneighbors mixin kneighbors x	0.125000
fits the graphlasso covariance model	covariance graph lasso cv fit	0.111111
getter	covariance empirical	0.100000
to maximize class separation	core linear discriminant analysis transform x	0.250000
return the decision	decision	0.027778
with the em	mixture gmmbase	0.034483
the weighted graph of neighbors for points	neighbors mixin radius neighbors graph	0.066667
feature importances	base decision tree feature importances	0.333333
returns the number of splitting iterations in the	kfold get n splits x y	0.111111
corresponds	y_true y_score	0.027027
estimate the precisions parameters of the precision	gaussian mixture estimate precisions nk	0.166667
of the function called with the given arguments	externals joblib memorized func get output	0.125000
we	memorized func	0.016949
for	boost classifier	1.000000
helper function for factorizing common classes param	fit first call clf classes	0.058824
problem this dataset is described in friedman [1]	datasets make	0.015625
path length from source to all reachable nodes	path length graph source cutoff	0.200000
neighbors for	mixin radius neighbors	0.125000
the maximum absolute value to be used for	max abs	0.047619
thresholding of array-like or scipy	binarize x	0.083333
predict the target	regressor predict x	0.333333
list of exception types	joblib parallel backend base get	0.066667
for the one-vs-one multi class libsvm in	one vs one	0.050000
boolean thresholding of array-like or scipy	binarize x	0.083333
img	img	1.000000
make cache size fit in	externals joblib memory reduce size	0.083333
embedding analysis on	embedding x n_neighbors n_components reg	0.200000
tolerance which is independent of	tolerance x	0.058824
the actual data loading for the lfw people	fetch lfw people	0.040000
computes	covariance empirical covariance	0.071429
fit with	cv fit x y	0.250000
a given	y scorer	0.111111
generative model	pca	0.047619
coefficient matrix	sparse coef mixin	0.083333
l1 distances between the vectors in x	paired manhattan distances x	0.500000
x relative	metrics threshold scorer call clf x y sample_weight	0.058824
func to	joblib sequential backend apply async func	0.250000
check	mixture base mixture check	1.000000
fit estimator and transform dataset	random trees embedding fit transform x y	1.000000
found and	search	0.019231
cross-validated orthogonal matching pursuit model	orthogonal matching pursuit cv	0.200000
return the shortest path length	utils single source shortest path length graph	0.333333
building a cv in a	core check cv cv	0.031250
warning class used to notify the	warning	0.083333
the local outlier factor of x (as bigger	local outlier factor decision function x	0.100000
for building a cv	cv cv x	0.031250
scale back the data	robust scaler inverse transform x	0.066667
apply clustering to a projection to	cluster spectral clustering affinity n_clusters	0.166667
we can also predict based on an	predict	0.006849
count	core bernoulli nb count x	1.000000
predict using the gaussian process regression model we	gaussian_process gaussian process regressor	0.058824
folders	joblib	0.007299
found and raise an exception	utils line search	0.029412
more in the :ref user guide <sparse_inverse_covariance>	emp_cov alpha cov_init mode	0.200000
the curve auc using the trapezoidal rule this	metrics auc	0.040000
object providing transparent gzip de compression	binary gzip	1.000000
constant block diagonal structure	biclusters shape n_clusters noise minval	0.058824
type introduces an 'l' suffix when using the	utils shape	0.013699
to avoid the hash depending from	externals joblib memorized func	0.013158
free energy f	free energy	0.066667
generative	pca	0.047619
dot-product kernel	dot product	0.250000
embedding for	embedding	0.040000
the score	score x	0.066667
input data point	cross val predict estimator x y	0.045455
back the data to the original representation	standard scaler inverse transform x copy	0.066667
coefficient	linear_model sparse coef mixin	0.090909
function code and	func code	0.200000
the separating hyperplane	base lib svm decision function	0.333333
the kernel k x	gaussian_process pairwise kernel call x	0.333333
for c in (l1_min_c infinity) the	c x y loss fit_intercept	0.030303
input validation	x y x y accept_sparse dtype	0.250000
return the kernel k x y and	rbf call x y	0.200000
read more in the :ref user guide <mean_squared_log_error>	y_true y_pred sample_weight multioutput	0.100000
content of the data home cache	data home data_home	0.055556
checker utility for building a cv	check cv cv x y classifier	0.031250
x x	x	0.013536
under the curve auc using the trapezoidal	auc x	0.040000
for each input	predict estimator	0.045455
to sparse	sparse	0.025000
maximizer of the reduced likelihood	gaussian process arg max reduced likelihood	0.250000
using the gp prior	x return_std return_cov	0.142857
a fully connected graph	graph	0.021277
to raise if estimator is used before fitting	not fitted error	0.500000
of the dual gap convergence criterion	dual gap emp_cov	0.071429
return a platform independent representation of	shape repr	0.013699
number of splitting iterations in the cross-validator	cross validator get n splits x y	0.125000
fit the model using x	factor fit x	1.000000
by cross-validation read more in the :ref	cross val	0.038462
linear assignment problem using the	linear assignment	0.090909
from	read	0.052632
component wise scale	robust scale x	0.125000
classes param logic estimators	classes	0.025641
to	sample_weight	0.018519
get parameters for	get params	0.100000
we don't store the	memory	0.015625
according to the	sample_weight	0.037037
returns the huber loss and the gradient	huber loss and gradient w x y	0.333333
cross-validated estimates for each input data point	x y cv	0.050000
of bag predictions and score	ensemble base bagging set oob score x y	0.250000
write array bytes to pickler file handle	joblib numpy array wrapper write array array pickler	0.333333
set the parameters of this	feature union set params	1.000000
for the lfw pairs dataset this operation is	fetch lfw pairs	0.018868
get the parameters of the	classifier get	0.200000
capture the arguments	check_pickle	0.040000
boost	boost	0.500000
linkage	linkage	0.833333
format	sparse coef mixin	0.083333
for the california housing dataset from	california housing	0.083333
descriptors of a memmap instance to reopen	memmap a	0.050000
such that for c in	c x y	0.030303
input data point	cross	0.037037
variance regression score function best possible	variance	0.058824
out cross-validator provides train/test indices	out	0.095238
report showing the main classification	classification report	0.250000
call predict_log_proba on the estimator with	predict log proba x	0.045455
for the lfw pairs dataset	datasets fetch lfw pairs	0.018868
calculate approximate perplexity for data x	decomposition latent dirichlet allocation perplexity x doc_topic_distr sub_sampling	1.000000
dense dictionary	dictionary	0.071429
given radius of a point or	radius	0.045455
list of edges for	make edges	0.066667
generate indices to split data into	predefined split split	0.250000
does nothing this transformer is stateless	feature_extraction hashing vectorizer fit x y	1.000000
generate a sparse random matrix given	utils random choice csc n_samples	0.333333
opening the right fileobject from a	joblib read fileobject	0.100000
best found	search cv	0.090909
computes the maximum likelihood covariance estimator parameters	covariance empirical covariance x assume_centered	0.166667
pairwise matrix in	pairwise x	0.166667
fit all transformers transform the data	union fit transform x	0.333333
ridge model	ridge	0.071429
smacof algorithm parameters	smacof	0.100000
prediction of init	init	0.076923
returns a lower bound on model	mixture dpgmmbase lower bound	0.071429
sparse and	x y sample_weight random_state	0.166667
pairwise matrix in n_jobs even slices	pairwise x y func n_jobs	0.111111
returns the number of splitting iterations in	base cross validator get n splits x	0.125000
introduces an 'l' suffix when using	shape repr	0.013699
constructor store the useful	externals joblib zndarray wrapper init filename init_args state	0.200000
model according to the given training	y sample_weight	0.035714
two clusterings of a set of	score labels_true labels_pred sparse	0.047619
compute area under the curve auc from prediction	auc score	0.052632
call	args kwargs	0.100000
used to build a batch of estimators within	ensemble parallel build estimators n_estimators ensemble x	0.166667
compute the gradient	multilayer perceptron compute	0.250000
reduced likelihood function	gaussian process reduced likelihood function	0.047619
predict the class labels for	classifier predict x	0.500000
load sample images for	load sample images	0.250000
log of the determinant of	log	0.018868
of	shape repr	0.013699
of init	base gradient boosting init	0.142857
of dataset and properly handle kernels	safe split estimator x y indices	0.200000
the sample weight array	sgd validate sample weight	0.333333
fit ridge regression model parameters	linear_model ridge fit x	1.000000
the number of splitting iterations in the	out get n splits x y groups	0.111111
to the binary classification	score y_true y_score average	0.076923
nu-support vector classification	nu svc	1.000000
the function called with the given arguments	joblib memorized func	0.014706
exception in an unfitted	unfitted name	0.142857
the solution to a sparse coding problem	decomposition sparse encode x dictionary gram	0.333333
return the decision path	decision path x	0.333333
return the kernel k x	dot product call x	0.200000
with block checkerboard structure for	checkerboard shape n_clusters noise minval	0.066667
the best found	model_selection base search cv predict proba	0.076923
used to build a batch of estimators within	build estimators n_estimators ensemble x y	0.166667
exception types to	externals joblib	0.004762
path with coordinate	path x	0.045455
return whether the file supports seeking	binary zlib file seekable	0.250000
exception types	parallel backend base get	0.066667
the sparse components	decomposition sparse pca transform x	0.500000
c in (l1_min_c infinity) the model	c x	0.030303
search over	search cv fit x	0.111111
matrix to	linear_model sparse coef mixin	0.090909
benjamini-hochberg procedure	fdr	0.142857
median and component wise scale	scale x	0.043478
clustering	clustering	0.350000
assignment problem using the hungarian	assignment	0.111111
returns the number of splitting iterations in the	predefined split get n splits x y	0.111111
can also predict based	predict x	0.011765
of time controlled by self	progress	0.100000
validity	x metric p metric_params	0.100000
class labels for	ensemble voting classifier	0.031250
and return the number of workers	externals joblib	0.004762
of exception	externals joblib parallel	0.014085
generate a random regression problem with sparse	sparse	0.025000
dist	dist	1.000000
determination regression score	metrics r2 score y_true y_pred sample_weight multioutput	0.125000
routine for validation and	dtype	0.062500
for data x	x doc_topic_distr sub_sampling	0.333333
of k-neighbors for	kneighbors mixin kneighbors	0.100000
return the kernel k x y and	gaussian_process dot product call x y	0.200000
text report showing the main classification	classification report	0.250000
compute the rectified linear unit function inplace	neural_network relu x	1.000000
columns of a csc/csr matrix in-place	utils inplace swap column x m n	0.250000
rfecv	rfecv	1.000000
ardregression model according to the given	linear_model ardregression	0.100000
non-negative matrix factorization nmf find two non-negative matrices	factorization x	0.043478
x and dot w h	decomposition beta divergence x w h	0.500000
compute elastic net path with coordinate descent	path x	0.045455
the wild lfw pairs dataset	datasets fetch lfw pairs	0.018868
fit the	core multi output estimator fit	0.200000
absolute sizes of training subsets and	translate train sizes	0.066667
recall the recall is	recall	0.028571
class for unbalanced datasets	weight class_weight y	0.200000
new points into embedding space	manifold locally linear embedding	0.062500
to implement the usual api and	fit x y	0.017964
the number of splitting iterations in	one out get n splits x	0.111111
true and false positives per binary	binary clf curve	0.090909
utility function opening the right fileobject from a	externals joblib read fileobject	0.100000
write the function code and	write func code	0.500000
cross-validated orthogonal matching pursuit model omp	orthogonal matching pursuit cv	0.200000
scale back	standard scaler inverse transform x	0.066667
utility function opening the right fileobject	fileobject	0.100000
a memmap instance to reopen on same file	memmap a	0.050000
target_variables	target_variables grid	1.000000
to	mixin transform	1.000000
of the scaler	max abs scaler	0.250000
of exception types to	externals joblib parallel backend	0.029412
for the precision matrix	covariance empirical covariance get precision	0.250000
validity of	params x metric p metric_params	0.100000
isotonic regression model : min sum w[i]	core isotonic regression y	0.066667
of	utils	0.029126
check the	neighbors check params	0.500000
predict_log_proba on	predict log proba x	0.045455
for each	y	0.002674
x by	x	0.001692
long	utils	0.009709
evaluate decision function output for x	x y sample_weight	0.012987
the process or thread pool	joblib multiprocessing	0.052632
augment dataset with an additional dummy	preprocessing add dummy	1.000000
median and component wise scale	preprocessing robust scale	0.125000
estimates for each input data	core	0.015385
ensure that x	x predict x	0.333333
"news" format strip lines beginning with the	datasets strip newsgroup	0.090909
pickle the descriptors of a	a	0.018182
the percentiles of x	x x percentiles grid_resolution	0.500000
result and return a reference	and shelve	0.200000
incremental fit on	discrete nb partial fit x y classes sample_weight	0.166667
introduces an	shape repr	0.013699
shrunk on the diagonal	covariance shrunk	0.066667
updates terminal regions to	terminal region tree terminal_regions leaf	0.066667
decision function output for x relative to	metrics threshold scorer call clf x y	0.058824
column class distributions	classes class_probability random_state	0.166667
function of	function x raw_values	0.250000
get the boolean mask indicating which features are	mixin get support mask	0.333333
of the local outlier factor of	neighbors local outlier factor	0.125000
return the shortest path length from source	single source shortest path length graph source cutoff	0.111111
lfw people dataset this operation is	fetch lfw people	0.040000
the submatrix corresponding	get submatrix	0.166667
and dense	y	0.002674
batch_update	batch_update	1.000000
f	f	1.000000
precisions parameters	precisions nk	0.166667
number of splitting iterations in the cross-validator	model_selection predefined split get n splits	0.111111
boolean mask x == missing_values	get mask x value_to_mask	0.333333
generator to create slices containing	utils gen batches	1.000000
computes multinomial loss and class	linear_model multinomial loss w x y alpha	0.333333
free energy f v =	bernoulli rbm free energy	0.066667
whitespace sensitive char-n-gram tokenization	feature_extraction vectorizer mixin char wb ngrams text_document	1.000000
each input	cross val	0.038462
indices in sorted array	indices tree bin_x left_mask right_mask	0.166667
matrix m	m k	1.000000
a one-vs-all fashion several regression and binary classification	y classes neg_label pos_label	0.111111
fit a multi-class classifier by	fit	0.003257
compute the	compute	0.117647
suffix when	shape repr	0.013699
cv in a user	core check cv cv x y	0.031250
indices to split data into	series split split	0.250000
memstr	memstr	1.000000
depending from	joblib memorized	0.015625
by quantile this classification dataset is constructed by	datasets make	0.015625
coverage error measure compute	coverage error y_true y_score sample_weight	0.166667
a logistic regression model	linear_model logistic regression path	0.333333
the initial centroids	cluster init centroids x k	0.166667
and target y	neural_network base multilayer perceptron partial	0.166667
change the default backend used by parallel inside	parallel backend backend n_jobs	0.166667
inplace row scaling of a csr or	utils inplace row scale x	0.142857
break the pairwise matrix in	parallel pairwise x y	0.166667
graph of k-neighbors	kneighbors graph	0.500000
by scaling each feature to a given range	minmax scale	0.142857
for the one-vs-one multi class libsvm in the	svm one vs one	0.050000
matrices from a	covariance_type n_components	0.333333
fit the	svm linear svr fit	0.333333
the local structure is retained	x x_embedded n_neighbors precomputed	0.200000
people dataset this	people	0.100000
a collection of jpeg pictures of famous people	funneled resize	0.142857
weiszfeld step	weiszfeld step	1.000000
break the pairwise matrix in	metrics parallel pairwise x	0.166667
omp solves n_targets orthogonal matching pursuit	linear_model orthogonal mp x y n_nonzero_coefs tol	0.200000
for validation	dtype	0.062500
full	full x means covars	0.166667
the number of splitting iterations in the cross-validator	model_selection leave one out get n splits	0.111111
for building a cv in	core check cv cv x	0.031250
estimate the precisions	gaussian mixture estimate precisions nk xk sk	0.166667
setting the parameters for the voting classifier valid	ensemble voting classifier	0.031250
the decision	ensemble ada boost classifier decision	0.333333
the hash depending from it	joblib memory	0.016949
the given args and	args	0.142857
generate a dense gaussian random matrix	core gaussian random matrix n_components n_features random_state	1.000000
last element of numpy array	last element arr	0.142857
function	decision function	0.025000
check initial parameters of the derived class	mixture check parameters x	0.166667
aligned	aligned	0.461538
the "friedman \#3" regression problem this dataset is	datasets make	0.015625
to compute log probabilities within a job	parallel predict log proba estimators estimators_features x n_classes	0.250000
the svmlight / libsvm format	svmlight file f n_features	0.066667
list of exception types to	externals	0.005747
performs clustering on x and returns cluster	cluster dbscan fit predict x y	0.166667
input validation on an array	utils check array array	0.250000
model from	manifold spectral	0.111111
california housing	california housing	0.083333
the search	base search cv fit x	0.166667
make predictions using a single binary	binary	0.031250
regression	regression	0.866667
ensure x min() >=	utils make nonnegative x	1.000000
normalize x by scaling rows and columns independently	normalize x	0.076923
parameters are well defined	parameters x	0.250000
loss and class probabilities	loss w x y	0.250000
the kddcup99	fetch brute kddcup99	0.166667
boolean thresholding of array-like or scipy	binarize x threshold copy	0.083333
as the maximizer	gaussian process arg max	0.047619
memmap instance	joblib reduce memmap	0.166667
actual fitting performing the search	core base search cv fit x y parameter_iterable	0.333333
compute the gradient	compute	0.058824
check if estimator adheres to scikit-learn conventions	utils check estimator estimator	0.250000
by cross-validation read more in the	model_selection cross val	0.250000
free energy f v = -	free energy	0.066667
finds the neighbors within a given radius of	neighbors lshforest radius neighbors x radius return_distance	0.500000
process or thread pool and	multiprocessing backend	0.038462
random projection p only changes the	johnson lindenstrauss min dim n_samples eps	0.142857
the	get	0.024096
transform x back to original space	decomposition kernel pca inverse transform x	1.000000
best found parameters	search cv predict proba x	0.076923
number of splitting iterations in	model_selection leave one out get n splits x	0.111111
computes a truncated randomized svd	utils randomized svd m n_components n_oversamples n_iter	0.250000
compute class covariance matrix	class cov x	0.250000
is the solution to a sparse coding problem	decomposition sparse encode x dictionary gram	0.333333
the unpickler to unpickle our numpy pickles	zip numpy unpickler	0.333333
binary gaussian process classification based on laplace approximation	binary gaussian process classifier laplace	1.000000
the recall is	metrics recall	0.033333
to avoid the hash depending from it	joblib memorized func	0.014706
hide warnings without visual nesting	ignore warnings call fn	0.200000
of feature name -> indices mappings	feature_extraction dict vectorizer fit x	0.250000
coverage error measure compute how far we	coverage error y_true y_score	0.166667
a contingency matrix describing the relationship between	metrics cluster contingency matrix	0.200000
for each	core cross val predict estimator	0.045455
absolute sizes of	core translate train sizes	0.066667
the weighted log probabilities	mixture base mixture	0.111111
lfw pairs dataset this dataset is a	datasets fetch lfw pairs	0.018868
batch of estimators within a	estimators n_estimators	0.083333
prefetch the tasks for the next batch	batch iterator	0.500000
don't store the timestamp when pickling to	reduce	0.034483
return a buffered	joblib buffered	0.333333
list of exception	externals joblib	0.004762
estimate the spherical wishart distribution	bayesian gaussian mixture estimate wishart spherical nk xk	0.333333
build	ensemble parallel build	0.047619
best found parameters	search cv predict proba	0.076923
the binary classification	y_true	0.043478
for c such that for c	c x y loss fit_intercept	0.030303
absolute error	absolute error y_true y_pred	0.142857
a cv in a user friendly	cv cv x y	0.031250
is a general function given points on	x y reorder	0.111111
on the training set according to	fit predict	0.055556
model from data	decomposition kernel pca	1.000000
number of jobs for	n jobs n_jobs	0.142857
sparse random	random choice csc	0.166667
whether the kernel is	kernel is	0.200000
number of splitting iterations in	out get n splits x y groups	0.111111
reconstruct the	joblib ndarray wrapper read unpickler	0.333333
each input data	predict	0.006849
perform classification on an array of test vectors	process classifier predict	1.000000
return the directory in which are	get output dir	0.047619
multi-class labels parameters	threshold	0.076923
blup parameters and evaluates the reduced likelihood	reduced likelihood	0.100000
exception	backend base	0.032258
embedding	embedding x n_neighbors	0.200000
for data x parameters	fit x y	0.005988
hide warnings	ignore warnings	0.166667
also predict based on an unfitted model by	predict x	0.011765
the number of splitting iterations in the	cross validator get n splits x	0.125000
data loading for the lfw pairs dataset this	datasets fetch lfw pairs	0.018868
vectors for reproducibility flips the	deterministic vector	0.076923
type introduces an 'l' suffix when using the	shape	0.011765
result of signature bind call holds	bound arguments	0.333333
skewed	skewed	1.000000
parameters for the voting classifier valid parameter	voting classifier set	0.037037
free energy f v = - log	free energy	0.066667
x y and optionally its gradient	gaussian_process rbf call x y eval_gradient	0.333333
list of feature names ordered by their indices	get feature names	0.090909
build a batch of estimators within	build estimators n_estimators	0.166667
implement a single boost	classifier boost iboost x y	1.000000
performs clustering on x and	fit predict x y	0.250000
the decision functions of	decision function	0.025000
in [rouseeuw1984]_ aiming at computing mcd	x n_support remaining_iterations initial_estimates	0.111111
fit lsi model to	decomposition truncated svd fit transform	1.000000
whether the kernel	dot product	0.250000
the process or	externals	0.005747
estimate the precisions parameters of the precision distribution	gaussian mixture estimate precisions	0.166667
fit	cv fit x y	0.250000
expected value of the log	log	0.018868
file	file	0.392857
estimator's fit method supports the	utils has fit parameter estimator	0.500000
test	core base	0.166667
scale back the data to the	scaler inverse transform x	0.052632
true and false positives per binary classification threshold	metrics binary clf curve y_true y_score	0.090909
returns false for indices increasingly apart the distance	verbosity filter index	0.055556
full covariance	full x means	0.166667
update the variational distributions for the means	mixture dpgmmbase update means	1.000000
undo the scaling	preprocessing min max scaler inverse transform	0.500000
sparse and dense	y sample_weight random_state	0.166667
boosted classifier from the training set x y	ensemble ada boost classifier fit x y	1.000000
the curve auc using the trapezoidal rule	auc x	0.040000
sure that an estimator implements the necessary methods	check estimator estimator	0.142857
indicate if wrapped estimator is using	core one vs one classifier pairwise	0.200000
fit all transformers transform the data	core feature union fit transform x	0.333333
number of free parameters in	n parameters	1.000000
file	externals joblib binary zlib file	0.166667
callable object decorating	memorized func	0.016949
low rank matrix with bell-shaped singular values	make low rank matrix	0.083333
an unfitted	unfitted	0.083333
the backend and	backend base	0.032258
and a	utils incremental	0.166667
dictionary	dict dictionary	0.111111
hash depending from it	externals	0.011494
to fit a single tree in parallel	ensemble parallel build trees tree	0.200000
by some authors as	preprocessing label binarizer	0.071429
multiplicative	multiplicative	0.714286
random n-class	n_informative n_redundant	0.333333
the curve auc from prediction scores note this	roc auc	0.166667
read the array corresponding to this wrapper	joblib numpy array wrapper read	0.500000
hash depending from	externals joblib memory	0.016949
solve the linear assignment problem using	utils linear assignment x	0.090909
the search over parameters	base search cv	0.026316
general function given points on a curve	y reorder	0.111111
factorization nmf find	decomposition non negative factorization x	0.043478
func to be run	base apply async func	0.250000
with permutations	core permutation test	0.500000
gamma	gamma	1.000000
the process	externals	0.005747
configure a copy of the base_estimator_	estimator append	0.142857
optimal batch	joblib auto batching mixin compute batch	0.333333
of x from y along the first	x z reg	0.066667
and return the number	externals joblib parallel	0.014085
error	error y_true	0.222222
track of time	time	0.047619
in the wild lfw pairs dataset this dataset	fetch lfw pairs	0.018868
a given cache	cache	0.111111
makes	x copy	1.000000
current	tell	0.125000
to fit a single tree	build trees tree forest x	0.142857
and inertia using a	inertia precompute dense x x_squared_norms centers	0.250000
non-negative matrix factorization nmf find two non-negative	decomposition non negative factorization	0.043478
print verbose message on	base mixture print verbose msg init	0.666667
create all the covariance	distribute covar matrix to match covariance	0.250000
sample weight array	linear_model base sgd validate sample weight sample_weight n_samples	0.333333
the neighbors within	radius neighbors x	0.166667
points that will be sampled	core parameter sampler len	0.333333
objective for the case method='lasso' is	y xy gram	0.090909
the curve auc using the trapezoidal rule	metrics auc	0.040000
the kernel is	stationary kernel mixin is	0.333333
described in [rouseeuw1984]_ aiming at computing mcd	covariance c step x n_support remaining_iterations initial_estimates	0.111111
the function call with the given	call func	0.100000
estimator's fit method supports the	utils has fit parameter	0.500000
generate	base shuffle split	0.142857
points in the grid	model_selection parameter grid	0.250000
predict using the trained model parameters	multilayer perceptron predict	0.333333
building a cv in	cv cv x y	0.031250
omp	omp	1.000000
folders	reduce	0.017241
reconfigure the backend and return the	externals joblib parallel backend base configure n_jobs	0.333333
theta as the maximizer of the reduced	process arg max reduced	0.200000
sample from the distribution p(v|h)	rbm sample visibles h rng	1.000000
back the data to the original	preprocessing standard scaler inverse transform	0.066667
online	partial fit x	0.500000
this operation is meant to	index_file_path data_folder_path slice_ color	0.033333
c in	c x y loss	0.030303
partially fit underlying	core one vs one classifier partial fit	0.166667
predicted target values for x	call estimator x	0.166667
run fit with	grid search cv fit	0.333333
to compute log probabilities within	ensemble parallel predict log proba	0.058824
cross-validated lasso using the	cv	0.009009
the data home	datasets clear data home data_home	0.076923
building a cv in a user friendly way	check cv cv x	0.031250
quantile this classification dataset is	datasets	0.015152
shutdown the process	joblib multiprocessing backend terminate	0.166667
hash	externals joblib memorized	0.013699
estimates for each input	val	0.037037
compute the decision function	decision function x	0.018868
squared euclidean	squared	0.083333
and update it with the split	update split	1.000000
to implement the usual api and hence	preprocessing binarizer fit x y	0.142857
descent fit is	fit	0.003257
propagation	propagation fit	1.000000
two columns of a csc/csr matrix in-place	utils inplace swap column x	0.250000
avoid the hash depending	joblib memorized func	0.014706
setting the parameters for the voting	voting	0.066667
list	joblib parallel backend	0.045455
a name	name	0.033333
return whether the file	externals joblib binary zlib file	0.166667
diagonal of the laplacian	diag laplacian	0.111111
a single boost using	ensemble ada boost classifier boost	0.100000
store the	joblib memory	0.016949
function used to compute log probabilities within	ensemble parallel predict log proba	0.058824
the binary	y_score average sample_weight	0.142857
arr assuming arr is in the log domain	utils logsumexp arr	1.000000
k x y and optionally its gradient	gaussian_process rbf call x y eval_gradient	0.333333
+ fn where tp is the	score y_true y_pred labels pos_label	0.027778
update the variational distributions for the precisions	mixture dpgmmbase update precisions x z	1.000000
length dimensions	dimensions	0.125000
indices to split data into training and test	time series split split x y groups	0.200000
the neighbors within a given radius	lshforest radius neighbors x radius	0.142857
dataset along any axis center to	axis	0.028169
back the data to the original representation	preprocessing robust scaler inverse transform x	0.066667
outlier on the training set according	outlier factor fit	0.200000
estimate model parameters with the em algorithm	mixture base mixture fit	0.200000
all the covariance matrices from a given template	to match covariance type tied_cv covariance_type	0.333333
matrix to	coef	0.058824
state	state	0.533333
download the 20 newsgroups	download 20newsgroups	0.200000
to fit an estimator within a job	ensemble parallel fit estimator estimator	0.333333
locally linear embedding	manifold locally linear embedding x n_neighbors n_components reg	0.071429
lsh forest on	neighbors lshforest	0.333333
automatically tune	feature_selection rfecv	1.000000
dataset this operation is meant to be cached	data_folder_path slice_ color resize	0.033333
score is	score y_true y_pred	0.076923
x for later	x	0.001692
procedure described in [rouseeuw1984]_ aiming at computing	x n_support remaining_iterations initial_estimates	0.111111
model	pca get	0.076923
the distance depending on the value of verbose	verbose	0.062500
convert coefficient	sparse coef	0.071429
dictionary learning finds a dictionary	dictionary learning	0.142857
sure centering is not enabled for sparse	robust scaler check array	0.250000
compute	decomposition base pca get	0.071429
from	joblib read	0.333333
regression	regression path x y	1.000000
of init	base gradient boosting init decision function x	0.142857
expression of the dual gap convergence criterion the	covariance dual gap emp_cov precision_ alpha	0.071429
for each	val predict estimator	0.045455
contingency matrix describing	contingency matrix labels_true labels_pred eps	0.166667
predict the target of	predict x	0.011765
factorize density check according to li et	check density density n_features	0.166667
estimate the precisions parameters	mixture bayesian gaussian mixture estimate precisions nk	0.166667
fit the model to data	multi output estimator fit x y	0.200000
don't store the	memorized	0.015873
to split data in train/test sets	split	0.027778
returns the number of splitting iterations in	kfold get n splits x	0.111111
gaussian process model	gaussian_process gaussian process	0.111111
predict regression target at each stage for	gradient boosting regressor staged predict	0.500000
fit label binarizer parameters	preprocessing label binarizer fit y	0.500000
to the cache	joblib memorized func	0.014706
suffix when using the	utils	0.009709
in parallel	parallel backend	0.030303
the local outlier factor of x (as	local outlier factor decision function x	0.100000
estimator with the best found	base search cv predict	0.076923
number of splitting iterations in	base kfold get n splits x	0.111111
perform a locally	locally	0.111111
embedding for non-linear dimensionality	embedding	0.040000
graph of neighbors	radius neighbors mixin radius neighbors graph	0.066667
generate train	core base shuffle split iter	0.166667
format check x	decomposition latent dirichlet allocation check	0.062500
predict raises an exception in an unfitted	estimators unfitted name	0.142857
lfw people dataset this operation is meant	lfw people data_folder_path slice_ color resize	0.333333
the callable case for pairwise_{distances	callable x y metric	0.083333
set the	linear model set	0.500000
to build from the c and cpp	build from c and cpp	0.500000
rand index adjusted for chance	metrics cluster adjusted rand score labels_true labels_pred	0.333333
hence	preprocessing binarizer fit	1.000000
means	means x	0.250000
get parameters	exponentiation get params	0.500000
of	joblib parallel	0.028571
for full covariance matrices	multivariate normal density full	0.166667
as target values parameters	neighbors supervised integer mixin	0.500000
graph-lasso objective function the objective	objective	0.076923
data and labels	y	0.002674
perform a locally linear embedding	locally linear embedding x n_neighbors	0.071429
estimates for each input	cross val predict estimator x y	0.045455
derivatives with respect	activations deltas	0.032258
compute the squared loss for regression	neural_network squared loss y_true y_pred	0.500000
the huber loss and the gradient	linear_model huber loss and gradient	0.333333
multiple files in svmlight format this	svmlight files files n_features	0.200000
mixin for converting coef_	sparse coef mixin	0.083333
random regression problem with sparse uncorrelated design	sparse uncorrelated n_samples n_features random_state	0.166667
compute the decision	decision	0.027778
filter	filter	1.000000
warning class used to notify	warning	0.083333
along an axis on	axis	0.014085
the decision function of the given observations	covariance outlier detection mixin decision function	0.333333
when using	shape	0.011765
feature importances	feature_selection get feature importances	1.000000
matrices w h whose	x w h	0.035714
dataset is	datasets make	0.015625
of the data home	clear data home	0.076923
param logic estimators that implement the partial_fit	utils check	0.023810
paired distances between	metrics paired distances	0.500000
the voting	voting	0.066667
for a calibration curve	calibration curve y_true y_prob	0.142857
for later scaling	scaler fit	0.153846
of samples in array-like x	utils num samples x	0.250000
fit x	fit x y	0.005988
huber loss and	huber loss and	0.166667
reconstruct	ndarray wrapper read unpickler	0.333333
back the data	robust scaler inverse transform x	0.066667
the	spectral	0.026316
memory is inefficient to train all data	y classes	0.027778
configure a copy of the	append	0.083333
a given dataset split	scorer	0.045455
labels	classifier mixin	0.333333
a given dataset	x y scorer	0.111111
setting the parameters for the voting classifier	voting classifier	0.035714
construct a featureunion from the given transformers	core make union	0.250000
label sets with a	preprocessing multi label binarizer transform	0.250000
row scaling	row	0.133333
class for classification loss	classification loss	1.000000
is restricted to the binary classification	y_true y_score	0.054054
solve the linear assignment problem using the hungarian	linear assignment	0.090909
coefficient of determination regression	r2	0.076923
read up to len b bytes into	externals joblib binary zlib file readinto	1.000000
predicted probabilities for a calibration	calibration	0.071429
evaluate	call x y eval_gradient	0.050000
for parallel processing this method	parallel	0.019231
patches of any n-dimensional	patches	0.055556
to data matrix x	x	0.001692
generates	model_selection repeated splits	1.000000
estimates for	cross val	0.038462
compute data precision matrix with the generative	decomposition base pca get precision	0.066667
a given cache	joblib cache	0.250000
training	regression	0.066667
grid of alpha values	linear_model alpha grid x y	0.166667
loader for the	data_home	0.111111
generate a signal	signal n_samples n_components	1.000000
fit all transformers using x	core feature union fit x	1.000000
feature	feature x	1.000000
coverage error measure compute how far we need	metrics coverage error y_true y_score	0.166667
the callable case	pairwise callable	0.083333
the local outlier factor	neighbors local outlier factor decision	0.125000
to the median and component wise scale	robust scale	0.125000
the array from the meta-information and	zndarray wrapper read unpickler	0.043478
the search over parameters	core base search cv fit x y	0.166667
of the dual gap convergence criterion the	dual gap emp_cov	0.071429
true and false positives per	clf curve y_true y_score	0.250000
fit the	rbm fit	0.333333
similarity of two clusterings of a set	score labels_true labels_pred sparse	0.047619
compute	feature_extraction compute	0.500000
avoid the hash	externals joblib memorized	0.013699
local	outlier factor local	0.500000
actual data loading for the lfw pairs dataset	datasets fetch lfw pairs	0.018868
determine the optimal batch size	joblib parallel backend base compute batch size	1.000000
of observations in x according	x	0.001692
binary	y_score	0.083333
label binarizer	preprocessing label binarizer	0.071429
attempts to retrieve a reliable	externals joblib get func	0.200000
classification task	score y_true	0.058824
parameters	y estimator parameters	0.500000
matrix product with the random matrix parameters	core base random projection transform x	0.500000
fit the model with x	rbfsampler fit x y	1.000000
cv	check cv cv	0.031250
class decorator for creating a class with a	externals add	0.142857
content of the data home cache	clear data home data_home	0.076923
significance of a cross-validated score with	score estimator x y cv	0.083333
generate cross-validated estimates for each input data point	x y cv	0.050000
parallel execution only a	externals joblib parallel	0.014085
of a wishart the	mixture wishart	0.125000
when using	utils	0.009709
number of splitting iterations in the cross-validator parameters	kfold get n splits x	0.111111
learn empirical variances from x	feature_selection variance threshold fit x	1.000000
i	i	1.000000
to repeatedly solve m*x=b	iter inv	0.200000
a fit	fit rfe	0.166667
the kernel k x	gaussian_process exponentiation call x	0.200000
and y is float32 then	y	0.002674
assignment problem using the hungarian algorithm	assignment	0.111111
a list of feature name -> indices mappings	dict vectorizer fit x	0.250000
matching pursuit	y n_nonzero_coefs	0.500000
from	externals	0.011494
requested by the callers	backend base effective	0.250000
for each input	val predict estimator x	0.045455
and return the content	externals joblib	0.004762
distances between the vectors in	distances	0.100000
private function used	ensemble parallel decision function estimators	0.333333
inplace row scaling of a csr	utils inplace row scale x scale	0.142857
a full lars path	omp path	0.100000
row of x	x y copy	0.142857
anova f-value for the	feature_selection f classif	0.200000
x to	transform x	0.016949
return the shortest path length from source to	single source shortest path length graph source cutoff	0.111111
returns the number of splitting iterations in the	split get n splits x y groups	0.111111
of x (as	decision function x	0.018868
the c and cpp files	c and cpp files	0.500000
each	cross val predict estimator x	0.045455
fit the model with	rbfsampler fit	0.250000
estimates for each input data point	y	0.002674
rate and potentially other states at	neural_network base optimizer iteration ends time_step	0.142857
score corresponds to the area under the precision-recall	score y_true y_score	0.025000
regression randomized logistic regression works by subsampling	randomized logistic regression	0.166667
predict is	clusterer compute labels predict	0.250000
perform a locally linear embedding analysis	manifold locally linear embedding x n_neighbors n_components	0.071429
helper function to	helper alpha	0.333333
parallel processing this method is meant to	joblib parallel	0.028571
partially fit underlying estimators	core one vs one classifier partial fit	0.166667
pickler to persist big data efficiently	numpy pickler	0.500000
the mean silhouette coefficient	metrics cluster silhouette score x labels	0.250000
fit linear	sgdclassifier fit x	0.333333
to unit	copy	0.062500
using the	repr	0.012500
labeled faces in the wild lfw pairs	fetch lfw pairs	0.018868
wild lfw pairs dataset this dataset is a	fetch lfw pairs subset	0.035714
the	covariance	0.028986
x_scale	x_scale	1.000000
of the dual gap convergence criterion the specific	covariance dual gap emp_cov precision_	0.071429
a lower bound on	mixture dpgmmbase lower bound	0.071429
the array from the meta-information and the	joblib zndarray wrapper read unpickler	0.043478
return the kernel k x y and	gaussian_process constant kernel call x y	0.333333
find the first prime element in the	hungarian state find prime in	0.333333
generate	n_features	0.083333
voting classifier	ensemble voting classifier	0.031250
function with the given arguments	func	0.011364
with the generative model	pca get	0.076923
search	core base search cv	0.033333
compute	base pca	0.071429
return the kernel k x	gaussian_process pairwise kernel call x	0.333333
index of the leaf	tree base decision tree apply	0.166667
online learning prevents rebuilding of cftree from scratch	cluster birch partial	1.000000
spectral embedding for	spectral embedding	0.200000
number of	effective n	0.333333
center x y and scale if the scale	cross_decomposition center scale xy x y scale	1.000000
random projection p only changes	johnson lindenstrauss min dim n_samples eps	0.142857
reproducibility flips the sign of elements of	utils deterministic vector sign flip	0.066667
generate train test indices	base shuffle split iter indices	0.250000
number of splitting iterations in the cross-validator	model_selection base kfold get n splits	0.111111
the	mixture base mixture	0.111111
routine for validation	directed dtype	0.166667
decomposition	decomposition	0.238095
validity of the	metric p metric_params	0.100000
number of splitting iterations in the cross-validator parameters	out get n splits x y	0.111111
transform the given label sets parameters	preprocessing multi label binarizer transform y	0.250000
non zero row of x	transform x y	0.031250
make sure that an estimator implements	core check estimator estimator	0.142857
the free energy f	rbm free energy	0.066667
and maximum to be used for later scaling	max scaler fit x y	1.000000
back the data to the original	preprocessing robust scaler inverse transform x	0.066667
to a large sparse linear system of equations	lsqr a b damp atol	0.500000
terminal regions (=leaves)	update terminal region tree terminal_regions leaf	0.200000
impute all missing values in x	preprocessing imputer transform x	1.000000
a binary metric for multilabel classification parameters	average binary score binary_metric y_true	0.500000
matrix_chol	matrix_chol	0.625000
the grid of alpha values for	linear_model alpha grid x y	0.166667
generate a sparse random projection matrix	random projection fit x y	0.333333
returns a nicely formatted statement displaying the function	func args kwargs object_name	0.333333
fun	fun fun	1.000000
data precision matrix with the generative	pca get precision	0.066667
breakdown	linear_model breakdown	0.333333
scale back the	scaler inverse transform x	0.052632
the given param_grid	grid search cv get param iterator	0.166667
the optimal batch size	joblib auto batching mixin compute batch size	0.333333
2d	2d	1.000000
updates terminal regions to median estimates	terminal region tree	0.100000
described in [rouseeuw1984]_ aiming at computing mcd	n_support remaining_iterations initial_estimates	0.111111
squared euclidean or frobenius norm	squared norm	0.500000
the posterior log probability of the samples x	multinomial nb joint log likelihood x	0.500000
the range of	utils randomized range finder	0.083333
ends	ends	1.000000
array corresponding to this	joblib numpy array	0.250000
timestamp when pickling to avoid the hash depending	externals joblib memorized func reduce	0.050000
in pipeline after transforms	core pipeline fit predict x	0.166667
when using the	repr	0.012500
generate cross-validated estimates for each input	estimator x y cv	0.050000
regression target at each stage for	gradient boosting regressor staged	0.500000
tp / tp + fp where tp is	score y_true y_pred labels pos_label	0.027778
number of splitting iterations in the cross-validator	get n splits	0.111111
back	preprocessing standard scaler inverse transform	0.066667
from	sample_weight	0.018519
eval_gradient	eval_gradient	0.312500
truncated	truncated x	0.200000
compute the median of	utils get median	0.166667
param logic estimators that implement the	utils check partial fit	0.038462
to make cache size fit in bytes_limit	externals joblib memory reduce size	0.083333
reconfigure the	base configure n_jobs parallel	0.500000
fits the oracle approximating shrinkage covariance model	covariance oas fit x	0.083333
transform with the	transform x	0.016949
model with stochastic gradient descent	coef_init intercept_init	0.333333
validation and conversion of csgraph inputs	graph csgraph directed dtype csr_output	0.166667
evaluate a score by cross-validation read more in	model_selection cross val score	0.166667
text report showing the main classification metrics read	metrics classification report	0.166667
for the voting classifier valid parameter	voting classifier set	0.037037
the average hamming loss	hamming loss y_true y_pred labels sample_weight	0.333333
estimates for each	val predict estimator x y	0.045455
used to hash objects that won't normally pickle	my hash	0.333333
to make cache size fit in bytes_limit	joblib memory reduce size	0.083333
the timestamp when pickling to avoid	joblib memorized func reduce	0.050000
gaussian process model fitting method	gaussian_process gaussian process fit x y	0.250000
that implement the partial_fit	utils check partial	0.038462
full covariance matrices	density full	0.166667
number of splitting iterations in the	out get n splits x y	0.111111
squared euclidean or frobenius norm of x	utils squared norm x	1.000000
feature mappings	feature_extraction dict vectorizer	0.100000
params with given gradients parameters	updates grads	0.076923
convert coefficient matrix to dense array format	mixin densify	0.100000
perform classification on an array of	neighbors nearest centroid predict	0.142857
returns the huber loss and the	linear_model huber loss and	0.166667
posterior log probability of	core multinomial nb joint log likelihood	0.083333
estimates for each input	val predict	0.045455
laplacian kernel	metrics laplacian kernel	0.166667
scale back	standard scaler inverse transform x copy	0.066667
estimator with the best found	model_selection base search cv predict proba x	0.076923
for elastic net parameter search	xy l1_ratio	0.250000
number of splitting iterations in the cross-validator	model_selection cviterable wrapper get n splits	0.111111
in the wild lfw pairs dataset this dataset	datasets fetch lfw pairs subset	0.035714
the given data	y	0.002674
score on the given data if the	score x y	0.030303
samples x to the separating hyperplane	base lib svm decision function x	0.250000
rand index adjusted for	adjusted rand score	0.333333
build a batch of estimators within a	parallel build estimators n_estimators ensemble x	0.166667
reverse	feature_selection selector mixin inverse transform	1.000000
right fileobject from a	externals joblib read fileobject fileobj	0.100000
transforms features by scaling	minmax scale x feature_range axis copy	0.200000
split data into	predefined split split	0.250000
a transform function to portion of selected features	preprocessing transform selected x transform selected copy	0.333333
node	cluster cfnode	1.000000
generate an array with block checkerboard	datasets make checkerboard	1.000000
as the maximizer of the	gaussian_process gaussian process arg max	0.047619
seeds for	bin seeds	0.250000
validity of the input	metric p metric_params	0.100000
update the dense dictionary factor in place	decomposition update dict dictionary y code verbose	1.000000
the huber loss and the gradient	huber loss and gradient	0.333333
checking for random	n_features	0.083333
is	externals joblib is	0.500000
estimate model parameters with	base mixture fit x	0.200000
not found and	line search	0.029412
the number of splitting iterations in	leave one out get n splits	0.111111
in the wild lfw pairs dataset	lfw pairs subset	0.035714
the closest cluster each	cluster	0.021277
np dot x w	dot w x	1.000000
non-negative matrix factorization nmf	negative factorization x	0.043478
selectfrommodel meta-transformer only once	feature_selection select from model partial	1.000000
u such that	flip u	0.047619
and q_ijs	params p neighbors degrees_of_freedom	0.250000
and false positives per binary classification threshold	metrics binary clf curve y_true y_score	0.090909
mutual information between continuous and discrete variables	mi cd c d n_neighbors	1.000000
lfw pairs dataset this dataset is	lfw pairs	0.018868
the hash depending	memorized func	0.016949
model	base pca get	0.076923
callable case for	pairwise callable	0.083333
we	joblib memorized func	0.014706
does not need to	y residual	0.500000
convert	coef mixin	0.090909
generate train	core base shuffle split	0.166667
determine absolute sizes of	translate train sizes	0.066667
class	ensemble forest classifier	0.666667
partially fit underlying estimators should be used	one vs one classifier partial fit x y	0.166667
estimates for each input data	core cross val predict estimator x	0.045455
of feature name -> indices mappings	feature_extraction dict vectorizer fit x y	0.250000
read more in the :ref user guide <mini_batch_kmeans>	mini batch kmeans	0.166667
calculate true and false positives per binary	metrics binary clf curve y_true y_score	0.090909
folders to	memory	0.015625
all methods a parallelbackend must implement	parallel backend	0.030303
regression target at each stage for	regressor staged	0.500000
return the kernel k x	gaussian_process exponentiation call x	0.200000
each input	core cross val predict	0.045455
opposite of the local outlier factor	local outlier factor decision	0.125000
beta-divergence of	beta	0.090909
the shortest path length from source to	single source shortest path length graph source cutoff	0.111111
covariance matrices from a given template	covariance type tied_cv covariance_type	0.333333
the validity of the input parameters	x metric p metric_params	0.100000
proper format	weight boosting validate x	1.000000
the paired cosine distances between x and y	metrics paired cosine distances x y	0.333333
to avoid the hash	joblib memorized func	0.014706
sum w[i] (y[i] - y_[i]) ** 2	y sample_weight y_min y_max	0.166667
transform on	transform x	0.016949
a list of feature names ordered by their	get feature names	0.090909
x format check x format and	allocation check	0.062500
used to fit an estimator within	fit estimator estimator x	0.055556
estimates for each input data	cross	0.037037
for the	covariance empirical covariance	0.071429
error regression loss read more	error y_true	0.111111
test_size and train_size at init	shuffle split init test_size train_size	0.250000
number of splitting iterations in	out get n splits	0.111111
the sparse components	sparse pca transform	0.500000
generate	generate	1.000000
too rare or too common	feature_extraction count vectorizer limit	0.250000
avoid the hash	joblib memorized	0.015625
a memmap instance to reopen on	memmap a	0.050000
adjusted for	metrics cluster adjusted	0.333333
returns the number of splitting iterations in the	predefined split get n splits	0.111111
computes the free energy f v =	free energy	0.066667
representation of	utils shape repr	0.013699
back the data to the original representation	inverse transform x copy	0.066667
probability calibration with	cv	0.009009
cross-validated estimates for each input data	cv	0.009009
for all meta estimators	meta estimator	0.062500
estimate the precisions parameters of	bayesian gaussian mixture estimate precisions	0.166667
compute the residual (= negative gradient)	ensemble binomial deviance negative gradient y	0.333333
generate train	core	0.015385
used to compute log probabilities within	parallel predict log proba	0.058824
apply a transform	preprocessing transform selected x transform	0.333333
tolerance which is independent of	cluster tolerance	0.058824
types	externals joblib parallel backend base	0.034483
number of splitting iterations in the cross-validator parameters	group out get n splits x y groups	0.111111
score by cross-validation read more in the :ref	model_selection cross val score estimator x	0.166667
out_file	out_file	1.000000
fit the model	multi output estimator fit	0.200000
samples can be different from	calibrated classifier	0.083333
with a metaclass	externals add metaclass metaclass	0.166667
fit ridge regression model parameters	linear_model base ridge cv fit	1.000000
stochastic gradient descent	base sgdregressor	0.100000
catch and hide warnings	warnings	0.076923
for	estimator x	0.030303
coverage error measure compute	coverage error y_true	0.166667
a given mapping	class_mapping	0.125000
estimate the precisions	gaussian mixture estimate precisions	0.166667
under the curve auc using the	auc	0.020408
distance of the samples x	svc decision function x	0.200000
given autocorrelation parameters theta	theta	0.076923
load text files with	datasets load files	0.500000
the kernel k x y and	gaussian_process dot product call x y	0.200000
standardize	with_centering with_scaling	0.200000
full	multivariate normal density full x means	0.166667
class weights for	utils compute class	0.166667
passive aggressive regressor read more	passive aggressive regressor	0.125000
center to the median and component wise scale	scale x	0.043478
compute the average hamming loss	hamming loss y_true y_pred labels sample_weight	0.333333
update the bound	mixture bound state	0.500000
graphlasso	graph lasso cv	0.333333
class weights for	class	0.071429
curve auc using the trapezoidal rule	metrics auc	0.040000
mixin class for all regression estimators in scikit-learn	regressor mixin	1.000000
r^2 coefficient of determination regression score function	r2 score y_true y_pred	0.125000
evaluate the accuracy of a classification	y_true	0.021739
each input data point	core cross val predict estimator	0.045455
apply clustering to	clustering affinity n_clusters	0.166667
input validation for standard	y x y accept_sparse dtype	0.250000
exception types	externals joblib parallel backend base	0.034483
update parameters with given gradients parameters	neural_network base optimizer update params grads	1.000000
number of splitting iterations in	one group out get n splits x	0.111111
of the kl divergence of p_ijs	manifold kl divergence	0.083333
l1 distances between the vectors in	metrics paired manhattan distances	0.083333
suffix when	utils	0.009709
create a six moves urllib namespace that resembles	module six moves urllib	0.333333
identity function	preprocessing identity	1.000000
and last element of numpy	and last element arr	0.166667
c such that for c in (l1_min_c infinity)	c	0.022222
the process or	backend	0.016949
split data into training and test	series split split x y groups	0.200000
using	factor	0.100000
the gaussian	gaussian_process gaussian	0.250000
rescale data so as to support sample_weight	linear_model rescale data x y sample_weight	1.000000
back the	standard scaler inverse transform x	0.066667
of determination regression score	r2 score y_true y_pred sample_weight multioutput	0.125000
estimator's fit method supports	utils has fit parameter estimator	0.500000
coverage error measure compute how far we need	metrics coverage error y_true y_score sample_weight	0.166667
such that for c	c	0.022222
of the determinant of a wishart	wishart	0.062500
the descriptors of a memmap instance to reopen	memmap a	0.050000
array	array	0.692308
oracle approximating shrinkage algorithm	oas x assume_centered	0.500000
data	cross val	0.038462
split data into training and test set	base shuffle split split x y groups	0.200000
density	density	0.347826
number of splitting iterations in the cross-validator parameters	one group out get n splits x	0.111111
for full	density full x means covars	0.166667
pairs dataset this dataset is a	pairs subset	0.125000
returns cluster labels	cluster dbscan	0.125000
like assert_all_finite but only	utils assert all finite	0.333333
learn vocabulary and idf return term-document	feature_extraction tfidf vectorizer fit transform raw_documents y	0.250000
matrices w h whose product approximates	x w h	0.035714
blup parameters and evaluates the reduced likelihood function	reduced likelihood function	0.041667
update	decomposition update dict	0.333333
to be captured	parallel backend base get exceptions	0.166667
kernel k x y and optionally its gradient	dot product call x y eval_gradient	0.333333
call predict_log_proba on	predict log proba	0.029412
data precision matrix with the generative model	decomposition base pca get precision	0.066667
private function used to fit a single tree	trees tree forest x	0.142857
and hence	preprocessing binarizer fit x y	0.142857
whether the kernel is stationary	kernel mixin is stationary	0.333333
generate	y	0.002674
lower bound for the mean	bound means	0.250000
voting classifier valid parameter keys	ensemble voting classifier set params	0.037037
and raise valueerror if not valid	base gradient boosting	0.100000
a text report showing	report	0.047619
write array bytes	numpy array wrapper write array array	0.500000
check input and compute prediction of init	base gradient boosting init decision	0.142857
private function used to partition estimators	ensemble partition estimators	0.200000
vectors individually to unit norm vector length	normalize x norm axis copy	0.200000
pickle-protocol - return state	core isotonic regression getstate	0.250000
estimate the precisions parameters of the	gaussian mixture estimate precisions nk xk sk	0.166667
independent representation	shape repr	0.013699
random matrix generation	check input size n_components n_features	0.200000
names ordered by their indices	names	0.090909
a tolerance which is independent of	cluster tolerance x tol	0.058824
range approximates the range of	randomized range	0.083333
building a cv in a	cv cv x y	0.031250
moved objects in six moves urllib_response	module six moves urllib response	0.333333
test	base shuffle split iter	0.166667
the kddcup99 dataset downloading it	datasets fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
thresholding	thresholding y output_type classes threshold	0.500000
check if there is any negative value	utils check non negative x whom	0.333333
utility for building a cv	cv cv x	0.031250
ward clustering based on	cluster ward tree x connectivity n_clusters return_distance	0.250000
opening the right fileobject from a filename	read fileobject fileobj filename mmap_mode	0.250000
check if vocabulary is empty or	check vocabulary	0.250000
operation is meant to be	index_file_path data_folder_path slice_ color	0.033333
into subcluster centroids dimension	cluster birch	0.090909
and predict_log_proba of the	predict log proba x	0.045455
decorator used to capture the	check_pickle	0.040000
to fit a single tree	build trees tree forest x y	0.142857
binary	average binary	0.500000
lower bound on model evidence based on	lower bound	0.071429
passive aggressive classifier read more in the	passive aggressive classifier	0.125000
avoid the hash depending from it	externals joblib memorized	0.013699
returns the number of splitting iterations in	get n splits x y	0.111111
determine the number of jobs	effective n jobs n_jobs	0.333333
the determinant of a wishart	wishart	0.062500
lad updates terminal regions	ensemble least absolute error update terminal region tree	0.200000
the l1	paired manhattan	0.333333
an arbitrary python object into	value filename	0.083333
the kl divergence of	manifold kl divergence	0.083333
estimator	estimator	0.338235
calculate approximate	latent dirichlet allocation	0.500000
of csgraph	utils sparsetools validate graph csgraph directed	0.250000
directory in which	get output dir	0.047619
a sparse random	utils random choice csc	0.333333
train test	shuffle split iter	0.166667
the l1 distances between the vectors	manhattan distances	0.083333
the long type introduces an	repr	0.012500
number of splitting iterations in the cross-validator	model_selection leave one out get n splits x	0.111111
the median and component wise scale	preprocessing robust scale x	0.125000
sample from the distribution p(h|v)	bernoulli rbm sample hiddens v rng	1.000000
find two non-negative matrices w h	x w h	0.035714
for reducing the size of the	items root_path	0.066667
train_sizes	train_sizes	1.000000
shift clustering using	shift	0.100000
of workers	parallel initialize	0.333333
given mapping	class_mapping	0.125000
edges for a 3d image	make edges 3d n_x n_y n_z	0.250000
representation of	shape repr	0.013699
shrunk on the diagonal read	shrunk	0.043478
pairwise matrix in	parallel pairwise x y	0.166667
classifier from	classifier	0.013699
log probability for	log multivariate	0.500000
a locally linear embedding analysis on the	locally linear embedding x n_neighbors	0.071429
a locally linear embedding analysis on the	manifold locally linear embedding x n_neighbors	0.071429
training subsets incrementally	incremental fit	0.500000
makes	copy	0.125000
v	v	0.315789
and	y sample_weight	0.071429
for each	predict estimator	0.045455
paired cosine distances between x	paired cosine distances x	0.333333
a given	x y scorer	0.111111
slice_	slice_	1.000000
the actual data loading for the lfw	lfw	0.068966
to the binary classification	y_true y_score average sample_weight	0.076923
samples x	x y sample_weight	0.012987
data	base pca get	0.076923
the score on the given	score x	0.033333
the number of splitting iterations in	cviterable wrapper get n splits x	0.111111
meta-information and the z-file	zndarray wrapper read unpickler	0.043478
the log-likelihood of a gaussian data set with	covariance score	0.071429
that for c	c x y	0.030303
computes the position	mds fit x y init	0.066667
independent representation	utils	0.009709
arpack iteration did not converge attributes	arpack no convergence	1.000000
data for binary classification used in hastie	make hastie	0.125000
relationship	cluster	0.021277
version of a read file object	read file fobj	1.000000
hash depending from	externals joblib memorized func	0.013158
fit linear model	sgdclassifier fit x y	0.333333
parallel processing this	externals joblib parallel	0.014085
placeholder for fit	fit	0.003257
minimum covariance determinant matrix	covariance fast mcd x	0.250000
the density model on	kernel density	0.083333
best	cv predict proba x	0.068966
convert coefficient	mixin	0.037037
folders to make cache size fit in	joblib memory reduce size	0.083333
the maximum likelihood covariance estimator parameters	covariance empirical covariance x	0.166667
from	memorized	0.015873
return a tolerance which is independent of	tolerance	0.045455
point	point	0.600000
x and y	x y sum_over_features size_threshold	0.250000
estimator and predict	and predict estimator	1.000000
reproducibility flips the sign	utils deterministic vector sign	0.066667
to be	preprocessing	0.105263
for	means	0.153846
separating hyperplane	svm one class svm decision function	0.333333
pairwise matrix in	metrics parallel pairwise	0.166667
collection of text documents into	vectorizer	0.022222
fit the model using x	linear_model base randomized linear model fit x	1.000000
the curve auc using the	auc x	0.040000
by	y	0.002674
a cv	cv cv x y classifier	0.031250
returns the number of splitting iterations in	one group out get n splits x	0.111111
decision tree classifier	decision tree classifier	1.000000
write	externals joblib numpy array wrapper write	1.000000
a platform	utils shape	0.013699
finds seeds for mean_shift	bin seeds x bin_size min_bin_freq	0.500000
in [rouseeuw1984]_ aiming at computing mcd	c step x n_support remaining_iterations initial_estimates	0.111111
buffered version of a read file object	joblib buffered read file fobj	0.500000
incrementally fit the model to	core multi output regressor partial fit x y	0.200000
and returns the labels	y	0.002674
get	randomized linear model get	0.500000
the curve auc	auc x y	0.040000
the directory in which are persisted the	output dir	0.047619
display the	parallel print	0.142857
regressor from the training set	regressor fit	0.500000
from distances	distances	0.100000
not enabled for sparse matrices	robust	0.090909
full covariance matrices	density full x means	0.166667
value of the log of the determinant	log	0.018868
predict regression target at each stage	gradient boosting regressor staged predict	0.500000
best found	model_selection base search cv predict	0.076923
of the dual gap convergence criterion the specific	dual gap emp_cov precision_	0.071429
the sample weight array	base sgd validate sample weight sample_weight n_samples	0.333333
true and false positives per binary	binary clf curve y_true y_score	0.090909
that each sample is predicted as	x check_input	0.250000
score function best possible score is	score y_true y_pred sample_weight	0.062500
which are going	multiprocessing backend	0.038462
log of probability estimates	logistic regression predict log proba	1.000000
training data and y	y	0.005348
svd by arpack or randomized	n_components svd_solver	1.000000
don't store the	joblib memorized func	0.014706
checker utility for building a cv in a	core check cv cv	0.031250
for indices increasingly apart the distance	externals joblib verbosity filter index	0.055556
classifier valid	classifier	0.013699
to build a batch of estimators within a	parallel build estimators n_estimators ensemble x	0.166667
function used to partition estimators	partition estimators	0.200000
of determination r^2	multi output regressor score x	0.200000
fit a multi-class	base sgdclassifier fit	0.076923
the	multiprocessing	0.045455
in [rouseeuw1984]_ aiming at computing	c step x n_support remaining_iterations initial_estimates	0.111111
fit estimator and predict values for a given	fit and predict estimator x y train	0.250000
scale back the data to the original representation	standard scaler inverse transform x copy	0.066667
utility for building a cv in a	core check cv cv x y classifier	0.031250
y1	y1	1.000000
path with	path	0.025641
the final estimator fits all	core pipeline	0.076923
compute data	base pca	0.071429
distribution	mean covar covariance_type	1.000000
workers requested by the callers	base effective	0.250000
an unfitted estimator	estimators unfitted name estimator	0.142857
hash depending	joblib memory	0.016949
gaussian	gaussian_process gaussian	0.250000
evaluate predicted target values for x relative	scorer call estimator x	0.166667
generate	x y	0.002155
fit the model and transform	fit transform	0.100000
dual gap convergence criterion the specific definition is	dual gap	0.071429
function opening the right fileobject from	read fileobject fileobj	0.100000
and evaluates the reduced likelihood function	process reduced likelihood function	0.047619
the validity of	metric p metric_params	0.100000
linear model parameters	base sgdregressor	0.200000
as the maximizer of the reduced	process arg max reduced	0.200000
types to	externals joblib parallel backend base	0.034483
common	vectorizer	0.022222
determine the optimal batch size	auto batching mixin compute batch size	0.333333
the k-neighbors of a point	neighbors kneighbors mixin kneighbors x	0.125000
log probability for	log	0.018868
call with the given	format call	0.200000
type	shape	0.011765
pool	externals joblib multiprocessing	0.052632
to be used for	preprocessing	0.105263
a locally linear embedding analysis on the	locally linear embedding x n_neighbors n_components	0.071429
boolean mask x == missing_values	mask x value_to_mask	0.333333
new subcluster into	cf subcluster subcluster	0.500000
strip the headers by removing everything before the	datasets strip	0.076923
getter for the	covariance empirical	0.100000
h whose product approximates the non-	h	0.041667
mean absolute	mean absolute	1.000000
for the california housing dataset	fetch california housing	0.083333
classifier on x and y	x y alpha c	0.333333
the model according to the given training data	y sample_weight	0.035714
regression model we can also predict	regressor predict	0.200000
area under the curve auc using	metrics auc x y	0.040000
thread	joblib	0.007299
check if vocabulary is empty or missing	feature_extraction vectorizer mixin check vocabulary	0.250000
lars path	path	0.025641
norm vector length	norm axis	1.000000
the :ref user guide <univariate_feature_selection>	fwe	0.200000
dict	dict	1.000000
the kddcup99 dataset	fetch brute kddcup99	0.166667
the vectors rows of u such that	u	0.032258
classification	y_true y_score pos_label sample_weight	0.066667
reducer function to a	externals joblib customizable	0.200000
path length from source to all	path length graph source	0.200000
with stochastic gradient descent	linear_model base sgdregressor partial	0.333333
inefficient to	classes	0.025641
platform independent representation	shape repr	0.013699
base class for all ensemble classes	base ensemble	1.000000
alpha	alpha	0.352941
and y	y gamma	1.000000
the oracle approximating shrinkage covariance model	covariance oas	0.083333
meant	index_file_path data_folder_path slice_ color	0.033333
to the cache for	joblib memorized	0.015625
from	fit	0.003257
the long type introduces an 'l'	shape	0.011765
the	memory	0.031250
transform binary labels back to	preprocessing label binarizer inverse transform	0.500000
number of cpus	cpu count	0.333333
a one-vs-all fashion several regression and binary	y classes neg_label pos_label	0.111111
or not	feature_extraction mask edges	0.500000
import path as a list of module names	resolv_alias win_characters	0.166667
function for factorizing common classes param	partial fit first call clf classes	0.058824
"fits the model to the training set x	predict x	0.011765
cache folders to make cache size fit	size	0.032258
a general function given points on	y reorder	0.111111
the hash depending from	externals joblib memory	0.016949
discrete target variable	y discrete_features n_neighbors	0.500000
terminal regions to median	terminal region tree	0.100000
parameters for this	params deep	0.111111
input data	cross val predict	0.045455
cf tree for	cluster birch fit x	0.200000
template method for updating terminal regions (=leaves)	loss function update terminal region tree terminal_regions leaf	0.200000
assumes x contains only categorical features	preprocessing one hot encoder fit transform x	1.000000
v h	v	0.052632
the search over parameters	base search cv fit x y	0.166667
the function called with the given arguments	joblib memorized func get output	0.125000
updates terminal	terminal region tree terminal_regions leaf x	0.066667
is worthy enough	cfsubcluster	0.111111
probabilities for each sample	samples x	0.142857
to unit norm vector length	normalize x norm axis copy	0.200000
function func with arguments *args and	func	0.011364
and breiman [2]	make friedman3 n_samples	0.166667
perform classification on samples in	svm base svc predict	0.111111
context of the memory	externals joblib memory	0.016949
returns the number of splitting iterations in	cviterable wrapper get n splits x	0.111111
and y read more in the :ref	y	0.002674
function total_seconds introduced in python2	utils total seconds delta	0.200000
seconds	seconds	1.000000
error regression loss read more in	error y_true y_pred	0.125000
to avoid the hash depending from it	externals joblib memorized	0.013699
for 1 iteration	total_samples batch_update parallel	0.500000
data to vectors and cluster	and cluster data vectors	0.333333
building a cv in a	core check cv cv x y	0.031250
using thresholding	thresholding y output_type	0.500000
fit the ardregression model according to the given	linear_model ardregression fit x	0.250000
also predict based	predict x	0.011765
rand index adjusted for	metrics cluster adjusted rand score labels_true	0.333333
compute the decision function	decision function	0.025000
the cache for	joblib memorized	0.015625
the local structure is retained	trustworthiness x x_embedded n_neighbors precomputed	0.200000
scale back the	scaler inverse transform	0.058824
neighbors within a given radius of a	lshforest radius neighbors x radius	0.142857
whether the kernel is	kernel mixin is	0.333333
last step in pipeline after transforms	pipeline	0.083333
special case the hasher	hasher	0.111111
approximates the range of	range finder	0.083333
bin_size	bin_size	1.000000
the cosine distance	neighbors lshforest compute distances query candidates	0.333333
coefficient matrix to	linear_model sparse coef mixin	0.090909
estimators should be used when memory is inefficient	y classes	0.027778
compute elastic net path with coordinate	path x	0.045455
and smooth feature occurrences	y	0.005348
shift	shift x bandwidth seeds bin_seeding	1.000000
active default	active	0.166667
each input data	predict estimator x y	0.045455
load the covertype dataset downloading it if necessary	datasets fetch covtype data_home download_if_missing random_state	0.333333
ledoit-wolf	ledoit wolf shrinkage x	0.250000
generate cross-validated estimates for each input	cv	0.009009
can actually run in parallel	externals joblib parallel backend	0.029412
feature names ordered by their indices	feature names	0.090909
binary labels back	label binarizer inverse	0.166667
found	search	0.115385
matrix given column class distributions parameters	classes class_probability random_state	0.166667
the right fileobject from a filename	read fileobject fileobj filename mmap_mode	0.250000
matrix shrunk on the diagonal read more	shrunk	0.043478
x y and optionally its gradient	product call x y eval_gradient	0.333333
for a given dataset	y scorer	0.111111
reconstruct the	feature_extraction reconstruct	0.333333
diagonal of the kernel k x x	compound kernel diag x	1.000000
write	joblib write	0.500000
call predict on the estimator with the best	cv predict x	0.125000
when memory is inefficient to train all	y classes	0.027778
random forest	random forest	1.000000
the pixel-to-pixel gradient connections edges are weighted with	img mask return_as dtype	0.166667
and false positives per binary	metrics binary clf curve	0.090909
covered	utils hungarian state	1.000000
perform classification on an array of test	neighbors nearest centroid predict	0.142857
cpp files	cpp files	1.000000
along an axis on a csr or	x axis	0.015385
lad updates terminal regions to median	ensemble least absolute error update terminal region tree	0.200000
or not	feature_extraction mask edges weights	0.500000
helper function for factorizing common classes param	partial fit first call clf classes	0.058824
detects the soft	class svm fit	0.125000
is restricted to the binary classification	y_true y_score pos_label	0.066667
of time	time	0.047619
coverage error measure compute how far	coverage error	0.166667
count and smooth feature occurrences	count x y	0.250000
predict_proba on the estimator with the best found	base search cv predict	0.076923
project data to vectors and cluster the	biclustering project and cluster data vectors n_clusters	0.333333
fit the	svr fit	0.333333
perform	responsibilities	0.142857
an extremely randomized tree	extra tree	1.000000
fit the model to	estimator fit x	0.200000
generate a random	n_samples n_features	0.250000
compute data precision matrix	get precision	0.052632
input validation for	accept_sparse dtype	0.200000
marginal	marginal	1.000000
learn the vocabulary dictionary and return term-document matrix	transform raw_documents y	0.100000
user provided precisions	mixture check precisions precisions covariance_type n_components n_features	0.250000
generate cross-validated estimates for each input data	estimator x y cv	0.050000
a transform	preprocessing transform selected x transform	0.333333
compute prediction of init	base gradient boosting init decision function	0.142857
a general function given points on	reorder	0.071429
the position of	mds fit x y	0.066667
building a cv in a user friendly	core check cv cv x	0.031250
transform binary labels back to	binarizer inverse transform	0.500000
validate user provided precisions	check precisions precisions covariance_type	0.250000
estimates for	core cross val predict	0.045455
graph of the	graph	0.042553
score the	score y_true y_pred	0.038462
the path of the scikit-learn data dir	data home data_home	0.055556
multiple files in svmlight	svmlight files files n_features dtype	0.200000
boolean thresholding of array-like or scipy sparse	preprocessing binarize x threshold copy	0.083333
as a sparse	decomposition sparse	0.111111
coefficient	linear_model sparse	0.076923
configure a copy of the base_estimator_ attribute	append	0.083333
maximizer of the reduced likelihood function	process arg max reduced likelihood function	0.333333
get the values	neural_network sgdoptimizer get	0.125000
back the data to	standard scaler inverse transform	0.066667
in the wild lfw pairs dataset this dataset	lfw pairs subset	0.035714
score the	score	0.010101
private function used to compute decisions within	ensemble parallel decision function estimators estimators_features x	0.500000
generative	base pca	0.071429
similarity of two clusterings	score labels_true labels_pred sparse	0.047619
boolean thresholding of array-like or scipy sparse matrix	preprocessing binarize	0.083333
or thread	joblib multiprocessing backend	0.052632
and concatenate results	y	0.002674
transformer on sparse	preprocessing imputer sparse	1.000000
call predict on the estimator with the	predict	0.006849
infers the dimension of a dataset of	dimension	0.050000
fit the model to	multi output estimator fit	0.200000
estimator with the best	cv predict proba x	0.068966
sparse components	sparse pca transform x	0.500000
function used to partition estimators	ensemble partition estimators	0.200000
print verbose message on the end	print verbose msg init end ll	0.333333
the grid of alpha values	alpha grid x y xy	0.166667
the neighbors within a given radius	neighbors x radius	0.142857
procedure described in [rouseeuw1984]_ aiming at computing mcd	x n_support remaining_iterations initial_estimates	0.111111
param	include self x	1.000000
get the	classifier get	0.200000
which are going to run in	joblib multiprocessing backend	0.052632
scale back the data to the original	scaler inverse transform x	0.052632
the weighted log	base mixture	0.111111
list of exception types to	parallel	0.019231
don't store the timestamp when pickling to avoid	externals joblib memorized func reduce	0.050000
number of splitting iterations in the	split get n splits x	0.111111
learn the vocabulary dictionary and return term-document matrix	feature_extraction count vectorizer fit transform raw_documents y	0.166667
csgraph	validate graph csgraph directed	0.250000
predict multi-class targets using underlying estimators	core output code classifier predict x	0.250000
fit label encoder	preprocessing label encoder fit y	0.500000
connectivity matrix	connectivity n_components	1.000000
the log probability under the model	mixture gmmbase score x y	0.500000
initialization of	initialize	0.181818
of estimators within a job	estimators	0.052632
unit norm vector length	normalize x norm axis copy	0.200000
the k-neighbors of a	neighbors kneighbors mixin kneighbors x n_neighbors	0.125000
returns the huber loss and	linear_model huber loss and	0.166667
the likelihood of the data under the model	mixture dpgmmbase score samples x	0.200000
along an axix on a csr or csc	axis x axis last_mean last_var	0.200000
filters the given args and kwargs using a	ignore_lst args kwargs	0.333333
probability calibration with sigmoid method platt 2000	sigmoid calibration df y sample_weight	0.500000
a batch of estimators within	estimators	0.052632
update	update h x w	0.500000
submatrix corresponding to	get submatrix	0.166667
back	inverse transform	0.062500
opening the right fileobject from a	externals joblib read fileobject fileobj	0.100000
binary and	binary	0.031250
optimal batch size	compute batch size	1.000000
data onto	transform x ridge_alpha	0.071429
only a fraction of time controlled by self	progress	0.100000
estimate the precisions	bayesian gaussian mixture estimate precisions nk xk sk	0.166667
precision matrix with the generative	precision	0.016667
the neighbors within a given radius of a	radius neighbors x radius	0.142857
provided precisions	check precisions precisions	0.250000
a callable that handles preprocessing and tokenization	feature_extraction vectorizer mixin build analyzer	0.333333
of last step in pipeline after transforms	core pipeline fit predict x y	0.166667
evaluates the reduced likelihood function for the	reduced likelihood function	0.041667
bias vectors	backprop x	1.000000
returns the number of splitting iterations in the	leave one out get n splits x y	0.111111
search over	base search cv fit x y	0.166667
y and	y	0.037433
of exception	externals joblib parallel backend	0.029412
maximum likelihood covariance estimator	covariance empirical covariance x	0.166667
kl divergence of p_ijs	manifold kl divergence	0.083333
returns n_neighbors	kneighbors x n_neighbors return_distance	0.500000
report	report	0.285714
perform	x responsibilities params min_covar	0.500000
handle the callable case	callable	0.058824
the svmlight / libsvm format into sparse csr	svmlight file f n_features dtype multilabel	0.066667
of neighbors for points in	neighbors	0.054054
median and component wise scale	preprocessing robust scale x	0.125000
that for c in (l1_min_c infinity) the	c x y loss	0.030303
handle the callable case for pairwise_{distances	metrics pairwise callable x y	0.083333
classification	y_true	0.173913
input	val predict estimator	0.045455
sign of elements	sign flip	0.066667
x	x z reg	0.066667
label sets with	preprocessing multi label binarizer transform	0.250000
expression of the dual gap convergence criterion the	covariance dual gap emp_cov	0.071429
generate cross-validated	cross val predict estimator x y cv	0.071429
data x	x doc_topic_distr	0.333333
numpy	externals joblib numpy	0.250000
build a batch of estimators within a job	ensemble parallel build estimators n_estimators ensemble x	0.166667
logistic regression	logistic regression path	0.333333
r^2 coefficient of determination regression score function	r2 score y_true y_pred sample_weight	0.125000
evaluate a score	score	0.020202
generate a random regression	regression n_samples n_features	1.000000
intercept	intercept	1.000000
and	y axis	0.250000
an unfitted	unfitted name	0.142857
return a	utils	0.009709
for each input data	predict estimator x y	0.045455
back to	preprocessing label binarizer inverse	0.166667
the number of splitting iterations in the cross-validator	model_selection base kfold get n splits	0.111111
theta	theta theta	1.000000
folder	joblib delete folder	1.000000
pairs dataset this dataset is a collection of	pairs subset	0.125000
under the curve auc using	auc x y	0.040000
compute area under the curve auc	auc x y	0.040000
transform on the estimator with	transform	0.011236
factor analysis fa a simple linear generative	factor analysis	0.166667
compute the maximum	preprocessing max	0.166667
fit kernelcenterer parameters	preprocessing kernel centerer fit k	1.000000
the mixture parameters	bayesian gaussian mixture	0.333333
locally linear embedding read more in the :ref	locally linear embedding	0.050000
cv in a user friendly	check cv cv x y	0.031250
the local outlier factor of	local outlier factor decision function	0.125000
display	print	0.076923
fit ridge regression model	linear_model ridge classifier fit x y	1.000000
for each mixture component	mixture gmmbase get covars	0.250000
fit the model from data in	manifold isomap fit	0.333333
generate	x	0.001692
fit the model to data	multi output estimator fit x	0.200000
evaluate	call x y	0.035714
dataset classification	datasets	0.015152
and a set	y axis	0.250000
matrix to	linear_model sparse	0.076923
type introduces an	utils	0.009709
constructs	function	0.021277
<mean_absolute_error>	sample_weight multioutput	0.500000
with likelihood terms for standard covariance types	log lik x initial_bound precs means	1.000000
score of	core score	0.166667
return the kernel k x	exponentiation call x	0.200000
random sample from	size replace	0.125000
returns a lower bound on	mixture dpgmmbase lower bound	0.071429
in bytes_limit	externals joblib memory	0.016949
convert	linear_model sparse	0.076923
new points into embedding	manifold locally linear embedding	0.062500
k	k	0.916667
and stored it as a zipped pickle	target_dir cache_path	0.142857
this is the time	time t	0.125000
a name for the	func name	0.047619
callable case for pairwise_{distances kernels}	callable x y	0.083333
used to compute log probabilities within a job	parallel predict log proba estimators estimators_features	0.250000
delegate	delegate	1.000000
computes multinomial loss and	linear_model multinomial loss w x y alpha	0.333333
median and component wise scale	scale	0.033333
shortest path length from source	utils single source shortest path length graph source	0.111111
computes the weighted graph of neighbors	neighbors mixin radius neighbors graph	0.066667
a given radius of a point	x radius	0.058824
of x and y	x y	0.002155
with a given mapping	transform y class_mapping	0.333333
fit the	core multi output estimator fit x y sample_weight	0.200000
new data into the already fitted lsh forest	neighbors lshforest partial fit x	0.200000
repeated splits for an arbitrary randomized	repeated splits	0.125000
to update terminal regions	error update terminal regions tree	0.500000
hessian in the case of a logistic loss	logistic grad hess w	1.000000
depending from	func	0.011364
solve the linear assignment problem using the	utils linear assignment	0.090909
svd parameters	svd m	0.500000
the best	cv	0.045045
log probability for full covariance	mixture log multivariate normal density full	0.333333
weights and	y	0.002674
load the rcv1 multilabel dataset	datasets fetch rcv1	0.333333
auto	auto	1.000000
last step in pipeline after transforms	core pipeline fit	0.166667
shutdown the process or	backend terminate	0.166667
gaussian process classification	gaussian process	0.083333
makes sure centering is not enabled for sparse	preprocessing robust scaler check array x copy	0.333333
x	gaussian_process dot product call x	0.200000
read value from cache and return it	externals joblib memorized result get	0.500000
t-sne objective function the absolute error of	error	0.020000
with	search cv	0.018182
to compute log probabilities within a job	predict log proba estimators estimators_features x	0.250000
from the meta-information and the	externals joblib zndarray wrapper read unpickler	0.043478
the rcv1 multilabel dataset downloading it if necessary	datasets fetch rcv1 data_home subset download_if_missing random_state	0.500000
returns the index of the leaf	apply	0.083333
of x	transform x y	0.031250
the process or thread pool	externals joblib multiprocessing	0.052632
reducer function to a given type in the	externals joblib customizable pickler register type	0.083333
model using x as training data	x skip_num_points	0.166667
h in multiplicative update nmf	decomposition multiplicative update h x w h	0.250000
introduces an 'l' suffix when using	utils shape repr	0.013699
for reproducibility flips the sign of elements	deterministic vector sign	0.066667
in-place	swap row x m n	1.000000
image samples in x into a matrix of	x	0.001692
graph of	graph	0.127660
w to minimize	x w	0.083333
the graphlasso covariance	covariance graph lasso cv fit	0.111111
decision function	decision function x raw_values	0.083333
regression	datasets make regression	1.000000
trained model parameters	base multilayer perceptron	0.142857
from features or	y sample_weight	0.017857
terminal regions to	terminal region tree terminal_regions	0.100000
stationary	stationary	1.000000
swaps two rows of a csc/csr matrix in-place	utils inplace swap row x m	0.250000
estimator on training subsets incrementally and compute scores	core incremental fit estimator estimator x y	0.200000
evaluate the density model on the data	neighbors kernel density score samples	0.250000
set x and	predict x y	0.043478
y - pred	y pred	0.250000
path parameters	omp path	0.100000
whose range approximates the range of	range	0.058824
as line_search_wolfe1 but fall back to line_search_wolfe2	wolfe12 f fprime xk pk	0.028571
computes the weighted graph of neighbors	mixin radius neighbors graph	0.066667
transform function to portion of selected features parameters	transform selected x transform selected	0.333333
model with coordinate descent parameters	linear_model	0.025641
fit the model and transform with	fit transform x	0.166667
score with the final estimator parameters	core pipeline score	1.000000
target_name	target_name	1.000000
normalize x according	log normalize x	0.200000
weighted graph of k-neighbors	kneighbors graph	0.500000
n_components	array n_components	1.000000
analysis a classifier	analysis	0.090909
validator	validator	1.000000
x y and	gaussian_process matern call x y	0.200000
estimates for	x y	0.002155
loader for the california housing dataset from statlib	california housing data_home download_if_missing	0.250000
set x and returns the	x y	0.002155
vectors individually to unit norm vector length	x norm axis copy	0.200000
base class for pca methods	base pca	0.071429
expression of the dual gap convergence criterion	dual gap emp_cov precision_	0.071429
line_search_wolfe2	wolfe12 f fprime xk pk	0.028571
for the case method='lasso' is :	xy gram	0.090909
classifier	classifier	0.397260
init	gradient boosting init decision function	0.142857
validation and	directed dtype	0.166667
a which this	externals	0.005747
long type introduces an 'l' suffix when	repr	0.012500
indices to split data into	model_selection cviterable wrapper split x	0.250000
compute non-negative matrix factorization nmf find two non-negative	non negative factorization x	0.043478
path with coordinate descent the	path x	0.045455
convert coefficient matrix to dense array format	sparse coef mixin densify	0.100000
computes	covariance empirical	0.200000
axis center to the median	x axis	0.015385
average of the decision	decision	0.027778
returns the huber loss and the gradient	linear_model huber loss and gradient w	0.333333
determinant with the	det fit x y	0.333333
back the	robust scaler inverse transform x	0.066667
recall the recall is the	metrics recall	0.033333
the	externals joblib memory	0.033898
x	transform x	0.084746
with the given arguments and	func call	0.047619
true and false positives per binary	metrics binary clf curve	0.090909
k	gaussian_process compound	1.000000
biclusters	consensus score	0.250000
a	a w axis	1.000000
of new samples can be different from	calibrated classifier	0.083333
prediction scores note this implementation is	metrics roc	0.040000
compute the l1 distances between the vectors	manhattan distances	0.083333
of the local outlier factor	local outlier factor decision	0.125000
radius-based neighbors searches	radius neighbors	0.043478
given template	type tied_cv	0.333333
deviance loss function	deviance	0.125000
function func with arguments *args and **kwargs,	func	0.011364
median of	median	0.066667
report showing the main classification metrics read	metrics classification report	0.166667
transforms features by scaling	scale x feature_range axis copy	0.200000
avoid the hash	func	0.011364
dictionary of all tokens in the raw documents	feature_extraction count vectorizer fit	0.125000
return precisions as a full	mixture dpgmmbase get precisions	0.250000
for the voting classifier valid parameter keys	voting classifier set	0.037037
input data point	core cross val predict estimator	0.045455
call predict on	predict	0.006849
full covariance matrices	full x means	0.166667
score	score estimator x	0.375000
inplace row scaling of	utils inplace row scale x	0.142857
function best possible score is	score y_true y_pred	0.038462
estimators that	utils check partial	0.038462
the log-likelihood of a gaussian data set	score	0.010101
in x into	x	0.001692
elastic net path with coordinate descent	linear_model enet path x	0.050000
such that for c in (l1_min_c infinity) the	c x	0.030303
loads data from module_path/data/data_file_name	load data module_path data_file_name	0.500000
updating terminal regions (=leaves)	update terminal region tree terminal_regions leaf	0.200000
uncompressed bytes from	binary zlib	0.166667
is restricted to the binary	y_score average	0.111111
long type introduces an 'l'	shape repr	0.013699
parameters and evaluates the reduced likelihood function	gaussian_process gaussian process reduced likelihood function	0.047619
a general function given points on	x y reorder	0.111111
elastic net path	enet path x	0.050000
neighbors	neighbors mixin radius neighbors	0.125000
the curve auc from prediction scores note	metrics roc auc score	0.166667
predict regression target at each stage for x	boosting regressor staged predict x	1.000000
utility for building a cv in a user	check cv cv x y classifier	0.031250
handle the callable case for	callable	0.058824
estimates for each input data point	x y	0.002155
given list of parameter objects and 'return_annotation'	init parameters return_annotation __validate_parameters__	0.125000
for c in (l1_min_c	c	0.022222
+ fn where tp is the number	score y_true y_pred labels pos_label	0.027778
contingency matrix describing the	contingency matrix labels_true	0.166667
the curve auc using the trapezoidal rule this	metrics auc x y	0.040000
the free energy f v = -	neural_network bernoulli rbm free energy	0.066667
fit linear model with stochastic gradient descent	linear_model base sgdregressor fit x y coef_init	1.000000
is float32 then dtype float32 is returned	metrics return float dtype	0.250000
mean and	incr mean	0.166667
a which this function is called to	externals joblib memorized func	0.013158
with constant block diagonal structure for biclustering	biclusters shape n_clusters noise minval	0.058824
depending	memorized func	0.016949
data from module_path/data/data_file_name	data module_path data_file_name	0.500000
to split data into	base shuffle split split	0.250000
read array from unpickler file handle	numpy array wrapper read array unpickler	1.000000
the lfw pairs dataset	lfw pairs	0.018868
median across axis 0 of a csc matrix	utils csc median axis 0	1.000000
classifiers	classifier	0.013699
score by cross-validation read more in the :ref	model_selection cross val score	0.166667
and cluster the result	and cluster	0.333333
a process or thread pool and return the	externals joblib multiprocessing backend	0.035714
incremental mean and	incr mean	0.166667
a gaussian	gaussian	0.029412
to by some authors	preprocessing label binarizer	0.071429
measure the similarity of two clusterings of	cluster fowlkes mallows score labels_true labels_pred sparse	0.333333
zipped pickle	target_dir cache_path	0.142857
by scaling each feature to a given	minmax scale	0.142857
to the binary classification task	y_true y_score pos_label	0.066667
of a csc matrix in-place	utils inplace swap row csc x m n	0.250000
an 'l' suffix when	repr	0.012500
wine	wine	0.666667
the autocorrelation parameters theta as the maximizer of	gaussian process arg max	0.047619
a byte string to the	externals joblib	0.004762
two columns of a csc/csr matrix in-place	utils inplace swap column x m	0.250000
c such that for c in	c x	0.030303
solve the isotonic regression model : min	core isotonic regression y	0.066667
thresholding of array-like or scipy sparse	binarize x threshold	0.083333
minimum covariance determinant mcd : robust estimator of	min cov det	0.500000
rational quadratic kernel	rational quadratic	1.000000
get the weights from an array of distances	get	0.012048
to cleanup a temporary folder if still existing	folder folder_path warn	0.250000
each non zero row of x	x y	0.002155
to build a batch of estimators within	build estimators n_estimators	0.166667
based on	neighbors	0.027027
bytes_limit	externals joblib memory reduce	0.030303
array-like or scipy sparse	preprocessing binarize	0.083333
any axis	x axis	0.030769
the gaussian process regression model we	gaussian process regressor	0.055556
the search over	search cv fit x	0.111111
the number of splitting iterations in	split get n splits	0.111111
output of transform is sometimes referred	transform y	0.023256
loading for the lfw	datasets fetch lfw	0.083333
an array	check array array	0.250000
incremental fit on a batch of samples	gaussian nb partial fit x y classes sample_weight	0.200000
corresponding to	externals joblib numpy	0.250000
and binary classification algorithms	y	0.002674
loading for the lfw pairs dataset	lfw pairs	0.018868
score for a fit across one fold	feature_selection rfe single fit rfe	0.200000
models computed by 'path' parameters	linear_model path residuals x y train test	0.250000
kappa	cohen kappa	0.250000
line_search_wolfe2 if suitable step length is	wolfe12 f fprime xk pk	0.028571
read an array using numpy memmap	joblib numpy array wrapper read mmap	1.000000
inplace row scaling of a csr or csc	inplace row scale	0.142857
input validation on an array	check array array	0.250000
do nothing and return	feature_extraction	0.037037
for full	normal density full	0.166667
find	utils hungarian state find	0.500000
curve auc	metrics auc	0.040000
huber loss and the gradient	linear_model huber loss and gradient w x y	0.333333
reproducibility flips the sign	utils deterministic vector sign flip	0.066667
precision matrix with	decomposition base pca get precision	0.066667
later	fit	0.003257
right fileobject from a	read fileobject	0.100000
cache key	joblib cache key	0.250000
multiple files in svmlight	svmlight files files n_features dtype multilabel	0.200000
dispatch	externals joblib parallel dispatch one	0.250000
fit is on grid	fit	0.003257
the c and	c and	0.500000
compute prediction of init	init	0.076923
the number of splitting iterations in the	group out get n splits x	0.111111
check if the test/test sizes are meaningful wrt	model_selection validate shuffle split n_samples test_size train_size	0.111111
estimates for each input	core cross val predict	0.045455
collection of text documents to	vectorizer	0.022222
low rank matrix with	datasets make low rank matrix	0.083333
embedding analysis on the	embedding x	0.200000
scale each non zero row of x	transform x y copy	0.142857
by definition a confusion matrix :math c	metrics confusion matrix	0.500000
incrementally	core multi output regressor partial	1.000000
actually run in parallel	externals joblib parallel	0.014085
number of splitting iterations in the	cviterable wrapper get n splits x	0.111111
handle the callable case for	callable x y	0.083333
the maximizer of the reduced likelihood function	process arg max reduced likelihood function	0.333333
along an axix on a	axis x axis	0.083333
the optimal	externals joblib auto batching mixin compute	0.333333
a temporary folder if still existing	joblib delete folder folder_path	0.250000
em update for	latent dirichlet allocation em step	0.500000
private function used to	function estimators	0.333333
state of	state	0.133333
free energy f	rbm free energy	0.066667
fit	linear_model base sgdclassifier fit x y	0.333333
the log of	log	0.018868
predict using the gaussian	gaussian	0.029412
fit the hierarchical clustering on the data	cluster agglomerative clustering fit	0.250000
avoid the hash depending from	memory	0.015625
to a	lsqr a	0.037037
the kernel k x	gaussian_process rbf call x	0.200000
the case of a logistic	logistic	0.047619
returns false for indices increasingly apart	externals joblib verbosity filter index	0.055556
is worthy enough to be merged if	cfsubcluster merge subcluster nominee_cluster threshold	1.000000
possible outcomes for samples in x	ensemble voting classifier predict	0.100000
for reducing	items root_path	0.066667
of the data onto the	ridge_alpha	0.052632
or lasso path using lars algorithm	linear_model lars path x y	0.100000
found and raise an exception	line search	0.029412
the precisions	precisions x	0.250000
the autocorrelation parameters theta as the maximizer of	process arg max	0.047619
check	check params	0.200000
isotonic regression model : min sum w[i] (y[i]	core isotonic regression y	0.066667
repr	repr	0.087500
separating hyperplane	svm	0.142857
thread	multiprocessing backend	0.038462
store the timestamp when pickling to avoid	joblib memory reduce	0.030303
build from the c and	build from c and	0.500000
a given radius of	radius	0.045455
of module names and a name	func name	0.047619
h whose	h n_components	0.166667
avoid	joblib memorized func	0.014706
output of transform is	transform y	0.023256
of biclusters	metrics cluster consensus score	0.250000
multiple files in svmlight format this function	svmlight files files n_features dtype	0.200000
decision function of the given observations	covariance outlier detection mixin decision function	0.333333
in parallel n_jobs is the is	n_jobs	0.023256
process or thread	externals joblib multiprocessing	0.052632
of vectors for reproducibility flips the sign of	utils deterministic vector sign	0.066667
fit all transformers	core feature union fit	0.500000
returns posterior probabilities	cv predict proba x	0.034483
r^2 coefficient of determination	metrics r2	0.125000
a calibration curve	calibration curve y_true y_prob normalize	0.142857
number of splitting iterations in the cross-validator parameters	model_selection cviterable wrapper get n splits x y	0.111111
search over parameters	search	0.019231
sizes of training subsets and validate 'train_sizes'	train sizes train_sizes n_max_training_samples	0.500000
function and	func	0.011364
gaussian process regression model we can also predict	gaussian process regressor predict	1.000000
weighted graph of neighbors for points in	radius neighbors graph	0.066667
an	shape	0.011765
a given cache key	cache key	0.250000
check according to li et	check	0.017857
inplace row scaling of a	utils inplace row scale	0.142857
relative	metrics threshold scorer call clf	0.333333
factor of	factor	0.100000
shortest path length	source shortest path length	0.333333
loading for the lfw pairs dataset this operation	lfw pairs	0.018868
pairs dataset this dataset is	pairs subset	0.125000
check input and compute prediction of init	boosting init decision	0.142857
matrix	matrix labels_true labels_pred eps sparse	1.000000
the parameters of this	params	0.085714
outlier on the training set according	outlier factor fit predict	0.200000
loading of moved objects in six moves urllib_request	module six moves urllib request	0.333333
hot	hot	1.000000
is restricted to the binary classification	y_true y_score pos_label sample_weight	0.066667
estimator with the best found parameters	search cv predict proba x	0.076923
documents	feature_extraction count vectorizer fit	0.125000
computes the paired	paired	0.100000
low	datasets make low	0.333333
update the	decomposition update	0.125000
a binary	average binary score	0.500000
model using x y	x y	0.002155
any negative value in	non negative x whom	0.200000
mixture	mixture	0.291667
skip test if being run on travis	utils check skip travis	1.000000
linear embedding analysis on the data	linear embedding x n_neighbors n_components reg	0.200000
agglomeration	agglomeration	1.000000
gen	gen	0.833333
inverse the transformation	transform inverse transform xred	1.000000
maximizer of the	gaussian_process gaussian process arg max	0.047619
log probability for full covariance matrices	log multivariate normal density full x means covars	0.333333
learn vocabulary and idf	transform raw_documents y	0.100000
a random projection p only changes the	core johnson lindenstrauss min dim n_samples	0.142857
analysis fa a simple linear generative model	analysis	0.045455
the number of free parameters in the model	gaussian mixture n parameters	1.000000
neighbors within a	lshforest radius neighbors x	0.166667
the parameters for the voting classifier	ensemble voting classifier set	0.037037
under the curve auc	auc	0.040816
estimate the precisions parameters	bayesian gaussian mixture estimate precisions nk	0.166667
back	preprocessing label binarizer inverse	0.166667
and	w x y	0.200000
grid of alpha values	alpha grid x y xy	0.166667
indicate if wrapped estimator is using a	core one vs one classifier pairwise	0.200000
for each	val predict estimator x y	0.045455
nmf find two non-negative matrices w h	x w h	0.035714
a score by cross-validation	core cross val score	0.333333
of the memory	joblib memory	0.016949
to build a	parallel build	0.047619
returns cluster	cluster	0.021277
scale	scale scale	0.250000
estimate the precisions parameters of the precision	bayesian gaussian mixture estimate precisions nk	0.166667
shrunk on	covariance shrunk	0.066667
building a cv in a user	core check cv cv x	0.031250
depending	func	0.011364
matrices w h	w h n_components	0.038462
long type introduces an	utils	0.009709
the callable case for pairwise_{distances kernels}	pairwise callable	0.083333
coefficient score the	score y_true y_pred normalize	0.125000
a which this function is called to	externals	0.005747
of a csc/csr matrix in-place	utils inplace swap row x	0.250000
parallelbackend must implement	parallel backend	0.030303
'l' suffix	repr	0.012500
to avoid	externals	0.011494
balanced	balanced	1.000000
number of splitting iterations in the	one out get n splits x y	0.111111
number of splitting iterations in	cross validator get n splits x y groups	0.125000
a func to be run	externals joblib pool manager mixin apply async func	0.250000
initial parameters of the derived	parameters	0.055556
the huber loss and the gradient	huber loss and gradient w x y	0.333333
multivariate bernoulli models	bernoulli	0.142857
indices to split data	split	0.055556
the model according to the	x y sample_weight	0.025974
state of the estimator	state	0.066667
fit a multi-class classifier by combining	base sgdclassifier fit	0.076923
predict raises an exception in an unfitted	estimators unfitted	0.142857
of array-like or scipy sparse	binarize x threshold	0.083333
arbitrary python object into one file	externals joblib dump value filename	0.083333
perform a locally linear embedding analysis on	locally linear embedding x n_neighbors n_components reg	0.071429
callable case for pairwise_{distances	callable x y	0.083333
according to a percentile of the highest	percentile	0.100000
to a large	lsqr a	0.037037
used to build a batch	build	0.037037
hc	hc	1.000000
read file object	read file	0.333333
the depth	memorized func check previous func code	0.333333
two clusterings	score labels_true labels_pred sparse	0.047619
type	utils	0.009709
fit the	estimator fit x y sample_weight	0.200000
approximates the range	utils randomized range	0.083333
the hash	joblib memorized	0.015625
by computing truncated	truncated x	0.200000
number of splitting iterations in the	group out get n splits x y	0.111111
for estimators	estimators estimators	0.500000
persist an arbitrary python object into one file	joblib dump value filename compress protocol	0.250000
thread pool and return the number of workers	externals joblib multiprocessing backend configure n_jobs parallel	1.000000
stacklevel is the	stacklevel	0.125000
to build a	build	0.037037
minimum and	min	0.045455
nearest neighbors	neighbors	0.027027
of neighbors for points in	radius neighbors mixin radius neighbors	0.125000
global clustering for the subclusters obtained	cluster birch global clustering	0.142857
project	cluster spectral biclustering project	1.000000
classifier valid parameter keys can be listed	classifier set params	0.125000
build a contingency matrix describing the relationship	cluster contingency matrix labels_true	0.333333
exception types to be captured	externals joblib parallel backend base get exceptions	0.166667
is a regressor	regressor	0.027027
hopefully pretty robust repr equivalent	safe repr value	1.000000
a which this function is called	externals joblib	0.004762
precision matrix with the	pca get precision	0.066667
create all the covariance	to match covariance	0.250000
directory in which are persisted	get output dir	0.047619
of the parallel execution	externals joblib parallel	0.014085
empty the function's cache	memorized func clear warn	0.250000
hash depending from	memory	0.015625
predict raises an exception in an unfitted	unfitted	0.083333
generate primal coefficients from dual coefficients for	coef dual_coef n_support support_vectors	0.333333
posterior probabilities	cv predict proba	0.034483
input data point	core cross val predict estimator x	0.045455
returns the number of splitting iterations in	group out get n splits	0.111111
two non-negative matrices w h whose product	w h	0.031250
long type introduces an 'l' suffix	shape	0.011765
get the	get	0.072289
within a given radius of a point or	radius	0.045455
unique	unique	1.000000
a transform function to portion of selected features	selected x transform selected copy	0.333333
approximate feature map to	core rbfsampler transform	0.333333
back the data to the original	inverse transform	0.062500
lad updates	ensemble least absolute error update	0.500000
fit the model	fit x	0.032051
matrix	matrix labels_true	1.000000
from the decision boundary for each class	core one vs rest classifier decision function	0.250000
likelihood of theta	likelihood theta eval_gradient	1.000000
the number of splitting iterations in	base cross validator get n splits	0.125000
a matrix of patch	feature_extraction patch extractor	0.200000
false positives per binary classification	binary clf curve y_true y_score pos_label sample_weight	0.090909
the callable case	metrics pairwise callable x y	0.083333
csv	csv	1.000000
covered	state	0.066667
anova f-value	feature_selection f classif x y	0.200000
assert that	check	0.017857
for c in (l1_min_c infinity) the model	c x y loss fit_intercept	0.030303
break the pairwise matrix	metrics parallel pairwise x y	0.166667
is the solution to a sparse coding problem	sparse encode x dictionary gram cov	0.333333
estimator on training subsets incrementally and compute	core incremental fit estimator estimator x y	0.200000
regressor that makes predictions using simple rules	dummy regressor	0.333333
input	predict estimator x y	0.045455
build a batch of estimators within a	parallel build estimators n_estimators ensemble x y	0.166667
transform binary labels back to	inverse transform y	0.500000
an array with block checkerboard structure for	checkerboard shape n_clusters noise minval	0.066667
predict based on an	predict x	0.011765
kernel	normalized kernel mixin	0.333333
huber loss and the gradient	linear_model huber loss and gradient w x	0.333333
boosted classifier from	ensemble ada boost classifier	0.200000
decision boundary for each class	core one vs rest classifier decision function x	0.250000
probabilities for each sample	samples	0.052632
area under the curve auc using the trapezoidal	auc	0.020408
scale to unit	preprocessing scale	0.090909
was opened for writing	writable	0.166667
returns the index of the leaf	tree apply	0.166667
covariance matrix shrunk on the	covariance shrunk covariance	0.090909
parallel processing this method is	externals joblib parallel	0.014085
be captured	externals joblib parallel backend base get exceptions	0.166667
average a binary metric for multilabel classification parameters	metrics average binary score binary_metric y_true y_score average	1.000000
implement a single	iboost x y	1.000000
evaluates the reduced likelihood function	reduced likelihood function	0.041667
and then the underlying estimator	x y	0.002155
c in	c x y	0.030303
elastic net path	enet path	0.050000
features	x features	1.000000
estimate class	utils compute class	0.166667
generate	base shuffle split iter	0.166667
list of exception types	externals joblib parallel backend base get	0.066667
update terminal regions	error update terminal regions	0.500000
introduced by a random projection p only changes	johnson lindenstrauss min dim n_samples eps	0.142857
opposite of the local outlier factor	neighbors local outlier factor decision function	0.125000
and dispatch	parallel dispatch	0.250000
estimates for each input	core cross val predict estimator	0.045455
on the training set	fit predict	0.055556
feature importances	get feature importances	1.000000
undo the	min max	0.250000
list of feature name -> indices mappings	dict vectorizer fit x y	0.250000
first and	first and	1.000000
compute decisions within a job	estimators_features x	0.111111
to build a batch of estimators within a	ensemble parallel build estimators n_estimators	0.166667
to fit an estimator within	fit estimator estimator x y	0.071429
the given arguments and	func call	0.047619
for the lfw pairs dataset this operation	lfw pairs	0.018868
standardize a dataset along any axis center to	x axis with_mean with_std	0.333333
is inefficient	classes	0.025641
to coefs and intercept for specified layer	layer	0.090909
fit estimator and predict values for a	model_selection fit and predict estimator x y train	0.250000
to avoid the hash depending from	joblib memorized	0.015625
best found parameters	core base search cv predict	0.076923
format check x format and	check	0.017857
hash depending from it	memorized func	0.016949
shortest	shortest	0.750000
any axis center to the median and	x axis	0.015385
the voting classifier valid parameter keys can	ensemble voting classifier set	0.037037
depending from it	externals joblib memorized func	0.013158
of x (as bigger is better	x	0.001692
shrunk covariance model according to	covariance shrunk covariance fit	0.083333
tolerance which is	tolerance	0.045455
sure that whenever scale	scale scale	0.250000
compute data precision matrix with the generative	base pca get precision	0.066667
the wild lfw	fetch lfw	0.041667
and class probabilities	y	0.002674
the svmlight / libsvm format into sparse	svmlight file f	0.066667
into the already fitted lsh forest	neighbors lshforest partial fit x	0.200000
method for updating terminal regions (=leaves)	function update terminal region tree terminal_regions leaf x	0.200000
predict_log_proba on the estimator with	predict log proba x	0.045455
as target values	neighbors supervised integer mixin	0.500000
remove a subcluster from	subcluster	0.090909
the voting classifier valid parameter	voting classifier	0.035714
a transformer from an arbitrary callable	transformer	0.100000
the shrunk covariance model according to the given	covariance shrunk covariance fit x	0.083333
of a csc matrix in-place	utils inplace swap row csc	0.250000
contingency matrix describing the relationship between labels	metrics cluster contingency matrix labels_true labels_pred eps sparse	0.200000
the shrunk ledoit-wolf	ledoit wolf x	0.250000
update the variational	mixture dpgmmbase update	0.250000
derived	resp	0.090909
scale back the data to	standard scaler inverse transform x	0.066667
median across axis 0 of a csc matrix	utils csc median axis 0 x	1.000000
estimator and predict values for	and predict estimator x y train	0.200000
k-neighbors of a point	neighbors kneighbors mixin kneighbors x n_neighbors	0.125000
fit the ardregression model according	linear_model ardregression fit x	0.250000
the grid of alpha values	linear_model alpha grid x y	0.166667
the sign of	sign	0.050000
read the z-file and return the content as	externals joblib read zfile file_handle	0.333333
we don't store the	joblib memorized	0.015625
building a cv in a user friendly	cv cv x y	0.031250
for each input data	cross	0.037037
cv in a	check cv cv x y classifier	0.031250
estimates for	estimator x y	0.038462
the coefficients and intercepts from	neural_network base multilayer perceptron unpack	0.250000
locally	locally	0.777778
decision function output for x	x y	0.002155
left_mask	left_mask	1.000000
the estimator with the best found parameters	core base search cv	0.066667
convert coefficient matrix to dense array format	linear_model sparse coef mixin densify	0.100000
tied covariance matrix	covariances tied resp x nk means	1.000000
depth a which this function is called to	externals joblib memorized func check previous func code	0.055556
number of splitting iterations in the	model_selection leave one group out get n splits	0.111111
the binary classification	score y_true y_score average	0.076923
of exception	base	0.014286
the recall the recall is the ratio	recall	0.028571
[rouseeuw1984]_ aiming at computing	covariance c step x n_support remaining_iterations initial_estimates	0.111111
in [rouseeuw1984]_ aiming at computing mcd	step x n_support remaining_iterations initial_estimates	0.111111
check if vocabulary is empty	mixin check vocabulary	0.250000
the weighted graph of neighbors	radius neighbors graph	0.066667
just there to implement the usual api and	y	0.008021
private function used to build a	parallel build	0.047619
and compute scores	y	0.005348
transform the	transform y	0.023256
estimate the precisions parameters of the	gaussian mixture estimate precisions nk xk	0.166667
leave one	leave one	1.000000
calculate true and false positives per binary	binary clf curve y_true y_score pos_label	0.090909
to partition estimators between jobs	partition estimators n_estimators n_jobs	0.200000
of an array shape under python 2 the	repr shape	0.166667
generate train	shuffle split	0.142857
the pairwise matrix in	parallel pairwise x y func	0.166667
for reproducibility flips the sign of elements	utils deterministic vector sign flip	0.066667
number of splitting iterations in the cross-validator parameters	model_selection cviterable wrapper get n splits x	0.111111
find the first prime element in the specified	find prime in	0.333333
a collection	vectorizer	0.044444
features using a one-hot aka one-of-k scheme	one hot encoder	0.200000
estimate sample weights	utils compute sample	0.100000
of determination regression score	metrics r2 score y_true y_pred sample_weight multioutput	0.125000
"friedman \#3" regression problem this dataset is described	datasets make	0.015625
callable case for pairwise_{distances	pairwise callable x y	0.083333
the training set according to the	fit	0.003257
function call with	externals joblib format call func	0.100000
by quantile this classification dataset is constructed	datasets make	0.015625
and gradient	and grad	1.000000
features	features x vocabulary high	1.000000
load text files with categories as subfolder names	datasets load files container_path description categories load_content	1.000000
sizes of training subsets and	core translate train sizes	0.066667
and dispatch them	externals joblib parallel dispatch one	0.250000
a large sparse	lsqr a	0.037037
return a	shape repr	0.013699
mallows	mallows	1.000000
returns the number of splitting iterations in	out get n splits x	0.111111
approximation	n_samples n_subsamples	0.333333
the generative	pca get	0.076923
call	joblib format call	0.200000
fits the graphlasso covariance model to x	covariance graph lasso cv fit x y	0.333333
the estimator with the best found	model_selection base search cv predict proba	0.076923
k-means	kmeans fit	1.000000
number of splitting iterations in the	base cross validator get n splits x	0.125000
rows of a csc matrix in-place	utils inplace swap row csc x	0.250000
computes the exponential chi-squared kernel	metrics chi2 kernel	0.333333
sample weights by class for unbalanced datasets	compute sample weight class_weight	0.500000
the kernel k x y and	gaussian_process compound kernel call x y	0.333333
check that y_true and y_pred belong to	metrics check reg targets y_true y_pred multioutput	0.333333
by using matrix product with the random matrix	random projection	0.125000
estimates the shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage	0.125000
similarity of two sets	score a b similarity	0.125000
of the dual gap convergence criterion the specific	covariance dual gap	0.071429
convert coefficient matrix to	linear_model sparse coef	0.076923
estimate the spherical wishart	gaussian mixture estimate wishart spherical nk xk	0.333333
compute the decision function of x	ensemble gradient boosting classifier decision function x	0.333333
back the data to the original representation parameters	standard scaler inverse transform x	0.066667
fit	lars ic fit	1.000000
the estimator has been refit	core base search cv	0.033333
using the gaussian process	gaussian_process gaussian process	0.111111
used for later	fit x	0.006410
ap from prediction scores this score corresponds to	score y_true y_score	0.025000
callable case	pairwise callable x y	0.083333
wild lfw pairs dataset	datasets fetch lfw pairs	0.018868
the number of splitting iterations in the	leave one out get n splits x y	0.111111
of a memmap instance to reopen on same	memmap a	0.050000
log probability for full	log multivariate normal density full	0.333333
of a memmap instance	memmap a	0.050000
determinant with	det fit x	0.333333
unfitted estimator	estimators unfitted name estimator	0.142857
covariance matrix shrunk on the diagonal read more	shrunk covariance	0.090909
a calibration curve	calibration curve y_true	0.142857
type	repr	0.012500
kernel between x and y	kernel x y gamma	0.500000
full covariance	normal density full x means covars	0.166667
to the file	zlib file	0.076923
compute the number of patches	compute n patches	1.000000
returns whether the	gaussian_process	0.093750
introduces an 'l' suffix when using the	utils	0.009709
reduction for memmap backed arrays	memmap backed	0.333333
prediction of init	base gradient boosting init decision function x	0.142857
factoranalysis model	decomposition factor analysis get	1.000000
of the local outlier factor of x (as	local outlier factor decision function x	0.100000
fit the model according to the	linear svr fit x y sample_weight	0.250000
the exponential chi-squared kernel x and y	metrics chi2 kernel x y	0.333333
arguments	joblib memorized func	0.014706
len	len	0.192308
the logarithm of the normalization constant	logz v s dets	0.200000
data precision matrix with the	decomposition base pca get precision	0.066667
random_state parameters for an estimator	estimator	0.014706
given arguments	memorized func get	0.125000
apply transforms and score	score x y	0.030303
finds	x n_neighbors return_distance	0.250000
data_file_name	data_file_name	1.000000
estimates for	core cross	0.045455
the curve auc using	metrics auc	0.040000
predict_proba on the estimator with the best found	search cv	0.036364
patches of	feature_extraction extract patches	0.083333
of x according to	x	0.001692
generate	n_samples n_components n_features	0.500000
matrix factorization nmf find two non-negative matrices	non negative factorization x	0.043478
remove cache folders to	externals	0.005747
curve auc from prediction scores note	metrics roc auc score	0.166667
element of numpy array or	element	0.083333
connectivity matrix	x connectivity n_components	1.000000
computes multidimensional scaling using	single dissimilarities metric n_components init	1.000000
fit the model by	fit	0.003257
fit a binary classifier on x and y	base sgdclassifier fit binary x y alpha	1.000000
x from y along the first axis	x z reg	0.066667
the score on	score	0.010101
center	preprocessing robust scaler	0.500000
generate train test	core base	0.083333
input validation for standard	x y accept_sparse dtype	0.250000
x format check x format	check	0.017857
a decision tree	tree	0.071429
images for image	images	0.111111
make and configure a copy of	ensemble make estimator append random_state	0.166667
note this implementation	sample_weight	0.018519
returns the number of splitting iterations in	one out get n splits x	0.111111
list of exception	backend base	0.032258
solution to a sparse coding problem	sparse encode x dictionary	0.333333
y eventually shuffle among same groups	model_selection shuffle y groups random_state	0.333333
of two clusterings of a set	score labels_true labels_pred	0.047619
x y	x y	0.038793
reproducibility flips	utils deterministic vector	0.076923
a random multilabel classification problem	datasets make multilabel classification	0.166667
which are going to	joblib multiprocessing backend	0.052632
and raise valueerror if not valid	ensemble base gradient boosting	0.111111
type introduces an 'l' suffix	utils shape repr	0.013699
dummy feature	dummy feature x value	1.000000
estimates for each input data point	cross	0.037037
that implement the	utils check partial fit	0.038462
similarity coefficient score	score y_true y_pred normalize sample_weight	0.125000
downloading it	subset data_home download_if_missing random_state	0.166667
labels the output of transform is sometimes referred	transform	0.011236
linear embedding analysis	linear embedding x n_neighbors n_components	0.200000
maximum	max axis	0.500000
for building a cv in a user	check cv cv x	0.031250
in friedman [1] and breiman [2]	friedman3 n_samples	0.166667
the sign of elements of	sign	0.050000
the variational lower bound for the	mixture dpgmmbase bound	0.166667
least	least	1.000000
matrix factorization	decomposition non negative factorization	0.043478
batch	batch	0.888889
validation on an array	utils check array array	0.250000
path with coordinate	enet path x	0.050000
return the number	externals joblib parallel	0.014085
bagging	bagging	0.875000
fit an estimator within a job	ensemble parallel fit estimator estimator x y sample_weight	0.333333
input validation on an array	array array accept_sparse	0.250000
select features according to a percentile of	select percentile	0.333333
the search over parameters	core base search cv fit	0.166667
to the cache for	memorized func	0.016949
from prediction scores note this implementation is	roc	0.033333
quantile this classification dataset is constructed by	datasets	0.015152
score of the underlying	score	0.010101
found and raise an exception if a	search	0.019231
factorize argument checking for random matrix generation	core check input size n_components n_features	0.200000
the reduced likelihood function for the given	gaussian process reduced likelihood function	0.047619
input data	val predict	0.045455
the estimator with the best	cv predict proba	0.068966
binary	y_score average sample_weight	0.142857
dictionary factor	dict dictionary	0.111111
fit the model to	multi output estimator fit x y sample_weight	0.200000
all meta estimators in	meta estimator	0.062500
of x	x z reg	0.066667
for c such that for c in	c x y	0.030303
multinomial deviance loss function for multi-class	multinomial deviance	0.250000
cache folders to make cache size fit	memory reduce size	0.083333
[1] and breiman [2]	make friedman3	0.166667
data onto	ridge_alpha	0.052632
tosequence	tosequence	1.000000
matrix factorization nmf find	non negative factorization x	0.043478
weighted graph of neighbors	neighbors radius neighbors mixin radius neighbors graph	0.066667
unsupervised	unsupervised	1.000000
validate user provided precisions	mixture check precisions precisions covariance_type n_components n_features	0.250000
matrix	n_components affinity	1.000000
the timestamp when pickling to	reduce	0.034483
learn the vocabulary dictionary and return term-document matrix	count vectorizer fit transform raw_documents y	0.166667
returns the huber loss and the gradient	linear_model huber loss and gradient w x	0.333333
coefficient of determination regression score function	metrics r2 score y_true y_pred sample_weight	0.125000
classification this function returns posterior probabilities of classification	classifier cv predict proba x	0.200000
read an array using numpy	joblib numpy array wrapper read	0.500000
algorithms	x n_components init eps	1.000000
private function used to fit an estimator	fit estimator estimator x y	0.071429
column scaling	column	0.083333
is	is	0.800000
fit the hierarchical clustering on the	cluster feature agglomeration fit	0.250000
check values of the basic	base mixture check initial	1.000000
to split data into	model_selection predefined split split	0.250000
mean squared logarithmic	mean squared log	1.000000
'l' suffix when	utils shape	0.013699
two covariance	covariance	0.014493
along an axix on a csr	axis x axis last_mean last_var	0.200000
combination of the dictionary atoms	coding mixin transform x	1.000000
x for a diagonal model	diag x means	0.500000
on x and	dbscan fit predict x y sample_weight	0.333333
input validation	y accept_sparse dtype	0.250000
model from data in	manifold spectral embedding	0.111111
first and last element of numpy array	first and last element arr	0.200000
the meta-information and	zndarray wrapper read unpickler	0.043478
store the timestamp when pickling to avoid	externals joblib memory reduce	0.030303
the reduced likelihood function	process reduced likelihood function	0.047619
in the svmlight / libsvm format into sparse	svmlight file f n_features dtype	0.066667
read an array	array wrapper read	0.500000
compute a logistic regression model	linear_model logistic regression path x y	0.333333
returns the number of splitting iterations in the	group out get n splits x	0.111111
relative	metrics predict scorer call estimator	1.000000
the file was opened for writing	file writable	0.250000
from x	fit x	0.006410
log probability for full covariance matrices	mixture log multivariate normal density full x	0.333333
current file position	externals joblib binary zlib file tell	0.333333
and configure a copy of the base_estimator_	append random_state	0.142857
handle the callable case for	metrics pairwise callable x y	0.083333
probabilities	probabilities	1.000000
lrd of a	distances_x neighbors_indices	0.047619
the submatrix corresponding to	mixin get submatrix	0.166667
long type introduces an 'l' suffix when using	utils shape repr	0.013699
returns a list of feature	get feature	0.125000
routine for validation and conversion of csgraph	utils sparsetools validate graph csgraph directed dtype csr_output	0.166667
p=none) generates a random sample from a	a size replace p	0.142857
an estimator	estimator estimator x	0.090909
wild lfw pairs dataset	fetch lfw pairs	0.018868
x	exponentiation call x	0.200000
iterator over estimators in	iter	0.050000
and evaluates the reduced likelihood function for the	reduced likelihood function	0.041667
get the parameters of the votingclassifier	classifier get params deep	1.000000
for c in (l1_min_c infinity) the	c	0.022222
don't store the timestamp when pickling to avoid	memory reduce	0.030303
estimate the precisions parameters	gaussian mixture estimate precisions	0.166667
distributions for the means	means x	0.250000
private function used to compute log	log	0.018868
helper function for parameter value indexing	model_selection index param value x v	0.200000
create all the covariance	match covariance	0.250000
training set according to the	factor fit predict	0.066667
dense array format	mixin densify	0.100000
function used to compute log probabilities within	predict log proba	0.029412
neighbors for points	mixin radius neighbors	0.125000
load sample images for image manipulation	datasets load sample images	0.250000
within a job	ensemble parallel	0.250000
orthogonal matching pursuit omp solves n_targets orthogonal	linear_model orthogonal mp x y	0.250000
files with categories as subfolder names	files container_path description categories load_content	0.500000
return feature names	get feature names	0.090909
non zero row of x	x y	0.002155
meta estimators in scikit-learn	meta estimator	0.062500
predict posterior probability	predict proba x	0.333333
representation of	utils	0.009709
clustering for the subclusters obtained	clustering x	0.142857
back	label binarizer inverse	0.166667
using matrix product with the random matrix	random projection transform x	0.333333
fit the model	multi output estimator fit x y	0.200000
fit the model using x as training data	fit x skip_num_points	0.500000
a single boost using	boost classifier boost	0.100000
for each	core cross val	0.043478
the neighbors	neighbors lshforest radius neighbors	0.166667
load datasets in the svmlight / libsvm	datasets load svmlight file f n_features dtype	0.500000
using a single binary estimator	predict binary estimator x	0.200000
x	x z	0.050000
dtype of x and y is float32	x y	0.002155
em update for	em step	0.500000
data point	core cross val predict estimator	0.045455
hash depending	memorized func	0.016949
the other and	y	0.002674
fit the	core multi output estimator fit x y	0.200000
elasticnet	elastic net	0.111111
indicate if wrapped estimator	core one vs one classifier pairwise	0.200000
log-transformed bounds on the theta	kernel bounds	0.333333
the breakdown point	breakdown point	0.333333
utility for building a cv in	core check cv cv	0.031250
scale back the data to	scaler inverse transform x copy	0.066667
cache for the	memorized func	0.016949
the test/test sizes are meaningful wrt to	validate shuffle split n_samples test_size train_size	0.111111
compute data covariance	covariance	0.014493
compute mean	utils mean	0.250000
label spreading computes the graph laplacian	semi_supervised label spreading build graph	0.142857
long type	utils shape repr	0.013699
predict_proba on the estimator with the best found	model_selection base search cv predict	0.076923
on the training set according	fit	0.003257
of x according to	inverse transform x	0.025641
persist an arbitrary python object into one file	externals joblib dump value filename compress protocol	0.250000
folders to	externals	0.005747
mlpclassifier	mlpclassifier	0.833333
nearest	nearest	1.000000
of exception types to	externals joblib parallel backend base get	0.066667
the actual data loading for the lfw	datasets fetch lfw	0.083333
precision ap from prediction scores	precision	0.016667
an estimator finds all parameters ending random_state and	ensemble set random states estimator random_state	1.000000
the objective function iterating once over all	coordinate descent x	0.333333
the hash depending from it	joblib memorized func	0.014706
calculate out of bag predictions and score	ensemble base forest set oob score x y	0.250000
function best possible score is 1	score y_true y_pred sample_weight	0.062500
data precision matrix with the generative model	precision	0.016667
negative value in an	negative	0.090909
fit linear model	linear_model linear regression fit x	1.000000
utility function opening the right fileobject from	read fileobject fileobj	0.100000
size=none replace=true p=none) generates a random sample from	size replace p	0.125000
compute non-negative matrix factorization nmf	negative factorization x	0.043478
the long type introduces an 'l' suffix when	utils shape repr	0.013699
k x y and	call x y	0.142857
relative	metrics predict scorer	1.000000
return the wine	wine	0.111111
extensions	extensions	0.857143
updates terminal regions to median estimates	terminal region tree terminal_regions leaf	0.066667
nmf find two non-negative matrices w h whose	w h n_components	0.038462
to retrieve a reliable	externals joblib get func	0.200000
matrix in-place	inplace swap row	1.000000
component	get covars	1.000000
to a	utils lsqr a	0.037037
the precision is the ratio tp / tp	precision	0.016667
parameters for the voting classifier valid parameter	ensemble voting classifier set	0.037037
binary classification task	recall curve y_true	0.142857
predict on the estimator with the	predict	0.006849
squared logarithmic error regression loss read more in	squared log error	0.166667
signature	externals signature	0.050000
the vocabulary dictionary and return term-document	fit transform raw_documents y	0.100000
a cv in a	core check cv cv	0.031250
number of splitting iterations in the cross-validator	model_selection base kfold get n splits x y	0.111111
computes the log-likelihood	covariance empirical covariance score	0.166667
output a function call	externals joblib function called str function_name args kwargs	0.250000
to the cache for	externals joblib memorized	0.013699
the isotonic regression model : min	core isotonic regression y	0.066667
linear embedding analysis on the data	linear embedding x	0.200000
embedding	embedding	0.360000
estimate class weights for	utils compute class	0.166667
train	split iter	0.166667
home cache	home data_home	0.500000
full covariance	density full x means covars	0.166667
std to	preprocessing standard	0.250000
building a cv in a	check cv cv x y	0.031250
inplace row	inplace row scale x	0.142857
tests involving both blas calls and multiprocessing	safe multiprocessing with blas func	0.500000
the model according to the given training	x y sample_weight	0.025974
provided data	neighbors radius neighbors	0.100000
the data	clear data	0.142857
the points in	parameter	0.083333
connectivity matrix	connectivity x connectivity n_components	1.000000
to the binary classification	score y_true y_score average sample_weight	0.076923
free energy f v = - log	bernoulli rbm free energy	0.066667
generate	core cross val	0.043478
matrices w h whose product approximates the	x w h	0.035714
error regression	log error	0.142857
calibration	core calibration	0.125000
gram	gram omp gram	0.500000
y and get	y	0.002674
fit with	search cv fit x y	0.111111
using matrix product with the random matrix	random projection transform	0.333333
two	cc x y	1.000000
for a given dataset	x y scorer	0.111111
and false positives per binary	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
binary classifier on x and y	binary x y alpha	0.500000
to update params with given gradients parameters	updates grads	0.076923
this implementation is restricted to the binary classification	y_true	0.043478
fileno	fileno	1.000000
new data by linear interpolation	sigmoid calibration	0.500000
labels	preprocessing label	0.166667
break the pairwise matrix	metrics parallel pairwise x y func	0.166667
classification on test vectors	core dummy regressor predict	0.250000
x and returns the transformed	fit transform x y w h	0.500000
train test	core base shuffle split	0.166667
mlp loss function and its corresponding derivatives	multilayer perceptron	0.071429
param logic estimators that implement the partial_fit api	utils check partial fit	0.038462
the initial centroids parameters	cluster init centroids x k init	0.166667
locally linear embedding	locally linear embedding	0.100000
equal to the average	average	0.066667
precision is the ratio tp	precision	0.016667
don't store the	joblib memorized	0.015625
fit the model	decomposition pca fit	0.250000
the voting classifier valid parameter keys can	ensemble voting classifier set params	0.037037
gradient	grad w x	1.000000
func to	joblib parallel backend base apply async func	0.250000
a cv in a user friendly way	check cv cv x	0.031250
of x from	x z	0.050000
returns the huber loss and the gradient	linear_model huber loss and gradient w x y	0.333333
kernel k	gaussian_process exp sine squared	1.000000
predict the target of new	cv predict x	0.125000
by using matrix product with the random matrix	random projection transform	0.333333
reconstruct the image from	feature_extraction reconstruct from	0.333333
non-negative matrices w	w	0.035714
logic estimators that	utils check	0.023810
depending from it	memorized func	0.016949
reconfigure the backend	parallel backend base configure	0.500000
check initial parameters of the	mixture base mixture check parameters	0.200000
laplacian matrix	laplacian	0.034483
a given mapping	transform y class_mapping	0.333333
mixin class for all classifiers in scikit-learn	classifier mixin	0.333333
leaves of	get leaves	0.111111
training data	cv	0.009009
precs	precs	1.000000
individually to unit	axis copy	0.166667
the time it take to	externals joblib squeeze time t	0.200000
relative	predict scorer	1.000000
with all	cv	0.009009
transformers	core feature union	0.250000
x y and optionally its gradient	gaussian_process dot product call x y eval_gradient	0.333333
estimator with the best found	base search cv	0.052632
non-negative matrix factorization nmf find	factorization	0.035714
given label sets parameters	preprocessing multi label binarizer	0.200000
to capture the arguments of a function	externals joblib delayed function check_pickle	0.333333
prime	prime	0.555556
orthogonal matching pursuit model	orthogonal matching pursuit	0.250000
checker utility for building a cv	core check cv cv	0.031250
the leaves	leaves	0.071429
the callable case	callable x y metric	0.083333
fit the model using	fit	0.009772
support vector regression	svr	0.142857
shrunk ledoit-wolf covariance	covariance ledoit wolf x assume_centered block_size	0.125000
this implementation is restricted to the binary	y_score average	0.111111
for reproducibility flips the sign of elements	deterministic vector sign flip	0.066667
the linear assignment problem using the hungarian algorithm	linear assignment x	0.090909
model using	linear_model	0.128205
to build a batch of	parallel build	0.047619
with	decomposition base	0.076923
returns the number of splitting iterations in the	one out get n splits x y groups	0.111111
species	species	0.857143
patches of any n-dimensional array in	feature_extraction extract patches	0.083333
neighbors for points	neighbors mixin radius neighbors	0.125000
random matrix	random choice	0.166667
batch of estimators within	estimators n_estimators ensemble	0.083333
deviance (= 2 * negative log-likelihood)	ensemble binomial deviance	0.333333
prediction scores this score corresponds to the	score y_true y_score	0.025000
"news" format strip the	strip newsgroup	0.090909
provide values	mixture gmmbase set covars covars	1.000000
file supports seeking	externals joblib binary zlib file seekable	0.250000
implement a single boost	weight boosting boost iboost x y	1.000000
input and compute prediction of init	init decision function	0.142857
cache for	externals joblib memorized func	0.013158
blobs for clustering	datasets make blobs	0.333333
the range of	range finder	0.083333
point	cross val predict	0.045455
large sparse linear system of equations	b damp atol	0.200000
test_size and train_size at init	split init test_size train_size	0.250000
check if there is any negative value in	utils check non negative x whom	0.333333
get parameters for this estimator	core base estimator get params deep	1.000000
training data and parameters	fit x y	0.005988
mean absolute error regression loss	mean absolute error	0.166667
check that the	mixture check	0.142857
get parameters of this kernel	exponentiation get params deep	0.500000
to split data into training and test	time series split split x y groups	0.200000
learning	learning	0.625000
computes the maximum likelihood covariance estimator	covariance empirical covariance	0.071429
cv in a	core check cv cv	0.031250
a byte string	externals joblib binary	0.200000
getter for the	empirical covariance	0.125000
paired	metrics paired	0.400000
given cache key	cache key	0.250000
list of regularization	pos_class cs	0.166667
in the wild lfw pairs	lfw pairs	0.018868
values for a	x y train	0.166667
generate train test	base shuffle split	0.142857
count and smooth	core multinomial nb count x y	0.250000
spectral embedding for non-linear	spectral embedding	0.200000
function opening the right fileobject from	externals joblib read fileobject	0.100000
curve auc using the trapezoidal	auc x y	0.040000
mutual information between two variables	mi x	1.000000
the process or	externals joblib multiprocessing	0.052632
a which this function is called	externals	0.005747
embedding analysis on	embedding x n_neighbors	0.200000
extract the first	joblib extract first	1.000000
determines the blup parameters and evaluates the reduced	reduced	0.062500
of exception types to	parallel backend base	0.037037
ensure that	utils check	0.023810
building a cv in a user friendly	check cv cv x y	0.031250
the kl divergence of p_ijs and	manifold kl divergence	0.083333
cache	memory	0.015625
rand index adjusted for chance	cluster adjusted rand	0.333333
out-of-bag scores	ensemble forest regressor set oob	1.000000
number of splitting iterations in the cross-validator parameters	predefined split get n splits x	0.111111
get	neural_network sgdoptimizer get	0.125000
a memmap instance to reopen on same file	reduce memmap a	0.050000
data	core cross	0.045455
the similarity of two sets of	a b similarity	0.125000
validity of the input parameters	params x metric p metric_params	0.100000
shift clustering using a flat	shift	0.100000
compute elastic net path	linear_model enet path	0.050000
non-negative matrix factorization nmf find two non-negative	non negative factorization	0.043478
anova f-value for the provided sample	feature_selection f classif	0.200000
covertype dataset downloading it if necessary	datasets fetch covtype data_home download_if_missing random_state	0.333333
neighbors within	neighbors	0.027027
a random sample from	size replace p	0.125000
a cv	cv cv x y	0.031250
linear assignment problem using the hungarian algorithm	utils linear assignment x	0.090909
non-negative matrix factorization nmf find two non-negative matrices	non negative factorization	0.043478
from all of	from	0.045455
find the	find	0.142857
lfw pairs dataset this dataset	fetch lfw pairs subset	0.035714
list of exception	externals joblib parallel backend base get	0.066667
backend and	backend	0.016949
two non-negative matrices w h whose product	x w h	0.035714
filename	filename	0.300000
solve the isotonic regression model	isotonic regression y	0.066667
the number of splitting iterations in the	leave one group out get n splits x	0.111111
kernel k x y and optionally its gradient	gaussian_process white kernel call x y eval_gradient	1.000000
kernel is stationary	kernel is stationary	0.400000
all the covariance	match covariance	0.250000
multi-outputs	eps	0.166667
in sorted array	tree bin_x left_mask right_mask	0.166667
the posterior log probability of the	nb joint log likelihood	0.066667
for each input data	val predict estimator x	0.045455
estimate the precisions parameters of the	mixture bayesian gaussian mixture estimate precisions	0.166667
covariance matrix shrunk on the diagonal read more	covariance shrunk covariance	0.090909
for _fit_coordinate_descent update	decomposition update	0.125000
a large sparse linear system	a	0.018182
shortest path length from source to all	single source shortest path length graph source cutoff	0.111111
constructs signature from	externals signature	0.050000
and transforms the	y	0.002674
moved objects in six moves urllib_request	module six moves urllib request	0.333333
and cramer variance update	and var x last_mean last_variance last_sample_count	0.500000
the cholesky decomposition	det cholesky	0.166667
20 newsgroups	20newsgroups	0.055556
of the leaf	tree apply	0.166667
svmlight / libsvm format into sparse	svmlight file f n_features	0.066667
error regression loss read more in the :ref	error	0.020000
parallel	externals joblib parallel backend	0.029412
the huber loss and the gradient	huber loss and gradient w x	0.333333
for each input data point	val predict estimator x	0.045455
handle the callable case	pairwise callable x	0.083333
perform a locally linear embedding analysis on	locally linear embedding	0.050000
y * np dot x w	linear_model intercept dot w x y	0.500000
back the data to	robust scaler inverse transform	0.066667
for c such that for c in (l1_min_c	c x	0.030303
as a sparse	sparse	0.025000
the kernel k	gaussian_process white kernel call	0.333333
logistic	logistic	0.523810
moved objects in six moves urllib_robotparser	module six moves urllib robotparser	0.333333
read the	read unpickler	0.200000
loading for the lfw people	datasets fetch lfw people	0.040000
parallel	parallel	0.192308
the process or thread	multiprocessing backend	0.038462
fit estimator and compute scores for a	core fit and score estimator	0.333333
any axis center to the median	axis	0.014085
all methods a parallelbackend must implement	backend base	0.032258
a single boost using the	boost	0.062500
a which this function	externals	0.005747
dense dictionary factor in	dict dictionary	0.111111
the sparse components	decomposition sparse pca transform	0.500000
the set of samples x	x y	0.002155
log probabilities within a	ensemble parallel predict log proba	0.058824
the similarity of two clusterings of a set	score labels_true labels_pred sparse	0.047619
decorator to	utils	0.009709
the distance of each sample from the decision	decision	0.027778
squared logarithmic error regression loss read more	squared log error	0.166667
persist an arbitrary python object into	filename compress protocol	0.250000
net path with coordinate descent	enet path	0.050000
aggressive classifier read more	aggressive classifier	0.166667
the content of the data home cache	datasets clear data home	0.076923
the points in the grid	parameter grid	0.200000
model parameters with the em algorithm	mixture gmmbase fit x y	0.250000
build a batch of estimators within a	build estimators n_estimators ensemble x	0.166667
the search over	core base search cv fit	0.166667
of array-like or scipy sparse	binarize x	0.083333
max absolute value of	preprocessing max abs	0.050000
sparse random projection matrix parameters	core base random projection fit	0.333333
dimension of a	dimension	0.050000
load and return the	load	0.178571
write array bytes to	numpy array wrapper write array array	0.500000
force the execution of	memorized	0.015873
to compute log probabilities within a job	predict log proba estimators estimators_features	0.250000
embedding space	embedding	0.040000
call with	format call	0.200000
perform the mstep	do mstep x responsibilities	1.000000
generative	decomposition base	0.076923
x	x y copy	0.142857
the dual gap convergence criterion the specific	covariance dual gap emp_cov	0.071429
out of bag predictions	ensemble base bagging set oob	0.250000
ward clustering based on a feature matrix	cluster ward tree x connectivity n_clusters	0.250000
the boolean mask x == missing_values	preprocessing get mask x value_to_mask	0.333333
from prediction scores note this implementation is	metrics roc	0.040000
column scaling of a csc/csr	column	0.083333
callable case for pairwise_{distances kernels}	callable	0.058824
return a platform independent	utils shape repr	0.013699
compute the	mixture compute	1.000000
update h	h x w h	0.500000
check	allocation check	0.062500
least squares solver	linear discriminant analysis solve lsqr x y	1.000000
train test	shuffle	0.083333
coefficients and intercepts from packed_parameters	neural_network base multilayer perceptron unpack packed_parameters	0.250000
utility for building a cv in a	cv cv x y	0.031250
leaf	apply x	0.166667
pool	joblib	0.007299
implementation of givens rotation	utils sym ortho a b	0.250000
inplace row scaling	utils inplace row scale x	0.142857
the given training data and parameters	fit x y	0.005988
for building a cv in	check cv cv x	0.031250
initializes the	mixture dpgmmbase initialize gamma	0.333333
generate	estimator x	0.030303
timestamp when pickling to avoid the	joblib memorized func reduce	0.050000
creates a biclustering for	spectral fit	0.250000
private function used to compute decisions within	parallel decision function estimators estimators_features x	0.500000
helper function to test error messages in exceptions	exceptions message function	1.000000
problem with sparse uncorrelated design	sparse uncorrelated n_samples	0.166667
embedding analysis on	embedding x	0.200000
predict class labels for samples in	linear_model linear classifier mixin predict	1.000000
call predict_log_proba on the estimator	predict log proba x	0.045455
transform a sequence of instances to a scipy	feature_extraction feature hasher transform raw_x	0.333333
and predicted probabilities for a calibration curve	core calibration curve y_true y_prob	0.142857
for	covariance empirical	0.100000
tuple	tuple	1.000000
routine for validation	dtype	0.062500
apply clustering to a	clustering affinity n_clusters n_components	0.166667
compute non-negative matrix factorization nmf find two non-negative	negative factorization	0.043478
to pickler file	pickler	0.083333
function call with the given	format call func	0.100000
text_document	text_document	1.000000
not found and raise an exception if a	search	0.019231
terminal regions to median	terminal	0.047619
the first prime element in the specified row	prime in row row	0.333333
should be used when memory is inefficient	classes	0.025641
and	call	0.052632
indices to split data into	split x	1.000000
predict multi-output variable	core multi output estimator predict	0.166667
a text report showing the main classification	classification report	0.250000
parallel processing this method	joblib parallel	0.028571
reduced likelihood function for the	gaussian process reduced likelihood function	0.047619
and dispatch them	externals joblib parallel dispatch	0.250000
coef_init	coef_init	1.000000
scale back the data to the original representation	scaler inverse transform	0.058824
independent	repr	0.012500
increasing	increasing	1.000000
perform a locally linear embedding analysis on the	manifold locally linear embedding x n_neighbors n_components	0.071429
function output for x	x y sample_weight	0.012987
the lrd of a	distances_x neighbors_indices	0.047619
sizes of training subsets	train sizes	0.066667
for binary classification used in hastie	hastie	0.076923
cf tree	cluster birch fit	0.200000
uncompressed bytes from the	binary zlib	0.166667
bicluster	core bicluster	0.500000
multi-layer perceptron regressor	mlpregressor	1.000000
check input and compute prediction of init	ensemble base gradient boosting init	0.142857
fit on the	fit x y	0.005988
kddcup99 dataset downloading it if necessary	datasets fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
x and dot	decomposition beta divergence x	0.250000
true and false positives per binary classification	binary clf curve y_true y_score pos_label sample_weight	0.090909
building a cv in a	cv cv x y classifier	0.031250
pool	externals joblib	0.004762
data	load data	0.500000
returns the number of splitting iterations in the	predefined split get n splits x	0.111111
the sign of elements of all the vectors	sign flip	0.066667
problem this dataset is described in	datasets	0.015152
utility for building a cv in	cv cv x	0.031250
absolute error of the kl divergence	manifold kl divergence error	0.100000
logistic regression cv	logistic regression cv	0.200000
thread	multiprocessing	0.045455
the usual api and hence	patch extractor fit x y	0.142857
the decision function	gradient boosting classifier decision function	0.166667
return the kernel k x y and	gaussian_process matern call x y	0.200000
multiclass	multiclass	0.461538
from features or distance matrix	sample_weight	0.018519
compute class priors from multioutput-multiclass target data parameters	utils class distribution y	1.000000
with sparse uncorrelated design	sparse uncorrelated	0.166667
count and	bernoulli nb count x y	0.250000
for indices increasingly apart the distance depending on	verbosity filter index	0.055556
fit ridge regression model parameters	linear_model base ridge cv fit x	1.000000
and the	y	0.005348
evaluate the density model	density score samples	0.250000
batch of estimators within a job	estimators n_estimators	0.083333
false for indices increasingly apart the distance	joblib verbosity filter index	0.055556
loading for the lfw pairs dataset this operation	datasets fetch lfw pairs	0.018868
retrieve the leaves	get leaves	0.111111
the oracle approximating shrinkage	oas fit	0.333333
loader for the california housing	fetch california housing data_home	0.250000
for the lfw pairs dataset this	fetch lfw pairs	0.018868
the gradient boosting	base gradient boosting	0.200000
the density model on the	kernel density	0.083333
to avoid the hash depending from it	joblib memory	0.016949
indices to split data into	model_selection predefined split split x	0.250000
target variable	discrete_features	0.285714
dispatch	joblib parallel dispatch	0.250000
the timestamp when pickling to avoid the hash	func reduce	0.050000
using x as training	x skip_num_points	0.166667
number of splitting iterations in the cross-validator parameters	get n splits x y groups	0.111111
force the execution of the function with	memorized	0.015873
the k-neighbors of a point	neighbors kneighbors mixin kneighbors x n_neighbors	0.125000
train test indices	split iter indices	0.250000
the dimension of a	dimension	0.050000
compute the laplacian kernel between x and y	metrics laplacian kernel x y gamma	0.333333
estimate the parameters of the	mixture estimate means nk xk	1.000000
axis center to the mean and component wise	x axis	0.015385
species distribution dataset from phillips et al 2006	datasets fetch species distributions	1.000000
training set according to the	factor fit	0.062500
and inertia using a full	inertia precompute dense x x_squared_norms centers distances	0.250000
states	states	1.000000
explained variance	metrics explained variance	0.166667
best	cv predict proba	0.068966
or fetch the effective stop words list	feature_extraction vectorizer mixin get stop words	0.200000
each input	cross val predict	0.045455
precisions	mixture check precisions precisions covariance_type n_components n_features	0.250000
mean and variance	incr mean variance	0.333333
such that for c in (l1_min_c	c x	0.030303
data to maximize class separation	core linear discriminant analysis transform	0.250000
with sparse uncorrelated design	make sparse uncorrelated	0.166667
regularization	path x y pos_class cs	0.166667
lfw pairs dataset this operation is meant	fetch lfw pairs index_file_path data_folder_path slice_ color	0.333333
linear_model	linear_model	0.128205
to split data into	model_selection cviterable wrapper split x	0.250000
estimator on training subsets incrementally and	model_selection incremental fit estimator estimator x y classes	0.200000
make and configure a	ensemble base ensemble make estimator append random_state	0.166667
the generative model	base	0.014286
classification on test vectors	core dummy regressor	0.200000
predict is invariant	predict	0.006849
dual gap convergence criterion the specific definition is	dual gap emp_cov	0.071429
data	decomposition base pca get	0.071429
data loading for the lfw	fetch lfw	0.083333
avoid the hash	memorized func	0.016949
estimator with the best found	model_selection base search cv predict proba	0.076923
decision	ensemble gradient boosting classifier decision	0.333333
backend and return the	externals joblib parallel backend	0.029412
voting classifier valid parameter	ensemble voting classifier	0.031250
a contingency matrix describing the	contingency matrix	0.166667
we can also predict	predict x	0.011765
bag predictions	ensemble base bagging set oob	0.250000
the long type introduces an 'l' suffix	utils shape	0.013699
retrieve the leaves of	get leaves	0.111111
reconstruct the image from all	feature_extraction reconstruct from	0.333333
feature importances	feature importances	1.000000
clear all covered matrix cells	utils hungarian state clear covers	1.000000
the one-vs-one	one vs one	0.050000
feature name -> indices mappings	dict vectorizer fit	0.250000
the recall is	recall	0.028571
update it with	update	0.035714
to avoid the	externals	0.011494
regression cv aka logit	regression cv	0.200000
compute the grid of alpha values for	linear_model alpha grid	0.166667
lfw pairs dataset this dataset is a collection	lfw pairs subset	0.035714
w h whose product approximates	x w h	0.035714
the depth	joblib memorized func check previous func code	0.333333
depending from it	joblib	0.014599
computes the free energy f v = -	neural_network bernoulli rbm free energy	0.066667
the best found	base search cv	0.052632
return the path of the scikit-learn data dir	datasets get data home data_home	1.000000
"news" format strip lines beginning with	datasets strip newsgroup	0.090909
convert coefficient matrix to	sparse coef	0.071429
full covariance	full x	0.166667
run a	externals	0.011494
the huber	huber	0.125000
computes the position of the	mds fit x	0.066667
a list of feature	get feature	0.125000
based on a	x connectivity n_clusters return_distance	0.250000
exception types	base	0.014286
mini-batch dictionary learning finds a dictionary a set	batch dictionary learning	0.142857
lfw pairs	lfw pairs subset	0.035714
fit the model according to the given	svm linear svc fit x y sample_weight	0.250000
cross-validated estimates	cv	0.009009
manager	manager	1.000000
str	str	1.000000
too common features	count vectorizer limit features x vocabulary high low	1.000000
incremental mean	incr mean	0.166667
to avoid the hash depending from it	externals	0.011494
the concentration parameter	concentration	0.125000
estimate the spherical wishart	gaussian mixture estimate wishart spherical	0.333333
linear embedding analysis on	linear embedding x	0.200000
write array bytes	externals joblib numpy array wrapper write array array	0.500000
data precision matrix	decomposition base pca get precision	0.066667
measure the similarity	metrics cluster fowlkes mallows	0.250000
apply clustering to	spectral clustering affinity n_clusters	0.166667
estimate model parameters with the em	mixture gmmbase fit x	0.250000
generate	n_samples	0.117647
returns the submatrix corresponding to	get submatrix	0.166667
neg	neg	1.000000
a general function given points on a curve	x y reorder	0.111111
back the data to the original representation	preprocessing robust scaler inverse transform	0.066667
of this	deep	0.153846
call with	externals joblib format call	0.200000
return a tolerance which is independent of	cluster tolerance x tol	0.058824
build a contingency matrix describing	contingency matrix labels_true labels_pred eps sparse	0.166667
mean absolute error	mean absolute error	0.166667
opening the right fileobject from a	read fileobject fileobj	0.100000
clone of self with given	clone with	1.000000
return the	get	0.012048
axis center to the	x axis	0.030769
of x and	decomposition beta divergence x	0.250000
the long type	repr	0.012500
the number of splitting iterations in the cross-validator	split get n splits x	0.111111
initializes the concentration	mixture dpgmmbase initialize gamma	0.333333
dot w	w	0.035714
train estimator on training subsets incrementally and compute	model_selection incremental fit estimator estimator x y	0.200000
for the lfw people	datasets fetch lfw people	0.040000
estimates for each	predict estimator x	0.045455
based on	x connectivity n_clusters return_distance	0.250000
the estimator with randomly drawn parameters	core randomized search cv	0.200000
transform binary labels	transform y	0.023256
the maximum absolute value to be	max abs	0.047619
predict	compute labels predict	0.250000
of the samples x	decision function x	0.018868
gaussian process regression	gaussian process regressor	0.055556
used to build a batch of estimators within	parallel build estimators n_estimators ensemble x y	0.166667
hide warnings without visual nesting	utils ignore warnings call fn	0.200000
template method for updating terminal	ensemble loss function update terminal	0.200000
faces in the wild lfw pairs dataset this	fetch lfw pairs subset	0.035714
of verbose	verbose	0.062500
the graphlasso covariance model to x	covariance graph lasso cv fit x y	0.333333
simply return the input array	neural_network identity	1.000000
the number of splitting iterations in the cross-validator	predefined split get n splits	0.111111
force the execution of the function with the	joblib memorized	0.015625
verbose message on initialization	verbose msg init beg n_init	1.000000
fit the model according to	fit x y sample_weight	0.040000
two rows of a csc/csr matrix	utils inplace	0.250000
fit a single	trees	0.083333
mcd from	candidates x n_support	1.000000
model according to the	x y sample_weight	0.025974
a truncated randomized svd parameters	utils randomized svd m n_components n_oversamples n_iter	0.250000
a collection of text documents to a matrix	vectorizer	0.022222
lower bound on	dpgmmbase lower bound	0.071429
train	base shuffle split iter	0.166667
the estimator with the best found	base search cv predict proba x	0.076923
isotonic regression model : min sum	core isotonic regression	0.055556
for each boosting iteration	boost classifier staged	1.000000
list of exception types to	joblib parallel backend base	0.058824
cf tree	cluster birch fit x y	0.200000
transforms features	x feature_range axis copy	1.000000
evaluate decision function output for x relative	metrics threshold scorer call clf x	0.058824
projection of the data onto	x ridge_alpha	0.071429
compute non-negative matrix factorization nmf	factorization	0.035714
scaling of	scaler	0.031250
estimator with the best found	search cv	0.090909
estimators that implement	utils check partial	0.038462
x	x y iter_offset	0.333333
splits for an arbitrary randomized cv splitter	splits	0.083333
depending from	joblib memorized func	0.014706
sign of elements of all the vectors	sign	0.050000
area under the curve auc using	auc x	0.040000
local outlier factor of x (as	neighbors local outlier factor decision function x	0.100000
similarity coefficient score the	score y_true	0.058824
computes an orthonormal	size n_iter power_iteration_normalizer	0.166667
curve auc using the trapezoidal rule this is	metrics auc x y	0.040000
get	get	0.253012
coordinate descent the elastic net optimization	l1_ratio	0.030303
shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x	0.125000
multi-task elasticnet model trained with l1/l2 mixed-norm	multi task elastic net	0.250000
estimate the spherical variance values	mixture estimate gaussian covariances spherical resp x nk	1.000000
fit the model using	linear_model base randomized linear model fit	1.000000
number of splitting iterations in the cross-validator parameters	model_selection leave one out get n splits	0.111111
to avoid	memory	0.015625
introduces an 'l' suffix when	shape	0.011765
compute k-means	kmeans fit x	1.000000
implement a single boost	base weight boosting boost iboost x y sample_weight	1.000000
names and a name for	get func name	0.047619
get the values	get	0.012048
least squares projection of the data onto the	transform x ridge_alpha	0.071429
get the	voting classifier get	0.200000
the context of the memory	memory	0.015625
generate train	base shuffle split	0.142857
center	cross_decomposition center	1.000000
the logistic loss	logistic loss	0.500000
utility for building a cv in	core check cv cv x	0.031250
fit linear model	base sgdclassifier fit x	0.333333
input vectors individually to unit norm vector length	norm axis copy	0.200000
cohen	cohen	1.000000
the training	fit predict	0.055556
boosted regressor from the training set x y	ensemble ada boost regressor fit x y	1.000000
x as	fit x y	0.005988
fit the calibrated model parameters	core calibrated classifier cv fit x	1.000000
to split data into training and test set	predefined split split x y groups	0.200000
finds indices in sorted array	matching indices tree bin_x left_mask right_mask	0.166667
the image from all	from	0.045455
cross-validated estimates for each input data point	val predict estimator x y cv	0.071429
the file position	externals joblib binary zlib file	0.083333
function is made of a shifted scaled version	mle precision_ alpha	1.000000
check the test_size and train_size at init	split init test_size train_size	0.250000
to dense array	mixin densify	0.100000
fit a single tree in parallel	ensemble parallel build trees tree forest	0.200000
train_size	train_size	0.857143
check to make sure	check	0.017857
the backend and return the number of	externals joblib parallel backend	0.029412
raises an exception in an unfitted estimator	estimators unfitted name estimator	0.142857
incremental mean and	mean	0.035714
unit norm vector length	x norm axis copy	0.200000
can actually run in parallel	joblib parallel	0.028571
is restricted to the binary classification task	y_true y_score	0.054054
the mixture parameters	mixture bayesian gaussian mixture	0.333333
the reduced likelihood function for	gaussian_process gaussian process reduced likelihood function	0.047619
default backend used by parallel	parallel backend backend	0.166667
for 1 iteration	x total_samples batch_update parallel	0.500000
thresholding of array-like or scipy sparse matrix	preprocessing binarize x threshold	0.083333
each	predict estimator	0.045455
neg_label	neg_label	1.000000
utility for building a cv in a user	check cv cv x	0.031250
python object from a file persisted with joblib	joblib load filename mmap_mode	1.000000
in x and y	x y sum_over_features	0.250000
list of exception	parallel backend base get	0.066667
template method for updating terminal	loss function update terminal	0.200000
compute class covariance	core class cov x y priors	0.250000
to the binary classification task	y_true	0.021739
fit estimator	fit	0.003257
which defines all methods a parallelbackend must implement	backend	0.016949
'l' suffix when using	shape	0.011765
terminal regions to median estimates	terminal region tree	0.100000
of the local outlier factor of	neighbors local outlier factor decision function	0.125000
of a memmap instance to	memmap a	0.050000
the training set x	predict x	0.011765
min_bin_freq	min_bin_freq	1.000000
transform x	fit transform x	0.166667
full lars path	linear_model omp path	0.100000
compute	score x	0.033333
determination regression score	metrics r2 score y_true	0.125000
generate a random multilabel classification problem	multilabel classification n_samples n_features n_classes n_labels	0.500000
each input	y	0.002674
the shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage x assume_centered block_size	0.125000
fit all transformers transform the data and concatenate	feature union fit transform x y	0.500000
computes an orthonormal matrix whose range approximates	size n_iter power_iteration_normalizer	0.166667
file	file x y	1.000000
a random multilabel classification	make multilabel classification	0.166667
new ndarray with aligned memory	aligned	0.076923
for each input data	core cross	0.045455
full covariance	multivariate normal density full x means covars min_covar	0.166667
break the pairwise matrix in	metrics parallel pairwise	0.166667
feature selector that removes all low-variance features	variance threshold	1.000000
two clusterings of a set	score labels_true labels_pred sparse	0.047619
a cv in a user friendly	check cv cv	0.031250
the complete cache directory	externals joblib memory clear warn	0.333333
for x relative to y_true	metrics threshold scorer call clf x y sample_weight	0.058824
implement randomized linear	randomized linear	0.500000
under the curve auc using the	metrics auc x y	0.040000
to the median and component wise scale	preprocessing robust scale x	0.125000
dataset this operation is meant to	index_file_path data_folder_path slice_ color	0.033333
apply a transform	transform selected x transform	0.333333
the model according to the given	sample_weight	0.037037
the proper format	base weight boosting validate	1.000000
given param_grid	param iterator	0.166667
with sparse uncorrelated design this dataset is	datasets make sparse uncorrelated n_samples	0.166667
predict	cv predict	0.041667
possible outcomes for	ensemble voting classifier	0.031250
estimates for each input data	cross val predict estimator	0.045455
for the one-vs-one multi class	one vs one	0.050000
split data into training and test	shuffle split split x y groups	0.200000
fit the model to	output estimator fit x y sample_weight	0.200000
predict_log_proba on the estimator	predict log proba x	0.045455
with	base pca	0.071429
to a	a	0.018182
when memory is inefficient to train	x y classes	0.027778
each	cross val	0.038462
to build a batch of estimators within	ensemble parallel build estimators n_estimators ensemble x y	0.166667
a func to	externals joblib pool manager mixin apply async func	0.250000
get the boolean mask indicating which features are	selector mixin get support mask	0.333333
predict using the trained	neural_network base multilayer perceptron predict	0.333333
the precision	metrics precision	0.033333
pickler file	pickler	0.083333
parameters for an estimator	estimator	0.014706
first prime element	prime	0.111111
returns the number of splitting iterations in	group out get n splits x y	0.111111
estimate the precisions parameters of	mixture bayesian gaussian mixture estimate precisions nk xk	0.166667
average log-likelihood	decomposition factor analysis score x y	0.333333
graph of neighbors for	neighbors mixin radius neighbors graph	0.066667
graph of neighbors	neighbors radius neighbors graph	0.066667
unnormalized posterior log probability	core base nb joint log likelihood	0.166667
partial dependence	ensemble partial dependence gbrt	1.000000
input	cross val predict estimator x y	0.045455
used to partition estimators	partition estimators	0.200000
cluster	metrics cluster	0.142857
:ref user guide <classification_report>	y_true y_pred labels target_names	0.200000
modified weiszfeld step	modified weiszfeld step	1.000000
the neighbors within a given radius of a	neighbors lshforest radius neighbors x radius	0.142857
extracts patches	patches	0.055556
cv in	core check cv cv x y classifier	0.031250
the dense dictionary	dict dictionary y	0.111111
determine the optimal	externals joblib parallel backend base compute	1.000000
the function called with the given arguments	joblib memorized func get	0.125000
function returns posterior probabilities	cv predict proba	0.034483
and return that transformed	y	0.002674
predicted probabilities for a calibration curve	calibration curve y_true y_prob normalize n_bins	0.142857
number of points on the grid	parameter grid len	0.333333
based on the	from	0.045455
estimate model parameters with the em	base mixture fit x y	0.200000
by quantile this classification dataset is constructed	datasets	0.015152
housing dataset from statlib	housing	0.111111
each input	estimator x	0.030303
class with a metaclass	externals add metaclass metaclass	0.166667
by definition a confusion matrix :math c is	metrics confusion matrix	0.500000
there to implement the usual api and	x y	0.006466
length is not found and	search	0.019231
lrd the lrd of a	distances_x neighbors_indices	0.047619
other and transforms	x y	0.002155
the reduced likelihood function for the given autocorrelation	reduced likelihood function	0.041667
fit the model	output estimator fit x y	0.200000
which defines all methods a parallelbackend must implement	parallel backend	0.030303
median of data with n_zeros additional zeros	utils get median data n_zeros	0.500000
compute elastic net path with coordinate	enet path	0.050000
x to the	x	0.003384
arr assuming arr is in the log domain	utils logsumexp arr axis	1.000000
read the z-file and return the	externals joblib read zfile file_handle	0.333333
of points based on the percentiles of x	from x x percentiles grid_resolution	0.333333
continuous tie-breaking ovr decision function	utils ovr decision function predictions confidences n_classes	0.333333
global clustering for the	cluster birch global clustering x	0.142857
the minimum covariance	covariance	0.014493
linear assignment problem using the hungarian algorithm	linear assignment	0.090909
array of test vectors x	x	0.001692
from features or	x y sample_weight	0.012987
given args and kwargs	args kwargs	0.100000
cache	externals joblib memory reduce	0.030303
predict on the estimator with	predict	0.006849
estimator with the best found parameters	core base search cv	0.066667
each input data point	cross val predict estimator	0.045455
predict is invariant of compute_labels	compute labels predict name clusterer	1.000000
the model and transform with the	transform x	0.016949
return a platform independent	repr	0.012500
with passive	passive	0.235294
of exception types to	externals joblib	0.004762
of	externals joblib parallel backend base get	0.066667
is not found and raise an exception	utils line search	0.029412
laplacian kernel between x and y	metrics laplacian kernel x y	0.333333
depending	externals	0.011494
the z-file	zfile file_handle	0.333333
is meant to	index_file_path data_folder_path slice_ color	0.033333
func to	async func	0.250000
validation of	svm base lib svm validate	0.500000
introduces an	repr	0.012500
time controlled by self	progress	0.100000
deterministic output from svd	utils svd	0.166667
hashing	hashing	0.625000
mutual information	feature_selection mutual info classif x	0.500000
contains only categorical features	preprocessing one hot encoder fit	0.500000
break the pairwise matrix in	parallel pairwise x y func	0.166667
precision ap from prediction scores this score	precision score	1.000000
the unnormalized posterior log probability of x i	core base nb joint log likelihood x	0.200000
windows cannot encode some characters	joblib clean win chars string	0.333333
for different probability thresholds note this	probas_pred pos_label sample_weight	0.066667
the precision the precision is the ratio	precision	0.016667
python object into one file	value filename	0.083333
full cases	mstep full gmm x responsibilities weighted_x_sum	0.250000
score	score estimator x y scoring	0.333333
check that the	cluster check	0.500000
regressor from the training set x	regressor fit x	1.000000
lfw people dataset this	datasets fetch lfw people	0.040000
l1 distances between	paired manhattan distances	0.083333
or	externals	0.005747
function opening the right fileobject from	joblib read fileobject fileobj	0.100000
whenever scale	scale scale	0.250000
private function used to build a batch of	parallel build	0.047619
find the least-squares solution to a large sparse	lsqr a	0.037037
calculate the posterior log probability	core multinomial nb joint log likelihood	0.083333
fit a single binary estimator one-vs-one	fit ovo binary estimator	1.000000
indices in sorted array of	matching indices tree bin_x left_mask right_mask	0.166667
the model using x as training data	x skip_num_points	0.166667
nothing and return the	feature_extraction	0.037037
of x	decision function x	0.018868
estimates for	predict estimator x	0.045455
passive	linear_model passive	0.200000
net path	enet path	0.050000
given radius of a point or	x radius	0.058824
and compute prediction of init	base gradient boosting init decision function x	0.142857
california housing dataset from	fetch california housing	0.083333
evaluate the density model on the	density score samples x	0.250000
k-neighbors of	kneighbors mixin kneighbors x n_neighbors	0.125000
return the directory in which are persisted the	dir	0.038462
local structure is retained	manifold trustworthiness x x_embedded n_neighbors precomputed	0.200000
estimates the shrunk ledoit-wolf	ledoit wolf x	0.250000
homogeneity metric of a cluster labeling given a	metrics cluster homogeneity score	0.500000
the kernel k	gaussian_process pairwise kernel	0.250000
possible score is 1	score y_true y_pred	0.038462
fit the model from data	manifold spectral embedding fit	0.333333
fit the rfe model	fit x	0.006410
jaccard similarity coefficient score the	score y_true y_pred normalize	0.125000
regression	make regression	1.000000
the covariance	mixture covar	0.125000
the recall the recall is the ratio	metrics recall	0.033333
by scaling each feature to	scale	0.033333
the search	search cv fit x y	0.111111
the score for a fit across one fold	feature_selection rfe single fit rfe	0.200000
distributions for the precisions	precisions x	0.250000
with the best found parameters	search cv	0.036364
graph of neighbors for points in	neighbors mixin radius neighbors graph	0.066667
for building a cv	core check cv cv x	0.031250
norm_order	norm_order	1.000000
the depth a which this function	externals joblib memorized func check previous func code	0.055556
silhouette coefficient	metrics cluster silhouette score x labels metric	0.250000
gaussian and label samples by quantile this	gaussian	0.029412
regression problem with sparse uncorrelated design	sparse uncorrelated	0.166667
two rows of a csc matrix in-place	utils inplace swap row csc x	0.250000
curve auc using the trapezoidal rule this	auc	0.020408
set of samples x	x y sample_weight	0.012987
atol	atol	1.000000
leaf	leaf	1.000000
scale back the data to	preprocessing standard scaler inverse transform	0.066667
covariance matrices from a given template	matrix to match covariance type tied_cv covariance_type n_components	0.333333
computes the position of	mds fit x y	0.066667
center to the median and component wise scale	robust scale	0.125000
of neighbors for	mixin radius neighbors	0.125000
with the best found parameters	core base search cv predict	0.076923
false positives per binary classification threshold	metrics binary clf curve y_true y_score	0.090909
estimates for each input data point	estimator x	0.030303
for each input data	cross val predict estimator x	0.045455
fit subclasses should implement this method!	decomposition base pca fit x y	0.333333
for the	empirical covariance get	0.166667
along an axix on a csr or	axis x axis last_mean last_var	0.200000
the neighbors	radius neighbors	0.043478
estimate the precisions parameters	mixture bayesian gaussian mixture estimate precisions	0.166667
coordinate descent the elastic net optimization function varies	l1_ratio	0.030303
test/test sizes are meaningful wrt	validate shuffle split n_samples test_size train_size	0.111111
weighted log probabilities for each sample	samples x	0.142857
fit an estimator	fit estimator estimator x	0.055556
parallel n_jobs is	n_jobs	0.023256
list of exception types to	externals joblib parallel backend	0.029412
odds ratio	odds estimator	1.000000
ward clustering	cluster ward	1.000000
transform is sometimes referred	transform y	0.023256
given type in	customizable pickler register type	0.333333
one-vs-all fashion several regression and binary classification	y classes neg_label pos_label	0.111111
whether the kernel is	dot product is	1.000000
distribution dataset from phillips et al 2006	distributions	0.166667
timestamp when pickling to avoid the hash	func reduce	0.050000
metaclass	metaclass metaclass	1.000000
func to be run	func	0.034091
to update terminal regions	error update terminal regions tree x	0.500000
random	random choice csc	0.166667
long type introduces an 'l' suffix when	utils	0.009709
regression problem with sparse uncorrelated design	make sparse uncorrelated	0.166667
feature->value dicts	feature_extraction dict vectorizer	0.100000
a which this function is	externals joblib	0.004762
used to build a	parallel build	0.047619
to build a batch of estimators within a	parallel build estimators n_estimators ensemble x y	0.166667
binary metric for multilabel classification parameters	metrics average binary score binary_metric y_true	0.500000
precision ap	precision	0.016667
1-way anova	feature_selection f oneway	0.333333
long type introduces an	shape	0.011765
calculates a covariance matrix shrunk	covariance shrunk covariance emp_cov shrinkage	0.250000
decision function	ada boost classifier decision function	0.166667
rbf	rbf	0.833333
filters the given args and kwargs using	joblib filter args func ignore_lst args kwargs	0.333333
an arbitrary python object into	filename	0.050000
cv in a user friendly	core check cv cv x y classifier	0.031250
cache folders	joblib memory	0.016949
and cv and linearsvc	fit liblinear x y c fit_intercept	0.142857
data	estimator x	0.030303
the curve auc using the	metrics auc x y	0.040000
generative model	base pca get	0.076923
classifier valid parameter keys can be listed with	classifier set	0.125000
rand index	rand score	1.000000
functions of the base	function x	0.030303
fit to data then transform	transformer mixin fit transform x	0.500000
whose range approximates the range	utils randomized range finder	0.083333
n_jobs is the	n_jobs	0.046512
the sample weight array	linear_model base sgd validate sample weight	0.333333
perform the covariance m step	mixture covar	0.125000
predict class log-probabilities for	ensemble bagging classifier predict log proba	1.000000
is restricted to the binary classification	score y_true y_score average	0.076923
validation and conversion of	dtype csr_output	0.166667
and compute prediction of init	ensemble base gradient boosting init decision function x	0.142857
cosine distances between x	cosine distances x	1.000000
and a set	x y	0.002155
input	y	0.002674
using numpy	joblib numpy	0.250000
persist an arbitrary python object into	dump value filename compress protocol	0.250000
if there	utils	0.009709
x with ability to accept precomputed	precomp distr x	0.333333
the dual gap convergence criterion the specific definition	covariance dual gap emp_cov precision_ alpha	0.071429
activations	activations	1.000000
matrix factorization	non negative factorization x	0.043478
are	mask	0.071429
fit and then predict labels	mixture gmmbase fit predict x y	0.333333
ward clustering based on	cluster ward tree x connectivity n_clusters	0.250000
logistic regression	logistic regression path x y	0.333333
shutdown the	backend terminate	0.166667
don't store	joblib memorized func	0.014706
transform	transform	0.235955
n_y	n_y	1.000000
calculate mean update and a youngs	utils incremental mean	0.250000
projection of the data onto	transform x ridge_alpha	0.071429
outlier factor	outlier factor decision function	0.500000
predict	labels predict	0.250000
func	backend base apply async func	0.250000
change the file position	externals joblib binary zlib file seek offset whence	1.000000
updating terminal regions (=leaves)	loss function update terminal region tree terminal_regions leaf	0.200000
run a	externals joblib parallel backend	0.029412
compute joint	manifold joint	1.000000
helper class for automagically batching jobs	auto batching mixin	0.333333
inner fit	neural_network bernoulli rbm fit v_pos rng	0.250000
estimator is using a precomputed gram	core	0.015385
initial parameters of the	parameters	0.055556
find the value in data	data	0.038462
scikit-learn data dir	data home data_home	0.055556
performs clustering on x and returns cluster labels	cluster dbscan fit predict x y	0.166667
evaluate the density model on the	kernel density score samples x	0.250000
gaussian process regression model we can	gaussian_process gaussian process regressor	0.058824
integer	integer	1.000000
compute the deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call y	0.333333
of parameters	estimator parameters	0.500000
leaf	tree base decision tree apply	0.166667
generate indices to split data into	shuffle split split	0.250000
kl	kl	0.750000
compute non-negative matrix factorization nmf find two non-negative	non negative factorization	0.043478
exception	parallel backend base get	0.066667
predict class at each stage for	gradient boosting classifier staged predict	0.500000
for tests involving both blas calls and multiprocessing	safe multiprocessing with blas func	0.500000
the depth	check previous func code	0.333333
method for updating terminal	ensemble loss function update terminal	0.200000
fit all transformers transform	feature union fit transform	0.333333
area under the curve auc from prediction scores	auc score	0.052632
generate cross-validated estimates for each input data	y cv	0.050000
split data into	base shuffle split split	0.250000
posterior probability	proba x	0.111111
inefficient to train all data	x y classes	0.027778
matrix product with the random matrix parameters	core base random projection transform	0.500000
blobs for clustering	blobs	0.125000
the curve auc from prediction scores note this	metrics roc auc	0.166667
a binary classifier on x	binary x	0.500000
fit all transformers transform the	core feature union fit transform	0.333333
n_subsamples	n_subsamples	1.000000
construct a featureunion from the	make union	0.250000
lad	least absolute error update	0.500000
log-det of the cholesky decomposition of matrices	log det cholesky matrix_chol covariance_type n_features	1.000000
y_[i]) ** 2	y sample_weight y_min y_max	0.166667
the wild lfw	lfw	0.034483
the data x which should contain	x y	0.002155
constructs signature	externals signature	0.050000
a new estimator with the same parameters	core clone estimator	0.333333
and return the number of	externals joblib parallel	0.014085
input checker utility for building a cv	check cv cv x	0.031250
matching pursuit problems	n_nonzero_coefs	0.090909
matrix factorization nmf find two non-negative	decomposition non negative factorization x	0.043478
set the diagonal of the laplacian	set diag laplacian	0.333333
posterior log probability	bernoulli nb joint log likelihood	0.083333
list	parallel backend base get	0.066667
for updating terminal	update terminal	0.142857
the oracle approximating shrinkage covariance model according to	covariance oas fit	0.083333
x using the	x	0.003384
perform	responsibilities params	0.500000
func to	parallel backend base apply async func	0.250000
incrementally fit	core multi output regressor partial fit x y	0.200000
avoid the hash	memorized	0.015873
as the maximizer of	gaussian process arg max	0.047619
eval function func	eval func	0.166667
log-likelihood of a	covariance score	0.071429
corresponding to test sets	iterator iter test	1.000000
an estimator within a	estimator estimator x	0.090909
to a large sparse linear system	utils lsqr a	0.037037
fit the gradient boosting model	base gradient boosting fit x	1.000000
of probability estimates	proba x	0.111111
shape (n_samples n_features)	decomposition infer	0.200000
two clusterings of a set	score labels_true labels_pred	0.047619
solve the linear assignment problem using	linear assignment x	0.090909
sparse and	y	0.002674
the number of splitting iterations in the	model_selection cviterable wrapper get n splits x y	0.111111
finds indices in sorted array of	find matching indices tree bin_x left_mask right_mask	0.166667
with the best found	core base search cv predict proba x	0.076923
fit all transformers transform the	union fit transform x	0.333333
the score for a fit	fit rfe estimator x y	0.166667
the mstep	do mstep	0.500000
strip lines	datasets strip	0.076923
estimate the tied covariance matrix	mixture estimate gaussian covariances tied resp x nk	1.000000
early stopping logic	cluster mini batch convergence model iteration_idx n_iter tol	1.000000
false positives per binary classification threshold	metrics binary clf curve	0.090909
to	parallel	0.019231
grid of points based on the	ensemble grid from	0.166667
run in parallel	parallel backend	0.030303
the average path length of	ensemble average path length	0.090909
cv in a	core check cv cv x y classifier	0.031250
the paired distances between x and y	paired distances x y	0.500000
covariance matrix	cov x y priors shrinkage	0.500000
find two non-negative matrices w h whose product	w h n_components	0.038462
density estimation read more in	density	0.043478
locally linear embedding analysis on the	manifold locally linear embedding	0.062500
x i	x	0.001692
one group out cross-validator provides train/test indices to	one group out	0.166667
compute log probabilities within	ensemble parallel predict log proba	0.058824
get the values used to	neural_network sgdoptimizer get	0.125000
convert coefficient	linear_model sparse	0.076923
array corresponding to this wrapper	joblib numpy array wrapper	0.333333
with respect to coefs and intercept for	activations deltas	0.032258
corresponding to this	externals joblib numpy	0.250000
normalize x by scaling rows	cluster scale normalize x	0.142857
coefficient	linear_model	0.025641
function used to compute log probabilities within	parallel predict log proba	0.058824
the generative	decomposition base	0.076923
note this implementation is	metrics roc	0.040000
for specified layer	grad layer n_samples	0.166667
the voting classifier valid parameter keys	voting classifier	0.035714
perform dbscan clustering from	cluster dbscan fit x y sample_weight	0.200000
normalize rows	normalize	0.100000
matrix cells	covers	0.166667
in multiplicative update nmf	multiplicative update h	0.500000
check initial parameters of the derived	mixture check parameters x	0.166667
load and return	load	0.178571
compute the decision function of	ada boost classifier decision function	0.166667
finds indices in	find matching indices	0.250000
k x y and optionally its gradient	gaussian_process exponentiation call x y eval_gradient	0.333333
the sign	sign flip	0.066667
component wise scale to unit variance	preprocessing scale	0.090909
prediction of init	init decision function x	0.142857
l1 distances between the vectors in	manhattan distances	0.083333
arguments	func get output	0.125000
independent representation of	utils shape repr	0.013699
for a calibration	calibration	0.071429
rand index adjusted for	cluster adjusted rand score	0.333333
logic estimators that implement the partial_fit api	utils check	0.023810
a mostly low rank matrix with bell-shaped singular	make low rank matrix	0.083333
1 and 2 in the wikipedia page	utils step1 state	0.142857
compute incremental mean and	incr mean	0.166667
"news" format strip lines	datasets strip newsgroup	0.090909
to avoid the hash	func	0.011364
the precision is the	precision	0.016667
covertype	covtype	0.125000
and the gradient	and gradient	1.000000
the distribution p(v|h)	visibles h rng	0.500000
provided 'means'	mixture check means means n_components n_features	0.333333
sample from the	rbm sample	0.500000
of samples x	x y	0.002155
mean squared logarithmic error regression loss	mean squared log error	0.200000
projection of the data onto the sparse components	decomposition sparse pca transform x ridge_alpha	0.200000
generates	repeated splits	0.125000
multi-class targets using underlying estimators	core output code	0.250000
the kernel k x	gaussian_process constant kernel call x	0.333333
this function first to assert that	check	0.017857
mean shift clustering	cluster mean shift x bandwidth	0.500000
fetch	datasets fetch	0.200000
this classification dataset is constructed by	datasets	0.015152
scale back the data to the original representation	inverse transform x copy	0.066667
apply clustering	spectral clustering	0.142857
of neighbors for points	radius neighbors mixin radius neighbors	0.125000
computes the position of the points	mds	0.050000
set the sample weight array	linear_model base sgd validate sample weight	0.333333
check	mixture check	0.428571
split data into training and test set	model_selection cviterable wrapper split x y groups	0.200000
checker utility for building a cv	check cv cv x	0.031250
returns the huber loss	linear_model huber loss	0.333333
kernel x and y	kernel x y gamma	0.500000
split data into training and test	time series split split x y groups	0.200000
batch_size elements from 0 to	batch_size	0.100000
sizes	translate train sizes	0.066667
descent fit is on	fit	0.003257
for full covariance	density full x means covars min_covar	0.166667
rare or too common features	feature_extraction count vectorizer limit features x vocabulary high	0.250000
center and scale the data parameters	preprocessing robust scaler transform x y	0.200000
memmap instance to reopen on	joblib reduce memmap	0.166667
remove a subcluster	subcluster	0.090909
base class for nearest neighbors estimators	neighbors base	1.000000
returns whether the kernel is stationary	gaussian_process stationary kernel mixin is stationary	1.000000
output for x relative to y_true	metrics threshold scorer call clf x y sample_weight	0.058824
rows of a csc/csr matrix in-place	utils inplace swap row x m	0.250000
fit the model	linear svr fit x	0.333333
compute the decision function of the	decision function x	0.018868
fit the model according to the given training	svm linear svr fit x y sample_weight	0.250000
each input data	val predict estimator x y	0.045455
collection of text	vectorizer	0.044444
zlib	zlib	1.000000
deviance	deviance call	0.333333
a random regression problem with sparse uncorrelated design	make sparse uncorrelated n_samples n_features	0.166667
of init	init decision function	0.142857
a locally linear embedding analysis on the	manifold locally linear embedding x	0.071429
load the kddcup99 dataset	kddcup99	0.090909
scale each non zero row of x	transform x y	0.031250
compute the mlp loss function and its	neural_network base multilayer perceptron	0.083333
compute class	utils class	1.000000
memmap instance to reopen on same file	memmap	0.066667
make predictions using a single binary estimator	binary estimator	0.090909
make and configure	base ensemble make estimator append random_state	0.166667
posterior log probability of the samples	multinomial nb joint log likelihood	0.083333
multi-class labels parameters	y threshold	0.166667
predict class probabilities	ensemble gradient boosting classifier predict proba	0.500000
regression model we can also predict based	regressor predict	0.200000
matching pursuit problems	n_nonzero_coefs tol	0.250000
the mean silhouette coefficient	metrics cluster silhouette score x labels metric sample_size	0.250000
optimal batch	compute batch	1.000000
the default backend	backend backend	0.333333
return a buffered	buffered	0.125000
private function used to compute decisions within a	decision function estimators estimators_features x	0.500000
the posterior log probability of the	core multinomial nb joint log likelihood	0.083333
elastic net path with coordinate	enet path x	0.050000
features according to the k highest scores	kbest	1.000000
predicted target values for x	estimator x	0.030303
determine	backend	0.016949
two clusterings of a	score labels_true labels_pred	0.047619
threshold value	threshold estimator importances threshold	0.500000
sparse combination of the dictionary atoms	sparse coding mixin transform x y	0.333333
function used to compute log	log	0.018868
given type in the dispatch table	type reduce_func	1.000000
fit gaussian process classification model	gaussian_process gaussian process classifier fit x y	0.500000
used to fit an estimator within	fit estimator estimator	0.055556
call transform on the	transform	0.011236
classifier valid parameter keys	classifier	0.013699
transform a sequence of instances to a	feature hasher transform raw_x	0.333333
the file	zlib file	0.230769
dataset and properly handle kernels	safe split estimator x y indices	0.200000
generate	shuffle split	0.142857
mostly low rank matrix with	low rank matrix	0.083333
x_discrete	x_discrete	1.000000
ridge and	x y	0.002155
fit	svr fit x y	0.333333
for validation and conversion of csgraph	validate graph csgraph directed dtype csr_output	0.166667
fit ridge regression	ridge classifier fit	1.000000
utility for building a cv in a	cv cv x	0.031250
the linear assignment problem using the hungarian	linear assignment	0.090909
boost	ada boost classifier boost	0.200000
estimates for each input data point	cross val predict estimator x y	0.045455
an array shape under python 2 the long	shape	0.011765
computes y * np dot x w	linear_model intercept dot w x y	0.500000
number of splitting iterations in the cross-validator	kfold get n splits x y groups	0.111111
make sure that an estimator implements the	core check estimator estimator	0.142857
the recall the recall is	recall	0.028571
boolean mask indicating which	support mask	0.125000
of vectors for reproducibility flips the sign of	deterministic vector sign	0.066667
for validation and conversion of csgraph	csgraph directed dtype csr_output	0.166667
a single boost	ada boost classifier boost	0.100000
log probability for full	log multivariate normal density full x means	0.333333
the gaussian process model fitting	gaussian_process gaussian process fit x y	0.250000
a which	externals joblib memorized func	0.013158
minimize the objective function iterating once over all	coordinate descent x	0.333333
compute the mlp loss function	neural_network base multilayer perceptron	0.083333
the dual gap convergence criterion	dual gap	0.071429
the shortest path length from source to all	utils single source shortest path length graph source	0.111111
the wild lfw pairs	lfw pairs	0.018868
to line_search_wolfe2 if	wolfe12 f fprime xk pk	0.028571
for validation and conversion of csgraph inputs	utils sparsetools validate graph csgraph directed dtype csr_output	0.166667
to fit a single tree	trees tree forest	0.142857
with the best found parameters	base search cv	0.052632
generate the random projection matrix	gaussian random projection make random matrix n_components n_features	1.000000
platform independent representation of	utils	0.009709
deviance loss function for binary classification	deviance	0.062500
matrices w	w	0.035714
from svd	svd	0.071429
fit the	pca fit	0.222222
an array shape under python 2	repr shape	0.166667
validation on an array	check array array accept_sparse	0.250000
w in multiplicative update nmf	multiplicative update w x w h	0.500000
batch and dispatch them	dispatch one batch	0.500000
[rouseeuw1984]_ aiming at computing mcd	step x n_support remaining_iterations initial_estimates	0.111111
predicted probabilities for a calibration curve	core calibration curve y_true y_prob normalize n_bins	0.142857
sparse	linear_model sparse	0.076923
search	search	0.115385
calculate true and false positives per binary classification	binary clf curve y_true y_score	0.090909
em algorithm and return the cluster weights	gmmbase	0.062500
sample from the distribution p(v|h)	neural_network bernoulli rbm sample visibles h rng	1.000000
the binary classification task	recall curve y_true	0.142857
selector	selector	1.000000
function called with the given arguments	externals joblib memorized func get output	0.125000
force	joblib memorized	0.015625
we can also predict based on an unfitted	predict	0.006849
curve auc using the trapezoidal rule this is	auc x	0.040000
the shortest path length from source	shortest path length graph source	0.111111
file object in write	externals joblib write	1.000000
return a tolerance which is	tolerance x tol	0.058824
huber loss and the gradient	huber loss and gradient w x y	0.333333
tokenizer	tokenizer	1.000000
generate an array with block	datasets make	0.015625
of init	ensemble base gradient boosting init decision function	0.142857
fit a single binary estimator one-vs-one	fit ovo binary estimator x y i	1.000000
fit the model with x	skewed chi2sampler fit x	1.000000
all estimators	all estimators	1.000000
combination of the dictionary atoms	coding mixin transform	1.000000
of a memmap instance to reopen on same	joblib reduce memmap a	0.050000
a batch of estimators within a	estimators n_estimators	0.083333
estimate model parameters with the em	mixture base mixture fit x y	0.200000
determine absolute sizes of training	translate train sizes	0.066667
is a general function given points on	y reorder	0.111111
species	fetch species	0.500000
the shrunk ledoit-wolf covariance	covariance ledoit wolf x assume_centered	0.125000
validity	params	0.028571
under the curve auc	metrics auc x	0.040000
label propagation classifier read more in	label propagation	0.200000
building a cv	cv cv x y	0.031250
array from the meta-information	externals joblib zndarray wrapper read unpickler	0.043478
as training data	isotonic regression	0.055556
linear embedding analysis on the data	linear embedding x n_neighbors	0.200000
given column class distributions parameters	classes class_probability	0.166667
generate train	base shuffle split iter	0.166667
each	val predict estimator x y	0.045455
a mask	mask	0.071429
process or thread pool	externals joblib multiprocessing	0.052632
for the california housing dataset from statlib	fetch california housing	0.083333
fit on the estimator with randomly drawn	randomized search cv fit x y	0.500000
that for c	c x y loss	0.030303
compute decision function of	decision function	0.050000
content of the data home cache	datasets clear data home	0.076923
the diagonal of the	diag	0.031250
aggressive algorithm	aggressive regressor	0.166667
generate isotropic gaussian blobs for clustering	datasets make blobs n_samples n_features centers cluster_std	0.333333
center	center	1.000000
variance regression score function	variance	0.058824
check the test_size and train_size at init	validate shuffle split init test_size train_size	0.250000
of feature names ordered by their	vectorizer get feature names	0.125000
a memmap instance to reopen on	externals joblib reduce memmap a	0.050000
the neighbors within a given radius of a	neighbors x radius	0.142857
absolute error regression loss read more	absolute error y_true y_pred	0.142857
undo the scaling of	preprocessing min max scaler inverse transform	0.500000
full lars path parameters	omp path	0.100000
dual gap convergence criterion	covariance dual gap emp_cov precision_	0.071429
rows of u such that	flip u	0.047619
memmap	array memmap	1.000000
estimator with the best found parameters	model_selection base search cv predict proba x	0.076923
generate	n_samples n_features n_classes n_labels	0.333333
best possible score is 1	score y_true y_pred sample_weight	0.062500
avoid the hash depending	externals joblib memorized func	0.013158
modify the sign of vectors for reproducibility flips	deterministic vector	0.076923
one group	one group	1.000000
the linear assignment problem using the	linear assignment	0.090909
rand index adjusted	adjusted rand	0.333333
download the 20 newsgroups data and stored it	download 20newsgroups	0.200000
read more in the :ref user guide	y_true y_pred	0.111111
perform the actual data loading for the lfw	fetch lfw	0.083333
to edges weighted	edges weights	1.000000
full	normal density full x means	0.166667
described in [rouseeuw1984]_ aiming at computing	step x n_support remaining_iterations initial_estimates	0.111111
for each input data	core cross val predict estimator x y	0.045455
under the curve auc from prediction scores note	metrics roc auc score	0.166667
check input and compute prediction of init	boosting init	0.142857
this	params	0.085714
parameters theta as the maximizer of the	arg max	0.047619
to unit norm parameters	preprocessing normalizer	1.000000
repeated splits for an arbitrary	repeated splits	0.125000
zero row of x	transform x	0.016949
reconfigure the backend and	backend base configure n_jobs	0.500000
delta	delta	1.000000
the model and transform with the	transform	0.011236
score is 1	score y_true y_pred sample_weight	0.062500
bag predictions and score	ensemble base bagging set oob score x y	0.250000
a byte string to the	externals joblib binary	0.200000
makes sure that whenever scale	scale scale copy	1.000000
the grid of alpha values	alpha grid x	0.166667
perform classification on	base svc predict	1.000000
general function given points on a curve	reorder	0.071429
x	x check_input	0.250000
returns false for indices increasingly apart the distance	joblib verbosity filter index	0.055556
empty	joblib memorized func clear warn	0.250000
dual gap convergence criterion the	covariance dual gap emp_cov	0.071429
a fit across one fold	feature_selection rfe single fit	0.200000
error regression	error y_true	0.111111
the number of splitting iterations in the	cross validator get n splits x y groups	0.125000
normalize x according to kluger's	cluster log normalize x	0.200000
fit on the estimator	fit	0.003257
k-neighbors of a point	kneighbors mixin kneighbors x n_neighbors	0.125000
return the compressor matching	externals joblib detect compressor	1.000000
compute non-negative matrix factorization nmf find two	non negative factorization	0.043478
the sign of	sign flip	0.066667
array-like or scipy	preprocessing binarize x threshold	0.083333
nmf find two non-negative matrices w h	w h	0.031250
workaround python 2 limitations of pickling instance methods	obj methodname	0.111111
predict class probabilities for x in 'soft' voting	ensemble voting classifier predict proba x	1.000000
k-fold iterator variant with non-overlapping	kfold	0.117647
indices to split data into	model_selection base kfold split x	0.250000
gaussian process classification gpc based on laplace approximation	gaussian process classifier	0.500000
cross-validated orthogonal matching pursuit model omp parameters	orthogonal matching pursuit cv	0.200000
predict class for	ensemble gradient boosting classifier predict	0.333333
false positives per binary classification threshold	metrics binary clf curve y_true	0.090909
filters the given args and kwargs	joblib filter args func ignore_lst args kwargs	0.333333
training and test	x y groups	0.500000
regressor	regressor	0.432432
cosine distance	neighbors lshforest compute distances query candidates	0.333333
strip lines beginning	datasets strip	0.076923
theta as the maximizer of the	gaussian process arg max	0.047619
mask to edges weighted or	feature_extraction mask edges weights mask edges weights	0.166667
to delete to	to delete	1.000000
and raise valueerror if not	ensemble base gradient boosting	0.111111
pursuit	pursuit	0.454545
return the kernel k	gaussian_process compound kernel call	0.333333
generates boolean masks corresponding to test sets	partition iterator iter test masks	1.000000
folders to make cache size fit in bytes_limit	joblib memory reduce size	0.083333
and concatenate results	x y	0.002155
store the timestamp when pickling to	joblib memorized func reduce	0.050000
the range of	utils randomized range	0.083333
diagonal of the kernel k	white kernel diag	1.000000
compute probabilities	proba	0.058824
matrix for label spreading computes the graph laplacian	semi_supervised label spreading build graph	0.142857
the training set according to	fit predict	0.055556
transform binary labels back to multi-class	binarizer inverse transform y threshold	0.333333
h	w h	0.031250
biclusters	consensus	0.111111
dense dictionary factor in place	dictionary y code verbose	0.333333
compute the l1 distances between the vectors	metrics paired manhattan distances	0.083333
return number of samples in array-like x	utils num samples x	0.250000
fn where tp is the number	score y_true y_pred labels pos_label	0.027778
unbalanced datasets	weight class_weight y	0.200000
transform documents to document-term matrix	tfidf vectorizer transform raw_documents copy	1.000000
update the dense dictionary factor in	update dict dictionary y	0.333333
retrieve a reliable	externals joblib get func	0.200000
in the	ensemble base	0.166667
for each sample	samples	0.052632
the sign of elements	sign	0.050000
batch_size elements from 0	batch_size	0.100000
perform dbscan clustering from vector	cluster dbscan x eps min_samples	0.200000
evaluate the significance of a cross-validated	x y cv	0.050000
tree regressor	tree regressor	1.000000
apply clustering	spectral clustering affinity n_clusters	0.166667
function used to fit	fit	0.003257
mean squared logarithmic error regression loss read more	metrics mean squared log error	0.200000
fit an estimator within a job	parallel fit estimator estimator	0.333333
return_std	return_std	1.000000
precisions parameters of the precision distribution	precisions nk xk sk	0.166667
model according to	y sample_weight	0.035714
gaussian process regression model we can also predict	gaussian_process gaussian process regressor predict x	1.000000
terminal regions	terminal regions tree x	1.000000
density model on	density	0.043478
tied	tied	1.000000
vectors for reproducibility flips the sign of elements	deterministic vector sign	0.066667
printer	printer	1.000000
partially fit underlying estimators should be used	core one vs one classifier partial fit x	0.166667
setting the parameters for the voting classifier valid	ensemble voting classifier set params	0.037037
to what extent the local structure is retained	x_embedded n_neighbors precomputed	0.200000
weighted graph of neighbors for points in x	radius neighbors graph x	0.500000
center and	preprocessing robust scaler transform x y	0.200000
multiple files	files files n_features dtype multilabel	0.500000
compute the loss of prediction pred and y	ensemble loss function call y pred sample_weight	0.333333
linear embedding read more in the	linear embedding	0.083333
the k-neighbors of a point	neighbors kneighbors mixin kneighbors	0.100000
x (as bigger is	x	0.001692
suitable step length is not found and raise	line search	0.029412
on the estimator with the best found	search cv predict proba x	0.076923
when memory is inefficient to train all	x y classes	0.027778
note	roc	0.033333
perform dbscan clustering from vector array or distance	cluster dbscan x	0.200000
for a sparse matrix	utils svds a	0.166667
and linearsvc	liblinear x y c fit_intercept	0.142857
likelihood function for the given autocorrelation parameters	likelihood function	0.142857
each input	core cross	0.045455
back the data to	standard scaler inverse transform x	0.066667
a locally linear embedding analysis on	locally linear embedding	0.050000
maximum absolute value to be used	max abs	0.047619
build a batch of estimators within a job	build estimators n_estimators ensemble x y	0.166667
the decision function of the	decision function	0.025000
test	shuffle split iter	0.166667
in [rouseeuw1984]_ aiming at computing mcd	covariance c step x n_support remaining_iterations initial_estimates	0.111111
variance	variance	0.529412
fit all transformers transform the data and concatenate	union fit transform x y	0.500000
and	scaler transform x y	0.200000
wild lfw pairs dataset this	fetch lfw pairs	0.018868
axis center to the median and	axis	0.014085
the reduced likelihood	gaussian process reduced likelihood	0.142857
rand index adjusted	cluster adjusted rand	0.333333
a mostly low	datasets make low	0.333333
barycenter weighted graph of k-neighbors	barycenter kneighbors graph	0.333333
private function used to build a batch	build	0.037037
p_h	p_h	1.000000
process or thread	multiprocessing backend	0.038462
by scaling each	scale	0.033333
x parameters	x n_neighbors reg n_jobs	0.500000
don't store the	joblib memory	0.016949
indices to split data into	model_selection cviterable wrapper split	0.250000
cls	cls	1.000000
source	source	0.500000
standardize a dataset along	with_mean with_std	0.200000
long type introduces an 'l'	utils	0.009709
matrix to dense array	sparse coef mixin densify	0.100000
autocorrelation parameters theta as the maximizer of the	gaussian_process gaussian process arg max	0.047619
the number of splitting iterations in the cross-validator	kfold get n splits x y groups	0.111111
of	base get	0.066667
file was opened for writing	file writable	0.250000
distance of the samples x	x	0.005076
an estimator implements the necessary	estimator estimator	0.052632
calculate approximate perplexity for data	decomposition latent dirichlet allocation perplexity	0.333333
this dataset is described in celeux	datasets make	0.015625
random regression problem with sparse uncorrelated design	make sparse uncorrelated n_samples n_features random_state	0.166667
constant kernel	constant kernel	0.250000
validate x whenever one tries to predict	base decision tree validate x predict x check_input	0.500000
density model on the	density	0.043478
a locally linear embedding analysis on the data	manifold locally linear embedding	0.062500
the directory in which are persisted	get output dir	0.047619
log-likelihood	score	0.010101
precisions	check precisions precisions covariance_type n_components	0.250000
precisions parameters of	precisions nk	0.166667
the given arguments	externals joblib memorized func get	0.125000
laplacian	spectral	0.026316
function	ensemble parallel decision function	1.000000
dispatch them	externals joblib parallel dispatch	0.250000
hash depending from it	externals joblib memory	0.016949
list	externals joblib	0.004762
ward	ward tree	1.000000
locally linear embedding analysis on the	manifold locally linear embedding x n_neighbors	0.071429
coverage error measure compute how far we need	coverage error y_true	0.166667
boolean thresholding of array-like or scipy sparse	binarize	0.045455
according to the given training	sample_weight	0.037037
autocorrelation parameters theta as the maximizer of the	process arg max	0.047619
file descriptor for the underlying file	externals joblib binary zlib file fileno	0.333333
step length is not found and raise	line search	0.029412
median and quantiles to be used for scaling	preprocessing robust scaler fit x y	0.500000
generate cross-validated estimates for each input	predict estimator x y cv	0.071429
list of exception types to	base	0.014286
with sparse uncorrelated design this dataset is	datasets make sparse uncorrelated n_samples n_features random_state	0.166667
of an array shape under python 2 the	shape	0.011765
measure the similarity of two clusterings of	metrics cluster fowlkes mallows score labels_true labels_pred sparse	0.333333
sets the flattened log-transformed non-fixed hyperparameters	gaussian_process exponentiation theta theta	0.333333
don't store the timestamp when pickling to	memory reduce	0.030303
problem with sparse uncorrelated design this dataset is	datasets make sparse uncorrelated n_samples	0.166667
to run a	externals joblib parallel backend	0.029412
this function returns posterior probabilities of classification	calibrated classifier cv predict proba x	0.200000
and dispatch	joblib parallel dispatch one	0.250000
generate indices to split data into	model_selection time series split split x	0.250000
with the best found	model_selection base search cv predict	0.076923
of approximate nearest	lshforest kneighbors x	0.500000
two non-negative matrices w h whose product approximates	w h	0.031250
generate	predict estimator x	0.045455
class	utils compute class	0.166667
sparse and dense inputs	x y sample_weight	0.012987
one-vs-all fashion several regression and binary	y classes neg_label pos_label	0.111111
placeholder for fit	fit x	0.006410
cleanup a temporary folder if still existing	externals joblib delete folder folder_path	0.250000
a one-hot aka one-of-k scheme	one hot encoder	0.200000
recall is the ratio tp / tp	metrics recall	0.033333
**kwargs, in the context of the memory	memory	0.015625
file_paths	file_paths	1.000000
base class for estimators	base	0.014286
on the training set according to	fit	0.003257
alpha	linear_model alpha	0.500000
as a sparse combination of the dictionary atoms	decomposition sparse coding mixin transform x y	0.333333
online learning prevents rebuilding of cftree from scratch	cluster birch partial fit	1.000000
estimate the spherical wishart distribution parameters	gaussian mixture estimate wishart spherical nk	0.333333
score by cross-validation	core cross val score estimator	0.333333
estimate the precisions parameters of the precision distribution	gaussian mixture estimate precisions nk xk sk	0.166667
check according to li	core check	0.111111
a transform function to portion of selected features	x transform selected	0.333333
divide	divide	1.000000
a general function given points on a	x y reorder	0.111111
more in the :ref user guide <univariate_feature_selection>	fwe	0.200000
outcomes for samples in x	ensemble voting classifier predict	0.100000
and dot	beta divergence	0.500000
r^2 coefficient of determination regression score	r2 score y_true y_pred	0.125000
transform the	transform	0.022472
func to be	sequential backend apply async func	0.250000
average hamming loss	metrics hamming loss y_true y_pred labels sample_weight	0.333333
all the covariance	matrix to match covariance	0.250000
the model parameters of	base mixture	0.111111
orthogonal matching pursuit step	n_nonzero_coefs tol	0.250000
fit the hierarchical	feature agglomeration fit	1.000000
parameters	model_selection base	1.000000
avoid the	joblib memorized func	0.014706
log probability	log multivariate	0.500000
inverse permutation	datasets inverse permutation	1.000000
net path with coordinate	path	0.025641
transform data to polynomial features	preprocessing polynomial features transform x	0.500000
mean absolute error regression loss read more in	metrics mean absolute error	0.166667
methods a parallelbackend must implement	parallel backend	0.030303
fall back to line_search_wolfe2 if suitable step length	wolfe12 f fprime xk pk	0.028571
transform a sequence of instances to	feature hasher transform raw_x y	0.333333
to split data into training and test	model_selection base shuffle split split x y groups	0.200000
homogeneity metric of a cluster labeling given	cluster homogeneity score labels_true labels_pred	0.500000
dtype of x and y	x y	0.002155
the maximizer of the reduced	process arg max reduced	0.200000
is restricted to the binary classification task	score y_true y_score average sample_weight	0.076923
fit the hierarchical clustering on	cluster feature agglomeration fit	0.250000
number of splitting iterations in the cross-validator	model_selection cviterable wrapper get n splits x y	0.111111
sizes of	train sizes	0.066667
dictionary factor	dictionary y	0.111111
returns the number of splitting iterations in the	base kfold get n splits x y	0.111111
to binary labels the output of transform	transform	0.011236
classifier valid	classifier set params	0.125000
workaround python 2 limitations	obj methodname	0.111111
module names and a name	name	0.033333
the similarity of two sets of biclusters	consensus score a b similarity	0.500000
of regularization	x y pos_class cs	0.166667
inplace row scaling of a	inplace row scale	0.142857
the reduced likelihood function for	process reduced likelihood function	0.047619
the maximum absolute value to be used	preprocessing max abs	0.050000
the model using x as training	x	0.003384
measure the	metrics cluster fowlkes mallows	0.250000
range of	range finder	0.083333
for the lfw people dataset this operation	fetch lfw people	0.040000
getter	empirical covariance get	0.166667
get parameters of this kernel	gaussian_process exponentiation get params deep	0.500000
fit	decomposition pca fit	0.250000
adaboost	weight boosting	1.000000
implementation is restricted to the binary classification task	y_true y_score pos_label sample_weight	0.066667
y as	y copy_x	0.333333
scale back the data to	standard scaler inverse transform x copy	0.066667
best found parameters	core base search cv predict proba	0.076923
the content of the data home	data home	0.076923
all meta estimators in	meta	0.043478
predict class probabilities at each stage	gradient boosting classifier staged predict proba	0.500000
cv in a	cv cv x y	0.031250
to make cache size fit	memory reduce size	0.083333
factorization	negative factorization x	0.043478
where tp is the number of	score y_true y_pred labels pos_label	0.055556
net path with coordinate	enet path x	0.050000
fetch	fetch	0.857143
scale if the scale parameter==true	scale	0.033333
list of exception	joblib parallel	0.028571
constructs signature from the given list	signature	0.047619
predict class	decision tree predict	0.500000
and predicted probabilities for a calibration curve	core calibration curve y_true y_prob normalize	0.142857
fit estimator and predict	model_selection fit and predict estimator x	1.000000
false positives per binary	binary clf curve y_true y_score pos_label	0.090909
wild lfw pairs dataset this dataset is a	datasets fetch lfw pairs	0.018868
sizes of training subsets and validate 'train_sizes'	sizes train_sizes n_max_training_samples	0.500000
module names and a name for the	get func name	0.047619
with a	externals with	1.000000
c such that for c in	c	0.022222
check initial parameters of the derived class	check parameters x	0.200000
generate indices to split data into	base shuffle split split x	0.250000
the unnormalized posterior log probability of x	nb joint log likelihood x	0.111111
process or thread pool	multiprocessing backend	0.076923
which are going to run	joblib multiprocessing backend effective	0.250000
the cache for the function	externals joblib memorized	0.013699
function varies for mono and multi-outputs	y eps n_alphas	0.250000
used to fit a single tree	build trees tree forest	0.142857
cross-validated estimates for each	x y cv	0.050000
scale back	inverse transform x copy	0.066667
compute the decision function	gradient boosting classifier decision function	0.166667
free energy f v = -	bernoulli rbm free energy	0.066667
multi-class targets using	core output code classifier	0.250000
deep	deep	0.384615
the image samples in x into	x	0.001692
quantile this classification dataset is constructed	datasets	0.015152
update the variational distributions for	mixture dpgmmbase update	0.250000
apply	transform	0.011236
private function used to compute log probabilities	parallel predict log proba	0.058824
fit x into an embedded space	manifold tsne fit transform x	1.000000
using the	utils	0.009709
get the	sgdoptimizer get	0.125000
or lasso path using lars algorithm [1] the	linear_model lars path x	0.100000
t-distributed stochastic neighbor embedding	tsne	0.166667
solve the linear assignment problem using the	linear assignment x	0.090909
returns the number of splitting iterations in	leave one out get n splits	0.111111
gaussian and label samples by	make gaussian	0.125000
gaussian and	gaussian	0.029412
introduces an	utils shape	0.013699
in place	y code verbose	0.333333
n_zeros	n_zeros	0.555556
to fit an estimator within a	fit estimator estimator x y	0.071429
exception	parallel backend base	0.037037
kernel k	gaussian_process exp sine squared call	1.000000
n_estimators	n_estimators	1.000000
the leaves of the	leaves	0.071429
compute elastic net path with coordinate descent the	enet path	0.050000
voting classifier valid parameter keys can be	ensemble voting classifier set	0.037037
inefficient to	y classes	0.027778
remove cache folders to	joblib memory	0.016949
priors	priors	1.000000
input data	x	0.001692
regression and cv and linearsvc	liblinear x y c fit_intercept	0.142857
precisions	precisions precisions covariance_type	0.250000
a text report showing the	report	0.047619
avoid the hash	externals joblib	0.009524
compute the centroids on x by	fit x y	0.005988
can actually run in parallel	externals joblib parallel	0.014085
for the lfw people	lfw people	0.040000
analysis fa a simple	analysis	0.045455
estimator	core base estimator	1.000000
threshold	estimator importances threshold	0.500000
scaling	scaler	0.187500
boolean thresholding of array-like or scipy sparse	binarize x	0.083333
using the gaussian process regression model	gaussian_process gaussian process regressor	0.058824
read	joblib read	0.333333
local outlier factor of	neighbors local outlier factor decision	0.125000
mutual information between two variables	mi x y	1.000000
returns	w	0.035714
return the path of the scikit-learn data dir	get data home data_home	1.000000
matrices w h whose product approximates the non-	w h	0.031250
check if y is in a multilabel format	is multilabel y	0.333333
validity of	params	0.028571
the number of	n	0.100000
of	base	0.014286
evaluate the density model on	kernel density score samples	0.250000
the recall is the	recall	0.028571
cv in	check cv cv	0.031250
of points on the grid	grid len	0.333333
data and	mixin score x y sample_weight	1.000000
number of splitting iterations in the	group out get n splits x	0.111111
range of	randomized range	0.083333
score by cross-validation read more in	model_selection cross val score estimator x y groups	0.166667
computes the weighted graph of neighbors for points	neighbors mixin radius neighbors graph	0.066667
each input data	predict estimator x	0.045455
uncompressed bytes	binary zlib	0.166667
spectral embedding for non-linear dimensionality	spectral embedding	0.200000
and	divergence	0.090909
flattened log-transformed non-fixed hyperparameters	compound kernel theta	1.000000
computing truncated	truncated	0.100000
exception types	parallel	0.019231
for x relative to y_true	scorer call estimator x y_true sample_weight	0.200000
for each input	val predict estimator	0.045455
generate train test indices	split iter indices	0.250000
restricted to the binary classification	y_true y_score pos_label	0.066667
coefficient of determination regression score function	metrics r2 score y_true	0.125000
compute the polynomial	polynomial	0.111111
is not found and raise an exception	line search	0.029412
initialize the model parameters	mixture base mixture initialize parameters x random_state	1.000000
for the case method='lasso' is	y xy gram	0.090909
callback	callback	0.625000
a transform function to portion of selected features	transform selected x transform selected	0.333333
kernel is stationary	stationary kernel mixin is stationary	0.333333
number of splitting iterations in the cross-validator parameters	model_selection predefined split get n splits	0.111111
hash	joblib memorized	0.015625
to build a batch of estimators within	build estimators n_estimators ensemble x	0.166667
generate cross-validated	cv	0.009009
with the generative	base	0.014286
detecting outliers in a gaussian distributed dataset	elliptic envelope	0.166667
raise_warning	raise_warning	1.000000
by scaling each feature to	preprocessing minmax scale	0.142857
to avoid the hash depending from	externals joblib memory	0.016949
kernel ridge regression	kernel ridge	0.500000
of dataset and	y	0.002674
gradient	gradient w x y epsilon	0.500000
estimates the autocorrelation parameters theta as the maximizer	gaussian process arg max	0.047619
convert coefficient matrix to sparse format	linear_model sparse coef mixin sparsify	1.000000
lfw pairs dataset this dataset is a	fetch lfw pairs	0.018868
single tree	trees tree forest	0.142857
kddcup99	datasets fetch brute kddcup99	0.166667
logic estimators that implement the partial_fit api	utils check partial fit	0.038462
leaves of the cf node	cluster birch get leaves	0.333333
load datasets in the svmlight / libsvm	datasets load svmlight file f n_features	0.500000
negative value in an	negative x	0.200000
columns of x	x	0.001692
of feature name -> indices mappings	feature_extraction dict vectorizer fit	0.250000
the precisions	precisions x z	0.250000
to	joblib	0.058394
'l' suffix when using	utils	0.009709
l1-penalized covariance estimator read	covariance graph lasso	0.166667
explained variance regression score function best	metrics explained variance	0.166667
compute joint probabilities p_ij from distances	joint probabilities distances desired_perplexity verbose	1.000000
to the binary classification	y_true y_score average	0.076923
sparse components	decomposition sparse pca transform x	0.500000
log of probability estimates	log proba	0.181818
v_pos	v_pos	1.000000
range approximates the range	utils randomized range finder	0.083333
handle the callable case	pairwise callable x y metric	0.083333
estimates the shrunk ledoit-wolf covariance	covariance ledoit wolf x assume_centered block_size	0.125000
pgroups	pgroups	1.000000
to be persisted instead	array wrapper	0.166667
a large sparse linear system of equations	utils lsqr a b damp atol	0.500000
values for x	estimator x	0.030303
to the binary classification task	y_true y_score average sample_weight	0.076923
scale back the data to the original representation	preprocessing robust scaler inverse transform x	0.066667
for x relative to y_true	metrics predict scorer call estimator x y_true sample_weight	0.200000
huber loss and the gradient	huber loss and gradient w x y epsilon	0.333333
introduces an 'l' suffix when	utils	0.009709
the gradient	gradient w x y	0.500000
mean squared error between two covariance	covariance error norm comp_cov norm scaling squared	0.250000
filters the	args func ignore_lst	0.500000
linear regression with combined l1 and l2	elastic net	0.111111
shrunk ledoit-wolf covariance	covariance ledoit wolf x	0.125000
variance regression score	variance	0.058824
of array-like or scipy	binarize x threshold copy	0.083333
data	val predict estimator	0.045455
gaussian and	make gaussian	0.125000
mean absolute error regression loss read more	mean absolute error	0.166667
under the curve auc from prediction scores	auc	0.020408
the scaler	preprocessing standard scaler	0.333333
compute the mean and	x y	0.002155
any axis center to the median and component	x axis	0.015385
continuous and discrete	cd c d n_neighbors	1.000000
the training set x and returns the labels	predict x y	0.043478
called with the given arguments	func	0.011364
name	name	0.200000
data	base pca	0.071429
spherical wishart distribution	wishart spherical nk xk sk	0.333333
run	grid	0.120000
point	core cross val predict estimator	0.045455
indices	iter indices	0.250000
predicted probabilities for a calibration curve	core calibration curve y_true y_prob normalize	0.142857
trained model	multilayer perceptron	0.071429
a	externals joblib binary	0.200000
avoid the	func	0.011364
perform standardization by centering and	preprocessing standard scaler transform x y copy	0.333333
of the decision functions	decision function x	0.018868
unfitted	unfitted	0.500000
compute the l1 distances between the vectors	metrics manhattan distances	0.083333
generate cross-validated estimates for each input data point	cross val predict estimator x y cv	0.071429
fits the oracle approximating shrinkage covariance model	covariance oas	0.083333
x relative to	metrics threshold scorer call clf x y	0.058824
the l1 distances between the vectors in x	paired manhattan distances x	0.500000
of array-like or scipy sparse	binarize x threshold copy	0.083333
returns the score on the given data	score x	0.033333
lasso using the lars algorithm	lars	0.090909
call with	call	0.052632
multi-class targets using underlying	core output code	0.250000
hash	externals joblib memory	0.016949
estimate model parameters with the em	mixture base mixture fit	0.200000
matrix	matrix labels_true labels_pred eps	1.000000
net path with coordinate descent the	linear_model enet path x	0.050000
element in the	in	0.090909
to binary labels the output of transform is	transform	0.011236
element of numpy array or sparse	element	0.083333
with	get	0.012048
there to implement the usual api and	y	0.008021
the number of splitting iterations in the	split get n splits x y groups	0.111111
for the lfw	lfw	0.068966
within a given radius of a	x radius	0.058824
sample weights	sample	0.032258
an estimator within	estimator estimator x y sample_weight	0.333333
ledoit-wolf	ledoit wolf x assume_centered block_size	0.250000
input	predict estimator	0.045455
a full lars path parameters	omp path	0.100000
wise scale	scale x	0.086957
according to the given	x y sample_weight	0.025974
score of	score	0.030303
factorization nmf find two non-negative	non negative factorization	0.043478
the k-neighbors	kneighbors mixin kneighbors x	0.125000
an adaboost	ada boost	1.000000
generate	core base	0.083333
update terminal regions	update terminal regions tree x	0.500000
the residual (= negative gradient)	ensemble binomial deviance negative gradient y	0.333333
area under the curve auc from prediction	auc score	0.052632
the index of the leaf	base decision tree apply	0.166667
implementation is restricted to the binary classification task	y_true	0.021739
for the california housing	california housing	0.083333
exception types	joblib parallel backend base get	0.066667
a fit across one fold	feature_selection rfe single fit rfe	0.200000
model is guaranteed not to be empty	svm l1 min	0.333333
windows cannot encode some characters in filename	clean win chars string	0.333333
fit the hierarchical clustering on the	cluster agglomerative clustering fit x y	0.250000
a cv in a	cv cv	0.031250
compute elastic net path with coordinate descent the	path	0.025641
high	high	1.000000
number of splitting iterations in	model_selection base kfold get n splits x y	0.111111
ridge	ridge	0.428571
implement a single boost	boost iboost x y sample_weight	1.000000
maximum absolute value to be	max abs	0.047619
log-det of the cholesky decomposition of matrices	det cholesky matrix_chol covariance_type n_features	1.000000
fit the rfe model	fit	0.003257
returns whether the kernel is stationary	gaussian_process kernel is stationary	1.000000
with the best found parameters	core base search cv predict proba x	0.076923
if fileobj	fileobj	0.111111
maximizer	arg max	0.047619
for building a cv in	core check cv cv	0.031250
for diagonal	diag	0.031250
is probably	core is	1.000000
find the first prime element in	utils hungarian state find prime in	0.333333
path with	enet path x	0.050000
a classification	y_true y_pred labels	0.125000
function and its corresponding derivatives with respect to	activations deltas	0.032258
strip the headers by removing everything	strip	0.055556
ledoit-wolf covariance matrix	covariance ledoit wolf x assume_centered	0.125000
dictionary learning finds a dictionary a set	mini batch dictionary learning	0.142857
for a fit	fit rfe	0.166667
name for the	func name	0.047619
returns the number of splitting iterations in	model_selection predefined split get n splits	0.111111
the covariance matrices from a given template	matrix to match covariance type tied_cv covariance_type	0.333333
cache folders to	memory	0.015625
model by computing truncated	truncated	0.100000
r^2 coefficient of determination regression score	r2 score y_true y_pred sample_weight multioutput	0.125000
global clustering for the subclusters obtained	cluster birch global clustering x	0.142857
compute elastic net path	enet path	0.050000
the function called with the given arguments	memorized func get output	0.125000
of an unsuccessful bst search since	n_samples_leaf	0.111111
call predict on	predict x	0.011765
list of exception types to be captured	joblib parallel backend base get exceptions	0.166667
compute the weighted log probabilities for each sample	base mixture score samples x	1.000000
image from all of its patches	from patches 2d patches image_size	0.333333
the one-vs-one multi class libsvm in the case	svm one vs one	0.050000
true and false positives per binary classification	binary clf curve	0.090909
images	images	0.666667
the right fileobject from	read fileobject	0.100000
returns the huber loss and the gradient	huber loss and gradient w x	0.333333
model with stochastic gradient descent	y coef_init intercept_init	0.333333
the number of splitting iterations in the cross-validator	one out get n splits	0.111111
fit	svr fit	0.333333
online learning prevents rebuilding of cftree from scratch	cluster birch partial fit x y	1.000000
generate cross-validated estimates	estimator x y cv	0.050000
computes multidimensional scaling using smacof	manifold smacof single dissimilarities metric n_components init	0.333333
functions of	function x	0.030303
the process or thread pool	multiprocessing backend	0.038462
make a scorer from a	metrics make scorer	1.000000
type introduces an 'l'	repr	0.012500
ledoit-wolf covariance	covariance ledoit wolf	0.125000
the given parameter	parameter estimator parameter	0.500000
of x and dot w h	divergence x w h	0.500000
which is equal to the average	average	0.066667
build a text report showing the main classification	classification report	0.250000
parallel processing this method is meant	parallel	0.019231
the given arguments	func get output	0.125000
probability estimates	proba	0.058824
we don't	memorized func	0.016949
threshold value	estimator importances threshold	0.500000
a byte string	externals joblib binary zlib	0.125000
fit ridge regression	ridge cv fit x y	1.000000
the reduced likelihood function	reduced likelihood function	0.083333
number of splitting iterations in	kfold get n splits x y groups	0.111111
returns the number of splitting iterations in the	cviterable wrapper get n splits	0.111111
matrix	n_components	0.083333
svmlight / libsvm format	svmlight file f n_features dtype	0.066667
make	ensemble make estimator	1.000000
exception types to	get	0.012048
wise scale to unit variance	preprocessing scale x	0.090909
multinomial loss	multinomial loss w x	1.000000
break the pairwise matrix in n_jobs	metrics parallel pairwise x y func n_jobs	0.111111
x and returns the labels	x y	0.002155
model to the data x which	x y	0.002155
of the dual gap convergence criterion the specific	dual gap emp_cov precision_ alpha	0.071429
return a platform	utils shape repr	0.013699
number of splitting iterations in the cross-validator	model_selection base cross validator get n splits	0.125000
fit ridge regression model parameters	linear_model ridge gcv fit x y sample_weight	1.000000
the posterior log probability of the samples	core bernoulli nb joint log likelihood	0.083333
perform classification on	neighbors nearest centroid predict	0.142857
x and returns the	x y w	0.500000
function cutting	cluster hc cut n_clusters children n_leaves	1.000000
projection the	projection	0.071429
using matrix product with the random matrix parameters	core base random projection	0.200000
returns the score	score x	0.066667
get the parameters of the votingclassifier	ensemble voting classifier get params deep	1.000000
in bytes_limit	externals joblib	0.004762
object to be persisted instead	array wrapper	0.166667
contingency matrix describing the relationship between labels	metrics cluster contingency matrix labels_true	0.200000
fit a binary classifier	linear_model base sgdclassifier fit binary	0.333333
for a full lars path parameters	path	0.025641
type introduces	shape	0.011765
row returns	row row	0.166667
covariance matrix	cov x	0.500000
number of splitting iterations in the cross-validator	base kfold get n splits x y	0.111111
the hash depending	joblib memorized	0.015625
with	decomposition base pca	0.071429
theilsenregressor class	linear_model lstsq x y indices fit_intercept	1.000000
the generative	base	0.014286
make cache size fit in bytes_limit	size	0.032258
each input	cross	0.037037
such that for c in (l1_min_c	c x y	0.030303
fit the	manifold tsne fit	0.333333
a list of feature	dict vectorizer get feature	0.200000
input data	y	0.002674
hyperparameters	hyperparameters	1.000000
mean and	x y	0.002155
sparse matrix	sparse	0.025000
opposite of the local outlier factor	local outlier factor	0.125000
x and y is	x y	0.002155
lfw people dataset this operation	datasets fetch lfw people	0.040000
the descriptors of a memmap instance to reopen	externals joblib reduce memmap a	0.050000
finds seeds	cluster get bin seeds x	0.250000
this classification dataset is constructed by taking a	datasets	0.015152
vectors rows of u such that	u	0.032258
the mean and component wise scale to unit	preprocessing scale x	0.090909
transform function to portion of selected features parameters	transform selected copy	0.333333
'l' suffix when using	repr	0.012500
evaluates the reduced	reduced	0.062500
func to	joblib pool manager mixin apply async func	0.250000
partially fit a single binary estimator	core partial fit binary estimator x y	1.000000
check the estimator and set the base_estimator_ attribute	ensemble bagging classifier validate estimator	0.333333
set the diagonal	set diag	0.333333
return number of samples	samples	0.052632
of the graph-lasso objective function the objective	covariance objective	0.125000
process or thread	backend	0.016949
cache folders	externals joblib memory	0.016949
generate a	n_samples	0.058824
minimum covariance determinant with the fastmcd algorithm	covariance min cov det fit	0.500000
curve auc using	metrics auc x y	0.040000
from the training	fit	0.019544
callable case for pairwise_{distances kernels}	metrics pairwise callable x y metric	0.083333
kernel k x y and optionally its gradient	matern call x y eval_gradient	0.333333
the gradient and	y	0.005348
the boolean mask x	preprocessing get mask x	0.333333
the neighbors within a	radius neighbors x	0.166667
bagging regressor	bagging regressor	1.000000
to make cache size fit in	size	0.032258
find the first prime element in the specified	hungarian state find prime in	0.333333
can also predict based on an unfitted	predict	0.006849
back the data to the	scaler inverse transform x copy	0.066667
lfw pairs dataset this dataset is a	fetch lfw pairs subset	0.035714
fit	base sgdclassifier fit	0.230769
preprocess	preprocess	1.000000
and component wise scale to unit variance	preprocessing scale x	0.090909
on the estimator with randomly drawn	randomized search cv	0.166667
c	c x y loss	0.030303
for x relative to y_true	predict scorer call estimator x y_true sample_weight	0.200000
fp where tp is the	score y_true y_pred labels pos_label	0.027778
transforms and predict_log_proba of	predict log proba x	0.045455
fit the hierarchical clustering on the data	cluster agglomerative clustering fit x	0.250000
estimator's fit method supports	utils has fit	0.500000
used to fit a single tree	trees tree forest x y	0.142857
number of splitting iterations in the	kfold get n splits x y	0.111111
timestamp when pickling to avoid	func reduce	0.050000
scaler	preprocessing max abs scaler	0.333333
of the local outlier factor of	local outlier factor decision	0.125000
returns whether the kernel	gaussian_process dot product	0.333333
handle the callable case for pairwise_{distances	metrics pairwise callable x y metric	0.083333
directory in which are persisted the	get output dir	0.047619
fit	output estimator fit	0.200000
fit the model according to the given training	svm linear svc fit x y sample_weight	0.250000
coefficient of determination regression score	metrics r2 score y_true y_pred	0.125000
pursuit omp solves n_targets orthogonal matching pursuit	linear_model orthogonal mp x y n_nonzero_coefs	0.200000
is equal to the average	average	0.066667
score	score y_true	0.117647
return a tolerance which	tolerance x	0.058824
true and predicted probabilities for a calibration curve	calibration curve y_true y_prob normalize n_bins	0.142857
note this implementation is restricted to	roc	0.033333
given arguments	externals joblib memorized func get output	0.125000
compute k-means clustering	cluster kmeans fit x y	1.000000
func	manager mixin apply async func	0.250000
the least-squares solution to a	lsqr a	0.037037
to line_search_wolfe2	wolfe12 f fprime xk pk	0.028571
fn	fn	1.000000
full covariance matrices	multivariate normal density full x	0.166667
cross-validated	x y cv	0.100000
the least-squares solution to a	utils lsqr a	0.037037
feature name -> indices mappings	dict vectorizer fit x y	0.250000
diagonal structure for biclustering	biclusters shape n_clusters noise minval	0.058824
best found parameters	base search cv predict proba x	0.076923
get the	mixin get	0.500000
the hash	externals joblib memorized func	0.013158
indices in sorted array of integers	indices tree bin_x left_mask right_mask	0.166667
of x from y along the	x z reg	0.066667
a projection to the normalized laplacian	spectral	0.026316
model with passive	passive	0.117647
reduced likelihood function for the given autocorrelation parameters	reduced likelihood function	0.041667
specified layer	layer	0.090909
partially fit underlying estimators should be used when	one vs one classifier partial fit	0.166667
to avoid the hash	externals joblib memorized	0.013699
we don't store the timestamp when pickling to	memorized func reduce	0.050000
cf tree for the input	cluster birch fit	0.200000
abc for libsvm-based classifiers	base svc	0.500000
dimensionality reduction using truncated svd aka lsa	truncated svd	0.500000
the voting classifier valid parameter	ensemble voting classifier set params	0.037037
sample weight array	linear_model base sgd validate sample weight	0.333333
theilsenregressor class	linear_model lstsq x y indices	1.000000
with sparse uncorrelated design	sparse uncorrelated n_samples n_features random_state	0.166667
elastic	elastic	1.000000
from file-like object until size bytes are read	joblib read bytes fp size error_template	0.500000
legacy gaussian process model class	gaussian process	0.083333
be explained by a bell-shaped curve of width	effective_rank tail_strength	0.125000
net path with coordinate descent	enet path x	0.050000
not found and raise an	utils line search	0.029412
target values for x relative	metrics predict scorer call estimator x	0.166667
the free energy f v = - log	free energy	0.066667
classifier valid parameter	classifier set params	0.125000
omp solves n_targets orthogonal matching pursuit	linear_model orthogonal mp x y n_nonzero_coefs	0.200000
one-vs-one	one vs one	0.050000
n_nonzero_coefs	n_nonzero_coefs	0.454545
median absolute error regression loss	metrics median absolute error y_true	0.166667
number of splitting iterations in the cross-validator parameters	group out get n splits x y	0.111111
given column class distributions parameters	classes class_probability random_state	0.166667
fit the ardregression model according to	linear_model ardregression fit x	0.250000
performs clustering on x and returns cluster	cluster dbscan fit predict x y sample_weight	0.166667
compute the mlp loss function and its corresponding	neural_network base multilayer perceptron	0.083333
turn a transformed real-valued array into a hash	projection to hash mixin	0.333333
terminal regions to median	terminal region tree terminal_regions leaf	0.066667
the curve auc using	metrics auc x y	0.040000
covariance matrices from a given template	covariance type tied_cv covariance_type n_components	0.333333
sparse inverse covariance w/ cross-validated choice of the	cv	0.009009
sample from the	sample	0.064516
the meta-information	joblib zndarray wrapper read unpickler	0.043478
samples in x into	x	0.001692
a covariance matrix	covariance	0.014493
returns the transformed data	w h	0.031250
one after the other and transforms the	y	0.002674
sure centering is not enabled for sparse	robust scaler check array x	0.250000
a list of feature name -> indices mappings	feature_extraction dict vectorizer	0.200000
the average path length	ensemble average path length	0.090909
terminal regions to median	terminal region	0.100000
terminal regions	terminal region tree terminal_regions	0.100000
[1] and breiman [2]	friedman3 n_samples noise	0.166667
opposite of the local outlier factor of	neighbors local outlier factor decision function	0.125000
score by cross-validation read more in the	model_selection cross val score	0.166667
check the test_size and train_size at init	shuffle split init test_size train_size	0.250000
fit the model with x	fit x y	0.011976
back the data to the original representation	preprocessing standard scaler inverse transform	0.066667
priors from multioutput-multiclass target	distribution	0.166667
estimate the precisions	gaussian mixture estimate precisions nk	0.166667
process regression gpr	process regressor	0.166667
binarization transformation for multiclass	binarize multiclass y classes	1.000000
batch of estimators within a	estimators n_estimators ensemble x	0.083333
file supports seeking	zlib file seekable	0.250000
theta as the maximizer of the reduced	gaussian_process gaussian process arg max reduced	0.200000
the	utils shape	0.027397
jaccard similarity coefficient score the	score	0.010101
thread pool	externals joblib	0.004762
normalize x according to kluger's log-interactions	normalize x	0.076923
a spherical model	spherical	0.090909
the optimal	joblib auto batching mixin compute	0.333333
the curve auc	auc score	0.052632
a	datasets make	0.015625
with sparse uncorrelated design this dataset is described	datasets make sparse uncorrelated n_samples n_features random_state	0.166667
and evaluates the reduced likelihood function for	reduced likelihood function	0.041667
subsets incrementally	incremental	0.166667
get number	get n	0.500000
score for a fit across one fold	feature_selection rfe single fit	0.200000
returns the submatrix corresponding to bicluster	bicluster mixin get submatrix	0.333333
long type introduces an 'l' suffix when using	utils shape	0.013699
and compute prediction of init	ensemble base gradient boosting init	0.142857
folders to	externals joblib memory reduce	0.030303
update the bound	mixture bound state log lik x	0.500000
first and last element	first and last element arr	0.200000
fit the model to the data x	neural_network bernoulli rbm fit x y	1.000000
number of splitting iterations in the cross-validator	model_selection leave one group out get n splits	0.111111
the callable case for	pairwise callable x y	0.083333
scale back the data to the	inverse transform x copy	0.066667
extracts patches of any	extract patches	0.083333
coefficient matrix	mixin	0.037037
random sample from a	a size	0.142857
for full covariance	full x means covars	0.166667
determine absolute sizes of training subsets	core translate train sizes	0.066667
computes the free energy f v	rbm free energy	0.066667
the labeled faces in the wild lfw	lfw	0.034483
extract the first	externals joblib extract first	1.000000
n_clusters	n_clusters	1.000000
a memmap instance to reopen on	reduce memmap a	0.050000
absolute error	error	0.020000
theta as the maximizer of	process arg max	0.047619
for	classifier	0.068493
generate train test	split	0.027778
hessian in the case of a multinomial loss	linear_model multinomial grad hess	1.000000
is inefficient	y classes	0.027778
l1 distances between the vectors in x	metrics paired manhattan distances x	0.500000
factorization nmf find two non-negative matrices	factorization x	0.043478
mixin class for all bicluster estimators in scikit-learn	bicluster mixin	0.333333
the curve auc using the trapezoidal rule	metrics auc x	0.040000
area under the curve auc using the	auc	0.020408
is	mixin is	1.000000
fit linear model with stochastic gradient descent	base sgdregressor partial fit	1.000000
perform dbscan clustering	cluster dbscan fit x	1.000000
encode the data as a sparse	decomposition sparse	0.111111
at x	x	0.001692
barycenter weighted	barycenter	0.090909
right fileobject from	externals joblib read fileobject	0.100000
apply the derivative of the logistic sigmoid	logistic derivative z delta	0.166667
the	externals joblib	0.023810
operation is meant to be cached by	index_file_path data_folder_path slice_ color	0.033333
the hash depending from	joblib memory	0.016949
lower bound on	lower bound	0.071429
project data to maximize class separation	core linear discriminant analysis transform x	0.250000
actual fitting performing	cv fit x y parameter_iterable	0.500000
and transform with	transform	0.011236
as the maximizer of the reduced likelihood	gaussian process arg max reduced likelihood	0.250000
the reduced likelihood function for the	gaussian_process gaussian process reduced likelihood function	0.047619
don't	joblib memorized	0.015625
constant block diagonal structure for biclustering	biclusters shape n_clusters noise minval	0.058824
the training set x	x	0.001692
nicely formatted statement displaying the function call	call func args kwargs object_name	0.333333
the usual api and	fit x y	0.017964
read value from cache and return	externals joblib memorized result	0.500000
type introduces an 'l' suffix	shape	0.011765
neighbors for points	radius neighbors	0.086957
the vectors rows of u such	flip u	0.047619
of neighbors for points	neighbors radius neighbors mixin radius neighbors	0.125000
a locally linear embedding analysis	manifold locally linear embedding x n_neighbors n_components reg	0.071429
data home cache	datasets clear data home data_home	0.076923
the weighted log probabilities for	mixture base mixture	0.111111
posterior log probability	core multinomial nb joint log likelihood	0.083333
features	features x	1.000000
wolfe12	wolfe12	1.000000
elastic net path with coordinate descent the	linear_model enet path	0.050000
the estimator with the best found	model_selection base search cv	0.040000
returns the number of splitting iterations in the	out get n splits x	0.111111
update h	w h	0.031250
distances	distances	0.350000
api and hence	decomposition sparse coder fit x y	0.142857
score the	score y_true	0.058824
tolerance	tolerance	0.227273
single binary estimator one-vs-one	ovo binary estimator x y i	0.500000
predict using the trained model parameters	multilayer perceptron predict x	0.333333
fp where tp is	score y_true y_pred labels pos_label	0.027778
also predict based	predict	0.006849
least squares solver	core linear discriminant analysis solve lsqr x	1.000000
write_func	write_func	1.000000
adjusted for chance	adjusted	0.125000
of alpha	linear_model alpha	0.500000
"returns	multi output classifier score	0.250000
base class for label	base label	1.000000
non zero row of x	x	0.001692
a class with a metaclass	externals add metaclass metaclass	0.166667
number of splitting iterations in the cross-validator	leave one out get n splits	0.111111
'l'	shape	0.011765
a sparse combination of the dictionary atoms	sparse coding mixin transform	0.333333
fit	estimator fit	0.200000
covariance model to	covariance	0.014493
c in	c	0.022222
cache folders	joblib memory reduce	0.030303
locally linear embedding analysis on the	locally linear embedding x n_neighbors	0.071429
last step in pipeline after transforms	pipeline fit predict	0.166667
remove	externals joblib	0.004762
evaluates the reduced	gaussian_process gaussian process reduced	0.125000
showing the main classification	classification	0.071429
to a	externals joblib	0.004762
for the voting classifier valid parameter	ensemble voting classifier	0.031250
weighted graph of neighbors for points in x	neighbors radius neighbors mixin radius neighbors graph x	0.500000
class with a metaclass	externals with metaclass	1.000000
quantile regression	quantile	1.000000
the coefficient of determination r^2	multi output regressor score	0.200000
mask	edges weights mask	0.333333
the model to the data	y	0.002674
x and returns the	fit transform x y w	0.500000
the position of the points	mds fit x	0.066667
log	regression predict log	0.500000
outlier factor of x (as	outlier factor decision function x	0.200000
minimum covariance determinant with the fastmcd algorithm	covariance min cov det fit x	0.500000
the estimator with the best found	core base search cv predict proba	0.076923
error	log error	0.142857
scale back the data	scaler inverse transform x copy	0.066667
the voting classifier valid parameter keys can	voting classifier set params	0.037037
ensemble classes	ensemble	0.100000
workers requested by the callers	backend base effective	0.250000
log-det of the cholesky decomposition of	log det cholesky matrix_chol	0.500000
perform a locally linear embedding analysis on the	manifold locally linear embedding	0.062500
coefs and intercept for specified layer	grad layer n_samples	0.166667
the ardregression model according to the given training	linear_model ardregression	0.100000
a fit across one fold	feature_selection rfe single fit rfe estimator	0.200000
the estimator with the best found parameters	search cv	0.036364
passive aggressive algorithm	passive aggressive regressor	0.125000
a single sample image parameters	sample image image_name	0.166667
curve auc using the trapezoidal rule this	metrics auc	0.040000
a cv	cv cv x	0.031250
messages while keeping track of time	time	0.047619
log sum_h exp(-e v h	v	0.052632
contingency matrix describing the relationship between labels	metrics cluster contingency matrix	0.200000
n_components	base spectral svd array n_components	1.000000
given arguments	func	0.022727
exp	exp	1.000000
fit the model by computing truncated svd	pca fit truncated x n_components	1.000000
from prediction scores note	roc	0.033333
input data point	val predict estimator x y	0.045455
make and configure a copy of the	ensemble make estimator append random_state	0.166667
random classifier	core dummy classifier	1.000000
sub_sampling	sub_sampling	1.000000
along an	axis x axis	0.083333
param logic estimators that implement the partial_fit	utils check partial	0.038462
scale back the data	preprocessing robust scaler inverse transform	0.066667
recall the recall is	metrics recall	0.033333
pool	joblib multiprocessing backend	0.052632
fit all transformers transform the data and concatenate	core feature union fit transform x y	0.500000
test	core	0.092308
jobs that can actually	jobs	0.111111
opening the right fileobject from a filename	joblib read fileobject fileobj filename mmap_mode	0.250000
function for the given autocorrelation parameters theta	function theta	1.000000
compute the median	get median	0.166667
cv in	core check cv cv x y	0.031250
generate cross-validated	x y cv	0.050000
when memory is inefficient to train	classes	0.025641
batch of estimators within a job	estimators n_estimators ensemble x y	0.083333
to the cache for the function	joblib memorized	0.015625
svmlight / libsvm format into sparse csr	svmlight file f n_features	0.066667
by a random projection p only changes	johnson lindenstrauss min dim n_samples	0.142857
using	linear_model base randomized	1.000000
restricted to the binary classification	score y_true y_score	0.025000
and linearsvc	fit liblinear x y c fit_intercept	0.142857
for the given param_grid	grid search cv get param iterator	0.166667
by computing truncated	truncated	0.100000
w in multiplicative update	decomposition multiplicative update w x w	0.500000
evaluate the density model on	kernel density score samples x	0.250000
the content of the data home cache	data home	0.076923
rngs	core setup module module	1.000000
get number of jobs for	get n jobs n_jobs	0.250000
back the data to the original representation parameters	preprocessing robust scaler inverse transform	0.066667
a large	lsqr a	0.037037
the score	model_selection score	0.166667
return a	utils shape	0.013699
kernel k	gaussian_process sum	1.000000
posterior log probability of the	multinomial nb joint log likelihood	0.083333
in [rouseeuw1984]_ aiming at computing	n_support remaining_iterations initial_estimates	0.111111
from a	joblib read	0.333333
true and false positives per binary	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
make sure no negative value in x	non neg array x whom	0.500000
number of splitting iterations in the cross-validator parameters	model_selection predefined split get n splits x y	0.111111
a callable that handles preprocessing and tokenization	vectorizer mixin build analyzer	0.333333
number of splitting iterations in the cross-validator parameters	split get n splits x y	0.111111
on the estimator with the best found parameters	core base search cv predict proba	0.076923
for the one-vs-one	one vs one	0.050000
list	parallel	0.019231
a	repr	0.012500
w to minimize the	x w	0.083333
search over parameters	core base search	0.111111
with the	base pca get	0.076923
fit a multi-class classifier	sgdclassifier fit	0.076923
function cache result	externals joblib memorized func	0.013158
class for all meta estimators in scikit-learn	meta estimator	0.062500
passive aggressive regressor read more in the	passive aggressive regressor	0.125000
with sparse uncorrelated design this dataset is	datasets make sparse uncorrelated n_samples n_features	0.166667
the logistic loss	logistic loss w x y	0.500000
the timestamp when pickling to avoid the hash	externals joblib memorized func reduce	0.050000
center	preprocessing robust scaler transform x	1.000000
derivatives with respect to each parameter weights and	x y activations deltas	0.500000
filters the	filter args func ignore_lst	0.500000
estimates for each	val predict	0.045455
single	single	1.000000
axis center to the mean	axis	0.014085
filename	filename mmap_mode	0.500000
anova f-value for the	feature_selection f classif x	0.200000
number of splitting iterations in the cross-validator	out get n splits x y groups	0.111111
model	linear_model	0.128205
logistic loss	logistic loss w x	0.500000
python object into	joblib dump value filename	0.083333
approximates the range	range finder	0.083333
timestamp when pickling to avoid the hash depending	externals joblib memory reduce	0.030303
transform array or sparse matrix x back to	inverse transform x dict_type	1.000000
in the wild lfw pairs dataset	datasets fetch lfw pairs	0.018868
for memmap backed arrays	memmap backed a m	0.333333
absolute value to be used	abs	0.083333
to build a batch of estimators within	parallel build estimators n_estimators	0.166667
covariance m step for full cases	covar mstep full gmm x responsibilities weighted_x_sum	1.000000
paired distances between x and y	paired distances x y	0.500000
batch of estimators within	estimators n_estimators	0.083333
estimators that implement the partial_fit	utils check partial fit	0.038462
log of probability estimates	linear_model logistic regression predict log proba x	1.000000
number of splitting iterations in the	predefined split get n splits x y groups	0.111111
after the other and transforms	x y	0.002155
elastic net parameter search parameters	x y xy l1_ratio	0.250000
returns the number of splitting iterations in	base cross validator get n splits x y	0.125000
find the least-squares solution to a large	utils lsqr a	0.037037
from source to all reachable nodes	source	0.100000
a subcluster	subcluster	0.090909
sample from the distribution p(v|h)	sample visibles h rng	1.000000
all the vectors rows of u such	flip u	0.047619
used for later	fit x y	0.005988
median absolute error regression loss read more in	metrics median absolute error y_true	0.166667
ndarray with aligned memory	aligned	0.076923
samples from	neighbors kernel density sample	1.000000
scale back the data to	inverse transform x	0.051282
similarity coefficient score the	score y_true y_pred	0.038462
estimate model parameters with the em	mixture gmmbase fit x y do_prediction	0.250000
number of splitting iterations in	kfold get n splits x	0.111111
list of exception types to be captured	exceptions	0.083333
the raw documents	feature_extraction count vectorizer fit raw_documents y	0.125000
the directory in	output dir	0.047619
descriptors of a memmap instance to reopen	externals joblib reduce memmap a	0.050000
and compute scores	and score	0.333333
the generative	decomposition	0.047619
run fit on one set	core fit grid point	0.500000
with respect	activations deltas	0.064516
best found	base search cv	0.052632
compute class covariance	class cov x y priors shrinkage	0.250000
for c such that for c in	c x y loss fit_intercept	0.030303
perform one gibbs sampling step	neural_network bernoulli rbm gibbs v	1.000000
fit all transformers transform	core feature union fit transform x	0.333333
utility for building a cv in a	core check cv cv x	0.031250
binary gaussian process classification based	binary gaussian process	0.333333
graph of neighbors for points	mixin radius neighbors graph	0.066667
returns the number of splitting iterations in the	base cross validator get n splits x y	0.125000
median absolute error regression loss read more	metrics median absolute error y_true	0.166667
reduced likelihood function for the given	gaussian process reduced likelihood function	0.047619
the leaf	base decision tree apply x	0.166667
predict regression target at each stage	ensemble gradient boosting regressor staged predict	0.500000
run fit on one set of	model_selection fit grid point x y estimator	0.500000
path with	linear_model enet path x	0.050000
too common features	count vectorizer limit features x vocabulary high	1.000000
matrix	coef mixin	0.090909
edges for a	edges	0.047619
the weighted graph of k-neighbors for	neighbors kneighbors mixin kneighbors graph	0.250000
a list of module names and a name	get func name	0.047619
text	text	0.555556
perform classification on an	nearest centroid predict	0.142857
a cv in a user	check cv cv x	0.031250
based on a feature matrix	x connectivity n_clusters	0.250000
estimator with the best found parameters	core base search cv predict	0.076923
intercept_	intercept x_offset y_offset x_scale	0.500000
fit	multi output estimator fit	0.200000
generate	predict	0.006849
median absolute error regression	metrics median absolute error y_true y_pred	0.166667
projection the components of	projection	0.071429
kernel k x y and	rbf call x y	0.200000
run fit on the estimator with	fit x y	0.005988
kernel ridge model parameters	core kernel ridge	0.500000
for memmap	memmap	0.066667
the absolute error of the kl divergence	kl divergence error	0.100000
other and	y	0.002674
determinant with	det fit	0.333333
relative	scorer call estimator	1.000000
the reduced likelihood function for the given	reduced likelihood function	0.041667
of the mixture parameters	bayesian gaussian mixture	0.333333
load dataset from multiple files in svmlight format	datasets load svmlight files files	0.500000
for each	core cross	0.045455
for building a cv in a user friendly	core check cv cv x y classifier	0.031250
the gaussian process model fitting	gaussian_process gaussian process fit x	0.250000
is described by its spectrum spectrum	spectrum	0.090909
download the 20 newsgroups data and	datasets download 20newsgroups	0.200000
terminal_regions	terminal_regions	1.000000
the binary classification	y_true y_score	0.054054
linear support vector regression	linear svr	1.000000
the diagonal of the laplacian	diag laplacian	0.111111
terminal	function update terminal	0.200000
text files with	files	0.100000
number of splitting iterations in the cross-validator	base cross validator get n splits x	0.125000
voting classifier valid parameter keys can be	voting classifier set	0.037037
seeds for	get bin seeds x	0.250000
the actual data loading for the lfw pairs	lfw pairs	0.018868
local outlier factor of	neighbors local outlier factor	0.125000
the provided	n_components	0.083333
center to the median and component wise scale	preprocessing robust scale	0.125000
rand index adjusted	adjusted rand score labels_true labels_pred	0.333333
distance of each sample from the decision	decision	0.027778
absolute sizes of	train sizes	0.066667
file-like object until size bytes are read	externals joblib read bytes fp size error_template	0.500000
return the query based on include_self param	neighbors query include self x include_self	0.333333
from all	from	0.045455
number of splitting iterations in the cross-validator	cviterable wrapper get n splits x	0.111111
note this	roc	0.033333
normalize rows and columns of x simultaneously so	cluster bistochastic normalize x max_iter tol	1.000000
check validity	check params	0.200000
of the leaf	decision tree apply x	0.166667
in the wild lfw pairs	lfw pairs subset	0.035714
we	externals joblib memorized func	0.013158
is equal to the average path length of	ensemble average path length	0.090909
the posterior log probability of	nb joint log likelihood	0.066667
read more in the :ref user guide <univariate_feature_selection>	fwe	0.200000
computes the gradient and the	x y alpha	0.181818
backend and return the number of workers	externals joblib parallel backend	0.029412
to build a batch of estimators within	parallel build estimators	0.166667
classification	y_true y_score	0.054054
to avoid the hash depending from	externals joblib	0.009524
to the training set x and returns	predict x y	0.043478
uncompressed bytes from the file	binary zlib file	0.062500
gaussian distribution	mixture bayesian gaussian	1.000000
independent	shape repr	0.013699
compute the unnormalized posterior log probability of	base nb joint log likelihood	0.166667
logistic regression model	linear_model logistic regression path x y	0.333333
inverse covariance w/ cross-validated choice of	cv	0.009009
back the data to the	standard scaler inverse transform x	0.066667
random permutation cross-validation iterator	shuffle split	0.142857
suffix when using	utils	0.009709
partially fit a single binary estimator	partial fit binary estimator	1.000000
while keeping track of time	time	0.047619
loss	loss w x y	0.250000
x	fit transform x	0.166667
took to	completed batch_size duration	0.333333
training set according to the	fit predict	0.055556
reduced likelihood function for	gaussian process reduced likelihood function	0.047619
estimator and set the base_estimator_ attribute	ensemble ada boost classifier validate estimator	0.333333
process or thread pool	externals joblib	0.004762
evaluate predicted target values for x relative	metrics predict scorer call estimator x	0.166667
set x and	x y	0.002155
reproducibility flips the sign of elements	deterministic vector sign	0.066667
for all meta estimators in scikit-learn	meta	0.043478
fit estimator and predict	model_selection fit and predict estimator	1.000000
estimator on training	fit estimator estimator	0.111111
pca	pca	0.238095
set	set	0.440000
bicluster	bicluster	0.900000
detects the soft boundary of the set	class svm fit	0.125000
image samples in x into a matrix	x	0.001692
wild lfw pairs dataset this dataset is a	lfw pairs subset	0.035714
train estimator on	estimator estimator	0.105263
found and raise an exception if	line search	0.029412
the already fitted lsh forest	neighbors lshforest partial fit x	0.200000
not found and raise	search	0.019231
is meant	index_file_path data_folder_path slice_ color	0.033333
parallel processing	externals joblib parallel	0.014085
name for the	name	0.033333
the search over parameters	core base search	0.111111
any axis center to	x axis	0.030769
position of the	mds fit x y	0.066667
computes the weighted graph of neighbors	neighbors radius neighbors graph	0.066667
training data and parameters	x y	0.010776
do nothing and	feature_extraction	0.037037
scale each non zero row of x	transform x	0.016949
uncompressed bytes from the file	joblib binary zlib file	0.066667
user provided precisions	check precisions precisions	0.250000
the blup parameters and evaluates the reduced	process reduced	0.125000
l1 distances between the vectors	paired manhattan distances	0.083333
calibration curve	calibration curve y_true y_prob normalize	0.142857
handle the callable case for pairwise_{distances	callable x	0.083333
detects the soft	one class svm fit	0.125000
number of splitting iterations in the cross-validator parameters	split get n splits x y groups	0.111111
multi-class targets	core output code classifier	0.250000
curve auc using the trapezoidal rule this is	auc x y	0.040000
clustering on x and returns cluster	cluster dbscan fit predict x y	0.166667
given type in the	type	0.125000
data	core cross val	0.043478
curve auc using the trapezoidal rule this	metrics auc x	0.040000
kernel k	gaussian_process constant kernel	0.333333
an array with block checkerboard	checkerboard	0.100000
break the pairwise matrix	metrics parallel pairwise x	0.166667
similarity of two sets of biclusters	metrics cluster consensus score a b similarity	0.500000
row of x to unit norm parameters	preprocessing normalizer transform x y copy	0.250000
the deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call	0.333333
exception types	externals joblib	0.004762
read array from unpickler file handle	read array unpickler	1.000000
validation	directed dtype	0.166667
estimates for each input	core cross val predict estimator x y	0.045455
lfw	datasets fetch lfw	0.125000
pairs dataset this operation is	pairs	0.055556
the depth a which this function is	externals joblib memorized func check previous func code	0.055556
scale back	scaler inverse transform x	0.052632
assignment problem using the	assignment	0.111111
likelihood function for the given autocorrelation parameters theta	likelihood function theta	1.000000
the curve auc from prediction scores	auc score	0.052632
number of splitting iterations in	leave one group out get n splits x	0.111111
logistic loss and gradient	linear_model logistic loss and grad w x y	0.500000
for the voting classifier valid parameter	voting classifier set params	0.037037
r^2 coefficient of determination regression score	metrics r2 score y_true y_pred	0.125000
parametergrid instance for the given param_grid	search cv get param iterator	0.166667
classification by definition a confusion matrix :math c	confusion matrix y_true y_pred labels	1.000000
train	base shuffle split	0.142857
length is not found and raise	line search	0.029412
make cache size fit	externals joblib memory reduce size	0.083333
generate a random n-class classification	classification n_samples n_features n_informative n_redundant	0.500000
from features	y sample_weight	0.017857
compute the grid of alpha values	linear_model alpha grid x y	0.166667
generate a cartesian product of input	utils cartesian	1.000000
to coefs and intercept for specified layer	grad layer	0.166667
rand index adjusted for	metrics cluster adjusted rand	0.333333
augment dataset with an additional	preprocessing add	1.000000
the number of splitting iterations in	model_selection base kfold get n splits x y	0.111111
make and configure a copy	make estimator append random_state	0.166667
thread pool	externals	0.005747
the validity of the input	metric p metric_params	0.100000
get the values used to	sgdoptimizer get	0.125000
a transform function to portion of selected features	transform selected copy	0.333333
estimates for each	cross val predict estimator x y	0.045455
fit the model	fit x y	0.023952
cosine distances between x and y	cosine distances x y	0.333333
the number of splitting iterations in	cross validator get n splits	0.125000
the similarity of two sets of biclusters	cluster consensus score a b similarity	0.500000
with stochastic gradient descent	base sgdregressor	0.100000
similarity of two clusterings of	score labels_true labels_pred	0.047619
to be captured	exceptions	0.083333
in representing each class	classifier	0.013699
nmf find two non-negative matrices w h whose	x w h n_components	0.038462
to bicluster	bicluster mixin get	0.500000
function output for x relative to	metrics threshold scorer call clf x	0.058824
data into the already fitted lsh forest	neighbors lshforest partial fit x	0.200000
tolerance which is independent of the	cluster tolerance x tol	0.058824
estimate model parameters	base mixture fit x	0.200000
cf tree for the	cluster birch	0.090909
outlyingness of observations in x according to	outlier detection mixin predict x	0.250000
path with coordinate descent	linear_model enet path	0.050000
from a	read	0.052632
using the	utils shape	0.013699
x and y	x y	0.023707
generate indices to split data into	split split	0.250000
undo the scaling	min max scaler inverse	0.500000
kddcup99 dataset downloading it	datasets fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
function called with the given arguments	externals joblib memorized func	0.013158
cohen's kappa a statistic that measures inter-annotator agreement	kappa score y1 y2 labels weights	0.500000
implementation is restricted to the binary	y_score average	0.111111
logistic loss and gradient	linear_model logistic loss and grad w	0.500000
an arbitrary python object into one	externals joblib dump value filename	0.083333
filters the given args and	ignore_lst args	1.000000
and evaluates the reduced likelihood function for the	gaussian_process gaussian process reduced likelihood function	0.047619
split data into	model_selection base kfold split x	0.250000
generate a sparse random projection matrix	random projection fit	0.333333
correct	correct	1.000000
model and transform with	transform	0.011236
the metric is invalid	undefined metric	0.333333
the derivative of the logistic sigmoid	logistic derivative z delta	0.166667
check if vocabulary is empty or missing	vectorizer mixin check vocabulary	0.250000
c such that for c in (l1_min_c infinity)	c x y loss fit_intercept	0.030303
swap	swap	0.833333
aggressive regressor read more in	aggressive regressor	0.166667
function call with the	joblib format call func	0.100000
matrices from	covariance_type n_components	0.333333
reconfigure the backend and	parallel backend base configure n_jobs	0.500000
pos_label	pos_label	1.000000
score on the given data if the	score	0.010101
partially fit underlying estimators should	one vs one classifier partial fit x	0.166667
return the path of	get	0.012048
computes the position of	mds fit x	0.066667
[1] and breiman [2]	make friedman3 n_samples noise random_state	0.166667
factorizing common classes param	first call clf classes	0.058824
labels	labels	0.857143
loss for	loss y_true	0.250000
flip	flip	1.000000
dual gap convergence criterion the	dual gap emp_cov precision_	0.071429
for reproducibility flips the sign	utils deterministic vector sign flip	0.066667
membership	z	0.090909
estimate the parameters of the gaussian distribution	mixture bayesian gaussian mixture estimate means nk	1.000000
in the given file as	compress	0.100000
determine whether y	y	0.002674
transforms features by scaling each feature to	scale x feature_range axis copy	0.200000
lasso with built-in cross-validation	lasso cv	0.333333
input and compute prediction of init	ensemble base gradient boosting init	0.142857
the	joblib multiprocessing backend	0.052632
cv in a	core check cv cv x y	0.031250
homogeneity metric of a cluster labeling given a	cluster homogeneity score labels_true	0.500000
decisions within a job	estimators_features x	0.111111
retrieve the leaves of	leaves	0.071429
the accuracy of a classification	y_true	0.021739
clf	clf	1.000000
compute the precision the precision is the	metrics precision	0.033333
sparse random projection matrix	random projection fit x y	0.333333
we don't store the	externals	0.011494
to produce a cosine lsh fingerprint	gaussian random projection hash	0.166667
data onto the	transform x ridge_alpha	0.071429
cross-validated lasso using	cv	0.009009
allocate a new ndarray with aligned	utils aligned zeros shape dtype order align	0.500000
for c such that for c in	c x	0.030303
the data in the	data compress	0.100000
classifier valid parameter keys can be	classifier set params	0.125000
each sample	samples x labels	1.000000
binary classification	y_true y_score pos_label	0.066667
for the means	means x	0.250000
to workaround python 2 limitations of pickling	obj methodname	0.111111
the training set	factor fit	0.062500
platform independent representation	utils shape	0.013699
zndarray	zndarray	1.000000
computes the gradient and the	w x y alpha	0.250000
computes multidimensional scaling using smacof	smacof single dissimilarities metric n_components init	0.333333
which	backend effective	1.000000
empty the function's cache	externals joblib memorized func clear warn	0.250000
each input data point	predict estimator	0.045455
predict using the kernel ridge model	kernel ridge predict	1.000000
degree	degree	1.000000
max	preprocessing max	0.166667
private method don't use directly	externals signature bind args kwargs partial	1.000000
neighbors_indices	neighbors_indices	1.000000
terminal regions	terminal region tree terminal_regions leaf	0.066667
parameters for the voting classifier	voting classifier set params	0.037037
evidence based on x and	x	0.001692
recompute log probabilities	core multinomial nb update feature log prob	1.000000
k-neighbors of	neighbors kneighbors mixin kneighbors	0.100000
the mean and component wise scale to unit	preprocessing scale	0.090909
regression problem with sparse	make sparse	0.125000
be used when memory is inefficient to train	classes	0.025641
call predict_log_proba on the estimator with the	predict log proba x	0.045455
the lrd of a sample is the inverse	distances_x neighbors_indices	0.047619
isotonic regression model : min	core isotonic regression	0.055556
calculate true and false positives per binary classification	binary clf curve y_true y_score pos_label	0.090909
input vectors individually to unit	copy	0.062500
we don't store	func	0.011364
x from	x	0.001692
cleanup a temporary folder if still existing	folder folder_path	0.250000
median absolute error regression	median absolute error y_true	0.166667
the best found	core base search cv predict proba	0.076923
decision function of	ensemble gradient boosting classifier decision function	0.166667
utility for building a cv	check cv cv	0.031250
laplacian matrix and convert it to a	laplacian	0.034483
different probability thresholds note	probas_pred pos_label sample_weight	0.066667
gap	gap emp_cov	1.000000
model by using the gp prior	x return_std return_cov	0.142857
r^2 coefficient of determination regression score function	metrics r2 score y_true y_pred sample_weight multioutput	0.125000
u such that	u	0.032258
get	voting classifier get	0.200000
cv in	check cv cv x y classifier	0.031250
compute the gradient of loss	perceptron compute loss	1.000000
average path length of	average path length	0.090909
for fit	fit	0.003257
cv in a user friendly	cv cv	0.031250
full covariance	full	0.055556
the score of	core score	0.166667
list of exception	joblib	0.007299
label sets	preprocessing multi label binarizer transform	0.250000
logistic	neural_network inplace logistic	0.333333
list of feature name -> indices mappings	feature_extraction dict vectorizer	0.200000
problem with sparse uncorrelated design	sparse uncorrelated n_samples n_features	0.166667
oracle approximating shrinkage covariance model according to the	covariance oas fit	0.083333
transform binary labels back to multi-class	inverse transform y threshold	0.333333
a locally linear embedding analysis on the data	locally linear embedding x n_neighbors	0.071429
false for indices increasingly apart the	verbosity filter index	0.055556
the curve auc using the trapezoidal	metrics auc x y	0.040000
scale back the	inverse transform x	0.051282
the points in the	parameter	0.083333
the similarity of two clusterings of	score labels_true labels_pred	0.047619
unnormalized posterior log probability of	core base nb joint log likelihood	0.166667
and false positives per binary classification	binary clf curve y_true	0.090909
loader for	data_home	0.166667
fit the model by computing	pca fit	0.111111
for building a cv in a	cv cv	0.031250
total log probability under	neighbors kernel density score x	0.333333
init	boosting init decision function x	0.142857
boost using the	classifier boost	0.100000
for building a cv in a user friendly	check cv cv x y	0.031250
determine the	externals joblib parallel backend base	0.034483
"news" format strip lines	strip newsgroup	0.090909
the elastic net optimization function	l1_ratio	0.030303
after the other and transforms the	y	0.002674
the number of splitting iterations in the	leave one out get n splits	0.111111
cross-validated estimates for	x y cv	0.050000
cov_computation_method	cov_computation_method	1.000000
of two clusterings of a set of points	score labels_true labels_pred	0.047619
array from the meta-information and the z-file	joblib zndarray wrapper read unpickler	0.043478
depending	memorized	0.015873
constructs a new estimator with the same parameters	core clone estimator	0.333333
ability to accept precomputed	precomp distr	0.250000
draw randomly sampled indices	ensemble generate indices random_state bootstrap n_population n_samples	1.000000
the kl divergence of	kl divergence	0.083333
median absolute error	metrics median absolute error	0.166667
utility for building a cv in	cv cv x y classifier	0.031250
loads data from module_path/data/data_file_name	datasets load data module_path data_file_name	0.500000
the file descriptor for the underlying file	externals joblib binary zlib file fileno	0.333333
signature from	signature	0.047619
and	x y	0.086207
in svmlight	svmlight	0.050000
fit the model with x	core rbfsampler fit x y	1.000000
log of	regression predict log	0.500000
fit the model to	output estimator fit x	0.200000
with a given cache	joblib cache	0.250000
a cv in a user friendly way	check cv cv x y	0.031250
orthogonal	orthogonal	1.000000
class with a	externals add	0.142857
n_classes	n_classes	1.000000
in friedman [1] and breiman [2]	make friedman3 n_samples noise	0.166667
are selected returns	feature_selection selector mixin	0.142857
mixin for linear classifiers	linear classifier mixin	1.000000
incrementally fit the model	core multi output regressor partial fit	0.200000
mixin for	mixin	0.222222
depth a which this function is	externals joblib memorized func check previous func code	0.055556
that can actually run in parallel	joblib parallel	0.028571
function the absolute error of	error	0.020000
center and scale	preprocessing robust scaler transform x y	0.200000
break the pairwise matrix in n_jobs even slices	parallel pairwise x y func n_jobs	0.111111
the initial centroids parameters	cluster init centroids	0.166667
data precision matrix	pca get precision	0.066667
of approximate nearest neighbors	neighbors lshforest kneighbors	0.500000
decision functions of the base	decision function x	0.018868
the callable case for pairwise_{distances kernels}	pairwise callable x	0.083333
prediction scores note	roc	0.033333
memory is inefficient	y classes	0.027778
last element	last element arr	0.142857
all tokens in the raw documents	feature_extraction count vectorizer fit raw_documents	0.125000
outlyingness of	outlier detection mixin predict	0.250000
validate 'train_sizes'	train_sizes n_max_training_samples	0.200000
compute a logistic regression model	linear_model logistic regression	0.333333
x format check x	allocation check	0.062500
general function given points on a curve	x y reorder	0.111111
to the cache for the	joblib memorized	0.015625
of x according to feature_range	x	0.001692
parallel processing this method is meant	externals joblib parallel	0.014085
model we can also predict	predict	0.006849
to build a batch of	build	0.037037
precisions parameters of the precision distribution	precisions nk	0.166667
the timestamp when pickling to avoid	joblib memory reduce	0.030303
return a platform independent representation of	utils shape	0.013699
of x from y	x z reg	0.066667
new samples can be different from the	calibrated classifier	0.083333
and then the	y	0.002674
the hash depending from	func	0.011364
generate cross-validated estimates for each input data point	predict estimator x y cv	0.071429
the voting classifier valid parameter keys	ensemble voting classifier	0.031250
predicts one class versus all others	multiclass x y alpha c	0.166667
vector	positivity	0.142857
similarity coefficient score the	score	0.010101
fit the model to data	fit x y sample_weight	0.040000
apply dimensionality reduction to x	decomposition factor analysis transform x	0.250000
of a memmap instance to reopen on same	externals joblib reduce memmap a	0.050000
elastic net path	path x	0.045455
number of splitting iterations in the	model_selection cviterable wrapper get n splits	0.111111
compute elastic net path with coordinate descent the	path x	0.045455
the wild lfw pairs dataset this dataset is	fetch lfw pairs	0.018868
calculate the posterior log probability of	nb joint log likelihood	0.066667
index of the leaf	base decision tree apply	0.166667
in [rouseeuw1984]_ aiming at computing	x n_support remaining_iterations initial_estimates	0.111111
mutual information between two variables	mi x y x_discrete y_discrete	1.000000
prediction pred and y	function call y pred sample_weight	0.500000
process or thread pool	externals	0.005747
build a batch of estimators within	parallel build estimators n_estimators	0.166667
svd	svd	0.571429
the kddcup99 dataset downloading it if necessary	kddcup99 subset data_home download_if_missing random_state	0.111111
sample from the decision boundary for each class	core one vs rest classifier decision function x	0.250000
apply feature map to	core nystroem transform	1.000000
a subcluster from	subcluster	0.090909
svmlight / libsvm file format	svmlight file x y f	1.000000
locally linear embedding analysis on the	locally linear embedding x n_neighbors n_components reg	0.071429
the wild lfw pairs dataset this dataset is	lfw pairs subset	0.035714
whether	dot product	0.250000
binary classification threshold	binary	0.031250
iterlists	iterlists	1.000000
autocorrelation parameters theta as the maximizer of the	gaussian process arg max	0.047619
get	joblib get	0.666667
a which this function is called to issue	externals joblib memorized	0.013699
a score by cross-validation read more in	model_selection cross val score estimator	0.166667
in the wild lfw pairs dataset this dataset	lfw pairs	0.018868
number of jobs which are going to run	joblib multiprocessing backend effective n jobs n_jobs	0.333333
determinant with	det	0.071429
with joblib	joblib	0.007299
with block checkerboard	checkerboard	0.100000
not enabled	preprocessing robust	0.111111
using a single binary	predict binary	0.200000
free energy f v	neural_network bernoulli rbm free energy	0.066667
the lfw pairs dataset this operation is meant	lfw pairs index_file_path data_folder_path slice_ color	0.333333
from the meta-information and	zndarray wrapper read unpickler	0.043478
perform dbscan	dbscan fit	1.000000
non-negative matrix factorization nmf find two non-negative matrices	decomposition non negative factorization	0.043478
indices increasingly apart	verbosity filter index	0.055556
given radius of a	radius	0.045455
equal to the average path length of	average path length	0.090909
fileobject	fileobject	0.500000
the weighted graph of neighbors	neighbors mixin radius neighbors graph	0.066667
uncompressed bytes from the	joblib binary zlib	0.333333
fit the calibrated model	calibrated classifier cv fit x	1.000000
the given arguments	joblib memorized func get	0.125000
p=none) generates a random sample from a	a size replace	0.142857
helper function to test error messages in exceptions	utils assert raise message exceptions message function	1.000000
for random matrix generation	core check input size n_components n_features	0.200000
meant to	index_file_path data_folder_path slice_ color	0.033333
transform x into subcluster centroids dimension	cluster birch transform x y	1.000000
boosting	boosting	0.888889
shortest path length from source to all reachable	single source shortest path length graph source cutoff	0.111111
x from y along the	x z reg	0.066667
clear all covered	hungarian state clear	1.000000
dtype of x	x	0.001692
the similarity of two clusterings of a set	score labels_true labels_pred	0.047619
the shortest path	utils single source shortest path	0.333333
posterior log probability of the samples	core bernoulli nb joint log likelihood	0.083333
are selected	feature_selection selector	0.142857
k x y and optionally its gradient	exponentiation call x y eval_gradient	0.333333
transform the given label sets parameters	preprocessing multi label binarizer transform	0.250000
of a wishart	wishart	0.062500
sample weight array	base sgd validate sample weight	0.333333
transform feature->value dicts to array or	feature_extraction dict vectorizer transform x y	0.200000
don't store	memorized func	0.016949
cf tree for the	cluster birch fit x	0.200000
'l' suffix	shape repr	0.013699
generate name est weight tuples excluding none	iter	0.050000
shrunk on the	shrunk	0.043478
graph of neighbors for points	neighbors radius neighbors graph	0.066667
the oracle approximating shrinkage covariance model according	covariance oas fit x	0.083333
logarithm of the normalization constant	logz v s dets n_features	0.200000
mean absolute error regression	mean absolute error	0.166667
back the data to the original representation parameters	scaler inverse transform x copy	0.066667
compute the grid of alpha values	linear_model alpha grid x y xy	0.166667
initial centroids	cluster init centroids	0.166667
h	h x w h	0.500000
the lfw pairs dataset this operation is meant	fetch lfw pairs index_file_path data_folder_path slice_ color	0.333333
the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage	0.125000
the cache for	externals joblib memorized	0.013699
dummyregressor is a regressor	regressor	0.027027
convert coefficient matrix to	linear_model sparse	0.076923
sure that an estimator implements the necessary	check estimator estimator	0.142857
for building a cv	check cv cv x y classifier	0.031250
get	selector mixin get	0.500000
predict class probabilities for	ensemble forest classifier predict proba	0.500000
the grid of alpha values	alpha grid	0.166667
determine absolute sizes of	sizes	0.050000
proper format	weight boosting validate x predict	1.000000
platform independent	utils shape repr	0.013699
fit multitaskelasticnet	multi task elastic net fit x y	1.000000
loss of prediction pred and y	ensemble loss function call y pred sample_weight	0.333333
e-step in em update	latent dirichlet allocation e step x cal_sstats random_init	1.000000
descent the elastic net optimization function varies for	l1_ratio	0.030303
precision the precision is the ratio tp /	metrics precision	0.033333
contingency matrix describing the	contingency matrix labels_true labels_pred eps sparse	0.166667
a buffered	externals joblib buffered	0.333333
the	externals joblib multiprocessing backend	0.035714
classification by definition a confusion matrix :math c	metrics confusion matrix y_true y_pred	1.000000
representation of	utils shape	0.013699
to the training set x and	predict x y	0.043478
regression	regression path x	1.000000
x as	x y	0.002155
y_max	y_max	1.000000
model and transform with	transform x	0.016949
the hash depending	externals joblib	0.009524
covariance matrices from a given template	match covariance type tied_cv covariance_type	0.333333
array with block checkerboard	checkerboard	0.100000
statistic that measures inter-annotator agreement	score y1 y2 labels weights	0.500000
with the best	cv predict	0.083333
k-neighbors of a point	kneighbors mixin kneighbors	0.100000
updating terminal	ensemble loss function update terminal	0.200000
corresponding to test sets	test	0.166667
last element of	last element arr	0.142857
generate a signal	signal n_samples n_components n_features	1.000000
of a memmap instance	joblib reduce memmap a	0.050000
and return the number of workers	externals joblib parallel initialize	1.000000
run fit on	fit	0.003257
the hash depending	memory	0.015625
given	y scorer	0.111111
of the leaf	apply	0.083333
to be captured	get exceptions	0.166667
the descriptors of a memmap instance	reduce memmap a	0.050000
incremental fit on a	gaussian nb partial fit x y classes sample_weight	0.200000
model	fit x y	0.035928
mse for the models computed by 'path' parameters	linear_model path residuals x y train	0.250000
c in (l1_min_c infinity) the model	c	0.022222
samples of length	samples	0.052632
elastic net path	linear_model enet path	0.050000
and maximum	max axis	0.500000
determine the number of jobs which are going	multiprocessing backend effective n jobs n_jobs	0.333333
by scaling each feature to	minmax scale x	0.142857
a contingency matrix describing the relationship	cluster contingency matrix labels_true	0.333333
lad updates terminal regions	least absolute error update terminal	0.200000
the k-neighbors	neighbors kneighbors mixin kneighbors x n_neighbors	0.125000
svmlight / libsvm file format	svmlight file x y f zero_based	1.000000
binomial deviance loss function	binomial deviance	0.250000
predict on the	predict x	0.011765
a matrix of patch data	patch extractor transform	0.200000
data to vectors and cluster the result	and cluster data vectors	0.333333
csgraph	sparsetools validate graph csgraph	0.250000
determine absolute sizes	core translate train sizes	0.066667
already fitted lsh forest	neighbors lshforest partial fit x	0.200000
the gaussian process	gaussian_process gaussian process	0.222222
count and smooth	count x y	0.250000
for full covariance	multivariate normal density full x	0.166667
mutual information between two variables	mi x y x_discrete	1.000000
not found and raise an exception if	search	0.019231
any axis center	x axis	0.030769
linear embedding read more	linear embedding	0.083333
points in the	core parameter	0.250000
for an estimator	estimator	0.014706
the minimum covariance determinant matrix	covariance fast mcd x support_fraction cov_computation_method random_state	0.250000
run fit with all	grid search cv fit	0.333333
a parallelbackend must implement	parallel backend base	0.037037
returns the number of splitting iterations in	model_selection leave one out get n splits	0.111111
helper	helper	0.700000
neighbors for points	neighbors	0.054054
load the rcv1 multilabel	rcv1	0.125000
n_jobs even	y func n_jobs	0.166667
kernel is	kernel operator is	0.333333
find two non-negative matrices w h whose	x w h	0.035714
x y and	rbf call x y	0.200000
the number of splitting iterations in the	leave pgroups out get n splits x	0.111111
x and	transform x y	0.031250
validation and conversion of	directed dtype csr_output	0.166667
of x (as bigger is	decision function x	0.018868
indices to split data into	model_selection base shuffle split split	0.250000
covariance m step	covar	0.153846
we don't	joblib	0.014599
a sparse random projection matrix	random projection fit	0.333333
warnings	warnings	0.615385
the local structure is retained	manifold trustworthiness x x_embedded n_neighbors precomputed	0.200000
indices to split data into	split split x	0.250000
of bag predictions	ensemble base forest set oob	0.250000
implement a single boost	weight boosting boost iboost x y sample_weight	1.000000
the blup parameters and evaluates the reduced	reduced	0.062500
check x format and make sure no	decomposition latent dirichlet allocation check non neg array	0.500000
to the average path	ensemble average path	0.142857
for c in	c x y loss fit_intercept	0.030303
samples from a	mixture sample	0.500000
function used to fit a single tree	trees tree forest x	0.142857
label binarizer parameters	preprocessing label binarizer	0.071429
backed arrays	backed a	1.000000
callers	effective	0.181818
break the pairwise matrix in n_jobs even	parallel pairwise x y func n_jobs	0.111111
apply dimensionality reduction	decomposition factor analysis	1.000000
score is	score y_true	0.058824
free energy f	neural_network bernoulli rbm free energy	0.066667
can also predict based on	predict	0.006849
perform a locally linear embedding analysis	locally linear embedding x n_neighbors n_components reg	0.071429
estimates for each input	cross val predict estimator	0.045455
reducer function to a	externals joblib customizable pickler	0.200000
boost using	ada boost classifier boost	0.100000
the model with x	x y	0.004310
the current	tell	0.125000
blobs for	blobs	0.125000
smacof	manifold smacof	0.400000
estimates	val predict estimator x	0.045455
class priors from multioutput-multiclass target data parameters	class distribution y	1.000000
train	base shuffle	0.166667
linear embedding analysis on	linear embedding x n_neighbors n_components reg	0.200000
voting classifier valid parameter keys	voting classifier	0.035714
function for the given autocorrelation parameters	function	0.021277
building a cv in a	cv cv	0.031250
estimate mutual information	feature_selection mutual info classif x y	0.500000
scale to unit	preprocessing scale x	0.090909
the curve auc using	auc x	0.040000
binary labels the output of transform	transform y	0.023256
for the voting classifier	ensemble voting classifier	0.031250
folders	externals joblib memory reduce	0.030303
estimator has been refit	base search cv	0.026316
x (as bigger is	function x	0.030303
"news" format strip the headers	strip newsgroup	0.090909
least squares projection of the data onto the	ridge_alpha	0.052632
a func to be	externals joblib pool manager mixin apply async func	0.250000
sparse uncorrelated design this dataset is described in	datasets make sparse uncorrelated n_samples n_features random_state	0.166667
c such that for c in (l1_min_c	c x y loss fit_intercept	0.030303
timestamp when pickling	func reduce	0.050000
elastic net parameter search	y xy l1_ratio	0.250000
random_state and sets them to	random_state	0.076923
point	core cross val predict estimator x y	0.045455
by its spectrum spectrum	spectrum n_samples	0.166667
the	base pca get	0.076923
the decision function	decision function	0.025000
diagonal of the kernel k	constant kernel diag	1.000000
calculate the posterior log probability of the samples	bernoulli nb joint log likelihood	0.083333
of width	effective_rank tail_strength	0.125000
graph of neighbors for points in	neighbors radius neighbors graph	0.066667
odds ratio	odds	1.000000
input	core cross val predict estimator x y	0.045455
the maximizer	process arg max	0.047619
in x and y	x y dense_output	0.333333
/ tp + fn where tp is	score y_true y_pred labels pos_label	0.027778
function with the given arguments and persist the	func call	0.047619
whose range approximates the range of	randomized range finder	0.083333
coefficient of determination regression score function	metrics r2 score y_true y_pred	0.125000
scale back the data to the	standard scaler inverse transform x copy	0.066667
given training data and parameters	x y	0.008621
best found parameters	model_selection base search cv predict proba x	0.076923
perplexity for data x	perplexity x doc_topic_distr	1.000000
under a size	externals joblib	0.004762
logarithm of the determinant	det	0.071429
factorization nmf find two	negative factorization	0.043478
initialization of the gaussian mixture parameters	mixture gaussian mixture initialize	1.000000
matrix factorization	negative factorization x	0.043478
orthogonal matching pursuit step on a precomputed gram	gram xy n_nonzero_coefs tol_0	1.000000
lfw pairs dataset this dataset	lfw pairs	0.018868
compute the decision function of the given	decision function	0.025000
voting classifier valid parameter keys can	voting classifier set	0.037037
update h in multiplicative update nmf	decomposition multiplicative update h x w h	0.250000
density estimation read more in the :ref user	density	0.043478
log probability for	mixture log	0.500000
the scaling of x according to	scaler inverse transform x	0.026316
partial dependence plots	ensemble plot partial dependence gbrt x	1.000000
exception types	externals	0.005747
leaves of the	get leaves	0.111111
predict on the estimator with the best found	search cv predict	0.037037
run fit on one set	model_selection fit grid point x	0.500000
cf tree for	cluster birch fit	0.200000
dual gap convergence criterion the specific definition	dual gap emp_cov precision_	0.071429
to build	ensemble parallel build	0.047619
boolean thresholding of array-like or scipy sparse matrix	preprocessing binarize x	0.083333
on the estimator with the best found parameters	base search cv	0.052632
the long	utils shape	0.013699
for building a cv in a user friendly	check cv cv x	0.031250
samples x	decision function x	0.018868
precision matrix	base pca get precision	0.066667
given parameter	estimator parameter	0.500000
of the function called with the given arguments	externals joblib memorized func get	0.125000
decision functions of	decision function	0.025000
a tolerance which	cluster tolerance	0.058824
a cosine lsh fingerprint	gaussian random projection hash	0.166667
compute decisions within	estimators_features x	0.111111
fit a multi-class classifier by combining binary	fit	0.003257
measure the similarity of	metrics cluster fowlkes mallows	0.250000
func to be run	async func	0.250000
returns the number of splitting iterations in the	get n splits x y	0.111111
data	core cross val predict	0.045455
complete cache directory	externals joblib memory clear warn	0.333333
estimate sample weights by	compute sample	0.100000
binary classification threshold	metrics binary	0.333333
sample from	sample	0.064516
evaluates the reduced	process reduced	0.125000
the lfw people dataset this operation is	lfw people	0.040000
svmlight / libsvm	svmlight file f	0.066667
a byte	externals joblib	0.004762
mixin class	estimator mixin	0.250000
fit	fit x y coef_init intercept_init	0.230769
median absolute error regression loss	metrics median absolute error y_true y_pred	0.166667
minimum covariance	covariance min	0.333333
transforms one after the other and transforms the	x y	0.002155
factor of x	factor decision function x	0.166667
the log of the determinant of a	log	0.018868
a dataset along any axis center to	axis	0.028169
avoid the	memorized func	0.016949
a list of feature names ordered by their	feature names	0.090909
used to build	ensemble parallel build	0.047619
index of the leaf	base decision tree apply x	0.166667
the decision path	decision path x	0.333333
compute log probabilities within a job	predict log proba estimators estimators_features x	0.250000
the local outlier factor	local outlier factor decision	0.125000
iterator over estimators	iter	0.050000
not enabled for sparse matrices	preprocessing robust	0.111111
faces in the wild lfw pairs dataset	fetch lfw pairs	0.018868
the meta-information and	joblib zndarray wrapper read unpickler	0.043478
to fit	fit	0.003257
retrieve or aggregate feature importances from estimator	feature_selection get feature importances estimator norm_order	1.000000
suffix when using	utils shape	0.013699
the solution to a sparse coding problem	sparse encode x dictionary	0.333333
matrices w h whose product	w h n_components	0.038462
data point	cross val	0.038462
data matrix x and target y	neural_network base multilayer perceptron partial	0.166667
memmap backed arrays	memmap backed a m	0.333333
fit linear model with stochastic gradient descent	linear_model base sgdregressor partial fit	1.000000
dense dictionary	dictionary y	0.111111
a collection of text	vectorizer	0.022222
coverage error measure compute	metrics coverage error y_true y_score sample_weight	0.166667
voting classifier valid parameter	voting classifier set	0.037037
values for x relative	metrics predict scorer call estimator x	0.166667
parameters theta as the maximizer of	gaussian_process gaussian process arg max	0.047619
binarization transformation using thresholding	binarize thresholding y output_type classes	1.000000
updates terminal regions to median	terminal	0.047619
information for reducing the size	items root_path	0.066667
an unfitted model by using the gp prior	x return_std return_cov	0.142857
compute scores	score	0.010101
california housing dataset from statlib	fetch california housing	0.083333
rand index adjusted	metrics cluster adjusted rand	0.333333
training set	fit	0.016287
calibration curve	calibration curve y_true y_prob	0.142857
the best found	core base search cv	0.033333
curve auc using the trapezoidal	metrics auc	0.040000
the centroids on	fit	0.003257
neighbors within a given radius of a point	neighbors x radius	0.142857
the neighbors within a	lshforest radius neighbors x	0.166667
that implement the partial_fit api need	utils check partial	0.038462
fit the ardregression model according to the	linear_model ardregression fit x	0.250000
sample from the distribution p(h|v)	neural_network bernoulli rbm sample hiddens v rng	1.000000
to retrieve a	externals	0.005747
the hash depending from it	externals joblib memorized func	0.013158
k-neighbors	neighbors kneighbors mixin kneighbors	0.100000
the test/test sizes are meaningful wrt to the	model_selection validate shuffle split n_samples test_size train_size	0.111111
validate x whenever one tries to predict	ensemble base forest validate x predict x	0.500000
absolute sizes of training subsets	core translate train sizes	0.066667
whether the kernel is	pairwise kernel is	1.000000
detects the soft boundary of the set of	one class svm fit	0.125000
density lrd	density	0.043478
verbose message on	verbose msg init	0.666667
the wild lfw pairs dataset	lfw pairs subset	0.035714
covariance model according to the given training	covariance	0.014493
net path	path x	0.045455
each input data	estimator x y	0.038462
returns the number of splitting iterations in	out get n splits	0.111111
evaluates the reduced likelihood function	gaussian_process gaussian process reduced likelihood function	0.047619
persist an arbitrary python object into one	joblib dump value filename compress protocol	0.250000
percentile	percentile	0.500000
scores note	roc	0.033333
voting classifier valid parameter	voting classifier	0.035714
the function call with the	externals joblib format call func	0.100000
fit the model to the	fit	0.003257
compute the weighted log	base mixture score	0.200000
of the decision functions of the	decision function	0.025000
compute data precision matrix with the	pca get precision	0.066667
indices in	neighbors find matching indices	0.250000
predict raises an exception in an unfitted	unfitted name	0.142857
lfw pairs dataset this dataset	lfw pairs subset	0.035714
estimate the spherical wishart distribution parameters	gaussian mixture estimate wishart spherical nk xk sk	0.333333
+ fp where tp is	score y_true y_pred labels pos_label	0.027778
true and false positives per binary	metrics binary clf curve y_true y_score	0.090909
a cv in	cv cv x y classifier	0.031250
implements feature hashing aka the hashing trick	feature hasher	1.000000
svmlight format this	svmlight	0.050000
the binary classification task	precision recall curve y_true	0.142857
transform new points into embedding	manifold locally linear embedding transform x	0.500000
block_size	block_size	1.000000
warning to notify	warning	0.083333
median absolute error regression loss read more in	median absolute error y_true	0.166667
of new samples can be different from	calibrated classifier cv	0.071429
warn	warn	1.000000
to	joblib memory reduce	0.030303
k x y and	dot product call x y	0.200000
computation of max	max	0.071429
helper to workaround python 2 limitations of pickling	utils parallel helper obj methodname	0.333333
the curve auc using the trapezoidal	auc x	0.040000
on x and returns cluster labels	cluster dbscan fit predict x y	0.166667
a contingency matrix describing	contingency matrix labels_true labels_pred	0.166667
using the	shape	0.011765
permutation_test_score	core permutation test score estimator x y	0.500000
generates indices to split data into	repeated splits split	1.000000
position of the points	mds fit x y	0.066667
non-negative matrices w h whose product approximates	x w h n_components	0.038462
number of splitting iterations in the cross-validator parameters	pgroups out get n splits x	0.111111
threshold	threshold	0.461538
unbalanced datasets	weight class_weight classes y	0.333333
predict class labels for x	ensemble voting classifier predict x	1.000000
a binary classifier on	binary	0.031250
after the other and transforms the	x y	0.002155
compute l1 and l2 regularization coefficients for	compute regularization alpha l1_ratio regularization	0.333333
l1_reg	l1_reg	1.000000
negative gradient)	deviance negative gradient y pred	1.000000
for c in (l1_min_c infinity)	c x y loss	0.030303
given dataset split	y scorer	0.111111
data to	data x y	1.000000
for each	core cross val predict	0.045455
number of splitting iterations in the	base kfold get n splits x	0.111111
build a batch of estimators within	ensemble parallel build estimators	0.166667
csgraph inputs	csgraph	0.111111
from features or distance	fit x y sample_weight	0.020000
logic estimators that implement the partial_fit	utils check partial fit	0.038462
sign correction to ensure deterministic output	flip u v u_based_decision	0.166667
x as a	fit x y	0.005988
fit_predict of last step in pipeline after transforms	pipeline fit predict x y	0.166667
determinant of a wishart the	mixture wishart	0.125000
average a binary metric for multilabel classification parameters	average binary score binary_metric y_true y_score average	1.000000
non-negative matrix factorization	decomposition non negative factorization	0.043478
to avoid the	externals joblib memorized	0.013699
arguments	externals joblib memorized func get output	0.125000
generate cross-validated estimates for each input	val predict estimator x y cv	0.071429
a sparse combination of the dictionary atoms	sparse coding mixin transform x y	0.333333
of exception types to	joblib parallel backend	0.045455
function opening the right fileobject	fileobject	0.100000
compute	neural_network base multilayer perceptron compute	0.250000
check initial parameters	mixture check parameters x	0.166667
return a callable that handles preprocessing and tokenization	mixin build analyzer	0.333333
and false positives per	clf curve y_true y_score	0.250000
with the best found parameters	search cv predict proba x	0.076923
points based on the percentiles of x	from x x percentiles grid_resolution	0.333333
and persist the	call	0.052632
product	projection transform x	0.333333
non-negative matrices w h whose	x w h	0.035714
inverse label binarization transformation using thresholding	preprocessing inverse binarize thresholding y output_type classes	1.000000
estimate the precisions parameters of the	bayesian gaussian mixture estimate precisions nk xk	0.166667
in-place	swap row x	1.000000
and perform dimensionality reduction	y	0.002674
normalize rows and columns	normalize	0.100000
centroids	centroids	1.000000
curve auc from prediction scores note this implementation	roc auc score	0.166667
to what extent the local structure is retained	trustworthiness x x_embedded n_neighbors precomputed	0.200000
w h whose	x w h n_components	0.038462
pairs dataset	pairs	0.111111
shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage x assume_centered	0.125000
of regularization	y pos_class cs	0.166667
are going to	joblib multiprocessing	0.052632
c in (l1_min_c infinity) the model	c x y loss fit_intercept	0.030303
fit the	svm linear svc fit	0.333333
that each sample is predicted as	check_input	0.100000
hash depending from	joblib memory	0.016949
back the data to the original	standard scaler inverse transform x copy	0.066667
to avoid code duplication between self _errors_svd	linear_model ridge gcv errors and values svd	0.333333
x from y along	x	0.001692
binary gaussian process classification based on	binary gaussian process	0.333333
computes the graph laplacian	graph	0.021277
whose range approximates the range	randomized range	0.083333
isotonic regression model :	core isotonic regression	0.055556
described by its spectrum spectrum	spectrum n_samples	0.166667
2d square and	tol raise_warning raise_exception	0.500000
parameters are well defined	parameters	0.111111
reduction for memmap backed arrays	memmap backed a m	0.333333
which is equal to the average	ensemble average	0.125000
coverage error measure compute	coverage error	0.166667
false positives per binary	metrics binary clf curve y_true	0.090909
for	covariance	0.028986
the process or thread	externals	0.005747
getter for the precision matrix	empirical covariance get precision	0.250000
sgdoptimizer	sgdoptimizer	0.833333
decision function output for x relative	metrics threshold scorer call clf x y	0.058824
input vectors individually to unit norm vector length	x norm axis copy	0.200000
on an array	array array	0.166667
of x from	x	0.001692
computes the position of the	mds	0.050000
decision tree regressor	tree decision tree regressor	0.166667
list of exception types	base	0.014286
wild lfw pairs	lfw pairs subset	0.035714
evaluate a score by cross-validation read more in	model_selection cross val score estimator x y groups	0.166667
index of the leaf	tree apply x	0.166667
under the curve auc from prediction scores note	roc auc score	0.166667
number of splitting iterations in the cross-validator parameters	cviterable wrapper get n splits x y groups	0.111111
with iterative fitting along	cv	0.018018
such that for c	c x y loss fit_intercept	0.030303
an array shape under python 2 the	shape	0.011765
helper function for factorizing common	first call clf	0.200000
number of splitting iterations in the cross-validator parameters	split get n splits	0.111111
learning rate and potentially other states at the	neural_network base optimizer iteration ends time_step	0.142857
classifier from the training	classifier fit	0.500000
the laplacian kernel between	laplacian kernel	0.166667
to split data into	series split split x	0.250000
computes the weighted graph of neighbors for points	mixin radius neighbors graph	0.066667
truncated svd by arpack or randomized	truncated x n_components svd_solver	0.333333
the recall is the ratio tp	recall	0.028571
the c	c	0.022222
for the voting classifier valid parameter keys	ensemble voting classifier set params	0.037037
that can actually run in parallel	parallel backend	0.030303
the linear assignment problem using the	utils linear assignment x	0.090909
log-likelihood of	score	0.010101
a grid	ensemble grid	0.111111
the right fileobject from	read fileobject fileobj	0.100000
returns the submatrix corresponding	mixin get submatrix	0.166667
by cross-validation	core cross val	0.043478
fit kernel ridge regression model parameters	core kernel ridge fit x y	1.000000
such that for c in	c	0.022222
mean and variance along an axix on a	utils mean variance axis x axis	0.142857
cvscore	cvscore	1.000000
check that predict raises an	utils check	0.023810
buffered	joblib buffered	0.333333
the search	core base search cv fit x	0.166667
new data into the already fitted lsh forest	neighbors lshforest partial fit x y	0.200000
under the curve auc using	auc	0.020408
dense array	densify	0.066667
is meant to be cached by	data_folder_path slice_ color resize	0.033333
estimate the precisions parameters of the precision	gaussian mixture estimate precisions nk xk sk	0.166667
train estimator on training subsets incrementally	incremental fit estimator estimator x	0.500000
unfitted model by using the gp prior	x return_std return_cov	0.142857
count	nb count	1.000000
v = - log sum_h exp(-e v h	v	0.052632
vectors	dummy regressor predict	1.000000
configure a copy of	estimator append	0.142857
famous people	funneled resize	0.142857
the number of splitting iterations in the cross-validator	leave one out get n splits x y	0.111111
generate cross-validated estimates	core cross val predict estimator x y cv	0.071429
back	scaler inverse transform	0.058824
remove cache	memory	0.015625
is the depth a which this function	externals joblib memorized func check previous func code	0.055556
call transform on	transform	0.011236
print verbose message on	mixture base mixture print verbose msg init	0.666667
in the wild lfw pairs dataset	fetch lfw pairs	0.018868
make predictions using a single binary	core predict binary	0.200000
regressor from the training	regressor fit	0.500000
dataset is	datasets	0.015152
true and false positives per binary	binary clf curve y_true y_score pos_label	0.090909
to build a batch of estimators within a	ensemble parallel build estimators	0.166667
it	joblib memory	0.016949
each	val predict estimator	0.045455
user provided precisions	precisions precisions covariance_type n_components	0.250000
input validation for	utils check x y x y accept_sparse dtype	0.250000
value of the log of the determinant of	log	0.018868
w in multiplicative update	multiplicative update w x w	0.500000
vectors for reproducibility flips the sign	utils deterministic vector sign flip	0.066667
exception types to	parallel backend base get	0.066667
similarity of two clusterings of a set	score labels_true labels_pred	0.047619
object	to_write filename	1.000000
the kernel	normalized kernel mixin	0.333333
to partition estimators	partition estimators	0.200000
utility for building a cv	check cv cv x y	0.031250
predict class probabilities for x	ensemble ada boost classifier staged predict proba x	1.000000
from sklearn	include_meta_estimators include_other type_filter include_dont_test	1.000000
similarity of	score a b similarity	0.125000
fit all transformers transform the data and	core feature union fit transform x y	0.500000
decision function to	decision function x	0.018868
dictionary learning finds a dictionary a set of	batch dictionary learning	0.142857
and a set	x y axis	0.250000
cholesky decomposition	cholesky	0.166667
values for a	train	0.117647
vectors for reproducibility flips the sign of elements	deterministic vector sign flip	0.066667
smacof algorithm	smacof	0.100000
implement the usual api and	fit x y	0.017964
fit the model to data matrix	fit	0.006515
check values of the basic parameters	check initial parameters x	1.000000
probabilities for a calibration curve	calibration curve y_true y_prob	0.142857
number of splitting iterations in the cross-validator	pgroups out get n splits x	0.111111
to	weights	0.166667
the file	file	0.142857
shrunk covariance model according to the given training	covariance shrunk covariance fit	0.083333
stratified k-folds cross-validator provides train/test indices	stratified kfold	0.200000
more in the :ref user guide <mini_batch_kmeans>	mini batch kmeans	0.166667
the callable case for	metrics pairwise callable x y metric	0.083333
split data into training and test set	series split split x y groups	0.200000
return the boston house-prices	boston return_x_y	0.250000
of biclusters	metrics cluster consensus	0.250000
arguments of a function	function	0.021277
scores note this implementation	roc	0.033333
along any axis	x axis	0.030769
partially fit underlying estimators should be used	one vs one classifier partial fit	0.166667
computes the probabilities p(h=1|v)	neural_network bernoulli rbm mean hiddens v	0.333333
a	utils eigs a	1.000000
a transform function to portion of selected features	transform selected	0.333333
fit the rfe	fit	0.003257
as the maximizer of the reduced likelihood function	process arg max reduced likelihood function	0.333333
don't store	joblib	0.014599
concentration parameter	concentration	0.125000
california housing dataset	california housing	0.083333
for elastic net parameter search parameters	xy l1_ratio	0.250000
calculate the posterior log probability of the	core multinomial nb joint log likelihood	0.083333
using	repr	0.012500
data from module_path/data/data_file_name	datasets load data module_path data_file_name	0.500000
int	int	0.500000
error between two	error	0.020000
returns a nicely formatted statement displaying	args kwargs object_name	0.166667
subcluster	subcluster new_subcluster1	0.166667
data loading for the lfw people dataset this	datasets fetch lfw people	0.040000
nmf initialization	decomposition initialize nmf	1.000000
the gradient and	w x y	0.133333
avoid the	externals joblib memorized	0.013699
number of splitting iterations in the cross-validator parameters	one group out get n splits	0.111111
inplace row scaling of	inplace row scale	0.142857
func	async func	0.250000
return a tolerance which is independent	cluster tolerance x tol	0.058824
to line_search_wolfe2 if suitable	wolfe12 f fprime xk pk	0.028571
to split data into	time series split split	0.250000
lfw pairs dataset this operation	lfw pairs	0.018868
cross-validated estimates for	y cv	0.050000
filters the given args and kwargs	func ignore_lst args kwargs	0.333333
token occurrences it turns	hashing	0.125000
for each input data	cross val predict estimator x y	0.045455
feature name -> indices mappings	dict vectorizer fit x	0.250000
neighbors within a	neighbors x	0.166667
to avoid the hash depending	externals joblib memorized func	0.013158
a given radius of a point or points	x radius	0.058824
wise scale to unit variance	preprocessing scale	0.090909
euclidean or frobenius norm of x	norm x	1.000000
x for	x means	0.500000
this is the time it take to	externals joblib squeeze time	0.200000
two non-negative matrices w h whose	w h	0.031250
compute the centroids on	fit	0.003257
compute the l1 distances between the vectors	paired manhattan distances	0.083333
matrix whose range approximates the range of	randomized range	0.083333
predictions using a single binary	predict binary	0.200000
curve auc using the trapezoidal rule	auc x y	0.040000
perform a locally	manifold locally	0.333333
w h	x w h n_components	0.038462
one class versus all others	multiclass x	0.166667
without visual nesting	call fn	1.000000
compute the unnormalized posterior log probability	nb joint log likelihood	0.033333
returns whether the kernel is	gaussian_process dot product is	1.000000
global clustering	global clustering x	0.142857
fit the model using x as training	fit x	0.012821
find the first prime	find prime	0.500000
run in parallel n_jobs is	n_jobs	0.023256
factorization nmf find two non-negative matrices w	negative factorization x w	0.500000
transform binary labels back	preprocessing label binarizer inverse transform y	0.500000
timestamp when pickling to avoid the hash	memory reduce	0.030303
return the directory	output dir	0.047619
linear assignment problem using the	utils linear assignment	0.090909
along an axix	axis x axis	0.083333
a fit across one fold	feature_selection rfe single fit rfe estimator x y	0.200000
and cpp files	and cpp files	1.000000
sorted array of integers	tree bin_x left_mask right_mask	0.166667
loading for the lfw people dataset	lfw people	0.040000
k x y and	product call x y	0.200000
x format check x	check	0.017857
from the meta-information	joblib zndarray wrapper read unpickler	0.043478
kernel	normalized kernel	0.250000
implementation is restricted to the binary classification task	recall curve y_true	0.142857
transform function to portion of selected features parameters	preprocessing transform selected x transform selected copy	0.333333
does nothing this transformer is stateless	feature_extraction hashing vectorizer partial	1.000000
matrix of patch	feature_extraction patch extractor	0.200000
suffix	shape repr	0.013699
function the absolute error of the	error	0.020000
the log of the	log	0.018868
target of new samples can be different from	core calibrated classifier cv	0.111111
likelihood function for the	likelihood function	0.142857
grid of alpha values for	alpha grid x	0.166667
log probability for full covariance matrices	log multivariate normal density full x means	0.333333
scale is zero we handle it correctly	handle zeros in scale scale	0.500000
maximum likelihood covariance estimator parameters	covariance empirical covariance x assume_centered	0.166667
bic or aic for model selection	ic	0.111111
hash to	joblib hash	1.000000
of exception types	joblib parallel backend base	0.058824
validity of the	params x metric p metric_params	0.100000
this is a general function given points on	x y reorder	0.111111
estimate model parameters with the	base mixture fit x	0.200000
kddcup99 dataset	brute kddcup99	0.166667
scale back	standard scaler inverse transform	0.066667
the timestamp when pickling to avoid the hash	joblib memorized func reduce	0.050000
regression based on k-nearest neighbors	kneighbors regressor	1.000000
shortest path length from source	single source shortest path length graph source cutoff	0.111111
single boost using	ensemble ada boost classifier boost	0.100000
for each input data	val predict estimator	0.045455
to workaround python 2 limitations of	obj methodname	0.111111
number of splitting iterations in the cross-validator	model_selection leave one out get n splits	0.111111
to x	transform x	0.016949
matrix factorization nmf find	negative factorization x	0.043478
set of samples x	x	0.001692
scale each non zero row of x	x y	0.002155
as training data	core isotonic regression	0.055556
func to	base apply async func	0.250000
compute the weighted log probabilities for each sample	base mixture score samples	1.000000
estimate model parameters with	base mixture fit	0.200000
finds	return_distance	0.222222
fit the	estimator fit x	0.200000
set	linear_model linear model set	0.500000
to split data into training and test set	model_selection time series split split x y groups	0.200000
generate	core cross val predict estimator x	0.045455
is equal to the average path	ensemble average path	0.142857
patches	extract patches	0.083333
read the	read	0.105263
right fileobject from a filename	read fileobject fileobj filename	0.250000
decision function	decision function x	0.075472
to	linear_model sparse coef	0.076923
pipeline after transforms	core pipeline fit predict x	0.166667
mse for the models computed by 'path' parameters	linear_model path residuals x y	0.250000
compressor matching	detect compressor	1.000000
shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage x assume_centered block_size	0.125000
the estimator with the best found	search cv	0.090909
determine absolute sizes of training subsets	translate train sizes	0.066667
boolean thresholding of array-like or scipy sparse	preprocessing binarize	0.083333
the lfw people	lfw people	0.040000
which defines all methods a parallelbackend must implement	parallel backend base	0.037037
diagonal of the kernel k x x	white kernel diag x	1.000000
estimators within a job	estimators n_estimators ensemble x y	0.083333
for c in (l1_min_c	c x y loss	0.030303
the parameters for the voting classifier valid parameter	ensemble voting classifier set params	0.037037
sure that	utils check	0.023810
type introduces an 'l'	shape	0.011765
likelihood	likelihood	0.888889
whether the kernel	pairwise kernel	0.250000
evaluate the density model	neighbors kernel density score samples	0.250000
last_sample_count	last_sample_count	1.000000
features by scaling each feature	min max scaler	0.083333
data	data	0.423077
make cache size fit in	memory reduce size	0.083333
the log-likelihood of	covariance score	0.071429
identify uniquely python objects containing numpy arrays	obj hash_name coerce_mmap	0.200000
voting classifier valid parameter keys can be	voting classifier	0.035714
an 'l' suffix when using the	utils shape	0.013699
x as	x y iter_offset	0.333333
load output	load output	1.000000
helper function for factorizing common	partial fit first call clf	0.200000
boost	ensemble ada boost classifier boost	0.200000
'l' suffix when	repr	0.012500
of the local outlier factor	local outlier factor	0.125000
compute the average log-likelihood	decomposition factor analysis score x	0.333333
compute elastic net path with	enet path	0.050000
in the svmlight / libsvm	svmlight file f n_features	0.066667
validity of parameters	params	0.028571
clear all covered matrix cells	hungarian state clear covers	1.000000
call predict on the estimator	predict	0.006849
in pipeline after transforms	core pipeline fit	0.166667
back the data to the original representation parameters	inverse transform x copy	0.066667
the grid of alpha values for	linear_model alpha grid x y xy	0.166667
a which this function is called to issue	externals	0.005747
list of feature names ordered by their indices	feature_extraction dict vectorizer get feature names	0.142857
folders to make cache size fit in	externals joblib memory reduce size	0.083333
trace of np dot x y t	trace dot x y	0.500000
dual gap convergence criterion the specific	covariance dual gap emp_cov	0.071429
inplace row scaling	inplace row scale	0.142857
predict class probabilities for x	ensemble forest classifier predict proba x	1.000000
rows	rows	1.000000
make and configure a copy of the base_estimator_	make estimator append random_state	0.166667
convert a sparse matrix to a given format	utils ensure sparse format	1.000000
update the variational distributions for the precisions	mixture dpgmmbase update precisions	1.000000
translate	translate	1.000000
for the lfw	fetch lfw	0.083333
compute the mean silhouette coefficient	metrics cluster silhouette score	0.250000
log of probability estimates	log proba x	0.333333
decorator to catch and hide warnings	warnings	0.076923
random_state and sets them to integers	random_state	0.076923
handle the callable case for pairwise_{distances kernels}	callable x y metric	0.083333
display the process of	print	0.076923
partial dependence plots	ensemble plot partial dependence gbrt	1.000000
the posterior log probability of	core multinomial nb joint log likelihood	0.083333
accuracy of a classification	y_true y_pred labels	0.125000
dataset along any axis center to the median	x axis	0.015385
the training set according to the	fit predict	0.055556
under	score x	0.033333
compute mutual information between two continuous variables	compute mi cc x y n_neighbors	1.000000
fit subclasses should implement this method!	decomposition base pca fit x	0.333333
when	shape repr	0.013699
distances between x and y	distances x y	0.285714
chunking it into mini-batches	mini batch kmeans	0.166667
check values of the basic parameters	base mixture check initial parameters x	1.000000
fit the hierarchical clustering on the	cluster agglomerative clustering fit	0.250000
run fit on the estimator with	fit	0.003257
center and scale the data	preprocessing robust scaler transform x y	0.200000
hash depending from it	joblib	0.014599
handle the callable case	callable x y metric	0.083333
x as training data and y	x y	0.004310
fit to data then transform	transformer mixin fit transform	0.500000
write array bytes to	externals joblib numpy array wrapper write array array	0.500000
a partial	partial	0.043478
feature names ordered by their	get feature names	0.090909
utility for building a cv in a user	check cv cv x y	0.031250
logic estimators that implement the	utils check	0.023810
lower bound on model evidence based	lower bound	0.071429
in n_jobs	func n_jobs	0.166667
per document with nonzero entries	feature_extraction count vectorizer inverse transform	0.166667
lik	lik	1.000000
element of numpy array or sparse matrix	element	0.083333
feature name -> indices mappings and transform x	feature_extraction dict vectorizer fit transform x y	1.000000
by some authors	preprocessing label binarizer	0.071429
estimate sample weights by class for unbalanced datasets	sample weight class_weight y	0.500000
transforms features by scaling	preprocessing minmax scale x feature_range axis copy	0.200000
area under the curve auc	metrics auc	0.040000
locally linear embedding analysis on	locally linear embedding	0.050000
sign correction to ensure deterministic output from	flip u v u_based_decision	0.166667
a	externals joblib sequential	1.000000
include_self	include_self	1.000000
path length of an unsuccessful bst search since	path length n_samples_leaf	1.000000
warnings	warnings obj	1.000000
or lasso path using lars algorithm [1]	linear_model lars path x y	0.100000
graphlasso covariance model to x	covariance graph lasso cv fit x y	0.333333
log of the determinant of a wishart	wishart log	0.500000
lasso path using lars algorithm [1] the	linear_model lars path x y	0.100000
predict confidence scores	linear_model linear	0.500000
samples from a gaussian distribution	mixture sample gaussian mean covar covariance_type	1.000000
return the directory in which are persisted	output dir	0.047619
back the data to the original	preprocessing standard scaler inverse transform x	0.066667
an arbitrary python object into	joblib dump value filename	0.083333
detects the soft boundary of the set of	class svm fit	0.125000
patches of any n-dimensional array in	extract patches	0.083333
where tp is	score y_true y_pred labels pos_label	0.055556
pickle-protocol - return state of the	core isotonic regression getstate	0.250000
case method='lasso' is :	xy gram	0.090909
target values for x relative to y_true	predict scorer call estimator x y_true sample_weight	0.200000
tree	tree forest	1.000000
binary	metrics binary	0.333333
build a contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true labels_pred eps sparse	0.200000
of neighbors for points	mixin radius neighbors	0.125000
for the one-vs-one multi class libsvm in	svm one vs one	0.050000
posterior log probability of the samples x	core bernoulli nb joint log likelihood x	0.500000
a calibration curve	core calibration curve y_true	0.142857
step in pipeline after transforms	pipeline fit predict x	0.166667
objective	covariance objective	0.125000
partially fit	core partial fit	0.500000
swaps two rows of a csr matrix in-place	utils inplace swap row csr x m n	0.250000
tolerance which is	cluster tolerance	0.058824
rand index adjusted for chance	adjusted rand	0.333333
score function best possible score is	score y_true y_pred	0.038462
and the	and	0.062500
the range of	range	0.058824
whenever scale is zero we handle it correctly	preprocessing handle zeros in scale scale	0.500000
make cache size fit in	joblib memory reduce size	0.083333
elastic net path with coordinate descent	enet path	0.050000
indices increasingly apart the distance	externals joblib verbosity filter index	0.055556
the vectors in x	x	0.005076
covariance w/ cross-validated choice of the	cv	0.009009
run a	externals joblib parallel	0.014085
extracts patches of any n-dimensional array in	extract patches	0.083333
remaining_iterations	remaining_iterations	1.000000
validation on an array	utils check array array accept_sparse	0.250000
median and	fit x y	0.005988
median	get median	0.166667
a platform independent	shape repr	0.013699
within a given radius	radius	0.045455
apply decision function to an array	quadratic discriminant analysis decision function	1.000000
least	ensemble least	1.000000
returns the number of splitting iterations in the	cross validator get n splits	0.125000
fit the model	randomized linear model fit	1.000000
to the binary classification task	score y_true y_score average sample_weight	0.076923
points	len	0.038462
as the maximizer	process arg max	0.047619
the number of splitting iterations in	base kfold get n splits x	0.111111
dictionary learning finds a dictionary	mini batch dictionary learning	0.142857
matrix to dense array	linear_model sparse coef mixin densify	0.100000
the local outlier	local outlier	0.250000
non-negative matrices w h whose product	x w h n_components	0.038462
determinant of a wishart	wishart	0.062500
and class	y	0.002674
to the average path length	ensemble average path length	0.090909
indices	base shuffle split iter indices	0.250000
weighted graph of neighbors for	radius neighbors graph	0.066667
compute elastic net path with coordinate	path	0.025641
a logistic regression	logistic regression path x y	0.333333
estimator using ransac algorithm	linear_model ransacregressor	0.500000
least squares solver	linear discriminant analysis solve lsqr x y shrinkage	1.000000
helper to	utils parallel helper	0.500000
the timestamp when pickling to	func reduce	0.050000
the one-vs-one multi class	one vs one	0.050000
weighted graph of neighbors for points in x	neighbors mixin radius neighbors graph x	0.500000
types	joblib	0.007299
given args and kwargs using a	args kwargs	0.100000
the position of the points in	mds fit x y init	0.066667
the mean and component wise scale	scale x	0.043478
minimum and maximum along an axis on	utils min max axis x axis	0.333333
store the timestamp when pickling	externals joblib memorized func reduce	0.050000
with respect to each parameter weights and	y activations deltas	0.500000
precisions	precisions nk	0.166667
the k-neighbors	kneighbors mixin kneighbors	0.100000
matrices w h whose product approximates	x w h n_components	0.038462
each input data	cross val predict estimator x y	0.045455
in the :ref user guide <mean_squared_log_error>	y_true y_pred sample_weight multioutput	0.100000
multi-output variable using a model trained	core multi output estimator	0.142857
the unnormalized posterior log probability of x	base nb joint log likelihood x	0.200000
set x and returns	x y	0.002155
private function used	function estimators	0.333333
position of the	mds	0.050000
data x	x y	0.006466
for indices increasingly apart the distance depending	externals joblib verbosity filter index	0.055556
regression model we can also predict	regressor predict x	0.166667
curve	curve y_true y_prob	1.000000
hash	externals joblib memorized func	0.013158
x and target y	base multilayer perceptron partial	0.166667
estimate the precisions parameters of the precision distribution	mixture bayesian gaussian mixture estimate precisions nk xk	0.166667
check the validity	check params x metric p metric_params	0.200000
pickle-protocol - set	core isotonic regression setstate	1.000000
estimator with the best found	search cv predict proba x	0.076923
finds the	n_neighbors return_distance	0.250000
retrieve the leaves of the	leaves	0.071429
log of probability estimates	logistic regression predict log proba x	1.000000
collect results from clf predict calls	voting classifier collect probas	1.000000
retrieve the leaves of the cf node	cluster birch get leaves	0.333333
the log-likelihood of a	score	0.010101
locally linear embedding analysis on	locally linear embedding x n_neighbors	0.071429
likelihood of the data under the model	mixture vbgmm score samples x	0.200000
parameters for the voting classifier	ensemble voting classifier set	0.037037
fit linear model with passive aggressive algorithm	linear_model passive aggressive classifier fit x y	1.000000
don't store the timestamp when pickling to avoid	memorized func reduce	0.050000
predict class at each stage	ensemble gradient boosting classifier staged predict	0.500000
the em algorithm and return the	mixture gmmbase	0.034483
the points in the grid	core parameter grid	0.500000
the neighbors	neighbors lshforest radius neighbors x	0.166667
kernel	kernel	0.453125
a cluster	cluster	0.021277
coefficient matrix	sparse	0.025000
to split data into training and test	model_selection base kfold split x y groups	0.200000
in representing each class with a	classifier	0.013699
in multiplicative update	multiplicative update h x w	0.500000
representation	utils	0.009709
compute	compute	0.705882
opposite of the local outlier factor of x	local outlier factor decision function x	0.100000
with the best found	base search cv predict	0.076923
return a platform independent representation	utils shape	0.013699
k-fold iterator variant with non-overlapping groups	group kfold	0.250000
persist an arbitrary python object into one file	filename compress protocol	0.250000
sparse components	sparse pca transform	0.500000
cutoff	cutoff	1.000000
the cache for the function	joblib memorized	0.015625
fit_binary	linear_model prepare fit binary est y	1.000000
get the boolean mask indicating which features are	get support mask	0.333333
fit the	decomposition pca fit	0.250000
compute non-negative matrix factorization	non negative factorization	0.043478
binary classifier on	binary	0.031250
permutation_test_score	core permutation test score estimator x y cv	0.166667
build a batch of estimators within	ensemble parallel build estimators n_estimators	0.166667
factorization nmf	decomposition non negative factorization	0.043478
method for updating terminal	function update terminal	0.200000
in bytes_limit	joblib	0.007299
k-means clustering algorithm	cluster k means x n_clusters init precompute_distances	0.500000
remove cache folders to make cache size fit	externals joblib memory reduce size	0.083333
oracle approximating shrinkage	oas fit	0.333333
each input data	core cross val predict estimator x	0.045455
dual gap convergence criterion the specific definition is	covariance dual gap emp_cov precision_ alpha	0.071429
orthogonal matching pursuit model omp parameters	orthogonal matching pursuit	0.250000
run fit on the estimator with	fit x	0.006410
number of splitting iterations in the cross-validator	get n splits x	0.111111
function opening the right fileobject from a	externals joblib read fileobject	0.100000
generate cross-validated estimates for each input data	val predict estimator x y cv	0.071429
suffix	utils	0.009709
the estimator with the best found	search cv predict	0.074074
list of feature names ordered by their	feature names	0.090909
for points in x	x	0.003384
to the given training data and parameters	x y	0.008621
note this implementation is restricted to the binary	sample_weight	0.018519
initial centroids parameters	cluster init centroids x	0.166667
compute mutual information between two variables	feature_selection compute mi x y x_discrete y_discrete	1.000000
with the given arguments	func get	0.100000
decision trees	decision tree	0.333333
returns the score on the given	score	0.010101
determine absolute sizes	sizes	0.050000
precision vector is positive-definite	precision positivity precision covariance_type	0.500000
predict using the multi-layer perceptron classifier	neural_network mlpclassifier predict	0.500000
back the data to the original	preprocessing standard scaler inverse transform x copy	0.066667
check the validity of the input	neighbors check params x metric p metric_params	0.200000
used to build a batch of	parallel build	0.047619
learning finds	learning	0.250000
the gaussian process regression model we can also	gaussian process regressor	0.055556
convert a sparse matrix to a given format	utils ensure sparse format spmatrix	1.000000
return the	datasets get	0.200000
depth	joblib memorized func check previous func code	0.333333
estimate the spherical wishart distribution parameters	mixture bayesian gaussian mixture estimate wishart spherical nk	0.333333
laplacian kernel between	metrics laplacian kernel	0.166667
estimates the shrunk ledoit-wolf	ledoit wolf shrinkage x assume_centered	0.250000
functions	function	0.021277
vectors for reproducibility flips the sign of elements	utils deterministic vector sign flip	0.066667
h whose	h	0.041667
and predict	and predict	1.000000
rank matrix with bell-shaped singular	rank matrix	0.166667
boolean mask x == missing_values	preprocessing get mask x value_to_mask	0.333333
fit the model	multi output estimator fit x y sample_weight	0.200000
locally linear embedding read more	locally linear embedding	0.050000
compute the decision function of the	decision function	0.025000
in sorted array of	tree bin_x left_mask right_mask	0.166667
for full	normal density full x means	0.166667
rfe	rfe	0.600000
precision ap from prediction	precision	0.016667
and false positives per binary classification	metrics binary clf curve y_true y_score	0.090909
back the data to the original representation parameters	scaler inverse transform x	0.052632
of init	init decision	0.142857
elastic net parameter search	xy l1_ratio	0.250000
fit the	core multi output estimator fit x	0.200000
back the	scaler inverse transform x	0.052632
implement a single boost	ada boost classifier boost iboost	1.000000
score by cross-validation read more in the :ref	cross val score estimator	0.166667
mean absolute error	metrics mean absolute error	0.166667
a cv in a user	check cv cv x y classifier	0.031250
we don't	externals joblib memorized	0.013699
the generative model	get	0.012048
the vocabulary dictionary and return term-document matrix	feature_extraction count vectorizer fit transform raw_documents y	0.166667
inplace column scaling of a csc/csr matrix	inplace column scale	0.166667
probabilities for a calibration curve	calibration curve y_true y_prob normalize n_bins	0.142857
search	search cv fit	0.111111
returns a lower bound on model evidence	dpgmmbase lower bound	0.071429
group out cross-validator	group out	0.142857
we don't store	memorized func	0.016949
number of splitting iterations in the cross-validator	pgroups out get n splits x y groups	0.111111
func to be run	joblib pool manager mixin apply async func	0.250000
dataset	dataset	1.000000
compute incremental mean	utils incr mean	0.166667
a locally linear embedding analysis on the	locally linear embedding	0.050000
determine the number of	effective n	0.333333
estimates the autocorrelation parameters theta as the maximizer	process arg max	0.047619
precisions	mixture check precisions precisions	0.250000
provided data parameters	neighbors kneighbors	1.000000
scale back the data to the original	scaler inverse transform	0.058824
check input and compute prediction of init	gradient boosting init decision function x	0.142857
returns	core	0.015385
perform a locally linear embedding analysis on the	locally linear embedding x n_neighbors n_components	0.071429
under windows this is the time it take	squeeze time	0.166667
eval_mse	eval_mse	1.000000
position	mds	0.050000
cv in a user friendly	core check cv cv	0.031250
a byte string to	externals joblib binary	0.200000
parametergrid instance for the given param_grid	grid search cv get param iterator	0.166667
file supports seeking	joblib binary zlib file seekable	0.250000
prediction of init	gradient boosting init decision function	0.142857
the curve auc from prediction scores note	roc auc	0.166667
perform a locally linear embedding	manifold locally linear embedding x n_neighbors n_components reg	0.071429
single boost using	classifier boost	0.100000
finds the neighbors within a given radius	neighbors lshforest radius neighbors x radius return_distance	0.500000
using x as training data	x skip_num_points	0.166667
absolute error regression loss read more in	absolute error	0.142857
absolute error of the	error	0.020000
tolerance which is independent	tolerance	0.045455
for building a cv	check cv cv x	0.031250
posterior log probability of the samples	bernoulli nb joint log likelihood	0.083333
generate a mostly low rank matrix with bell-shaped	datasets make low rank matrix n_samples n_features	0.500000
return the query based on	neighbors query	0.333333
returns the number of splitting iterations in	leave one group out get n splits	0.111111
finds indices	indices	0.055556
log probabilities for each sample	samples	0.052632
to avoid the	joblib memorized func	0.014706
build a batch of estimators within	build estimators n_estimators ensemble	0.166667
return a platform independent	shape	0.011765
the data home	datasets clear data home	0.076923
the one-vs-one	svm one vs one	0.050000
mode	mode	0.625000
with the	base	0.014286
finds indices	neighbors find matching indices	0.250000
log probabilities within a job	predict log proba estimators estimators_features	0.250000
theta as the maximizer	process arg max	0.047619
reproducibility flips the sign of	utils deterministic vector sign	0.066667
sample from	bernoulli rbm sample	0.500000
adjusted	metrics cluster adjusted	0.333333
to split data into	time series split split x	0.250000
given args and kwargs using a list	args kwargs	0.100000
performs the synchronous ascending phase	neighbors lshforest get candidates query max_depth bin_queries n_neighbors	0.333333
update	step x	0.333333
undo the	preprocessing min max	0.333333
job	estimators estimators_features x	0.333333
function cache result and	externals joblib memorized func	0.013158
coordinate descent the elastic net	l1_ratio	0.030303
compute log probabilities within a	ensemble parallel predict log proba	0.058824
matrices w h whose product	x w h n_components	0.038462
exception types to	joblib	0.007299
line_search_wolfe2 if	wolfe12 f fprime xk pk	0.028571
lsi model	decomposition truncated svd	1.000000
strip the headers by	strip	0.055556
the parallel execution	externals joblib parallel	0.014085
cache for the function	memorized	0.015873
to implement the usual api and hence	patch extractor fit x y	0.142857
projection the components	projection	0.071429
out of bag predictions	ensemble base forest set oob	0.250000
transform on the estimator with the	transform x	0.016949
from it	memorized	0.015873
building a cv in	cv cv	0.031250
fit naive bayes classifier	discrete nb fit	1.000000
class weights for unbalanced datasets	utils compute class weight class_weight classes y	0.500000
or	backend	0.016949
get parameters for this estimator	estimator get params deep	1.000000
blas	blas	1.000000
stacklevel is the depth	joblib memorized func check previous func code stacklevel	1.000000
the grid of alpha values for	linear_model alpha grid	0.166667
jaccard similarity coefficient score	score	0.010101
factorize density	density density n_features	1.000000
transform feature->value dicts to array or sparse matrix	feature_extraction dict vectorizer transform x	0.200000
of data	data	0.038462
l1 distances between the	metrics paired manhattan distances	0.083333
the main classification metrics read	metrics classification	0.052632
estimates for each input data point	predict estimator x	0.045455
list	parallel backend base	0.037037
of estimators within	estimators n_estimators ensemble x	0.083333
weighted graph of neighbors	mixin radius neighbors graph	0.066667
check validity of	check params	0.200000
estimates for each input	estimator x y	0.038462
compute directory associated with a	to dir cachedir func argument_hash	0.250000
transform on the estimator	transform	0.011236
estimate model parameters with the em algorithm	base mixture fit	0.200000
w[i] (y[i] - y_[i]) ** 2	sample_weight y_min y_max	0.166667
the decision function of	decision function x	0.018868
boosted classifier/regressor from the training set x y	ensemble base weight boosting fit x y sample_weight	1.000000
terminal	terminal region	0.100000
of init	gradient boosting init	0.142857
model with passive aggressive algorithm	linear_model passive aggressive classifier	0.250000
shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage x	0.125000
returns first and last element of numpy	core first and last element arr	0.250000
l1 distances between the	paired manhattan distances	0.083333
the dimension of	dimension	0.050000
the dense dictionary	dictionary y	0.111111
minimum distances between one point and a set	pairwise distances argmin x y	0.333333
voting classifier valid parameter keys can be listed	voting classifier set	0.037037
x format check x format and	dirichlet allocation check	0.062500
there to implement the usual api and hence	patch extractor fit x y	0.142857
back	inverse	0.166667
fit the model	embedding fit	0.333333
vectors for reproducibility flips the sign of	deterministic vector sign flip	0.066667
to	parallel backend base get	0.066667
a random projection p only changes	johnson lindenstrauss min dim n_samples	0.142857
the absolute error of the kl divergence	manifold kl divergence error	0.100000
fit on the estimator with randomly drawn parameters	core randomized search cv fit	1.000000
full covariance	full x means covars	0.166667
sum_h exp(-e v	v	0.052632
univariate feature selection	base filter	0.333333
maximizer of the reduced	process arg max reduced	0.200000
the oracle approximating shrinkage covariance model according	covariance oas fit	0.083333
number of splitting iterations in the	model_selection base cross validator get n splits x	0.125000
for each input	cross val predict estimator	0.045455
predict using the trained model	neural_network base multilayer perceptron predict	0.333333
h	h x w h beta_loss	0.500000
the right fileobject from a	read fileobject	0.100000
worthy enough to be merged if	merge subcluster nominee_cluster threshold	0.250000
or too common features	feature_extraction count vectorizer limit features	0.250000
linear assignment problem using	utils linear assignment x	0.090909
curve auc using the	auc x y	0.040000
pickle-protocol - set state	core isotonic regression setstate state	0.250000
to	coef	0.058824
"news" format strip	strip newsgroup	0.181818
a dense	random_state	0.076923
update	decomposition update	0.250000
fit linear model	fit x y sample_weight	0.020000
the models computed by 'path' parameters	linear_model path residuals x y train	0.250000
feature names ordered by their	vectorizer get feature names	0.125000
signal as a sparse combination of dictionary elements	make sparse coded signal	1.000000
compute elastic net path	linear_model enet path x	0.050000
data augmented with n_zeros for the given rank	rank rank data n_negative n_zeros	1.000000
and false positives per binary classification	metrics binary clf curve	0.090909
descent fit	fit	0.003257
and transform	transform x	0.016949
then return the score of the underlying	rfe score	0.200000
this score corresponds	score y_true y_score	0.025000
the squared	neural_network squared	0.500000
the :ref user guide <sparse_inverse_covariance>	emp_cov alpha cov_init mode	0.200000
a which this	externals joblib	0.004762
norm	norm	0.625000
like assert_all_finite but only for	utils assert all finite x	0.333333
with the	decomposition base pca get	0.071429
of estimators within	estimators n_estimators	0.083333
for parameter value indexing	model_selection index param value x v	0.200000
we	joblib memory	0.016949
the shortest path length	utils single source shortest path length	0.333333
wishart	wishart	0.500000
an 'l' suffix when	utils shape repr	0.013699
learn vocabulary and idf return term-document matrix	fit transform raw_documents y	0.100000
the 20 newsgroups data and	20newsgroups	0.055556
set the parameters of	core pipeline set	0.250000
median	metrics median	1.000000
gradient	grad	0.166667
product	projection transform x y	0.333333
hash to identify uniquely python objects containing numpy	joblib hash obj hash_name coerce_mmap	0.250000
the process or	joblib multiprocessing	0.052632
filters the given args and	func ignore_lst args	1.000000
l1 distances between	metrics paired manhattan distances	0.083333
transform function to portion of selected features parameters	x transform selected	0.333333
get the directory corresponding to the	func get func dir mkdir	0.333333
download the	download	0.142857
call predict_log_proba on the	predict log proba	0.029412
input and compute prediction of init	gradient boosting init	0.142857
to output a function call	externals joblib function called str function_name args kwargs	0.250000
predict on the estimator with	predict x	0.011765
insert a	insert cf	0.500000
updating terminal regions (=leaves)	function update terminal region tree terminal_regions leaf x	0.200000
expected value of the log of the	log	0.018868
don't store	externals joblib memorized	0.013699
for	predict	0.006849
set the parameters	set	0.160000
methods a parallelbackend must implement	backend base	0.032258
inplace column scaling of	inplace column scale x scale	0.166667
orthogonal matching pursuit	x y n_nonzero_coefs tol	0.250000
if the test/test sizes are meaningful wrt to	shuffle split n_samples test_size train_size	0.111111
the local outlier factor of x	local outlier factor decision function x	0.100000
by computing truncated svd by arpack or randomized	truncated x n_components svd_solver	0.333333
of a memmap instance to	externals joblib reduce memmap a	0.050000
kernel	gaussian_process kernel	0.666667
estimate model parameters with the em algorithm	mixture base mixture fit x	0.200000
helper function for _fit_coordinate_descent update	decomposition update	0.125000
the hash	func	0.011364
is worthy enough to	cfsubcluster	0.111111
perform classification on samples in x	svm base svc predict x	1.000000
memory is inefficient to	classes	0.025641
batch	one batch	0.500000
decision functions of the	decision function	0.025000
get the parameters of the votingclassifier	voting classifier get params deep	1.000000
list of	backend base	0.032258
binary labels the output of transform is sometimes	transform	0.011236
theilsenregressor class	linear_model lstsq x	1.000000
gap	gap emp_cov precision_ alpha	1.000000
test vectors x	x	0.001692
a mask	edges weights mask	0.333333
recall the recall is the ratio tp	recall	0.028571
pickle the descriptors of a memmap instance	reduce memmap a	0.050000
each sample	samples	0.157895
check that the parameters are well defined	bayesian gaussian mixture check parameters x	1.000000
it	externals joblib	0.009524
computes the position of the points	mds fit x y	0.066667
transform binary labels back	label binarizer inverse transform	0.500000
reduced	process reduced	0.125000
timestamp when pickling	joblib memory reduce	0.030303
compute log probabilities within a job	ensemble parallel predict log proba estimators estimators_features x	0.250000
exception types	joblib parallel	0.028571
avoid the	externals joblib	0.009524
return the shortest path length from source	shortest path length graph source	0.111111
a list of edges for	feature_extraction make edges	0.066667
optimal	externals joblib auto batching mixin compute	0.333333
compute a logistic regression	logistic regression path x y	0.333333
the scaling	scaler inverse transform	0.029412
of a dataset of shape (n_samples n_features)	decomposition infer	0.200000
for x relative	metrics threshold scorer call clf x	0.058824
a which this	externals joblib memorized func check	0.125000
the memory	memory	0.015625
dataset is described by its spectrum spectrum	spectrum n_samples n_features	0.166667
get the directory corresponding to the cache	externals joblib memorized func get func dir mkdir	0.500000
of points	axis metric	0.250000
pursuit omp solves n_targets orthogonal	linear_model orthogonal mp x y	0.250000
seeds	bin seeds	0.250000
directory in which are persisted the result	get output dir	0.047619
get	neighbors get	0.125000
maximum likelihood estimator	empirical	0.055556
estimate the spherical wishart distribution	mixture bayesian gaussian mixture estimate wishart spherical nk	0.333333
for a given	x y scorer	0.111111
prediction scores note this implementation is restricted	roc	0.033333
length is not found and raise an	search	0.019231
validate x whenever one tries to predict	tree base decision tree validate x predict x	0.500000
transform function to portion of selected features	x transform selected	0.333333
values	y train	0.166667
partition estimators between jobs	partition estimators n_estimators n_jobs	0.200000
types to be captured	backend base get exceptions	0.166667
parameter value indexing	model_selection index param value	0.200000
cohen's kappa a	cohen kappa	0.250000
the best found parameters	search cv	0.036364
the neighbors within a given radius	neighbors lshforest radius neighbors x radius	0.142857
a collection of raw	vectorizer	0.022222
for a calibration curve	core calibration curve y_true y_prob normalize	0.142857
a collection of text documents to	vectorizer	0.022222
the unpickler	unpickler	0.090909
indices increasingly apart the distance depending on the	joblib verbosity filter index	0.055556
each	cross val predict estimator x y	0.045455
the function call with the	joblib format call func	0.100000
the case of a multinomial	linear_model multinomial	0.200000
callable case for pairwise_{distances	metrics pairwise callable x y metric	0.083333
on the training set	factor fit	0.062500
updates terminal regions	terminal region tree terminal_regions	0.100000
return whether the file	joblib binary zlib file	0.133333
across axis 0	axis 0	1.000000
covariance estimator with	covariance	0.014493
return posterior probabilities	core quadratic discriminant analysis predict proba	0.333333
shrunk ledoit-wolf	ledoit wolf	0.111111
by scaling each feature to	scale x	0.043478
the gaussian	gaussian	0.058824
r2	r2	0.384615
global clustering for the subclusters obtained	global clustering x	0.142857
is a classifier	classifier	0.013699
predict the target of new	cv predict	0.041667
handle the callable case for pairwise_{distances	pairwise callable	0.083333
parameter	parameter	0.500000
and configure a copy of the	append random_state	0.142857
an array with block checkerboard structure	checkerboard shape n_clusters noise minval	0.066667
split data into	model_selection predefined split split	0.250000
to update params with given gradients	updates grads	0.076923
binary labels back	binarizer inverse	0.166667
temporary folder if still existing	folder folder_path	0.250000
explained variance regression score function best possible	explained variance	0.166667
compute a logistic	logistic	0.047619
build a batch of estimators within	parallel build estimators n_estimators ensemble	0.166667
user provided precisions	check precisions precisions covariance_type n_components n_features	0.250000
an arbitrary python object into one file	externals joblib dump value filename	0.083333
scale back the data to the original	preprocessing standard scaler inverse transform x	0.066667
number of splitting iterations in	split get n splits x y groups	0.111111
check_input	check_input	0.500000
a byte string to the file	externals joblib binary zlib file	0.083333
data precision matrix with the generative model	get precision	0.052632
label propagation module	label propagation	0.200000
compute density of a sparse vector	utils density w	1.000000
pool implementation with customizable pickling reducers	pickling pool	1.000000
leaf	tree base decision tree apply x	0.166667
to coefs and intercept for specified layer	layer n_samples	0.166667
a test that was skipped	skip test	0.200000
and return encoded labels	transform y	0.023256
update	update h	0.500000
the voting classifier	ensemble voting classifier set	0.037037
perform classification on an array	neighbors nearest centroid predict	0.142857
download	download	0.857143
each input	core cross val	0.043478
is the time it take to	externals joblib squeeze time t	0.200000
perplexity for data	perplexity	0.181818
normalize x by scaling rows and columns	cluster scale normalize x	0.142857
indices to split data into	kfold split	0.250000
mostly low rank matrix with bell-shaped singular	low rank matrix	0.083333
for each	cross val predict estimator	0.045455
the array from the meta-information and the	externals joblib zndarray wrapper read unpickler	0.043478
for binary classification used in hastie et al	datasets make hastie 10 2 n_samples random_state	0.166667
the precision matrix	covariance empirical covariance get precision	0.250000
parameters and evaluates the reduced likelihood function for	reduced likelihood function	0.041667
explained variance regression	explained variance	0.166667
split data into training and test set	split split x y groups	0.200000
each non zero row of x	transform x y	0.031250
array bytes to	array array	0.166667
along an axis on a	axis	0.014085
print	mixture print	1.000000
class covariance	class cov	0.250000
generator to create	gen	0.166667
wild lfw pairs dataset this dataset is	datasets fetch lfw pairs	0.018868
kddcup99	fetch brute kddcup99	0.166667
the largest k singular values/vectors for a sparse	svds a k ncv tol	0.166667
apply clustering to a projection	cluster spectral clustering affinity n_clusters	0.166667
print verbose message on the end of iteration	mixture print verbose msg init end ll	0.333333
global clustering for the subclusters obtained after fitting	global clustering x	0.142857
compute elastic net path with coordinate	enet path x	0.050000
scale back the data to the original representation	preprocessing standard scaler inverse transform x copy	0.066667
a single tree	trees tree forest x	0.142857
the score for a fit	fit rfe estimator	0.166667
the breakdown	linear_model breakdown	0.333333
determination	metrics r2	0.125000
negative value in	whom	0.166667
a cv in a	check cv cv x y	0.031250
estimate the parameters of the	mixture estimate means nk	1.000000
and dense inputs	x y	0.002155
of a csc matrix in-place	utils inplace swap row csc x m	0.250000
clustering on x and	dbscan fit predict x y	0.333333
scale	robust scale	0.125000
x from y along the first	x z	0.050000
distances_x	distances_x	1.000000
run fit on one set	core fit grid point x y	0.500000
for indices increasingly apart the distance	joblib verbosity filter index	0.055556
spherical wishart	wishart spherical nk xk sk	0.333333
approximates the range	randomized range	0.083333
filters the given args and kwargs using	externals joblib filter args func ignore_lst args kwargs	0.333333
oracle approximating shrinkage	oas	0.200000
random forest classifier	random forest classifier	1.000000
row scaling of a csr	row	0.066667
maximum likelihood covariance estimator parameters	covariance empirical covariance	0.071429
given cache key	joblib cache key	0.250000
if being run on travis	travis	0.142857
the bound	mixture bound state log	0.500000
predict class log-probabilities	classifier predict log proba	1.000000
a statistic that measures inter-annotator agreement	score y1 y2 labels weights	0.500000
type	type	0.625000
and set the base_estimator_ attribute	ensemble bagging classifier validate	1.000000
parameters	estimator parameters	0.500000
first	first	0.875000
to run	joblib	0.007299
generate	cross val predict estimator	0.045455
return the likelihood of the data	x	0.003384
fit the	multi output estimator fit x	0.200000
factorization nmf find two non-negative matrices w h	factorization x w h n_components	1.000000
this classification dataset is constructed by taking a	datasets make	0.015625
elastic net path with coordinate	path	0.025641
such that for c in (l1_min_c infinity)	c x y loss	0.030303
solve the isotonic regression model :	isotonic regression	0.055556
shrunk ledoit-wolf	ledoit wolf shrinkage x assume_centered	0.250000
best found	model_selection base search cv predict proba	0.076923
matrix whose range approximates the range of	range finder	0.083333
to compute log probabilities within a	ensemble parallel predict log proba	0.058824
of transform is sometimes	transform y	0.023256
adjusted	cluster adjusted	0.333333
boolean thresholding of array-like or scipy sparse	preprocessing binarize x threshold	0.083333
return the kernel k	gaussian_process white kernel	0.333333
predict_proba on the estimator with the best found	model_selection base search cv predict proba x	0.076923
the kernel k x	exponentiation call x	0.200000
mean squared logarithmic error regression	mean squared log error	0.200000
base class for forests	base forest	0.333333
data from module_path/data/data_file_name	load data module_path data_file_name	0.500000
estimators within	estimators	0.052632
the curve auc using	auc x y	0.040000
of the function called with the given arguments	func get output	0.125000
on test vectors	core dummy	0.142857
mlp	multilayer perceptron	0.071429
on x by chunking it into mini-batches	mini batch kmeans fit x y	0.500000
base class for mixture models	base mixture	0.111111
register a	externals joblib register	1.000000
that for c in (l1_min_c infinity)	c x y loss	0.030303
for the lfw people dataset this operation	lfw people	0.040000
avoid the hash	joblib memory	0.016949
call predict_log_proba on the estimator with the best	cv predict log proba	0.500000
ledoit-wolf	ledoit wolf shrinkage	0.250000
the voting classifier valid parameter keys	ensemble voting classifier set params	0.037037
covariance parameters for each mixture component	mixture gmmbase get covars	0.250000
of csgraph inputs	csgraph directed	0.250000
data and parameters	x y	0.010776
evaluate a score	score estimator x y groups	0.333333
also predict based on an	predict x	0.011765
wise scale to unit	preprocessing scale x	0.090909
number of splitting iterations in	model_selection leave pgroups out get n splits x	0.111111
descent the elastic net optimization	l1_ratio	0.030303
value	value x v	1.000000
locally linear embedding analysis	locally linear embedding	0.050000
point	predict estimator	0.045455
returns whether the kernel	gaussian_process pairwise kernel	0.250000
shrunk	shrunk	0.304348
one-vs-one multi	one vs one	0.050000
validate user provided precisions	mixture check precisions precisions covariance_type	0.250000
of csgraph	graph csgraph directed	0.250000
x y	x y max_samples max_depth	1.000000
w h whose product	w h n_components	0.038462
parameters theta as the maximizer of the	process arg max	0.047619
split data in train/test sets	split	0.027778
meta-information and	externals joblib zndarray wrapper read unpickler	0.043478
suffix when using	shape repr	0.013699
x relative to	metrics threshold scorer call clf x y sample_weight	0.058824
to maximize class separation	core linear discriminant analysis transform	0.250000
the wild lfw	datasets fetch lfw	0.041667
a transformed real-valued array into a	projection to	0.166667
using	utils shape	0.013699
w/ cross-validated choice of the	cv	0.009009
the 20 newsgroups data	20newsgroups	0.055556
a cv in a user friendly	core check cv cv	0.031250
line_search_wolfe1 but fall back to line_search_wolfe2 if suitable	wolfe12 f fprime xk pk	0.028571
don't store the	memory	0.015625
of two clusterings	score labels_true labels_pred	0.047619
and compute scores	x y classes	0.055556
input validation	y x y accept_sparse dtype	0.250000
shrunk on the diagonal read more in the	covariance shrunk	0.066667
rescale data so as to support	linear_model rescale data x y	1.000000
returns the bound term related to proportions	bound proportions z	0.333333
generate an array with block checkerboard structure	make checkerboard shape n_clusters noise minval	0.500000
boolean mask	get mask	0.333333
non-negative matrix factorization	negative factorization x	0.043478
exception types to	base get	0.066667
any axis center to the mean and	axis	0.014085
the graph-lasso objective function the objective	objective	0.076923
+ fn where tp is the number of	score y_true y_pred labels pos_label	0.027778
that for c in (l1_min_c infinity) the model	c x	0.030303
boosted classifier from the training	ensemble ada boost classifier fit	0.500000
contingency matrix describing the relationship	cluster contingency matrix labels_true labels_pred	0.333333
evaluates the reduced likelihood	gaussian_process gaussian process reduced likelihood	0.142857
maximizer of the	arg max	0.047619
scale back the data to	preprocessing standard scaler inverse transform x copy	0.066667
nans	clean nans	1.000000
fit the model to data	fit x y	0.011976
evaluates the reduced likelihood	gaussian process reduced likelihood	0.142857
returns a list of feature	vectorizer get feature	0.200000
rescale data so as to support	linear_model rescale data x	1.000000
input data point	predict estimator x	0.045455
input validation on an array	check array array accept_sparse	0.250000
for a 3d image	3d n_x n_y n_z	0.333333
recall the recall	recall	0.028571
the number of splitting iterations in the	get n splits x	0.111111
transforms and score with the final estimator parameters	core pipeline score x y sample_weight	0.500000
coverage error measure compute how	metrics coverage error	0.166667
approximates the range of	range	0.058824
the similarity of two	a b similarity	0.125000
convert string beta_loss	beta_loss	0.166667
update the	update	0.035714
generate an array	make	0.083333
note	sample_weight	0.018519
the free energy f v = -	free energy	0.066667
coefficient matrix to	sparse coef	0.071429
breiman [2]	make friedman3	0.166667
vocabulary dictionary and return term-document matrix	feature_extraction count vectorizer fit transform raw_documents y	0.166667
boosted classifier from the training set x	ensemble ada boost classifier fit x	1.000000
fit estimator and compute scores for	core fit and score estimator	0.333333
of x for later	x	0.001692
lstsq	lstsq	1.000000
for each input	core	0.015385
split data into	model_selection time series split split	0.250000
the long type introduces an 'l' suffix when	repr	0.012500
implement the usual api and	x y	0.006466
into the already fitted lsh forest	neighbors lshforest partial fit x y	0.200000
or thread	multiprocessing backend	0.038462
if the test/test sizes are meaningful wrt	shuffle split n_samples test_size train_size	0.111111
determine	parallel backend	0.030303
folders	memory reduce	0.030303
path length from source	path length graph source cutoff	0.200000
utility for building a cv	cv cv	0.031250
file	binary zlib file	0.187500
determinant with the fastmcd algorithm	cov det fit x	1.000000
estimators from the training	fit	0.006515
validation of	svm validate	0.500000
the labeled faces in the wild lfw pairs	lfw pairs subset	0.035714
used to compute log probabilities within a job	ensemble parallel predict log proba estimators estimators_features	0.250000
csgraph	utils sparsetools validate graph csgraph directed	0.250000
compute data precision matrix	precision	0.016667
numpy arrays	numpy	0.083333
wrapped function	func	0.011364
opening the right fileobject from a	read fileobject	0.100000
for each input data	cross val predict	0.045455
project data to vectors and cluster the	biclustering project and cluster data vectors	0.333333
kl divergence of p_ijs and	manifold kl divergence	0.083333
matrices	covariance_type n_components	0.333333
inefficient to train	x y classes	0.027778
generate a grid of points	ensemble grid	0.111111
the function call with the given arguments	format call func	0.100000
the kddcup99 dataset downloading it	fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
in the svmlight / libsvm	svmlight file f	0.066667
graph of neighbors for points in	radius neighbors mixin radius neighbors graph	0.066667
training set x and returns the	x y	0.002155
dataset is described by its spectrum spectrum	spectrum n_samples	0.166667
the trained model	multilayer perceptron	0.071429
r^2 coefficient of determination regression score	r2 score y_true y_pred sample_weight	0.125000
deltas	deltas	1.000000
estimate the tied covariance matrix	mixture estimate gaussian covariances tied resp x	1.000000
list of	joblib parallel backend	0.045455
code	get func code	1.000000
evaluate	score samples	0.500000
predict if	predict	0.006849
"news" format strip lines beginning with	strip newsgroup	0.090909
maximizer	gaussian process arg max	0.047619
meta-transformer for selecting features based on importance weights	select from model	0.333333
center to the median and component wise scale	scale	0.033333
the huber loss and the gradient	huber loss and gradient w x y epsilon	0.333333
generates boolean masks corresponding to test sets	base cross validator iter test masks	1.000000
wild lfw pairs dataset this dataset is	datasets fetch lfw pairs subset	0.035714
diagonal of the laplacian matrix	diag laplacian	0.111111
check the gaussian mixture parameters are well defined	mixture gaussian mixture check parameters x	1.000000
computes the position	mds fit	0.066667
to polynomial features	preprocessing polynomial features	0.500000
for the case method='lasso' is	xy gram	0.090909
error regression loss read more	error y_true y_pred	0.125000
getter for the	covariance get	0.166667
along an axis on a csr or	axis x axis	0.083333
the number of splitting iterations in	base kfold get n splits	0.111111
of exception	backend base	0.032258
compute the sigmoid	metrics sigmoid	0.333333
call with the	externals joblib format call	0.200000
but fall back to line_search_wolfe2 if suitable	wolfe12 f fprime xk pk	0.028571
of y eventually shuffle among same groups	model_selection shuffle y groups random_state	0.333333
we don't store	memorized	0.015873
data parameters	y sample_weight	0.017857
estimate model parameters with the em	mixture gmmbase fit	0.250000
orthogonal matching	orthogonal matching	1.000000
logistic regression cv aka logit	logistic regression cv	0.200000
anova f-value for the provided	feature_selection f classif x y	0.200000
splits for an arbitrary	splits	0.083333
false positives per binary classification	binary clf curve y_true y_score	0.090909
don't store the timestamp when pickling to avoid	reduce	0.034483
sign correction to ensure	flip u v u_based_decision	0.166667
transform a sequence of instances to a	feature_extraction feature hasher transform raw_x	0.333333
function of	function	0.106383
performs clustering on	fit predict	0.055556
transform binary labels back to multi-class labels parameters	preprocessing label binarizer inverse transform y threshold	0.333333
for validation and	directed dtype	0.166667
scale each non zero row of x	x y copy	0.142857
number of workers	parallel initialize	0.333333
the normalized laplacian	spectral	0.026316
swaps two rows of a csr matrix in-place	utils inplace swap row csr	0.250000
calculates	emp_cov shrinkage	0.500000
lfw pairs dataset this	fetch lfw pairs subset	0.035714
gaussian process model fitting	gaussian_process gaussian process fit x y	0.250000
transform x back to its original space	decomposition truncated svd inverse transform x	1.000000
classifier	classifier set params	0.125000
cf tree	cluster birch	0.090909
selected	feature_selection selector mixin	0.142857
of csgraph	graph csgraph	0.250000
computes the gradient and	x y alpha	0.181818
class	ensemble bagging classifier	0.200000
determines the blup parameters and evaluates the reduced	process reduced	0.125000
construct a pipeline from	core make pipeline	0.250000
can actually run in parallel	parallel	0.019231
used to fit an estimator within a job	parallel fit estimator estimator x	0.333333
w h whose product approximates the non-	w h n_components	0.038462
and **kwargs, in the context of the memory	externals joblib memory	0.016949
scale	preprocessing robust scale	0.125000
depending	externals joblib memory	0.016949
shortest	source shortest	0.333333
under the curve auc using the trapezoidal	metrics auc	0.040000
categories as subfolder names	container_path description categories load_content	0.500000
coverage error measure compute how far we	coverage error y_true	0.166667
for each boosting iteration	classifier staged	0.333333
the pairwise matrix in	metrics parallel pairwise x	0.166667
hash depending from	externals joblib memorized	0.013699
whether the kernel	kernel	0.046875
is inefficient to train all data	y classes	0.027778
check	dirichlet allocation check	0.062500
number of splitting iterations in the	base kfold get n splits	0.111111
with sparse uncorrelated design this dataset is described	datasets make sparse uncorrelated n_samples n_features	0.166667
list sparse matrix or similar	accept_sparse dtype order	1.000000
one after the other and	x y	0.002155
the precision is the ratio tp / tp	metrics precision	0.033333
fit is on grid	fit x	0.006410
data loading for the lfw pairs dataset this	fetch lfw pairs	0.018868
document-term	hashing	0.125000
the scaler	standard scaler	0.333333
evaluates the reduced likelihood	reduced likelihood	0.100000
with the best found	model_selection base search cv predict proba x	0.076923
data	core cross val predict estimator	0.045455
computes the barycenter weighted	barycenter	0.090909
complete cache directory	memory clear warn	0.333333
convert coefficient matrix to dense array	densify	0.066667
the covariance m step for	mixture covar	0.125000
function call	joblib format call func	0.100000
graphlasso covariance model to x	covariance graph lasso cv fit x	0.333333
label samples by quantile this classification dataset is	datasets	0.015152
write array bytes to	write array array	0.500000
of the breakdown point	linear_model breakdown point	0.333333
to the binary classification task	metrics precision recall curve y_true	0.142857
loss and the gradient	loss and gradient w x y epsilon	1.000000
compute the	mixture compute log	1.000000
building a cv in a	check cv cv	0.031250
evaluate predicted target values for x	call estimator x	0.166667
travis	travis	0.714286
the leaf that each sample is predicted as	apply x check_input	0.500000
best found	model_selection base search cv	0.040000
logic estimators that implement the partial_fit	utils check	0.023810
x return leaf indices	x	0.001692
get the parameters of	get	0.012048
types	backend	0.016949
quantiles	robust	0.090909
calculate approximate log-likelihood as score	decomposition latent dirichlet allocation score x	1.000000
in the dispatch table	reduce_func	0.125000
convert coefficient	linear_model sparse coef	0.076923
laplacian kernel between x	metrics laplacian kernel x	0.333333
whether the kernel is stationary	kernel operator is stationary	0.333333
building a cv in a	check cv cv x	0.031250
flattened log-transformed non-fixed hyperparameters	gaussian_process compound kernel theta theta	0.333333
the svmlight / libsvm format into sparse csr	svmlight file f n_features dtype	0.066667
seeds for	seeds x	0.250000
all meta estimators in scikit-learn	meta	0.043478
run fit with all sets	grid search cv fit	0.333333
intercept_init	intercept_init	1.000000
center x y and scale if the scale	center scale xy x y scale	1.000000
compute the residual (= negative gradient)	ensemble binomial deviance negative gradient y pred	0.333333
number of jobs which	backend effective n jobs n_jobs	0.333333
measure the similarity	fowlkes mallows	0.250000
theta as the maximizer of the reduced likelihood	arg max reduced likelihood	0.250000
loader for the california housing dataset from statlib	fetch california housing data_home download_if_missing	0.250000
operator	operator	1.000000
and properly handle kernels	utils safe split estimator x y indices	0.200000
computes the weighted graph of neighbors for points	neighbors radius neighbors mixin radius neighbors graph	0.066667
precisions parameters of the	precisions nk	0.166667
the linear	utils linear	0.333333
provides common code for text vectorizers tokenization logic	vectorizer mixin	1.000000
outlyingness of observations in x according	outlier detection mixin predict x	0.250000
train estimator on training subsets incrementally and compute	core incremental fit estimator estimator x y classes	0.200000
predict multi-class targets using underlying	core output code classifier predict x	0.250000
requested by the callers	effective	0.090909
generate a grid of points based on the	grid from	0.166667
class	class	0.571429
function returns posterior probabilities of	cv predict proba	0.034483
the means	means x z	0.250000
fit the model according to the given	svr fit x y sample_weight	0.250000
words	words	1.000000
and configure a copy of the base_estimator_	estimator append random_state	0.142857
labels back	preprocessing label binarizer inverse	0.166667
precision is the ratio tp / tp	metrics precision	0.033333
sample from	size replace	0.125000
cv in a user friendly way	cv cv x y classifier	0.031250
r^2 coefficient of determination regression score function	r2 score y_true	0.125000
returns the number of splitting iterations in the	get n splits x y groups	0.111111
by scaling each feature to a given range	preprocessing minmax scale x	0.142857
covariance parameters for each mixture	mixture gmmbase	0.034483
indices increasingly apart the	externals joblib verbosity filter index	0.055556
logic estimators that implement the partial_fit api need	utils check partial fit	0.038462
of rows in x and y	x y	0.002155
vocabulary dictionary and return term-document matrix	fit transform raw_documents y	0.100000
updates terminal regions to median	terminal region tree terminal_regions leaf	0.066667
of the kl divergence of p_ijs	kl divergence	0.083333
the oracle approximating shrinkage covariance model according to	covariance oas	0.083333
skip test if being run on travis	check skip travis	1.000000
fit linear model with passive aggressive algorithm	passive aggressive regressor fit	1.000000
log-likelihood of a gaussian data	covariance score	0.071429
by scaling	preprocessing minmax scale x	0.142857
building a cv in a user	check cv cv	0.031250
shortest path	utils single source shortest path	0.333333
in [rouseeuw1984]_ aiming at computing mcd	n_support remaining_iterations initial_estimates	0.111111
returns whether the kernel is stationary	gaussian_process dot product is stationary	1.000000
multi-class targets using	output code classifier	0.250000
meta-information and	joblib zndarray wrapper read unpickler	0.043478
train	core base shuffle split	0.166667
model fitting method	fit	0.003257
abort	parallel backend base abort	1.000000
for the california housing dataset from	fetch california housing	0.083333
the cache for the	memorized func	0.016949
the huber loss and the gradient	linear_model huber loss and gradient w x	0.333333
an exception in an unfitted	unfitted name	0.142857
write	func write	0.500000
predict the target of	cv predict x	0.125000
number of splitting iterations in the cross-validator	base cross validator get n splits	0.125000
data samples in x	x	0.001692
percentiles	percentiles grid_resolution	0.250000
from source to	graph source	0.200000
x and y	x y alpha c	0.333333
gram	omp gram	0.500000
compute the grid of alpha values	alpha grid x y xy	0.166667
ridge_alpha	ridge_alpha	0.263158
apply clustering to a projection to	clustering affinity n_clusters	0.166667
private function used to compute	ensemble parallel decision function estimators	0.333333
coverage error measure compute how	metrics coverage error y_true	0.166667
center	robust scaler transform	1.000000
model we can also predict based on	predict x	0.011765
a func to be	externals joblib parallel backend base apply async func	0.250000
calibration with isotonic regression or sigmoid	calibrated classifier cv	0.071429
rows of a csc/csr matrix in-place	utils inplace swap row x m n	0.250000
data	predict estimator	0.045455
weights	weights	0.833333
given	core	0.015385
determine absolute sizes of training subsets	sizes	0.050000
of the kernel k	compound kernel	0.333333
generate an array with constant block diagonal structure	make biclusters shape n_clusters noise minval	1.000000
absolute error regression loss	absolute error y_true	0.142857
the number of splitting iterations in the cross-validator	model_selection leave one out get n splits x	0.111111
range approximates the range	range	0.058824
fit the model with x	core skewed chi2sampler fit x y	1.000000
according to the given training data	x y sample_weight	0.025974
abstract base class for various loss functions	loss function	0.333333
returns	gaussian_process	0.625000
fit on the	fit	0.003257
fit	estimator fit x y	0.200000
this score corresponds to the area under the	score y_true y_score	0.025000
with n_zeros for	n_zeros	0.111111
of the kl divergence of p_ijs and	manifold kl divergence	0.083333
the number of splitting iterations in the	predefined split get n splits	0.111111
average of the decision functions of the	decision function	0.025000
indices	indices	0.555556
length from source to	length graph source cutoff	0.200000
such that for c	c x y loss	0.030303
to split data into training and test set	model_selection cviterable wrapper split x y groups	0.200000
estimates	predict estimator x	0.045455
lower bound	bound	0.083333
belongs to	mean shift	0.125000
pairs dataset this	pairs subset	0.125000
a list of edges	feature_extraction make edges	0.066667
the covertype dataset downloading it if necessary	datasets fetch covtype data_home download_if_missing random_state shuffle	0.333333
import path as a	resolv_alias win_characters	0.166667
evaluate the density model on	neighbors kernel density score samples	0.250000
to avoid the hash depending	func	0.011364
of approximate nearest	lshforest kneighbors	0.500000
the shortest path	shortest path	0.333333
and transforms	x y	0.002155
class at each stage	boosting classifier staged	0.500000
coverage error measure compute how far we	metrics coverage error y_true	0.166667
update the dense dictionary factor in	decomposition update dict dictionary	0.333333
estimates the autocorrelation parameters theta as the maximizer	gaussian_process gaussian process arg max	0.047619
mini-batch dictionary learning finds a dictionary	dictionary learning	0.142857
estimates for	cross val predict estimator	0.045455
compute class covariance matrix	class cov x y	0.250000
and compute prediction of init	init decision	0.142857
nicely formatted statement displaying the function call	format call func args kwargs object_name	0.333333
force the	memorized	0.015873
by scaling each feature	preprocessing minmax scale x	0.142857
a tolerance which is independent	tolerance	0.045455
returns the number of splitting iterations in	one group out get n splits x y	0.111111
linear least squares with l2 regularization	ridge	0.071429
decisions within	estimators_features	0.071429
this score corresponds to the area under	score y_true y_score	0.025000
cache	memorized func	0.016949
density model on	neighbors kernel density	0.090909
exception types	joblib parallel backend base	0.058824
x (as bigger	function x	0.030303
= (gamma <x y> + coef0)^degree	degree gamma	1.000000
factorize density check according to li et al	check density density n_features	0.166667
on the estimator with the best found parameters	model_selection base search cv predict proba x	0.076923
remove too rare or too common	feature_extraction count vectorizer limit	0.250000
check x	allocation check	0.062500
non-negative matrices w h	x w h	0.035714
introduces an	shape	0.011765
aggressive regressor read more in the :ref user	aggressive regressor	0.166667
c in (l1_min_c infinity) the model	c x y loss	0.030303
utility for building a cv	core check cv cv x	0.031250
a hash to identify uniquely python	externals joblib hash obj hash_name coerce_mmap	0.333333
evaluate the significance of a cross-validated	y cv	0.050000
kwargs using a list of arguments to	kwargs	0.076923
logistic regression cv aka	logistic regression cv	0.200000
cache result	externals joblib memorized	0.013699
calculate true and false positives per	clf curve	0.250000
return the number of cpus	externals joblib cpu count	1.000000
transform binary labels back to multi-class labels	label binarizer inverse transform y threshold	0.333333
clear the state of the gradient boosting model	base gradient boosting clear state	1.000000
score function best possible score is 1	score y_true y_pred sample_weight	0.062500
its patches	patches 2d patches image_size	0.333333
shrunk on the diagonal read more	covariance shrunk	0.066667
range approximates the range	range finder	0.083333
dummy feature	dummy feature	1.000000
number of splitting iterations in	one group out get n splits x y	0.111111
in	in	0.545455
dimensions	dimensions rng	0.333333
according to feature_range	preprocessing min max	0.333333
return the iris	iris	0.111111
opening the right fileobject from	read fileobject fileobj	0.100000
apply the derivative of the logistic sigmoid function	inplace logistic derivative z delta	0.166667
of a cluster	metrics cluster	0.142857
the neighbors within a	neighbors	0.027027
scale back the data to	preprocessing robust scaler inverse transform x	0.066667
scale back the data to the original	preprocessing standard scaler inverse transform x copy	0.066667
of the cf node	cluster birch get	0.333333
k x	gaussian_process rbf call x	0.200000
contingency matrix describing	contingency matrix	0.166667
back to line_search_wolfe2 if suitable step length	wolfe12 f fprime xk pk	0.028571
dataset is constructed by taking a	datasets make	0.015625
classification used in hastie	hastie	0.076923
should be used when memory is inefficient	y classes	0.027778
fit the model by computing full svd	decomposition pca fit full	1.000000
private function used to build a batch of	ensemble parallel build	0.047619
false discovery rate this uses the benjamini-hochberg procedure	fdr	0.142857
absolute sizes	translate train sizes	0.066667
we don't	externals	0.011494
test/test sizes are meaningful wrt to the	shuffle split n_samples test_size train_size	0.111111
predict the target of	predict	0.006849
compute log probabilities within a job	parallel predict log proba estimators estimators_features x n_classes	0.250000
propagation	propagation	0.538462
estimate class weights	class	0.071429
factorization	factorization x	0.043478
regression cv aka	regression cv	0.200000
evidence based on x and membership	x z	0.050000
on the estimator with the best found	core base search cv predict proba x	0.076923
matrix to dense array	densify	0.066667
filepath	filepath	1.000000
calculate the posterior log probability of	core multinomial nb joint log likelihood	0.083333
convert coefficient matrix to dense array	coef mixin densify	0.100000
regression based on	regressor	0.027027
points in	mode	0.125000
n_jobs is	n_jobs	0.046512
compute the deviance	deviance	0.062500
are going to run in parallel	joblib multiprocessing	0.052632
x by	x y	0.002155
sampler	sampler	1.000000
or sparse matrix x	x	0.001692
axis	axis	0.112676
fit	sgdclassifier fit x	0.333333
gap	gap	1.000000
a reliable	externals joblib	0.004762
ap from prediction scores this score	score	0.010101
mean shift	mean shift x	0.500000
visibles	visibles	1.000000
dictionary learning finds a dictionary a set of	mini batch dictionary learning	0.142857
matrix factorization nmf	decomposition non negative factorization	0.043478
return the disk usage	externals joblib disk used path	0.250000
meant to be cached by a joblib	index_file_path data_folder_path slice_ color	0.033333
load datasets in the svmlight / libsvm format	datasets load svmlight file f n_features	0.500000
avoid the hash depending from	joblib memory	0.016949
update the dense dictionary factor	decomposition update dict dictionary y	0.333333
items to delete to keep	items to delete root_path bytes_limit	0.500000
train test indices	indices	0.055556
fit	pca fit	0.333333
updating terminal regions (=leaves)	update terminal region tree terminal_regions leaf x	0.200000
get feature names from	get feature names	0.090909
with stochastic gradient descent	coef_init intercept_init	0.333333
wild lfw pairs dataset this dataset	fetch lfw pairs	0.018868
density model	density	0.043478
number of splitting iterations in the cross-validator	leave pgroups out get n splits x	0.111111
kernel k	gaussian_process white kernel call	0.333333
a sparse random matrix given	utils random choice	0.333333
execution only	externals joblib	0.004762
average a	y_score average	0.111111
inplace column	inplace column scale	0.166667
fit across one fold	feature_selection rfe single fit rfe estimator	0.200000
computes the gradient and the	y alpha	0.222222
on left-out data for a full	x_train y_train x_test y_test	0.200000
handle the callable case for pairwise_{distances	pairwise callable x	0.083333
format check x format and	allocation check	0.062500
to the separating hyperplane	lib svm decision function	0.333333
x y and	call x y	0.392857
orthogonal matching pursuit step on a precomputed gram	gram omp gram xy n_nonzero_coefs tol_0	1.000000
path with coordinate descent the	path	0.025641
clustering for the subclusters obtained after	clustering	0.050000
function of the given	function	0.021277
a tolerance which is	cluster tolerance	0.058824
isotonic regression model :	isotonic regression y	0.066667
model we can also predict based on an	predict x	0.011765
and configure a copy of	append random_state	0.142857
the gaussian process regression model we can also	gaussian_process gaussian process regressor	0.058824
wrapped function cache result	joblib memorized func	0.014706
embedding analysis on the	embedding	0.040000
calculate the posterior log probability of the samples	nb joint log likelihood	0.066667
of the dual gap convergence criterion the	covariance dual gap emp_cov	0.071429
search over parameters	core base search cv	0.033333
returns a nicely formatted statement displaying the	args kwargs object_name	0.166667
leaves	get leaves	0.111111
graph of the	img to graph	0.333333
submatrix	submatrix	0.454545
model to data matrix x and target y	neural_network base multilayer perceptron partial	0.166667
timestamp when pickling to avoid the hash	externals joblib memory reduce	0.030303
data and stored it as a zipped pickle	target_dir cache_path	0.142857
randomized linear models for	randomized linear model	0.076923
estimate model parameters with the	mixture base mixture fit x y	0.200000
versus all others	multiclass x y	0.166667
factor analysis fa a	factor analysis	0.166667
that target y is of a non-regression type	utils check classification targets y	1.000000
updates terminal regions	terminal region	0.100000
if the test/test sizes are meaningful wrt to	model_selection validate shuffle split n_samples test_size train_size	0.111111
actual data loading for the lfw people	datasets fetch lfw people	0.040000
the linear assignment problem using the hungarian	utils linear assignment	0.090909
building a cv in a user	check cv cv x y	0.031250
wise scale	scale	0.066667
+ fp where tp is the number	score y_true y_pred labels pos_label	0.027778
a new ndarray with aligned memory	utils aligned	0.500000
copy_x	copy_x	0.625000
list of exception types to	externals joblib	0.004762
the decision function of	decision function x raw_values	0.083333
calculate the posterior log probability of the	multinomial nb joint log likelihood	0.083333
features are selected returns	feature_selection	0.066667
predict the target	cv predict	0.041667
and y	y alpha	0.111111
a raw file object e g created	raw file	0.200000
elastic net path	path	0.025641
mi	mi	0.833333
by scaling each feature	scale x	0.043478
count	multinomial nb count x	1.000000
from a	a size replace p	0.142857
kernel between x and y	kernel x y	0.500000
check that predict is	utils check clusterer compute labels predict	0.250000
to avoid the hash	memory	0.015625
the number of splitting iterations in	model_selection leave pgroups out get n splits	0.111111
k x y and optionally its gradient	gaussian_process dot product call x y eval_gradient	0.333333
creates a biclustering for	base spectral fit	0.250000
and inertia using a full distance	inertia precompute dense x x_squared_norms centers distances	0.250000
distance of the samples x	function x	0.030303
for a calibration	core calibration	0.125000
decision tree	tree	0.071429
hide warnings without visual nesting	warnings call fn	0.200000
single binary	predict binary	0.200000
incremental fit on a batch of	gaussian nb partial fit x y classes sample_weight	0.200000
the ardregression model according to	linear_model ardregression	0.100000
to partition estimators	ensemble partition estimators	0.200000
w to	x w ht	0.250000
by a random projection p only changes the	johnson lindenstrauss min dim n_samples	0.142857
hence	patch extractor fit	1.000000
varies for mono and multi-outputs	x y eps n_alphas	0.250000
samples can be different from	core calibrated classifier	0.083333
is not found and raise an exception	search	0.019231
aligned memory	aligned	0.076923
generalized cross-validation it allows efficient leave-one-out cross-validation	gcv	0.166667
coefficient matrix to	sparse	0.025000
the svmlight / libsvm	svmlight file f n_features dtype	0.066667
false positives per binary classification threshold	binary clf curve y_true	0.090909
matern kernel	matern	0.166667
fit a multi-class classifier	linear_model base sgdclassifier fit	0.076923
diagonal of the kernel k	exponentiation diag	1.000000
model with passive aggressive algorithm	linear_model passive aggressive regressor	0.250000
can also predict based	predict	0.006849
spherical wishart	wishart spherical	0.333333
routine for validation and	directed dtype	0.166667
the file descriptor for the underlying file	joblib binary zlib file fileno	0.333333
find the first prime element	utils hungarian state find prime	0.500000
cross-validated	val predict estimator x y cv	0.071429
hash depending from	joblib memorized	0.015625
the trained	multilayer perceptron	0.071429
a locally linear embedding analysis on the data	locally linear embedding x	0.071429
sparse	decomposition sparse	0.222222
coefficient matrix	linear_model	0.025641
list of edges	make edges	0.066667
locally linear embedding analysis on the data	locally linear embedding x n_neighbors	0.071429
for parameter value indexing	model_selection index param value	0.200000
sure that an estimator implements the necessary methods	core check estimator estimator	0.142857
shortest path length	single source shortest path length graph	0.333333
is not found and raise	line search	0.029412
terminal regions to median estimates	terminal region tree terminal_regions leaf x	0.066667
neighbors within a	neighbors lshforest radius neighbors x	0.166667
probabilities of	proba	0.088235
cross	cross	0.185185
the generative model	base pca get	0.076923
array with block checkerboard structure	checkerboard shape n_clusters noise minval	0.066667
to cleanup a temporary folder if still existing	delete folder folder_path warn	0.250000
each input	val predict estimator	0.045455
reconstruct the array	ndarray wrapper read unpickler	0.333333
with stochastic gradient descent	linear_model base sgdregressor	0.250000
covariance model according to	covariance	0.014493
find the first prime element in the	find prime in	0.333333
along any axis center to the mean	axis	0.014085
to split data into	kfold split x	0.250000
the process	multiprocessing backend	0.038462
fits the shrunk covariance model according	covariance shrunk covariance fit	0.083333
and compute prediction of init	gradient boosting init decision function	0.142857
estimates for	core cross val	0.043478
is the depth a which this	externals joblib memorized func check previous func code	0.055556
log probability for full covariance matrices	mixture log multivariate normal density full x means	0.333333
empty the	joblib memorized func clear warn	0.250000
graph of neighbors for points in x	radius neighbors mixin radius neighbors graph x	0.500000
list of exception types to	get	0.012048
inplace row scaling of a csr	inplace row scale x scale	0.142857
the parameters for the voting classifier valid parameter	ensemble voting classifier	0.031250
log probability for full covariance	log multivariate normal density full	0.333333
expression of the dual gap convergence criterion	dual gap emp_cov	0.071429
a name	get func name	0.047619
sample from the	bernoulli rbm sample	0.500000
estimate the spherical wishart	gaussian mixture estimate wishart spherical nk xk sk	0.333333
returns the number of splitting iterations in the	model_selection leave pgroups out get n splits x	0.111111
fit linear model with passive aggressive algorithm	passive aggressive classifier fit x y coef_init	1.000000
for the voting classifier valid	ensemble voting classifier set	0.037037
for parallel processing	joblib parallel	0.028571
computes the	metric	0.071429
arbitrary python object	filename	0.050000
perform dbscan clustering from vector	cluster dbscan	0.125000
fit an estimator within	fit estimator estimator x y sample_weight	0.071429
the number of splitting iterations in the cross-validator	leave one group out get n splits x	0.111111
is	utils is	1.000000
function used to fit an estimator within	fit estimator estimator	0.055556
shortest path length from source to all reachable	shortest path length graph source cutoff	0.111111
used to build a batch of estimators within	build estimators n_estimators	0.166667
generate a signal	signal n_samples	1.000000
the model and transform with	transform x	0.016949
importances the	importances	0.100000
by the	base	0.014286
read an array using numpy memmap	externals joblib numpy array wrapper read mmap	1.000000
to split data	split	0.055556
the svmlight / libsvm format	svmlight file f n_features dtype multilabel	0.066667
absolute error regression loss read	absolute error	0.142857
with respect to coefs	n_samples activations deltas	0.166667
for building a cv in a user	check cv cv x y	0.031250
axis center to the median and component wise	axis	0.014085
in bytes_limit	memory	0.015625
compute the	mixture compute log det	1.000000
described in friedman [1] and breiman [2]	friedman3 n_samples noise	0.166667
fit	linear svr fit x	0.333333
the number of splitting iterations in	model_selection leave pgroups out get n splits x	0.111111
i	i data	1.000000
factor of	factor decision function	0.500000
matthews	matthews	1.000000
training set according to the	fit	0.003257
types	externals	0.005747
for a fit	fit rfe estimator x y	0.166667
return the score for a fit	fit rfe estimator x	0.166667
format check	allocation check	0.062500
cca canonical correlation analysis	cca	1.000000
perform classification on an array of	nearest centroid predict	0.142857
grid of	grid	0.040000
updating terminal regions (=leaves)	terminal region tree terminal_regions leaf	0.066667
mean update and	mean	0.035714
the l1 distances between the	paired manhattan distances	0.083333
windows cannot encode some characters in	clean win chars string	0.333333
for each input	cross val predict estimator x	0.045455
fit label binarizer	preprocessing label binarizer fit	0.500000
the graphlasso	graph lasso cv fit	0.333333
estimates for each input	val predict estimator x	0.045455
the california housing dataset from	datasets fetch california housing	0.083333
sparse and dense	y	0.002674
each input data point	estimator x y	0.038462
input and compute prediction of init	boosting init	0.142857
first and last element of numpy	first and last element arr	0.200000
the parameters of the votingclassifier	params deep	0.111111
and breiman [2]	make friedman3 n_samples noise	0.166667
meant to be cached by	index_file_path data_folder_path slice_ color	0.033333
not enabled for	robust	0.090909
train test indices	base shuffle split iter indices	0.250000
cross-validated	cross val predict estimator x y cv	0.071429
autocorrelation parameters theta as the maximizer	gaussian_process gaussian process arg max	0.047619
and evaluates the reduced likelihood	process reduced likelihood	0.142857
the voting classifier	voting classifier	0.035714
embedding analysis	embedding x	0.200000
trace of np dot	decomposition trace dot	1.000000
of exception	base get	0.066667
the variational lower bound for the mean parameters	mixture dpgmmbase bound means	0.250000
or too common features	feature_extraction count vectorizer limit features x vocabulary high	0.250000
we don't store the timestamp when pickling to	joblib memory reduce	0.030303
under the curve auc using the trapezoidal	metrics auc x	0.040000
and configure a	estimator append random_state	0.142857
linear model with stochastic gradient descent	coef_init intercept_init	0.333333
maximum along an axis on a	max axis x axis	0.142857
used to fit a single tree	trees tree	0.142857
grid of alpha values	alpha grid x	0.166667
cv	cv	0.054054
content of the data home	data home data_home	0.055556
orthogonal matching pursuit omp solves n_targets orthogonal	linear_model orthogonal mp x	0.250000
selected features	feature_selection	0.200000
values for a given	y train	0.166667
connectivity	connectivity	1.000000
under a	externals joblib	0.004762
don't store	func	0.011364
the laplacian kernel between x and y	metrics laplacian kernel x y gamma	0.333333
columns of a csc/csr matrix in-place	utils inplace swap column	0.250000
fits the oracle approximating shrinkage covariance model according	covariance oas	0.083333
log of	sgdclassifier predict log	0.500000
theta as the maximizer of the reduced likelihood	gaussian process arg max reduced likelihood	0.250000
separating hyperplane	svm base lib svm decision function	0.333333
of new samples can be different from	core calibrated classifier	0.083333
and false positives per binary classification	binary clf curve y_true y_score pos_label	0.090909
explained	explained	1.000000
bernoulli	bernoulli	0.857143
with	add	0.071429
implement a single boost	ada boost classifier boost iboost x y	1.000000
for c in (l1_min_c infinity)	c x	0.030303
base class for bagging meta-estimator	base bagging	1.000000
and predicted probabilities for a calibration	core calibration	0.125000
implement a single	iboost x	1.000000
function for factorizing common classes param logic estimators	fit first call clf classes	0.058824
extracts patches of any n-dimensional	feature_extraction extract patches	0.083333
providing transparent zlib de compression	binary zlib	0.166667
terminal regions to	terminal	0.047619
returns the number of splitting iterations in the	model_selection predefined split get n splits x	0.111111
comp_cov	comp_cov	1.000000
binary classification used in hastie	make hastie	0.125000
trustworthiness	trustworthiness	1.000000
implement a single boost	boost iboost	1.000000
parameters for the voting classifier	ensemble voting classifier	0.031250
print verbose message on the end of iteration	print verbose msg init end ll	0.333333
fit_binary	linear_model prepare fit binary est	1.000000
fit an estimator within	fit estimator estimator	0.055556
of data under each gaussian	mixture gmmbase	0.034483
line_search_wolfe2 if suitable step length is not found	line search wolfe12 f fprime xk pk	1.000000
avoid the hash depending from	externals joblib memorized func	0.013158
the long type introduces an 'l'	utils	0.009709
element in the specified row	in row row	0.250000
lrd the lrd of a sample is the	distances_x neighbors_indices	0.047619
inplace row scaling of	utils inplace row scale x scale	0.142857
to the cache for the function	joblib memorized func	0.014706
of a function	joblib delayed function	0.200000
to split data into training and test set	series split split x y groups	0.200000
parameters and evaluates the reduced likelihood function for	process reduced likelihood function	0.047619
the weighted graph of neighbors	radius neighbors mixin radius neighbors graph	0.066667
scale back the data to the original	robust scaler inverse transform	0.066667
load the kddcup99 dataset downloading it if	fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
estimates for each input data point	predict estimator x y	0.045455
the maximum absolute value to	max abs	0.047619
compute a logistic regression	logistic regression path	0.333333
least squares	ensemble least squares error	1.000000
of the data home	data home data_home	0.055556
under each gaussian in the model	mixture gmmbase	0.034483
for each input data	predict estimator	0.045455
input data point	estimator	0.014706
lars algorithm	lars	0.090909
x y as training data	x y copy_x	0.333333
the arguments of a function	externals joblib delayed function	0.200000
predict apply	predict	0.013699
call	call	0.421053
the pairwise matrix	metrics parallel pairwise x	0.166667
data	cross val predict estimator	0.045455
forest	forest	0.692308
a lower bound on	dpgmmbase lower bound	0.071429
dual gap convergence criterion the specific	dual gap emp_cov precision_	0.071429
l1-penalized	graph lasso	0.500000
read more in the :ref user guide <image_feature_extraction>	patch extractor	0.090909
a parallelbackend which	backend	0.033898
false for indices increasingly apart the distance	verbosity filter index	0.055556
apply clustering to a projection to the normalized	spectral clustering	0.142857
to cleanup a temporary folder if still existing	delete folder folder_path	0.250000
given arguments	externals joblib memorized func	0.013158
the long type introduces	utils shape	0.013699
underlying	core one vs one classifier	0.111111
predict class or regression value for x	base decision tree predict x check_input	1.000000
center	center scale xy	1.000000
log probability for full	log multivariate normal density full x means covars	0.333333
l1 and l2 regularization coefficients for w	regularization alpha l1_ratio regularization	0.250000
compute area under the curve auc using the	metrics auc x	0.040000
observations in x according	x	0.001692
list of module names and a name for	func name	0.047619
length from source to all reachable nodes	length graph source	0.200000
returns the number of splitting iterations in	get n splits x	0.111111
ward clustering based on a	cluster ward tree x connectivity n_clusters return_distance	0.250000
line_search_wolfe2 if suitable step length is not found	search wolfe12 f fprime xk pk	1.000000
a locally linear embedding	manifold locally linear embedding x n_neighbors	0.071429
fit the kernel density model on the data	neighbors kernel density fit x y	0.250000
the autocorrelation parameters theta as the maximizer	gaussian_process gaussian process arg max	0.047619
each input data	x y	0.002155
of vectors for reproducibility flips the sign	utils deterministic vector sign	0.066667
matrix	sparse	0.025000
in the wild lfw pairs dataset this dataset	datasets fetch lfw pairs	0.018868
fit kernel ridge regression model parameters	core kernel ridge fit x	1.000000
binary classification used in hastie et al	hastie 10 2 n_samples random_state	0.166667
folders to make cache size fit in bytes_limit	size	0.032258
used to partition	partition	0.100000
to x using the	transform x	0.016949
numpy	joblib numpy	0.250000
the shortest path length	single source shortest path length graph	0.333333
generates data for binary classification used in hastie	hastie	0.076923
compute	perceptron compute	0.250000
test indices	shuffle split iter indices	0.250000
the laplacian kernel	laplacian kernel	0.166667
the shortest path length	source shortest path length graph	0.333333
run fit on	model_selection fit grid	1.000000
indices in sorted array of integers	matching indices tree bin_x left_mask right_mask	0.166667
of two clusterings	score labels_true labels_pred sparse	0.047619
curve auc from prediction scores note this implementation	metrics roc auc	0.166667
bagging meta-estimator	bagging	0.125000
hash depending from it	memory	0.015625
logistic regression model	linear_model logistic regression path x	0.333333
the number of splitting iterations in	pgroups out get n splits x	0.111111
load the 20 newsgroups dataset and	datasets fetch 20newsgroups	0.333333
estimators that implement the	utils check	0.023810
elastic net model	elastic net	0.111111
the hash depending from	externals joblib memorized func	0.013158
with_mean	with_mean	1.000000
callable case	metrics pairwise callable	0.083333
spherical	spherical nk	1.000000
for indices increasingly apart the distance depending on	externals joblib verbosity filter index	0.055556
returns the transformed	w h	0.031250
data and	x y sample_weight	0.012987
outlier factor	outlier factor decision	0.500000
if dtype of x and y is float32	x y	0.002155
coefficient of determination	metrics r2	0.125000
auc	auc	0.102041
of determination regression score	metrics r2 score y_true y_pred sample_weight	0.125000
abort	backend base abort	1.000000
structure for	shape n_clusters noise minval	0.333333
on the training set according to the	fit	0.003257
cache folders	reduce	0.017241
compute the deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call y pred sample_weight	0.333333
a sparse combination of the dictionary atoms	decomposition sparse coding mixin transform x y	0.333333
add documentation	add doc func doc	1.000000
"returns the	multi output classifier score x	0.250000
element in	in	0.090909
lad updates terminal regions to median estimates	least absolute error update terminal region	0.200000
whether the kernel is	kernel operator is	0.333333
x (as bigger is	decision function x	0.018868
each input data point	val	0.037037
the data home	clear data home data_home	0.076923
fit linear model with passive aggressive algorithm	linear_model passive aggressive classifier fit x	1.000000
computes	w x y alpha	0.250000
check	neighbors check	0.333333
single boost using the	boost	0.062500
reduced likelihood function	reduced likelihood function	0.083333
neighbors	radius neighbors mixin radius neighbors	0.125000
spectrum spectrum	spectrum n_samples n_features	0.166667
handle the callable case for pairwise_{distances	metrics pairwise callable	0.083333
check according to li et al	core check	0.111111
the covariance	matrix to match covariance	0.250000
a mostly low rank matrix with bell-shaped singular	low rank matrix	0.083333
in the wild lfw	lfw	0.034483
of the data home	data home	0.076923
estimator is using	core	0.015385
fit the hierarchical	feature agglomeration fit x y	1.000000
for mono and	x y	0.002155
for a calibration curve	calibration curve y_true y_prob normalize n_bins	0.142857
l1	paired manhattan	0.333333
number of splitting iterations in the cross-validator	predefined split get n splits	0.111111
generate a random n-class classification problem	make classification n_samples n_features n_informative n_redundant	0.500000
of array-like or scipy	preprocessing binarize x	0.083333
indices	shuffle split iter indices	0.250000
of csgraph inputs	validate graph csgraph directed	0.250000
fit ridge regression	ridge fit x	1.000000
fit the model to	output estimator fit	0.200000
best found parameters	base search cv predict	0.076923
default backend used by parallel	parallel backend backend n_jobs	0.166667
count and	nb count x y	0.250000
leave one group out cross-validator provides	leave one group out	0.200000
log probability	mixture log	0.500000
coefficient of determination regression score function	r2 score y_true y_pred	0.125000
neural_network	neural_network	0.714286
number of splitting iterations in the cross-validator parameters	model_selection cviterable wrapper get n splits	0.111111
score on	score x	0.033333
fit	lars fit	1.000000
to a function	externals	0.005747
regression gpr	regressor	0.027027
a cv in a	core check cv cv x y	0.031250
fit the	linear svc fit x	0.333333
regression score	score y_true	0.058824
compute cosine similarity between samples	metrics cosine similarity	0.333333
scale back the data	preprocessing standard scaler inverse transform x copy	0.066667
the backend and return the	externals joblib parallel backend	0.029412
print verbose message on the	mixture base mixture print verbose msg init	0.333333
validity	metric p metric_params	0.100000
for validation and conversion of csgraph	utils sparsetools validate graph csgraph directed dtype csr_output	0.166667
model we can also predict based	predict x	0.011765
given type	customizable pickler register type	0.333333
parameter weights and	x y	0.002155
dense dictionary factor in	dictionary y	0.111111
of a csr matrix in-place	utils inplace swap row csr	0.250000
fit is on grid of	fit	0.003257
for each input	core cross	0.045455
the timestamp when pickling to avoid the hash	reduce	0.034483
min_covar	min_covar	1.000000
matrix x	x	0.003384
build a batch of estimators within a	build estimators n_estimators ensemble x y	0.166667
autocorrelation parameters theta as the maximizer of the	arg max	0.047619
linear	sgdregressor	0.181818
the separating hyperplane	lib svm decision function	0.333333
of neighbors	neighbors	0.054054
a locally	manifold locally	0.333333
true and false positives per binary classification	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
rand index adjusted for chance	adjusted rand score	0.333333
class at each stage for	classifier staged	0.333333
check input and compute prediction of init	init decision function	0.142857
make a large circle containing a smaller circle	datasets make circles n_samples shuffle noise random_state	1.000000
number of splitting iterations in the	base kfold get n splits x y	0.111111
matrix factorization nmf	factorization	0.035714
from features or	sample_weight	0.018519
along any axis center to the median	x axis	0.015385
base class for kfold groupkfold and stratifiedkfold	base kfold	1.000000
paired cosine distances between x	metrics paired cosine distances x	0.333333
k-neighbors of a	neighbors kneighbors mixin kneighbors x n_neighbors	0.125000
the linear assignment problem using the	utils linear assignment	0.090909
an affinity matrix for x using the	x y	0.002155
kl divergence	kl divergence	0.083333
dataset along any axis	axis	0.028169
mlp loss function and its	multilayer perceptron	0.071429
generate a grid	ensemble grid	0.111111
that for c in (l1_min_c infinity)	c x	0.030303
the maximum likelihood covariance estimator parameters	covariance empirical covariance x assume_centered	0.166667
maximizer	gaussian_process gaussian process arg max	0.047619
two sets of biclusters	consensus score	0.250000
inplace row	utils inplace row scale	0.142857
of exception types to be captured	parallel backend base get exceptions	0.166667
wrapped function cache result and return	joblib memorized func	0.014706
matching pursuit omp solves n_targets orthogonal matching pursuit	linear_model orthogonal mp x y n_nonzero_coefs	0.200000
to update terminal	update terminal	0.142857
the kl divergence of p_ijs and	kl divergence	0.083333
the test_size and train_size at	test_size train_size	0.200000
setting the parameters for the voting classifier valid	voting classifier set	0.037037
matrix	coef	0.058824
'l' suffix when	shape	0.011765
diagonal of the kernel	kernel diag	1.000000
generate indices to split data into	series split split	0.250000
corresponds to the area under	y_true y_score	0.027027
centroid	centroid	0.833333
cohen's kappa a	metrics cohen kappa	0.250000
the callable case for pairwise_{distances	callable x y	0.083333
fit ridge regression model	linear_model ridge classifier fit x	1.000000
contingency	contingency	1.000000
log probability for full	mixture log multivariate normal density full x means	0.333333
a dataset along any axis center	x axis	0.030769
classification task	curve y_true	0.125000
fit ridge regression	ridge classifier fit x y	1.000000
input data point	core cross val predict	0.045455
module names and a name for	name	0.033333
random matrix generation	input size n_components n_features	0.200000
of the dual gap convergence criterion the	dual gap emp_cov precision_ alpha	0.071429
dataset is constructed by taking a	datasets	0.015152
and configure a copy	append random_state	0.142857
the grid of alpha values	linear_model alpha grid x	0.166667
multinomial loss and class probabilities	multinomial loss w x y	0.333333
the score of the	score	0.010101
on the estimator with the best found	base search cv	0.052632
collect results from clf predict calls	collect probas	1.000000
function func with arguments *args and **kwargs, in	func	0.011364
the cf	birch	0.125000
back the	preprocessing standard scaler inverse transform	0.066667
compute decision function of x	decision function x	0.037736
partition estimators between jobs	ensemble partition estimators n_estimators n_jobs	0.200000
text report showing the main	report	0.047619
the	ensemble base	0.333333
ardregression model according to	linear_model ardregression	0.100000
constructs a transformer from an arbitrary callable	function transformer	0.333333
a cv in a user friendly	check cv cv x	0.031250
check_pickle	check_pickle	0.200000
a memmap instance to reopen on same file	externals joblib reduce memmap a	0.050000
process or	joblib	0.007299
the hierarchical clustering on the data	cluster feature agglomeration	0.125000
the posterior log probability of the	bernoulli nb joint log likelihood	0.083333
the derivative of the logistic sigmoid	inplace logistic derivative z delta	0.166667
underlying estimators should be	one vs one classifier	0.125000
this operation is meant to be cached by	data_folder_path slice_ color resize	0.033333
x	fit predict x	0.250000
given parameter	parameter	0.083333
vocabulary dictionary and return term-document	feature_extraction count vectorizer fit transform raw_documents y	0.166667
input checker utility for building a cv in	check cv cv x	0.031250
as the maximizer of	arg max	0.047619
of an array shape under python 2	shape	0.011765
fit with all	search cv fit x y	0.111111
to a large sparse linear system of equations	utils lsqr a b damp atol	0.500000
position of the	mds fit	0.066667
all kernel operators	kernel operator	0.142857
compute the decision function of	decision function	0.025000
return the directory in which are	dir	0.038462
error between	error	0.020000
fit estimator and compute scores for a	core fit and score estimator x	0.333333
to dense array format	sparse coef mixin densify	0.100000
training set according	factor fit	0.062500
the decision function of x	ada boost classifier decision function x	0.333333
fit naive bayes classifier according to x y	discrete nb fit x y sample_weight	1.000000
get the directory corresponding to the cache	joblib memorized func get func dir mkdir	0.500000
building a cv	cv cv x y classifier	0.031250
and false positives per binary classification	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
the covertype dataset downloading it if necessary	datasets fetch covtype data_home download_if_missing random_state	0.333333
the svmlight / libsvm format into	svmlight file f	0.066667
opening the right fileobject	fileobject fileobj	0.200000
if suitable step length is not found	line search	0.029412
full covariance matrices	full x means covars	0.166667
helper to workaround python 2 limitations	utils parallel helper obj methodname	0.333333
project data to vectors and cluster the result	spectral biclustering project and cluster data vectors n_clusters	0.333333
fit the model from data in	manifold spectral embedding fit	0.333333
win	win	1.000000
regression target	regressor	0.027027
check	latent dirichlet allocation check	0.062500
best found parameters	search cv predict	0.074074
ward clustering	cluster ward tree x	1.000000
equal to the average path length	ensemble average path length	0.090909
cross-validated least angle regression model read more in	lars cv	0.333333
module names and a name for the	func name	0.047619
and scale the	transform x y	0.031250
the process or thread pool	joblib	0.007299
k-neighbors	kneighbors	0.125000
determinant	det fit	0.333333
types	externals joblib parallel	0.014085
is	y_true y_pred beta labels	0.500000
pairwise matrix	parallel pairwise x y	0.166667
initial centroids parameters	cluster init centroids	0.166667
independent representation of	utils	0.009709
whose range approximates the range	utils randomized range	0.083333
tp + fp where tp is the number	score y_true y_pred labels pos_label	0.027778
the gradient boosting	ensemble base gradient boosting	0.222222
log probabilities within	parallel predict log proba	0.058824
predict the	predict x	0.035294
fitted model parameters	fit x y	0.005988
w h whose product approximates the	w h n_components	0.038462
to avoid	func	0.011364
scale back the data	inverse transform	0.062500
the score for a fit	fit rfe	0.166667
initialize the model parameters of	mixture base mixture initialize	0.333333
calculate mean	incremental mean	0.166667
of the local outlier factor of	neighbors local outlier factor decision	0.125000
read an array using numpy	numpy array wrapper read	1.000000
a fit across one fold	feature_selection rfe single fit rfe estimator x	0.200000
data as a sparse	sparse	0.025000
func to be	manager mixin apply async func	0.250000
random	random choice	0.166667
slices going up	even slices	1.000000
avoid	externals joblib	0.009524
fit label encoder	label encoder fit transform	1.000000
two clusterings matching 1d integer arrays	clusterings labels_true labels_pred	0.500000
lasso_stability_path	linear_model lasso stability path x y	1.000000
remove cache	joblib	0.007299
on	dbscan fit predict	0.333333
the decision functions of the	decision function x	0.018868
determine the optimal	joblib auto batching mixin compute	0.333333
to ensure deterministic output from svd	svd	0.071429
tolerance which	tolerance	0.045455
calculate approximate perplexity for data x	decomposition latent dirichlet allocation perplexity x doc_topic_distr	1.000000
read value from	result	0.166667
dimensions	dimensions	0.750000
to fit a single	build trees	0.142857
perform the	x responsibilities params min_covar	0.500000
restricted to the binary classification task	y_true y_score	0.054054
cf tree for the input data	cluster birch fit x	0.200000
full	multivariate normal density full x means covars	0.166667
update	base optimizer update	1.000000
estimate the precisions parameters of the precision distribution	mixture bayesian gaussian mixture estimate precisions	0.166667
disk	disk	1.000000
two sets of biclusters	cluster consensus	0.250000
building a cv	check cv cv x y classifier	0.031250
mean update	mean	0.035714
online computation of	partial fit	0.111111
single boost using	boost classifier boost	0.100000
fit the hierarchical clustering on the data parameters	cluster agglomerative clustering fit x	0.250000
a byte string to	externals joblib	0.004762
descriptors of a memmap instance to reopen	joblib reduce memmap a	0.050000
the output of transform	transform	0.011236
input	val	0.037037
export	export	1.000000
x according	inverse transform x	0.025641
computes the gradient and	y alpha	0.222222
true and false positives per	clf curve y_true y_score pos_label	0.250000
when	utils	0.009709
is described by its spectrum spectrum	spectrum n_samples n_features	0.166667
a name for	name	0.033333
hash depending	externals joblib memorized	0.013699
returns whether the kernel	gaussian_process stationary kernel mixin	1.000000
is in a multilabel format	is multilabel	1.000000
for	boosting classifier	0.250000
whose range approximates the range	range	0.058824
best class label for each sample in x	one vs one classifier predict x	1.000000
the callable case	metrics pairwise callable x	0.083333
median absolute error	median absolute error y_true	0.166667
callable case for pairwise_{distances	pairwise callable x	0.083333
fit the model and transform with	fit transform	0.100000
building a cv in a user friendly way	cv cv x	0.031250
sample weight array	sgd validate sample weight sample_weight n_samples	0.333333
residues on left-out data for a full lars	residues x_train y_train x_test y_test	0.083333
fit the model with	fit	0.009772
the logistic loss	logistic loss w x	0.500000
strip the headers by removing	strip	0.055556
fit linear model	linear_model base sgdclassifier fit x	0.333333
inplace column scaling of a csc/csr matrix	utils inplace column scale	0.166667
terminal regions (=leaves)	update terminal region tree terminal_regions leaf x	0.200000
a list of feature	vectorizer get feature	0.200000
configure a	estimator append	0.142857
generate indices to split data into	model_selection predefined split split x	0.250000
to the cache	memorized func	0.016949
placeholder for fit	fit x y	0.005988
fit the	svr fit x	0.333333
linear embedding analysis on	linear embedding x n_neighbors n_components	0.200000
private function used to compute decisions within a	parallel decision function estimators estimators_features x	0.500000
predicts one class versus all others	multiclass x	0.166667
median and component wise scale	robust scale	0.125000
scale back the data to the	preprocessing standard scaler inverse transform x copy	0.066667
recall	metrics recall	0.033333
inverse the transformation	cluster agglomeration transform inverse transform xred	1.000000
to fit a single	trees	0.083333
the euclidean or frobenius norm of x	utils norm x	0.333333
the callers	effective	0.090909
two rows of a csc matrix in-place	utils inplace swap row csc x m n	0.250000
underlying estimators should be used when memory is	core one vs one classifier	0.111111
scaling	scaler inverse transform	0.029412
routine for validation and conversion of csgraph	validate graph csgraph directed dtype csr_output	0.166667
the number of splitting iterations in the cross-validator	pgroups out get n splits x y	0.111111
maximizer of the reduced likelihood function	gaussian_process gaussian process arg max reduced likelihood function	0.333333
estimates for each input data point	core	0.015385
factorization nmf find two non-negative	non negative factorization x	0.043478
of regularization parameters	path x y pos_class cs	0.166667
and evaluates the reduced likelihood function for the	gaussian process reduced likelihood function	0.047619
check that predict raises an exception in	utils check estimators	0.142857
binary	binary score	0.500000
return staged predictions for	classifier staged predict	0.333333
stored it as a zipped pickle	target_dir cache_path	0.142857
predict using the trained model	base multilayer perceptron predict x	0.333333
region	region	1.000000
modified weiszfeld step	modified weiszfeld step x	1.000000
updates terminal regions	terminal region tree terminal_regions leaf	0.066667
em update	latent dirichlet allocation em step	0.500000
predict posterior probability of data	predict proba x	0.333333
points in	parameter	0.083333
estimates for each	cross val predict estimator x	0.045455
loss and	loss w x y	0.250000
remove cache	externals	0.005747
of the local outlier factor of x	local outlier factor decision function x	0.100000
of u	u	0.032258
parameters for the voting classifier valid parameter keys	voting classifier	0.035714
the maximizer of the reduced likelihood	gaussian process arg max reduced likelihood	0.250000
predict data using	predict x	0.011765
labels to binary labels the output of transform	transform	0.011236
load_content	load_content	1.000000
of exception	joblib parallel backend base get	0.066667
incrementally fit the	core multi output regressor partial fit x y	0.200000
derived class	x resp	0.166667
return whether the file supports seeking	zlib file seekable	0.250000
of the leaf	base decision tree apply	0.166667
an arbitrary python object into one	value filename	0.083333
is sometimes referred to by some authors as	preprocessing label binarizer	0.071429
the grid of alpha values for	alpha grid x	0.166667
augment dataset with an additional dummy feature	preprocessing add dummy feature x value	1.000000
set the	pipeline set	0.250000
the :ref user guide <mini_batch_kmeans>	mini batch kmeans	0.166667
fit lsi model	decomposition truncated svd fit	1.000000
split data into	split split x	0.250000
binary classification task	y_true	0.021739
thread	joblib multiprocessing	0.052632
mutual information between two continuous variables	mi cc x y n_neighbors	1.000000
directory in which are persisted the result of	get output dir	0.047619
estimator on training subsets incrementally	incremental fit estimator estimator x	0.500000
to accept precomputed	precomp distr	0.250000
training	factor fit	0.062500
the weighted graph of neighbors for points	mixin radius neighbors graph	0.066667
for a full lars path parameters	linear_model omp path	0.100000
count and smooth feature	core bernoulli nb count x y	0.250000
cv	core check cv cv x y classifier	0.031250
compute mean and variance along an axix	mean variance axis x axis	0.142857
to split data into training and test	base shuffle split split x y groups	0.200000
repeatedly solve m*x=b	iter inv	0.200000
'l' suffix	utils	0.009709
the array from the meta-information and the z-file	externals joblib zndarray wrapper read unpickler	0.043478
fit linear model with passive aggressive algorithm	linear_model passive aggressive regressor partial fit x y	1.000000
recall is the ratio	metrics recall	0.033333
a lower bound on model evidence based on	mixture dpgmmbase lower bound	0.071429
in bytes_limit	memory reduce	0.030303
assignment	assignment	0.555556
decision tree	decision tree	0.666667
approximates the range of	randomized range	0.083333
building a cv in a user friendly way	check cv cv x y	0.031250
maximum likelihood covariance estimator parameters	covariance empirical covariance x	0.166667
utility function opening the right fileobject from	externals joblib read fileobject fileobj	0.100000
and predicted probabilities for a calibration curve	calibration curve	0.142857
y_offset	y_offset	1.000000
determination regression score function	r2 score y_true y_pred sample_weight	0.125000
decision boundary for each class	one vs rest classifier decision function x	0.250000
compute labels and inertia using a full	labels inertia precompute dense x x_squared_norms centers distances	0.250000
a new ndarray	utils	0.009709
the initial centroids	cluster init centroids x k init	0.166667
isotonic regression model : min sum w[i]	isotonic regression	0.055556
store the timestamp when pickling	func reduce	0.050000
element of numpy array	element	0.083333
handle the callable case for pairwise_{distances	metrics pairwise callable x	0.083333
boost using	ensemble ada boost classifier boost	0.100000
a calibration curve	calibration curve	0.142857
each input	cross val predict estimator x	0.045455
total log probability under the	neighbors kernel density score x	0.333333
folders to	memory reduce	0.030303
compute prediction of init	boosting init	0.142857
compute elastic net path with coordinate descent	path	0.025641
the generative model	decomposition	0.047619
and scaling	x y	0.002155
to update terminal	error update terminal	0.500000
precision matrix with the generative	decomposition base pca get precision	0.066667
global clustering for the subclusters obtained after	global clustering	0.142857
random sample from	size	0.032258
determine the number of jobs which	backend effective n jobs n_jobs	0.333333
boost using the	ensemble ada boost classifier boost	0.100000
estimates for each input	estimator x	0.030303
initialize the model	mixture initialize	1.000000
the state	state	0.066667
of the loss	loss	0.027027
estimate the spherical wishart distribution	gaussian mixture estimate wishart spherical nk	0.333333
of the data under the model	mixture dpgmmbase score samples x	0.200000
compute the squared loss for	neural_network squared loss y_true	0.500000
returns the bound term related to proportions	dpgmmbase bound proportions z	0.333333
lfw people dataset this operation	fetch lfw people	0.040000
to avoid the hash depending from it	externals joblib	0.009524
cross-validated estimates for	cross val predict estimator x y cv	0.071429
with the best found parameters	search cv predict proba	0.076923
or lasso path using lars algorithm [1] the	linear_model lars path x y	0.100000
fit estimator and predict values for a given	model_selection fit and predict estimator x y train	0.250000
the number of splitting iterations in	split get n splits x	0.111111
array-like or scipy sparse matrix	preprocessing binarize x threshold copy	0.083333
one-vs-all fashion several regression	classes neg_label pos_label	0.333333
this classification dataset is constructed	datasets make	0.015625
density lrd the lrd of	density distances_x neighbors_indices	0.200000
class covariance matrix	class cov x y priors	0.250000
a truncated randomized svd	utils randomized svd m n_components n_oversamples n_iter	0.250000
the array from the meta-information	externals joblib zndarray wrapper read unpickler	0.043478
the case of a multinomial	multinomial	0.083333
right fileobject from a	read fileobject fileobj	0.100000
multilabel classification	make multilabel classification	0.166667
in x into a matrix	x	0.001692
number of splitting iterations in the cross-validator parameters	one out get n splits	0.111111
a covariance matrix shrunk	shrunk covariance	0.090909
classifier from the training set x	classifier fit x	1.000000
generate train test	shuffle	0.083333
the dual gap convergence criterion the	covariance dual gap	0.071429
decision function output for x relative to y_true	metrics threshold scorer call clf x	0.058824
estimators that implement the partial_fit api need	utils check partial	0.038462
fit a multi-class classifier by combining binary classifiers	sgdclassifier fit	0.076923
conversion of	csr_output	0.111111
predict is invariant of	clusterer compute labels predict	0.250000
a text report showing the main	report	0.047619
model from data in	decomposition mini batch sparse pca	1.000000
unfitted estimator	unfitted name estimator	0.142857
then return the score of the	rfe score	0.200000
be used for later scaling	scaler fit	0.153846
least-squares solution to a	utils lsqr a	0.037037
given training	core	0.015385
compute data	base	0.014286
as the maximizer of the reduced	arg max reduced	0.200000
write the function code and	func write func code	0.500000
of a logistic	logistic	0.047619
the california housing dataset from	fetch california housing	0.083333
load and return the wine dataset classification	datasets load wine return_x_y	1.000000
a tolerance which is independent of the	tolerance x	0.058824
exception types to	parallel backend	0.030303
possible score is	score y_true y_pred sample_weight multioutput	0.062500
the wild lfw pairs dataset this	datasets fetch lfw pairs	0.018868
generate cross-validated estimates for each	cross val predict estimator x y cv	0.071429
byte string to the file	joblib binary zlib file	0.066667
with the best found	core base search cv predict	0.076923
of neighbors for	neighbors radius neighbors	0.100000
tail_strength	tail_strength	1.000000
calibration with	cv	0.009009
decision	decision	0.472222
to the cache for	memorized	0.015873
fit a binary classifier on x and y	sgdclassifier fit binary x y alpha c	1.000000
position	mds fit x y init	0.066667
pickling reduction	externals joblib reduce	0.333333
and last element	and last element arr	0.166667
outlyingness of observations in	outlier detection mixin predict	0.250000
learn vocabulary and idf return	vectorizer fit transform raw_documents y	0.100000
20 newsgroups data and	20newsgroups	0.055556
value of the log of the	log	0.018868
and return	transform y	0.023256
return the kernel k x y and	exponentiation call x y	0.200000
precision	precision	0.116667
of jobs	jobs n_jobs	0.100000
train test	iter	0.050000
the hash depending from it	joblib	0.014599
of approximate nearest neighbors	neighbors lshforest	0.333333
of loss	loss	0.027027
to make cache size fit	externals joblib memory reduce size	0.083333
of the kl divergence of p_ijs and	kl divergence	0.083333
the process or thread pool	backend	0.016949
compute true and predicted probabilities for a calibration	calibration	0.071429
the complete cache directory	joblib memory clear warn	0.333333
loss	loss	0.351351
constructs signature from the	signature	0.047619
laplacian kernel	laplacian kernel	0.166667
given training data and	x y	0.002155
returns n_neighbors of approximate nearest neighbors	neighbors lshforest kneighbors x n_neighbors return_distance	1.000000
x as	fit x y iter_offset	0.333333
the search over	base search cv fit x y	0.166667
update h	h	0.041667
the search over parameters	core base search cv fit x	0.166667
error regression loss read more in	error y_true	0.111111
back the data to the	scaler inverse transform	0.058824
in multiplicative update nmf	multiplicative update	0.333333
validity	params x metric p metric_params	0.100000
blobs for clustering	make blobs	0.333333
homogeneity metric of a cluster labeling given a	cluster homogeneity	0.500000
low rank matrix with	make low rank matrix	0.083333
distr	distr	1.000000
the neighbors within a	neighbors lshforest radius neighbors x	0.166667
validity of	metric p metric_params	0.100000
one after the other and	y	0.002674
stratified k-folds cross-validator provides train/test indices to	stratified kfold	0.200000
solve the linear assignment problem using the hungarian	utils linear assignment x	0.090909
a platform independent representation	utils shape repr	0.013699
training set	fit predict	0.055556
lad updates terminal regions	least absolute error update terminal region	0.200000
if y -	y	0.002674
found parameters	base search	0.200000
func to be run	mixin apply async func	0.250000
position of the points in	mds fit x	0.066667
discrete	discrete	1.000000
for the lfw pairs	lfw pairs	0.018868
get a signature object for the passed callable	externals signature obj	0.200000
full covariance	density full x	0.166667
compute mutual information between two variables	compute mi x y x_discrete y_discrete	1.000000
global clustering for the subclusters obtained after	birch global clustering x	0.142857
compute decisions within a	estimators_features x	0.111111
test	split	0.027778
the largest k singular values/vectors for a sparse	a k ncv tol	0.166667
of the data onto the sparse components	sparse pca transform x ridge_alpha	0.200000
opening the right fileobject from	externals joblib read fileobject fileobj	0.100000
a locally linear	locally linear	0.200000
compute	base	0.014286
regression or lasso path using lars algorithm [1]	linear_model lars path x	0.100000
vector	vector	1.000000
voting classifier valid parameter keys can be listed	ensemble voting classifier set params	0.037037
routine for validation and conversion of csgraph inputs	utils sparsetools validate graph csgraph directed dtype csr_output	0.166667
best class label for each sample in	vs one classifier predict	0.500000
sets of biclusters	consensus	0.111111
lasso_stability_path	linear_model lasso stability path x	1.000000
elastic net parameter search parameters	y xy l1_ratio	0.250000
the decision function of	ada boost classifier decision function	0.166667
breiman [2]	friedman3	0.090909
standardize a dataset along any axis center	x axis with_centering with_scaling	0.333333
fit	svm linear svc fit x	0.333333
net path with coordinate descent the	linear_model enet path	0.050000
a sparse random	utils random choice	0.333333
fit	fit rfe estimator x	0.166667
lrd of	distances_x neighbors_indices	0.047619
with likelihood terms for standard covariance types	state log lik x initial_bound precs means	1.000000
duration	duration	1.000000
from prediction scores note this	metrics roc	0.040000
returns false for indices increasingly apart	joblib verbosity filter index	0.055556
binary	y_score average	0.111111
of cpus	cpu count	0.333333
the shrunk ledoit-wolf	ledoit wolf x assume_centered	0.250000
matrix x	x dict_type	0.200000
avoid the hash depending from it	joblib memorized	0.015625
to unit norm vector length	preprocessing normalize x norm axis copy	0.200000
normalize x by scaling	scale normalize x	0.142857
random projection p only changes	core johnson lindenstrauss min dim n_samples eps	0.142857
check the estimator and set the base_estimator_ attribute	ensemble ada boost regressor validate estimator	0.333333
using one-hot encoding	preprocessing one hot encoder	0.500000
transforms features by scaling each feature to	preprocessing minmax scale x feature_range axis copy	0.200000
kddcup99	brute kddcup99	0.166667
computes the position of the points in	mds fit	0.066667
return the kernel k x	gaussian_process compound kernel call x	0.333333
estimators that implement the partial_fit api need to	utils check partial fit	0.038462
message on	msg init	0.666667
logistic loss	linear_model logistic loss w x y	0.500000
of csgraph inputs	utils sparsetools validate graph csgraph directed	0.250000
embedding analysis on the data	embedding	0.040000
learn empirical variances from	feature_selection variance threshold fit	1.000000
check that predict is invariant	utils check clusterer compute labels predict	0.250000
in pipeline after transforms	core pipeline fit predict x y	0.166667
compute the weighted log probabilities for each sample	mixture base mixture score samples x	1.000000
youngs and cramer variance update	and var x last_mean last_variance last_sample_count	0.500000
memmap instance to reopen on	externals joblib reduce memmap	0.142857
normalize x according to kluger's log-interactions scheme	normalize x	0.076923
boost using the	ada boost classifier boost	0.100000
scale	preprocessing robust scale x	0.125000
fbeta	fbeta	1.000000
precision-recall pairs for different probability thresholds note this	probas_pred pos_label sample_weight	0.066667
score by cross-validation read more in	model_selection cross val score estimator x y	0.166667
curve of width	effective_rank tail_strength	0.125000
'l' suffix when using the	shape	0.011765
note this	metrics roc	0.040000
to multi-class	y threshold	0.166667
fit onehotencoder to x	preprocessing one hot encoder fit x y	1.000000
shortest path	shortest path	0.333333
leaves of	leaves	0.071429
process or thread	joblib	0.007299
y and class_weight	targets y	0.500000
compute out-of-bag scores	ensemble forest regressor set oob score	1.000000
compute the average log-likelihood	decomposition factor analysis score x y	0.333333
base class for random	base random	1.000000
we don't store	joblib memory	0.016949
check x format check x format	latent dirichlet allocation check	0.062500
load the kddcup99	fetch brute kddcup99	0.166667
generate the random projection matrix parameters	core gaussian random projection make random matrix n_components	1.000000
to	parallel backend base	0.037037
the position	mds fit	0.066667
approximates the range	range	0.058824
the normalization constant	logz v s dets	0.200000
shift	shift x	1.000000
transform binary labels back to multi-class labels parameters	binarizer inverse transform y threshold	0.333333
for c in (l1_min_c	c x	0.030303
fit	estimator fit x y sample_weight	0.200000
emp_cov	emp_cov	1.000000
k-fold iterator variant with non-overlapping groups	kfold	0.058824
of the gradient boosting	ensemble base gradient boosting	0.111111
returns false for indices increasingly apart the	verbosity filter index	0.055556
cv in a	cv cv x y classifier	0.031250
utility for building a cv in a user	core check cv cv x	0.031250
compute data precision matrix with	base pca get precision	0.066667
decision function	ensemble gradient boosting classifier decision function	0.166667
score by cross-validation read more in	model_selection cross val score	0.166667
handle the callable case for pairwise_{distances kernels}	pairwise callable x	0.083333
covariance matrix shrunk on the diagonal read	covariance shrunk covariance	0.090909
of transform is	transform y	0.023256
returns a list of edges for	make edges	0.066667
solution to a large sparse	a	0.018182
rare or too common features	feature_extraction count vectorizer limit features x vocabulary	0.250000
fit gaussian naive bayes	gaussian nb fit	1.000000
number of splitting iterations in	kfold get n splits	0.111111
net path with coordinate descent the	path x	0.045455
input and compute prediction of init	gradient boosting init decision	0.142857
binary logistic loss for classification	neural_network binary log loss y_true y_prob	0.500000
number of splitting iterations in the cross-validator	model_selection base cross validator get n splits x	0.125000
kernel k x y and	gaussian_process sum call x y	1.000000
cv	core check cv cv x	0.031250
of init	init decision function x	0.142857
lfw pairs dataset this dataset is	datasets fetch lfw pairs subset	0.035714
on the estimator with the best found	search cv	0.090909
for the lfw pairs dataset this operation is	datasets fetch lfw pairs	0.018868
determine the optimal	backend base compute	1.000000
the weighted graph of neighbors for	neighbors radius neighbors graph	0.066667
gaussian and label samples by quantile	gaussian	0.029412
logistic regression model	linear_model logistic regression	0.333333
cc	cc	0.833333
terminal	ensemble loss function update terminal	0.200000
back the data	robust scaler inverse transform	0.066667
cv in a user	cv cv	0.031250
the objective	covariance objective	0.125000
setting the parameters for the voting classifier valid	voting classifier	0.035714
it take to	joblib squeeze	1.000000
distances between the vectors in x and y	distances x y	0.285714
factorization nmf find two	non negative factorization	0.043478
the neighbors within a given radius	radius neighbors x radius	0.142857
long type introduces an 'l' suffix when	utils shape repr	0.013699
f-beta score is the weighted harmonic mean	metrics fbeta score y_true	0.333333
stratified k-folds cross validation iterator	stratified kfold	0.200000
display the message on stout	parallel print	0.142857
computes the mean squared error between two covariance	empirical covariance error norm comp_cov norm scaling squared	1.000000
fit to data then transform	core transformer mixin fit transform x	0.500000
split time series data samples	time series split	0.250000
make sure that	check	0.017857
row-wise squared euclidean norm of x	utils row norms x squared	1.000000
gradient	gradient	0.909091
smacof algorithm	manifold smacof	0.200000
dimension of a dataset of	dimension	0.050000
compute the centroids on x	fit x	0.006410
cross-validated estimates for each input data	y cv	0.050000
the recall	metrics recall	0.033333
from the file	file	0.035714
for indices increasingly apart the distance	verbosity filter index	0.055556
the number of splitting iterations in the cross-validator	get n splits	0.111111
for building a cv	check cv cv	0.031250
elastic net path with coordinate	linear_model enet path x	0.050000
least-squares solution to a large sparse linear	lsqr a	0.037037
mean squared error between two	error norm comp_cov norm scaling squared	0.166667
private function used to fit an estimator within	fit estimator estimator x y sample_weight	0.071429
used to partition estimators between jobs	ensemble partition estimators n_estimators n_jobs	0.200000
used by logistic regression and cv and linearsvc	svm fit liblinear x y c fit_intercept	0.142857
precision is the ratio tp	metrics precision	0.033333
diagonal of the laplacian matrix and	diag laplacian	0.111111
graph of	to graph	0.333333
estimates for each input data point	core cross val predict	0.045455
each sample	samples x labels metric	1.000000
by logistic regression and cv and linearsvc	liblinear x y c fit_intercept	0.142857
jaccard similarity coefficient score the	score y_true y_pred normalize sample_weight	0.125000
classification this function returns posterior probabilities of	cv predict proba x	0.034483
for full covariance matrices	normal density full x means covars	0.166667
search over	core base search cv fit x	0.166667
scale back the data	preprocessing standard scaler inverse transform	0.066667
perform dbscan	dbscan fit x	1.000000
log-likelihood of a gaussian data set with self	covariance score x_test	1.000000
the svmlight / libsvm format into sparse	svmlight file f n_features dtype	0.066667
calibration	calibration	0.500000
homogeneity metric of a cluster labeling given a	cluster homogeneity score	0.500000
the	utils	0.019417
false for indices increasingly apart	verbosity filter index	0.055556
make and configure a copy of the	base ensemble make estimator append random_state	0.166667
computes the weighted graph of neighbors for	mixin radius neighbors graph	0.066667
of exception	externals joblib	0.004762
all cross-validators implementations must define _iter_test_masks or _iter_test_indices	cross validator	1.000000
the position of the points in	mds fit x y	0.066667
make and configure	ensemble make estimator append random_state	0.166667
generate random samples from a gaussian distribution	sample gaussian mean covar covariance_type n_samples	1.000000
the shortest path length from source to all	shortest path length graph source cutoff	0.111111
a general function given points on a curve	y reorder	0.111111
the precision is the ratio tp /	precision	0.016667
for full covariance	multivariate normal density full x means	0.166667
transform on the estimator	transform x	0.016949
spherical wishart distribution	wishart spherical nk	0.333333
w	w x w	0.500000
cv	core check cv cv	0.031250
format	coef mixin	0.090909
incremental mean	mean	0.035714
area under the curve auc	auc	0.040816
list of exception types to be captured	backend base get exceptions	0.166667
insert a new subcluster into	insert cf subcluster subcluster	1.000000
a binary metric for multilabel classification parameters	metrics average binary score binary_metric y_true	0.500000
parameters and evaluates the reduced likelihood	process reduced likelihood	0.142857
check if there is any negative	utils check non negative x whom	0.333333
implement a single boost	ada boost classifier boost iboost x	1.000000
neighbors for	neighbors radius neighbors	0.100000
search over parameters	core base search cv fit x	0.166667
estimates for each input	cross val predict	0.045455
returns a lower bound on model evidence based	mixture dpgmmbase lower bound	0.071429
affinity	affinity	0.714286
submatrix corresponding to bicluster i	bicluster mixin get submatrix i	0.333333
restricted to the binary classification task	curve y_true	0.125000
predict regression target at each stage	regressor staged predict	0.500000
fit a single tree	trees tree forest	0.142857
collection of text documents into a scipy	vectorizer	0.022222
the unnormalized posterior log probability	base nb joint log likelihood	0.166667
kernel k x y and optionally its gradient	product call x y eval_gradient	0.333333
remove cache folders to	externals joblib memory reduce	0.030303
linear model with passive aggressive algorithm	linear_model passive aggressive classifier partial	1.000000
the estimator with the best found	search cv predict proba	0.076923
number of splitting iterations in the cross-validator	pgroups out get n splits	0.111111
explained variance regression score function best possible	metrics explained variance	0.166667
and set the base_estimator_ attribute	ensemble ada boost regressor validate	1.000000
transforms the image samples in x into	x	0.001692
non-negative matrix factorization nmf find two non-negative	negative factorization x	0.043478
for each input data point	estimator x y	0.038462
that implement the partial_fit	utils check partial fit	0.038462
score for a fit across one fold	feature_selection rfe single fit rfe estimator	0.200000
of neighbors for	neighbors	0.054054
return a platform	utils shape	0.013699
a decision tree regressor from the	tree decision tree regressor	0.166667
a parameter weights	weights dist weights	0.142857
parameters for the voting classifier valid parameter keys	voting classifier set	0.037037
data	estimator x y	0.038462
the index of the leaf	tree apply x	0.166667
private helper function for parameter value indexing	model_selection index param value x	0.200000
returns the number of splitting iterations in	cviterable wrapper get n splits x y	0.111111
synchronous ascending phase	neighbors lshforest get candidates query max_depth bin_queries n_neighbors	0.333333
solution to a large sparse	lsqr a	0.037037
with	pca get	0.076923
fit a binary classifier on x	linear_model base sgdclassifier fit binary x	1.000000
transform data back to its original space	decomposition nmf inverse transform w	1.000000
multinomial	linear_model multinomial	0.200000
explained variance regression score function best	explained variance	0.166667
sign of elements of all the vectors rows	sign flip	0.066667
local structure is retained	x x_embedded n_neighbors precomputed	0.200000
for this	deep	0.076923
all the covariance matrices from a given template	matrix to match covariance type tied_cv covariance_type	0.333333
similarity of two clusterings of a	score labels_true labels_pred	0.047619
return the kernel k	gaussian_process constant kernel call	0.333333
fit the	rbfsampler fit	0.250000
sequence	feature_extraction	0.037037
huber loss and the gradient	huber loss and gradient w x	0.333333
decision function of	decision function x	0.018868
people dataset this operation is meant	people data_folder_path slice_ color resize	0.333333
the initial centroids parameters	cluster init centroids x	0.166667
lad updates terminal regions to median	ensemble least absolute error update terminal	0.200000
the nonzero componentwise l1 cross-distances between	gaussian_process l1 cross distances	0.111111
actually run in parallel n_jobs is	n_jobs	0.023256
from the meta-information and the	zndarray wrapper read unpickler	0.043478
matrix to	linear_model	0.025641
to multi-class labels	threshold	0.076923
cross-validated estimates for each input data	core cross val predict estimator x y cv	0.071429
decision function of the given	decision function	0.025000
the log-likelihood	covariance score	0.071429
the wild lfw pairs dataset	datasets fetch lfw pairs subset	0.035714
patches of any	extract patches	0.083333
template method for updating terminal regions (=leaves)	terminal region tree terminal_regions leaf x	0.066667
insert	insert	0.857143
generate a random multilabel classification problem	make multilabel classification n_samples n_features n_classes	0.500000
finds all parameters ending	ensemble set random states	1.000000
labeled faces in the wild lfw pairs dataset	lfw pairs	0.018868
for c in (l1_min_c infinity) the	c x	0.030303
memmap instance to reopen	externals joblib reduce memmap	0.142857
gradient and the	w x y	0.133333
also predict based on	predict x	0.011765
subset	subset	1.000000
predict labels	predict x	0.011765
kappa a statistic that measures inter-annotator agreement	kappa score y1 y2 labels weights	0.500000
train test	base shuffle split iter	0.166667
can also predict based on an	predict	0.006849
of observations in x according to the fitted	x	0.001692
given type	pickler register type	0.333333
reconfigure	configure n_jobs parallel	0.200000
shutdown the process or	terminate	0.090909
compute decisions within a	estimators_features	0.071429
return the shortest path length from source to	shortest path length graph source cutoff	0.111111
check initial parameters	check parameters	0.200000
documents	feature_extraction count vectorizer fit raw_documents	0.125000
or regression value for x	x check_input	0.250000
returns false for indices increasingly apart	verbosity filter index	0.055556
fit gaussian process classification model	gaussian_process gaussian process classifier fit x	0.500000
arbitrary python object into one file	value filename	0.083333
for parallel processing this method	joblib parallel	0.028571
combination of the dictionary atoms	coding mixin transform x y	1.000000
reconfigure the backend and return the	externals joblib parallel backend base configure n_jobs parallel	0.333333
from features	sample_weight	0.018519
generates boolean masks corresponding to test sets	base cross validator iter test masks x y	1.000000
sigma	sigma	1.000000
full	density full x means	0.166667
compute decisions within	estimators_features	0.071429
thresholding of array-like or scipy sparse	binarize x threshold copy	0.083333
the wild lfw pairs	fetch lfw pairs	0.018868
on the estimator with the best found parameters	search cv	0.036364
local outlier factor of x	local outlier factor decision function x	0.100000
list of exception	externals	0.005747
smacof	smacof	0.700000
linear classifiers	linear classifier	1.000000
with the generative	decomposition	0.047619
by a random	n_samples	0.058824
outlier on the	outlier	0.100000
query based on	neighbors query	0.333333
objective	objective	0.461538
fit the model using x y as training	core isotonic regression fit x y	1.000000
apply clustering	cluster spectral clustering affinity n_clusters n_components	0.166667
the search over parameters	base search cv fit x	0.166667
arguments	func get	0.100000
the exponential chi-squared kernel x	metrics chi2 kernel x	0.333333
underlying estimators should be used when memory	core one vs one classifier	0.111111
validate user provided precisions	check precisions precisions covariance_type n_components	0.250000
computes the weighted graph of neighbors	radius neighbors mixin radius neighbors graph	0.066667
a single boost using	boost	0.062500
generate a random regression problem with sparse	make sparse	0.125000
fit label encoder and return encoded labels	preprocessing label encoder fit transform y	0.200000
compute k-means	kmeans fit x y	1.000000
in the :ref user guide <mean_absolute_error>	y_true y_pred sample_weight multioutput	0.100000
of feature name -> indices mappings	dict vectorizer fit	0.250000
x and dot w	beta divergence x w	0.500000
fit with	cv fit x	0.250000
a classification	y_true	0.021739
laplacian kernel between x	laplacian kernel x	0.333333
is restricted to the binary classification	y_true y_score average sample_weight	0.076923
the curve auc from	auc score	0.052632
vectors individually to unit norm vector length	preprocessing normalize x norm axis copy	0.200000
for factorizing common classes param logic	first call clf classes	0.058824
leave one group	leave one group	1.000000
a func to be	externals joblib sequential backend apply async func	0.250000
sometimes referred to by some authors	preprocessing label binarizer	0.071429
that implement the	utils check partial	0.038462
the f-beta score is the weighted harmonic mean	metrics fbeta score y_true y_pred beta labels	0.333333
fit a binary	fit binary	0.200000
from	y sample_weight	0.017857
x and	x y	0.008621
area under the curve auc using	metrics auc x	0.040000
multiple files in svmlight format this function	svmlight files files n_features	0.200000
confidence scores for samples	linear_model linear classifier mixin decision	0.500000
into a matrix of patch	patch extractor	0.090909
with the generative	get	0.012048
ledoitwolf estimator ledoit-wolf is a	ledoit wolf	0.111111
update the dense	decomposition update dict	0.333333
for building a cv in a user friendly	cv cv	0.031250
determination r^2	multi output regressor score	0.200000
inplace row	inplace row scale	0.142857
random regression problem with sparse uncorrelated design	sparse uncorrelated	0.166667
apply the derivative	derivative z delta	0.333333
reconfigure the backend and return the number	externals joblib parallel backend base configure n_jobs parallel	0.333333
squared euclidean or frobenius norm of	utils squared norm	0.500000
by scaling each	scale x	0.043478
contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true labels_pred	0.200000
estimators within a	estimators n_estimators ensemble x y	0.083333
filters the given args and kwargs using	func ignore_lst args kwargs	0.333333
unfitted	estimators unfitted	0.142857
solution to a large	utils lsqr a	0.037037
predict class at each stage	boosting classifier staged predict	0.500000
a	externals joblib binary zlib	0.125000
y_pred	y_pred	0.857143
private function used to fit an estimator	fit estimator estimator x	0.055556
computes the weighted graph of neighbors for	neighbors graph	0.066667
memory	memory	0.093750
shrunk on the diagonal read more in	shrunk	0.043478
returns the huber loss and the gradient	huber loss and gradient w x y epsilon	0.333333
global clustering for the subclusters obtained	birch global clustering	0.142857
pickle the descriptors of a memmap instance to	externals joblib reduce memmap a	0.050000
predict_log_proba on the estimator with the best found	search cv predict log proba x	1.000000
compute incremental mean	mean	0.035714
inefficient to train	classes	0.025641
returns the submatrix corresponding to bicluster i	bicluster mixin get submatrix i data	0.333333
binary classification task	score y_true y_score	0.025000
the boolean mask indicating which features are	support mask	0.125000
huber loss and the gradient	huber loss and gradient w	0.333333
inefficient to train all	classes	0.025641
of exception types to	joblib parallel backend base	0.058824
weighted graph of neighbors for points in	mixin radius neighbors graph	0.066667
and dot	divergence	0.090909
matrix factorization nmf find two non-negative matrices	factorization x	0.043478
estimate mutual information	feature_selection mutual info regression	0.500000
fit the model with	fit transform	0.100000
when using the	utils shape	0.013699
given arguments and persist the	func call	0.047619
grid of alpha values	linear_model alpha grid	0.166667
load csv file	datasets load csv f	1.000000
cache for the	joblib memorized	0.015625
cache	externals joblib	0.004762
labeled faces in the wild lfw pairs	fetch lfw pairs subset	0.035714
to each parameter weights and	x y	0.002155
dataset regression	datasets	0.015152
to unit variance	preprocessing	0.052632
extracts patches of any n-dimensional array	extract patches	0.083333
the hash depending	externals joblib memory	0.016949
add an item to six	externals add move move	0.333333
cholesky decomposition of	det cholesky	0.166667
voting classifier valid parameter keys can	ensemble voting classifier	0.031250
cv in a user friendly	core check cv cv x y	0.031250
and last element of numpy array	and last element arr	0.166667
estimates for each input data point	cross val predict estimator x	0.045455
scores note this implementation is restricted to	metrics roc	0.040000
given radius	x radius	0.058824
handles divide-by-zero	metrics prf divide numerator denominator metric modifier	0.250000
update	update w	1.000000
call transform on the estimator	transform x	0.016949
labels back to	inverse	0.055556
blobs	blobs	0.750000
tree classifier	tree classifier	1.000000
compute cosine similarity between samples in x	metrics cosine similarity x	1.000000
returns a lower bound on model evidence based	lower bound	0.071429
load the covertype dataset	datasets fetch covtype	0.333333
of the samples x to the separating hyperplane	svm base lib svm decision function x	0.250000
return the kernel k	gaussian_process compound kernel	0.333333
problem this dataset is described in	datasets make	0.015625
the case method='lasso' is	y xy gram	0.090909
fit the	tsne fit	0.333333
feature name -> indices mappings and transform x	dict vectorizer fit transform x y	1.000000
list of exception types	parallel backend	0.030303
determine absolute sizes of training	train sizes	0.066667
remove too rare or too common features	feature_extraction count vectorizer limit features	0.250000
array of distances and a parameter weights	weights dist weights	0.142857
estimates	core cross val predict estimator x	0.045455
fit a binary classifier on x	fit binary x	1.000000
of determination regression score	metrics r2 score y_true	0.125000
sample from a	a size	0.142857
set the intercept_	set intercept x_offset y_offset x_scale	1.000000
long	shape repr	0.013699
to fit an estimator within a job	parallel fit estimator estimator x y	0.333333
the provided 'means'	mixture check means means n_components n_features	0.333333
error	error y_true y_pred	0.125000
used to fit an estimator	fit estimator estimator x	0.055556
the hash depending from	externals joblib	0.009524
loader for the california housing dataset	california housing data_home	0.250000
k-neighbors	kneighbors mixin kneighbors x	0.125000
false for indices increasingly apart the	joblib verbosity filter index	0.055556
loading for the lfw pairs	fetch lfw pairs	0.018868
iterate over	feature_selection iterate columns	1.000000
verbose message on the	verbose msg init	0.333333
number of splitting iterations in the	out get n splits x	0.111111
of csgraph inputs	graph csgraph	0.250000
non-negative matrix factorization nmf find two non-negative	factorization x	0.043478
memory is inefficient to train	y classes	0.027778
curve auc from prediction scores note	roc auc	0.166667
minimum distances between one point	pairwise distances argmin	1.000000
apply transforms and	x y	0.002155
fit linear model with stochastic gradient descent	base sgdregressor partial fit x	1.000000
the best found	search cv predict	0.074074
meant	data_folder_path slice_ color resize	0.033333
the generative	decomposition base pca	0.071429
and score	score x y	0.090909
predict_proba on the estimator with the best found	base search cv predict proba	0.076923
windows cannot encode some characters in	externals joblib clean win chars string	0.333333
check that predict raises an exception in an	utils check estimators	0.142857
a single boost using the	classifier boost	0.100000
as the maximizer of the	arg max	0.047619
a raw file object e g created with	raw file	0.200000
dense dictionary factor	dict dictionary	0.111111
training set according to	fit predict	0.055556
the number of splitting iterations in	cross validator get n splits x y	0.125000
back the data to the	preprocessing robust scaler inverse transform	0.066667
make and configure a copy of the	make estimator append random_state	0.166667
estimate the precisions	bayesian gaussian mixture estimate precisions	0.166667
not found and raise an exception	utils line search	0.029412
generate a random multilabel classification	make multilabel classification n_samples n_features	0.500000
vectors for reproducibility flips the sign	deterministic vector sign	0.066667
descent fit is	fit x	0.006410
decision function of the	decision function x	0.018868
of max absolute value of	preprocessing max abs	0.050000
the estimator with the best found parameters	core base search cv predict proba	0.076923
check the validity of	neighbors check params x metric p metric_params	0.200000
test/test sizes are meaningful wrt to	validate shuffle split n_samples test_size train_size	0.111111
a helper class for managing	manager mixin	0.500000
the training set according	factor fit	0.062500
of exception types to	parallel backend base get	0.066667
a sparse random	utils random	0.333333
confidence scores	linear_model linear	0.500000
the end of iteration	end ll	0.166667
compute the number of	compute n	1.000000
a sparse random projection matrix	random projection fit x y	0.333333
generate train	core base shuffle	0.166667
unnormalized posterior log probability	base nb joint log likelihood	0.166667
compute non-negative matrix factorization nmf find	negative factorization x	0.043478
estimates for	predict estimator	0.045455
we	externals	0.011494
sparse uncorrelated design	sparse uncorrelated n_samples n_features random_state	0.166667
get	externals joblib get	0.142857
predict class probabilities at each stage for x	gradient boosting classifier staged predict proba x	1.000000
binarization transformation for multiclass	binarize multiclass y	1.000000
predict	decision tree predict	0.500000
hierarchical clustering on the data	cluster feature agglomeration	0.125000
check input and compute prediction of init	gradient boosting init decision function	0.142857
estimators within a	estimators n_estimators	0.083333
check if estimator adheres	utils check estimator estimator	0.250000
by class for unbalanced datasets	weight class_weight y indices	0.200000
r^2 coefficient of determination	r2	0.076923
restricted to the binary	y_score average	0.111111
of the breakdown	linear_model breakdown	0.333333
the significance of a cross-validated score with	score estimator x y cv	0.083333
the voting classifier valid	ensemble voting classifier set	0.037037
incremental mean and	utils incr mean	0.166667
to dense array format	linear_model sparse coef mixin densify	0.100000
linear models for feature selection this implements the	linear model	0.090909
calculates a covariance matrix shrunk on the	covariance shrunk covariance emp_cov shrinkage	0.250000
edges for a 3d image	edges 3d n_x n_y n_z	0.250000
search over	core base search	0.111111
for full covariance matrices	normal density full	0.166667
fitted model parameters	fit x	0.006410
finds indices in sorted array	neighbors find matching indices tree bin_x left_mask right_mask	0.166667
theta as the maximizer of the	gaussian_process gaussian process arg max	0.047619
windows this is the time it take to	externals joblib squeeze time	0.200000
the voting	ensemble voting	0.142857
func to be	async func	0.250000
eigen_solver	eigen_solver	0.454545
tokens	ngrams tokens	1.000000
wolf	wolf	1.000000
log probability for full covariance matrices	log multivariate normal density full	0.333333
for data x	x doc_topic_distr	0.333333
the array from the meta-information	zndarray wrapper read unpickler	0.043478
collect results from clf predict calls	classifier collect probas	1.000000
can be different from the	core calibrated classifier cv	0.111111
scale back the	standard scaler inverse transform	0.066667
restricted to the binary	y_score average sample_weight	0.142857
generate a random multilabel classification problem	multilabel classification n_samples n_features	0.500000
used to fit an estimator within a job	ensemble parallel fit estimator estimator	0.333333
its maximum absolute value	max abs	0.047619
the best found parameters	core base search cv predict proba	0.076923
block checkerboard structure for biclustering	checkerboard shape n_clusters noise minval	0.066667
non-negative matrices w h	w h	0.031250
corresponding derivatives with respect	activations deltas	0.032258
probability calibration with sigmoid method platt 2000 parameters	sigmoid calibration df y	0.500000
generate names	name	0.033333
generate a mostly low rank matrix with	low rank matrix n_samples n_features	0.500000
maximum likelihood covariance estimator	covariance empirical covariance	0.071429
inplace row scaling of a csr	inplace row scale x	0.142857
a locally linear embedding	locally linear embedding	0.050000
any axis center to the median	x axis	0.015385
we	joblib memorized	0.015625
that for c in (l1_min_c infinity) the model	c x y loss	0.030303
backend factory	backend name factory	1.000000
the	empirical covariance get	0.166667
find two non-negative matrices w	w	0.035714
graph	to graph	0.333333
from source to all reachable	source	0.100000
calculate true and false positives per binary	binary clf curve	0.090909
fit the rfe	fit x	0.006410
data point	x	0.001692
fit an estimator	fit estimator estimator x y	0.071429
collection of jpeg pictures of famous people	funneled resize	0.142857
multi-task l1/l2	multi task	1.000000
long type introduces an	utils shape	0.013699
log probability for	mixture log multivariate	0.500000
check a	check	0.017857
of exception types	joblib parallel	0.028571
input validation for	x y accept_sparse dtype	0.250000
function returns posterior probabilities of classification	calibrated classifier cv predict proba	0.200000
the number of estimators	len	0.038462
new estimator with the same parameters	core clone estimator safe	0.333333
type introduces an	shape repr	0.013699
number of splitting iterations in	get n splits	0.111111
the neighbors within	neighbors	0.027027
found	base search	0.200000
exception types to	joblib parallel backend base get	0.066667
mean silhouette coefficient	metrics cluster silhouette score x labels metric	0.250000
that for c in (l1_min_c	c x y loss	0.030303
actual data loading for the lfw	lfw	0.068966
quantiles to	preprocessing robust	0.111111
of init	base gradient boosting init decision	0.142857
avoid the hash depending	memorized func	0.016949
directory in which are persisted the result of	dir	0.038462
a single boost	ensemble ada boost classifier boost	0.100000
u such	flip u	0.047619
update params with given gradients parameters	updates grads	0.076923
normalize x by	normalize x	0.076923
an extra-trees classifier	extra trees classifier	1.000000
the dense dictionary factor	dictionary	0.071429
inverse permutation p	datasets inverse permutation p	1.000000
prefetch the tasks for the next	iterator	0.166667
convert coefficient matrix to	mixin sparsify	0.500000
array bytes	array array	0.166667
right fileobject from	externals joblib read fileobject fileobj	0.100000
thresholding	thresholding y output_type classes	0.500000
of array-like or scipy sparse	preprocessing binarize x	0.083333
or thread pool and	multiprocessing backend	0.038462
the l1 distances between	paired manhattan distances	0.083333
matrix for x using	x	0.001692
if the test/test sizes are meaningful wrt	validate shuffle split n_samples test_size train_size	0.111111
and predicted probabilities for a calibration	calibration	0.071429
apply transforms and	y	0.002674
implementation is restricted to the binary classification	y_true y_score pos_label sample_weight	0.066667
parameters for the voting classifier valid parameter keys	ensemble voting classifier set	0.037037
the pairwise matrix	pairwise	0.066667
scale back the data to the original	inverse transform x copy	0.066667
the vectors rows of u	u	0.032258
estimator adheres	estimator estimator	0.052632
the number of splitting iterations in the cross-validator	cross validator get n splits x y	0.125000
building a cv in a user	core check cv cv x y classifier	0.031250
the leaf	tree base decision tree apply	0.166667
cv aka logit maxent	cv	0.009009
a contingency matrix describing the	contingency matrix labels_true labels_pred eps	0.166667
delete all the content of the data	datasets clear data	0.142857
svmlight / libsvm format into sparse csr matrix	svmlight file f n_features	0.066667
x (as bigger is better	decision function x	0.018868
avoid the hash depending from it	joblib memorized func	0.014706
finds indices in sorted array of integers	matching indices tree bin_x left_mask right_mask	0.166667
a contingency matrix describing the relationship between labels	metrics cluster contingency matrix labels_true labels_pred	0.200000
private function used to partition	partition	0.100000
fit the gradient boosting model	ensemble base gradient boosting fit x	1.000000
compute the grid of alpha values	alpha grid x y	0.166667
full	density full	0.166667
projection the components of the	projection	0.071429
the index'th estimator	getitem index	0.333333
for each input	estimator x	0.030303
generate cross-validated estimates for	core cross val predict estimator x y cv	0.071429
the approximate feature map to x	core rbfsampler transform x y	0.333333
the time	time	0.047619
store the timestamp when pickling to	externals joblib memorized func reduce	0.050000
ward	ward	1.000000
the f-beta score is the weighted harmonic mean	metrics fbeta score y_true	0.333333
function called with the given arguments	joblib memorized func get output	0.125000
predict class log-probabilities for x	ensemble gradient boosting classifier predict log proba x	1.000000
model parameters	fit x y do_prediction	0.166667
from prediction scores this score	score	0.010101
locally linear embedding analysis on the data	manifold locally linear embedding x n_neighbors n_components reg	0.071429
last_variance	last_variance	1.000000
raw_x	raw_x	1.000000
in multiplicative	multiplicative	0.285714
from source to all reachable nodes	graph source cutoff	0.200000
described in friedman [1] and breiman [2]	make friedman3 n_samples noise	0.166667
logistic regression and cv and linearsvc	liblinear x y c fit_intercept	0.142857
returns the huber	huber	0.125000
with coordinate descent the elastic net optimization	l1_ratio	0.030303
mutual information	feature_selection mutual info	0.500000
the number of splitting iterations in the	base cross validator get n splits	0.125000
compute the median of	get median	0.166667
or thread pool and return the number of	externals joblib multiprocessing backend	0.035714
and target y	multilayer perceptron partial	0.166667
for the case method='lasso' is	x y xy gram	0.090909
least squares	least squares error	1.000000
parameters for the voting classifier valid	ensemble voting classifier set	0.037037
find	hungarian state find	0.500000
and hide warnings	utils ignore warnings	0.142857
probabilities for a calibration curve	core calibration curve	0.142857
getter for the	empirical	0.055556
dummy classifier to test pipelining and meta-estimators	checking classifier	1.000000
the local outlier factor	local outlier factor	0.125000
undo the scaling of	min max scaler inverse transform	0.500000
stderr depending on verbosity	msg msg_args	0.200000
array-like or scipy sparse matrix	preprocessing binarize	0.083333
matrix whose range approximates the range	range finder	0.083333
returns the number of splitting iterations in the	model_selection leave pgroups out get n splits	0.111111
regression and cv and	x y	0.002155
also predict based on an unfitted model by	predict	0.006849
boosted classifier/regressor from the training set	ensemble base weight boosting fit	0.500000
check x format check	dirichlet allocation check	0.062500
that for c	c	0.022222
the process or	joblib	0.007299
x y and scale if the scale parameter==true	scale xy x y scale	0.500000
transform data to polynomial features	preprocessing polynomial features transform	0.500000
an 'l' suffix when using	shape	0.011765
lasso path using lars algorithm	linear_model lars path x	0.100000
mean and variance along an	utils mean variance axis x axis	0.142857
replace=true p=none) generates a random sample from a	a size	0.142857
dataset is constructed by taking	datasets	0.015152
check the validity	neighbors check params x metric p metric_params	0.200000
a one-vs-all fashion several regression and	y classes neg_label pos_label	0.111111
force the execution of the	memorized	0.015873
descent fit	fit x	0.006410
log-likelihood of a gaussian data set with	score	0.010101
and a youngs	utils	0.009709
that for c in	c x y loss	0.030303
erase the complete cache directory	externals joblib memory clear warn	0.333333
fp where tp is the number of	score y_true y_pred labels pos_label	0.027778
compute the l1 distances between	manhattan distances	0.083333
rng	rng	1.000000
whether the file was opened for writing	zlib file writable	0.250000
for validation and conversion of	dtype csr_output	0.166667
predict apply predict_proba	predict	0.013699
net path with	path	0.025641
vbgmm	vbgmm	1.000000
fit a binary classifier on x and y	base sgdclassifier fit binary x y	1.000000
x y and optionally its gradient	rbf call x y eval_gradient	0.333333
the function with the given arguments	func	0.011364
euclidean or frobenius norm of	utils norm	0.333333
fit the model to data	fit	0.013029
matrix	matrix n_components	1.000000
samples in x and y	x y dense_output	0.333333
function the absolute error	error	0.020000
to avoid	externals joblib memory	0.016949
the weighted graph of neighbors for points in	neighbors radius neighbors graph	0.066667
global clustering for the subclusters obtained after fitting	cluster birch global clustering	0.142857
back the data to the original representation parameters	preprocessing standard scaler inverse transform	0.066667
y as training	y copy_x	0.333333
helper to workaround python 2 limitations of pickling	helper obj methodname	0.333333
a random sample from	size replace	0.125000
found and raise an	search	0.019231
this is a general function given points on	reorder	0.071429
c in (l1_min_c infinity) the	c x y loss fit_intercept	0.030303
diagonal	diag resp	1.000000
coerce_mmap	coerce_mmap	1.000000
of the laplacian matrix	laplacian	0.034483
and dense	x y	0.002155
log of probability estimates	predict log proba	0.058824
local outlier	local outlier	0.250000
mean and component wise scale to unit	preprocessing scale	0.090909
wise squaring of array-likes and sparse	utils safe sqr x copy	0.125000
wild lfw pairs dataset this dataset is a	lfw pairs	0.018868
fit the model using x y as	linear_model base randomized linear model fit x y	0.333333
of neighbors for points in	radius neighbors	0.086957
binary classification used in hastie	hastie	0.076923
contains valid	probabilistic predictions	0.500000
matrix factorization nmf find two	negative factorization x	0.043478
an extremely randomized tree regressor	extra tree regressor	1.000000
and compute prediction of init	boosting init	0.142857
persist an arbitrary	compress protocol	0.333333
svmlight / libsvm format into sparse csr	svmlight file f	0.066667
input data point	val predict	0.045455
generate	cross val	0.038462
path	linear_model enet path	0.050000
maximum to be used for later scaling	max scaler fit	1.000000
the decision functions of the	decision function	0.025000
scores this score corresponds to the area under	score y_true y_score	0.025000
raises an exception in an unfitted estimator	unfitted name estimator	0.142857
x according to feature_range	inverse transform x	0.025641
similarity of two sets of	a b similarity	0.125000
to compute decisions within a	estimators_features x	0.111111
ensure deterministic output from svd	utils svd	0.166667
hessian in the case of a logistic loss	linear_model logistic grad hess	1.000000
pairwise matrix in n_jobs even	pairwise x y func n_jobs	0.111111
kernel k	gaussian_process sum call	1.000000
check initial parameters of the derived class	mixture base mixture check parameters	0.200000
the array from the meta-information and the	zndarray wrapper read unpickler	0.043478
is the solution to a sparse coding problem	sparse encode x	0.333333
xred	xred	1.000000
on test	core	0.030769
fit the model with	chi2sampler fit	0.250000
a reducer function to a	externals joblib	0.004762
boost using	classifier boost	0.100000
compute receiver operating characteristic roc note	metrics roc curve	0.142857
platform independent representation of	utils shape	0.013699
the model	base randomized linear model	0.500000
list of exception types	externals joblib	0.004762
bound for c such that for c in	c x y	0.030303
lower bound on model	mixture dpgmmbase lower bound	0.071429
the kernel	normalized kernel	0.250000
point	y	0.002674
mostly low rank matrix with bell-shaped singular	datasets make low rank matrix	0.083333
back the data to the original	robust scaler inverse transform x	0.066667
given parameter	parameter estimator parameter	0.500000
depending	externals joblib memorized	0.013699
ic	ic	0.555556
raised when configuration should fallback to another	fallback to	1.000000
suffix when using the	repr	0.012500
an ensemble of totally random trees	random trees embedding	0.250000
with a given cache key	externals joblib cache key	0.250000
squared logarithmic	squared log	1.000000
requested by the callers	externals joblib effective	0.200000
underlying estimators should be used when memory is	one vs one classifier	0.125000
the l1 distances between the vectors	metrics paired manhattan distances	0.083333
vectors for reproducibility flips the	utils deterministic vector	0.076923
model with stochastic gradient descent	linear_model base sgdregressor partial	0.333333
the context of the memory	joblib memory	0.016949
each input	core cross val predict estimator x y	0.045455
estimates	predict estimator x y	0.045455
compute area under the curve auc using the	metrics auc x y	0.040000
updates terminal regions to median	terminal region tree	0.100000
sklearn metrics pairwise	pairwise	0.066667
scores note this implementation is	metrics roc	0.040000
predict using the trained model	neural_network base multilayer perceptron predict x	0.333333
for mean_shift	bin_size min_bin_freq	0.500000
the local outlier factor of	local outlier factor	0.125000
to bicluster	core bicluster mixin get	0.500000
decides whether it is time to stop training	neural_network base optimizer trigger stopping msg verbose	0.250000
representation	shape	0.011765
h	w h beta_loss	0.500000
a nicely formatted statement displaying the	args kwargs object_name	0.166667
array bytes to pickler file handle	array array pickler	0.333333
the array from the meta-information	joblib zndarray wrapper read unpickler	0.043478
from	externals joblib memorized	0.013699
whose range approximates the range of	range finder	0.083333
a contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true labels_pred eps	0.200000
model parameters	linear_model base	0.250000
for	empirical covariance get	0.166667
lad updates terminal	least absolute error update terminal	0.200000
each non zero row of x	x y copy	0.142857
mean squared logarithmic error regression loss read more	mean squared log error	0.200000
prediction of init	gradient boosting init decision	0.142857
update the concentration parameters	mixture dpgmmbase update concentration z	1.000000
indices to split data into training and test	split x y groups	1.000000
posterior log probability of	multinomial nb joint log likelihood	0.083333
the hash	memorized func	0.016949
n_trials	n_trials	1.000000
function for the given autocorrelation	function	0.021277
the largest k singular values/vectors for a	a k ncv tol	0.166667
fit	svc fit x y	0.333333
graph of neighbors for points in x	neighbors graph x	0.500000
a tolerance which is independent of the dataset	tolerance	0.045455
compute labels and inertia using	labels inertia precompute dense x x_squared_norms centers	0.250000
implementation is restricted to the binary classification	y_true	0.043478
global clustering	global clustering	0.142857
compute	x y	0.002155
factorize argument checking for random matrix generation	input size n_components n_features	0.200000
predicted probabilities for a calibration curve	calibration curve y_true y_prob	0.142857
context manager and decorator to ignore warnings	utils ignore warnings obj	1.000000
shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage	0.125000
text in	text	0.222222
the given training data and	y	0.002674
similarity of two sets of	score a b similarity	0.125000
a	externals joblib sequential backend apply	1.000000
back the data to the original representation parameters	robust scaler inverse transform	0.066667
the directory in which are persisted	dir	0.038462
line_search_wolfe2 if suitable step	wolfe12 f fprime xk pk	0.028571
compute the decision function	ensemble ada boost classifier decision function	0.166667
the weighted graph of neighbors for points	radius neighbors mixin radius neighbors graph	0.066667
a single binary estimator	binary estimator x	0.363636
label encoder	label encoder	0.500000
of	joblib parallel backend base	0.058824
build a batch of estimators within a job	ensemble parallel build estimators n_estimators ensemble x y	0.166667
compute data covariance with	covariance	0.028986
kfold	kfold	0.294118
score is the	score y_true	0.058824
mini-batch dictionary learning finds a dictionary a	mini batch dictionary learning	0.142857
probabilities of possible outcomes for samples in x	svm base svc predict proba	0.333333
check if fileobj	fileobj	0.111111
value of verbose	verbose	0.062500
the callable case for	pairwise callable x	0.083333
usage	used path	0.250000
to the median and component wise scale	scale x	0.043478
each	y	0.002674
the weighted graph of neighbors for points	neighbors radius neighbors graph	0.066667
estimate the precisions parameters of the precision	mixture bayesian gaussian mixture estimate precisions nk	0.166667
false positives per binary	metrics binary clf curve y_true y_score pos_label	0.090909
convert coefficient matrix to	mixin	0.037037
the parallel execution only	externals joblib parallel	0.014085
of feature names ordered by their	dict vectorizer get feature names	0.142857
return the breast	breast	0.111111
number of splitting iterations in	split get n splits x	0.111111
proportions	proportions	1.000000
a cross-validated	estimator x y cv	0.050000
for the lfw pairs	fetch lfw pairs	0.018868
meant to be cached by a	index_file_path data_folder_path slice_ color	0.033333
the scaler	max abs scaler	0.250000
of exception	joblib	0.007299
capture	check_pickle	0.040000
to build	parallel build	0.047619
faces in the wild lfw pairs dataset	fetch lfw pairs subset	0.035714
a with	externals joblib	0.004762
learn vocabulary and idf return	raw_documents y	0.250000
of x and dot w	beta divergence x w	0.500000
each input data	val predict estimator x	0.045455
precisions	precisions nk xk sk	0.166667
mostly low rank matrix with bell-shaped singular values	make low rank matrix	0.083333
used to capture the arguments of	check_pickle	0.040000
array in place using strides	arr patch_shape extraction_step	0.166667
fit the model to data	output estimator fit x y sample_weight	0.200000
predict multi-output variable using	core multi output estimator predict x	0.166667
rand index adjusted	adjusted rand score	0.333333
backing	backing	1.000000
init_args	init_args	1.000000
the absolute error of	error	0.020000
compute prediction of init	ensemble base gradient boosting init decision	0.142857
return the shortest	utils single source shortest	0.333333
the given param_grid	search cv get param iterator	0.166667
of	get	0.012048
a cv in a user friendly way	core check cv cv x y	0.031250
x format check x	dirichlet allocation check	0.062500
estimator with the best found parameters	search cv	0.036364
predict new data by linear interpolation	core sigmoid calibration predict t	1.000000
when memory is inefficient	classes	0.025641
absolute sizes of training	sizes	0.050000
sample weight array	base sgd validate sample weight sample_weight	0.333333
model according to	x y sample_weight	0.025974
the function call with the given	format call func	0.100000
log probability	mixture log multivariate normal	0.500000
regression problem with sparse uncorrelated design	make sparse uncorrelated n_samples n_features	0.166667
according to the	x y sample_weight	0.025974
a temporary folder if still existing	folder folder_path warn	0.250000
the number of splitting iterations in	leave pgroups out get n splits	0.111111
format check x format	decomposition latent dirichlet allocation check	0.062500
exponentiation	exponentiation	0.833333
if dtype of x and y	x y	0.002155
breiman [2]	friedman3 n_samples noise random_state	0.166667
fit with all	cv fit	0.200000
by a random projection p only changes	core johnson lindenstrauss min dim n_samples	0.142857
fit the model	svm linear svr fit	0.333333
generate cross-validated estimates for	y cv	0.050000
strip	strip	0.388889
sum	sum	1.000000
given dataset	scorer	0.045455
used to fit an estimator within a job	ensemble parallel fit estimator estimator x y	0.333333
the boolean mask	get mask	0.333333
array-like or scipy sparse	preprocessing binarize x threshold copy	0.083333
retrieve a reliable	externals joblib	0.004762
matrix for label spreading	semi_supervised label spreading build	0.250000
be used for later scaling	scaler fit x y	0.200000
back the	preprocessing standard scaler inverse transform x	0.066667
matrix factorization nmf find two non-negative matrices	negative factorization x	0.043478
path with coordinate descent the	enet path	0.050000
learn the vocabulary dictionary and return term-document	count vectorizer fit transform raw_documents y	0.166667
and	and	0.875000
the determinant of a wishart the	mixture wishart	0.125000
the free energy f v	free energy	0.066667
fit the kernel density model on	neighbors kernel density fit	0.250000
run fit on the	fit x	0.006410
c in (l1_min_c infinity)	c x	0.030303
parameters of this	params	0.085714
the scaling of	scaler inverse	0.250000
wrapped function cache result and return a	joblib memorized func	0.014706
validation and conversion of csgraph	utils sparsetools validate graph csgraph directed dtype csr_output	0.166667
total log probability	neighbors kernel density	0.090909
function for factorizing common classes param	first call clf classes	0.058824
under the curve auc using the	auc x	0.040000
of possible outcomes for samples	ensemble voting classifier predict	0.100000
derivative	derivative	0.750000
for parallel processing this method is meant	parallel	0.019231
graph matrix for label spreading computes the graph	semi_supervised label spreading build graph	0.142857
the dimension of a dataset	dimension	0.050000
reconfigure the	configure	0.142857
slices	slices	1.000000
data as a sparse	decomposition sparse	0.111111
fit all transformers transform the data and	feature union fit transform x y	0.500000
a cv in	cv cv x	0.031250
using a single binary estimator	binary estimator	0.090909
generate indices to split data into	shuffle split split x	0.250000
matrix factorization nmf find	factorization x	0.043478
the number of splitting iterations in the	get n splits x y	0.111111
of determination regression	r2	0.076923
norm of x	x	0.001692
returns a list of edges for a	edges	0.047619
the lfw pairs dataset this operation is	lfw pairs	0.018868
multi-class targets	output code	0.200000
of the loss is	loss	0.027027
fit for one	fit	0.003257
number of splitting iterations in the	pgroups out get n splits	0.111111
data x which	x y	0.002155
loss	loss w	1.000000
build a contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true	0.200000
function of the given	function x raw_values	0.250000
fit across one fold	feature_selection rfe single fit rfe estimator x	0.200000
matrix factorization nmf find two	factorization x	0.043478
best possible score is 1	score y_true y_pred	0.038462
read an array using numpy memmap	joblib numpy array wrapper read mmap unpickler	1.000000
name	get func name	0.047619
batch of estimators within	estimators n_estimators ensemble x	0.083333
time_step	time_step	1.000000
avoid	joblib	0.014599
then	transformer	0.100000
sizes of training	sizes	0.050000
folders	joblib memory	0.016949
force the	joblib memorized	0.015625
the number of splitting iterations in	one out get n splits	0.111111
file object providing transparent gzip de compression	binary gzip file	0.500000
covariance matrix shrunk on	shrunk covariance	0.090909
sparse random projection matrix parameters	core base random projection fit x	0.333333
x and y	x y alpha	0.090909
predict probability for each possible outcome	semi_supervised base label propagation predict proba	1.000000
set the sample weight array	sgd validate sample weight	0.333333
update the dense dictionary factor	decomposition update dict dictionary	0.333333
minimize the objective function iterating once over all	coordinate descent	0.333333
an array	array wrapper	0.166667
largest k singular values/vectors for a sparse	a k ncv tol	0.166667
the estimator with the best found parameters	model_selection base search cv predict proba x	0.076923
implementation is restricted to the binary classification	y_true y_score average	0.076923
the file supports seeking	zlib file seekable	0.250000
classification metrics	metrics classification	0.052632
compute prediction of init	boosting init decision	0.142857
the long type introduces an 'l' suffix	shape	0.011765
clustering on x and returns cluster	cluster dbscan fit predict x y sample_weight	0.166667
random matrix generation	core check input size n_components n_features	0.200000
the similarity of two clusterings	score labels_true labels_pred	0.047619
the process or	externals joblib multiprocessing backend	0.035714
x and returns	x y w	0.500000
back the data to the original representation	standard scaler inverse transform x	0.066667
p=none) generates a random sample from	size replace p	0.125000
get the boolean mask indicating which	selector mixin get support mask	0.333333
coefficient matrix	linear_model sparse coef	0.076923
blup parameters and evaluates the reduced likelihood	process reduced likelihood	0.142857
abstract base class for naive bayes estimators	base nb	1.000000
for validation and conversion of csgraph inputs	sparsetools validate graph csgraph directed dtype csr_output	0.166667
estimates for	val predict estimator	0.045455
returns the submatrix corresponding to bicluster i	bicluster mixin get submatrix i	0.333333
suffix when using the	utils shape	0.013699
the free energy f v	neural_network bernoulli rbm free energy	0.066667
the leaves of the	get leaves	0.111111
computes the free energy f v = -	free energy	0.066667
task	recall curve	1.000000
a locally linear embedding analysis	locally linear embedding x	0.071429
fit the hierarchical clustering on the data parameters	cluster agglomerative clustering fit	0.250000
is meant to be cached	data_folder_path slice_ color resize	0.033333
load dataset from multiple files in svmlight format	datasets load svmlight files files n_features dtype multilabel	0.500000
list of exception types	parallel backend base	0.037037
matrix factorization nmf find two non-negative matrices	non negative factorization	0.043478
axix	last_mean last_var	1.000000
methods for outliers detection with covariance estimators	outlier detection mixin	0.500000
for multiclass	multiclass y	0.500000
target_variables	target_variables	1.000000
weights for unbalanced datasets	weight class_weight classes	0.333333
bic or aic for model	ic	0.111111
for	core cross val	0.043478
backprop	backprop	0.833333
a hash to identify uniquely python objects	externals joblib hash obj hash_name coerce_mmap	0.333333
matrix whose range approximates the range of a	utils randomized range finder a	0.166667
coefficient matrix to dense array format	sparse coef mixin densify	0.100000
an arbitrary python object	dump value filename	0.083333
the directory in which are persisted the result	dir	0.038462
the number of splitting iterations in	model_selection base kfold get n splits x	0.111111
reliable function code hash	joblib get func code func	1.000000
number of splitting iterations in the cross-validator	base kfold get n splits x	0.111111
covers	covers	0.833333
used to build a batch of estimators within	ensemble parallel build estimators n_estimators	0.166667
the timestamp when pickling	joblib memorized func reduce	0.050000
a reducer function to a given type	externals joblib customizable pickler register type	0.083333
make sure that an estimator implements the necessary	check estimator estimator	0.142857
fit multitaskelasticnet model with coordinate descent parameters	linear_model multi task elastic net fit x	1.000000
estimator on training subsets incrementally and compute	model_selection incremental fit estimator estimator x y	0.200000
data	pca	0.047619
used to fit a single tree in parallel	ensemble parallel build trees tree forest	0.200000
return a tolerance which is	cluster tolerance x tol	0.058824
the number of splitting iterations in the	pgroups out get n splits	0.111111
directory in which are	dir	0.038462
compute prediction of init	ensemble base gradient boosting init	0.142857
returns the number of splitting iterations in	model_selection leave one group out get n splits	0.111111
memmap instance	reduce memmap	0.166667
matrix shrunk on	covariance shrunk	0.066667
break the pairwise matrix in n_jobs	pairwise x y func n_jobs	0.111111
estimators that implement the partial_fit api	utils check partial	0.038462
model using x as training data and y	x y	0.004310
clustering for the	clustering x	0.142857
score is	score y_true y_pred beta labels	0.500000
strip the headers by removing everything before the	strip	0.055556
meta-information and the z-file	externals joblib zndarray wrapper read unpickler	0.043478
normalized mutual information between two	metrics cluster normalized mutual info score	1.000000
reconfigure the backend and	parallel backend base configure	0.500000
theta as the maximizer	gaussian process arg max	0.047619
passive aggressive classifier read more in	passive aggressive classifier	0.125000
along an axis on a csr or csc	x axis	0.015385
faces in the wild lfw	lfw	0.034483
classification dataset is constructed by	datasets	0.015152
shutdown the	multiprocessing backend terminate	0.166667
regression score function	score y_true	0.058824
cache folders to make cache size fit in	joblib memory reduce size	0.083333
the maximizer of the reduced	gaussian process arg max reduced	0.200000
getitem	getitem	1.000000
reduce x to	selector mixin transform x	0.500000
callable case for pairwise_{distances kernels}	pairwise callable	0.083333
learn vocabulary and idf return	transform raw_documents y	0.100000
of the samples x	function x	0.030303
ransacregressor	ransacregressor	0.833333
n_alphas	n_alphas	1.000000
fit	core multi output estimator fit x	0.200000
check initial parameters of	mixture check parameters x	0.166667
by scaling each feature to	preprocessing minmax scale x	0.142857
for factorizing common classes param logic estimators	partial fit first call clf classes	0.058824
for the voting classifier	voting classifier set	0.037037
compute non-negative matrix factorization nmf	decomposition non negative factorization	0.043478
x as a	fit x	0.006410
fit to data then transform it	core transformer mixin fit transform x y	0.500000
full	normal density full x means covars	0.166667
covariance model according to the given	covariance	0.014493
pool	joblib multiprocessing	0.052632
label spreading computes the graph	semi_supervised label spreading build graph	0.142857
compute log probabilities	parallel predict log proba	0.058824
the mixture parameters	gaussian mixture	0.200000
decision function of the given observations	covariance outlier detection mixin decision function x	0.333333
types to	joblib parallel backend base get	0.066667
the number of splitting iterations in	kfold get n splits	0.111111
of estimators within	estimators n_estimators ensemble	0.083333
parameters theta as the maximizer of the reduced	gaussian_process gaussian process arg max reduced	0.200000
fit	core multi output estimator fit x y	0.200000
for each input data point	val predict estimator	0.045455
set	manifold set	0.500000
constructor store the	externals joblib ndarray wrapper init filename subclass allow_mmap	0.200000
with n_zeros	n_zeros	0.111111
timestamp when pickling to	func reduce	0.050000
depending from	joblib	0.014599
returns the huber loss	huber loss	0.333333
an estimator	estimator estimator x y	0.333333
wishart distribution parameters	wishart	0.062500
return the shortest path length from source	shortest path length graph source cutoff	0.111111
all the covariance matrices from a given template	covariance type tied_cv covariance_type	0.333333
the file was opened for writing	binary zlib file writable	0.250000
string to the file	file	0.035714
of theta	theta eval_gradient	0.500000
the median of data with n_zeros additional zeros	utils get median data n_zeros	0.500000
log probabilities of	log proba	0.090909
model fitting	fit	0.003257
fit	fit rfe estimator x y	0.166667
the normalized laplacian	eigen_solver	0.090909
rows in x and y	x y	0.002155
kernel k x y and optionally its gradient	gaussian_process rbf call x y eval_gradient	0.333333
back the data to the	preprocessing standard scaler inverse transform x copy	0.066667
finds indices	find matching indices	0.250000
the number of splitting iterations in the	predefined split get n splits x y groups	0.111111
the time it take to	joblib squeeze time t	0.200000
is inefficient to train	classes	0.025641
loss and the gradient	loss and gradient	1.000000
features are selected	feature_selection selector	0.142857
label sets with	preprocessing multi label binarizer	0.200000
neighbors within	neighbors lshforest radius neighbors	0.166667
the generative model	decomposition base	0.076923
locally linear embedding analysis on the	manifold locally linear embedding x	0.071429
data to vectors and cluster the result	and cluster data vectors n_clusters	0.333333
flattened log-transformed non-fixed hyperparameters	gaussian_process kernel operator theta theta	0.333333
pool	multiprocessing backend	0.038462
list of feature	feature	0.055556
generates integer indices	indices	0.055556
the binary	y_score average	0.111111
of the given data x	x y	0.002155
transform feature->value dicts to array or sparse	feature_extraction dict vectorizer transform x	0.200000
new_subcluster2	new_subcluster2	1.000000
can also predict based on an unfitted	predict x	0.011765
the relationship	cluster	0.021277
transform documents to document-term matrix	vectorizer transform raw_documents copy	1.000000
mlp loss	multilayer perceptron	0.071429
the function code and	func code	0.200000
ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x assume_centered	0.125000
return a tolerance which is independent	cluster tolerance x	0.058824
return the kernel k	gaussian_process pairwise kernel	0.250000
private function used to compute decisions within a	decision function estimators estimators_features	0.500000
the process of the parallel execution	externals joblib parallel	0.014085
under the curve auc from prediction scores	auc score	0.052632
inverse	inverse	0.500000
fit the model to	output estimator fit x y	0.200000
fits a minimum covariance	covariance min	0.333333
some authors as	preprocessing label binarizer	0.071429
median of data	get median data	0.333333
the decision function	decision function x raw_values	0.083333
an adaboost regressor	ada boost regressor	1.000000
fitted model parameters	fit x y sample_weight	0.020000
the gradient and	x y	0.004310
for the lfw people dataset this operation is	datasets fetch lfw people	0.040000
matrix	k	0.083333
under the curve auc using	metrics auc x y	0.040000
lfw people dataset	fetch lfw people	0.040000
hence	preprocessing binarizer	1.000000
the binary classification	y_true y_score average	0.076923
store	joblib memorized	0.015625
euclidean or frobenius norm of x	utils norm x	0.333333
train estimator on training subsets incrementally and	model_selection incremental fit estimator estimator x y classes	0.200000
to the training set x and returns	x y	0.002155
of the dual gap convergence criterion the	covariance dual gap emp_cov precision_ alpha	0.071429
em update for	em step x	0.500000
function for _fit_coordinate_descent update	decomposition update	0.125000
from prediction scores this score corresponds to the	score y_true y_score	0.025000
platform independent	shape repr	0.013699
a batch of estimators within a	estimators n_estimators ensemble	0.083333
method for updating terminal regions (=leaves)	update terminal region tree terminal_regions leaf x	0.200000
number of jobs which are going	multiprocessing backend effective n jobs n_jobs	0.333333
with categories as subfolder names	container_path description categories	0.500000
to evaluate the accuracy of a classification	y_true	0.021739
bound for c such that for c	c x y loss	0.030303
each input data point	cross val	0.038462
boost using	boost classifier boost	0.100000
coefficient of determination regression score function	metrics r2 score y_true y_pred sample_weight multioutput	0.125000
probability calibration with sigmoid method platt 2000 parameters	sigmoid calibration df y sample_weight	0.500000
implementation is restricted to the binary classification task	y_true y_score average sample_weight	0.076923
of the decision functions of	decision function x	0.018868
spherical wishart distribution parameters	wishart spherical nk xk sk	0.333333
memorized	memorized	0.079365
number of splitting iterations in the cross-validator parameters	get n splits x y	0.111111
for training data	classifier	0.013699
path with coordinate	linear_model enet path	0.050000
data	predict	0.006849
k x y and	x y	0.008621
updating terminal	update terminal	0.142857
[rouseeuw1984]_ aiming at computing mcd	x n_support remaining_iterations initial_estimates	0.111111
for each input	core cross val predict	0.045455
from	joblib memory	0.016949
mutual information	feature_selection mutual info classif	0.500000
cv in a user friendly	cv cv x y classifier	0.031250
maximizer of the	gaussian process arg max	0.047619
don't	externals joblib memory	0.016949
fit x into an embedded space and return	manifold tsne fit transform x y	0.500000
for parallel processing this method	externals joblib parallel	0.014085
call predict_proba on the estimator with the best	cv predict proba x	0.068966
utility for building a cv in	check cv cv x	0.031250
depending from it	memorized	0.015873
returns the number of splitting iterations in	base kfold get n splits x	0.111111
target variable	classif x y discrete_features	1.000000
mostly low rank matrix with	datasets make low rank matrix	0.083333
estimates for	core cross val predict estimator	0.045455
the score on the	score x	0.033333
ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x	0.125000
file-like object until size bytes are	bytes fp size error_template	0.333333
an 'l' suffix	repr	0.012500
the elastic net optimization function varies	l1_ratio	0.030303
classification on an array of test	gaussian process classifier	0.500000
n_packs	n_packs n_samples	1.000000
corresponding derivatives with respect to each parameter	activations deltas	0.032258
approximate feature map to x	core rbfsampler transform x y	0.333333
select features according to the k highest scores	select kbest	1.000000
the output of transform is sometimes	transform y	0.023256
fits the shrunk covariance model according to the	covariance shrunk covariance fit	0.083333
coefficient matrix to	coef mixin	0.090909
coefficient matrix to dense array	coef mixin densify	0.100000
significance of a cross-validated score	score estimator x y cv	0.083333
a which	externals joblib memorized	0.013699
all the content of the data home	clear data home data_home	0.076923
predict_log_proba on	predict log proba	0.029412
vocabulary dictionary and return term-document	count vectorizer fit transform raw_documents y	0.166667
on test vectors x	core dummy regressor predict x	0.250000
time under windows this is the time	time t	0.125000
computes a truncated randomized svd parameters	utils randomized svd m n_components n_oversamples n_iter	0.250000
for species	datasets fetch species	0.500000
to split data in	split	0.027778
function and cache	func	0.011364
compute the recall the recall is the ratio	recall	0.028571
mean and component wise scale	scale	0.033333
which are going to run in	joblib multiprocessing backend effective	0.250000
linear embedding read more in the :ref	linear embedding	0.083333
that for c in	c x y loss fit_intercept	0.030303
by using the gp prior	x return_std return_cov	0.142857
solution to a sparse coding problem	sparse encode x dictionary gram cov	0.333333
within a given radius of	x radius	0.058824
net path	linear_model enet path	0.050000
estimator with the best found parameters	base search cv predict	0.076923
sample weights by class	utils compute sample	0.100000
of the function with the given arguments and	func call	0.047619
read array from unpickler file handle	array wrapper read array unpickler	1.000000
a cv in a	check cv cv x	0.031250
of array-like or scipy sparse matrix	preprocessing binarize x threshold	0.083333
run fit	fit x y	0.005988
predict class probabilities for	ensemble bagging classifier predict proba	0.500000
a memmap instance	reduce memmap a	0.050000
wise scale	preprocessing robust scale x	0.125000
of the dual gap convergence criterion	covariance dual gap	0.071429
force the execution of the function	externals joblib memorized	0.013699
signal as a sparse combination of dictionary elements	datasets make sparse coded signal	1.000000
fit the	skewed chi2sampler fit	0.250000
the model by computing truncated	truncated x	0.200000
path	path x	0.136364
significance of a cross-validated	estimator x y cv	0.050000
helper function for factorizing common classes param logic	partial fit first call clf classes	0.058824
generate an	make	0.083333
mutual information between two variables	mi	0.166667
function of	function x	0.030303
the depth a	externals joblib memorized func check previous func code	0.055556
according to the given training	y sample_weight	0.035714
of parameter objects and 'return_annotation'	init parameters return_annotation __validate_parameters__	0.125000
loader for the labeled	subset data_home	0.125000
svd by arpack or randomized	x n_components svd_solver	1.000000
cohen's kappa a	kappa	0.111111
estimators within a	estimators	0.052632
transform x into subcluster centroids dimension	cluster birch transform x	1.000000
the scaler	max scaler	0.333333
curve auc using the trapezoidal	auc	0.020408
cross-validated estimates for each	predict estimator x y cv	0.071429
independent representation	shape	0.011765
set the sample weight array	sgd validate sample weight sample_weight	0.333333
the process	multiprocessing	0.045455
python object into one file	externals joblib dump value filename	0.083333
to	memory	0.031250
graph of neighbors for points	radius neighbors mixin radius neighbors graph	0.066667
elements from 0 to n	n	0.050000
the squared loss for	neural_network squared loss y_true	0.500000
finds	n_neighbors return_distance	0.250000
procedure described in [rouseeuw1984]_ aiming at computing mcd	covariance c step x n_support remaining_iterations initial_estimates	0.111111
data x with	x	0.001692
covariance matrix	covariance	0.043478
and y_prob	y_prob	0.142857
accuracy classification score	accuracy score y_true y_pred normalize sample_weight	1.000000
naive bayes	nb	0.222222
generates boolean masks corresponding to test sets	core partition iterator iter test masks	1.000000
boolean thresholding of array-like or scipy sparse matrix	binarize x	0.083333
in x according	x	0.001692
kernel k x y and	matern call x y	0.200000
prediction scores this score corresponds to the area	score y_true y_score	0.025000
false positives per binary classification threshold	binary clf curve y_true y_score pos_label	0.090909
inner fit for one mini-batch	neural_network bernoulli rbm fit v_pos rng	0.250000
point	val predict estimator	0.045455
thread	externals joblib multiprocessing backend	0.035714
vocabulary dictionary and return term-document matrix	count vectorizer fit transform raw_documents y	0.166667
completed	completed	1.000000
list of exception types to be captured	externals joblib parallel backend base get exceptions	0.166667
shrunk covariance model according	covariance shrunk covariance fit	0.083333
is a raw file object e g created	is raw file	1.000000
a cv in a	check cv cv x y classifier	0.031250
selected affinity then applies spectral clustering	cluster spectral clustering fit	0.500000
to	linear_model	0.025641
significance of a cross-validated	y cv	0.050000
returns the number of splitting iterations in	model_selection base kfold get n splits x y	0.111111
-	core isotonic regression	0.055556
random multilabel classification	multilabel classification	0.166667
linkage agglomerative	linkage tree x	1.000000
spherical wishart	wishart spherical nk	0.333333
error	error	0.240000
hash depending	memorized	0.015873
fit the model to the data x	fit x y	0.005988
x (as bigger is better i	decision function x	0.018868
for building a cv in a user	core check cv cv x y classifier	0.031250
dispatch them	parallel dispatch	0.250000
for	y	0.002674
function code and the	func code	0.200000
can be different from	core calibrated classifier cv	0.111111
arbitrary python object into	filename	0.050000
leave	leave	1.000000
a cluster	metrics cluster	0.142857
model using	linear_model lars	1.000000
a locally linear embedding analysis	manifold locally linear embedding x	0.071429
curve auc	auc x y	0.040000
used to build a batch of estimators within	build estimators n_estimators ensemble	0.166667
the descriptors of a memmap instance	memmap a	0.050000
and component wise scale to unit	preprocessing scale x	0.090909
for the voting classifier valid parameter keys	voting classifier set params	0.037037
generalized exponential correlation model	gaussian_process generalized exponential theta d	1.000000
used to capture the arguments	check_pickle	0.040000
a cross-validated score with	score estimator x y cv	0.083333
function opening the right fileobject from a	joblib read fileobject fileobj	0.100000
best possible score is	score y_true y_pred sample_weight multioutput	0.062500
generate a grid of points based on	grid from	0.166667
number of splitting iterations in the cross-validator parameters	pgroups out get n splits	0.111111
as a sparse combination of dictionary elements	datasets make sparse coded	1.000000
fit the model by computing truncated	decomposition pca fit truncated	1.000000
call transform	transform x	0.016949
function call with	format call func	0.100000
returns the score on	score x y	0.030303
pickle-protocol - set state of the	core isotonic regression setstate state	0.250000
neighbors within a given radius	radius neighbors x radius	0.142857
the loss	ensemble loss	0.166667
of famous people	funneled resize	0.142857
predict posterior probability	predict proba	0.250000
score is the	score y_true y_pred beta	0.500000
vectors individually to unit	copy	0.062500
function output for x relative to y_true	metrics threshold scorer call clf x y sample_weight	0.058824
the neighbors within a given radius of	neighbors lshforest radius neighbors x radius	0.142857
kwargs using a list of	kwargs	0.076923
the directory corresponding to	func dir mkdir	0.166667
transform	x transform	0.333333
cross-validated estimates for each input	predict estimator x y cv	0.071429
length is not found and raise an exception	search	0.019231
true and false positives per binary	binary clf curve y_true	0.090909
call predict_log_proba	predict log proba x	0.045455
given data x	x y	0.002155
configure a	append	0.083333
target y is of a non-regression type	classification targets y	1.000000
a projection to the normalized laplacian	n_components eigen_solver	0.166667
on	n_components	0.083333
x and	beta divergence x	0.250000
generator to create slices containing	gen batches	1.000000
k-neighbors of a	neighbors kneighbors mixin kneighbors x	0.125000
to avoid the	func	0.011364
estimates	predict	0.006849
a cv	check cv cv	0.031250
factorize density check	core check density density n_features	0.166667
a score	score estimator	0.250000
using a single binary estimator	core predict binary estimator	0.200000
function best possible score is	score y_true y_pred sample_weight multioutput	0.062500
of init	boosting init decision function x	0.142857
the number of splitting iterations in the cross-validator	out get n splits x y groups	0.111111
a youngs	utils	0.009709
regression target	gradient boosting regressor	0.500000
of parameters and raise valueerror if not valid	base gradient boosting	0.100000
for fit subclasses should implement this method!	decomposition base pca fit	0.333333
fit a	linear_model base sgdclassifier fit	0.153846
of csgraph	csgraph	0.111111
precomp	precomp	1.000000
the sign of vectors for reproducibility flips the	deterministic vector	0.076923
the binary	y_score	0.083333
to the cache for	joblib memorized func	0.014706
compute the precision the precision	metrics precision	0.033333
seeds	bin seeds x	0.250000
two	cc x	1.000000
data with n_zeros additional zeros	data n_zeros	0.500000
data and concatenate	y	0.002674
training	factor fit predict	0.066667
column scaling of a	column	0.083333
loads data	data	0.038462
perform	x responsibilities params	0.500000
center kernel	preprocessing kernel centerer transform k y copy	0.500000
update the bound	mixture bound state log	0.500000
a filename	fileobj filename	0.500000
binary labels back	inverse	0.055556
length is not found and	line search	0.029412
subclasses should implement this method!	decomposition base pca	0.071429
catch and hide warnings	utils ignore warnings	0.142857
generate train test indices	iter indices	0.250000
weighted graph of k-neighbors for points in x	neighbors kneighbors mixin kneighbors graph x n_neighbors mode	0.333333
temporary folder if still existing	joblib delete folder folder_path	0.250000
data point	x y	0.002155
isotonic regression model	isotonic regression	0.111111
w to minimize	w	0.035714
hastie	hastie	0.461538
reliable	joblib get func	1.000000
a	externals joblib pool manager mixin apply	1.000000
representation of	repr	0.012500
cv in a user friendly	core check cv cv x	0.031250
of the data home cache	data home	0.076923
search over parameters	core base search cv fit x y	0.166667
shrunk ledoit-wolf	ledoit wolf x assume_centered block_size	0.250000
each input data point	y	0.002674
fit ridge regression	ridge gcv fit	1.000000
with the generative	decomposition base pca get	0.071429
predict labels for data	predict	0.006849
can also predict	predict	0.006849
relationship between labels	metrics cluster	0.142857
the data	x	0.003384
fit the model to	core multi output estimator fit x	0.200000
decision path	decision path	0.333333
generate a	n_samples n_components n_features	0.500000
standardize a dataset along any axis center to	axis with_centering with_scaling	0.333333
absolute error of the kl divergence of	manifold kl divergence error	0.100000
from features	fit x y sample_weight	0.020000
to the training set x	predict x	0.011765
array from the meta-information	joblib zndarray wrapper read unpickler	0.043478
update terminal	error update terminal	0.500000
x by scaling	x	0.001692
laplace approximation	classifier laplace	0.333333
the number of splitting iterations in the	kfold get n splits x	0.111111
arbitrary python object into	value filename	0.083333
for the california housing dataset	california housing	0.083333
fit linear model with	fit x y coef_init intercept_init	0.230769
compute the average hamming loss	metrics hamming loss y_true y_pred labels sample_weight	0.333333
of samples x	x y sample_weight	0.012987
apply the approximate feature map to x	core rbfsampler transform x	0.333333
of x from y along	x z reg	0.066667
num	num	1.000000
to a large sparse linear system	a	0.018182
low rank matrix with	low rank matrix	0.083333
for the given param_grid	cv get param iterator	0.166667
the free energy f v =	neural_network bernoulli rbm free energy	0.066667
function used to partition estimators between jobs	partition estimators n_estimators n_jobs	0.200000
the pairwise matrix	parallel pairwise x y func	0.166667
truncated randomized svd	randomized svd m n_components n_oversamples n_iter	0.500000
fit gaussian process classification model parameters	gaussian_process gaussian process classifier fit	0.500000
or lasso path using lars algorithm [1] the	linear_model lars path	0.100000
given type	type	0.125000
between x	x	0.001692
parameters with the	mixture	0.041667
axis center to the mean and component	x axis	0.015385
implement a single boost	weight boosting boost iboost	1.000000
of exception types to	parallel backend	0.030303
sure centering is not enabled for	robust scaler check array	0.250000
a fraction of time controlled by self	progress	0.100000
fit the model to data	estimator fit x y	0.200000
selected features	feature_selection selector	0.142857
low	low	0.750000
incremental fit	discrete nb partial fit x y classes sample_weight	0.166667
validation and conversion of csgraph	csgraph directed dtype csr_output	0.166667
elastic net path with coordinate	enet path	0.050000
get the parameters of the	get	0.012048
run fit on one set of parameters	model_selection fit grid point x y estimator parameters	1.000000
the local outlier factor	local outlier factor decision function	0.125000
actual fitting performing the search over parameters	base search cv fit x y parameter_iterable	0.333333
helper	helper alpha y	0.333333
shrunk covariance model according to the	covariance shrunk covariance fit x	0.083333
the curve auc using the trapezoidal	auc	0.020408
getter	covariance empirical covariance	0.071429
returns a list of edges	make edges	0.066667
elastic net optimization function	l1_ratio	0.030303
estimates the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x	0.125000
score	score y_true y_pred normalize sample_weight	0.125000
indices to split data into	kfold split x	0.250000
a mostly low	make low	0.333333
update nmf	update h x w	0.500000
utility for building a cv in a user	cv cv x	0.031250
posterior probabilities of classification	calibrated classifier cv predict proba	0.200000
posterior log probability of	bernoulli nb joint log likelihood	0.083333
for all meta estimators	meta	0.043478
uncompressed bytes from	externals joblib binary zlib	0.125000
operation is meant to be cached by a	index_file_path data_folder_path slice_ color	0.033333
return the kernel k x	gaussian_process matern call x	0.200000
representation	utils shape	0.013699
break the pairwise matrix in	metrics parallel pairwise x y	0.166667
training data and	y	0.005348
graph of neighbors	neighbors mixin radius neighbors graph	0.066667
check the test_size and train_size at	test_size train_size	0.200000
compute mutual information between two variables	feature_selection compute mi x	1.000000
covariance estimator	covariance	0.014493
u_based_decision	u_based_decision	1.000000
write array bytes to pickler file	wrapper write array array pickler	0.333333
too common features	count vectorizer limit features	1.000000
all the content of the data home	datasets clear data home data_home	0.076923
norm vector length	x norm axis	1.000000
compute area under the curve auc using	auc x y	0.040000
apply transforms and	x y sample_weight	0.012987
solve the isotonic regression model	core isotonic regression y	0.066667
perform dbscan clustering from vector array or	cluster dbscan x	0.200000
randomized linear	randomized linear	0.500000
factorization nmf	decomposition non negative factorization x	0.043478
fit	linear_model base sgdclassifier fit x	0.333333
a single sample	sample	0.032258
product with the	projection transform x	0.333333
return	shape repr	0.013699
for each input data point	val	0.037037
or	externals joblib multiprocessing backend	0.035714
perform dbscan clustering from vector array	cluster dbscan	0.125000
under the curve auc	metrics auc	0.040000
factorizing common classes param	fit first call clf classes	0.058824
predict multi-output variable using a	core multi output estimator predict x	0.166667
area under the curve auc from prediction scores	auc	0.020408
individually to unit norm vector length	norm axis copy	0.200000
descent fit is on grid	fit x	0.006410
x format check	allocation check	0.062500
apply clustering to a projection to	cluster spectral clustering affinity n_clusters n_components	0.166667
of the cholesky decomposition of	cholesky	0.083333
for selecting features based on importance weights	select from model	0.333333
input and compute prediction of init	base gradient boosting init	0.142857
is the solution to a sparse coding problem	sparse encode x dictionary	0.333333
for the lfw pairs dataset this operation	fetch lfw pairs	0.018868
estimator and predict values for a	and predict estimator x y train	0.200000
predict class	ensemble gradient boosting classifier predict	0.666667
building a cv	cv cv x	0.031250
depth	previous func code	0.333333
the f-beta score is the weighted harmonic mean	metrics fbeta score y_true y_pred beta	0.333333
that can actually run in parallel n_jobs	n_jobs	0.023256
updates terminal regions to median	terminal region	0.100000
results from clf predict calls	probas x	1.000000
updates terminal	terminal region	0.100000
matrices w h whose product approximates the non-	x w h	0.035714
back to	binarizer inverse	0.166667
loading of moved objects in six moves urllib_response	module six moves urllib response	0.333333
end of iteration	end ll	0.166667
determination r^2	multi output regressor score x y	0.200000
this function returns posterior probabilities of classification	classifier cv predict proba	0.200000
input data point	core	0.015385
curve auc using the trapezoidal rule	auc x	0.040000
the sign of elements of all the	sign	0.050000
[rouseeuw1984]_ aiming at computing mcd	covariance c step x n_support remaining_iterations initial_estimates	0.111111
eigenvalue decomposition	value norm_laplacian	0.142857
of the laplacian matrix and	laplacian	0.034483
procedure described in [rouseeuw1984]_ aiming at computing mcd	n_support remaining_iterations initial_estimates	0.111111
fit estimator and predict values for	core fit and predict estimator x y train	0.250000
function opening the right fileobject from	joblib read fileobject	0.100000
parameters theta as the maximizer of the reduced	gaussian process arg max reduced	0.200000
that implement the partial_fit api need to	utils check partial fit	0.038462
determination regression	r2	0.076923
scores this score corresponds	score y_true y_score	0.025000
compute the grid of alpha values	linear_model alpha grid x	0.166667
the data onto	ridge_alpha	0.052632
to split data into	model_selection cviterable wrapper split	0.250000
predict using the gaussian process regression model	gaussian_process gaussian process regressor	0.058824
apply clustering to a projection	spectral clustering affinity n_clusters	0.166667
standardize a dataset along any axis center	x axis with_mean with_std	0.333333
scale back the data to the original representation	inverse transform	0.062500
and false positives per binary classification	metrics binary clf curve y_true y_score pos_label	0.090909
expression of the dual gap convergence criterion the	covariance dual gap emp_cov precision_	0.071429
meta-information and	zndarray wrapper read unpickler	0.043478
cv	core check cv cv x y	0.031250
checker utility for building a cv in a	core check cv cv x y	0.031250
fit is	fit x	0.006410
generate cross-validated estimates for each	y cv	0.050000
creates a biclustering	spectral fit	0.250000
one-vs-all fashion several regression and binary classification algorithms	y classes neg_label pos_label	0.111111
locally linear embedding analysis on	locally linear embedding x n_neighbors n_components	0.071429
parameters and evaluates the reduced likelihood	reduced likelihood	0.100000
export a decision	export graphviz decision_tree out_file max_depth feature_names	0.333333
wrapped function cache result and	joblib memorized func	0.014706
of last step in pipeline after transforms	core pipeline fit predict	0.166667
predict class	predict	0.006849
mixin for radius-based neighbors searches	radius neighbors mixin	1.000000
reduce	reduce	0.086207
curve	curve	0.857143
building a cv in	core check cv cv x y	0.031250
for full covariance	normal density full x means covars	0.166667
theta as the maximizer of the reduced likelihood	process arg max reduced likelihood	0.250000
k-fold iterator variant	kfold	0.117647
grid	model_selection parameter grid	0.250000
return the	externals joblib get func	0.200000
can also predict based on an unfitted model	predict	0.006849
check to make sure weights are valid	neighbors check weights weights	1.000000
dictionary 'params' parameters	core pprint params offset printer	0.250000
a projection to	spectral	0.026316
score by cross-validation read more in the :ref	model_selection cross val score estimator	0.166667
model	decomposition base pca	0.071429
two clusterings of a	score labels_true labels_pred sparse	0.047619
type introduces an	utils shape repr	0.013699
shutdown the process or thread pool	base terminate	0.500000
absolute error regression loss read more in	absolute error y_true y_pred	0.142857
covtype	covtype	0.625000
for the voting classifier	ensemble voting classifier set	0.037037
fit all transformers transform the data and	union fit transform x y	0.500000
of the derived	x resp	0.166667
regression model we can	regressor	0.027027
the timestamp when pickling to avoid the	externals joblib memorized func reduce	0.050000
partition estimators	ensemble partition estimators	0.200000
decision function for	classifier decision function x	1.000000
fit multitaskelasticnet model with coordinate descent parameters	linear_model multi task elastic net fit x y	1.000000
check input and compute prediction of init	boosting init decision function	0.142857
centroids on x	fit x	0.006410
the case method='lasso' is	xy gram	0.090909
getter for the precision matrix	covariance empirical covariance get precision	0.250000
each input data point	cross val predict estimator x y	0.045455
generate polynomial and interaction features	polynomial features	0.333333
build a contingency matrix describing	contingency matrix labels_true labels_pred	0.166667
by its spectrum spectrum	spectrum n_samples n_features	0.166667
incrementally fit	core multi output regressor partial fit x	0.200000
document with nonzero entries	feature_extraction count vectorizer inverse transform	0.166667
vectors for reproducibility flips the sign of	utils deterministic vector sign	0.066667
the number of splitting iterations in the cross-validator	model_selection base kfold get n splits x y	0.111111
dense array format	linear_model sparse coef mixin densify	0.100000
classifier valid parameter keys can be listed	classifier	0.013699
function output for x relative to	metrics threshold scorer call clf x y	0.058824
determination regression score	r2 score y_true y_pred	0.125000
fit the model	fit x y sample_weight	0.040000
decides whether it is time	neural_network base optimizer trigger stopping msg verbose	0.250000
corresponding derivatives with respect to each parameter weights	activations deltas	0.032258
generate a sparse random	utils random choice csc n_samples	0.333333
friedman3	friedman3	0.454545
[1] and breiman [2]	make friedman3 n_samples noise	0.166667
reverse	feature_selection selector mixin inverse transform x	1.000000
gaussian and label samples by quantile this classification	make gaussian	0.125000
run in parallel	joblib parallel backend	0.045455
false positives per binary	binary clf curve y_true y_score pos_label sample_weight	0.090909
project data to vectors and cluster	project and cluster data vectors	0.333333
for data x with ability to	x	0.001692
with a	externals add	0.142857
linear embedding read more in the :ref user	linear embedding	0.083333
for building a cv in a	core check cv cv x y	0.031250
the shrunk	shrunk	0.043478
partially fit underlying	one vs one classifier partial fit x	0.166667
the decision function	decision function x	0.018868
mean and variance	utils mean variance	0.500000
run fit on the	fit	0.003257
recall the recall is the ratio tp /	metrics recall	0.033333
creates an affinity matrix for x using the	x y	0.002155
non-overlapping	label	0.045455
of the log of the determinant	log	0.018868
median absolute error regression loss read more in	metrics median absolute error y_true y_pred	0.166667
check that predict	utils check clusterer compute labels predict	0.250000
n-dimensional array in place using strides	arr patch_shape extraction_step	0.166667
returns the number of splitting iterations in the	group out get n splits	0.111111
average	y_score average	0.111111
true and false positives per	clf curve y_true y_score pos_label sample_weight	0.250000
binary labels the output of transform is	transform	0.011236
c such that for c in (l1_min_c infinity)	c x y	0.030303
two sets of biclusters	consensus	0.111111
of totally random trees	random trees embedding	0.250000
array-like or scipy sparse	binarize x	0.083333
is a raw file object e g created	joblib is raw file	1.000000
compute the laplacian kernel between x	laplacian kernel x	0.333333
optimization objective for the case method='lasso' is	xy gram	0.090909
decision function of	decision function	0.025000
we don't	func	0.011364
returns the bound	bound	0.083333
is a	is	0.066667
which are going	multiprocessing backend effective	1.000000
a single binary	binary	0.125000
predict using the trained	base multilayer perceptron predict x	0.333333
the parallel execution only a fraction of	externals joblib parallel	0.014085
update params with given gradients	updates grads	0.076923
to the binary classification task	score y_true y_score	0.025000
the wine	wine	0.111111
for factorizing common classes param	partial fit first call clf classes	0.058824
and a name	name	0.033333
a which this function is called to	externals joblib	0.004762
check	model_selection check	1.000000
for the lfw people dataset this operation is	fetch lfw people	0.040000
convert coefficient matrix	linear_model sparse coef	0.076923
note this implementation is	sample_weight	0.018519
generate isotropic gaussian blobs for clustering	blobs n_samples n_features centers cluster_std	0.333333
check that	utils check clusterer	1.000000
maximum likelihood estimator covariance model according to the	covariance empirical covariance fit	0.166667
procedure described in [rouseeuw1984]_ aiming at computing	n_support remaining_iterations initial_estimates	0.111111
sparse and dense inputs	y	0.002674
w to	w ht	0.250000
this dataset is described in celeux et	datasets	0.015152
number of splitting iterations in the	get n splits x y	0.111111
given radius of	x radius	0.058824
of exception types	joblib	0.007299
compute the weighted log	mixture base mixture score	0.111111
of the parallel execution only a	externals joblib parallel	0.014085
fit linear model with passive aggressive algorithm	passive aggressive regressor fit x	1.000000
kernel	stationary kernel	0.333333
apply clustering to a	cluster spectral clustering affinity n_clusters	0.166667
compute a continuous tie-breaking ovr decision function	utils ovr decision function predictions confidences n_classes	0.333333
of u such	flip u	0.047619
the leaf	tree apply	0.166667
compute area under the curve auc using the	auc x	0.040000
for unbalanced datasets	weight class_weight y	0.200000
predict regression target at each stage	boosting regressor staged predict	0.500000
determinant with	det fit x y	0.333333
matrix x and target y	neural_network base multilayer perceptron partial	0.166667
compute prediction of init	gradient boosting init	0.142857
under the curve auc using the trapezoidal rule	auc x	0.040000
of neighbors for points in	neighbors mixin radius neighbors	0.125000
function to a	externals joblib customizable pickler register	0.200000
the log-likelihood of	score	0.010101
factor of x (as bigger is better i	factor decision function x	0.166667
locally linear embedding analysis on the data	locally linear embedding	0.050000
update the variational distributions for the precisions	mixture dpgmmbase update precisions x	1.000000
propagation module	propagation	0.076923
meta	meta	0.260870
a cv in a user friendly way	cv cv	0.031250
fit the model	estimator fit	0.200000
exponential chi-squared kernel x	metrics chi2 kernel x	0.333333
actually run in parallel	parallel backend	0.030303
to	joblib parallel backend base get	0.066667
solve the isotonic regression model	core isotonic regression	0.055556
compute elastic net path	path x	0.045455
and handles divide-by-zero	metrics prf divide numerator denominator metric modifier	0.250000
job	parallel	0.019231
for factorizing common classes	partial fit first call clf classes	0.058824
returns the number of splitting iterations in	kfold get n splits x y groups	0.111111
= tanh(gamma <x y> + coef0)	gamma coef0	1.000000
a list of edges for a	make edges	0.066667
finds seeds	get bin seeds x	0.250000
timestamp when pickling to	externals joblib memorized func reduce	0.050000
fit the rfe model and	fit x y	0.005988
q_ijs	params p neighbors degrees_of_freedom	0.250000
minimum and maximum	utils min max	0.500000
load sample images for image	datasets load sample images	0.250000
point	val predict estimator x	0.045455
loader	subset data_home	0.125000
of the breakdown point	breakdown point	0.333333
array-like or scipy	binarize x	0.083333
a batch of estimators within a job	estimators n_estimators ensemble	0.083333
swaps two columns of a csc/csr matrix in-place	utils inplace swap column x m n	0.250000
passive aggressive algorithm	linear_model passive aggressive regressor	0.250000
message on the end of iteration	msg init end ll	0.333333
sure that	core check	0.111111
force the execution	externals joblib memorized	0.013699
training set x and returns	predict x y	0.043478
a given radius of a point or	radius	0.045455
evaluate a score by cross-validation read more in	cross val score estimator x	0.166667
display the process of the parallel execution	externals joblib parallel print	0.125000
youngs and	and	0.062500
isolation	isolation	1.000000
of determination	metrics r2	0.125000
number of splitting iterations in	cviterable wrapper get n splits x y	0.111111
the silhouette coefficient	metrics cluster silhouette	0.333333
return the path of the	datasets get	0.200000
absolute error regression loss read more in the	absolute error	0.142857
check initial parameters of	mixture check parameters	0.166667
pairwise matrix	metrics parallel pairwise x y func	0.166667
decisions within	estimators_features x	0.111111
the callers	externals joblib effective	0.200000
false for indices increasingly apart the distance depending	externals joblib verbosity filter index	0.055556
bytes_limit	externals joblib	0.004762
function call	call func	0.100000
checker utility for building a cv in	check cv cv	0.031250
of a logistic	linear_model logistic	0.111111
to avoid	memorized func	0.016949
a cv in a user	check cv cv x y	0.031250
training subsets and validate 'train_sizes'	train_sizes n_max_training_samples	0.200000
perform classification on samples in x	svm one class svm predict x	1.000000
given cache	joblib cache	0.250000
store the timestamp when pickling to avoid the	externals joblib memorized func reduce	0.050000
k x	dot product call x	0.200000
the dense dictionary factor in	dict dictionary y	0.111111
each input data point	core cross val predict estimator x y	0.045455
process of the parallel	parallel	0.019231
evaluates the reduced likelihood function	process reduced likelihood function	0.047619
a calibration curve	core calibration curve	0.142857
the closest cluster each sample	cluster	0.021277
don't	externals joblib memorized func	0.013158
x and returns the transformed data	x y w h	0.500000
of a cross-validated score with	score estimator x y cv	0.083333
is meant to be cached by a	index_file_path data_folder_path slice_ color	0.033333
data home	datasets clear data home	0.076923
loading for the lfw pairs dataset this	lfw pairs	0.018868
determine absolute sizes of	train sizes	0.066667
a score by cross-validation read more in	cross val score	0.166667
and variance along an axix on	variance axis x axis	0.090909
determine absolute sizes of training	core translate train sizes	0.066667
extent the local structure is retained	x x_embedded n_neighbors precomputed	0.200000
load text files with categories as subfolder names	datasets load files container_path description categories	1.000000
back the data to	scaler inverse transform	0.058824
to output a function	externals joblib function called str function_name	0.250000
of a test that was skipped	skip test	0.200000
median absolute error regression loss read	median absolute error y_true	0.166667
the voting classifier valid parameter	ensemble voting classifier	0.031250
computes the position	mds fit x y	0.066667
corresponding to test sets	core partition iterator iter test	1.000000
for reducing the size	items root_path	0.066667
number of splitting iterations in the	leave one out get n splits	0.111111
and evaluates the reduced likelihood function	gaussian process reduced likelihood function	0.047619
the number of splitting iterations in	one group out get n splits x	0.111111
bandwidth	bandwidth	1.000000
also predict	predict x	0.011765
fit kernel ridge regression model parameters	core kernel ridge fit x y sample_weight	1.000000
compute class covariance	core class cov	0.250000
indicate if wrapped estimator is using a precomputed	core one vs one classifier pairwise	0.200000
in the raw documents	feature_extraction count vectorizer fit raw_documents y	0.125000
and hide warnings without visual nesting	utils ignore warnings call fn	0.200000
call predict on the estimator with	predict x	0.011765
using numpy	numpy	0.083333
x according to	inverse transform x	0.025641
csgraph inputs	graph csgraph	0.250000
output of transform is sometimes referred to by	transform y	0.023256
computes the log-likelihood of a gaussian data	covariance empirical covariance score	0.166667
data and	score x y	0.030303
model according to the given	sample_weight	0.037037
it	func	0.011364
the distortion introduced by a random	n_samples	0.058824
given param_grid	grid search cv get param iterator	0.166667
the gp prior	return_std return_cov	0.142857
signature from the given list of	externals signature	0.050000
loss read more in the :ref user guide	y_true y_pred	0.074074
the recall the recall is the ratio tp	metrics recall	0.033333
dense dictionary factor	dict dictionary y	0.111111
of the kernel	normalized kernel	0.250000
center kernel matrix	preprocessing kernel centerer transform k y copy	0.500000
distribute	distribute	1.000000
and cpp	and cpp	1.000000
incremental fit on a batch	discrete nb partial fit x y classes sample_weight	0.166667
arguments	joblib memorized func get output	0.125000
position of the points in	mds fit x y	0.066667
getter for	empirical	0.055556
curve auc from prediction scores note this	metrics roc auc score	0.166667
the gaussian process regression model	gaussian process regressor	0.055556
downloading it if necessary	data_home subset download_if_missing random_state	1.000000
sample from the	neural_network bernoulli rbm sample	0.500000
function opening the right fileobject from	read fileobject	0.100000
an 'l' suffix when using	utils shape repr	0.013699
outlyingness of observations in x according	covariance outlier detection mixin predict x	0.250000
a sparse	a	0.018182
check that the parameters are well defined	gaussian mixture check parameters x	1.000000
single tree in parallel	ensemble parallel build trees tree	0.200000
signature from the given list of	signature	0.047619
matrix factorization nmf	negative factorization x	0.043478
inplace column scaling of a	inplace column scale	0.166667
mean squared error between	error norm comp_cov norm scaling squared	0.166667
each mixture component	mixture gmmbase get covars	0.250000
perform classification on test vectors	core dummy	0.142857
transform is sometimes	transform y	0.023256
a	utils shape repr	0.013699
leaf that each sample is predicted as	tree base decision tree apply x check_input	0.500000
customizable	customizable	0.833333
predict regression target at each stage for	ensemble gradient boosting regressor staged predict	0.500000
predict is invariant	labels predict	0.250000
dictionary factor in place	dict dictionary y code verbose	0.333333
check the estimator	estimator	0.044118
return a tolerance which is independent of	tolerance x tol	0.058824
target of new samples can be different from	calibrated classifier	0.083333
the position	mds fit x y init	0.066667
for the given autocorrelation parameters theta	theta	0.076923
the data x which	x y	0.002155
compute data precision matrix with the generative	get precision	0.052632
fit_predict of last step in pipeline after transforms	pipeline	0.083333
non-negative matrix factorization	factorization	0.035714
to run in	externals joblib	0.004762
fit the model	output estimator fit x	0.200000
binary classification task	y_true y_score	0.054054
create all the covariance	matrix to match covariance	0.250000
set the diagonal of the laplacian matrix and	set diag laplacian	0.333333
regions	regions tree x	1.000000
continuous target variable	regression x y discrete_features n_neighbors	1.000000
on training subsets incrementally	core incremental fit	0.500000
predict the	cv predict	0.041667
compute l1 and l2 regularization coefficients for w	compute regularization alpha l1_ratio regularization	0.333333
estimate sample weights by class	compute sample	0.100000
the number of splitting iterations in the cross-validator	model_selection base cross validator get n splits x	0.125000
var	var	1.000000
does nothing this transformer is stateless	feature_extraction hashing vectorizer fit x	1.000000
list of exception	externals joblib parallel backend base	0.034483
shutdown the process or thread	backend base terminate	0.500000
0 if y - pred	y pred	0.250000
each	cross	0.037037
for the precisions	precisions x	0.250000
kernel k x y and optionally its gradient	gaussian_process compound kernel call x y eval_gradient	1.000000
rand index adjusted for	adjusted rand	0.333333
on the value of verbose	verbose	0.062500
model using x as training	x skip_num_points	0.166667
selected	selected	0.714286
back the	standard scaler inverse transform	0.066667
h	h n_components	0.166667
homogeneity metric of a cluster labeling given	metrics cluster homogeneity score	0.500000
estimator with the best found parameters	model_selection base search cv	0.120000
private function used to partition estimators	partition estimators	0.200000
parameters theta as the maximizer	gaussian process arg max	0.047619
of training subsets and validate 'train_sizes'	train_sizes n_max_training_samples	0.200000
k-fold iterator variant with non-overlapping labels	label kfold	0.250000
computes the log-likelihood of a gaussian data set	covariance empirical covariance score	0.166667
boundary of the set of samples x	x y sample_weight	0.012987
generate cross-validated estimates for	x y cv	0.050000
first prime element in the specified row	prime in row row	0.333333
the number of splitting iterations in	out get n splits x y	0.111111
the long type introduces	shape	0.011765
"news" format strip the	datasets strip newsgroup	0.090909
back the	preprocessing robust scaler inverse transform	0.066667
the posterior log probability of the	core bernoulli nb joint log likelihood	0.083333
quantiles to be used	preprocessing robust	0.111111
function used to build a batch	ensemble parallel build	0.047619
compute the weighted	mixture base mixture score	0.111111
the one-vs-one multi	svm one vs one	0.050000
time it take	squeeze time	0.166667
the deviance	deviance call y	0.333333
cross-validated score with permutations	core permutation test score estimator x y cv	0.166667
helper function to test error messages in exceptions	raise message exceptions message function	1.000000
the process or thread	joblib multiprocessing	0.052632
update h	h beta_loss	0.333333
linear embedding read more in	linear embedding	0.083333
the precision	precision	0.016667
probability calibration with sigmoid method platt 2000 parameters	core sigmoid calibration df	0.500000
simply return the input array	neural_network identity x	1.000000
distances between the	distances	0.100000
error regression loss read more in the	error	0.060000
shutdown	joblib multiprocessing backend terminate	0.166667
with stochastic gradient descent	y coef_init intercept_init	0.333333
scale back the data to	robust scaler inverse transform	0.066667
next	next	1.000000
vocabulary dictionary and return term-document matrix	transform raw_documents y	0.100000
randomly drawn parameters	core randomized search cv	0.200000
binary classification	y_true	0.043478
under windows this is the time	time t	0.125000
mostly low	make low	0.333333
mean and variance along an axix on	utils mean variance axis x axis	0.142857
huber	linear_model huber	0.333333
getstate	getstate	1.000000
avoid the hash depending	joblib memory	0.016949
full	multivariate normal density full x means covars min_covar	0.166667
k-folds cross validation iterator	kfold	0.058824
a binary	metrics average binary score	0.500000
for multiclass	multiclass y classes	0.500000
predicted probabilities for a calibration curve	calibration curve y_true	0.142857
regression problem with sparse uncorrelated design	make sparse uncorrelated n_samples	0.166667
sparse and	y sample_weight random_state	0.166667
a which this function is	externals joblib memorized func	0.013158
apply transforms and	y sample_weight	0.017857
cache for the	joblib memorized func	0.014706
thresholding of array-like or scipy sparse	preprocessing binarize	0.083333
grid of	ensemble grid	0.111111
content of the data	data	0.038462
coverage file from	coverage	0.125000
from prediction scores note this implementation is restricted	metrics roc	0.040000
func to be run	manager mixin apply async func	0.250000
blobs	datasets make blobs	0.333333
kernel k x	gaussian_process rbf call x	0.200000
the smacof	manifold smacof	0.200000
raises an exception in an unfitted	estimators unfitted name	0.142857
of the function with the given arguments	func	0.011364
theta as the maximizer	gaussian_process gaussian process arg max	0.047619
convert coefficient matrix to	sparse coef mixin	0.083333
fit with all	search cv fit	0.111111
graph of neighbors for points in x	mixin radius neighbors graph x	0.500000
solve the linear assignment problem using the hungarian	utils linear assignment	0.090909
we don't store the	joblib memorized func	0.014706
arbitrary python object into one file	dump value filename	0.083333
problem this dataset is described in friedman	datasets make	0.015625
validation and conversion of csgraph	graph csgraph directed dtype csr_output	0.166667
for all meta	meta	0.043478
this implementation is restricted to the binary classification	y_true y_score pos_label sample_weight	0.066667
transform with the	transform	0.011236
average a	average	0.066667
returns the number of splitting iterations in the	pgroups out get n splits x	0.111111
exception types to	joblib parallel	0.028571
h in multiplicative update nmf	multiplicative update h x w h beta_loss	0.250000
step length is not found	line search	0.029412
regression	regression y	1.000000
run fit on one set of	core fit grid point x y	0.500000
found and raise an exception if a	line search	0.029412
to avoid the	externals joblib memory	0.016949
actual fitting performing the	x y parameter_iterable	0.500000
of transform is sometimes referred to	transform	0.011236
y as	y	0.005348
jobs that can actually run in parallel n_jobs	jobs n_jobs	0.100000
x y t	x y	0.002155
right fileobject from a filename	externals joblib read fileobject fileobj filename	0.250000
type suitable for scipy sparse indices	int	0.100000
val	val	0.185185
returns the index'th estimator	getitem index	0.333333
data to maximize class separation	core linear discriminant analysis transform x	0.250000
new estimator with the same parameters	core clone estimator	0.333333
allocate	zeros shape dtype order align	1.000000
w h	w h	0.062500
the	empirical	0.055556
squared logarithmic error regression loss	squared log error	0.166667
masks	masks	0.750000
locally linear embedding analysis	manifold locally linear embedding x	0.071429
estimate sample	sample	0.032258
check the	mixture check	0.285714
number of splitting iterations in the cross-validator	predefined split get n splits x	0.111111
false for indices increasingly apart the distance depending	joblib verbosity filter index	0.055556
out cross-validator	out	0.095238
as	neighbors local outlier	0.142857
precisions	check precisions precisions covariance_type	0.250000
expression of the dual gap convergence criterion the	dual gap emp_cov precision_ alpha	0.071429
a covariance matrix shrunk on the	covariance shrunk covariance	0.090909
sample from a	a size replace p	0.142857
pursuit model omp parameters	pursuit	0.181818
to dense array	coef mixin densify	0.100000
init	boosting init decision function	0.142857
c in (l1_min_c infinity)	c x y loss fit_intercept	0.030303
inverse the transformation	inverse transform xred	1.000000
constructs a transformer from an arbitrary callable	transformer	0.100000
log-det of the cholesky decomposition of	cholesky matrix_chol	0.500000
generate cross-validated	core cross val predict estimator x y cv	0.071429
updates terminal regions to	terminal	0.047619
the l1 distances between the vectors	paired manhattan distances	0.083333
samples in x into a matrix of	x	0.001692
on the training	factor fit predict	0.066667
warning used when the metric	metric warning	1.000000
cv in	cv cv x	0.031250
between two covariance estimators	covariance	0.014493
dictionary	dictionary	0.428571
update the dense	update dict	0.333333
and return the number of workers	externals joblib parallel	0.014085
check x format and	dirichlet allocation check	0.062500
from source to all reachable	graph source cutoff	0.200000
curve	curve y_true y_prob normalize	1.000000
each input	predict estimator x y	0.045455
log	log	0.320755
the position of	mds fit	0.066667
estimate mutual information	feature_selection mutual info regression x y	0.500000
predict is invariant of	predict	0.006849
of x i	x	0.001692
tolerance which is independent of the dataset	cluster tolerance	0.058824
include_dont_test	include_dont_test	1.000000
x for later	x y	0.002155
score with permutations	core permutation test score	1.000000
thresholding	thresholding y output_type	0.500000
given arguments and persist the output values	func call	0.047619
seek	seek	1.000000
computes the weighted graph of neighbors	radius neighbors graph	0.066667
estimator	estimator estimator	0.105263
k x	gaussian_process matern call x	0.200000
jpeg pictures of famous people	funneled resize	0.142857
class covariance	class cov x y	0.250000
row scaling of	row	0.066667
the maximizer of the reduced likelihood function	arg max reduced likelihood function	0.333333
timestamp when pickling	externals joblib memory reduce	0.030303
a hash	hash mixin	0.500000
number of splitting iterations in the	model_selection base cross validator get n splits	0.125000
fit the model	svr fit x	0.333333
check initial parameters	base mixture check parameters x	0.200000
with the best found parameters	model_selection base search cv predict proba	0.076923
class for unbalanced datasets	weight class_weight y indices	0.200000
returns the number of splitting iterations in	leave one out get n splits x	0.111111
the state of	state	0.066667
pinvh	pinvh	1.000000
apply a mask	weights mask	0.333333
to the separating hyperplane	base lib svm decision function	0.333333
the gaussian process model	gaussian process predict	0.500000
median absolute error regression loss read	median absolute error	0.166667
evaluation of the graph-lasso objective function the objective	covariance objective	0.125000
return the directory	dir	0.038462
classification on test vectors	core dummy	0.142857
to build a batch of estimators within a	ensemble parallel build estimators n_estimators ensemble	0.166667
permutation	permutation	0.833333
make sure that an estimator implements	check estimator estimator	0.142857
number of splitting iterations in the	kfold get n splits x y groups	0.111111
scale back the	preprocessing standard scaler inverse transform x copy	0.066667
when memory is inefficient to	x y classes	0.027778
discrete target variable	discrete_features n_neighbors	0.500000
generate train	split	0.027778
the closest cluster each sample in	cluster	0.021277
a function	function	0.021277
process	joblib multiprocessing backend	0.052632
cf tree for	cluster birch fit x y	0.200000
estimate model parameters	mixture base mixture fit	0.200000
log-transformed bounds on the	kernel bounds	0.333333
schedule a func	externals joblib sequential backend apply async func callback	0.250000
return the number of workers	externals joblib	0.004762
by a random projection p only changes the	core johnson lindenstrauss min dim n_samples	0.142857
compute log(det a for a symmetric equivalent to	utils fast logdet a	0.500000
the exponential chi-squared kernel x and y	metrics chi2 kernel x y gamma	0.333333
for	core cross val predict	0.045455
types to	backend base get	0.066667
inverse label binarization transformation for	preprocessing inverse binarize	0.250000
affinity matrix for x using	x	0.001692
regions (=leaves)	region tree terminal_regions leaf x	1.000000
prefetch the tasks for the next batch and	one batch iterator	0.500000
input data point	core cross	0.045455
run fit	fit grid	1.000000
a sparse	sparse	0.050000
for binary classification used in hastie et al	make hastie 10 2 n_samples random_state	0.166667
predict class at each stage for x	boosting classifier staged predict x	1.000000
computes the log-likelihood of a	covariance empirical covariance score	0.166667
solution to a large	lsqr a	0.037037
swaps two rows of a csr matrix in-place	utils inplace swap row csr x m	0.250000
of x	transform x	0.050847
kddcup99 dataset	kddcup99	0.090909
update the dense dictionary	update dict dictionary y	0.333333
returns the submatrix corresponding to	submatrix	0.090909
generate	n_samples n_features n_classes	0.333333
fit a multi-class classifier by combining	sgdclassifier fit	0.076923
loader for the california housing	california housing data_home download_if_missing	0.250000
any axis center	axis	0.028169
back the data	preprocessing standard scaler inverse transform x copy	0.066667
validate input params	linear_model base sgd validate params	1.000000
array or sparse matrix x	x	0.001692
number of splitting iterations in	out get n splits x	0.111111
of y and class_weight	targets y	0.500000
list of exception types to	externals joblib parallel backend base	0.034483
lfw people	lfw people	0.040000
predict class probabilities for	ensemble gradient boosting classifier predict proba	0.500000
or not	feature_extraction mask	0.500000
for each	cross val	0.038462
the number of splitting iterations in the	one group out get n splits x y	0.111111
_fit_coordinate_descent update	update	0.035714
the labeled faces in the wild lfw pairs	fetch lfw pairs subset	0.035714
generate train test	split iter	0.166667
learn the vocabulary dictionary and return term-document	fit transform raw_documents y	0.100000
compute the maximum	max	0.071429
of x (as bigger	function x	0.030303
data point	estimator x y	0.038462
routine for validation and conversion of	dtype csr_output	0.166667
for fit	fit x	0.006410
matrices from a	covariance_type	0.083333
make and configure a copy	ensemble make estimator append random_state	0.166667
the paired cosine distances between	paired cosine distances	0.333333
type suitable	int	0.100000
undo	min max	0.250000
load the kddcup99 dataset downloading it	brute kddcup99 subset data_home download_if_missing random_state	0.111111
matching pursuit omp solves n_targets orthogonal	linear_model orthogonal mp x	0.250000
returns the number of splitting iterations in	model_selection predefined split get n splits x	0.111111
return the file descriptor for the underlying file	binary zlib file fileno	0.333333
getter for	covariance get	0.166667
the decision function of the given	decision function x	0.018868
in parallel	joblib parallel	0.028571
parametergrid instance for the given param_grid	param iterator	0.166667
right fileobject from a filename	read fileobject fileobj filename mmap_mode	0.250000
and false positives per binary classification threshold	binary clf curve	0.090909
a tolerance which is independent of the dataset	cluster tolerance	0.058824
dual gap convergence criterion the specific definition	covariance dual gap emp_cov	0.071429
error regression loss read	error	0.060000
slices containing	batches	0.166667
local outlier factor of x	neighbors local outlier factor decision function x	0.100000
h whose product approximates	h n_components	0.166667
return a tolerance which	tolerance	0.045455
function to a	externals joblib customizable pickler	0.200000
as the maximizer of the	gaussian process arg max	0.047619
load dataset from	datasets load	0.083333
to avoid the hash depending	memorized	0.015873
fit with all sets	cv fit	0.200000
mean_shift	bin_size min_bin_freq	0.500000
restricted to the binary classification	y_true y_score average sample_weight	0.076923
a contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true labels_pred eps sparse	0.200000
which are going to run in parallel	joblib multiprocessing backend	0.052632
pool	externals	0.005747
the meta-information and	externals joblib zndarray wrapper read unpickler	0.043478
get the parameters of	voting classifier get	0.200000
batch and dispatch them	parallel dispatch one batch	0.500000
returns the submatrix corresponding	get submatrix	0.166667
keeping track of time	time	0.047619
for each input data	cross val predict estimator	0.045455
the kernel is	kernel operator is	0.333333
function called with the given arguments	func get	0.100000
mean absolute	metrics mean absolute	1.000000
for a calibration curve	calibration curve y_true	0.142857
converting coef_	sparse coef	0.071429
laplacian matrix and convert	laplacian	0.034483
back the data to the original representation	scaler inverse transform x copy	0.066667
random_state	random_state	0.384615
the estimator with the best found	model_selection base search cv predict	0.076923
by cross-validation read more in the	cross val	0.038462
oracle approximating shrinkage	oas x assume_centered	0.500000
evaluates the reduced likelihood function for	gaussian process reduced likelihood function	0.047619
strip the headers	strip	0.055556
thread	joblib multiprocessing backend	0.052632
model to the training set x	x	0.001692
transform the data	transform	0.011236
perform standardization by centering and scaling	standard scaler transform x y copy	0.333333
seeds	get bin seeds	0.250000
the estimator with randomly drawn	randomized search cv	0.166667
score with	score estimator x y	0.125000
opposite of the local	neighbors local	0.250000
of the scaler	min max scaler	0.083333
solution to a sparse coding problem	decomposition sparse encode x dictionary gram cov	0.333333
determine the optimal	joblib parallel backend base compute	1.000000
fit estimator and transform	embedding fit transform x y	1.000000
folders to make cache size fit	externals joblib memory reduce size	0.083333
locally linear embedding	manifold locally linear embedding x n_neighbors	0.071429
principal component analysis pca	pca	0.047619
procedure described in [rouseeuw1984]_ aiming at computing	c step x n_support remaining_iterations initial_estimates	0.111111
sparse random projection matrix parameters	core base random projection	0.200000
classification used in hastie et al	datasets make hastie 10 2 n_samples random_state	0.166667
the laplacian kernel between x and y	metrics laplacian kernel x y	0.333333
vectors for reproducibility flips the sign of elements	utils deterministic vector sign	0.066667
the binary classification task	curve y_true	0.125000
fit the model according to	linear svr fit x y sample_weight	0.250000
solve	solve	1.000000
global	cluster birch global	1.000000
to the cache for the function	externals joblib memorized func	0.013158
fit the	svm linear svr fit x	0.333333
return posterior probabilities	core quadratic discriminant analysis predict proba x	0.333333
the backend and return the number of workers	externals joblib parallel backend base	0.034483
number of splitting iterations in	group out get n splits x y groups	0.111111
meta estimators	meta estimator	0.062500
estimators that	utils check partial fit	0.038462
generate train test	iter	0.050000
computes the log-likelihood of a gaussian data	empirical covariance score	0.166667
predict confidence scores for samples	linear_model linear classifier mixin decision function	0.500000
when memory is inefficient to train all data	x y classes	0.027778
the binary classification	y_true y_score average sample_weight	0.076923
check the validity of the input parameters	check params x metric p metric_params	0.200000
display the process of the	print	0.076923
labels back to	binarizer inverse	0.166667
dense dictionary	dict dictionary y	0.111111
to	parallel backend	0.030303
sure centering is not enabled for sparse	preprocessing robust scaler check array	0.250000
center and scale the data parameters	robust scaler transform x y	0.200000
initialization	initialize x	0.400000
names	names	0.636364
voting classifier valid parameter keys can be	ensemble voting classifier	0.031250
decorator used to capture the arguments	check_pickle	0.040000
least-squares solution to a large	utils lsqr a	0.037037
of neighbors for points	neighbors	0.054054
tokens	tokens	1.000000
model fitting	fit x	0.006410
input data	core cross val predict	0.045455
the decision	ensemble gradient boosting classifier decision	0.333333
job	ensemble parallel	0.250000
perform classification on an array of test vectors	gaussian_process gaussian process classifier predict	1.000000
homogeneity metric of a cluster labeling given	cluster homogeneity score	0.500000
graph of neighbors for points	neighbors radius neighbors mixin radius neighbors graph	0.066667
along an axis on a	x axis	0.015385
a score by cross-validation read more in	model_selection cross val score estimator x y	0.166667
ledoitwolf estimator ledoit-wolf is a particular form of	ledoit wolf	0.111111
estimates for each	val	0.037037
permutation_test_score	core permutation test score estimator	0.500000
and variance along an axix	variance axis x axis	0.090909
cv aka logit	cv	0.009009
the pairwise matrix in	metrics parallel pairwise	0.166667
inplace row scaling of a csr or	inplace row scale x scale	0.142857
exception in an unfitted	estimators unfitted name	0.142857
classification loss	loss y_true	0.250000
validate input params	base sgd validate params	1.000000
multinomial loss and	multinomial loss w x y	0.333333
in the wild lfw pairs dataset this	fetch lfw pairs subset	0.035714
store the timestamp when pickling	memorized func reduce	0.050000
helper	helper alpha y v	0.333333
the hash depending from it	joblib memorized	0.015625
the covariance	mixture distribute covar matrix to match covariance	0.250000
of a csc/csr matrix in-place	utils inplace swap column x	0.250000
for the given param_grid	param iterator	0.166667
label sets	preprocessing multi label binarizer transform y	0.250000
number of splitting iterations in the	one group out get n splits x	0.111111
returns the submatrix corresponding to bicluster i	core bicluster mixin get submatrix i	0.333333
x y and scale if the scale parameter==true	x y scale	0.500000
with the best found	search cv predict	0.074074
the number of free parameters in the model	mixture n parameters	1.000000
exception types to be captured	base get exceptions	0.166667
the number of splitting iterations in	group out get n splits x	0.111111
a single boost using the	ada boost classifier boost	0.100000
create dataset abstraction	linear_model make dataset	1.000000
parallel processing this method	parallel	0.019231
x and	predict x y	0.086957
for building a cv in a user	core check cv cv x	0.031250
for each	predict estimator x y	0.045455
evaluate the density model on the data	density score samples	0.250000
discrete target variable	classif x y discrete_features n_neighbors	1.000000
shrunk ledoit-wolf	ledoit wolf shrinkage	0.250000
capture the arguments of a function	delayed function check_pickle	0.333333
and gradient	and grad w x	1.000000
all the covariance matrices from a given template	matrix to match covariance type tied_cv covariance_type n_components	0.333333
discrete	n_neighbors	0.125000
reduced likelihood	process reduced likelihood	0.142857
classifier valid parameter keys can be	classifier set	0.125000
computes an orthonormal matrix whose range	size n_iter power_iteration_normalizer	0.166667
fit the model by computing truncated svd	decomposition pca fit truncated x n_components	1.000000
matrix whose range approximates the range of	utils randomized range finder	0.083333
sparse uncorrelated design	make sparse uncorrelated n_samples	0.166667
kddcup99 dataset downloading it if necessary	brute kddcup99 subset data_home download_if_missing random_state	0.111111
fit	fit x y coef_init	0.250000
transform a sequence of instances to a scipy	feature hasher transform raw_x y	0.333333
weiszfeld step	weiszfeld step x x_old	1.000000
the precision is the	metrics precision	0.033333
process regression model we can also predict	process regressor predict x	0.500000
sparse random matrix given	random choice csc	0.166667
of data under each gaussian in	mixture gmmbase	0.034483
an arbitrary python object	externals joblib dump value filename	0.083333
coefficient matrix	linear_model sparse coef mixin	0.090909
get	linear_model base randomized linear model get	0.500000
product with	projection transform	0.333333
the model by computing full svd	full	0.055556
return the kernel k	gaussian_process pairwise kernel call	0.333333
embedding analysis on the data	embedding x n_neighbors n_components	0.200000
for each input data	val	0.037037
from the c and	from c and	1.000000
in n_jobs even	y func n_jobs	0.166667
the voting classifier valid parameter keys can be	voting classifier set	0.037037
check if vocabulary is empty or	mixin check vocabulary	0.250000
h	x w h	0.035714
computes the free energy f v	bernoulli rbm free energy	0.066667
score by cross-validation read more in the :ref	model_selection cross val score estimator x y groups	0.166667
cross-validated estimates for each input data point	core cross val predict estimator x y cv	0.071429
be captured	base get exceptions	0.166667
columns of	columns x columns	0.250000
fit multitaskelasticnet	multi task elastic net fit x	1.000000
matching	matching	1.000000
private function used to	parallel decision function estimators	0.333333
the data and	x y	0.002155
force the execution	memorized	0.015873
imgs	imgs	1.000000
for a given dataset split	scorer	0.045455
avoid the hash depending from it	externals	0.011494
estimate model parameters with the em algorithm	mixture base mixture fit x y	0.200000
input data point	cross val predict	0.045455
nonzero componentwise l1 cross-distances between the vectors	gaussian_process l1 cross distances	0.111111
fit an estimator within a job	parallel fit estimator estimator x	0.333333
as training data	orthogonal matching pursuit cv	0.200000
samples from a gaussian	mixture sample gaussian	1.000000
to	func	0.011364
is restricted to the binary classification task	score y_true y_score	0.025000
the number of splitting iterations in the cross-validator	model_selection cviterable wrapper get n splits x y	0.111111
regressor from the training set x y	regressor fit x y sample_weight	1.000000
fit the model	svc fit	0.333333
the boolean mask	preprocessing get mask	0.333333
an affinity matrix for x using the	x	0.001692
output of transform is	transform	0.011236
the k-neighbors	kneighbors mixin kneighbors x n_neighbors	0.125000
analysis a	analysis	0.090909
cv in a user friendly way	check cv cv x y	0.031250
x	inverse transform x	0.025641
a grid of points based on	ensemble grid from	0.166667
absolute error regression	absolute error y_true	0.142857
transform on the estimator with the best found	search cv transform x	1.000000
x into a matrix of	x	0.001692
of a memmap instance to reopen on	memmap a	0.050000
dictionary learning finds a dictionary a set	batch dictionary learning	0.142857
function opening the right fileobject from a	read fileobject	0.100000
cluster the result	cluster	0.021277
factorizing common classes param logic estimators that	utils check partial fit first call clf classes	0.333333
decision functions of the base classifiers	ensemble bagging classifier decision function x	0.333333
generate a sparse random projection matrix	random projection	0.125000
and 'return_annotation'	init parameters return_annotation __validate_parameters__	0.125000
decision function of x	ensemble ada boost classifier decision function x	0.333333
mini-batch dictionary learning finds a dictionary	mini batch dictionary learning	0.142857
of x and dot w	divergence x w	0.500000
to split data into	base kfold split x	0.250000
wild lfw	fetch lfw	0.041667
ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x assume_centered block_size	0.125000
generate the random projection matrix	random projection make random matrix n_components n_features	1.000000
of distances and a parameter weights	weights dist weights	0.142857
of x from y along the	x z	0.050000
the timestamp when pickling	func reduce	0.050000
a binary classifier on x and y	binary x y alpha	0.500000
unpickler	unpickler	0.636364
creates an affinity matrix for x using	x	0.001692
reconstruct the	reconstruct	0.125000
net path with coordinate	path x	0.045455
fit the model	linear svr fit x y	0.333333
feature name -> indices mappings	feature_extraction dict vectorizer	0.200000
setting the parameters for the voting classifier	ensemble voting classifier set	0.037037
list of feature names ordered by their indices	vectorizer get feature names	0.125000
the timestamp when pickling to avoid the hash	joblib memory reduce	0.030303
k-neighbors of a	kneighbors mixin kneighbors x n_neighbors	0.125000
the validity of the	params x metric p metric_params	0.100000
by scaling each feature	scale	0.033333
back the data to the original representation parameters	inverse transform x	0.051282
corresponds to the area under the precision-recall curve	y_true y_score	0.027027
number of splitting iterations in the	one out get n splits	0.111111
input data	cross val predict estimator	0.045455
x relative to y_true	predict scorer call estimator x y_true sample_weight	0.200000
callable case for pairwise_{distances kernels}	pairwise callable x y	0.083333
binarization transformation using thresholding	binarize thresholding	1.000000
empty the function's	joblib memorized func clear warn	0.250000
callable case	callable	0.058824
a full distance matrix	cluster	0.021277
score function best possible score is 1	score y_true y_pred sample_weight multioutput	0.062500
sparse uncorrelated design this dataset is described	datasets make sparse uncorrelated	0.166667
fits the shrunk covariance model according to	covariance shrunk covariance fit x	0.083333
least squares projection of the data onto	transform x ridge_alpha	0.071429
fit the hierarchical clustering on the	cluster feature agglomeration fit x	0.250000
axis center	x axis	0.030769
used when memory is inefficient to train all	y classes	0.027778
significance of a cross-validated	x y cv	0.050000
the l1 distances between the vectors in	manhattan distances	0.083333
posterior log probability of the samples	nb joint log likelihood	0.066667
fit kernel ridge regression model	kernel ridge fit	1.000000
categories as subfolder names	container_path description categories	0.500000
transform data back to its original space	decomposition randomized pca inverse transform	1.000000
the training set according to the	factor fit	0.062500
incrementally fit the	core multi output regressor partial fit x	0.200000
curve auc using the	metrics auc x	0.040000
faces in the wild lfw pairs dataset	datasets fetch lfw pairs subset	0.035714
context of the memory	memory	0.015625
a tolerance which is independent	cluster tolerance x	0.058824
run in parallel n_jobs is the is	n_jobs	0.023256
number of splitting iterations in	model_selection leave one group out get n splits	0.111111
non-negative matrix factorization nmf	non negative factorization x	0.043478
center x	center scale xy x	1.000000
is sometimes referred to by some authors	preprocessing label binarizer	0.071429
an array	array array accept_sparse	0.250000
"news" format strip the headers by	datasets strip newsgroup	0.090909
descriptors of a memmap instance to reopen on	reduce memmap a	0.050000
not found	search	0.019231
test/test sizes are meaningful wrt to the	model_selection validate shuffle split n_samples test_size train_size	0.111111
fit the model from	manifold spectral embedding fit	0.333333
reproducibility flips	deterministic vector	0.076923
variance regression	variance	0.058824
generate indices to split data into	model_selection cviterable wrapper split x	0.250000
global clustering for the subclusters obtained after	cluster birch global clustering x	0.142857
predicted probabilities for a calibration curve	core calibration curve	0.142857
binary classifier predicts one class versus all others	multiclass x	0.166667
update	error update	0.500000
series	series	1.000000
the usual api and hence	decomposition sparse coder fit x y	0.142857
not enabled for	preprocessing robust	0.111111
parameter weights and	y	0.002674
model parameters with the em	mixture gmmbase fit	0.250000
a binary	average binary	0.500000
to the binary classification	y_true y_score pos_label	0.066667
apply clustering to a projection to the	spectral clustering affinity n_clusters n_components	0.166667
standardize a dataset along any	with_centering with_scaling	0.200000
solution to a	lsqr a	0.037037
lfw people dataset this operation is meant	datasets fetch lfw people data_folder_path slice_ color resize	0.333333
long type introduces an 'l'	utils shape	0.013699
two sets of biclusters	metrics cluster consensus score	0.250000
dual gap convergence criterion the specific	covariance dual gap	0.071429
fit all transformers transform the	feature union fit transform x	0.333333
within a job	estimators estimators_features x	0.333333
a byte string to	externals	0.005747
tolerance which is	cluster tolerance x	0.058824
constructor store the useful information for	externals joblib zndarray wrapper init filename init_args state	0.200000
the weighted graph of neighbors	neighbors radius neighbors mixin radius neighbors graph	0.066667
returns the huber loss and	huber loss and	0.166667
if dtype of x and y is	x y	0.002155
the precision matrix	empirical covariance get precision	0.250000
approximates the range of	randomized range finder	0.083333
a list of feature names ordered by their	vectorizer get feature names	0.125000
data onto the sparse components	sparse pca transform x ridge_alpha	0.200000
suffix	utils shape repr	0.013699
number of splitting iterations in the	leave pgroups out get n splits	0.111111
the target	regressor	0.054054
estimates for each	predict	0.006849
the scaler	min max scaler	0.083333
build a	build	0.037037
the kddcup99 dataset downloading it if	brute kddcup99 subset data_home download_if_missing random_state	0.111111
- log sum_h exp(-e v	v	0.052632
for the	covariance get	0.166667
for the voting classifier valid parameter keys can	ensemble voting classifier	0.031250
function called with the given arguments	joblib memorized func get	0.125000
inplace column scaling of a csc/csr matrix	inplace column scale x scale	0.166667
estimates the shrunk ledoit-wolf	ledoit wolf	0.111111
in pipeline after transforms	pipeline fit predict	0.166667
fit	lars cv fit	1.000000
for factorizing common classes	fit first call clf classes	0.058824
matrix to dense array	coef mixin densify	0.100000
of the scaler	preprocessing max abs scaler	0.333333
and std to be used for later scaling	preprocessing standard scaler fit x y	1.000000
reduced likelihood function for the given	reduced likelihood function	0.041667
return parametergrid instance for the given param_grid	grid search cv get param iterator	0.166667
connectivity matrix	connectivity x connectivity n_components affinity	1.000000
sparse uncorrelated design this dataset is	datasets make sparse uncorrelated n_samples n_features random_state	0.166667
avoid	memorized	0.015873
the similarity of two sets of biclusters	metrics cluster consensus score a b similarity	0.500000
explained variance regression score function	explained variance	0.166667
for reducing the size of	items root_path	0.066667
repeated splits	repeated splits	0.125000
the paired distances between x and y	metrics paired distances x y	0.500000
model parameters with the em	mixture gmmbase fit x y do_prediction	0.250000
this function returns posterior probabilities	cv predict proba	0.034483
compute the	feature_extraction compute	0.500000
the number of splitting iterations in	model_selection predefined split get n splits	0.111111
a list of edges for	make edges	0.066667
rankdata	rankdata	1.000000
returns a lower bound on	lower bound	0.071429
given dataset	x y scorer	0.111111
the number of splitting iterations in the	base cross validator get n splits x y	0.125000
under	score x y	0.030303
transforms the image samples in x	x	0.001692
workers requested by the callers	joblib effective	0.200000
from the training set x	fit x	0.025641
grid of points based on the	grid from	0.166667
kernel k x y and optionally its gradient	exponentiation call x y eval_gradient	0.333333
load the kddcup99 dataset	fetch brute kddcup99	0.166667
a memmap instance to reopen on same	externals joblib reduce memmap a	0.050000
return a platform	shape	0.011765
transform is sometimes referred to	transform	0.011236
and configure a	append random_state	0.142857
the first prime	prime	0.111111
call predict_log_proba on	predict log proba x	0.045455
according to the given	sample_weight	0.037037
repr	joblib safe repr	1.000000
list of exception types	backend base	0.032258
to build a batch of estimators within a	build estimators	0.166667
export	export graphviz decision_tree out_file max_depth feature_names	0.333333
write array bytes to pickler	joblib numpy array wrapper write array array pickler	0.333333
number of splitting iterations in the cross-validator	model_selection cviterable wrapper get n splits x	0.111111
inplace row scaling of	utils inplace row scale	0.142857
create all the covariance	covariance	0.014493
exception	backend	0.016949
the fastmcd algorithm	cov	0.100000
check initial parameters of the derived	mixture check parameters	0.166667
voting classifier valid	voting classifier set params	0.037037
x format check x	latent dirichlet allocation check	0.062500
boost	boost classifier boost	0.200000
pca methods	pca	0.047619
the blup parameters and evaluates the reduced	gaussian process reduced	0.125000
suitable step length is not found and raise	utils line search	0.029412
the em algorithm	mixture gmmbase	0.068966
building a cv in a user	core check cv cv	0.031250
perform classification on test vectors x	core dummy regressor predict x	0.250000
the range	utils randomized range	0.083333
model parameters with the em	mixture gmmbase fit x y	0.250000
scale back the data	preprocessing standard scaler inverse transform x	0.066667
fit underlying estimators	core one vs one classifier fit	1.000000
true and false positives per binary classification	metrics binary clf curve y_true y_score pos_label	0.090909
embedding analysis on the data	embedding x	0.200000
if the test/test sizes are meaningful wrt	model_selection validate shuffle split n_samples test_size train_size	0.111111
search	core base search	0.111111
with passive aggressive algorithm	passive aggressive regressor	0.125000
factorization nmf find two non-negative	factorization	0.035714
actual fitting performing the search over	search cv fit x y parameter_iterable	0.333333
fit estimator and predict	core fit and predict estimator x y	1.000000
if the estimator has been refit	core base search cv	0.033333
find the median across axis 0	median axis 0	0.333333
predict raises an exception in an unfitted estimator	estimators unfitted name estimator	0.142857
estimates for	y	0.002674
coefficient score	score y_true y_pred normalize sample_weight	0.125000
the best found	core base search cv predict	0.076923
as	xy	0.076923
cpu	cpu	1.000000
check x	dirichlet allocation check	0.062500
center and scale the data	robust scaler transform x y	0.200000
whether the kernel is stationary	dot product is stationary	1.000000
linear embedding	linear embedding x n_neighbors n_components	0.200000
matrix whose range approximates the range of	range	0.058824
ridge regression with built-in cross-validation	ridge cv	0.500000
routine for validation and conversion of csgraph	sparsetools validate graph csgraph directed dtype csr_output	0.166667
check x format check x format and	dirichlet allocation check	0.062500
function used to build a batch of	build	0.037037
timestamp when pickling to	memory reduce	0.030303
best found parameters	search cv	0.036364
parameters for the voting classifier valid parameter	voting classifier set params	0.037037
api and	y	0.008021
x for a diagonal	diag x means	0.500000
the probabilities p(h=1|v)	neural_network bernoulli rbm mean hiddens v	0.333333
n_samples_leaf	n_samples_leaf	0.555556
to x	fit x	0.006410
curve auc from prediction	auc score	0.052632
time it take	squeeze time t	0.166667
corresponds to the area under the precision-recall	y_true y_score	0.027027
of an array shape under python 2 the	utils shape repr shape	0.166667
convert a collection of raw documents	vectorizer	0.022222
creates a	base spectral fit	0.250000
likelihood function for the given autocorrelation	likelihood function	0.142857
with respect to coefs	activations deltas	0.032258
depending from	externals	0.011494
a cross-validated score	score estimator x y cv	0.083333
compute data precision matrix with the generative model	pca get precision	0.066667
the function call with	format call func	0.100000
factorizing common classes param logic	first call clf classes	0.058824
actual fitting performing the	y parameter_iterable	0.500000
jobs for the computation	jobs n_jobs	0.100000
transformers	transformer	0.100000
the number of splitting iterations in the	base kfold get n splits x y	0.111111
trained	base multilayer perceptron	0.142857
evaluates the reduced likelihood function for the given	process reduced likelihood function	0.047619
include	include	1.000000
for each input	estimator	0.014706
fit the model	svm linear svc fit x	0.333333
x y as	x y	0.004310
check if vocabulary is empty	check vocabulary	0.250000
test/test sizes are meaningful wrt	model_selection validate shuffle split n_samples test_size train_size	0.111111
reduced likelihood function for	reduced likelihood function	0.041667
an 'l' suffix when	utils shape	0.013699
returns the number of splitting iterations in the	get n splits	0.111111
score for a fit	fit	0.003257
the number of splitting iterations in	group out get n splits x y	0.111111
embedding for non-linear	embedding	0.040000
score for a fit	fit rfe estimator x	0.166667
kernel k x y and optionally its gradient	gaussian_process product call x y eval_gradient	1.000000
precision is the ratio tp /	metrics precision	0.033333
check that	utils check estimators	0.142857
cf tree for the input data	cluster birch fit x y	0.200000
batch_size elements from 0 to n	n batch_size	1.000000
a tolerance which is independent	cluster tolerance	0.058824
long type introduces an 'l' suffix	repr	0.012500
check x format check x	latent dirichlet allocation check	0.062500
zeros	zeros	1.000000
update w in multiplicative update nmf	multiplicative update w x w h	0.500000
new samples can be different from the	core calibrated classifier cv	0.111111
update for	step x	0.333333
seeds for mean_shift	get bin seeds x bin_size min_bin_freq	0.500000
matrix factorization nmf find two non-negative	negative factorization x	0.043478
from features or distance	sample_weight	0.018519
itree which is equal to the average	average	0.066667
estimate the spherical wishart distribution	gaussian mixture estimate wishart spherical nk xk sk	0.333333
for factorizing common	fit first call clf	0.200000
kneighbors	kneighbors	0.625000
computes the cosine distance	neighbors lshforest compute distances query candidates	0.333333
the logistic	logistic	0.142857
approximates the range of	utils randomized range	0.083333
callable case for pairwise_{distances kernels}	callable x y metric	0.083333
the polynomial	metrics polynomial	0.333333
mean silhouette coefficient	metrics cluster silhouette score x	0.250000
fits the maximum likelihood estimator	empirical	0.055556
returns the number of splitting iterations in	kfold get n splits x y	0.111111
weiszfeld	weiszfeld	1.000000
thresholding	thresholding y	0.500000
the given list of parameter objects and 'return_annotation'	init parameters return_annotation __validate_parameters__	0.125000
log of probability estimates	sgdclassifier predict log proba	1.000000
the trained model parameters	neural_network base multilayer perceptron	0.083333
determinant	det	0.214286
process or	multiprocessing	0.045455
log probabilities within a job	predict log proba estimators estimators_features x	0.250000
fit is	fit	0.003257
the average path length	average path length	0.090909
in the wild lfw pairs	datasets fetch lfw pairs	0.018868
with a given cache key	cache key	0.250000
estimator with the best found parameters	model_selection base search cv predict	0.076923
compute the l1 distances between	metrics manhattan distances	0.083333
solution to a sparse coding problem	decomposition sparse encode x dictionary	0.333333
ledoitwolf estimator ledoit-wolf is	ledoit wolf	0.111111
requested by the callers	base effective	0.250000
batching	batching	1.000000
folder	delete folder	1.000000
can be different from	core calibrated classifier	0.083333
for c such that for c	c	0.022222
number of samples in array-like x	utils num samples x	0.250000
is inefficient to train	y classes	0.027778
the vectors rows of u such	u	0.032258
and component wise scale	scale	0.066667
sign correction to ensure deterministic	flip u v u_based_decision	0.166667
reconstruct the array from the meta-information and the	joblib zndarray wrapper read unpickler	0.043478
labels	label	0.090909
similarity of two	a b similarity	0.125000
callback used by joblib parallel's multiprocessing backend	batch completion call back	1.000000
homogeneity metric of a cluster labeling given a	metrics cluster homogeneity score labels_true labels_pred	0.500000
store	joblib memory	0.016949
x	x doc_topic_distr sub_sampling	0.333333
of exception types to	externals joblib parallel	0.014085
distribution	distribution	0.833333
calculates a covariance matrix shrunk	shrunk covariance emp_cov shrinkage	0.250000
fetch an mldata org data set	fetch mldata dataname target_name data_name transpose_data	1.000000
representing each class with	classifier	0.013699
transform function to portion of selected features	selected x transform selected copy	0.333333
instance for the given param_grid	param iterator	0.166667
all the vectors rows of u	u	0.032258
back the data to the	preprocessing robust scaler inverse transform x	0.066667
neighbors for points in	neighbors mixin radius neighbors	0.125000
its corresponding derivatives with respect	activations deltas	0.032258
of dataset and properly handle kernels	utils safe split estimator x y indices	0.200000
number of splitting iterations in the	one out get n splits x y groups	0.111111
folders to make cache size fit in	size	0.032258
initial centroids parameters	cluster init centroids x k init random_state	0.166667
homogeneity metric of a cluster labeling given	cluster homogeneity score labels_true	0.500000
projection to the normalized	spectral	0.026316
samples x to the separating hyperplane	lib svm decision function x	0.250000
the position of the	mds fit x	0.066667
compute data precision matrix with the	decomposition base pca get precision	0.066667
the kernel k	gaussian_process constant kernel call	0.333333
used to build a batch of estimators within	parallel build estimators n_estimators	0.166667
rank	rank	1.000000
call predict	predict x	0.011765
for reproducibility flips the sign of elements of	deterministic vector sign	0.066667
for each input data point	val predict estimator x y	0.045455
matrix	sparse coef mixin	0.083333
predict confidence scores for	linear_model linear classifier mixin	0.500000
random	n_samples	0.058824
pickler file handle	pickler	0.083333
to build a batch of estimators within	ensemble parallel build estimators	0.166667
function called with the given arguments	memorized func get	0.125000
implementation is restricted to the binary classification task	y_true y_score pos_label	0.066667
elem	elem	1.000000
the logistic loss	linear_model logistic loss	0.500000
kernel density estimation read more in the :ref	kernel density	0.083333
for each input	core cross val predict estimator x	0.045455
generates a random sample from	size	0.032258
labels_pred	labels_pred	1.000000
constructs signature from the given	externals signature	0.050000
dataset along any axis center to the median	axis	0.014085
indices increasingly apart the distance depending	joblib verbosity filter index	0.055556
compute log probabilities	ensemble parallel predict log proba	0.058824
the number of splitting iterations in	get n splits	0.111111
non-negative matrix factorization nmf find two non-negative matrices	negative factorization	0.043478
fit the model according to	svm linear svr fit x y sample_weight	0.250000
the precision is the ratio tp	metrics precision	0.033333
nothing and return the estimator	feature_extraction	0.037037
contain a partial	partial	0.043478
embedding read more in the	embedding	0.040000
estimator and predict	and predict estimator x y	1.000000
the range	randomized range finder	0.083333
get the directory corresponding	get func dir mkdir	0.333333
compute the l1	manhattan	0.125000
to the cache for the	joblib memorized func	0.014706
measure the similarity of two clusterings	metrics cluster fowlkes mallows score labels_true labels_pred sparse	0.333333
fit ridge regression model parameters	linear_model ridge fit x y	1.000000
kernel k x	gaussian_process constant kernel call x	0.333333
the right fileobject from a filename	joblib read fileobject fileobj filename	0.250000
the model according to the given training data	x y sample_weight	0.025974
the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x assume_centered	0.125000
backend and	backend base	0.032258
maximizer of the reduced likelihood	gaussian_process gaussian process arg max reduced likelihood	0.250000
and perform	y	0.002674
return a tolerance which is	tolerance x	0.058824
from the training set x y	fit x y sample_weight	0.060000
type introduces	utils shape	0.013699
tolerance which is independent	cluster tolerance x tol	0.058824
data to	data x	1.000000
return the kernel k x	product call x	0.200000
to the cache for the	memorized	0.015873
multinomial deviance loss function for	multinomial deviance	0.250000
the submatrix corresponding to bicluster i	core bicluster mixin get submatrix i data	0.333333
generate the random projection matrix	gaussian random projection make random matrix n_components	1.000000
an 'l' suffix	utils shape	0.013699
don't store the	externals	0.011494
get the parameters of	classifier get	0.200000
the paired cosine distances between x	metrics paired cosine distances x	0.333333
exceptions	raise message exceptions	1.000000
perform the	responsibilities params	0.500000
data to vectors and cluster the	and cluster data vectors n_clusters	0.333333
transform binary labels back to multi-class labels	inverse transform y threshold	0.333333
predict_proba on the estimator with the best found	search cv predict proba x	0.076923
the long type	shape repr	0.013699
by using matrix product with the random matrix	random projection transform x y	0.333333
new	make_default	0.166667
function to preprocess the text before tokenization	feature_extraction vectorizer mixin build preprocessor	0.142857
from the meta-information and	joblib zndarray wrapper read unpickler	0.043478
coverage error measure	metrics coverage error y_true y_score sample_weight	0.166667
path length from source to all	path length graph source cutoff	0.200000
number of splitting iterations in the cross-validator	leave one group out get n splits x	0.111111
of loss	loss grad	0.250000
the training set x and	x y	0.002155
generate isotropic gaussian blobs for clustering	make blobs n_samples n_features centers cluster_std	0.333333
format check x format and	dirichlet allocation check	0.062500
parameters for the voting classifier valid	ensemble voting classifier set params	0.037037
compute log(det a for a symmetric equivalent	utils fast logdet a	0.500000
estimates	x	0.001692
for x relative to	metrics threshold scorer call clf x y sample_weight	0.058824
private function used to compute	decision function estimators	0.333333
apply clustering to a	spectral clustering affinity n_clusters n_components	0.166667
on the estimator with randomly drawn parameters	core randomized search cv	0.200000
pixel-to-pixel gradient connections edges are weighted with the	img mask return_as dtype	0.166667
predict is invariant	compute labels predict	0.250000
determine the optimal	externals joblib auto batching mixin compute	0.333333
the score for a fit	fit rfe estimator x	0.166667
a sparse matrix	utils svds a	0.166667
the precision the precision is	precision	0.016667
to avoid the hash depending from it	externals joblib memorized func	0.013158
a correction to raw minimum covariance determinant estimates	covariance min cov det correct covariance	0.500000
sparse and dense	x y sample_weight random_state	0.166667
building a cv	check cv cv x	0.031250
validate user provided precisions	precisions precisions covariance_type	0.250000
single sample	sample	0.032258
inefficient	y classes	0.027778
labels the output of transform	transform y	0.023256
fit estimator and predict	core fit and predict estimator x	1.000000
randomized linear models for feature	randomized linear model	0.076923
path length of	path length	0.333333
the callable case	callable	0.058824
fit on the estimator	fit x	0.006410
a name for	func name	0.047619
labels the output of transform is sometimes	transform y	0.023256
loader for the labeled faces in the wild	data_home	0.055556
the optimal batch size	base compute batch size	1.000000
the base classifiers	ensemble bagging classifier	0.200000
avoid the	joblib	0.014599
actual fitting performing	y parameter_iterable	0.500000
to the median and component wise scale	scale	0.033333
constructs signature from	signature	0.047619
patches of any n-dimensional array	extract patches	0.083333
the number of splitting iterations in the	split get n splits x	0.111111
the unnormalized posterior log probability of	base nb joint log likelihood	0.166667
largest k singular values/vectors for a	svds a k ncv tol	0.166667
wishart distribution	wishart	0.125000
to avoid	memorized	0.015873
capture the	check_pickle	0.040000
opening the right fileobject	fileobject	0.100000
introduces an 'l'	shape repr	0.013699
grid	ensemble grid	0.111111
for the voting classifier valid parameter keys can	voting classifier set	0.037037
generate an array with block checkerboard	make checkerboard	1.000000
estimates for each input data	predict	0.006849
position of the points	mds fit	0.066667
evaluate decision function output for x relative to	metrics threshold scorer call clf x	0.058824
boolean thresholding of array-like or scipy	preprocessing binarize x threshold copy	0.083333
in an unfitted	estimators unfitted	0.142857
the shortest path length from source to all	shortest path length graph source	0.111111
jaccard	jaccard	1.000000
training set x and	predict x y	0.043478
fit estimator and predict values	fit and predict estimator x y train	0.250000
to	base get	0.066667
and variance along an axix on a	variance axis x axis	0.090909
compute a logistic regression model	linear_model logistic regression path x	0.333333
global clustering for the subclusters	global clustering	0.142857
generate a random n-class classification problem	classification n_samples n_features n_informative n_redundant	0.500000
independent	utils shape	0.013699
to make cache size fit	size	0.032258
input validation for	check x y x y accept_sparse dtype	0.250000
for later	fit	0.003257
w h whose product approximates the	x w h n_components	0.038462
the one-vs-one multi class libsvm in the	one vs one	0.050000
generate a mostly low rank matrix with bell-shaped	make low rank matrix n_samples n_features	0.500000
the number of splitting iterations in the cross-validator	model_selection leave one group out get n splits	0.111111
normal	normal	1.000000
load text	datasets load	0.083333
estimate the diagonal covariance vectors	mixture estimate gaussian covariances diag resp	1.000000
distance of the samples x to the	x	0.003384
check	neighbors check params	0.500000
workaround python 2 limitations of pickling instance	obj methodname	0.111111
does not need to update terminal regions	update terminal regions tree x y residual	1.000000
log-transformed bounds on the theta	exponentiation bounds	0.333333
total log probability under the model	neighbors kernel density score x y	0.333333
precision ap from prediction scores this score corresponds	precision score y_true y_score	1.000000
detects the soft boundary of the set	svm one class svm fit	0.125000
compute the gradient of	base multilayer perceptron compute	0.250000
check if vocabulary is empty or	feature_extraction vectorizer mixin check vocabulary	0.250000
input validation for standard estimators	check x y x y accept_sparse dtype	0.250000
for the voting classifier valid	ensemble voting classifier	0.031250
build a batch	parallel build	0.047619
initial centroids	cluster init centroids x k	0.166667
callable case for pairwise_{distances	callable	0.058824
predict the target of new	predict x	0.011765
data home cache	data home data_home	0.055556
in the wild lfw	datasets fetch lfw	0.041667
coverage	coverage	0.750000
return the file descriptor for the underlying file	externals joblib binary zlib file fileno	0.333333
catch and hide warnings without visual nesting	utils ignore warnings call fn	0.200000
for samples	classifier mixin decision function	1.000000
avoid the hash depending from	memorized func	0.016949
the gradient of the loss	loss	0.027027
x and returns the transformed data	fit transform x y w h	0.500000
spluinv	sp lu inv	1.000000
is the solution to a sparse coding problem	decomposition sparse encode x dictionary	0.333333
the process or thread	joblib	0.007299
sample weights by class for unbalanced datasets	compute sample weight class_weight y indices	0.500000
for each input data point	predict estimator x y	0.045455
this implementation is restricted to the binary classification	y_true y_score	0.054054
factorization	negative factorization	0.043478
fit label encoder	label encoder fit	1.000000
precision is the ratio tp / tp +	precision	0.016667
for full covariance matrices	density full x means covars min_covar	0.166667
kernel	kernel operator	0.142857
contingency matrix describing the relationship	cluster contingency matrix labels_true labels_pred eps sparse	0.333333
the callable case for pairwise_{distances kernels}	callable x y	0.083333
for full	full	0.111111
write array bytes	write array array	0.500000
dictionary factor in	dictionary y	0.111111
label sets with a	preprocessing multi label binarizer	0.200000
one-vs-one	ovo	0.166667
perform a locally linear embedding analysis	locally linear embedding x n_neighbors	0.071429
batch and dispatch	joblib parallel dispatch one batch	0.500000
a locally linear embedding analysis on	locally linear embedding x n_neighbors n_components	0.071429
fit across one fold	feature_selection rfe single fit rfe estimator x y	0.200000
on the estimator with the best found	model_selection base search cv predict proba x	0.076923
function func with	func	0.011364
xy	xy	0.769231
kernel x and y	kernel x y	0.500000
showing the main classification metrics read	metrics classification	0.052632
compute the number of patches	feature_extraction compute n patches	1.000000
of the reduced	reduced	0.062500
the backend and return the	externals joblib parallel backend base	0.034483
estimate the parameters of the gaussian distribution	bayesian gaussian mixture estimate means nk	1.000000
building a cv in	check cv cv	0.031250
line_search_wolfe1 but fall back to line_search_wolfe2	wolfe12 f fprime xk pk	0.028571
get parameters of this	get params deep	0.400000
onto	transform x ridge_alpha	0.071429
locally linear embedding analysis on the data	manifold locally linear embedding x n_neighbors	0.071429
of module names and a name for	get func name	0.047619
return feature	get feature	0.125000
weighted graph of neighbors	radius neighbors mixin radius neighbors graph	0.066667
output_type	output_type	1.000000
the shrunk ledoit-wolf covariance	covariance ledoit wolf x	0.125000
for the given param_grid	get param iterator	0.166667
a given dataset split	x y scorer	0.111111
descent fit is on grid of	fit x	0.006410
nmf	h	0.041667
fit the model by	pca fit	0.111111
neighbors within a given radius	neighbors lshforest radius neighbors x radius	0.142857
a which this function	externals joblib	0.004762
generate a dense	n_features random_state	1.000000
feature names from	feature names	0.090909
k-fold iterator variant	group kfold	0.250000
ensure_ready	ensure_ready	1.000000
contingency matrix describing the	contingency matrix labels_true labels_pred eps	0.166667
sample weights by class for	utils compute sample	0.100000
general function given points on a	reorder	0.071429
write array bytes to pickler file	numpy array wrapper write array array pickler	0.333333
logistic regression and cv and linearsvc	fit liblinear x y c fit_intercept	0.142857
the given param_grid	param iterator	0.166667
load sample	datasets load sample	1.000000
execution only a fraction	externals joblib	0.004762
back to	label binarizer inverse	0.166667
evaluate a score by cross-validation read more in	cross val score estimator x y	0.166667
fits the shrunk covariance model according to	covariance shrunk covariance fit	0.083333
covariance estimator read more	covariance	0.014493
the training set x and returns	x y	0.002155
oracle approximating shrinkage covariance model	covariance oas fit	0.083333
kernel k x	gaussian_process dot product call x	0.200000
pairwise matrix	pairwise x y	0.166667
to the data x which should contain	x y	0.002155
mean silhouette coefficient	metrics cluster silhouette score x labels metric sample_size	0.250000
call predict_log_proba on the	predict log proba x	0.045455
y_score	y_score	0.416667
extracts patches of	patches	0.055556
elastic net path with	path x	0.045455
x_test	x_test	0.416667
weighted graph of k-neighbors for	neighbors kneighbors mixin kneighbors graph	0.250000
later scaling	scaler fit	0.153846
evaluate decision function output for x relative to	metrics threshold scorer call clf x y sample_weight	0.058824
process or thread pool	externals joblib multiprocessing backend	0.035714
func	backend apply async func	0.250000
the binary classification task	score y_true y_score average	0.076923
along an axix on a csr or	axis x axis	0.083333
whether the file was opened for writing	joblib binary zlib file writable	0.250000
the similarity of two	score a b similarity	0.125000
binary_metric	binary_metric	1.000000
orthogonal matching pursuit step on a precomputed	xy n_nonzero_coefs tol_0	1.000000
data x	x doc_topic_distr sub_sampling	0.333333
update h in multiplicative update nmf	decomposition multiplicative update h x w h beta_loss	0.250000
function cache result	joblib memorized func	0.014706
of x (as bigger is better i	function x	0.030303
reconstruct the image from all of its patches	reconstruct from patches 2d patches image_size	1.000000
to split data into	kfold split	0.250000
the data home	data home	0.076923
recall is the ratio tp	recall	0.028571
loader for the labeled faces	subset data_home	0.125000
n_iter	n_iter	1.000000
single boost	ensemble ada boost classifier boost	0.100000
format	linear_model sparse	0.076923
coefficient matrix to dense array	sparse coef mixin densify	0.100000
is a raw file object e g created	externals joblib is raw file	1.000000
the gaussian process regression model	gaussian_process gaussian process regressor	0.058824
the huber loss and the	linear_model huber loss and	0.166667
the dense dictionary factor	dict dictionary y	0.111111
call wrapped function cache result and return a	externals joblib memorized func call	0.200000
fobj	fobj	0.625000
fit to data then transform	core transformer mixin fit transform x y	0.500000
fit linear model	fit x y	0.029940
list of exception types to	base get	0.066667
reconstruct the image from	reconstruct from	0.333333
fit on the estimator with randomly drawn parameters	core randomized search cv fit x y	1.000000
folders to make cache size fit	reduce size	0.083333
binary classification	y_true y_score average sample_weight	0.076923
a tolerance which	cluster tolerance x	0.058824
a job	estimators estimators_features x	0.333333
the transformed data	h	0.041667
the search	base search	0.100000
weighted graph of neighbors for	radius neighbors mixin radius neighbors graph	0.066667
variance along an axix	variance axis x axis	0.090909
of a memmap instance to	reduce memmap a	0.050000
platform independent representation	shape	0.011765
we don't store the timestamp when pickling	joblib memorized func reduce	0.050000
perform dbscan clustering from vector array or distance	cluster dbscan	0.125000
covariance matrix shrunk on the	shrunk covariance	0.090909
coefficient	coef	0.058824
point	core cross val	0.043478
compute the initial centroids parameters	cluster init centroids x k init random_state	0.166667
constructs signature from the given	signature	0.047619
long type introduces an	repr	0.012500
estimates the shrunk ledoit-wolf covariance	covariance ledoit wolf	0.125000
nicely formatted statement displaying the function call with	joblib format call func args kwargs object_name	0.333333
reg	reg	1.000000
prediction of init	boosting init	0.142857
hastie	datasets make hastie	0.125000
a hash to identify uniquely python objects containing	externals joblib hash obj hash_name coerce_mmap	0.333333
run fit on one set	core fit grid point x y estimator	0.500000
to avoid the hash depending from it	joblib	0.014599
sk	sk	1.000000
function used to build a	parallel build	0.047619
return the kernel k	gaussian_process constant kernel	0.333333
inverse label binarization transformation	preprocessing inverse binarize	0.500000
compute the l1 distances between	paired manhattan distances	0.083333
linear assignment problem using the	utils linear assignment x	0.090909
- y_[i]) ** 2	y sample_weight y_min y_max	0.166667
build a batch of estimators within a	ensemble parallel build estimators n_estimators ensemble x y	0.166667
x and y	x y sum_over_features	0.250000
squared	squared	0.666667
for full covariance matrices	density full x means	0.166667
the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x assume_centered block_size	0.125000
class covariance matrix	class cov	0.250000
oob	oob	1.000000
compute the	base mixture score	0.200000
transform binary labels back to multi-class labels parameters	label binarizer inverse transform y threshold	0.333333
avoid the hash depending from	memorized	0.015873
validate x whenever one tries to predict	base forest validate x predict x	0.500000
to the training set x and returns the	predict x y	0.043478
contingency matrix describing	contingency matrix labels_true	0.166667
apply clustering to a projection to	spectral clustering	0.142857
compute cosine similarity between	metrics cosine similarity	0.333333
of exception types	parallel	0.019231
solution to a	utils lsqr a	0.037037
loading for the lfw	lfw	0.068966
returns the number of splitting iterations in the	model_selection leave one group out get n splits	0.111111
scale back the data to the	preprocessing robust scaler inverse transform x	0.066667
predict on the	predict	0.006849
target s	neural_network mlpclassifier	0.333333
dictionary	dict dictionary y	0.111111
clustering for the	clustering	0.050000
features	features x vocabulary high low	1.000000
cramer variance update	var x last_mean last_variance last_sample_count	1.000000
format check x	allocation check	0.062500
all the covariance	distribute covar matrix to match covariance	0.250000
the submatrix corresponding to bicluster	core bicluster mixin get submatrix	0.333333
the kernel k x y and	gaussian_process pairwise kernel call x y	0.333333
for full covariance	density full x	0.166667
estimators within a job	estimators	0.052632
measure the similarity of two clusterings	fowlkes mallows score labels_true labels_pred	0.333333
dual gap convergence criterion	dual gap	0.071429
spatial independence correlation model pure nugget	gaussian_process pure nugget theta d	1.000000
squared euclidean norm of x	x squared	1.000000
model by computing full svd on x	full x n_components	0.333333
loss	loss y_true	0.250000
just there to implement the usual api and	fit x y	0.017964
returns n_neighbors	n_neighbors return_distance	0.250000
split data according to	split	0.027778
perform dbscan clustering from vector array or distance	cluster dbscan x eps	0.200000
compute the weighted log probabilities for each sample	mixture base mixture score samples	1.000000
lars path	omp path	0.100000
fit linear model	base sgdclassifier fit x y	0.333333
can actually run in parallel	joblib parallel backend	0.045455
timestamp when pickling to avoid	joblib memorized func reduce	0.050000
inplace column	utils inplace column scale x	0.166667
fixes	cluster fix connectivity x	1.000000
computes the	x y alpha	0.181818
thread	backend	0.016949
20 newsgroups data and stored	20newsgroups	0.055556
exception types	externals joblib parallel	0.014085
helper to check the test_size and train_size at	test_size train_size	0.200000
for the lfw people	fetch lfw people	0.040000
compute prediction of init	gradient boosting init decision function x	0.142857
20 newsgroups data and stored it	20newsgroups	0.055556
a general function given points on a	reorder	0.071429
that for c in (l1_min_c infinity) the model	c x y loss fit_intercept	0.030303
function call with the given	joblib format call func	0.100000
assignment problem using the hungarian algorithm	assignment x	0.250000
not found and raise	line search	0.029412
generate indices to split data into	model_selection time series split split	0.250000
absolute sizes of training subsets	sizes	0.050000
position of	mds fit	0.066667
coef	coef	0.294118
check x format check x	decomposition latent dirichlet allocation check	0.062500
for full covariance	full	0.055556
call with	joblib format call	0.200000
terminal regions	terminal region tree	0.100000
measure the similarity of two clusterings of a	metrics cluster fowlkes mallows score labels_true labels_pred	0.333333
the	neural_network	0.142857
to the data x which	x y	0.002155
returns the number of splitting iterations in the	cross validator get n splits x y	0.125000
the data home cache	clear data home data_home	0.076923
between	metrics	0.130435
to split data according to	split	0.027778
x from y along the first axis we	x	0.001692
compute elastic net path	enet path x	0.050000
binarization transformation using	binarize	0.045455
number of splitting iterations in	base cross validator get n splits	0.125000
apply clustering to a projection to	spectral clustering affinity n_clusters n_components	0.166667
angle regression or lasso path using lars algorithm	linear_model lars path x y	0.100000
graph of the	to graph	0.333333
inplace column scaling of a	inplace column scale x scale	0.166667
and transform	transform	0.011236
when memory is inefficient to	classes	0.025641
depending	joblib memory	0.016949
single sample image parameters	sample image image_name	0.166667
along an axis on a csr	axis x axis	0.083333
called with the given arguments	joblib memorized func get	0.125000
found and	utils line search	0.029412
problem with sparse uncorrelated design	make sparse uncorrelated n_samples n_features random_state	0.166667
the neighbors	radius neighbors x	0.166667
of points based on the	from	0.045455
california	datasets fetch california	0.333333
diagonal	diag	0.250000
unpickler to unpickle our numpy pickles	zip numpy unpickler	0.333333
vocabulary dictionary and return term-document	vectorizer fit transform raw_documents y	0.100000
schedule a func to be run	externals joblib sequential backend apply async func callback	0.250000
to build a batch of estimators within	build estimators n_estimators ensemble x y	0.166667
faces in the wild lfw pairs dataset this	lfw pairs	0.018868
target values for x relative to y_true	metrics predict scorer call estimator x y_true sample_weight	0.200000
soft boundary of the set of samples x	x	0.001692
kappa a	metrics cohen kappa	0.250000
back the data to the original representation parameters	inverse transform	0.062500
the posterior log probability	multinomial nb joint log likelihood	0.083333
compressor	compressor	1.000000
log probabilities within a job	ensemble parallel predict log proba estimators estimators_features	0.250000
predicts one class versus all others	multiclass	0.076923
from	memory	0.015625
perform a locally linear embedding analysis	locally linear embedding x	0.071429
initialize the model parameters	base mixture initialize parameters x	1.000000
learn vocabulary and idf return term-document	tfidf vectorizer fit transform raw_documents y	0.250000
covariance matrix shrunk on the diagonal	covariance shrunk covariance	0.090909
updating terminal regions (=leaves)	function update terminal region tree terminal_regions leaf	0.200000
on the estimator with the best found parameters	model_selection base search cv predict proba	0.076923
seeds	get bin seeds x	0.250000
feature names ordered by their indices	get feature names	0.090909
apply dimensionality reduction to	decomposition factor analysis transform	1.000000
scale	preprocessing max abs scaler transform x	1.000000
after the other and	x y	0.002155
shortest path length from source	shortest path length graph source cutoff	0.111111
generate a grid of	grid	0.040000
the dense dictionary factor	dict dictionary	0.111111
importances the higher the	importances	0.100000
we don't store	externals joblib	0.009524
predict using the trained	multilayer perceptron predict x	0.333333
using a single binary	binary	0.031250
the index of the leaf	tree apply	0.166667
number of jobs for the	n jobs n_jobs	0.142857
a platform	repr	0.012500
matrix to	sparse coef mixin	0.083333
and predicted probabilities for a calibration curve	core calibration curve y_true y_prob normalize n_bins	0.142857
ap from prediction scores this score corresponds	score y_true y_score	0.025000
tolerance which is	tolerance x	0.058824
estimate model parameters with the em algorithm	mixture gmmbase fit x y	0.250000
the coefficient of determination r^2	multi output regressor score x y	0.200000
kernel k	gaussian_process pairwise kernel call	0.333333
list of exception types to	parallel backend base	0.037037
the maximum	preprocessing max	0.166667
of x according	transform x	0.016949
e-step in em update	latent dirichlet allocation e step x cal_sstats	1.000000
index of the leaf	tree apply	0.166667
avoid the hash	externals	0.011494
the given arguments	memorized func get output	0.125000
wild lfw pairs	fetch lfw pairs	0.018868
lrd the lrd of	distances_x neighbors_indices	0.047619
of possible outcomes for	ensemble voting classifier	0.031250
to avoid the hash	externals joblib memorized func	0.013158
estimates for each input	val predict estimator x y	0.045455
numpy	numpy	0.583333
compute the decision function	ada boost classifier decision function	0.166667
the training set x y	fit x y sample_weight	0.060000
compute non-negative matrix factorization nmf find	factorization x	0.043478
classifier predicts one class versus all others	multiclass x	0.166667
w in multiplicative update nmf	decomposition multiplicative update w x w h	0.500000
to line_search_wolfe2 if suitable step length	wolfe12 f fprime xk pk	0.028571
log of the determinant of a	log	0.018868
search	search cv fit x	0.111111
log probability	mixture log multivariate normal density	0.250000
helper to workaround python 2 limitations	parallel helper obj methodname	0.333333
the vocabulary dictionary and return term-document	feature_extraction count vectorizer fit transform raw_documents y	0.166667
this is the time it take to	joblib squeeze time t	0.200000
thread pool and return the number of workers	externals joblib parallel initialize backend	1.000000
described by its spectrum spectrum	spectrum	0.090909
y and scale if the scale parameter==true	y scale	0.500000
avoid the hash depending from	joblib memorized func	0.014706
in x according to the fitted	x	0.001692
generate random	n_samples	0.058824
binary	metrics average binary	0.500000
error of the kl divergence	kl divergence error	0.100000
center	preprocessing robust	0.111111
w h	w h n_components	0.038462
oneway	oneway	1.000000
suitable step length is not found	search	0.019231
transform feature->value dicts to	feature_extraction dict vectorizer transform	0.200000
used in hastie et al	datasets make hastie 10 2 n_samples random_state	0.166667
does nothing this transformer is stateless	feature_extraction hashing vectorizer	1.000000
under the curve auc using the trapezoidal rule	auc	0.020408
function returns posterior probabilities of classification	classifier cv predict proba	0.200000
mean and variance along an axix on a	mean variance axis x axis last_mean last_var	0.333333
submatrix corresponding to bicluster i	core bicluster mixin get submatrix i	0.333333
boolean thresholding of array-like or scipy	preprocessing binarize x	0.083333
predict label	gmmbase predict x	1.000000
swaps two rows of a csc/csr matrix in-place	utils inplace swap row x	0.250000
local outlier factor	local outlier factor decision function	0.125000
remove cache folders to	memory reduce	0.030303
for each input data	cross val	0.038462
the isotonic regression model : min sum	core isotonic regression y	0.066667
the number of splitting iterations in	model_selection base cross validator get n splits	0.125000
estimates for each input data	cross val predict estimator x y	0.045455
its spectrum spectrum	spectrum n_samples n_features	0.166667
across one fold	feature_selection rfe single	1.000000
regression problem this dataset is described in friedman	datasets	0.015152
in multiplicative update	decomposition multiplicative update h x	0.500000
p_w	p_w	1.000000
a score by cross-validation read more in	cross val score estimator x y groups	0.166667
along an axis on a csr	x axis	0.015385
filters	args func ignore_lst	0.500000
covariance	cov x y	0.500000
prefetch the tasks for the next batch	one batch iterator	0.500000
used by logistic regression and cv and linearsvc	fit liblinear x y c fit_intercept	0.142857
lrd the lrd	distances_x neighbors_indices	0.047619
to make cache size fit in bytes_limit	memory reduce size	0.083333
helper function to test error messages in exceptions	message exceptions message function	1.000000
the median of data	utils get median data	0.333333
lad updates terminal	ensemble least absolute error update terminal region tree	0.200000
path with coordinate descent the	linear_model enet path x	0.050000
validation of y and class_weight	svm validate targets y	1.000000
the vocabulary dictionary and return term-document	transform raw_documents y	0.100000
the lfw pairs dataset	fetch lfw pairs	0.018868
to the average path length of	ensemble average path length	0.090909
the lfw	fetch lfw	0.083333
utility for building a cv in a user	check cv cv	0.031250
and a name	func name	0.047619
break the pairwise matrix	parallel pairwise x y	0.166667
on the training set according to the	factor fit	0.062500
hypercube	hypercube	1.000000
luinv	lu inv	1.000000
dataset this operation is meant to be cached	index_file_path data_folder_path slice_ color	0.033333
the main classification metrics read more	metrics classification	0.052632
1 iteration	total_samples batch_update parallel	0.500000
update terminal regions	update terminal regions	0.500000
batch of estimators within	estimators	0.052632
number of points	len	0.038462
warnings	utils ignore warnings	0.142857
step length is not found and	utils line search	0.029412
validate user provided precisions	precisions precisions covariance_type n_components	0.250000
matrix factorization nmf find	non negative factorization	0.043478
convert	sparse coef mixin	0.083333
em	decomposition latent dirichlet allocation em	1.000000
columns of	columns	0.111111
factorization nmf find two non-negative matrices	decomposition non negative factorization	0.043478
an arbitrary python object into	externals joblib dump value filename	0.083333
loader for the california housing dataset from statlib	fetch california housing data_home	0.250000
jaccard similarity coefficient score	score y_true	0.058824
n_jobs even slices	x y func n_jobs	0.166667
long type introduces an 'l' suffix when	utils shape	0.013699
image from all of	from	0.045455
transforms features by scaling each feature to a	preprocessing minmax scale x feature_range axis copy	0.200000
receiver operating characteristic roc note this	metrics roc curve	0.142857
accept_sparse	accept_sparse	1.000000
coefficient matrix to dense array format	densify	0.066667
solution to a sparse coding problem	sparse encode x dictionary gram	0.333333
the null space	manifold null space	0.333333
a locally linear embedding analysis	locally linear embedding x n_neighbors n_components	0.071429
load sample images	load sample images	0.250000
as the maximizer of the reduced likelihood	gaussian_process gaussian process arg max reduced likelihood	0.250000
x format and make sure no	non neg array	0.250000
file as a	externals	0.005747
ada	ada	1.000000
cv in	core check cv cv x	0.031250
estimates for	x	0.001692
actual data loading for the lfw	datasets fetch lfw	0.083333
division and handles divide-by-zero	metrics prf divide numerator denominator metric modifier	0.250000
precisions	precisions precisions covariance_type n_components	0.250000
analysis fa a simple linear	analysis	0.045455
using the gaussian process regression	gaussian_process gaussian process regressor	0.058824
data	cross	0.037037
for each input data point	cross val predict estimator	0.045455
checker utility for building a cv in	check cv cv x	0.031250
svd parameters	svd	0.071429
hiddens	hiddens	1.000000
dictionary factor	dictionary	0.071429
m	m	1.000000
names for	names	0.090909
windows this is the time it take to	joblib squeeze time t	0.200000
indices	matching indices	0.250000
list of exception types	base get	0.066667
score on the	score	0.010101
area under the curve auc using the trapezoidal	metrics auc x y	0.040000
long type introduces	utils shape	0.013699
explained variance	explained variance	0.166667
return posterior probabilities of	core quadratic discriminant analysis predict proba	0.333333
determinant matrix	fast mcd x support_fraction cov_computation_method random_state	1.000000
random multilabel classification	datasets make multilabel classification	0.166667
in-place	swap row	1.000000
return the file descriptor for the underlying file	joblib binary zlib file fileno	0.333333
object in	externals	0.005747
this wrapper	wrapper	0.125000
the pairwise matrix in	pairwise x y	0.166667
we don't	memorized	0.015873
coefficient matrix to dense array	mixin densify	0.100000
build a batch	ensemble parallel build	0.047619
backend and	parallel backend	0.030303
to fit a single tree	build trees tree forest	0.142857
the number of splitting iterations in the	leave pgroups out get n splits	0.111111
k-neighbors of a	kneighbors mixin kneighbors	0.100000
values used to update params with given gradients	updates grads	0.076923
fit lsi	truncated svd fit	1.000000
c	c x	0.030303
with aligned memory	aligned	0.076923
types to	externals joblib parallel backend	0.029412
build a contingency matrix describing the	contingency matrix labels_true labels_pred eps sparse	0.166667
a random sample from a given 1-d array	utils choice a size	0.250000
for full covariance matrices	multivariate normal density full x means covars min_covar	0.166667
the cache for the function	externals joblib memorized func	0.013158
to run a	externals joblib	0.004762
from it	externals joblib memorized func	0.013158
a locally linear embedding analysis on the data	locally linear embedding x n_neighbors n_components reg	0.071429
each input data point	x	0.001692
perform dbscan clustering from vector array or	cluster dbscan x eps min_samples metric	0.200000
on the estimator with the best found parameters	base search cv predict proba x	0.076923
generative	decomposition base pca get	0.071429
list of exception types to	parallel backend	0.030303
last step in pipeline after transforms	core pipeline fit predict	0.166667
barycenter	barycenter	0.545455
list	joblib parallel	0.028571
rfe model and	x y	0.002155
the leaves	get leaves	0.111111
minimum and maximum	utils min max axis	0.500000
for the one-vs-one multi class libsvm	svm one vs one	0.050000
estimate the precisions	mixture bayesian gaussian mixture estimate precisions nk	0.166667
array or sparse matrix x	x dict_type	0.200000
utility for building a cv	cv cv x y classifier	0.031250
introduces an 'l' suffix	utils	0.009709
cosine distances between x and y read	cosine distances x y	0.333333
input samples	tree decision tree	0.500000
score by cross-validation	core cross val score estimator x y	0.333333
undo the scaling of	min max scaler	0.083333
dictionary of all tokens in the raw documents	feature_extraction count vectorizer fit raw_documents y	0.125000
find the first prime element in the	state find prime in	0.333333
learning finds a	learning	0.125000
axis center to the mean and component	axis	0.014085
posterior log probability of the	core multinomial nb joint log likelihood	0.083333
errors	errors	1.000000
provided precisions	mixture check precisions precisions covariance_type n_components	0.250000
which are going to run	joblib multiprocessing backend	0.052632
the number of splitting iterations in	base cross validator get n splits x	0.125000
the minimum	min	0.045455
test	y_test	0.285714
opposite of the local outlier factor of	local outlier factor decision	0.125000
length is not found and raise	utils line search	0.029412
handle the callable case for	pairwise callable x y	0.083333
compute incremental mean and variance	incr mean variance	0.333333
factorization nmf	non negative factorization	0.043478
class or regression value for x	x check_input	0.250000
don't	memorized	0.015873
each non zero row of x	x	0.001692
iterate over columns of a	feature_selection iterate columns x columns	0.250000
perform a locally linear embedding analysis on the	locally linear embedding x n_neighbors	0.071429
is restricted to the binary classification task	precision recall curve y_true	0.142857
in friedman [1] and breiman [2]	friedman3 n_samples noise	0.166667
logdet	logdet	1.000000
what extent the local structure is retained	x_embedded n_neighbors precomputed	0.200000
for full covariance matrices	multivariate normal density full x means	0.166667
build a contingency matrix describing the	contingency matrix labels_true	0.166667
pairwise matrix in n_jobs	pairwise x y func n_jobs	0.111111
for full covariance matrices	normal density full x means covars min_covar	0.166667
calculate approximate perplexity for	decomposition latent dirichlet allocation perplexity	0.333333
relative to y_true	metrics threshold scorer call clf	0.333333
probabilities of possible outcomes for samples in x	ensemble voting classifier predict proba	0.200000
generates integer indices corresponding to test sets	iter test indices	0.333333
compute non-negative matrix factorization nmf find two non-negative	negative factorization x	0.043478
run	grid search	1.000000
the number of splitting iterations in	model_selection leave one group out get n splits	0.111111
fit linear model with stochastic gradient descent	sgdclassifier fit x y coef_init intercept_init	1.000000
fits the graphlasso covariance	covariance graph lasso cv fit	0.111111
the index of the leaf	base decision tree apply x	0.166667
using x y	x y	0.002155
point	core cross val predict	0.045455
new subcluster into	subcluster subcluster	0.500000
a random regression problem with sparse	sparse	0.025000
estimates	core cross val predict estimator x y	0.045455
x	predict x	0.058824
fit a binary classifier on	fit binary	0.200000
generative model	get	0.012048
the process or	multiprocessing	0.045455
the number of splitting iterations in the	model_selection leave one group out get n splits	0.111111
compute mean and	utils mean	0.250000
sure centering is	scaler check array x	1.000000
evaluate the significance of a cross-validated	estimator x y cv	0.050000
this dataset is described in friedman	datasets	0.015152
x y and optionally its gradient	exponentiation call x y eval_gradient	0.333333
fit the	svm linear svc fit x	0.333333
dual gap convergence criterion the	covariance dual gap	0.071429
change the default backend	backend backend	0.333333
to this	joblib	0.007299
ht	ht	1.000000
the timestamp when pickling to avoid	externals joblib memorized func reduce	0.050000
using matrix product with the random matrix parameters	core base random projection transform	0.500000
number of splitting iterations in the cross-validator parameters	one out get n splits x	0.111111
class priors from multioutput-multiclass target	class distribution	1.000000
predict label	gmmbase predict	1.000000
by the callers	joblib effective	0.200000
the number of splitting iterations in the	model_selection leave one out get n splits	0.111111
wild lfw pairs dataset this dataset	fetch lfw pairs subset	0.035714
the leaf	tree base decision tree apply x	0.166667
callable case for pairwise_{distances	callable x	0.083333
print verbose message on the	print verbose msg init	0.333333
log probability	mixture log multivariate	0.500000
estimators that implement	utils check partial fit	0.038462
and returns the transformed data	y w h	0.500000
and a name for the	func name	0.047619
matrix factorization nmf find two non-negative matrices	decomposition non negative factorization x	0.043478
the data and	y	0.002674
the weighted log probabilities	base mixture	0.111111
covariance w/ cross-validated choice of	cv	0.009009
regressor from the training set x y	regressor fit x y	1.000000
into a matrix of patch data	patch extractor transform	0.200000
rows of u	u	0.032258
directory	output dir	0.047619
list of exception	get	0.012048
scale back the data to	preprocessing standard scaler inverse transform x	0.066667
the local outlier factor of x	neighbors local outlier factor decision function x	0.100000
used to capture	check_pickle	0.040000
'l'	utils shape	0.013699
the points in	core parameter	0.250000
label encoder	preprocessing label encoder	0.666667
probabilities of classification this function returns posterior probabilities	cv predict proba x	0.034483
variance along an	variance axis x axis	0.090909
inplace column scaling	inplace column scale x	0.166667
implicit data conversions happening in	data conversion	0.333333
the score on the given data if	score x y	0.030303
to capture the arguments	check_pickle	0.040000
computes the paired cosine distances between	metrics paired cosine distances	0.333333
evaluate the	score samples x	0.500000
helper function	helper alpha y v	0.333333
/ tp + fp where tp is	score y_true y_pred labels pos_label	0.027778
call with the given	externals joblib format call	0.200000
fit across one fold	feature_selection rfe single fit rfe	0.200000
fit the model with	core skewed chi2sampler fit	0.250000
suitable step length is not found and raise	search	0.019231
array of a single sample	sample	0.032258
helper function	helper alpha	0.333333
x for a spherical model	spherical x means covars	0.500000
boosted regressor from the training	ensemble ada boost regressor fit	0.500000
the pairwise matrix	metrics parallel pairwise x y	0.166667
a	eigs a	1.000000
we don't store the	memorized func	0.016949
compute mutual information between two continuous variables	feature_selection compute mi cc x y n_neighbors	1.000000
to catch and hide warnings	warnings	0.076923
update reporter with new iteration	ensemble verbose reporter update j est	1.000000
perplexity for data x	perplexity x	1.000000
coefficient	sparse coef	0.071429
the california	fetch california	0.333333
project data to vectors and cluster the result	cluster spectral biclustering project and cluster data vectors	0.333333
the estimator has been refit	base search cv	0.026316
the gaussian process regression model we can	gaussian_process gaussian process regressor	0.058824
an arbitrary python object into one	joblib dump value filename	0.083333
fit linear model with passive aggressive algorithm	linear_model passive aggressive regressor fit	1.000000
offset	offset	1.000000
for mono and multi-outputs	x y eps n_alphas	0.250000
model_selection	model_selection	0.833333
gram matrix	gram	0.083333
compute log probabilities within	parallel predict log proba	0.058824
x format check	check	0.017857
estimator with the best found parameters	search cv predict proba	0.076923
estimates for each	val predict estimator x	0.045455
reconstruct the image from all of its patches	feature_extraction reconstruct from patches 2d patches image_size	1.000000
check initial parameters of	base mixture check parameters x	0.200000
x from	x z reg	0.066667
we don't store the	externals joblib memory	0.016949
load and return the wine	load wine	1.000000
predict using the gaussian process regression	gaussian process regressor	0.055556
on the training	factor fit	0.062500
non-negative matrix factorization nmf find two	decomposition non negative factorization x	0.043478
or thread pool	joblib	0.007299
a list of module names and a name	name	0.033333
axis center to the mean and	x axis	0.015385
the mean squared error between two covariance	covariance error norm comp_cov norm scaling squared	0.250000
array with block checkerboard structure for biclustering	checkerboard shape n_clusters noise minval	0.066667
data into the already fitted lsh forest	neighbors lshforest partial fit	0.200000
hash depending	joblib memorized func	0.014706
that implement the partial_fit api need	utils check partial fit	0.038462
check x format check x format and	latent dirichlet allocation check	0.062500
lars using bic or aic for	lars ic	0.250000
w h whose product approximates the	w h	0.031250
ledoit-wolf covariance	covariance ledoit wolf x assume_centered block_size	0.125000
meant to be	index_file_path data_folder_path slice_ color	0.033333
the mean squared	norm comp_cov norm scaling squared	0.500000
the number of splitting iterations in the cross-validator	split get n splits x y	0.111111
convert coefficient matrix to dense array	sparse coef mixin densify	0.100000
function and cache result	func	0.011364
fit on	fit x y	0.005988
estimates for each input	predict estimator	0.045455
names and a name for the	get func name	0.047619
for factorizing common classes param	fit first call clf classes	0.058824
mean squared error between two covariance estimators	covariance error norm comp_cov norm scaling squared	0.250000
a platform independent	utils	0.009709
compute class priors from multioutput-multiclass target	utils class distribution	1.000000
determines the blup parameters and evaluates the reduced	gaussian_process gaussian process reduced	0.125000
diagonal of the kernel k	pairwise kernel diag	1.000000
or	feature_extraction mask	0.500000
retrieve the leaves of the cf	birch get leaves	0.333333
center to the median and component wise scale	preprocessing robust scale x	0.125000
terminal	terminal region tree terminal_regions leaf x	0.066667
y is	y	0.002674
an array list sparse matrix or similar	check array array accept_sparse dtype order	0.500000
x	x predict x	0.333333
binary gaussian	binary gaussian	1.000000
of parameters and raise valueerror if not valid	ensemble base gradient boosting	0.111111
an arbitrary python object into one file	value filename	0.083333
model to the training set x and returns	x y	0.002155
on the estimator with the best found	base search cv predict proba x	0.076923
arguments of a function	joblib delayed function	0.200000
whether the file	externals joblib binary zlib file	0.166667
a binary	binary score	0.500000
swaps two rows of a csc matrix in-place	utils inplace swap row csc	0.250000
and transform x	transform x y	0.031250
squared logarithmic error regression loss read	squared log error	0.166667
parameter objects and 'return_annotation'	init parameters return_annotation __validate_parameters__	0.125000
breiman [2]	make friedman3 n_samples noise random_state	0.166667
an arbitrary python object into one	dump value filename	0.083333
to avoid the hash depending from	externals	0.011494
target variable	regression x y discrete_features	1.000000
returns posterior probabilities of classification	calibrated classifier cv predict proba	0.200000
samples x	function x	0.030303
cv and linearsvc	svm fit liblinear x y c fit_intercept	0.142857
the calibrated	calibrated classifier cv	0.071429
the search over	search cv fit	0.111111
remove too rare or too common features	feature_extraction count vectorizer limit features x vocabulary high	0.250000
from features or distance	x y sample_weight	0.012987
introduces an 'l' suffix when using	repr	0.012500
homogeneity metric of a cluster labeling given	cluster homogeneity	0.500000
function call with	call func	0.100000
list of exception	externals joblib parallel	0.014085
the	repr	0.025000
binary classification task	y_true y_score average	0.076923
type introduces an 'l' suffix when using	utils shape repr	0.013699
boston	boston	1.000000
autocorrelation parameters theta as the maximizer	process arg max	0.047619
detects the soft boundary of the set of	svm one class svm fit	0.125000
coefficient of determination regression score	r2 score y_true y_pred sample_weight multioutput	0.125000
we	memory	0.015625
print verbose message on	print verbose msg init	0.666667
along any axis center	axis	0.028169
log probabilities	ensemble parallel predict log proba	0.058824
run a	externals joblib parallel backend base	0.034483
is	y_true y_pred beta	0.500000
factorizing common classes param logic estimators	first call clf classes	0.058824
range approximates the range of	utils randomized range finder	0.083333
the process	joblib	0.007299
y as	y xy	0.333333
precisions	check precisions precisions covariance_type n_components n_features	0.250000
function to	function x	0.030303
of vectors for reproducibility flips the sign of	utils deterministic vector sign flip	0.066667
one-vs-one multi class	one vs one	0.050000
the best found parameters	model_selection base search cv	0.120000
transform feature->value dicts to array or sparse matrix	feature_extraction dict vectorizer transform	0.200000
factorization	factorization	0.214286
multilayer	multilayer	1.000000
wrapped function cache	joblib memorized func	0.014706
estimates for	core	0.015385
call with the given arguments	call	0.052632
get the directory corresponding to the	get func dir mkdir	0.333333
to line_search_wolfe2 if suitable step	wolfe12 f fprime xk pk	0.028571
thresholding of array-like or scipy sparse	binarize	0.045455
non-overlapping	group	0.083333
matrix generation	input size n_components	1.000000
array bytes to pickler file	array array pickler	0.333333
the selected affinity then applies spectral clustering	cluster spectral clustering fit	0.500000
plot	plot	1.000000
for mean_shift	x bin_size min_bin_freq	0.500000
problem with sparse uncorrelated design	make sparse uncorrelated	0.166667
the optimal batch size	externals joblib auto batching mixin compute batch size	0.333333
selected returns	feature_selection	0.066667
arbitrary python object into one file	filename	0.050000
right fileobject	fileobject fileobj	0.200000
the variational lower bound for the mean	mixture dpgmmbase bound means	0.250000
shutdown the process or	multiprocessing backend terminate	0.166667
find two non-negative matrices w h whose	x w h n_components	0.038462
classification by definition a confusion matrix :math c	metrics confusion matrix y_true y_pred labels sample_weight	1.000000
each	predict estimator x	0.045455
blup parameters and evaluates the reduced likelihood function	gaussian_process gaussian process reduced likelihood function	0.047619
embedding analysis	embedding x n_neighbors n_components reg	0.200000
don't store the timestamp when pickling to avoid	joblib memory reduce	0.030303
fit the model using x as training data	neighbors unsupervised mixin fit x	0.500000
decision	gradient boosting classifier decision	0.333333
of a cluster	cluster	0.021277
least angle regression model	lars	0.090909
whether the file supports seeking	zlib file seekable	0.250000
be used for later	fit x y	0.005988
transform new points into embedding space	manifold locally linear embedding transform x	0.500000
does nothing this transformer is stateless	feature_extraction hashing vectorizer fit	1.000000
the hierarchical clustering on	cluster feature agglomeration	0.125000
the number of jobs which are going	multiprocessing backend effective n jobs n_jobs	0.333333
parallel execution	externals joblib parallel	0.014085
the cholesky decomposition	cholesky	0.083333
to avoid	joblib memorized func	0.014706
global clustering for the subclusters obtained after fitting	birch global clustering x	0.142857
length of an unsuccessful bst search since the	length n_samples_leaf	0.500000
fit	fit	0.315961
keep the	root_path bytes_limit	0.500000
estimates for each input data point	cross val predict estimator	0.045455
train estimator	estimator estimator x	0.181818
of the local outlier factor	neighbors local outlier factor decision function	0.125000
data precision matrix with the generative	base pca get precision	0.066667
scale back the data to the original	standard scaler inverse transform x	0.066667
parameters theta as the maximizer	process arg max	0.047619
finds the neighbors within a given radius of	lshforest radius neighbors x radius return_distance	0.500000
the laplacian matrix	laplacian	0.034483
to a	externals joblib customizable pickler	0.200000
avoid the hash depending from it	externals joblib memorized func	0.013158
include_self	x include_self	1.000000
helper function to output a function call	externals joblib function called str function_name args kwargs	0.250000
the binary classification	y_true y_score pos_label	0.066667
boolean thresholding of array-like or scipy sparse matrix	preprocessing binarize x threshold copy	0.083333
test/test sizes are meaningful wrt to the	validate shuffle split n_samples test_size train_size	0.111111
patch	patch extractor	0.090909
average hamming loss	hamming loss y_true y_pred labels sample_weight	0.333333
mostly low rank matrix with	make low rank matrix	0.083333
from it	joblib memorized	0.015625
items to delete to	items to delete	1.000000
computes multidimensional scaling using smacof algorithm	smacof single dissimilarities metric n_components init	0.333333
dual gap convergence criterion the specific definition is	covariance dual gap emp_cov	0.071429
detects the soft boundary	one class svm fit	0.125000
make and configure a	ensemble make estimator append random_state	0.166667
for validation and conversion	dtype csr_output	0.166667
if the test/test sizes are meaningful wrt to	validate shuffle split n_samples test_size train_size	0.111111
deviance	deviance	0.375000
fit the kernel density model on the data	neighbors kernel density fit	0.250000
input	x y	0.002155
a text report showing the main classification metrics	metrics classification report	0.166667
is meant	data_folder_path slice_ color resize	0.033333
the kl divergence of p_ijs	kl divergence	0.083333
x which	x	0.001692
and return that transformed output	y	0.002674
of the mixture parameters	gaussian mixture	0.200000
all meta estimators in scikit-learn	meta estimator	0.062500
build or fetch the effective stop words list	feature_extraction vectorizer mixin get stop words	0.200000
for	cross val predict	0.045455
finds seeds for	bin seeds x	0.250000
precision is the ratio tp /	precision	0.016667
loader for the california housing dataset	california housing data_home download_if_missing	0.250000
the weighted graph of neighbors	neighbors graph	0.066667
each input	val predict	0.045455
estimate model parameters with	fit x	0.006410
batch of estimators within a job	estimators	0.052632
softmax	softmax	1.000000
product	product	1.000000
the autocorrelation parameters theta as the maximizer	process arg max	0.047619
the derived	resp	0.090909
and y	y dense_output	1.000000
median absolute error regression loss	median absolute error y_true y_pred	0.166667
classification	classification	0.642857
contingency matrix describing	contingency matrix labels_true labels_pred eps sparse	0.166667
labeled faces in the wild lfw pairs	datasets fetch lfw pairs subset	0.035714
estimates	estimator x	0.030303
of each sample from the decision	decision	0.027778
decision function of the	decision function	0.025000
passive aggressive regressor read more in	passive aggressive regressor	0.125000
x	scale xy x	1.000000
the hash depending from	memorized	0.015873
trace of np dot	trace dot	1.000000
compute probabilities of	proba	0.058824
private function used to fit a single tree	trees tree forest	0.142857
randomized linear models for feature selection this	randomized linear model	0.076923
callable case	metrics pairwise callable x y	0.083333
fits the oracle approximating shrinkage covariance model	covariance oas fit	0.083333
with a given cache	cache	0.111111
to fit an estimator	fit estimator estimator x y sample_weight	0.071429
introduces an 'l' suffix when using the	shape repr	0.013699
coverage error measure compute how	metrics coverage error y_true y_score sample_weight	0.166667
for different probability thresholds note this implementation	probas_pred pos_label sample_weight	0.066667
methods for outliers detection with covariance estimators	detection mixin	0.500000
coverage error measure compute how far we	metrics coverage error y_true y_score sample_weight	0.166667
non-negative matrices w h	w h n_components	0.038462
finds the	x n_neighbors return_distance	0.250000
the decision boundary for each class	core one vs rest classifier decision function	0.250000
the svmlight / libsvm format	svmlight file f	0.066667
shortest path length from source to all reachable	source shortest path length graph source	0.111111
generator to create	utils gen	1.000000
the decision path	decision path	0.333333
strip lines	strip	0.055556
if	utils	0.029126
cache result and	joblib memorized	0.015625
normalize rows and	normalize	0.100000
cluster-distance	cluster kmeans	1.000000
random_state and sets	random_state	0.076923
helper function	helper	0.100000
reconstruct the array from the meta-information and the	zndarray wrapper read unpickler	0.043478
is meant to be cached by a	data_folder_path slice_ color resize	0.033333
the number of splitting iterations in	kfold get n splits x y groups	0.111111
variance along an axix on a csr	variance axis x axis	0.090909
fit on the estimator with randomly drawn	randomized search cv fit	0.500000
write array bytes	joblib numpy array wrapper write array array	0.500000
return the shortest path length from source to	source shortest path length graph source cutoff	0.111111
search over parameters	search cv fit x y	0.111111
the model from data in	manifold spectral embedding	0.111111
perform classification on an array	nearest centroid predict	0.142857
beta-divergence of x and dot w h	beta divergence x w h beta	0.500000
input data	predict estimator	0.045455
range approximates the range of a	utils randomized range finder a	0.166667
each input	predict estimator x	0.045455
update for 1 iteration	step x total_samples batch_update parallel	1.000000
tanh	tanh	1.000000
load datasets in the svmlight / libsvm format	datasets load svmlight file f n_features dtype	0.500000
labeled faces in the wild lfw pairs	lfw pairs	0.018868
arguments	memorized func	0.016949
turn a transformed real-valued array into a hash	projection to hash	0.333333
a nicely formatted statement displaying the function call	externals joblib format call func args kwargs object_name	0.333333
return the score for a fit	fit rfe estimator	0.166667
to split data into	predefined split split	0.250000
compute the grid of alpha values for	alpha grid x y xy	0.166667
element of	element	0.083333
the local outlier factor of	neighbors local outlier factor decision function	0.125000
each sample	core one vs	1.000000
embedding analysis on the	embedding x n_neighbors n_components reg	0.200000
ridge classifier with built-in cross-validation	ridge classifier cv	1.000000
fit a binary classifier on	linear_model base sgdclassifier fit binary	0.333333
load the kddcup99 dataset downloading it	kddcup99 subset data_home download_if_missing random_state	0.111111
training	fit predict	0.055556
backend and return the number of workers	externals joblib parallel backend base	0.034483
normalize x according to	normalize x	0.076923
image parameters	image image_name	1.000000
predict posterior probability of data	predict proba	0.250000
returns the number of splitting iterations in the	base kfold get n splits x	0.111111
and compute prediction of init	init	0.076923
for validation and	dtype	0.062500
clone of self with given hyperparameters	kernel clone with	1.000000
back to line_search_wolfe2 if suitable step length is	wolfe12 f fprime xk pk	0.028571
of x and dot w h	beta divergence x w h	0.500000
return precisions as	mixture dpgmmbase get precisions	0.250000
- pred	pred	0.142857
sign of	sign	0.050000
input data	core cross val predict estimator x	0.045455
estimator with the best found	model_selection base search cv	0.040000
python object into one	filename	0.050000
list of edges	edges	0.047619
the data in the given	data compress	0.100000
the exponential chi-squared	metrics chi2	0.333333
used to build a batch	ensemble parallel build	0.047619
get a signature object for the passed	externals signature	0.050000
log	sgdclassifier predict log	0.500000
implement a single boost	ensemble ada boost classifier boost iboost	1.000000
query	query	0.750000
the grid of alpha values for	alpha grid x y	0.166667
tests involving both blas calls and multiprocessing	multiprocessing with blas func	0.500000
and configure a copy of the	estimator append random_state	0.142857
predict based on	predict	0.006849
scale back the	preprocessing standard scaler inverse transform	0.066667
for reproducibility flips the sign	deterministic vector sign	0.066667
the median of data	median data	0.333333
predict class probabilities at each stage for x	boosting classifier staged predict proba x	1.000000
optimal batch	auto batching mixin compute batch	0.333333
the dual gap convergence criterion the specific	dual gap emp_cov	0.071429
single boost	ada boost classifier boost	0.100000
check x format check x format and	allocation check	0.062500
the least-squares solution to a	a	0.018182
perform dbscan clustering from vector array or distance	cluster dbscan x eps min_samples	0.200000
precision is the	metrics precision	0.033333
calculate approximate log-likelihood as score	decomposition latent dirichlet allocation score x y	1.000000
hastie et al	datasets make hastie 10 2 n_samples random_state	0.166667
load sample images for	datasets load sample images	0.250000
split data into training and test	model_selection time series split split x y groups	0.200000
the k-neighbors of a	kneighbors mixin kneighbors x n_neighbors	0.125000
clone of self with given hyperparameters theta	kernel clone with theta theta	1.000000
vs	vs	0.833333
with the best	cv	0.045045
to avoid the	joblib memorized	0.015625
returns the number of splitting iterations in	cross validator get n splits x y groups	0.125000
the number of splitting iterations in	predefined split get n splits x y groups	0.111111
convert coefficient matrix to sparse format	sparse coef mixin sparsify	1.000000
for the models computed by 'path' parameters	linear_model path residuals x y	0.250000
multiple files in	files files n_features dtype multilabel	0.500000
precisions parameters of the	precisions	0.066667
fit a multi-class classifier	base sgdclassifier fit	0.076923
individually to unit norm vector length	normalize x norm axis copy	0.200000
tolerance which	tolerance x tol	0.058824
the pairwise matrix in	metrics parallel pairwise x y func	0.166667
kernels which are normalized k x x =1	normalized kernel	0.250000
the number of splitting iterations in the cross-validator	leave one out get n splits x	0.111111
cache folders to	memory reduce	0.030303
returns the number of splitting iterations in	get n splits x y groups	0.111111
function the objective	covariance objective	0.125000
mean	utils incr mean	0.166667
cache	externals joblib cache	0.250000
samples x to	x	0.003384
time it take to	joblib squeeze time	0.200000
estimates the autocorrelation parameters theta as the maximizer	arg max	0.047619
multiple files in svmlight format this	svmlight files files n_features dtype multilabel	0.200000
as a zipped pickle	target_dir cache_path	0.142857
coefficient matrix to dense array	linear_model sparse coef mixin densify	0.100000
mean	utils mean	0.250000
return an iterator over the (key [values]) pairs	externals iterlists d	1.000000
constructor store the useful	externals joblib ndarray wrapper init filename subclass allow_mmap	0.200000
spherical model	spherical	0.090909
helper function to	helper alpha y v	0.333333
to build a batch of estimators within	parallel build estimators n_estimators ensemble	0.166667
export a decision tree	tree export graphviz decision_tree out_file max_depth feature_names	1.000000
global clustering for the subclusters obtained after	cluster birch global clustering	0.142857
indices to split data into training and test	model_selection time series split split x y groups	0.200000
callable case for	metrics pairwise callable x y metric	0.083333
x format check x format	allocation check	0.062500
of determination regression score	r2 score y_true y_pred	0.125000
in the context of the memory	externals joblib memory	0.016949
initialize the model parameters of the	base mixture initialize x	0.333333
estimators within	estimators n_estimators	0.083333
k-neighbors of	kneighbors mixin kneighbors	0.100000
number of splitting iterations in the cross-validator	leave pgroups out get n splits	0.111111
the data and concatenate results	y	0.002674
classification used in hastie et al	hastie 10 2 n_samples random_state	0.166667
call wrapped function cache result and return a	joblib memorized func call	0.200000
introduced by a random	n_samples eps	0.125000
used to fit a single tree in parallel	ensemble parallel build trees tree	0.200000
target values for x relative	predict scorer call estimator x	0.166667
fit linear model with stochastic gradient descent	base sgdregressor fit x y	1.000000
mean shift clustering using	mean shift	0.125000
fit the model using x as training	neighbors local outlier factor fit x y	0.333333
showing the main classification metrics	metrics classification	0.052632
the default backend used by parallel inside	parallel backend backend n_jobs	0.166667
and dense inputs	x y sample_weight	0.012987
to output	called str function_name	0.250000
sample weight array	linear_model base sgd validate sample weight sample_weight	0.333333
length from source	length graph source cutoff	0.200000
generate a grid of points based on the	ensemble grid from	0.166667
verbose message on the end of iteration	verbose msg init end ll	0.333333
the callable case for pairwise_{distances	pairwise callable x y metric	0.083333
a contingency matrix describing the relationship between labels	metrics cluster contingency matrix labels_true	0.200000
fetch an mldata	fetch mldata	1.000000
this implementation is restricted to the binary classification	y_true y_score average sample_weight	0.076923
matrix factorization nmf find two non-negative	decomposition non negative factorization	0.043478
used to fit an estimator	fit estimator estimator	0.055556
the score on the given data if	score	0.010101
estimates for each input data	val predict estimator x	0.045455
decision function of	gradient boosting classifier decision function	0.166667
split data into	kfold split x	0.250000
process or thread	multiprocessing	0.045455
seeds for	seeds	0.111111
fit the model with x	core skewed chi2sampler fit x	1.000000
input data	core cross	0.045455
reproducibility flips the sign of elements of all	utils deterministic vector sign	0.066667
to fit a single tree in parallel	ensemble parallel build trees tree forest x y	0.200000
fit the model to	core multi output estimator fit x y sample_weight	0.200000
range approximates the range of a	range finder a	0.166667
from source to all reachable nodes	graph source	0.200000
using numpy	externals joblib numpy	0.250000
checker utility for building a cv	core check cv cv x y	0.031250
restricted boltzmann machine rbm	rbm	0.166667
paired distances between x	paired distances x	0.500000
implement a single boost	boost classifier boost iboost x y	1.000000
the backend and return the number of	externals joblib parallel backend base	0.034483
process or thread	externals joblib	0.004762
the long type introduces an 'l'	utils shape	0.013699
logistic loss	linear_model logistic loss w	0.500000
training data and parameters	y	0.013369
k	white	0.142857
a sparse combination of the dictionary atoms	decomposition sparse coding mixin transform	0.333333
of matrices	covariance_type n_features	0.500000
from a given template	type tied_cv	0.333333
the decision functions of	decision function x	0.018868
for each input	estimator x y	0.038462
makes sure centering is not enabled for	robust scaler check array x copy	0.333333
approximate nearest neighbors	neighbors lshforest	0.333333
metric	metric	0.428571
generate	core base shuffle	0.166667
log probabilities within a job	ensemble parallel predict log proba estimators estimators_features x	0.250000
to make cache size fit in bytes_limit	reduce size	0.083333
cross-validated	y cv	0.100000
curve auc using the trapezoidal	metrics auc x	0.040000
\#3" regression problem this dataset is described in	datasets	0.015152
samples in x	predict	0.006849
requested by the	backend base	0.032258
for each input data point	cross	0.037037
component wise scale	preprocessing robust scale x	0.125000
biclusters	cluster consensus score	0.250000
input and compute prediction of init	gradient boosting init decision function x	0.142857
arguments	func	0.011364
score on the given	score	0.010101
indices increasingly apart the distance depending on	verbosity filter index	0.055556
full covariance matrices	multivariate normal density full x means covars min_covar	0.166667
the reduced	reduced	0.125000
the callable case for pairwise_{distances	callable x	0.083333
method	method	1.000000
on laplace approximation	classifier laplace	0.333333
embedding analysis on the data	embedding x n_neighbors	0.200000
the given args and kwargs using a list	args kwargs	0.100000
maximum likelihood estimator covariance model according to	covariance empirical covariance fit x	0.166667
kernel k x	gaussian_process product call x	1.000000
of the local outlier factor of	local outlier factor decision function	0.125000
fit underlying estimators	core one vs one classifier fit x y	1.000000
sparse and dense inputs	x y	0.002155
decorator for creating a class with	add	0.071429
posterior log probability	nb joint log likelihood	0.066667
cv in a user friendly way	core check cv cv x y	0.031250
the model parameters of	mixture base mixture	0.111111
cross-validated lasso	cv	0.009009
inplace row scaling of a csr or	utils inplace row scale x scale	0.142857
std to be used for later scaling	preprocessing standard scaler fit	1.000000
median absolute error regression loss	median absolute error y_true	0.166667
even	even	0.833333
graphlasso covariance model	covariance graph lasso cv	0.111111
matching pursuit model omp	matching pursuit	0.333333
return the shortest path length from source to	source shortest path length graph source	0.111111
user provided precisions	precisions precisions covariance_type	0.250000
and return the content as a string	externals joblib	0.004762
compute non-negative matrix factorization nmf find two	negative factorization x	0.043478
weighted graph of neighbors for points	radius neighbors graph	0.066667
load and	load	0.178571
incrementally fit the	core multi output regressor partial fit	0.200000
'l'	repr	0.012500
compute the maximum absolute value to be	max abs	0.047619
the dual gap convergence criterion the specific definition	covariance dual gap emp_cov precision_	0.071429
decides whether it is time to	neural_network base optimizer trigger stopping msg verbose	0.250000
and	x y axis	0.250000
test that was skipped	skip test	0.200000
determinant matrix	fast mcd	1.000000
test/test sizes are meaningful wrt	shuffle split n_samples test_size train_size	0.111111
with the best found	search cv	0.090909
for all meta	meta estimator	0.062500
get a signature object	externals signature obj	0.200000
validation	lib svm validate	0.500000
we can also predict based on	predict x	0.011765
detects the soft boundary of the	one class svm fit	0.125000
the lfw people dataset this	datasets fetch lfw people	0.040000
the scikit-learn data dir	data home data_home	0.055556
api and	fit x y	0.017964
the labeled faces in the wild lfw pairs	datasets fetch lfw pairs subset	0.035714
subcluster	subcluster	0.545455
the callable case for pairwise_{distances kernels}	pairwise callable x y metric	0.083333
the score of	score	0.030303
oracle approximating shrinkage covariance	covariance oas	0.083333
to avoid the hash depending	externals	0.011494
score the	score y_true y_pred normalize	0.125000
return the breast cancer wisconsin	breast cancer return_x_y	0.250000
the kl divergence of p_ijs	manifold kl divergence	0.083333
for the lfw people dataset this operation is	lfw people	0.040000
predict based on an unfitted model	predict	0.006849
x format check x format	latent dirichlet allocation check	0.062500
this operation is meant to be cached	data_folder_path slice_ color resize	0.033333
to the given training data and parameters	fit x y	0.005988
get the weights	get	0.012048
for updating terminal regions (=leaves)	function update terminal region tree terminal_regions leaf x	0.200000
the covariance	to match covariance	0.250000
scale back the data to the original	inverse transform x	0.051282
onto the sparse components	decomposition sparse pca transform x ridge_alpha	0.200000
convert	linear_model	0.025641
estimates for each input data point	x	0.001692
the density model	kernel density	0.083333
the sample weight array	base sgd validate sample weight sample_weight	0.333333
covar	covar	0.384615
and compute	y	0.005348
p_ijs and q_ijs	params p neighbors degrees_of_freedom	0.250000
the gradient	gradient w x y epsilon	0.500000
select features according to a percentile	select percentile	0.333333
matrix factorization nmf	factorization x	0.043478
the bound	mixture bound state log lik x	0.500000
classifier predicts one class versus all others	multiclass x y alpha	0.166667
the em algorithm and	mixture gmmbase	0.034483
train estimator on training subsets incrementally	model_selection incremental fit estimator estimator x	0.500000
of a classification	y_true y_pred labels	0.125000
inverse the	inverse	0.055556
the legacy gaussian process model class	gaussian process	0.083333
for the lfw people dataset this	datasets fetch lfw people	0.040000
input vectors individually to unit norm vector length	normalize x norm axis copy	0.200000
output for x relative to y_true	metrics threshold scorer call clf x	0.058824
fit all transformers transform the data	core feature union fit transform	0.333333
by scaling each	preprocessing minmax scale	0.142857
function used to fit a single tree	build trees tree forest x	0.142857
types to be captured	base get exceptions	0.166667
writable	writable	0.833333
of k-neighbors for	neighbors kneighbors mixin kneighbors	0.100000
function returns posterior probabilities of classification	calibrated classifier cv predict proba x	0.200000
inverse label binarization transformation using thresholding	preprocessing inverse binarize thresholding y output_type classes threshold	1.000000
model we can also predict based	predict	0.006849
estimate the precisions parameters of the precision distribution	bayesian gaussian mixture estimate precisions	0.166667
elastic net path with coordinate	linear_model enet path	0.050000
returns a lower bound on model evidence	lower bound	0.071429
that for c in (l1_min_c infinity) the	c x y	0.030303
to	memorized	0.015873
to	joblib memorized	0.015625
base classifiers	ensemble bagging classifier	0.200000
returns whether	gaussian_process pairwise	0.333333
in svmlight format this function is	svmlight	0.050000
points in the	model_selection parameter	0.500000
matrix factorization nmf find two non-negative matrices	decomposition non negative factorization	0.043478
of csgraph inputs	sparsetools validate graph csgraph directed	0.250000
check that predict is invariant of compute_labels	utils check clusterer compute labels predict name clusterer	1.000000
generate an array with constant	make	0.041667
samples in x	x y	0.002155
input checker utility for building a cv	core check cv cv x	0.031250
computing truncated svd	truncated x n_components	0.200000
function of the	function	0.021277
of exception types	base get	0.066667
model we can also predict based on an	predict	0.006849
the posterior log probability of	core bernoulli nb joint log likelihood	0.083333
number of splitting iterations in the cross-validator	one group out get n splits x	0.111111
sag solver for ridge and logisticregression sag stands	linear_model sag solver x y sample_weight	1.000000
returns false for indices increasingly apart the distance	externals joblib verbosity filter index	0.055556
determine absolute sizes of training subsets and	sizes	0.050000
the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x	0.125000
the number of splitting iterations in the	group out get n splits	0.111111
fit a	core fit	0.333333
incremental principal components analysis ipca	incremental pca	1.000000
least-squares solution to a large sparse linear	utils lsqr a	0.037037
the samples x	function x	0.030303
the variational distributions	mixture dpgmmbase	0.166667
function opening the right fileobject from a filename	externals joblib read fileobject fileobj filename mmap_mode	0.250000
decorrelation	decorrelation	1.000000
from the meta-information and the	joblib zndarray wrapper read unpickler	0.043478
pairs for different probability thresholds note this implementation	probas_pred pos_label sample_weight	0.066667
remove too rare or too common features	feature_extraction count vectorizer limit features x vocabulary	0.250000
by scaling each feature to a given range	scale	0.033333
1 if dtype of x	x	0.001692
find	find	0.857143
cleanup a temporary folder if still existing	delete folder folder_path warn	0.250000
the breakdown point	linear_model breakdown point	0.333333
line_search_wolfe2 if suitable step length	wolfe12 f fprime xk pk	0.028571
and update it with the split subclusters	update split subclusters	1.000000
fit	skewed chi2sampler fit	0.250000
calculate the posterior log probability of the samples	core multinomial nb joint log likelihood	0.083333
under the curve auc using	auc x	0.040000
find two non-negative matrices w h whose	w h n_components	0.038462
one set	point x y	0.500000
perform dbscan clustering from vector array or distance	cluster dbscan x eps min_samples metric	0.200000
perform a locally linear embedding analysis on the	manifold locally linear embedding x n_neighbors	0.071429
k	gaussian_process white	1.000000
sign of vectors for reproducibility flips the sign	utils deterministic vector sign flip	0.066667
compute the beta-divergence	beta	0.090909
hierarchical clustering on the	cluster feature agglomeration	0.125000
array-like or scipy sparse	binarize x threshold	0.083333
fit the model to data	estimator fit x	0.200000
temporary folder if still existing	delete folder folder_path	0.250000
func to be	base apply async func	0.250000
a batch of estimators within a job	estimators n_estimators ensemble x y	0.083333
returns the score	score x y	0.060606
posterior log probability of the samples x	nb joint log likelihood x	0.222222
inplace column scaling of a	utils inplace column scale x scale	0.166667
for c	c	0.022222
spectrum	spectrum	0.454545
in n_jobs	n_jobs	0.023256
mono and	y	0.002674
estimator adheres to scikit-learn	estimator estimator	0.052632
fit the	multi output estimator fit x y	0.200000
median across axis 0	median axis 0 x	0.333333
non-negative matrix factorization	factorization x	0.043478
selecting features based on importance weights	select from model	0.333333
determine absolute sizes of training subsets and	train sizes	0.066667
transform function to portion of selected features parameters	x transform selected copy	0.333333
validity of the input	x metric p metric_params	0.100000
normalize x according to	cluster log normalize x	0.200000
evaluate the density model	kernel density score samples x	0.250000
with randomly drawn	randomized search cv	0.166667
the null	null	0.125000
opening the right fileobject from a filename	joblib read fileobject fileobj filename	0.250000
the usual api and	x y	0.006466
array-like or scipy	binarize x threshold copy	0.083333
kernel	kernel mixin	0.333333
importances the higher	importances	0.100000
returns n_neighbors of	kneighbors x n_neighbors return_distance	0.500000
parameters of the votingclassifier	params deep	0.111111
validate user provided precisions	check precisions precisions covariance_type n_components n_features	0.250000
train estimator on training subsets incrementally	core incremental fit estimator estimator	0.500000
to evaluate the accuracy of a classification	y_true y_pred	0.037037
dual gap convergence criterion the specific definition is	dual gap emp_cov precision_ alpha	0.071429
routine for validation and conversion of	directed dtype csr_output	0.166667
scaling of x according	scaler inverse transform x	0.026316
for the california housing	fetch california housing	0.083333
of the data home cache	clear data home	0.076923
fit the model to data	multi output estimator fit	0.200000
back	robust scaler inverse transform x	0.066667
lad updates terminal regions to	ensemble least absolute error update terminal region	0.200000
perform a locally linear embedding	locally linear embedding x	0.071429
is restricted to the binary classification	y_true y_score average	0.076923
found	line search	0.029412
of a cross-validated score with permutations	core permutation test score estimator x y cv	0.166667
input checker utility for building a cv in	cv cv x y classifier	0.031250
based on	tree x connectivity n_clusters	0.250000
the	decomposition base	0.076923
sample from the distribution p(h|v)	sample hiddens v rng	1.000000
isotonic regression model : min	core isotonic regression y	0.066667
voting classifier valid parameter keys can	ensemble voting classifier set params	0.037037
r^2 coefficient of determination regression score	r2 score y_true	0.125000
calculate out of bag predictions	ensemble base bagging set oob	0.250000
get the	selector mixin get	0.500000
hence	decomposition sparse coder fit	1.000000
the voting classifier valid	voting classifier set params	0.037037
score with	score estimator	0.125000
fit x into an embedded space	manifold tsne fit x y	1.000000
class with	classifier	0.013699
logic estimators that implement	utils check partial fit	0.038462
for elastic net parameter search	y xy l1_ratio	0.250000
returns false for indices increasingly apart the	externals joblib verbosity filter index	0.055556
shrunk ledoit-wolf covariance matrix	covariance ledoit wolf x assume_centered	0.125000
fit linear	fit x y coef_init intercept_init	0.230769
function used to fit a single	build trees	0.142857
convert	ensure	0.166667
a class with a	externals add	0.142857
estimate model parameters	base mixture fit x y	0.200000
the memory	joblib memory	0.016949
its corresponding derivatives with respect to	activations deltas	0.032258
run fit	fit x	0.006410
probabilities for a calibration	calibration	0.071429
array-like or scipy	binarize x threshold	0.083333
from a given 1-d array	utils choice a size replace p	0.250000
call with the given	call	0.052632
retrieve a reliable	externals joblib get	0.142857
the deviance	deviance call	0.333333
dataset downloading it	subset data_home download_if_missing random_state	0.166667
too rare or	feature_extraction	0.037037
loader for the california housing dataset from statlib	datasets fetch california housing data_home	0.250000
oracle approximating shrinkage covariance	covariance oas fit x	0.083333
with respect to	activations deltas	0.064516
centroids on	fit	0.003257
locally linear embedding	locally linear embedding x n_neighbors n_components reg	0.071429
compute the decision function of	ensemble gradient boosting classifier decision function	0.166667
polynomial	polynomial	0.777778
number of splitting iterations in	predefined split get n splits x y	0.111111
the	externals joblib memorized func	0.026316
csgraph inputs	validate graph csgraph directed	0.250000
the l1	manhattan	0.125000
on x	x n_components	0.500000
indices to split data according	split	0.027778
array-like or scipy sparse	binarize x threshold copy	0.083333
the neighbors within a	lshforest radius neighbors	0.166667
block checkerboard structure for	checkerboard shape n_clusters noise minval	0.066667
base class for all	base	0.014286
the wild lfw pairs dataset this	lfw pairs subset	0.035714
fit	core skewed chi2sampler fit	0.250000
this dataset is described in celeux et al	datasets make	0.015625
compute the mean silhouette coefficient	metrics cluster silhouette score x labels metric sample_size	0.250000
graph of	img to graph	0.333333
full lars path	omp path	0.100000
validate input	linear_model base sgd validate	1.000000
the curve auc from prediction scores note	metrics roc auc	0.166667
data in the given file as	data compress	0.100000
data under each gaussian	mixture gmmbase	0.034483
for building a cv in	cv cv	0.031250
the query	query	0.125000
the leaf	apply	0.083333
right fileobject from a filename	joblib read fileobject fileobj filename	0.250000
the curve auc	metrics auc	0.040000
covariance matrices from a given template	matrix to match covariance type tied_cv covariance_type	0.333333
initial parameters	parameters	0.055556
predict the target of new samples	cv predict x	0.125000
windows this is the time	time t	0.125000
of a memmap instance to reopen on same	reduce memmap a	0.050000
of x (as	x	0.001692
cross-validated estimates for	val predict estimator x y cv	0.071429
helper	helper alpha	0.333333
the timestamp when pickling	memory reduce	0.030303
fit gaussian process classification model parameters	gaussian_process gaussian process classifier fit x y	0.500000
train	core	0.015385
the parameters for the voting classifier valid	voting classifier set params	0.037037
jobs that	jobs	0.111111
and conversion	csr_output	0.111111
is connected true or not false	is connected	1.000000
kernel k	gaussian_process pairwise kernel	0.250000
of the data onto the	transform x ridge_alpha	0.071429
of exception types	base	0.014286
of feature	feature_extraction dict vectorizer get feature	0.200000
for binary classification used in hastie	make hastie	0.125000
the decision	decision	0.111111
x y and	squared call x y	1.000000
conditional property using the descriptor	iff has attr descriptor	0.083333
compute the mlp	neural_network base multilayer perceptron	0.083333
density lrd the lrd of a sample	density distances_x neighbors_indices	0.200000
data	val	0.037037
implement a single boost	classifier boost iboost	1.000000
linear embedding analysis on the data	linear embedding	0.083333
voting classifier valid	voting classifier set	0.037037
remove	externals	0.005747
estimate sample weights by class for unbalanced datasets	compute sample weight class_weight y indices	0.500000
the laplacian matrix and convert it to	laplacian	0.034483
by scaling each feature	minmax scale	0.142857
fit_predict of last step in pipeline after transforms	pipeline fit predict	0.166667
indices in	find matching indices	0.250000
false positives per binary classification threshold	binary clf curve	0.090909
and variance along an	variance axis x axis	0.090909
to the file	joblib binary zlib file	0.066667
callable case for pairwise_{distances kernels}	pairwise callable x	0.083333
whether the kernel is stationary	stationary kernel mixin is stationary	0.333333
reconstruct the array from the meta-information	zndarray wrapper read unpickler	0.043478
samples	decision function x	0.018868
for building a cv in a	check cv cv x y	0.031250
partially fit underlying estimators	one vs one classifier partial fit x	0.166667
content of the data home	datasets clear data home data_home	0.076923
labels back to	label binarizer inverse	0.166667
introduces an 'l' suffix	utils shape	0.013699
that implement the	utils check	0.023810
wild lfw pairs dataset	lfw pairs	0.018868
number of splitting iterations in the	model_selection leave pgroups out get n splits	0.111111
from it	memorized func	0.016949
a	svds a	0.166667
step length is not found	utils line search	0.029412
to data matrix x and target y	multilayer perceptron partial	0.166667
rank matrix with	rank matrix	0.166667
the unnormalized posterior log probability	core base nb joint log likelihood	0.166667
the number of splitting iterations in the	model_selection cviterable wrapper get n splits x	0.111111
data under the model	mixture vbgmm score samples x	0.200000
generate cross-validated	estimator x y cv	0.050000
estimate class weights for unbalanced datasets	compute class weight class_weight classes	0.500000
unnormalized posterior log probability of x	base nb joint log likelihood x	0.200000
reliable	joblib get	0.333333
calculate true and false positives per binary	binary clf curve y_true	0.090909
a mask to edges weighted or	feature_extraction mask edges weights mask edges weights	0.166667
a locally linear embedding analysis on	manifold locally linear embedding x n_neighbors n_components	0.071429
of array-like or scipy sparse matrix	preprocessing binarize	0.083333
a single tree in parallel	ensemble parallel build trees tree forest	0.200000
an array shape under python 2	shape	0.011765
number of splitting iterations in the cross-validator parameters	get n splits	0.111111
function cache result and	joblib memorized func	0.014706
mixin class for all	mixin	0.037037
used to update params with given gradients	updates grads	0.076923
check x format check x format	check	0.017857
estimate class weights for	class	0.071429
each parameter weights and	x y	0.002155
all the vectors rows of u such that	u	0.032258
validate x whenever one tries to predict apply	tree base decision tree validate x predict x	0.500000
check the validity of the input	check params x metric p metric_params	0.200000
the search over parameters	core base search cv	0.033333
implement the usual api and hence	preprocessing binarizer fit x y	0.142857
warning used	warning	0.166667
with the best found	search cv predict proba	0.076923
w h whose product approximates	x w h n_components	0.038462
a locally linear embedding analysis	locally linear embedding	0.050000
in parallel n_jobs is the	n_jobs	0.023256
linear embedding analysis on the	linear embedding x n_neighbors n_components	0.200000
the shortest path length from source	utils single source shortest path length graph source	0.111111
species	datasets fetch species	0.500000
train estimator on training subsets incrementally	core incremental fit estimator estimator x	0.500000
an extremely randomized	extra	1.000000
break the pairwise matrix	pairwise	0.066667
fit a multi-class classifier by combining	fit	0.003257
compute elastic net path with coordinate descent the	enet path x	0.050000
raw	raw	1.000000
x w	w x	1.000000
long type introduces	repr	0.012500
other and	x y	0.002155
estimates for each input	core	0.015385
sort features by name	feature_extraction count vectorizer sort features x vocabulary	1.000000
fit the model according to the given training	svc fit x y sample_weight	0.250000
vectors	vectors	1.000000
list of	parallel backend base	0.037037
shutdown the process or thread pool	backend terminate	0.166667
list of regularization	path x y pos_class cs	0.166667
all the content of the data home cache	data home	0.076923
number of splitting iterations in the	predefined split get n splits x	0.111111
scaling	scaler fit	0.076923
the local outlier factor of x (as	neighbors local outlier factor decision function x	0.100000
model according to the given training data	sample_weight	0.037037
update	optimizer update	1.000000
train estimator on training subsets incrementally and	model_selection incremental fit estimator estimator x y	0.200000
lfw pairs dataset this dataset is a	datasets fetch lfw pairs subset	0.035714
the model using x y	x y sample_weight	0.012987
diagonal of the kernel k	dot product diag	1.000000
reducer function to a	externals joblib	0.004762
of the dual gap convergence criterion	covariance dual gap emp_cov	0.071429
transform is sometimes referred to by	transform	0.011236
coded	coded	1.000000
sample from the distribution p(v|h)	bernoulli rbm sample visibles h rng	1.000000
init	base gradient boosting init	0.142857
shortest path length from source to all reachable	utils single source shortest path length graph source	0.111111
the l1 distances between the vectors in x	metrics paired manhattan distances x	0.500000
c such that for c	c x y loss	0.030303
load datasets in	datasets load	0.083333
return the directory in which are persisted	dir	0.038462
unit norm vector length	norm axis copy	0.200000
the callers	backend base effective	0.250000
build a batch of estimators within a job	ensemble parallel build estimators	0.166667
perform a locally linear embedding analysis on the	locally linear embedding x n_neighbors n_components reg	0.071429
shutdown	terminate	0.090909
is inefficient to	x y classes	0.027778
non-negative matrix factorization	non negative factorization	0.043478
split data into	series split split	0.250000
in bytes_limit	reduce	0.017241
fall back to line_search_wolfe2 if	wolfe12 f fprime xk pk	0.028571
in hastie	make hastie	0.125000
the kernel k x y and	exponentiation call x y	0.200000
k-fold iterator variant with non-overlapping	group kfold	0.250000
reconfigure the backend	backend base configure n_jobs	0.500000
shortest path length from source	source shortest path length graph source	0.111111
a memmap instance to reopen on	joblib reduce memmap a	0.050000
the output of transform is	transform	0.011236
local outlier factor of x (as bigger	local outlier factor decision function x	0.100000
predict class probabilities at each stage for	boosting classifier staged predict proba	0.500000
to fit an estimator within a job	parallel fit estimator estimator	0.333333
the search	search cv	0.018182
global clustering for the subclusters	birch global clustering	0.142857
to avoid the hash depending	externals joblib memory	0.016949
in	ensemble	0.400000
the paired cosine distances between	metrics paired cosine distances	0.333333
reduce_func	reduce_func	0.625000
computes the position of the points	mds fit x	0.066667
inplace row scaling of a	utils inplace row scale x scale	0.142857
an arbitrary python object	joblib dump value filename	0.083333
rows of u	flip u	0.047619
logistic regression and cv and	y	0.002674
and component wise scale	scale x	0.086957
to build a batch	build	0.037037
norm vector length	x norm	1.000000
build a batch of estimators within	parallel build estimators n_estimators ensemble x	0.166667
function	externals joblib delayed function	0.200000
true and false positives per binary classification	binary clf curve y_true	0.090909
check the gaussian mixture	mixture gaussian mixture check	1.000000
output of transform	transform	0.011236
used when memory is inefficient	x y classes	0.027778
outliers detection with covariance	detection	0.166667
<affinity_propagation>	s preference convergence_iter max_iter	1.000000
func	joblib pool manager mixin apply async func	0.250000
get the parameters	get	0.012048
write	write	0.818182
a given radius	x radius	0.058824
calculate true and false positives per binary	binary clf curve y_true y_score pos_label sample_weight	0.090909
cv in a	core check cv cv x	0.031250
dataset is constructed by	datasets make	0.015625
length from source to all	length graph source	0.200000
everything before the first blank line	header	0.090909
outlier factor of x (as bigger is better	outlier factor decision function x	0.200000
a platform independent	repr	0.012500
transform on the estimator with the best	cv transform x	0.500000
fit label encoder and return	preprocessing label encoder fit transform y	0.200000
kernel k x	call x	0.142857
the directory in which are	output dir	0.047619
a list of feature	feature	0.055556
for the given param_grid	model_selection grid search cv get param iterator	0.166667
under the	score	0.010101
is invariant of compute_labels	name clusterer	0.250000
step length is not found and raise an	search	0.019231
of the kl divergence of	kl divergence	0.083333
used for later scaling	scaler fit x y	0.200000
perform the mstep	do mstep x responsibilities params min_covar	1.000000
theilsenregressor class	linear_model lstsq	1.000000
compute mean and variance along an	mean variance axis x axis	0.142857
display the message on stout or	externals joblib parallel print	0.125000
utility for building a cv in a	check cv cv x y	0.031250
a gaussian data set with self	x_test	0.083333
classifier predicts one class versus all others	multiclass x y	0.166667
suitable step length is not found and	search	0.019231
a parallelbackend must implement	backend	0.016949
the likelihood of the data under the model	mixture vbgmm score samples x	0.200000
the approximate feature map to x	core rbfsampler transform x	0.333333
the training set x and returns the	predict x y	0.043478
transform binary labels back to	label binarizer inverse transform y	0.500000
the cholesky decomposition of	log det cholesky	0.166667
negative	negative	0.636364
for full	multivariate normal density full	0.166667
and cluster the	and cluster	0.333333
and breiman [2]	friedman3 n_samples	0.166667
defines all methods a parallelbackend must implement	parallel backend base	0.037037
predict on the estimator with the best	cv predict x	0.125000
process	backend	0.016949
forest to	forest	0.076923
decision tree regressor from the training	tree decision tree regressor fit	0.250000
s	s	1.000000
and configure a copy of	estimator append random_state	0.142857
of u	flip u	0.047619
of the log	log	0.018868
compute the score of	score	0.020202
the logarithm of the determinant	det	0.071429
covariance determinant mcd : robust estimator	cov det	0.200000
given param_grid	cv get param iterator	0.166667
the number of splitting iterations in the	cviterable wrapper get n splits x y groups	0.111111
aggressive	aggressive	1.000000
partially fit a single binary estimator	core partial fit binary estimator x	1.000000
the model according to	y sample_weight	0.035714
class to repeatedly solve m*x=b	iter inv	0.200000
along any axis center to the	axis	0.028169
mostly low rank matrix with bell-shaped	low rank matrix	0.083333
sign of vectors for reproducibility flips the sign	utils deterministic vector sign	0.066667
compute non-negative matrix factorization nmf find	decomposition non negative factorization	0.043478
indices to split data into training and test	model_selection predefined split split x y groups	0.200000
fit linear model	sgdclassifier fit	0.076923
\#3" regression problem this dataset is	datasets make	0.015625
a given dataset	y scorer	0.111111
for the lfw	datasets fetch lfw	0.083333
fit the model according to the given	fit x y sample_weight	0.040000
tp + fn where tp is the number	score y_true y_pred labels pos_label	0.027778
legacy gaussian mixture model	gmm	0.166667
from a given 1-d array	utils choice a size	0.250000
the given param_grid	get param iterator	0.166667
curve auc using the trapezoidal rule	metrics auc x y	0.040000
the search	base search cv	0.026316
output for x	x y sample_weight	0.012987
cache for the function	externals joblib memorized func	0.013158
generate a random multilabel classification problem	datasets make multilabel classification n_samples n_features	0.500000
the vocabulary dictionary and return term-document matrix	fit transform raw_documents y	0.100000
platform independent	repr	0.012500
return_annotation	return_annotation	1.000000
binarize	binarize	0.272727
fit linear model with stochastic gradient descent	linear_model base sgdregressor fit x y	1.000000
and then the underlying estimator on	y	0.002674
timestamp when pickling to avoid the hash depending	memory reduce	0.030303
the number of splitting iterations in the cross-validator	out get n splits x	0.111111
predict_log_proba on the estimator with the best	cv predict log proba	0.500000
any n-dimensional array in place using strides	arr patch_shape extraction_step	0.166667
parameters with given gradients parameters	params grads	1.000000
context manager and decorator to ignore	utils ignore	1.000000
of a read file object	read file	0.333333
and a	x y axis	0.250000
compute minimum and maximum	utils min max	0.500000
area under the curve auc	auc score	0.052632
and y is float32	y	0.002674
a single	build trees	0.142857
calculate true and false positives per binary classification	binary clf curve	0.090909
estimate the precisions parameters of the precision	gaussian mixture estimate precisions	0.166667
patches that will be extracted	patches i_h i_w p_h p_w	0.250000
the wild lfw pairs	fetch lfw pairs subset	0.035714
error regression loss	error y_true y_pred	0.125000
thread pool	joblib multiprocessing	0.052632
expected value of the log of	log	0.018868
california housing dataset from statlib	datasets fetch california housing	0.083333
the kernel k	pairwise kernel	0.250000
raw documents	feature_extraction count vectorizer fit raw_documents y	0.125000
the data	datasets clear data	0.142857
return the kernel k x y and	gaussian_process pairwise kernel call x y	0.333333
suffix when using	utils shape repr	0.013699
np dot	dot	0.250000
variance regression score function best possible score is	variance score y_true y_pred sample_weight multioutput	1.000000
the best found	core base search cv predict proba x	0.076923
covariance determinant matrix	covariance fast mcd x support_fraction cov_computation_method random_state	0.250000
user provided precisions	precisions precisions	0.250000
handle the callable case for	pairwise callable x	0.083333
linear models for feature selection	linear model	0.090909
fit the model	output estimator fit x y sample_weight	0.200000
run fit on the	fit x y	0.005988
with the best found	model_selection base search cv predict proba	0.076923
weighted graph of neighbors for	neighbors radius neighbors mixin radius neighbors graph	0.066667
predict based	predict x	0.011765
callable case for	pairwise callable x	0.083333
display the process of the parallel	parallel print	0.142857
get the parameters of the	voting classifier get	0.200000
for all kernel operators	kernel operator	0.142857
extract	joblib extract	0.500000
introduces an 'l' suffix when using the	utils shape	0.013699
of a memmap instance to reopen	externals joblib reduce memmap a	0.050000
maximizer of the reduced likelihood	process arg max reduced likelihood	0.250000
func	base apply async func	0.250000
weighted graph of neighbors for	mixin radius neighbors graph	0.066667
return the kernel k x	gaussian_process white kernel call x	0.333333
matrix factorization nmf find two non-negative matrices w	decomposition non negative factorization x w	0.500000
under the curve auc using the trapezoidal rule	metrics auc	0.040000
timestamp when pickling to	reduce	0.034483
n_classes-1	label encoder	0.500000
long type introduces an 'l' suffix	shape repr	0.013699
compute the residues	residues	0.125000
in	externals joblib	0.004762
model using	linear_model lasso	1.000000
linear model with passive	linear_model passive	0.400000
cache folders to make cache size fit in	size	0.032258
gbrt	gbrt	1.000000
to the training set x and returns the	x y	0.002155
to	externals joblib parallel	0.014085
default	default	1.000000
rand index adjusted	cluster adjusted rand score labels_true	0.333333
of classification this function returns posterior probabilities of	cv predict proba x	0.034483
w h whose product	x w h n_components	0.038462
run fit on one set of	core fit grid point x y estimator	0.500000
initialize latent variables	decomposition latent dirichlet allocation init latent vars n_features	1.000000
read	externals joblib read	0.333333
boosted classifier/regressor from the training set x	ensemble base weight boosting fit x	1.000000
for a	svds a	0.166667
compute the largest k singular values/vectors for a	utils svds a k ncv tol	0.166667
class for all meta	meta	0.043478
returns the number of splitting iterations in	model_selection leave pgroups out get n splits x	0.111111
loading for the lfw people dataset	fetch lfw people	0.040000
checkerboard	checkerboard	0.600000
the cache	externals joblib memorized func	0.013158
list of exception types to	backend base get	0.066667
multiprocessing	utils if safe multiprocessing	1.000000
for c in (l1_min_c infinity) the model	c x y	0.030303
to dense array format	coef mixin densify	0.100000
u	flip u	0.047619
kernel k x y and optionally its gradient	gaussian_process sum call x y eval_gradient	1.000000
estimates for each input	predict estimator x	0.045455
input data point	cross val predict estimator x	0.045455
build a process or thread pool and	multiprocessing backend	0.038462
multi-class targets	output code classifier	0.250000
input checker utility for building a cv	cv cv x y classifier	0.031250
the kernel k	gaussian_process compound kernel call	0.333333
the decision function of the given observations	covariance outlier detection mixin decision function x raw_values	0.333333
back to	inverse	0.055556
for the precision matrix	empirical covariance get precision	0.250000
generate an array	datasets make	0.031250
building a cv in	cv cv x y classifier	0.031250
u such	u	0.032258
number of splitting iterations in the	model_selection leave one out get n splits x	0.111111
given format	format spmatrix	1.000000
can also predict	predict x	0.011765
the boolean mask indicating which features are selected	support mask	0.125000
classes param	classes	0.025641
each input	val predict estimator x	0.045455
svmlight / libsvm format into	svmlight file f n_features	0.066667
extract the first	extract first	1.000000
leaves of the cf	birch get leaves	0.333333
fit the model with	skewed chi2sampler fit	0.250000
function varies for mono and	x y	0.002155
for each mixture	mixture gmmbase	0.034483
return number of samples in array-like	utils num samples	0.250000
single binary estimator	core predict binary estimator x	0.200000
get the values used to update params	neural_network sgdoptimizer get	0.125000
step length is not found and raise	utils line search	0.029412
compute mean and variance	utils mean variance	0.500000
first and last element of numpy array or	first and last element arr	0.200000
function used to build	build	0.037037
the kernel ridge model	kernel ridge	0.250000
ridge regression	ridge gcv	1.000000
check initial parameters of the derived class	mixture base mixture check parameters x	0.200000
repeated stratified k-fold cross validator	repeated stratified kfold	1.000000
permutations	core permutation test	0.500000
binary	metrics average binary score	0.500000
the number of splitting iterations in the	pgroups out get n splits x y	0.111111
the oracle approximating shrinkage covariance model according to	covariance oas fit x	0.083333
embedding read more in	embedding	0.040000
estimates for each	x y	0.002155
model parameters	fit x y	0.011976
for the lfw pairs dataset this	datasets fetch lfw pairs	0.018868
funneled	funneled	1.000000
tell	tell	0.625000
of array-like or scipy sparse	binarize	0.045455
the reduced likelihood function for the given autocorrelation	gaussian_process gaussian process reduced likelihood function	0.047619
w h whose	x w h	0.035714
f-beta score is the weighted harmonic mean	metrics fbeta score y_true y_pred beta	0.333333
number of splitting iterations in the cross-validator	group out get n splits x y	0.111111
remove	externals joblib memory	0.016949
fit the model	svc fit x	0.333333
boosted classifier from the	ensemble ada boost classifier	0.200000
of init	init	0.076923
showing the main classification metrics read more	metrics classification	0.052632
core	core	0.076923
coefficient matrix to dense array	densify	0.066667
length is not found	search	0.019231
compute a logistic regression	logistic regression	0.200000
for each input data point	x y	0.002155
linear embedding analysis on the	linear embedding x n_neighbors n_components reg	0.200000
an array with block checkerboard structure for biclustering	checkerboard shape n_clusters noise minval	0.066667
unnormalized posterior log probability of x i	base nb joint log likelihood x	0.200000
generate	n_samples n_features	0.416667
the number of splitting iterations in the	kfold get n splits x y groups	0.111111
compute k-means clustering	cluster kmeans fit x	1.000000
perform the	x responsibilities	0.500000
the curve auc using the trapezoidal rule this	metrics auc x	0.040000
20newsgroups	20newsgroups	0.277778
of a gaussian data set with self	x_test	0.083333
estimate model parameters with the em	base mixture fit x	0.200000
the score for a fit across one fold	feature_selection rfe single fit	0.200000
beta-divergence of x and dot w h	decomposition beta divergence x w h beta	0.500000
return the kernel k x y and	gaussian_process white kernel call x y	0.333333
cohen's kappa	kappa	0.111111
fit	multi output estimator fit x	0.200000
back the data to the original	standard scaler inverse transform x	0.066667
net model with iterative fitting along	net cv	0.333333
set	linear model set	0.500000
of x	x	0.027073
of exception types to	externals	0.005747
estimates for	core cross val predict estimator x y	0.045455
process regression model we	process regressor	0.166667
using	shape repr	0.013699
to avoid the	joblib	0.014599
compute the precision the precision is the ratio	precision	0.016667
exception types	parallel backend base	0.037037
a conditional property using the descriptor protocol	iff has attr descriptor	0.083333
of the decision	decision	0.027778
term related to proportions	proportions z	1.000000
data loading for the lfw	lfw	0.068966
labels the output of transform is	transform	0.011236
block checkerboard structure	checkerboard shape n_clusters noise minval	0.066667
scaling features of x	scaler transform x	1.000000
nicely formatted statement displaying the function call with	call func args kwargs object_name	0.333333
we don't store the timestamp when pickling	reduce	0.034483
of p_ijs and q_ijs	params p neighbors degrees_of_freedom	0.250000
list of exception types	joblib parallel backend	0.045455
exception types to	externals joblib parallel	0.014085
for each input data	estimator	0.014706
perform dbscan	dbscan fit x y	1.000000
integer indices corresponding to test sets	core partition iterator iter test indices	0.333333
a cv in	check cv cv x y classifier	0.031250
back the data to the original representation parameters	scaler inverse transform	0.058824
reproducibility flips the sign	deterministic vector sign flip	0.066667
cv in a user	check cv cv x y	0.031250
compute log probabilities within a	predict log proba	0.029412
of two clusterings of a set of	score labels_true labels_pred	0.047619
average of the decision functions	decision function	0.025000
method for updating terminal	terminal	0.047619
covariance	cov	0.100000
matrix generation	check input size n_components	1.000000
lad updates terminal regions to	least absolute error update terminal region tree terminal_regions	0.200000
used to build a	ensemble parallel build	0.047619
importances the higher the more important the	importances	0.100000
and predict_log_proba of	predict log proba x	0.045455
memory is inefficient to	x y classes	0.027778
the best found	model_selection base search cv predict proba x	0.076923
locally linear embedding read more in	locally linear embedding	0.050000
for each input	predict estimator x y	0.045455
callers	externals joblib effective	0.200000
cross-validated choice	cv	0.009009
actual data loading for the lfw pairs dataset	lfw pairs	0.018868
name for	get func name	0.047619
compute the per-sample average log-likelihood of	mixture base mixture score	0.111111
get	linear model get	0.500000
and	fit x y	0.023952
patch	feature_extraction patch extractor	0.200000
callers	joblib effective	0.200000
cv in a	cv cv	0.031250
to emulate function total_seconds introduced in python2	utils total seconds delta	0.200000
classifier predicts one class versus all others	multiclass	0.076923
the descriptors of a memmap instance to	externals joblib reduce memmap a	0.050000
perform dbscan	dbscan	0.166667
k-fold iterator variant with	label kfold	0.250000
multiple files in	files files n_features	0.500000
implement a single	iboost x y sample_weight	1.000000
exception types to	backend	0.016949
friedman [1] and breiman [2]	make friedman3 n_samples noise	0.166667
a byte string to the	externals joblib binary zlib	0.125000
the california housing dataset from statlib	fetch california housing	0.083333
the callers	base effective	0.250000
of the data under the model	mixture vbgmm score samples x	0.200000
estimates for each input	cross val	0.038462
vars	vars	1.000000
avoid the hash depending	memory	0.015625
elasticnet with built-in cross-validation	elastic net cv	0.333333
as training data	copy_x	0.125000
sym	sym	1.000000
compute data precision matrix	base pca get precision	0.066667
perform a locally linear embedding analysis	locally linear embedding x n_neighbors n_components	0.071429
list of regularization parameters	y pos_class cs	0.166667
coverage error measure	metrics coverage error y_true	0.166667
posterior probabilities of classification	core calibrated classifier cv predict proba	0.200000
calculate out of bag predictions	ensemble base forest set oob	0.250000
and	decomposition beta divergence	0.500000
projection to	spectral	0.026316
for data x with ability	x	0.001692
strip lines beginning with	strip	0.055556
this classification dataset is constructed by taking	datasets	0.015152
center and	robust scaler transform x y	0.200000
the text before tokenization	feature_extraction vectorizer mixin build preprocessor	0.142857
transform a sequence of instances to a scipy	feature_extraction feature hasher transform raw_x y	0.333333
inverse label binarization transformation using thresholding	preprocessing inverse binarize thresholding y	1.000000
with categories	categories	0.142857
to	memorized func	0.016949
this dataset is	datasets	0.030303
predict_proba on the estimator with the best found	model_selection base search cv predict proba	0.076923
whether y is monotonically correlated with x	core check increasing x y	0.333333
of x (as bigger	decision function x	0.018868
initialization of the gaussian mixture parameters	mixture gaussian mixture initialize x	1.000000
the wild lfw pairs	datasets fetch lfw pairs	0.018868
format check x format	dirichlet allocation check	0.062500
computes the maximum likelihood covariance estimator parameters	covariance empirical covariance x	0.166667
indices increasingly apart the	verbosity filter index	0.055556
multitaskelasticnet model with coordinate descent parameters	linear_model multi task elastic net	1.000000
memmap instance to	joblib reduce memmap	0.166667
a random multilabel	datasets make multilabel	0.333333
instance for the given param_grid	get param iterator	0.166667
write the function code and the	write func code	0.500000
to evaluate the accuracy of a classification	y_true y_pred labels sample_weight	0.125000
we don't	joblib memorized	0.015625
cross-validated estimates for	estimator x y cv	0.050000
type introduces an 'l'	utils	0.009709
to a projection to the normalized laplacian	spectral	0.026316
scorer	scorer	0.272727
reconstruct the image	feature_extraction reconstruct	0.333333
fit the kernel density model	neighbors kernel density fit x	0.250000
final estimator parameters	core pipeline	0.076923
routine for validation and conversion	directed dtype csr_output	0.166667
mldata	mldata	0.857143
x y as training data	x y xy	0.333333
absolute value to be used for later scaling	abs scaler fit	1.000000
scores note this	roc	0.033333
multiple files in svmlight format this function is	svmlight files files n_features	0.200000
the neighbors	lshforest radius neighbors	0.166667
using the gaussian process regression model we	gaussian process regressor	0.055556
a which this function is	externals	0.005747
the decision functions of the base classifiers	ensemble bagging classifier decision function	0.333333
pairwise similarity matrix	metrics cluster pairwise similarity a b similarity	0.500000
a cv in a user friendly	cv cv x	0.031250
from distances using just nearest	nn distances	0.500000
adjusted for chance	metrics cluster adjusted	0.333333
wild lfw	datasets fetch lfw	0.041667
function varies for mono and multi-outputs	x y eps n_alphas	0.250000
cross-validated choice of the	cv	0.009009
to catch and hide warnings without visual nesting	utils ignore warnings call fn	0.200000
creates an affinity matrix for x using	x y	0.002155
grid of points based on	ensemble grid from	0.166667
placeholder for fit subclasses should implement this method!	decomposition base pca fit x y	0.333333
shortest	utils single source shortest	0.333333
fit estimator using ransac algorithm	linear_model ransacregressor fit x y	1.000000
a locally linear embedding analysis on the data	locally linear embedding	0.050000
finds the	return_distance	0.222222
graph of neighbors for	radius neighbors graph	0.066667
[rouseeuw1984]_ aiming at computing mcd	c step x n_support remaining_iterations initial_estimates	0.111111
strip lines beginning with	datasets strip	0.076923
return precisions as a	mixture dpgmmbase get precisions	0.250000
input data	estimator x y	0.038462
return staged predictions for x	ada boost classifier staged predict x	1.000000
generate cross-validated estimates for each input data point	estimator x y cv	0.050000
the file was opened for writing	zlib file writable	0.250000
a random n-class	n_informative n_redundant	0.333333
sample weights by class	sample	0.032258
median	utils get median	0.166667
non zero row of x	x y copy	0.142857
mean silhouette coefficient	metrics cluster silhouette score x labels	0.250000
a random regression problem with sparse uncorrelated design	sparse uncorrelated n_samples	0.166667
points that will be sampled	model_selection parameter sampler len	0.333333
fit all transformers transform the data	feature union fit transform	0.333333
return the	externals joblib parallel	0.014085
x to the separating hyperplane	svm one class svm decision function x	0.250000
under windows this is the time	time	0.047619
private helper function for parameter value indexing	model_selection index param value x v indices	0.200000
test	test	0.833333
of compute_labels	name clusterer	0.250000
probas	probas	0.833333
any axis	axis	0.028169
separating hyperplane	lib svm decision function	0.333333
the submatrix corresponding to bicluster	bicluster mixin get submatrix	0.333333
each boosting iteration	staged	0.083333
subcluster from	subcluster	0.090909
to the binary	y_score average sample_weight	0.142857
posterior log probability of the samples	core multinomial nb joint log likelihood	0.083333
log probability for full	mixture log multivariate normal density full	0.333333
estimates the shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage x assume_centered block_size	0.125000
introduces an 'l' suffix when using	utils	0.009709
to split data into training and test set	base kfold split x y groups	0.200000
contingency matrix describing the relationship	cluster contingency matrix labels_true labels_pred eps	0.333333
a score by cross-validation read more in	cross val score estimator x y	0.166667
implement randomized linear models for	randomized linear model	0.076923
matrix factorization nmf	non negative factorization	0.043478
returns the number of splitting iterations in	split get n splits	0.111111
replace	replace	1.000000
of x	inverse transform x	0.025641
timestamp when pickling to avoid	memorized func reduce	0.050000
submatrix corresponding to bicluster i	core bicluster mixin get submatrix i data	0.333333
also predict based on an	predict	0.006849
one group out	one group out	0.166667
helper function	helper alpha y	0.333333
the breakdown	breakdown	0.125000
the output of transform is sometimes referred to	transform	0.011236
of exception types	externals joblib	0.004762
predict class for x	ensemble forest classifier predict x	1.000000
check if vocabulary is empty or missing not	mixin check vocabulary	0.250000
thread	externals	0.005747
remove cache folders	externals joblib	0.004762
rbm	rbm	0.833333
minimum and maximum along an axis on	min max axis x axis	0.333333
a locally linear embedding analysis on the	manifold locally linear embedding x n_neighbors n_components reg	0.071429
of a function	delayed function	0.200000
fit linear model with stochastic gradient descent	base sgdregressor partial fit x y	1.000000
use this function first to assert that	check	0.017857
of the samples x	x	0.008460
training and test	y groups	0.500000
dummyclassifier is a classifier	classifier	0.013699
eigenvalues and eigenvectors of the square	m sigma	0.250000
to data matrix x and target y	base multilayer perceptron partial	0.166667
and a set of points	y axis metric	1.000000
precisions	check precisions precisions	0.250000
full covariance	density full	0.166667
back	preprocessing robust scaler inverse transform	0.066667
range of a	randomized range finder a	0.166667
best found	base search cv predict proba	0.076923
within a given radius of	radius	0.045455
call predict on the	predict	0.006849
of neighbors	neighbors radius neighbors	0.100000
fit the	fit x y	0.029940
neighbors	neighbors x	0.166667
nmf find two non-negative matrices w h	x w h n_components	0.038462
kddcup99 dataset downloading it if	brute kddcup99 subset data_home download_if_missing random_state	0.111111
coverage error measure	coverage error	0.166667
vectors for reproducibility flips	deterministic vector	0.076923
produce a cosine lsh fingerprint	gaussian random projection hash	0.166667
em update	dirichlet allocation em step x	0.500000
when the metric	metric	0.071429
pool	externals joblib multiprocessing backend	0.035714
for reproducibility flips the sign of	deterministic vector sign	0.066667
x using	x	0.003384
feature_extraction	feature_extraction	0.185185
the kernel k x y and	x y	0.008621
of edges	feature_extraction make edges	0.066667
a correction to raw minimum covariance determinant estimates	covariance min cov det correct covariance data	0.500000
type introduces	utils	0.009709
size	size	0.290323
point	predict estimator x y	0.045455
pool	multiprocessing	0.045455
projections	projection	0.071429
and configure a copy	estimator append random_state	0.142857
log of probability estimates	linear_model logistic regression predict log proba	1.000000
number of splitting iterations in the	base cross validator get n splits x y	0.125000
best found	model_selection base search cv predict proba x	0.076923
index of the leaf	apply	0.083333
classifier valid parameter keys can be listed	classifier set	0.125000
transform function to portion of selected features parameters	selected x transform selected copy	0.333333
convert a collection of raw	vectorizer	0.022222
compute the	multilayer perceptron compute	0.250000
as a sparse combination of the dictionary atoms	decomposition sparse coding mixin transform	0.333333
the autocorrelation parameters theta as the maximizer	gaussian process arg max	0.047619
in the wild lfw	fetch lfw	0.041667
generate cross-validated estimates for each input	x y cv	0.050000
private helper function for factorizing common classes	fit first call clf classes	0.058824
compute the	neural_network base multilayer perceptron compute	0.250000
in the context of the memory	joblib memory	0.016949
lad updates terminal regions to median estimates	least absolute error update terminal region tree	0.200000
sparse random projection matrix parameters	core base random projection fit x y	0.333333
a	utils shape	0.013699
input and compute prediction of init	boosting init decision function x	0.142857
matrices w h whose product approximates the non-	w h n_components	0.038462
linear models for feature	linear model	0.090909
of csgraph	validate graph csgraph directed	0.250000
classifier valid parameter keys	classifier set	0.125000
log odds ratio	log odds estimator	1.000000
calculates a covariance matrix shrunk on	covariance shrunk covariance emp_cov shrinkage	0.250000
and target y	base multilayer perceptron partial	0.166667
filters the given args and kwargs	filter args func ignore_lst args kwargs	0.333333
not found and raise an exception	line search	0.029412
for x using the	x y	0.002155
computes the position	mds	0.050000
user provided precisions	check precisions precisions covariance_type n_components	0.250000
x	transform x y	0.031250
the covariance	covariance	0.014493
data	base	0.014286
transform feature->value dicts to array or sparse matrix	feature_extraction dict vectorizer transform x y	0.200000
a which this function is called to	externals joblib memorized	0.013699
of feature	dict vectorizer get feature	0.200000
with the best	cv predict proba x	0.068966
find the least-squares solution to a large sparse	a	0.018182
net path with coordinate descent	linear_model enet path	0.050000
generates indices to split data into	model_selection repeated splits split	1.000000
returns the bound	dpgmmbase bound	0.333333
of edges for a	make edges	0.066667
one-vs-one multi class libsvm	one vs one	0.050000
compute the score	core score	0.166667
input data	estimator x	0.030303
computes the paired distances between x and y	paired distances x y metric	1.000000
a platform independent representation	shape	0.011765
fit an estimator within	fit estimator estimator x	0.055556
perplexity for data x	perplexity x doc_topic_distr sub_sampling	1.000000
1 iteration	x total_samples batch_update parallel	0.500000
compute the precision the precision is	metrics precision	0.033333
or sparse matrix x	x dict_type	0.200000
false for indices increasingly apart the	externals joblib verbosity filter index	0.055556
the shortest path length from source	single source shortest path length graph source cutoff	0.111111
the maximizer of	gaussian process arg max	0.047619
incremental fit on a	discrete nb partial fit x y classes sample_weight	0.166667
input data	core cross val	0.043478
time it take to	externals joblib squeeze time	0.200000
write array bytes to pickler file	joblib numpy array wrapper write array array pickler	0.333333
array corresponding to	externals joblib numpy array	0.250000
array bytes to pickler	array array pickler	0.333333
fit linear model with	fit	0.009772
of the cholesky decomposition of	log det cholesky	0.166667
1 if dtype of x and y	x y	0.002155
path	linear_model omp path	0.100000
the loss of	ensemble loss	0.166667
+ fp where tp is the number of	score y_true y_pred labels pos_label	0.027778
file_handle	file_handle	1.000000
a	externals joblib pool manager mixin	1.000000
to split data according to a	split	0.027778
initialize the model parameters	mixture base mixture initialize x	0.333333
the similarity of	score a b similarity	0.125000
store the	memorized func	0.016949
the mean and component wise scale	scale	0.033333
average path length	average path length	0.090909
isotonic regression model	core isotonic regression	0.055556
insert a	insert	0.142857
the precision the precision is the ratio tp	metrics precision	0.033333
mutual information	feature_selection mutual info regression	0.500000
count and smooth	multinomial nb count x y	0.250000
kernel is	is	0.066667
the callable case for	callable x y	0.083333
unfitted	estimators unfitted name	0.142857
evaluate a score by cross-validation read more in	cross val score estimator	0.166667
the callable case for pairwise_{distances	metrics pairwise callable	0.083333
anova f-value	feature_selection f classif	0.200000
new ndarray with aligned	aligned	0.076923
single tree	trees tree	0.142857
silhouette coefficient	metrics cluster silhouette score x	0.250000
it	memorized	0.015873
the k-neighbors of	neighbors kneighbors mixin kneighbors x n_neighbors	0.125000
the wild lfw pairs dataset this dataset	datasets fetch lfw pairs subset	0.035714
avoid	externals joblib memory	0.016949
+ fp where tp is the	score y_true y_pred labels pos_label	0.027778
to split data into training and test set	model_selection base shuffle split split x y groups	0.200000
beta	beta	0.454545
compute incremental mean and variance	utils incr mean variance	0.333333
found parameters	core base search	0.222222
with a	externals joblib with	1.000000
according to the	y sample_weight	0.035714
the data home cache	datasets clear data home	0.076923
return feature names for output features parameters	preprocessing polynomial features get feature names input_features	1.000000
in	reduce	0.017241
fit a single binary estimator	core fit binary estimator x y classes	1.000000
predictions using a single binary	core predict binary	0.200000
diagonal of the kernel k	compound kernel diag	1.000000
to split data into training and test set	kfold split x y groups	0.200000
we don't store the timestamp when pickling	memory reduce	0.030303
a new estimator with the same parameters	core clone estimator safe	0.333333
generate cross-validated estimates for each	core cross val predict estimator x y cv	0.071429
one-vs-one multi class	svm one vs one	0.050000
each input data point	val predict	0.045455
clean	clean	1.000000
generative model	decomposition base	0.076923
matrix factorization	decomposition non negative factorization x	0.043478
count and smooth	bernoulli nb count x y	0.250000
obj	obj	1.000000
remove cache folders	joblib memory	0.016949
and a name for the	get func name	0.047619
x parameters	x n_neighbors	0.500000
hash depending from it	externals joblib	0.009524
with the given arguments	memorized func get output	0.125000
is the solution to a sparse coding problem	decomposition sparse encode x	0.333333
intercept for specified layer	grad layer	0.166667
for the precision matrix	get precision	0.052632
number of splitting iterations in the cross-validator parameters	leave pgroups out get n splits	0.111111
process or thread	joblib multiprocessing	0.052632
get a signature object	externals signature	0.050000
data in the given file	data compress	0.100000
info	info	0.714286
implement a single boost	ensemble ada boost classifier boost iboost x y	1.000000
classifier from the	classifier	0.013699
model with stochastic gradient descent	x y coef_init intercept_init	0.333333
than	mode metric	1.000000
n_z	n_z	1.000000
given mapping	transform y class_mapping	0.333333
boolean mask	mask	0.142857
incremental	incremental	0.833333
returns the number of splitting iterations in	model_selection cviterable wrapper get n splits x y	0.111111
the :ref user guide <classification_report>	y_true y_pred labels target_names	0.200000
fit all transformers transform	core feature union fit transform	0.333333
estimator with the best found	core base search cv predict	0.076923
point	cross val predict estimator x	0.045455
compute area under the curve auc using	metrics auc x	0.040000
the mean squared error between	error norm comp_cov norm scaling squared	0.166667
and persist	call	0.052632
init	ensemble base gradient boosting init decision function	0.142857
random regression problem with sparse uncorrelated design	make sparse uncorrelated n_samples n_features	0.166667
for parallel processing this method is	joblib parallel	0.028571
rand index adjusted	metrics cluster adjusted rand score labels_true labels_pred	0.333333
estimate the precisions	bayesian gaussian mixture estimate precisions nk	0.166667
make	ensemble make	1.000000
indices increasingly apart	externals joblib verbosity filter index	0.055556
collect	collect	1.000000
array-like or scipy sparse matrix	binarize x threshold copy	0.083333
store	joblib memorized func	0.014706
k-neighbors of a point	kneighbors mixin kneighbors x	0.125000
boolean mask	preprocessing get mask	0.333333
regression target at each stage	ensemble gradient boosting regressor staged	0.500000
of the scaler	standard scaler	0.333333
fit ridge regression model parameters	linear_model ridge gcv fit	1.000000
float	float	1.000000
set with self	x_test y	0.142857
extracts patches of	feature_extraction extract patches	0.083333
the descriptors of a memmap instance to reopen	reduce memmap a	0.050000
perform the	x responsibilities params	0.500000
wraps a check to show a useful description	named check	1.000000
class covariance matrix	core class cov	0.250000
x for	x	0.003384
of determination r^2	multi output regressor score	0.200000
generative model	base	0.014286
affinity matrix for x using	x y	0.002155
threshold	importances threshold	0.500000
the callable case for pairwise_{distances kernels}	metrics pairwise callable x y metric	0.083333
erase the complete cache directory	memory clear warn	0.333333
minimum and maximum	min max axis x	0.500000
precision matrix	covariance get precision	0.250000
given args and	args	0.142857
derivative of the logistic sigmoid	logistic derivative z delta	0.166667
centers	centers	1.000000
initialization of	initialize x resp	0.500000
check initial parameters of the derived	base mixture check parameters	0.200000
an 'l'	shape	0.011765
compute data precision matrix with the	precision	0.016667
with the given arguments	joblib memorized func get	0.125000
building a cv in a user friendly	check cv cv	0.031250
each input data	core	0.015385
in multiplicative update nmf	multiplicative update h x	0.500000
svmlight / libsvm format into	svmlight file f	0.066667
in multiplicative update	decomposition multiplicative update w	1.000000
for reproducibility flips the sign of elements of	utils deterministic vector sign flip	0.066667
the l1	metrics paired manhattan	0.333333
x for a	x means	0.500000
for full	density full	0.166667
not found	line search	0.029412
the given parameter	parameter	0.083333
the maximizer of the	process arg max	0.047619
collision between names of	job lib collision warning	0.333333
predict class probabilities for x	classifier predict proba x	1.000000
pipeline after transforms	pipeline fit predict x y	0.166667
a given test	y_test scorer	0.333333
curve	curve y_true y_prob normalize n_bins	1.000000
apply clustering to a projection	spectral clustering affinity n_clusters n_components	0.166667
the curve auc from prediction scores	auc	0.020408
to the separating hyperplane	svm base lib svm decision function	0.333333
the shrunk covariance model according	covariance shrunk covariance fit	0.083333
initialize the model parameters of the derived class	mixture base mixture initialize x resp	0.500000
matrix	linear_model sparse	0.076923
maximizer of	process arg max	0.047619
of neighbors for points	neighbors radius neighbors	0.100000
in n_jobs	y func n_jobs	0.166667
lfw pairs dataset this operation is meant	lfw pairs index_file_path data_folder_path slice_ color	0.333333
transform binary labels back	binarizer inverse transform	0.500000
kmeans	kmeans	1.000000
computes an orthonormal matrix whose range approximates the	size n_iter power_iteration_normalizer	0.166667
that implement the partial_fit api need	utils check	0.023810
problem this dataset is described	datasets make	0.015625
on stout or stderr depending on verbosity	msg msg_args	0.200000
an array shape under python 2 the	shape repr shape	0.166667
computes pairwise similarity matrix	metrics cluster pairwise similarity a b similarity	0.500000
a cv in a user	check cv cv	0.031250
predict using the kernel ridge model parameters	core kernel ridge predict x	1.000000
externals	externals	0.028736
sure centering is not enabled	preprocessing robust scaler check array x	0.250000
parameters theta	theta	0.076923
fit estimator and transform	embedding fit transform x y sample_weight	1.000000
contingency matrix describing the relationship between labels	metrics cluster contingency matrix labels_true labels_pred eps	0.200000
validation and conversion of csgraph inputs	utils sparsetools validate graph csgraph directed dtype csr_output	0.166667
the number of splitting iterations in the cross-validator	one group out get n splits	0.111111
and cluster	and cluster	0.333333
the test_size and train_size at init	init test_size train_size	0.250000
of exception types to	base	0.014286
nearest centroid classifier	nearest centroid	1.000000
generate a grid	grid	0.040000
silhouette coefficient	metrics cluster silhouette score x labels metric sample_size	0.250000
the number of splitting iterations in the cross-validator	model_selection predefined split get n splits x	0.111111
the kddcup99 dataset downloading it	kddcup99 subset data_home download_if_missing random_state	0.111111
the best found parameters	base search cv predict proba	0.076923
the determinant	det	0.071429
of this estimator	params	0.028571
reduced likelihood function for the given autocorrelation parameters	gaussian_process gaussian process reduced likelihood function	0.047619
a calibration curve	core calibration curve y_true y_prob normalize	0.142857
regression problem this dataset is described	datasets make	0.015625
the model by computing truncated	truncated	0.100000
a random multilabel classification problem	multilabel classification	0.166667
building a cv in a user friendly way	core check cv cv x y classifier	0.031250
for full covariance matrices	full x means covars	0.166667
residues	residues	0.750000
the	joblib memorized	0.031250
and persist the output	call	0.052632
tied covariance matrix	covariances tied resp x nk	1.000000
a function	externals joblib function	1.000000
indices in sorted array of integers	neighbors find matching indices tree bin_x left_mask right_mask	0.166667
number of splitting iterations in	leave one out get n splits	0.111111
a mostly low rank matrix with	datasets make low rank matrix	0.083333
the l1 distances between the vectors in x	metrics manhattan distances x	0.500000
matrices w h whose product	x w h	0.035714
number of splitting iterations in the cross-validator parameters	leave pgroups out get n splits x	0.111111
solution to a large sparse linear	utils lsqr a	0.037037
a locally linear embedding	manifold locally linear embedding x	0.071429
for full covariance	normal density full	0.166667
gaussianrandomprojection to produce a cosine lsh fingerprint	gaussian random projection hash	0.166667
estimate the diagonal covariance vectors	mixture estimate gaussian covariances diag resp x nk	1.000000
model we can also predict based on	predict	0.006849
init	ensemble base gradient boosting init decision function x	0.142857
deviance	deviance call y	0.333333
find the	state find	0.500000
for reproducibility flips the sign of	deterministic vector sign flip	0.066667
generate indices to split data into	model_selection cviterable wrapper split	0.250000
store the timestamp when pickling	externals joblib memory reduce	0.030303
the linear assignment problem using the hungarian algorithm	utils linear assignment	0.090909
is meant to be	index_file_path data_folder_path slice_ color	0.033333
determine the	parallel backend base	0.037037
fit	core multi output estimator fit x y sample_weight	0.200000
scaling of	scaler inverse	0.250000
huber loss and the gradient	linear_model huber loss and gradient w	0.333333
estimators should be used when memory is inefficient	classes	0.025641
iterate over columns of a matrix	feature_selection iterate columns x columns	0.250000
of exception types to	backend	0.016949
display	joblib parallel print	0.166667
class covariance matrix	core class cov x	0.250000
a collection of text documents	vectorizer	0.022222
multiple files in	files files n_features dtype	0.500000
pairwise matrix in	parallel pairwise x y func	0.166667
sample_weight	x y sample_weight	0.012987
to build a batch of	ensemble parallel build	0.047619
compute log probabilities within a	parallel predict log proba	0.058824
the number of splitting iterations in the	leave one out get n splits x	0.111111
data to vectors and cluster	and cluster data vectors n_clusters	0.333333
utility for building a cv in	core check cv cv x y classifier	0.031250
introduces an 'l' suffix when using	shape	0.011765
log-det of	matrix_chol	0.125000
apply transforms and score	score x y sample_weight	0.250000
retrieve the leaves	leaves	0.071429
a random sample from a	a size replace	0.142857
mask to edges weighted	mask edges weights mask edges weights	0.333333
of exception	joblib parallel backend	0.045455
predict using the trained model parameters	neural_network base multilayer perceptron predict x	0.333333
return the directory in	get output dir	0.047619
x	fit x y iter_offset	0.333333
return a tolerance which is independent	cluster tolerance	0.058824
embedding analysis on the	embedding x n_neighbors	0.200000
list of regularization parameters	x y pos_class cs	0.166667
sparse uncorrelated design	sparse uncorrelated	0.166667
x and	fit transform x y	0.250000
inplace column scaling of a csc/csr matrix	utils inplace column scale x	0.166667
non-negative matrix factorization	negative factorization	0.043478
the kddcup99 dataset downloading it if necessary	datasets fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
list of	base	0.014286
explained variance regression score	metrics explained variance	0.166667
store the timestamp when pickling to avoid	externals joblib memorized func reduce	0.050000
to a large sparse linear	a	0.018182
graph-lasso objective function the objective	covariance objective	0.125000
call wrapped function cache result	joblib memorized func call	0.200000
that for c in	c x	0.030303
x and	decomposition beta divergence x	0.250000
for c in (l1_min_c infinity)	c x y	0.030303
multiple files in svmlight format this function	svmlight files files	0.200000
feature names ordered by their	feature_extraction dict vectorizer get feature names	0.142857
rand index adjusted	cluster adjusted rand score	0.333333
write the	joblib write	0.500000
gaussian	core gaussian	1.000000
of a csc/csr matrix in-place	utils inplace swap column	0.250000
step length is not found and	search	0.019231
generate indices to split data into	time series split split	0.250000
on the estimator with the best found parameters	base search cv predict proba	0.076923
to unit	axis copy	0.166667
compute data precision matrix with	get precision	0.052632
the average log-likelihood	decomposition factor analysis score	0.333333
a cross-validated score with permutations	core permutation test score estimator x y cv	0.166667
test_size and train_size at init	model_selection validate shuffle split init test_size train_size	0.250000
cross-validated estimates for each	cv	0.009009
returns the number of splitting iterations in	predefined split get n splits x y groups	0.111111
mcd	mcd	1.000000
can actually run in parallel	parallel backend	0.030303
covariance determinant matrix	covariance fast mcd x support_fraction cov_computation_method	0.250000
"returns the mean accuracy	multi output classifier score x	0.250000
the other and transforms	y	0.002674
make	ensemble base ensemble make estimator	1.000000
computes the free energy f	rbm free energy	0.066667
update the dense dictionary factor	update dict dictionary	0.333333
used to build a batch of estimators within	parallel build estimators n_estimators ensemble	0.166667
set the parameters	core pipeline set	0.250000
sparse uncorrelated design	make sparse uncorrelated	0.166667
nothing and	feature_extraction	0.037037
based on	from	0.045455
matrix to	linear_model sparse coef	0.076923
a collection of	vectorizer	0.044444
with the	pca	0.047619
jaccard similarity coefficient score the	score y_true y_pred	0.038462
partition	ensemble partition	0.200000
function for factorizing common	first call clf	0.200000
f-beta score is the weighted harmonic mean of	metrics fbeta score y_true y_pred beta	0.333333
lrd of a sample is the inverse of	distances_x neighbors_indices	0.047619
the rfe model and	x y	0.002155
metadata	metadata	1.000000
to fit an estimator within a	fit estimator estimator x	0.055556
we don't	joblib memory	0.016949
init	base gradient boosting init decision function x	0.142857
write array bytes	array wrapper write array array	0.500000
estimates	cross	0.037037
the number of splitting iterations in	out get n splits x	0.111111
precisions parameters of the precision	precisions nk xk	0.166667
cf	birch	0.125000
maximum	max axis x	0.500000
assumes	transform	0.011236
get the directory corresponding to the cache for	externals joblib memorized func get func dir mkdir	0.500000
score on the given data if	score x	0.033333
length of an unsuccessful bst search since	length n_samples_leaf	0.500000
the estimator with the best	cv predict proba x	0.068966
private function used to partition estimators between jobs	partition estimators n_estimators n_jobs	0.200000
create subset of dataset and	y	0.002674
determine absolute sizes	translate train sizes	0.066667
single boost using	ada boost classifier boost	0.100000
on an array list sparse matrix or similar	check array array accept_sparse dtype order	0.500000
pairwise matrix in n_jobs	metrics parallel pairwise x y func n_jobs	0.111111
the wild lfw pairs dataset this dataset is	datasets fetch lfw pairs subset	0.035714
delayed	delayed	1.000000
normalize x by scaling rows and columns independently	cluster scale normalize x	0.142857
data if the estimator has been refit	core base search cv	0.033333
sparse matrix	accept_sparse dtype	0.200000
lars using bic or aic for model selection	lars ic	0.250000
the position of the	mds fit x y	0.066667
k x y and	gaussian_process rbf call x y	0.200000
an array of test vectors x	x	0.001692
binary classification used in hastie et al	make hastie 10 2 n_samples random_state	0.166667
values for x relative	predict scorer call estimator x	0.166667
the precision matrix	get precision	0.052632
cross-validated	predict estimator x y cv	0.071429
break the pairwise matrix	metrics parallel pairwise	0.166667
out of bag predictions and score	ensemble base bagging set oob score x y	0.250000
the	mixture bayesian gaussian mixture	0.333333
folder	utils delete folder	1.000000
the neighbors within a	neighbors lshforest radius neighbors	0.166667
logistic regression aka logit maxent classifier	logistic regression	0.200000
bin_x	bin_x	1.000000
quantile this classification dataset is constructed	datasets make	0.015625
the data	y	0.002674
transforms features by scaling each	preprocessing minmax scale x feature_range axis copy	0.200000
the number of splitting iterations in the	one out get n splits x	0.111111
for the california housing dataset from statlib	california housing	0.083333
to	sparse coef	0.071429
used to hash	hash	0.083333
by scaling	scale	0.033333
x with ability to accept precomputed doc_topic_distr	precomp distr x doc_topic_distr sub_sampling	0.500000
the largest k singular values/vectors for a	utils svds a k ncv tol	0.166667
fit_predict of last step in pipeline after transforms	core pipeline fit predict	0.166667
python object into	dump value filename	0.083333
callable that handles preprocessing and tokenization	vectorizer mixin build analyzer	0.333333
this operation is meant	data_folder_path slice_ color resize	0.033333
perform a locally linear embedding analysis on	locally linear embedding x n_neighbors n_components	0.071429
print verbose	print verbose	1.000000
the search	search	0.019231
build a contingency matrix describing the relationship	cluster contingency matrix	0.333333
implementation is restricted to the binary classification	y_true y_score average sample_weight	0.076923
template method for updating terminal	update terminal	0.142857
select features	select	0.100000
based on	x connectivity n_clusters	0.250000
estimate model	fit	0.009772
on a given	scorer	0.090909
solution to a large sparse linear system	a	0.018182
thread pool	externals joblib multiprocessing	0.052632
text report showing the	report	0.047619
if the given estimator	estimator	0.014706
scale back the data to the	inverse transform x	0.051282
initialize the model parameters	base mixture initialize x	0.333333
evaluates the reduced likelihood function for	gaussian_process gaussian process reduced likelihood function	0.047619
true and false positives per binary classification	binary clf curve y_true y_score pos_label	0.090909
estimates the minimum covariance determinant matrix	covariance fast mcd x	0.250000
fit estimator using ransac algorithm	linear_model ransacregressor fit	1.000000
split data into	base shuffle split split x	0.250000
and variance along an axix on a csr	variance axis x axis last_mean last_var	0.142857
return the active default	externals joblib get active	1.000000
check that predict raises an exception	utils check estimators	0.142857
precisions	precisions	0.466667
handle the callable case	callable x y	0.083333
the directory	get output dir	0.047619
elastic net path with	linear_model enet path	0.050000
classification	y_true y_pred	0.037037
and return the breast cancer wisconsin	breast cancer return_x_y	0.250000
rcv1 multilabel dataset	datasets fetch rcv1	0.333333
onto the	x ridge_alpha	0.071429
model parameters with the em algorithm	mixture gmmbase fit x	0.250000
column scaling of	column	0.083333
clear all covered matrix cells	state clear covers	1.000000
of the unpickler	unpickler	0.090909
n_jobs	y func n_jobs	0.166667
generates a random sample from	size replace p	0.125000
calculate the posterior log probability	bernoulli nb joint log likelihood	0.083333
dataset along any axis center to the	x axis	0.030769
display the message	joblib parallel print	0.166667
parameters for the voting classifier valid	ensemble voting classifier	0.031250
optimization function varies for mono and	y	0.002674
the maximum likelihood covariance estimator parameters	covariance empirical covariance	0.071429
make cache size fit	joblib memory reduce size	0.083333
default backend used by parallel inside a with	externals joblib parallel backend backend n_jobs	1.000000
computes the weighted graph of neighbors for	neighbors radius neighbors graph	0.066667
center	robust	0.090909
derived	x resp	0.166667
pairwise matrix	metrics parallel pairwise	0.166667
the right fileobject from a	joblib read fileobject fileobj	0.100000
returns the number of splitting iterations in the	model_selection leave one out get n splits	0.111111
average of the decision functions of the base	decision function x	0.018868
by computing truncated svd	truncated x n_components	0.200000
usual api and	fit x y	0.017964
samples from a gaussian distribution	sample gaussian mean covar covariance_type	1.000000
full	normal density full x	0.166667
collection of raw	vectorizer	0.022222
pairs dataset this operation is meant to be	pairs index_file_path data_folder_path slice_ color	0.333333
returns the number of splitting iterations in	cross validator get n splits x y	0.125000
with the best found	base search cv predict proba	0.076923
write	array wrapper write	1.000000
the labeled faces in the wild lfw pairs	datasets fetch lfw pairs	0.018868
process or	externals	0.005747
to build from the c and cpp files	build from c and cpp files	0.500000
binary classification	y_true y_score average	0.076923
and score with the final estimator parameters	core pipeline score x y	0.500000
return the decision path	decision path	0.333333
the shortest path	single source shortest path	0.333333
quantile this classification dataset is	datasets make	0.015625
block diagonal structure for biclustering	biclusters shape n_clusters noise minval	0.058824
the neighbors within a given radius of	neighbors x radius	0.142857
suffix when	utils shape repr	0.013699
data point	estimator	0.014706
precision matrix with the	get precision	0.052632
format check	latent dirichlet allocation check	0.062500
range	utils randomized range finder	0.083333
estimate mutual information	feature_selection mutual info classif x	0.500000
desired_perplexity	desired_perplexity	1.000000
the search over parameters	base search	0.100000
of the parallel	parallel	0.019231
to the file	binary zlib file	0.062500
back the data	preprocessing robust scaler inverse transform x	0.066667
compute minimum distances between one point and	metrics pairwise distances argmin x y axis	0.500000
an 'l'	utils shape	0.013699
loading for the lfw people dataset	datasets fetch lfw people	0.040000
the number of splitting iterations in the	model_selection predefined split get n splits x y	0.111111
cross-validated estimates for each input data point	estimator x y cv	0.050000
full covariance matrices	multivariate normal density full	0.166667
weighted graph of	graph	0.085106
argument checking for random matrix generation	check input size n_components n_features	0.200000
cf tree for	cluster birch	0.090909
a temporary folder if still existing	delete folder folder_path	0.250000
negative value in an array	negative x	0.200000
logic estimators that	utils check partial fit	0.038462
on the estimator with the best found	search cv predict proba	0.076923
are going to run in parallel	externals joblib multiprocessing	0.052632
kernel k	gaussian_process white kernel	0.333333
the laplacian kernel	metrics laplacian kernel	0.166667
apply clustering to	spectral clustering affinity n_clusters n_components	0.166667
a locally linear embedding analysis on	manifold locally linear embedding x n_neighbors n_components reg	0.071429
an array	quadratic discriminant	0.500000
constructs signature from the	externals signature	0.050000
the estimator with the best	cv	0.045045
mean silhouette coefficient	metrics cluster silhouette score	0.250000
columns of a	columns x columns	0.250000
that implement the partial_fit	utils check	0.023810
run fit with all	grid search cv fit x	0.333333
lfw pairs dataset	lfw pairs	0.037736
the submatrix corresponding to	get submatrix	0.166667
across axis 0	axis 0 x	1.000000
estimates for each	core cross val predict estimator	0.045455
net model with iterative fitting along a regularization	net cv	0.333333
gaussian random	gaussian random	1.000000
case method='lasso' is :	x y xy gram	0.090909
callable case for	pairwise callable x y	0.083333
c such that for c in	c x y loss fit_intercept	0.030303
graph of k-neighbors for	kneighbors mixin kneighbors graph	0.250000
the log probability under the model	mixture gmmbase score	0.500000
l1-penalized covariance estimator read more in	covariance graph lasso	0.166667
of exception	externals joblib parallel backend base get	0.066667
reconstruct the	externals joblib ndarray wrapper read unpickler	0.333333
input validation for standard	check x y x y accept_sparse dtype	0.250000
fit	linear svr fit	0.333333
jobs for	jobs n_jobs	0.100000
w h whose product approximates the	x w h	0.035714
under the curve auc using the	metrics auc	0.040000
returns the huber loss and the gradient	huber loss and gradient	0.333333
x from	x z	0.050000
terminal regions to median estimates	terminal region tree terminal_regions	0.100000
ransac random sample consensus algorithm	ransacregressor	0.166667
used for later scaling	scaler fit	0.153846
exception types to	parallel	0.019231
graph	img to graph	0.333333
non-negative matrix factorization nmf find two non-negative matrices	negative factorization x	0.043478
cross-validated estimates	val predict estimator x y cv	0.071429
diagonal of the laplacian matrix and convert	diag laplacian	0.111111
the index of the leaf	tree base decision tree apply	0.166667
or thread	backend	0.016949
write array bytes to pickler file handle	array wrapper write array array pickler	0.333333
metric_params	metric_params	1.000000
aggressive classifier read more in	aggressive classifier	0.166667
predict class labels for samples in	linear classifier mixin predict	1.000000
the density model on	density	0.043478
trace of np dot x y t	decomposition trace dot x y	0.500000
rest	rest	1.000000
transform it into tf-idf vectors	vectorized subset remove data_home	1.000000
perform a locally linear embedding analysis	manifold locally linear embedding x n_neighbors	0.071429
func	mixin apply async func	0.250000
fit for	fit	0.003257
error regression loss	error	0.060000
classification task	precision recall curve y_true	0.142857
standardize a dataset	with_mean with_std	0.200000
shortest path length from source to	single source shortest path length graph source	0.111111
number of splitting iterations in the cross-validator	one out get n splits x y groups	0.111111
of a function	externals joblib delayed function	0.200000
n_jobs even	x y func n_jobs	0.166667
nugget	nugget theta d	1.000000
factorization nmf find	factorization x	0.043478
for the models computed by 'path' parameters	linear_model path residuals x y train	0.250000
y t	y	0.002674
absolute sizes of training subsets	translate train sizes	0.066667
apply the derivative of the logistic sigmoid	neural_network inplace logistic derivative z delta	0.166667
for c in (l1_min_c infinity) the	c x y loss	0.030303
density lrd the	density	0.043478
the lfw	datasets fetch lfw	0.083333
scale back the	standard scaler inverse transform x	0.066667
make and configure a copy of the base_estimator_	base ensemble make estimator append random_state	0.166667
outlyingness of	covariance outlier detection mixin predict	0.250000
estimates	y	0.002674
columns of a	columns	0.111111
effective_rank	effective_rank	1.000000
corresponding	mkdir	0.125000
reconstruct the image	reconstruct	0.125000
the one-vs-one multi class	svm one vs one	0.050000
problem this dataset is	datasets	0.015152
normalize x	scale normalize x	0.142857
return a platform	utils	0.009709
"returns	multi output classifier score x	0.250000
samples	predict	0.006849
to update terminal regions	update terminal regions	0.500000
compute out-of-bag scores	ensemble forest regressor set oob score x y	1.000000
graph of neighbors for points	neighbors mixin radius neighbors graph	0.066667
a cv	check cv cv x y	0.031250
undo the scaling	min max scaler inverse transform	0.500000
and return the	externals joblib	0.019048
precision vector	precision positivity precision	0.250000
set x and returns the	predict x y	0.043478
change the default backend	backend backend n_jobs	0.333333
described in [rouseeuw1984]_ aiming at computing	covariance c step x n_support remaining_iterations initial_estimates	0.111111
element of numpy	element	0.083333
metrics read	metrics	0.043478
backend and return the number of	externals joblib parallel backend	0.029412
the process or	externals joblib	0.004762
for full covariance matrices	normal density full x	0.166667
passive aggressive classifier read more in the :ref	passive aggressive classifier	0.125000
the logistic loss	linear_model logistic loss w	0.500000
fit the	svm linear svr fit x y	0.333333
the shrunk covariance model according to the	covariance shrunk covariance fit	0.083333
the least-squares solution to a large sparse	lsqr a	0.037037
compute data	pca get	0.076923
predict is	compute labels predict	0.250000
embedding read more in the :ref	embedding	0.040000
to	spectral	0.052632
binarize labels	preprocessing label binarize	0.333333
of the laplacian	laplacian	0.034483
check the validity of the	check params x metric p metric_params	0.200000
with respect to each parameter weights	activations deltas	0.032258
the directory in which are	get output dir	0.047619
quantiles to be used for scaling	preprocessing robust scaler	0.500000
to compute decisions within a job	estimators_features	0.071429
classifier on x and y	x y	0.002155
independent representation of	utils shape	0.013699
indices	neighbors find matching indices	0.250000
cv	cv cv	0.031250
matrix product with the random matrix	random projection transform	0.333333
mixin class for all transformers in scikit-learn	transformer mixin	0.500000
store the timestamp when pickling to	joblib memory reduce	0.030303
clustering on the data	cluster	0.021277
precision is the ratio	precision	0.016667
multi-class labels	threshold	0.076923
factorization nmf find	negative factorization x	0.043478
locally linear embedding analysis on the	manifold locally linear embedding x n_neighbors n_components	0.071429
for building a cv in a user	cv cv	0.031250
of edges for	feature_extraction make edges	0.066667
get a list of	utils	0.009709
matrix	inplace	0.166667
we can also predict based on	predict	0.006849
estimates for each input	predict	0.006849
null space	manifold null space	0.333333
sign of elements of all	sign	0.050000
utility function opening the right fileobject from	joblib read fileobject	0.100000
least squares solver	core linear discriminant analysis solve lsqr x y	1.000000
turn a transformed real-valued array into	projection to	0.166667
log	predict log	0.500000
point	estimator	0.014706
non zero row of x	transform x y copy	0.142857
data home	clear data home data_home	0.076923
back the	scaler inverse transform x copy	0.066667
input	core cross val predict estimator x	0.045455
the estimator and set the base_estimator_ attribute	ensemble ada boost classifier validate estimator	0.333333
returns the index of the leaf	base decision tree apply x	0.166667
learn vocabulary and idf	fit transform raw_documents y	0.100000
data set with self	x_test	0.083333
actual data loading for the lfw people dataset	fetch lfw people	0.040000
search	base search cv fit	0.166667
terminal regions to median	terminal region tree terminal_regions	0.100000
write array bytes to pickler	numpy array wrapper write array array pickler	0.333333
count and smooth feature	count x y	0.250000
each input data point	val predict estimator x y	0.045455
depth a which this function is called	externals joblib memorized func check previous func code	0.055556
a mask to edges weighted	mask edges weights mask edges weights	0.333333
generates integer indices corresponding to test sets	test indices	0.333333
list	base	0.014286
log-det of the cholesky decomposition	log det cholesky matrix_chol	0.500000
score by cross-validation read more in the	cross val score estimator x y	0.166667
full	full x means covars min_covar	0.166667
loader for the california housing dataset from	datasets fetch california housing data_home download_if_missing	0.250000
to the binary classification task	recall curve y_true	0.142857
a random regression problem with sparse uncorrelated design	sparse uncorrelated n_samples n_features random_state	0.166667
return the kernel k x y and	gaussian_process rbf call x y	0.200000
handle the callable case for pairwise_{distances kernels}	metrics pairwise callable x y metric	0.083333
regularization parameters	y pos_class cs	0.166667
oracle approximating shrinkage covariance model according to	covariance oas fit x	0.083333
hopefully pretty robust repr equivalent	repr value	1.000000
delete all the content of the data	data	0.038462
the cache	externals joblib memorized	0.013699
neighbors	neighbors radius neighbors	0.100000
descent	descent	1.000000
compute the decision function of the given observations	covariance outlier detection mixin decision function x	0.333333
'l' suffix when using the	shape repr	0.013699
outlier on the training	outlier factor fit	0.200000
fit linear model	linear_model linear regression fit	1.000000
cluster is worthy enough to be merged if	cluster cfsubcluster merge subcluster nominee_cluster threshold	1.000000
the first prime element in the specified	prime in	0.166667
inverse the	agglomeration transform inverse	0.500000
check that the	gaussian mixture check	1.000000
and properly handle kernels	safe split estimator x y indices	0.200000
d	d	0.833333
free energy f v = -	neural_network bernoulli rbm free energy	0.066667
and compute prediction of init	boosting init decision	0.142857
target values for x relative	scorer call estimator x	0.166667
number of splitting iterations in the cross-validator	get n splits x y	0.111111
can also predict based on an	predict x	0.011765
oracle approximating shrinkage covariance model according to	covariance oas fit	0.083333
run in parallel n_jobs	n_jobs	0.023256
the free energy f v =	free energy	0.066667
python object into one file	dump value filename	0.083333
of the local outlier	local outlier	0.250000
voting classifier valid parameter keys can be listed	ensemble voting classifier	0.031250
each input data point	val predict estimator	0.045455
the directory	output dir	0.047619
process or thread pool and return the	externals joblib multiprocessing backend	0.035714
generates data for binary classification used in hastie	datasets make hastie	0.125000
voting classifier valid parameter keys can	ensemble voting classifier set	0.037037
computation of min	min	0.045455
tolerance which	cluster tolerance x	0.058824
lad updates terminal regions to	least absolute error update terminal	0.200000
the estimator with the best found parameters	search cv predict proba x	0.076923
argument checking for random matrix generation	core check input size n_components n_features	0.200000
format well suited for eigenvalue decomposition	value norm_laplacian	0.142857
function opening the right fileobject from a filename	read fileobject fileobj filename	0.250000
estimates for each	core	0.015385
sparse components	decomposition sparse pca	0.500000
sure centering is not enabled for sparse matrices	preprocessing robust scaler check array	0.250000
convert coefficient matrix	linear_model sparse	0.076923
approximation of the breakdown point	breakdown point n_samples n_subsamples	1.000000
california housing	fetch california housing	0.083333
of	externals joblib parallel backend base	0.034483
building a cv in	core check cv cv x	0.031250
estimator with the best	cv predict proba	0.068966
model	fit x y sample_weight	0.020000
fit	regression cv fit x y	1.000000
perform classification on an array of test vectors	neighbors nearest centroid predict	0.142857
on the estimator with the best	cv predict	0.083333
already fitted lsh forest	neighbors lshforest partial fit	0.200000
hash depending from it	joblib memorized func	0.014706
a which this	externals joblib memorized func	0.013158
transforms one after the other and	y	0.002674
metrics read more	metrics	0.043478
cv in a	check cv cv x y	0.031250
the function called with the given arguments	func get	0.100000
model	base randomized linear model	0.500000
huber loss	linear_model huber loss	0.333333
unnormalized posterior log probability of x	nb joint log likelihood x	0.111111
transform with	transform x	0.016949
fit linear model with passive aggressive algorithm	passive aggressive classifier fit	1.000000
update h in multiplicative update	multiplicative update h x w h beta_loss	0.250000
the variational lower bound for	mixture dpgmmbase bound	0.166667
input data	val predict estimator	0.045455
score	score x	0.166667
build a batch of estimators within a	parallel build estimators	0.166667
isomap embedding non-linear dimensionality reduction through isometric mapping	isomap	0.166667
depending from it	joblib memorized	0.015625
integer indices corresponding to test sets	partition iterator iter test indices	0.333333
the sign of vectors for reproducibility flips	utils deterministic vector	0.076923
evaluate decision function output for x	x	0.001692
input vectors individually to unit norm vector length	preprocessing normalize x norm axis copy	0.200000
regression target at each stage	boosting regressor staged	0.500000
the gradient and the	w x y	0.133333
cosine similarity	cosine similarity	1.000000
the score	score x y	0.060606
l1	l1	1.000000
binary classification task	score y_true y_score average	0.076923
standard	standard	0.833333
estimate class weights	compute class	0.166667
actual data loading for the lfw pairs	lfw pairs	0.018868
according to a percentile of the highest scores	percentile	0.100000
print verbose message on the end of iteration	base mixture print verbose msg init end ll	0.333333
svmlight / libsvm format into sparse	svmlight file f	0.066667
the california housing dataset from	california housing	0.083333
compute labels and inertia using a	labels inertia precompute dense x x_squared_norms centers	0.250000
when using	shape repr	0.013699
the optimization objective for the case method='lasso' is	y xy gram	0.090909
the number of splitting iterations in the cross-validator	get n splits x	0.111111
boosted classifier/regressor from the training	ensemble base weight boosting fit	0.500000
decision functions of	decision function x	0.018868
models for feature selection this implements the	model	0.058824
for elastic net parameter search parameters	l1_ratio	0.030303
generate	core base shuffle split iter	0.166667
indices to split data into	model_selection time series split split x	0.250000
a cv in a user	core check cv cv x	0.031250
loader for the labeled	data_home	0.055556
nugget	nugget theta	1.000000
the position of the points	mds	0.050000
this function returns posterior probabilities of classification	calibrated classifier cv predict proba	0.200000
long type introduces an	shape repr	0.013699
compute the k-way softmax function inplace	neural_network softmax x	1.000000
a file object providing transparent zlib de compression	binary zlib file	0.062500
trained model	base multilayer perceptron	0.142857
to fit an estimator within	fit estimator estimator x	0.055556
log-det of the cholesky decomposition of matrices	cholesky matrix_chol covariance_type n_features	1.000000
the model to the	neural_network bernoulli rbm	0.333333
for the models computed by 'path' parameters	linear_model path residuals x y train test	0.250000
dirichlet	dirichlet	1.000000
the huber loss	linear_model huber loss	0.333333
disk usage in a	disk used path	0.250000
process classification gpc based on laplace approximation	process classifier	0.500000
checker utility for building a cv	check cv cv x y	0.031250
solve the isotonic regression model : min	core isotonic regression	0.055556
and hence	decomposition sparse coder fit x y	0.142857
classification dataset is constructed	datasets make	0.015625
in hastie et al	make hastie 10 2 n_samples random_state	0.166667
the submatrix corresponding to bicluster i	bicluster mixin get submatrix i data	0.333333
compute mean and variance along an axix	utils mean variance axis x axis	0.142857
a tolerance which is	cluster tolerance x	0.058824
raises an exception in an unfitted	unfitted name	0.142857
reconstruct a python object from a file persisted	load filename mmap_mode	0.333333
probabilities for	proba	0.117647
on x and	fit predict x y sample_weight	0.333333
linear models for	linear model	0.090909
for mono and	y	0.002674
call with the given arguments	format call	0.200000
bound term related to proportions	bound proportions z	0.333333
evaluate the density model on the	neighbors kernel density score samples	0.250000
exception types to	joblib parallel backend base	0.058824
by scaling each	minmax scale	0.142857
estimates for	cross val predict estimator x y	0.045455
just there to implement the usual api and	x y	0.006466
process or	backend	0.016949
opening the right fileobject from a	joblib read fileobject fileobj	0.100000
multiclass	multiclass y	0.500000
for parallel processing this	externals joblib parallel	0.014085
maximum along an axis on a csr or	max axis x axis	0.142857
predict class probabilities for x	ensemble ada boost classifier predict proba x	1.000000
sparse random matrix	random	0.058824
a collection of raw documents	vectorizer	0.022222
transformed real-valued array into a hash	projection to hash mixin	0.333333
compute joint	joint	0.285714
the reduced likelihood function for the	gaussian process reduced likelihood function	0.047619
the hash depending from	joblib memorized func	0.014706
sample	sample	0.290323
opposite of the local outlier factor of	local outlier factor decision function	0.125000
pairwise matrix in n_jobs	parallel pairwise x y func n_jobs	0.111111
process or	externals joblib multiprocessing backend	0.035714
with	base pca get	0.076923
of x (as bigger is better	function x	0.030303
with sparse uncorrelated design this dataset is	datasets make sparse uncorrelated	0.166667
the density model on	neighbors kernel density	0.090909
x for a	x means covars	0.500000
e-step in em update	latent dirichlet allocation e step	1.000000
data and labels	x y	0.002155
predict based on an unfitted	predict	0.006849
the decision function of	ensemble ada boost classifier decision function	0.166667
x and y	x y dense_output	0.333333
is not found and raise an exception if	search	0.019231
classification on	base svc	0.500000
used to fit a single tree	build trees tree	0.142857
of a multinomial	linear_model multinomial	0.200000
compute the precision the precision is the	precision	0.016667
the test_size and train_size at init	validate shuffle split init test_size train_size	0.250000
variance along an axix on	variance axis x axis last_mean last_var	0.142857
clustering on	fit predict	0.055556
the dual gap convergence criterion	covariance dual gap emp_cov	0.071429
base class for hyper parameter search with cross-validation	base search cv	0.026316
compute binary	binary	0.031250
leave-one-label_out cross-validation	leave one label out	0.500000
fit the ardregression model according	linear_model ardregression fit	0.250000
to split data into	model_selection base kfold split x	0.250000
w to minimize the	w ht l1_reg	0.250000
function used to fit a single tree	trees tree forest	0.142857
fit linear model	linear_model linear regression fit x y sample_weight	1.000000
curve auc	auc score	0.052632
of u such	u	0.032258
the weighted graph of neighbors	neighbors radius neighbors graph	0.066667
x (as bigger is better	function x	0.030303
back the data to	preprocessing standard scaler inverse transform	0.066667
node and update it with the split	cluster cfnode update split	1.000000
decision function of	ada boost classifier decision function	0.166667
fit an estimator within a job	ensemble parallel fit estimator estimator x	0.333333
for each input data point	x	0.001692
it	externals joblib memorized	0.013699
average log-likelihood	decomposition factor analysis score x	0.333333
inplace column	inplace column scale x scale	0.166667
cache result and return a	joblib memorized	0.015625
parallel execution only a fraction of	externals joblib parallel	0.014085
kernel k x y and optionally its gradient	gaussian_process constant kernel call x y eval_gradient	1.000000
memory is inefficient to train all	y classes	0.027778
validation and conversion of csgraph inputs	sparsetools validate graph csgraph directed dtype csr_output	0.166667
precisions parameters of the	precisions nk xk	0.166667
trees	tree	0.071429
train/test indices to split time series data samples	time series split	0.250000
kappa	kappa	0.666667
em algorithm and return the	gmmbase	0.062500
a byte	externals joblib binary zlib	0.125000
covars	covars	1.000000
in sorted array of integers	tree bin_x left_mask right_mask	0.166667
and scale	x y	0.002155
multi-output variable using a	core multi output estimator	0.142857
exceptions	assert raise message exceptions	1.000000
construct a featureunion from	make union	0.250000
elastic net path with coordinate descent the	enet path	0.050000
weighted graph of neighbors for	neighbors radius neighbors graph	0.066667
compute incremental mean and variance	mean variance	0.166667
validation	base lib svm validate	0.500000
fits the oracle approximating shrinkage covariance	covariance oas fit	0.083333
graph of neighbors for	neighbors radius neighbors mixin radius neighbors graph	0.066667
corresponding	numpy	0.083333
the residual (= negative gradient)	ensemble binomial deviance negative gradient	0.333333
for indices increasingly apart the	verbosity filter index	0.055556
temporary folder if still existing	utils delete folder folder_path warn	0.250000
fit the	svc fit	0.333333
mean and variance	utils incr mean variance	0.333333
path	linear_model enet path x	0.050000
to split data into	predefined split split x	0.250000
the	covariance get	0.166667
probability calibration with sigmoid method platt 2000	core sigmoid calibration df y	0.500000
verbosity	verbosity	1.000000
predict new data by linear interpolation	core sigmoid calibration predict	1.000000
local	local	0.636364
each input data point	predict	0.006849
of edges for	make edges	0.066667
split data into	time series split split x	0.250000
fit all transformers transform the	feature union fit transform	0.333333
a collection of text documents to a	vectorizer	0.022222
input	core cross val predict	0.045455
predict class probabilities	ensemble forest classifier predict proba	0.500000
for each input	core cross val	0.043478
the depth a which this	externals joblib memorized func check previous func code	0.055556
the log-likelihood of a gaussian	covariance score	0.071429
a class with	add	0.071429
to assert that	check	0.017857
array corresponding to this	externals joblib numpy array	0.250000
generate	base shuffle	0.166667
by scaling each feature	minmax scale x	0.142857
build a batch of	build	0.037037
apply clustering to	clustering	0.050000
sure that an estimator implements the	core check estimator estimator	0.142857
solution to a sparse coding problem	decomposition sparse encode x dictionary gram	0.333333
according to a percentile of the	percentile	0.100000
that for c in (l1_min_c infinity) the	c x	0.030303
binary gaussian process classification	binary gaussian process	0.333333
the search	core base search	0.111111
generates data for binary classification used in hastie	make hastie	0.125000
fit a binary classifier on x and y	linear_model base sgdclassifier fit binary x y	1.000000
used to partition estimators between jobs	partition estimators n_estimators n_jobs	0.200000
the effective stop words list	feature_extraction vectorizer mixin get stop words	0.200000
skip	skip	0.714286
binarize labels in a	preprocessing label binarize	0.333333
with sparse uncorrelated design	make sparse uncorrelated n_samples n_features	0.166667
thresholding of array-like or scipy	preprocessing binarize	0.083333
np dot x y	dot x y	0.250000
a transform	transform selected x transform	0.333333
samples	decision function	0.025000
the maximizer of	process arg max	0.047619
parameters	core	0.123077
as the maximizer	gaussian_process gaussian process arg max	0.047619
the absolute error of the kl divergence of	manifold kl divergence error	0.100000
reproducibility flips the sign of elements of	utils deterministic vector sign	0.066667
squares	squares	1.000000
loss and class	loss w x y	0.250000
the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf	0.125000
binarization transformation for multiclass	binarize multiclass	1.000000
of determination regression score function	metrics r2 score y_true y_pred	0.125000
stratified shufflesplit cross-validator provides train/test indices to	stratified shuffle	0.333333
full covariance matrices	multivariate normal density full x means covars	0.166667
of exception types	externals joblib parallel backend base get	0.066667
the largest k singular values/vectors for	k ncv tol	0.166667
exception	parallel backend	0.030303
apply dimensionality reduction on x	decomposition randomized pca transform x y	1.000000
param logic estimators that implement	utils check partial	0.038462
error regression	error	0.060000
error of the kl divergence of p_ijs	manifold kl divergence error	0.100000
rand index adjusted for chance	cluster adjusted rand score labels_true	0.333333
estimates for	estimator	0.014706
used to fit a single tree in parallel	ensemble parallel build trees tree forest x y	0.200000
predict_log_proba	predict log proba	0.058824
to build a batch of estimators within a	build estimators n_estimators ensemble x	0.166667
estimates for	val	0.037037
and maximum along an axis on	max axis x axis	0.142857
multiple files in svmlight format this function is	svmlight files files	0.200000
fit label encoder and return encoded labels	label encoder fit transform y	0.200000
getter for	covariance empirical covariance get	0.166667
for validation	directed dtype	0.166667
of determination regression score function	r2 score y_true y_pred sample_weight	0.125000
determine absolute sizes of training subsets and	translate train sizes	0.066667
predict is	predict	0.006849
estimates	cross val predict estimator	0.045455
weights and	x y	0.002155
all the covariance matrices from a given template	match covariance type tied_cv covariance_type n_components	0.333333
safe	safe	1.000000
outlier on the training set according to the	outlier factor fit	0.200000
is the solution to a sparse coding problem	decomposition sparse encode	0.333333
logic estimators that implement the partial_fit api need	utils check	0.023810
matrix of patch data	patch extractor transform	0.200000
labels the output of transform is	transform y	0.023256
by scaling each feature to a	preprocessing minmax scale	0.142857
score	score y_true y_pred sample_weight	0.062500
dataset and	x y	0.002155
the lfw pairs dataset this	datasets fetch lfw pairs	0.018868
exception types to	externals joblib parallel backend base get	0.066667
compute non-negative matrix factorization nmf find two non-negative	factorization	0.035714
parameters of this	params deep	0.222222
thread pool	joblib multiprocessing backend	0.052632
parallelbackend must implement	backend base	0.032258
calculate true and false positives per binary	metrics binary clf curve y_true	0.090909
under the curve auc from	auc	0.020408
autocorrelation parameters theta	theta	0.076923
linear embedding analysis on	linear embedding x n_neighbors	0.200000
load images	datasets load	0.083333
finds radius neighbors from the candidates obtained	radius neighbors query max_depth bin_queries radius	1.000000
dispatch them	joblib parallel dispatch	0.250000
dual gap convergence criterion the	covariance dual gap emp_cov precision_ alpha	0.071429
training data and	x y	0.004310
of x (as bigger is better i	x	0.001692
sure that array is 2d square and symmetric	utils check symmetric array tol raise_warning raise_exception	0.500000
directory in which are persisted the result of	output dir	0.047619
vectors for reproducibility flips	utils deterministic vector	0.076923
fit the	linear svr fit x	0.333333
bic or aic	ic	0.111111
mean and variance along an axix	utils mean variance axis x axis	0.142857
the array from the meta-information and	joblib zndarray wrapper read unpickler	0.043478
cv in a	check cv cv x	0.031250
length is not found and raise an	line search	0.029412
the timestamp when pickling to avoid the	joblib memory reduce	0.030303
make cache size fit in	reduce size	0.083333
to split data into	model_selection base shuffle split split x	0.250000
projection of the data onto the sparse components	sparse pca transform x ridge_alpha	0.200000
compute elastic net path with	linear_model enet path	0.050000
binarization transformation	binarize	0.090909
of a hermetian matrix	utils pinvh a cond rcond lower	0.200000
of shape (n_samples n_features)	decomposition infer	0.200000
given radius of	radius	0.045455
k x y and	rbf call x y	0.200000
given arguments	joblib memorized func get	0.125000
validity of the	x metric p metric_params	0.100000
like assert_all_finite but only for	utils assert all finite	0.333333
the process of the parallel execution only a	externals joblib parallel	0.014085
tokens	mixin word ngrams tokens	1.000000
using a single binary estimator	predict binary estimator	0.200000
matrices w h whose	w h n_components	0.038462
of the dual gap convergence criterion the specific	covariance dual gap emp_cov precision_ alpha	0.071429
likelihood of	likelihood	0.111111
newsgroup	newsgroup	0.714286
parameters for the voting classifier valid parameter keys	ensemble voting classifier set params	0.037037
the search	base search cv fit	0.166667
print verbose message on the end of	print verbose msg init end ll	0.333333
text files with categories as subfolder names	files container_path description categories	0.500000
mutual information between two	metrics cluster mutual info score	1.000000
a decision tree regressor from	tree decision tree regressor	0.166667
for a calibration curve	core calibration curve y_true	0.142857
denominator	denominator	1.000000
fit	sgdclassifier fit	0.230769
the score for a fit across one fold	feature_selection rfe single fit rfe estimator x	0.200000
to cleanup a temporary folder if still existing	externals joblib delete folder folder_path	0.250000
elastic net parameter search	x y xy l1_ratio	0.250000
two rows of a csr matrix in-place	utils inplace swap row csr	0.250000
a mask to edges weighted or not	feature_extraction mask edges weights mask edges weights	0.166667
randomized svd	randomized svd m	0.500000
maximum absolute value to be	preprocessing max abs	0.050000
compute the unnormalized posterior log probability of x	nb joint log likelihood x	0.111111
class weights	utils compute class	0.166667
the model using x	x y	0.002155
as the maximizer of the reduced likelihood function	arg max reduced likelihood function	0.333333
false positives per	clf curve y_true	0.250000
skip test if being run on travis	skip travis	1.000000
search over	base search	0.100000
compute l1 and l2 regularization coefficients for	decomposition compute regularization alpha l1_ratio regularization	0.333333
range approximates the range of	utils randomized range	0.083333
finds the k-neighbors of a	neighbors kneighbors mixin kneighbors x n_neighbors return_distance	0.250000
used to compute log probabilities	predict log proba	0.029412
global clustering for	global clustering	0.142857
posterior log probability of	nb joint log likelihood	0.066667
diagonal structure	biclusters shape n_clusters noise minval	0.058824
the timestamp when pickling	externals joblib memorized func reduce	0.050000
given data if the estimator has been refit	core base search cv	0.033333
to compute decisions within	estimators_features x	0.111111
compute labels	labels	0.142857
exception types	joblib	0.007299
predicted probabilities for a calibration curve	calibration curve y_true y_prob normalize	0.142857
factorizing common classes param logic estimators	partial fit first call clf classes	0.058824
in the specified row returns	in row row	0.250000
handle the callable case for pairwise_{distances kernels}	callable x y	0.083333
free	free	1.000000
getter for	empirical covariance get	0.166667
base class for all kernel operators	kernel operator	0.142857
return a tolerance which	tolerance x tol	0.058824
call transform	transform	0.011236
a platform independent	utils shape repr	0.013699
descriptors of a memmap instance	memmap a	0.050000
in n_jobs even slices	x y func n_jobs	0.166667
count	count	1.000000
returns n_neighbors of approximate nearest	lshforest kneighbors x n_neighbors return_distance	1.000000
cv in a user friendly way	cv cv	0.031250
fit the ardregression model according to the given	linear_model ardregression fit	0.250000
estimator and	x y sample_weight	0.012987
pictures of famous people	funneled resize	0.142857
mean absolute error regression loss read more	metrics mean absolute error	0.166667
sparse random projection matrix	random projection fit	0.333333
curve auc from prediction scores	auc	0.020408
is the depth	func check previous func code	0.333333
elasticnet model trained with	elastic net	0.111111
a large sparse	a	0.018182
the generative model	pca	0.047619
coefficient of determination regression	metrics r2	0.125000
along any axis	axis	0.028169
apply a mask to edges weighted or not	feature_extraction mask edges weights mask edges weights	0.166667
the function call with the given	externals joblib format call func	0.100000
return staged predictions for x	classifier staged predict x	0.500000
non-overlapping labels	label	0.045455
directory corresponding to	func dir mkdir	0.166667
solution to a large sparse	utils lsqr a	0.037037
perform	x responsibilities	0.500000
an array with constant block diagonal structure for	biclusters shape n_clusters noise minval	0.058824
orthogonal matching pursuit	omp x y n_nonzero_coefs tol	0.500000
covered	hungarian state	1.000000
estimators from sklearn	estimators include_meta_estimators include_other type_filter include_dont_test	1.000000
transform data	transform	0.011236
logistic	linear_model logistic	0.333333
depending	externals joblib memorized func	0.013158
platform independent representation of	shape	0.011765
group out cross-validator provides	group out	0.142857
\#3" regression problem this dataset is described	datasets	0.015152
oracle approximating shrinkage covariance model according	covariance oas fit x	0.083333
within a given radius of a point	x radius	0.058824
the lars algorithm	lars	0.090909
transform binary labels back	label binarizer inverse transform y	0.500000
operation is meant to be cached by	data_folder_path slice_ color resize	0.033333
back the	inverse transform x copy	0.066667
building a cv in a user friendly	core check cv cv x y classifier	0.031250
is the solution to a sparse coding problem	sparse encode x dictionary gram	0.333333
scaler	scaler	0.250000
binary	average binary score	0.500000
check x format and	allocation check	0.062500
used by parallel inside	parallel	0.019231
of last step in pipeline after transforms	pipeline fit predict	0.166667
terminal regions (=leaves)	function update terminal region tree terminal_regions leaf	0.200000
approximation of	n_samples n_subsamples	0.333333
the kddcup99 dataset downloading it if necessary	brute kddcup99 subset data_home download_if_missing random_state	0.111111
not found and raise an exception if	utils line search	0.029412
check according to	check	0.017857
coverage error measure	metrics coverage error	0.166667
to line_search_wolfe2 if suitable step length is not	wolfe12 f fprime xk pk	0.028571
context manager and decorator to ignore warnings	utils ignore warnings	0.142857
the default backend used by parallel	parallel backend backend n_jobs	0.166667
to unit	preprocessing	0.052632
exception	get	0.012048
indices to split data into	model_selection base shuffle split split x	0.250000
build a text report showing the	report	0.047619
the huber loss and the	huber loss and	0.166667
'return_annotation'	init parameters return_annotation __validate_parameters__	0.125000
clusterings	labels_true labels_pred	1.000000
"news" format strip the headers	datasets strip newsgroup	0.090909
defines all methods a parallelbackend must implement	backend base	0.032258
neighbors within	neighbors x	0.166667
error regression loss read more in	log error	0.142857
fit	model_selection fit	0.500000
the median and	x y	0.002155
coverage error measure compute	metrics coverage error	0.166667
slices containing batch_size elements from 0 to n	batches n batch_size	1.000000
x (as bigger	x	0.001692
the voting classifier valid parameter	voting classifier set params	0.037037
store the timestamp when pickling	joblib memory reduce	0.030303
generates a random sample from a	a size replace	0.142857
class versus all others	multiclass x y	0.166667
to be captured	backend base get exceptions	0.166667
matrix product with the random matrix	random projection transform x y	0.333333
apply clustering to a projection	cluster spectral clustering affinity n_clusters n_components	0.166667
of exception types to be captured	backend base get exceptions	0.166667
wild lfw pairs dataset this	lfw pairs	0.018868
estimates for each	predict estimator x y	0.045455
embedding for non-linear dimensionality reduction	embedding	0.040000
return the current file position	joblib binary zlib file tell	0.333333
the minimum covariance determinant matrix	covariance fast mcd x	0.250000
transform binary	transform	0.011236
right fileobject from	read fileobject	0.100000
of neighbors for	radius neighbors mixin radius neighbors	0.125000
matching pursuit problems	y n_nonzero_coefs tol	0.250000
of the function called with the given arguments	memorized func get output	0.125000
using the lars algorithm	lars	0.090909
from it	externals joblib memory	0.016949
does nothing this transformer is stateless	feature_extraction hashing vectorizer partial fit	1.000000
into a matrix of patch data	feature_extraction patch extractor transform	0.200000
and false positives per binary classification	binary clf curve y_true y_score	0.090909
determine the optimal batch	auto batching mixin compute batch	0.333333
for x relative to y_true	metrics threshold scorer call clf x	0.058824
mean squared logarithmic	metrics mean squared	1.000000
regression and cv and linearsvc	svm fit liblinear x y c fit_intercept	0.142857
for the voting classifier	voting classifier	0.035714
make a large circle containing a smaller circle	make circles n_samples shuffle noise random_state	1.000000
check input and compute prediction of init	base gradient boosting init decision function	0.142857
precisions	precisions precisions covariance_type n_components n_features	0.250000
returns n_neighbors	x n_neighbors return_distance	0.250000
with the	decomposition base	0.076923
to a projection to	spectral	0.026316
undo	preprocessing min max	0.333333
sign of elements of	sign	0.050000
for full covariance	density full x means covars	0.166667
train test	base shuffle split	0.142857
l1	metrics manhattan	0.333333
under the curve auc using the trapezoidal rule	metrics auc x	0.040000
preference	preference	1.000000
array with constant block diagonal structure	biclusters shape n_clusters noise minval	0.058824
absolute error regression loss	absolute error	0.142857
lad updates	least absolute error update	0.500000
build a batch of estimators within	ensemble parallel build estimators n_estimators ensemble	0.166667
apply dimensionality reduction to x using	decomposition factor analysis transform x	0.250000
generate a random n-class	n_samples n_features n_informative n_redundant	1.000000
is the time it take to	joblib squeeze time	0.200000
the binary classification	y_true y_score pos_label sample_weight	0.066667
types	externals joblib parallel backend base get	0.066667
private function used to compute decisions within	decision function estimators estimators_features	0.500000
which defines all methods a parallelbackend must implement	backend base	0.032258
of x according to	transform x	0.016949
method for updating terminal regions (=leaves)	update terminal region tree terminal_regions leaf	0.200000
methods a parallelbackend must implement	backend	0.016949
area under the curve auc using the trapezoidal	auc x y	0.040000
and predict_log_proba of the final estimator parameters	core pipeline predict log proba x	0.500000
values for x relative to y_true	scorer call estimator x y_true sample_weight	0.200000
make	base ensemble make estimator	1.000000
and evaluates the reduced likelihood function for	gaussian_process gaussian process reduced likelihood function	0.047619
function the objective	objective	0.076923
in n_jobs even slices	n_jobs	0.023256
score with permutations	core permutation test score estimator x	0.500000
of array-like or scipy	preprocessing binarize	0.083333
spatial independence correlation model pure nugget	gaussian_process pure nugget	1.000000
inplace column	utils inplace column scale	0.166667
zhu et al [1]	ensemble samme proba estimator n_classes x	1.000000
point	estimator x	0.030303
values for	x y train	0.166667
check x format	allocation check	0.062500
and dense inputs	y sample_weight random_state	0.166667
dimensionality reduction on	decomposition randomized pca	1.000000
mutual information	feature_selection mutual info classif x y	0.500000
svd	x n_components	0.500000
append	append	0.416667
two non-negative matrices w h whose product approximates	x w h	0.035714
to build from the	build from	0.250000
a subcluster from	subcluster new_subcluster1	0.166667
a single binary estimator	predict binary estimator x	0.200000
fit on the estimator with	fit	0.003257
the kernel	kernel mixin	0.333333
warning to	warning	0.083333
callable case for	metrics pairwise callable x y	0.083333
this function returns posterior probabilities of classification	core calibrated classifier cv predict proba x	0.200000
random matrix	random matrix n_components	1.000000
cache result and return a	externals joblib memorized	0.013699
of array-like or scipy sparse matrix	binarize x	0.083333
to	joblib parallel backend base	0.058824
false for indices increasingly apart the distance depending	verbosity filter index	0.055556
in	externals joblib memory reduce	0.030303
found and raise an	line search	0.029412
compute the maximum absolute value to	max abs	0.047619
from source to	source	0.100000
and binary classification algorithms are	y	0.002674
with constant block diagonal structure	biclusters shape n_clusters noise minval	0.058824
to capture the arguments of	check_pickle	0.040000
on x and	x	0.001692
the largest k singular values/vectors for a sparse	utils svds a k ncv tol	0.166667
the content of the data	datasets clear data	0.142857
number of splitting iterations in	model_selection predefined split get n splits x y	0.111111
dual gap convergence criterion	dual gap emp_cov precision_	0.071429
model parameters	model	0.058824
load the kddcup99 dataset downloading it	fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
parameters and evaluates the reduced likelihood	gaussian_process gaussian process reduced likelihood	0.142857
low	make low	0.333333
get a signature object for the passed callable	externals signature	0.050000
classification by definition a confusion matrix :math c	metrics confusion matrix y_true	1.000000
list of feature name -> indices mappings	feature_extraction dict vectorizer fit	0.250000
cohen's kappa a statistic that measures inter-annotator agreement	cohen kappa score y1 y2 labels weights	0.500000
last element of numpy array or sparse matrix	last element arr	0.142857
node	cluster	0.021277
step	step	0.750000
to split data into training and test set	time series split split x y groups	0.200000
the shortest path length from source to	shortest path length graph source cutoff	0.111111
the right fileobject from a filename	joblib read fileobject fileobj filename mmap_mode	0.250000
bound	bound	0.583333
the set of samples x	x y sample_weight	0.012987
the descriptors of a memmap instance to	memmap a	0.050000
of two clusterings of a set of	score labels_true labels_pred sparse	0.047619
a projection to the normalized	spectral	0.026316
the samples x to the separating hyperplane	svm one class svm decision function x	0.250000
fit the	svr fit x y	0.333333
for unbalanced datasets	weight class_weight y indices	0.200000
predict regression target at each stage for	regressor staged predict	0.500000
under the curve auc from prediction scores note	metrics roc auc	0.166667
matrix	matrix	1.000000
the cache for the function	memorized func	0.016949
the number of splitting iterations in the	cviterable wrapper get n splits x	0.111111
for building a cv in a	cv cv x	0.031250
tests involving both blas calls and multiprocessing	if safe multiprocessing with blas func	0.500000
what extent the local structure is retained	trustworthiness x x_embedded n_neighbors precomputed	0.200000
build a contingency matrix describing the	contingency matrix labels_true labels_pred eps	0.166667
minimum distances between one point	pairwise distances argmin x	1.000000
types to be captured	parallel backend base get exceptions	0.166667
the maximizer of	arg max	0.047619
for each input data point	core cross val predict estimator	0.045455
and false positives per binary	binary clf curve y_true y_score pos_label sample_weight	0.090909
used	used	1.000000
the score on the given	score x y	0.030303
which is equal to the average path length	ensemble average path length	0.090909
the function call	format call func	0.100000
platform independent representation	utils shape repr	0.013699
the california housing dataset from statlib	california housing	0.083333
reduced likelihood function for the given	gaussian_process gaussian process reduced likelihood function	0.047619
n_packs	n_packs	1.000000
the sample weight array	sgd validate sample weight sample_weight	0.333333
and returns the	y	0.002674
fit the model	linear svc fit x y	0.333333
the pairwise matrix in n_jobs even	pairwise x y func n_jobs	0.111111
in svmlight format this function is equivalent to	svmlight	0.050000
input and compute prediction of init	init decision function x	0.142857
predict	cv predict x	0.125000
of prediction pred and y	function call y pred sample_weight	0.500000
call transform on the	transform x	0.016949
perform classification	svc predict	1.000000
of a cross-validated	cv	0.009009
avoid the	externals	0.011494
used to build a batch of estimators within	build estimators	0.166667
the validity of the	x metric p metric_params	0.100000
return_distance	return_distance	0.555556
of x for later	x y	0.002155
score is the	score y_true y_pred	0.038462
derivatives with respect to	activations deltas	0.032258
similarity of two clusterings	score labels_true labels_pred	0.047619
does not need to update terminal regions	error update terminal regions tree x y residual	1.000000
precision matrix with the	decomposition base pca get precision	0.066667
kernel k x y and	product call x y	0.200000
cross-validated estimates for each input	estimator x y cv	0.050000
matrix shrunk on the diagonal read	covariance shrunk	0.066667
creates	cluster base spectral fit	0.250000
fit the hierarchical clustering on the data	cluster feature agglomeration fit	0.250000
structure	shape n_clusters noise minval	0.333333
this function returns posterior probabilities of classification	classifier cv predict proba x	0.200000
locally linear	manifold locally linear	0.250000
returns the number of splitting iterations in	cviterable wrapper get n splits x y groups	0.111111
of the decision functions	decision function	0.025000
pursuit omp solves n_targets orthogonal matching pursuit problems	linear_model orthogonal mp x y n_nonzero_coefs	0.200000
the curve auc using the trapezoidal rule this	auc x	0.040000
by definition a confusion matrix :math c is	confusion matrix	0.500000
estimators should be used when memory is inefficient	x y classes	0.027778
the array corresponding to this wrapper	externals joblib numpy array wrapper	0.333333
anova f-value for the provided	feature_selection f classif	0.200000
removing everything before the first blank line	header	0.090909
parameters and raise valueerror if not valid	base gradient boosting	0.100000
first to assert that	check	0.017857
to compute decisions within a	estimators_features	0.071429
of the data home	datasets clear data home data_home	0.076923
that implement the partial_fit api	utils check	0.023810
the number of jobs which are going to	joblib multiprocessing backend effective n jobs n_jobs	0.333333
of	parallel backend	0.030303
cv in a user	core check cv cv x y classifier	0.031250
the long type introduces	repr	0.012500
solution to a large	a	0.018182
make and configure a	base ensemble make estimator append random_state	0.166667
the accuracy of a classification	y_true y_pred labels	0.125000
individually to unit norm vector length	preprocessing normalize x norm axis copy	0.200000
boost	classifier boost	0.200000
update the	decomposition update dict	0.333333
leave-one-label_out cross-validation iterator	leave one label out	0.500000
the weighted graph of neighbors for	neighbors graph	0.066667
arguments	joblib memorized func get	0.125000
the oracle approximating shrinkage covariance model	covariance oas fit x	0.083333
cv in a user	core check cv cv	0.031250
kernel k x	gaussian_process exp sine squared call x	1.000000
squaring of array-likes and sparse matrices	utils safe sqr x copy	0.125000
computes the	alpha	0.235294
returns the score on the given	score x y	0.030303
multi-task l1/l2 lasso with built-in cross-validation	multi task lasso cv	1.000000
return staged predictions for	ada boost classifier staged predict	1.000000
to be captured	base get exceptions	0.166667
for a calibration curve	calibration curve y_true y_prob normalize	0.142857
corresponds to the	y_true y_score	0.027027
depending from	joblib memory	0.016949
given dataset	y scorer	0.111111
inefficient to train all data	y classes	0.027778
grid of alpha values for	alpha grid	0.166667
full covariance matrices	multivariate normal density full x means	0.166667
compute area under the curve auc	auc score	0.052632
perform mean shift clustering	cluster mean shift x bandwidth seeds	0.500000
strip the	datasets strip	0.076923
metrics read more in the	metrics	0.043478
and component wise scale	robust scale x	0.125000
determine the optimal batch	backend base compute batch	1.000000
predict	decision function x	0.018868
the model using x	x	0.006768
then	mixture gmmbase	0.034483
rand index adjusted for	adjusted rand score labels_true labels_pred	0.333333
curve auc using	auc x y	0.040000
case of a multinomial	multinomial	0.083333
of the scaler	preprocessing standard scaler	0.333333
to avoid the hash depending from it	func	0.011364
locally linear embedding analysis on the data	manifold locally linear embedding x	0.071429
the number of splitting iterations in the cross-validator	base kfold get n splits x	0.111111
and its corresponding derivatives with respect	activations deltas	0.032258
of determination r^2	multi output regressor score x y	0.200000
/ tp + fn where tp is the	score y_true y_pred labels pos_label	0.027778
do	do	1.000000
a large sparse linear system of equations	lsqr a b damp atol	0.500000
quantiles to be	preprocessing robust	0.111111
python object into	filename	0.050000
orthogonal matching pursuit	n_nonzero_coefs tol	0.250000
best found	search cv predict proba	0.076923
evaluates the reduced likelihood function for the given	gaussian process reduced likelihood function	0.047619
percentiles	percentiles	1.000000
linear embedding analysis on the	linear embedding x	0.200000
for c such that for c	c x y	0.030303
when using the	utils shape repr	0.013699
cpp	cpp	1.000000
faces in the wild lfw pairs	fetch lfw pairs	0.018868
pairs for different probability thresholds note	probas_pred pos_label sample_weight	0.066667
submatrix corresponding to	mixin get submatrix	0.166667
with respect	n_samples activations deltas	0.166667
jaccard similarity coefficient score	score y_true y_pred normalize	0.125000
compute	feature_selection compute	1.000000
fit a multi-class classifier by	base sgdclassifier fit	0.076923
and scale	y	0.002674
predict the target	cv predict x	0.125000
rand index adjusted for chance	metrics cluster adjusted rand score	0.333333
memory is inefficient	x y classes	0.027778
cv in a	check cv cv	0.031250
of the given observations	covariance outlier detection mixin	0.250000
gram matrix	linear_model gram omp gram	0.500000
indices	split iter indices	0.250000
reliable	joblib	0.007299
the kernel	stationary kernel	0.333333
used for scaling	scaler fit	0.076923
of edges for	edges	0.047619
fit with	search cv fit x	0.111111
mean update and a youngs	utils incremental mean	0.250000
terminal regions to median estimates	terminal region	0.100000
checker utility for building a cv in a	check cv cv x y	0.031250
cv in	core check cv cv	0.031250
data and concatenate	x y	0.002155
decision function of the	decision function x raw_values	0.083333
the right fileobject from	externals joblib read fileobject	0.100000
lfw pairs dataset this operation is meant	datasets fetch lfw pairs index_file_path data_folder_path slice_ color	0.333333
"friedman \#3" regression problem this dataset is described	datasets	0.015152
generate	choice csc n_samples	1.000000
in an unfitted estimator	unfitted name estimator	0.142857
a new ndarray with	utils	0.009709
kddcup99 dataset	datasets fetch brute kddcup99	0.166667
the timestamp when pickling	reduce	0.034483
and y read more	y	0.002674
by quantile this classification dataset is constructed by	datasets	0.015152
the shortest path length from source	shortest path length graph source cutoff	0.111111
x	x n_neighbors reg n_jobs	0.500000
generate isotropic gaussian blobs for	blobs n_samples n_features centers cluster_std	0.333333
the gaussian process model	gaussian_process gaussian process	0.111111
exponential chi-squared kernel	metrics chi2 kernel	0.333333
svmlight / libsvm format into sparse csr matrix	svmlight file f	0.066667
params	params	0.171429
call predict_log_proba on the estimator with the best	cv predict log proba x	0.500000
the hash	externals	0.011494
computes multinomial loss and class probabilities	linear_model multinomial loss w x y alpha	0.333333
to the cache	memorized	0.015873
(y[i] - y_[i]) ** 2	sample_weight y_min y_max	0.166667
test vectors	core dummy regressor	0.200000
scale back the data	standard scaler inverse transform	0.066667
indices to split data into training and test	model_selection cviterable wrapper split x y groups	0.200000
cf	cf	1.000000
the model	randomized linear model	0.076923
hash_name	hash_name	1.000000
the curve auc from	auc	0.020408
fit	embedding fit	0.333333
of transform is sometimes referred	transform y	0.023256
timestamp when pickling to avoid	externals joblib memorized func reduce	0.050000
a which this function	externals joblib memorized func check	0.125000
store the timestamp when pickling to avoid	joblib memorized func reduce	0.050000
make predictions using a single binary estimator	predict binary estimator x	0.200000
from the c	from c	1.000000
the number of splitting iterations in	group out get n splits x y groups	0.111111
fits the oracle approximating shrinkage	oas	0.100000
range approximates the range	randomized range finder	0.083333
remove cache folders to make cache size fit	joblib memory reduce size	0.083333
transformation	transform xred	0.500000
initial centroids	cluster init centroids x k init random_state	0.166667
'l' suffix when using	utils shape repr	0.013699
two non-negative matrices w h whose product	x w h n_components	0.038462
than	mode	0.250000
absolute sizes	core translate train sizes	0.066667
batch of estimators within	estimators n_estimators ensemble x y	0.083333
bin	bin	1.000000
projection p only changes the	johnson lindenstrauss min dim	0.500000
param	param	1.000000
estimate the precisions parameters of the	gaussian mixture estimate precisions nk	0.166667
set	pipeline set	0.250000
job	estimators estimators_features	0.333333
weighted graph of neighbors for points in x	radius neighbors mixin radius neighbors graph x	0.500000
predict_log_proba of the final estimator parameters	core pipeline predict log proba	1.000000
factorization nmf find two non-negative matrices w h	decomposition non negative factorization x w h	1.000000
split data into	shuffle split split x	0.250000
such that for c in (l1_min_c infinity)	c x y	0.030303
scale to unit variance	preprocessing scale	0.090909
probability calibration with sigmoid method platt 2000	core sigmoid calibration df y sample_weight	0.500000
type suitable for scipy sparse	int	0.100000
a temporary folder if still existing	delete folder folder_path warn	0.250000
regression problem with sparse uncorrelated design	sparse uncorrelated n_samples n_features	0.166667
for parallel	joblib parallel	0.028571
make	ensemble base ensemble make	1.000000
the hash depending	externals joblib memorized	0.013699
of probability estimates	proba	0.058824
image samples in x	x	0.001692
sample from the decision	decision	0.027778
"news" format	newsgroup	0.285714
estimators that implement the partial_fit	utils check partial	0.038462
persist an arbitrary python object into one	filename compress protocol	0.250000
non-negative matrices w h whose product	x w h	0.035714
restricted to the binary classification	y_true	0.043478
getter for the precision matrix	covariance get precision	0.250000
the samples x	svc decision function x	0.200000
the labeled faces in the wild lfw	datasets fetch lfw	0.041667
compute probabilities of possible outcomes for samples	ensemble voting classifier predict proba	0.200000
as training data	xy	0.076923
x (as bigger is better i	function x	0.030303
inefficient to train all	x y classes	0.027778
mixin class	mixin	0.037037
get the values used to update	get	0.012048
cv in a user friendly way	core check cv cv x	0.031250
the kernel k x	dot product call x	0.200000
transform	transform selected x transform	0.333333
estimates the minimum covariance determinant matrix	covariance fast mcd x support_fraction	0.250000
the number of splitting iterations in the	one group out get n splits	0.111111
reconstruct the	wrapper read unpickler	0.333333
array with constant block diagonal structure for	biclusters shape n_clusters noise minval	0.058824
kwargs	kwargs	0.461538
param logic estimators that	utils check partial fit	0.038462
by quantile this classification dataset is	datasets	0.015152
x y and optionally its gradient	matern call x y eval_gradient	0.333333
wild lfw pairs dataset	fetch lfw pairs subset	0.035714
area under the curve auc from	auc	0.020408
column class distributions parameters	classes class_probability random_state	0.166667
to build a batch	ensemble parallel build	0.047619
don't	externals	0.011494
points based on	from	0.045455
validation	dtype	0.062500
perform standardization by centering and scaling	preprocessing standard scaler transform x y copy	0.333333
tp + fp where tp is the	score y_true y_pred labels pos_label	0.027778
a tolerance which	tolerance x	0.058824
terms per document with nonzero entries	feature_extraction count vectorizer inverse transform	0.166667
covariance model according	covariance	0.014493
avoid the hash depending	externals joblib memory	0.016949
svm	svm	0.714286
platform	repr	0.012500
fit	cv fit	0.400000
under the model	score	0.010101
temporary folder if still existing	externals joblib delete folder folder_path	0.250000
a conditional property using the descriptor	iff has attr descriptor	0.083333
along any axis center to the mean	x axis	0.015385
function returns posterior probabilities of classification	core calibrated classifier cv predict proba	0.200000
input data point	predict	0.006849
a batch of estimators within a	estimators n_estimators ensemble x	0.083333
number of splitting iterations in the cross-validator	predefined split get n splits x y groups	0.111111
check	check params x	0.500000
e-step in em update	decomposition latent dirichlet allocation e step x cal_sstats	1.000000
exp(-e v	v	0.052632
coefficient matrix to	linear_model sparse	0.076923
check that	utils check clusterer compute labels	1.000000
projection to the normalized laplacian	eigen_solver	0.090909
to a large sparse linear	lsqr a	0.037037
thread pool	joblib	0.007299
input data	val predict estimator x y	0.045455
given	x y scorer	0.111111
information for reducing	items root_path	0.066667
of array-like or scipy sparse matrix	preprocessing binarize x threshold copy	0.083333
to split data into training and test	predefined split split x y groups	0.200000
weighted graph of neighbors for	neighbors mixin radius neighbors graph	0.066667
build a	ensemble parallel build	0.047619
classification	y_true y_pred labels	0.125000
return the shortest path length	single source shortest path length	0.333333
length from source to all reachable	length graph source cutoff	0.200000
function for factorizing common classes	fit first call clf classes	0.058824
sparse random projection matrix	random projection	0.125000
ensemble	ensemble	0.500000
test	base	0.014286
coefficient	coef mixin	0.090909
sparse and	y sample_weight	0.017857
the shrunk ledoit-wolf	ledoit wolf shrinkage x assume_centered block_size	0.250000
get the boolean mask indicating which	mixin get support mask	0.333333
introduces an 'l'	utils shape repr	0.013699
to the binary	y_score	0.083333
lfw people dataset	lfw people	0.040000
compute class covariance	class cov x	0.250000
for tests involving both blas calls and multiprocessing	if safe multiprocessing with blas func	0.500000
x y and optionally its gradient	dot product call x y eval_gradient	0.333333
the number of splitting iterations in the	group out get n splits x y	0.111111
folders to	reduce	0.017241
return the kernel k x	gaussian_process constant kernel call x	0.333333
locally linear embedding analysis	locally linear embedding x n_neighbors n_components reg	0.071429
x and y	x y gamma	0.666667
as the maximizer of the reduced likelihood function	gaussian_process gaussian process arg max reduced likelihood function	0.333333
returns a list of edges	edges	0.047619
calculates a covariance matrix shrunk on the diagonal	covariance shrunk covariance emp_cov shrinkage	0.250000
private function used to fit an estimator	fit estimator estimator	0.055556
the given training data and	x y	0.002155
callable case for pairwise_{distances	callable x y metric	0.083333
paired cosine distances between x and y read	metrics paired cosine distances x y	0.333333
for full	full x means	0.166667
new samples can be different from	core calibrated classifier cv	0.111111
forest regressor	forest regressor	1.000000
and scale the data parameters	y	0.002674
from the training set	fit	0.013029
perplexity for	perplexity	0.181818
estimator on training	fit estimator estimator x	0.111111
sample images	sample images	0.250000
the case method='lasso' is :	x y xy gram	0.090909
huber loss and the	linear_model huber loss and	0.166667
compute decision function of x for each iteration	ensemble gradient boosting classifier staged decision function x	1.000000
the multi-layer perceptron classifier parameters	neural_network mlpclassifier	0.333333
display the	externals joblib parallel print	0.125000
solution to a large sparse linear	lsqr a	0.037037
the model to data matrix x	x	0.001692
neighbors	lshforest radius neighbors x	0.166667
the k-neighbors	neighbors kneighbors mixin kneighbors x	0.125000
the time	time t	0.125000
lasso path using lars algorithm [1] the optimization	linear_model lars path x	0.100000
and intercepts from packed_parameters	neural_network base multilayer perceptron unpack packed_parameters	0.250000
derivative of the logistic sigmoid function	neural_network inplace logistic derivative z delta	0.166667
and cv and	y	0.002674
write the function code and the filename	memorized func write func code filename	1.000000
anova f-value	feature_selection f classif x	0.200000
computes the position of the	mds fit x y init	0.066667
to the binary classification task	curve y_true	0.125000
of x to unit norm parameters	preprocessing normalizer transform x y copy	0.250000
performs clustering on x	dbscan fit predict x	0.333333
of max absolute value	max abs	0.047619
creates a biclustering	cluster base spectral fit	0.250000
gradient and the	y	0.005348
process of the parallel execution	externals joblib parallel	0.014085
points that will be sampled	parameter sampler len	0.333333
full covariance	multivariate normal density full	0.166667
the model according to the given training	sample_weight	0.037037
k x y and optionally its gradient	gaussian_process matern call x y eval_gradient	0.333333
update w in multiplicative update nmf	decomposition multiplicative update w x w h beta_loss	0.500000
the meta-information and the z-file	joblib zndarray wrapper read unpickler	0.043478
call predict on the estimator	predict x	0.011765
for a calibration curve	core calibration curve	0.142857
elastic net path with	path	0.025641
the decision function of the given	decision function	0.025000
sparse coding	sparse coding	1.000000
the meta-information and the z-file	externals joblib zndarray wrapper read unpickler	0.043478
returns the huber loss and the gradient	huber loss and gradient w	0.333333
function	func	0.034091
we don't store	memory	0.015625
function returns posterior probabilities of	cv predict proba x	0.034483
voting classifier valid parameter keys can be listed	voting classifier	0.035714
described in friedman [1] and breiman [2]	make friedman3	0.166667
squared euclidean norm	squared	0.083333
calculates a covariance matrix shrunk on the diagonal	shrunk covariance emp_cov shrinkage	0.250000
computes the weighted graph of neighbors	neighbors radius neighbors mixin radius neighbors graph	0.066667
score function	score y_true	0.058824
stratified k-folds cross-validator provides train/test indices to split	stratified kfold	0.200000
workers requested by the callers	externals joblib effective	0.200000
kappa	metrics cohen kappa	0.250000
transform feature->value dicts to array or	feature_extraction dict vectorizer transform x	0.200000
sure centering is not enabled for sparse	preprocessing robust scaler check array x	0.250000
under the	score x y	0.030303
max_samples	max_samples	1.000000
normalized mutual	normalized mutual	1.000000
transform x	transform x y	0.031250
items	items	1.000000
timestamp when pickling to avoid the hash	memorized func reduce	0.050000
a fit	fit rfe estimator	0.166667
of a cross-validated	estimator x y cv	0.050000
the number of splitting iterations in the	out get n splits x y	0.111111
the	externals joblib multiprocessing	0.052632
a locally linear embedding	locally linear embedding x n_neighbors n_components reg	0.071429
a memmap instance	joblib reduce memmap a	0.050000
em update	allocation em step	0.500000
product with	projection	0.071429
h	h	0.541667
extent the local structure is retained	x_embedded n_neighbors precomputed	0.200000
inplace row scaling of a csr matrix	utils inplace csr row scale	1.000000
find the least-squares solution to a	a	0.018182
apply clustering to a projection to	spectral clustering affinity n_clusters	0.166667
the number of splitting iterations in	model_selection base kfold get n splits	0.111111
under the model	score x y	0.030303
opening the right fileobject from a filename	read fileobject fileobj filename	0.250000
training subsets incrementally	model_selection incremental fit	0.500000
predictions using a single binary estimator	binary estimator x	0.090909
lasso linear model with iterative fitting along	lasso cv	0.333333
the right fileobject	fileobject	0.100000
linear embedding analysis on the	linear embedding x n_neighbors	0.200000
learn vocabulary and idf return term-document	transform raw_documents y	0.100000
thread pool and return the number of	externals joblib multiprocessing backend	0.035714
lowest bound for c such that for c	c x y loss fit_intercept	0.030303
copy of y	y	0.002674
indices increasingly apart the	joblib verbosity filter index	0.055556
fit the model using x as training data	tsne fit x skip_num_points	0.500000
of the decision functions of the base classifiers	ensemble bagging classifier decision function x	0.333333
model	randomized linear model	0.076923
factorizing common classes	partial fit first call clf classes	0.058824
of transform is sometimes referred to by	transform y	0.023256
the boolean mask x	get mask x	0.333333
the function call with the	format call func	0.100000
l1-penalized covariance estimator read more in the	covariance graph lasso	0.166667
data	datasets clear data	0.142857
parallel processing this method is meant to	externals joblib parallel	0.014085
to a large sparse linear system of equations	a b damp atol	0.500000
linear model	linear_model linear regression	1.000000
curve auc	metrics auc x	0.040000
generate a signal	signal n_samples n_components n_features n_nonzero_coefs	1.000000
columns	columns	0.666667
two clusterings of a set of points	score labels_true labels_pred sparse	0.047619
shape	shape	0.070588
scaling features of	scaler transform	0.500000
on the grid	parameter grid	0.200000
the variational lower bound for	mixture dpgmmbase bound means	0.250000
partially fit underlying estimators should be used	one vs one classifier partial fit x	0.166667
candidates	candidates	1.000000
to implement the usual api and	x y	0.006466
coefficient score the	score y_true y_pred normalize sample_weight	0.125000
collect	voting classifier collect	1.000000
compute prediction of init	base gradient boosting init decision	0.142857
curve auc from prediction scores note	roc auc score	0.166667
non-negative matrix factorization nmf find two non-negative matrices	factorization	0.035714
a platform independent representation	utils	0.009709
trace of np dot x y	trace dot x y	0.500000
test/test sizes are meaningful wrt to	shuffle split n_samples test_size train_size	0.111111
the one-vs-one multi class libsvm in	svm one vs one	0.050000
a filename	fileobj filename mmap_mode	0.500000
to	sparse coef mixin	0.083333
precision matrix	pca get precision	0.066667
showing the main classification metrics read more in	metrics classification	0.052632
doc_topic_distr	doc_topic_distr	1.000000
different probability thresholds	probas_pred pos_label	0.200000
normalize x by scaling rows	scale normalize x	0.142857
fit the model using x as training data	neighbors local outlier factor fit x	0.333333
for the lfw pairs dataset	lfw pairs	0.018868
and scale the data parameters	scaler transform x y	0.200000
breakdown point	linear_model breakdown point	0.333333
class	compute class	0.166667
implementation is restricted to the binary classification task	score y_true y_score	0.025000
utility function opening the right fileobject from	externals joblib read fileobject	0.100000
a platform independent	utils shape	0.013699
path	enet path	0.050000
full	multivariate normal density full x	0.166667
returns the score on the given data if	score	0.010101
the	pca get	0.076923
underlying estimators	one vs one classifier	0.125000
back the	preprocessing robust scaler inverse transform x	0.066667
loader for the labeled faces in	data_home	0.055556
the one-vs-one multi class libsvm in the case	one vs one	0.050000
persist an	compress protocol	0.333333
on training subsets incrementally	model_selection incremental fit	0.500000
estimator with randomly drawn parameters	core randomized search cv	0.200000
_fit_coordinate_descent update	decomposition update	0.125000
returns true if the given estimator	estimator	0.014706
shortest path length from source to all reachable	source shortest path length graph source cutoff	0.111111
with the generative	base pca	0.071429
fit the kernel density model on the data	neighbors kernel density fit x	0.250000
don't	joblib memory	0.016949
predict multi-output variable using a model trained	core multi output estimator predict	0.166667
get	gaussian_process exponentiation get	1.000000
the points in the	core parameter	0.250000
func	func	0.090909
computes	empirical	0.111111
detects	svm one class svm fit	0.125000
the recall is the ratio tp	metrics recall	0.033333
update the variational distributions for the means	mixture dpgmmbase update means x	1.000000
to multi-class labels parameters	threshold	0.076923
cache	joblib cache	0.250000
project data to vectors and cluster the result	spectral biclustering project and cluster data vectors	0.333333
provided precisions	precisions precisions	0.250000
multi-class labels	y threshold	0.166667
x (as	function x	0.030303
in x according to	x	0.001692
non-negative matrices w h whose product approximates	w h	0.031250
sizes of training	train sizes	0.066667
predict using the gaussian process	gaussian_process gaussian process	0.111111
avoid the hash depending	memorized	0.015873
of matrices	covariance_type	0.083333
evaluates the reduced likelihood function for the given	gaussian_process gaussian process reduced likelihood function	0.047619
k-neighbors of a	kneighbors mixin kneighbors x	0.125000
coverage error measure compute how far	metrics coverage error y_true	0.166667
generate a mostly	n_samples n_features	0.083333
for scaling	scaler fit	0.076923
for building a cv in a	core check cv cv	0.031250
return the number	externals joblib	0.014286
the blup parameters and evaluates the reduced likelihood	reduced likelihood	0.100000
returns the number of splitting iterations in the	leave one group out get n splits	0.111111
(y[i] - y_[i]) ** 2	y sample_weight y_min y_max	0.166667
neighbors from the	neighbors	0.027027
as a sparse combination of dictionary elements	make sparse coded	1.000000
binary labels back to	inverse	0.055556
for each input data	val predict estimator x y	0.045455
on the estimator with the best found	model_selection base search cv predict	0.076923
step in pipeline after transforms	pipeline fit predict	0.166667
for the voting classifier valid parameter	voting classifier	0.035714
back	preprocessing standard scaler inverse transform x copy	0.066667
classifier valid	classifier set	0.125000
list	joblib parallel backend base get	0.066667
homogeneity metric of a cluster labeling given a	cluster homogeneity score labels_true labels_pred	0.500000
to retrieve a reliable	externals joblib get	0.142857
estimates for	predict	0.006849
transforms and score with the final estimator parameters	core pipeline score x y	0.500000
residues on left-out data for	residues x_train y_train x_test y_test	0.083333
generate a random multilabel classification problem	multilabel classification n_samples n_features n_classes	0.500000
fits the graphlasso covariance	covariance graph lasso cv	0.111111
median absolute error regression loss read more in	metrics median absolute error	0.166667
merge	merge	1.000000
that	utils check partial	0.038462
read an array using numpy	externals joblib numpy array wrapper read	0.500000
compute area under the curve auc using the	auc	0.020408
factorization nmf find	decomposition non negative factorization	0.043478
the voting classifier valid parameter	ensemble voting classifier set	0.037037
true and false positives per binary classification threshold	binary clf curve	0.090909
fits the oracle approximating shrinkage	oas fit x	0.333333
the callable case for pairwise_{distances kernels}	callable x y metric	0.083333
reducing the size	items root_path	0.066667
lfw pairs dataset this	datasets fetch lfw pairs subset	0.035714
reducer function to a given type	externals joblib customizable pickler register type	0.083333
dual gap convergence criterion the	dual gap	0.071429
cross-validated estimates	y cv	0.050000
anova f-value for	feature_selection f classif	0.200000
to a	spectral	0.026316
generate	make	0.208333
estimates	x y	0.002155
mds	mds	0.250000
low rank matrix with bell-shaped singular	make low rank matrix	0.083333
classification this function returns posterior probabilities	cv predict proba x	0.034483
determine	joblib parallel backend	0.045455
unbalanced datasets	weight class_weight classes	0.333333
centering	scaler	0.031250
names for	name	0.033333
this is the time it take to	joblib squeeze time	0.200000
indices to split data into	split split	0.250000
the given arguments	joblib memorized func	0.014706
calculate the posterior log probability	nb joint log likelihood	0.066667
factor of	factor decision	0.500000
a full lars path	path	0.025641
of neighbors	mixin radius neighbors	0.125000
vocabulary	vocabulary	1.000000
total_seconds introduced in python2	utils total seconds delta	0.200000
routine for validation and conversion	dtype csr_output	0.166667
for full covariance	multivariate normal density full x means covars	0.166667
and transform with the	transform x	0.016949
the directory in	get output dir	0.047619
given file as a	externals	0.005747
format check	check	0.017857
using the	shape repr	0.013699
em update	em step	0.500000
using a single binary	core predict binary	0.200000
a sparse format well suited for eigenvalue decomposition	value norm_laplacian	0.142857
loader for the	data_home download_if_missing	0.200000
along an axis on a	axis x axis	0.083333
given type in the	customizable pickler register type	0.333333
multi-output variable	core multi output estimator	0.142857
x	x n_neighbors	0.500000
compute the loss of	ensemble loss	0.166667
an array	check array array accept_sparse	0.250000
absolute value to be used for later scaling	abs scaler fit x y	1.000000
func to be run	pool manager mixin apply async func	0.250000
creating a class with a	externals add	0.142857
write array bytes to	joblib numpy array wrapper write array array	0.500000
global clustering for the subclusters obtained after fitting	birch global clustering	0.142857
mixin class for all meta estimators in	meta estimator mixin	0.250000
biclusters	metrics cluster consensus score	0.250000
fit kernel ridge regression model	kernel ridge fit x y	1.000000
assignment problem using the	assignment x	0.250000
compute data precision matrix	decomposition base pca get precision	0.066667
by scaling	minmax scale	0.142857
compute class covariance	class cov x y priors	0.250000
break the pairwise matrix	pairwise x y func	0.166667
estimator with randomly drawn	randomized search cv	0.166667
a minimum covariance	covariance min	0.333333
locally linear embedding	manifold locally linear embedding	0.062500
bin_queries	bin_queries	1.000000
used to fit	fit	0.003257
n_components	svd array n_components	1.000000
under a size limit	externals joblib	0.004762
construct an array array of a type suitable	feature_extraction make int array	1.000000
the estimator with the best found	base search cv predict	0.076923
diagonal of the kernel k x x	constant kernel diag x	1.000000
store the timestamp when pickling to avoid	memorized func reduce	0.050000
number of splitting iterations in the cross-validator	one group out get n splits x y	0.111111
python object from a file persisted with joblib	externals joblib load filename mmap_mode	1.000000
predict posterior probability of data under each gaussian	mixture gmmbase predict proba x	1.000000
weighted percentile of array with sample_weight	utils weighted percentile array sample_weight percentile	0.333333
read value from cache and return	externals joblib memorized result get	0.500000
dataset this operation is meant to be	index_file_path data_folder_path slice_ color	0.033333
setting the parameters for the voting classifier	ensemble voting classifier set params	0.037037
under	score	0.010101
determination regression score function	metrics r2 score y_true	0.125000
performs clustering on x and	dbscan fit predict x y	0.333333
propagation	propagation fit x y	1.000000
get the weights	neighbors get	0.125000
raise if estimator is used before fitting	not fitted error	0.500000
utility for building a cv in a user	core check cv cv x y classifier	0.031250
estimate model	fit x	0.019231
product with the	projection transform x y	0.333333
for reproducibility flips	utils deterministic vector	0.076923
computes the weighted graph of neighbors for	neighbors mixin radius neighbors graph	0.066667
fit	core fit	0.333333
remove	joblib memory	0.016949
number of points on	len	0.038462
don't store the timestamp when pickling to avoid	func reduce	0.050000
global clustering for the subclusters obtained after	birch global clustering	0.142857
the voting classifier valid parameter keys	voting classifier set params	0.037037
20 newsgroups data	20newsgroups	0.055556
used to compute log probabilities within a job	parallel predict log proba estimators estimators_features x	0.250000
of exception types to be captured	externals joblib parallel backend base get exceptions	0.166667
the training set x and	predict x y	0.043478
for building a cv in a	check cv cv	0.031250
model	linear model	0.090909
standardize a dataset along any axis center	axis with_mean with_std	0.333333
corresponding to test sets	iter test	1.000000
call transform on the estimator with	transform x	0.016949
the beta-divergence of x and dot w h	divergence x w h beta	0.500000
a reliable	externals joblib get func	0.200000
sample from	size	0.032258
problem with sparse uncorrelated design	make sparse uncorrelated n_samples n_features	0.166667
k	pairwise	0.066667
apply trees	apply	0.083333
copy of y eventually shuffle among same groups	model_selection shuffle y groups random_state	0.333333
of last step in pipeline after transforms	core pipeline fit	0.166667
the optimal batch	base compute batch	1.000000
signature bind call holds	bound arguments	0.333333
return a tolerance which is independent	tolerance	0.045455
private function used to build	build	0.037037
collect results from clf predict calls	ensemble voting classifier collect probas x	1.000000
types to	backend base	0.032258
break the pairwise matrix in	metrics parallel pairwise x y func	0.166667
classifier valid parameter keys can be listed with	classifier set params	0.125000
pool	pool	1.000000
signature from the given	signature	0.047619
for label spreading computes the graph laplacian	semi_supervised label spreading build graph	0.142857
check if vocabulary is empty	vectorizer mixin check vocabulary	0.250000
parameters for the voting classifier	voting classifier	0.035714
a sparse	utils svds a	0.166667
and return the boston house-prices	boston return_x_y	0.250000
to_write	to_write	1.000000
make cache size fit in bytes_limit	reduce size	0.083333
found and	line search	0.029412
build a process or thread pool	multiprocessing backend	0.038462
fit a binary	base sgdclassifier fit binary	0.333333
of patch data	patch extractor transform	0.200000
model to the training set x and	predict x y	0.043478
exception types	backend	0.016949
k-neighbors searches	kneighbors	0.125000
of moved objects in six moves urllib_error	module six moves urllib error	0.333333
parameters for	params	0.028571
length is not found and raise an exception	line search	0.029412
generate the random projection matrix	sparse random projection make random matrix n_components	1.000000
message on the	msg init	0.333333
estimates for each input data point	core cross val	0.043478
to fit an estimator within	fit estimator estimator	0.055556
point	val predict	0.045455
correction to raw minimum covariance determinant estimates	covariance min cov det correct covariance	0.500000
hash depending	externals	0.011494
to build a batch of estimators within	build estimators n_estimators ensemble	0.166667
hash depending from	externals joblib	0.009524
the derivative of the logistic sigmoid function	inplace logistic derivative z delta	0.166667
w to minimize the	x w ht l1_reg	0.250000
setting the parameters for the voting classifier	voting classifier set	0.037037
tolerance which is independent of	cluster tolerance x	0.058824
boost using the	boost	0.062500
strip the headers by removing everything before	strip	0.055556
estimates for each input data point	val predict estimator x y	0.045455
of array-like or scipy sparse matrix	binarize	0.045455
bytes_limit	memory	0.015625
x for a spherical	spherical x means covars	0.500000
zero	zero	0.833333
check if y is in a multilabel format	utils is multilabel y	0.333333
for building a cv in a user friendly	cv cv x y classifier	0.031250
regression target at each stage	gradient boosting regressor staged	0.500000
and cv and linearsvc	svm fit liblinear x y c fit_intercept	0.142857
finds indices in sorted array	indices tree bin_x left_mask right_mask	0.166667
for each sample	samples x	0.142857
a memmap instance to	reduce memmap a	0.050000
check according to	core check	0.111111
strip the headers by removing everything	datasets strip	0.076923
of u such that	u	0.032258
cv in a user	check cv cv x	0.031250
parameters theta as the maximizer of the	gaussian process arg max	0.047619
the weighted percentile of array with sample_weight	utils weighted percentile array sample_weight percentile	0.333333
a document-term	hashing	0.125000
write the function code and the filename	memorized func write func code filename func_code	1.000000
number of splitting iterations in	model_selection base kfold get n splits x	0.111111
the isotonic regression model :	isotonic regression	0.055556
the precision is the ratio tp /	metrics precision	0.033333
estimates for each	core cross val predict estimator x	0.045455
the local outlier factor of x (as bigger	neighbors local outlier factor decision function x	0.100000
list of feature names ordered by their	get feature names	0.090909
the callable case	pairwise callable x y metric	0.083333
function cache result and return a	externals joblib memorized func	0.013158
update reporter	reporter update	1.000000
generative	pca get	0.076923
step in pipeline after transforms	pipeline	0.083333
the curve auc using the trapezoidal	metrics auc x	0.040000
shutdown the process or thread	joblib parallel backend base terminate	0.500000
transforms features by scaling each	minmax scale x feature_range axis copy	0.200000
of the parallel execution only a fraction of	externals joblib parallel	0.014085
image from all	from	0.045455
compute the grid of alpha values	linear_model alpha grid	0.166667
transform new points into embedding	manifold locally linear embedding transform	0.500000
the test/test sizes are meaningful wrt	shuffle split n_samples test_size train_size	0.111111
the loss of	loss	0.027027
array from the meta-information and	zndarray wrapper read unpickler	0.043478
actual fitting performing	parameter_iterable	0.142857
label sets with a	preprocessing multi label binarizer transform y	0.250000
compute the l1 distances between the	paired manhattan distances	0.083333
error of the kl divergence of	kl divergence error	0.100000
n_informative	n_informative	1.000000
score for a fit across one fold	feature_selection rfe single fit rfe estimator x y	0.200000
index'th estimator	getitem index	0.333333
optimization objective for the case method='lasso' is :	y xy gram	0.090909
of the gaussian mixture parameters	mixture gaussian mixture	0.333333
the number of splitting iterations in	model_selection base cross validator get n splits x	0.125000
similarity coefficient score	score y_true	0.058824
samples	decision	0.027778
compute the l1 distances between the vectors in	manhattan distances	0.083333
lowest bound for c such that for c	c x y	0.030303
net model with iterative fitting along a	net cv	0.333333
csgraph inputs	csgraph directed	0.250000
depth	check previous func code	0.333333
convert a collection of text	vectorizer	0.022222
probability calibration with sigmoid method platt 2000	core sigmoid calibration df	0.500000
a locally linear embedding analysis	manifold locally linear embedding	0.062500
and bias vectors	backprop x y	0.200000
of possible outcomes for samples in	svm base svc predict	0.222222
backed arrays	backed	0.166667
scoring	scoring	1.000000
maximum absolute value to be used	preprocessing max abs	0.050000
the neighbors	lshforest radius neighbors x	0.166667
an estimator	estimator estimator	0.052632
for c such that for c in	c	0.022222
the score on	score x y	0.030303
call with the	call	0.052632
score of the	score	0.010101
random_init	random_init	1.000000
reproducibility flips the sign of	deterministic vector sign flip	0.066667
filters the given args and kwargs using	args func ignore_lst args kwargs	0.333333
back the	standard scaler inverse transform x copy	0.066667
for each input data	core cross val	0.043478
usage in a	used path	0.250000
the shortest path length from source	source shortest path length graph source	0.111111
number of splitting iterations in the cross-validator	group out get n splits	0.111111
fit the model	svm linear svr fit x	0.333333
indices increasingly apart	joblib verbosity filter index	0.055556
20 newsgroups data and stored it as a	20newsgroups	0.055556
validate	validate	0.714286
number of jobs	n jobs n_jobs	0.285714
feature	feature_selection get feature	1.000000
input	predict estimator x	0.045455
compute the grid of alpha values for	linear_model alpha grid x y xy	0.166667
determination regression score function	r2 score y_true	0.125000
to dense array	densify	0.066667
california housing dataset	fetch california housing	0.083333
curve auc using the	auc	0.020408
filters the given args and kwargs using a	filter args func ignore_lst args kwargs	0.333333
fit a single tree	build trees tree forest x y	0.142857
dictionary of all tokens in the raw documents	feature_extraction count vectorizer	0.125000
back the data to	preprocessing robust scaler inverse transform x	0.066667
check the validity of the input parameters	neighbors check params x metric p metric_params	0.200000
component	component	1.000000
the dual gap convergence criterion the specific	dual gap emp_cov precision_ alpha	0.071429
wild lfw pairs dataset this dataset is	fetch lfw pairs subset	0.035714
update reporter with new iteration	reporter update j est	1.000000
download	datasets download	0.500000
which should contain a partial	partial	0.043478
leaf	base decision tree apply x	0.166667
the centroids on x by	fit x y	0.005988
number of splitting iterations in the cross-validator parameters	model_selection base kfold get n splits	0.111111
inverse label binarization transformation for multiclass	preprocessing inverse binarize multiclass y classes	1.000000
under the curve auc using the trapezoidal	metrics auc x y	0.040000
estimator and predict values	and predict estimator x y train	0.200000
compute elastic net path with coordinate	linear_model enet path	0.050000
best possible score is	score y_true y_pred sample_weight	0.062500
true and false positives per	clf curve	0.250000
callable case	callable x y	0.083333
attempts to retrieve a reliable function code hash	externals joblib get func code func	0.250000
compute minimum distances between one point	metrics pairwise distances argmin x	1.000000
classification	core calibrated classifier	0.083333
loss and gradient	loss and grad w x	1.000000
remove cache folders	joblib memory reduce	0.030303
vectors individually to unit norm vector length	norm axis copy	0.200000
cross-validated estimates for each	estimator x y cv	0.050000
estimates for	predict estimator x y	0.045455
versus all others	multiclass	0.076923
an estimator predicting	prior probability estimator	1.000000
calibration curve	core calibration curve y_true y_prob	0.142857
get the weights from an	get	0.012048
the absolute error	error	0.020000
load dataset from multiple files in svmlight	datasets load svmlight files files n_features	0.500000
using the gaussian process regression model	gaussian process regressor	0.055556
the covariance matrices from a given template	match covariance type tied_cv covariance_type n_components	0.333333
label sets with	preprocessing multi label binarizer transform y	0.250000
non-negative matrix factorization nmf find two non-negative	decomposition non negative factorization x	0.043478
call predict_proba on the estimator with the best	cv	0.018018
binary classification	score y_true y_score average sample_weight	0.076923
data precision matrix with	pca get precision	0.066667
filters the	func ignore_lst	0.500000
whether the file	file	0.071429
template method for updating terminal regions (=leaves)	terminal region tree terminal_regions leaf	0.066667
cv in	cv cv x y	0.031250
generates integer indices corresponding to test sets	core partition iterator iter test indices	0.333333
generate names for estimators	core name estimators estimators	0.500000
soft boundary of the set of samples x	x y	0.002155
check input and compute prediction of init	ensemble base gradient boosting init decision function	0.142857
the right fileobject from a	joblib read fileobject	0.100000
generate train test indices	core base shuffle split iter indices	0.250000
decorator for creating a class with a	externals add	0.142857
recall the recall is the ratio tp	metrics recall	0.033333
a file	file	0.035714
factorizing common classes param	partial fit first call clf classes	0.058824
sample from	rbm sample	0.500000
don't store	joblib memorized	0.015625
c such that for c	c x y loss fit_intercept	0.030303
implement randomized linear models for feature selection this	randomized linear model	0.076923
csgraph inputs	utils sparsetools validate graph csgraph	0.250000
the training set	fit predict	0.055556
a which this function is called to issue	externals joblib memorized func	0.013158
average gradient the gradient of the loss	loss	0.027027
of the samples x to the	x	0.003384
the model according to the given	x y sample_weight	0.025974
posterior probability of	proba	0.029412
actual fitting performing the	fit x y parameter_iterable	0.500000
precomputed	precomputed	1.000000
evaluate a score by cross-validation	core cross val score estimator	0.333333
log probability	log multivariate normal	0.500000
number of splitting iterations in the cross-validator parameters	out get n splits x	0.111111
avoid	externals	0.011494
the	joblib memory	0.033898
compute	base multilayer perceptron compute	0.250000
partially fit underlying estimators should	one vs one classifier partial fit	0.166667
estimator on training subsets incrementally	model_selection incremental fit estimator estimator	0.500000
global clustering for the	birch global clustering x	0.142857
eval function func with arguments *args and	eval func	0.166667
regression target	ensemble gradient boosting regressor	1.000000
private function used to fit a single	build trees	0.142857
stratified shufflesplit cross-validator provides train/test	stratified shuffle	0.333333
load dataset from multiple files in svmlight format	datasets load svmlight files files n_features	0.500000
back the data to	preprocessing standard scaler inverse transform x	0.066667
labeled faces in the wild lfw	lfw	0.034483
of feature	feature	0.055556
helper function for factorizing common	fit first call clf	0.200000
optimal batch size	externals joblib auto batching mixin compute batch size	0.333333
inverse label binarization transformation using thresholding	preprocessing inverse binarize thresholding y output_type	1.000000
from features or	fit x y sample_weight	0.020000
adjusted for	adjusted	0.125000
set the parameters of this estimator	core feature union set params	1.000000
scores	scores	1.000000
uncompressed bytes	externals joblib binary zlib	0.125000
neighbors for	neighbors mixin radius neighbors	0.125000
rows of a csc/csr matrix in-place	utils inplace swap row x	0.250000
generate train test	core base shuffle	0.166667
cache result and return	externals joblib memorized	0.013699
inplace row	utils inplace row scale x	0.142857
incremental mean	utils incr mean	0.166667
all the content of the data home	clear data home	0.076923
intercept for specified layer	layer	0.090909
parameter weights and bias vectors	backprop x y	0.200000
fit the model	svm linear svr fit x y	0.333333
the number of estimators in	len	0.038462
dispatch them	parallel dispatch one	0.250000
iterate	iterate columns	1.000000
fit the	fit transform	0.100000
range of a	utils randomized range finder a	0.166667
logistic loss	logistic loss w	0.500000
bag predictions and score	ensemble base forest set oob score x y	0.250000
pursuit omp solves n_targets orthogonal matching pursuit	linear_model orthogonal mp x y n_nonzero_coefs tol	0.200000
min_value	min_value	1.000000
low rank matrix with bell-shaped singular	low rank matrix	0.083333
compute the recall the recall is the	metrics recall	0.033333
the end	end ll	0.166667
back the data to the original	preprocessing robust scaler inverse transform	0.066667
fit the model	estimator fit x	0.200000
the training set x y	fit x y	0.023952
shutdown the process or thread pool	joblib multiprocessing backend terminate	0.166667
get number of jobs for the	get n jobs n_jobs	0.250000
non-negative matrix factorization nmf find	non negative factorization	0.043478
rand index	rand score labels_true	1.000000
oracle approximating shrinkage covariance model according	covariance oas fit	0.083333
return a platform independent representation	shape repr	0.013699
of feature names ordered by their	get feature names	0.090909
number of splitting iterations in the cross-validator	split get n splits	0.111111
to float	decomposition beta loss to float	1.000000
lower bound for the mean parameters	bound means	0.250000
fit the hierarchical clustering on the	cluster feature agglomeration fit x y	0.250000
spherical wishart distribution parameters	wishart spherical nk xk	0.333333
computes the position of the points in	mds fit x y	0.066667
data	decomposition base	0.076923
of init	gradient boosting init decision function	0.142857
predict is invariant of	labels predict	0.250000
the number of patches that will be extracted	n patches i_h i_w p_h p_w	0.333333
learn vocabulary and idf return term-document matrix	transform raw_documents y	0.100000
remove a subcluster from	subcluster new_subcluster1 new_subcluster2	0.166667
predict	clusterer compute labels predict	0.250000
on	base	0.014286
kernel k x	exponentiation call x	0.200000
a	function	0.021277
computes the paired distances between x and y	metrics paired distances x y metric	1.000000
user provided precisions	precisions precisions covariance_type n_components n_features	0.250000
absolute error	absolute error y_true	0.142857
number of splitting iterations in	cviterable wrapper get n splits x y groups	0.111111
along any axis center	x axis	0.030769
when the metric is invalid	undefined metric	0.333333
ledoitwolf estimator ledoit-wolf is a particular form	ledoit wolf	0.111111
within a given radius	x radius	0.058824
mini	mini	1.000000
the estimator with the best found parameters	model_selection base search cv	0.120000
the neighbors within a	radius neighbors	0.043478
graph of neighbors for	mixin radius neighbors graph	0.066667
and compute scores	x y	0.004310
avoid the hash depending from it	joblib memory	0.016949
callable case	pairwise callable x	0.083333
with isotonic regression or sigmoid	calibrated classifier cv	0.071429
fit the gradient boosting model	ensemble base gradient boosting fit x y sample_weight	1.000000
random projections	random projection	0.125000
on a given test	y_test scorer	0.333333
to be captured	externals joblib parallel backend base get exceptions	0.166667
partially	partial	0.173913
a reliable function code hash	externals joblib get func code func	0.250000
the given param_grid	model_selection grid search cv get param iterator	0.166667
a subcluster	subcluster new_subcluster1 new_subcluster2	0.166667
buffered	buffered	0.750000
the first blank line	header	0.090909
the right fileobject from a filename	read fileobject fileobj filename	0.250000
computes the free energy f	bernoulli rbm free energy	0.066667
compute the deviance (= 2 * negative log-likelihood)	ensemble binomial deviance	0.333333
dummy feature	dummy feature x	1.000000
number of	n	0.150000
checker utility for building a cv	cv cv	0.031250
distances using just nearest	nn distances	0.500000
matrix factorization nmf find two non-negative	non negative factorization	0.043478
delete all the content of the data home	datasets clear data home	0.076923
returns whether	gaussian_process stationary	0.333333
path with coordinate descent the	linear_model enet path	0.050000
signature from the given list	externals signature	0.050000
local outlier	neighbors local outlier	0.142857
diagonal of the kernel k	normalized kernel mixin diag	1.000000
in parallel	externals joblib parallel backend	0.029412
sparse format	sparse coef	0.071429
number of splitting iterations in	leave pgroups out get n splits x y	0.111111
n_discard	n_discard	1.000000
number of splitting iterations in the	split get n splits x y	0.111111
its corresponding derivatives with respect to each parameter	activations deltas	0.032258
hence	decomposition sparse coder	1.000000
the backend and return the number of workers	externals joblib parallel backend	0.029412
scaler	abs scaler	0.333333
to each parameter weights and	y	0.002674
is stationary	is stationary	0.714286
that makes predictions using simple rules	dummy	0.200000
compute the maximum absolute value to be	preprocessing max abs	0.050000
for full	normal density full x	0.166667
precisions	mixture check precisions precisions covariance_type n_components	0.250000
fit the model using x as	fit x	0.012821
mapping from feature integer indices to feature name	count vectorizer get feature names	1.000000
damp	damp	1.000000
precompute_distances	precompute_distances	1.000000
partially fit a single binary estimator one-vs-one	partial fit ovo binary estimator x y i	1.000000
c	c x y	0.030303
multi-class targets using underlying	output code classifier	0.250000
the precision is	precision	0.016667
svd parameters	svd m n_components	0.500000
one set of	point x y estimator	0.500000
the derived class	resp	0.090909
with the final estimator fits all	core pipeline	0.076923
build a batch of estimators within a job	build estimators n_estimators	0.166667
fit a binary	linear_model base sgdclassifier fit binary	0.333333
train estimator on training subsets incrementally	incremental fit estimator estimator	0.500000
parallelbackend must implement	backend	0.016949
number of splitting iterations in the cross-validator	one group out get n splits	0.111111
the density model on the data	density	0.043478
w to minimize	w ht l1_reg	0.250000
of determination regression	metrics r2	0.125000
em update for	allocation em step x	0.500000
scale back the data to	standard scaler inverse transform	0.066667
maximize class separation	core linear discriminant analysis transform x	0.250000
list of	backend base get	0.066667
factorize density check according to li	check density density n_features	0.166667
compute the minimum	preprocessing min	0.166667
call wrapped function cache result and return	joblib memorized func call	0.200000
a covariance matrix shrunk on the diagonal read	shrunk covariance	0.090909
number of splitting iterations in the	leave pgroups out get n splits x	0.111111
arpack error	arpack error	1.000000
of possible outcomes for samples in x	ensemble voting classifier predict	0.100000
method for updating terminal regions (=leaves)	terminal region tree terminal_regions leaf x	0.066667
lfw pairs dataset	fetch lfw pairs subset	0.035714
fit	fit rfe	0.166667
points in x parameters	x n_neighbors mode	0.500000
to hash objects that won't normally pickle	my hash	0.333333
too rare or too common features	feature_extraction count vectorizer limit features x vocabulary high	0.250000
covariance	matrix to match covariance	0.250000
number of splitting iterations in the cross-validator parameters	predefined split get n splits x y groups	0.111111
general function given points on	x y reorder	0.111111
text files with categories as subfolder names	files container_path description categories load_content	0.500000
for full	density full x means	0.166667
multi-class	y threshold	0.166667
return a tolerance which is independent of the	tolerance	0.045455
locally linear embedding read more in the	locally linear embedding	0.050000
x	fit x	0.025641
load the kddcup99 dataset downloading it if	datasets fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
windows this is the time it take	squeeze time	0.166667
the lfw pairs dataset this operation is	fetch lfw pairs	0.018868
anova f-value for	feature_selection f classif x y	0.200000
fit linear model with	fit x	0.019231
the shortest path length	source shortest path length	0.333333
validate user provided precisions	mixture check precisions precisions	0.250000
coverage error measure	coverage error y_true y_score sample_weight	0.166667
inner fit for one	neural_network bernoulli rbm fit v_pos rng	0.250000
lad updates terminal regions to	least absolute error update terminal region	0.200000
check that predict raises an exception in	utils check	0.023810
compute class covariance	core class cov x y	0.250000
generate the random projection matrix parameters	core base random projection make random matrix n_components	1.000000
to coefs and intercept for specified layer	grad layer n_samples	0.166667
this operation is meant to be cached by	index_file_path data_folder_path slice_ color	0.033333
the bound term related to proportions	bound proportions z	0.333333
the hash	joblib memory	0.016949
output of transform is sometimes	transform	0.011236
type introduces an 'l' suffix when using the	shape repr	0.013699
the position of the points in	mds fit	0.066667
"returns the mean	multi output classifier score	0.250000
estimators that implement the	utils check partial	0.038462
compute the gradient of loss	perceptron compute loss grad	1.000000
update it with the	update	0.035714
a cv	check cv cv x	0.031250
computes the weighted graph of neighbors for points	radius neighbors mixin radius neighbors graph	0.066667
a batch of estimators within	estimators n_estimators ensemble	0.083333
call predict_proba on the estimator with the best	cv predict proba	0.068966
from file-like object until size bytes are read	read bytes fp size error_template	0.500000
shutdown the process or thread pool	terminate	0.090909
estimate the spherical wishart distribution parameters	bayesian gaussian mixture estimate wishart spherical nk xk	0.333333
actual data loading for the lfw people	fetch lfw people	0.040000
implement randomized	randomized	0.090909
a transformed real-valued array into a hash	projection to hash mixin	0.333333
avoid the hash depending from	joblib	0.014599
estimator with the best found	base search cv predict proba	0.076923
with self	x_test	0.083333
model using	linear_model base randomized linear model	0.500000
used to capture the arguments of a function	joblib delayed function check_pickle	0.333333
partially fit underlying estimators should be used when	core one vs one classifier partial fit	0.166667
classification	classifier	0.027397
multilabel classification	multilabel classification	0.166667
non-negative matrix factorization nmf find	decomposition non negative factorization x	0.043478
n_jobs even slices	func n_jobs	0.166667
posterior log probability of the samples x	bernoulli nb joint log likelihood x	0.500000
reproducibility flips the	utils deterministic vector	0.076923
data loading for the lfw pairs	datasets fetch lfw pairs	0.018868
a score by cross-validation	core cross val score estimator x y scoring	0.333333
l1-penalized covariance estimator read more	covariance graph lasso	0.166667
observations in x according to	x	0.001692
to split data into training and test	split split x y groups	0.200000
if suitable step length is not found and	search	0.019231
with the best found parameters	core base search cv	0.066667
quantiles to be used for	preprocessing robust	0.111111
bic or aic for	ic	0.111111
check x format check x format	allocation check	0.062500
log of	predict log	0.500000
randomized svd	randomized svd	0.500000
func	joblib sequential backend apply async func	0.250000
neighbors for	neighbors radius neighbors mixin radius neighbors	0.125000
with the best found parameters	model_selection base search cv	0.120000
for building a cv in a	check cv cv x	0.031250
for a	a	0.018182
the hash depending from	externals joblib memorized	0.013699
the number of splitting iterations in the cross-validator	one out get n splits x y	0.111111
gaussian data set with self	x_test	0.083333
the already fitted lsh forest	neighbors lshforest partial fit x y	0.200000
function used to fit an estimator within a	fit estimator estimator x	0.055556
the dense dictionary factor in place	dict dictionary y code verbose	0.333333
observations in x according to the fitted	x	0.001692
for building a cv in	core check cv cv x y classifier	0.031250
precision matrix	get precision	0.105263
split data into	model_selection base shuffle split split x	0.250000
coefs and intercept for specified layer	layer n_samples	0.166667
scaler	preprocessing min max scaler	0.200000
utility function opening the right fileobject from a	read fileobject fileobj	0.100000
kl divergence of	kl divergence	0.083333
a sparse	make sparse	0.125000
data onto	x ridge_alpha	0.071429
format check x	check	0.017857
matching pursuit	n_nonzero_coefs	0.090909
described in friedman [1] and breiman [2]	friedman3 n_samples noise random_state	0.166667
calibration curve	core calibration curve	0.142857
isotonic regression model : min sum	isotonic regression y	0.066667
target values for x relative to y_true	scorer call estimator x y_true sample_weight	0.200000
blup parameters and evaluates the reduced	gaussian_process gaussian process reduced	0.125000
for the models computed by 'path' parameters	linear_model path residuals x	0.250000
a wishart	wishart	0.062500
if suitable step length is not found	utils line search	0.029412
computes multidimensional scaling using smacof algorithm	manifold smacof single dissimilarities metric n_components init	0.333333
data	cross val predict estimator x y	0.045455
std	standard	0.166667
to split data into	split split x	0.250000
raises an exception in an unfitted	estimators unfitted	0.142857
is the time it take to	joblib squeeze time t	0.200000
and the gradient	and gradient w x	1.000000
scale back the data to the original	inverse transform	0.062500
a tolerance which is	tolerance x tol	0.058824
load dataset from multiple files in svmlight format	datasets load svmlight files files n_features dtype	0.500000
x y and	gaussian_process exponentiation call x y	0.200000
determinant	cov det	0.200000
hash	hash mixin	0.500000
and a set of	y axis	0.250000
values for a given dataset split	y train	0.166667
number of splitting iterations in the cross-validator	model_selection predefined split get n splits x	0.111111
compute scores for	score	0.010101
the timestamp when pickling to avoid the	memory reduce	0.030303
read value from cache	joblib memorized result get	1.000000
compute the recall the recall	metrics recall	0.033333
each	core cross val predict	0.045455
model to data matrix x and target y	multilayer perceptron partial	0.166667
for x relative	metrics threshold scorer call clf x y	0.058824
the	covariance empirical covariance get	0.166667
all the content of the data	data	0.038462
process	joblib	0.007299
timestamp when pickling to avoid the hash	joblib memorized func reduce	0.050000
downloading it if	subset data_home download_if_missing random_state	0.166667
single tree	build trees tree forest x	0.142857
generate cross-validated estimates for each input	cross val predict estimator x y cv	0.071429
base class for shufflesplit and stratifiedshufflesplit	base shuffle split	0.142857
such that for c	c x	0.030303
test vectors	core dummy	0.285714
win_characters	win_characters	1.000000
kernel k x y and optionally its gradient	gaussian_process exp sine squared call x y eval_gradient	1.000000
actually run in parallel n_jobs	n_jobs	0.023256
long type	utils	0.009709
binary classifier predicts one class versus all others	multiclass x y alpha	0.166667
creates a biclustering for x	base spectral fit x	1.000000
the index of the leaf	decision tree apply	0.166667
types	get	0.012048
length of	length	0.125000
and conversion of	csr_output	0.111111
utility for building a cv	cv cv x y	0.031250
spherical	spherical resp x nk	1.000000
coefficient	sparse coef mixin	0.083333
be captured	get exceptions	0.166667
private function used to compute log probabilities within	ensemble parallel predict log proba	0.058824
center x	cross_decomposition center scale xy x	1.000000
the other and transforms the	x y	0.002155
inplace row	utils inplace row scale x scale	0.142857
a	lsqr a	0.037037
construct a featureunion from the given transformers	make union	0.250000
predict based on an unfitted model by	predict	0.006849
for the one-vs-one multi	svm one vs one	0.050000
to run	externals joblib	0.004762
for the means	means x z	0.250000
validate x whenever one tries to predict apply	base forest validate x predict x	0.500000
wrapper for kernels	kernel	0.015625
lower	lower	1.000000
submatrix corresponding	mixin get submatrix	0.166667
in the svmlight / libsvm format into sparse	svmlight file f	0.066667
the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf x	0.125000
fall back to line_search_wolfe2 if suitable	wolfe12 f fprime xk pk	0.028571
to catch and hide warnings without visual nesting	warnings call fn	0.200000
measure the similarity of two clusterings of a	cluster fowlkes mallows score labels_true labels_pred	0.333333
the dual gap convergence criterion the specific	dual gap	0.071429
the generative	get	0.012048
check if the test/test sizes are meaningful wrt	shuffle split n_samples test_size train_size	0.111111
process	process predict	1.000000
list	backend base get	0.066667
calculates a	emp_cov shrinkage	0.500000
estimate the spherical wishart	mixture bayesian gaussian mixture estimate wishart spherical nk	0.333333
sorted array	tree bin_x left_mask right_mask	0.166667
change the default backend used by parallel inside	parallel backend backend	0.166667
given 1-d array	utils choice	1.000000
polynomial kernel between x and y : k	polynomial kernel	1.000000
sample images for	sample images	0.250000
read the array corresponding to this wrapper	joblib numpy array wrapper read unpickler	1.000000
called with the given arguments	func get	0.100000
exceptions	exceptions	0.500000
dictionary learning finds a dictionary	batch dictionary learning	0.142857
number of splitting iterations in the cross-validator	cross validator get n splits x	0.125000
dual gap convergence criterion the specific definition	covariance dual gap emp_cov precision_	0.071429
to unpickle our numpy pickles	zip numpy	1.000000
the gaussian mixture parameters	mixture gaussian mixture	0.333333
compute the gradient of loss	base multilayer perceptron compute loss	1.000000
of exception types	backend base get	0.066667
gaussian process regression model we can also	gaussian process regressor	0.055556
this score corresponds to the	score y_true y_score	0.025000
data precision matrix with the generative	decomposition base pca get precision	0.066667
x	x	0.323181
ardregression model according	linear_model ardregression	0.100000
jobs that can	jobs	0.111111
a covariance	covariance	0.014493
dataset	datasets fetch	0.600000
hess	hess	1.000000
likelihood function for	likelihood function	0.142857
from all of its patches	from patches 2d patches image_size	0.333333
model to x	x	0.001692
from x	fit x y	0.005988
return	utils shape repr	0.013699
linear assignment problem using the	linear assignment x	0.090909
and y	y alpha c	1.000000
the binary classification task	y_true y_score average sample_weight	0.076923
return the shortest path length	shortest path length	0.333333
update	neural_network base optimizer update	1.000000
with the best found	search cv predict proba x	0.076923
it	externals joblib memory	0.016949
the gaussian process	gaussian process	0.083333
perform a locally linear embedding analysis on	locally linear embedding x	0.071429
different probability thresholds note this implementation is restricted	probas_pred pos_label sample_weight	0.066667
score by cross-validation read more in the :ref	cross val score estimator x y groups	0.166667
thresholding of array-like or scipy	preprocessing binarize x	0.083333
predict on the estimator	predict	0.006849
load sample images	datasets load sample images	0.250000
the svmlight / libsvm format into sparse csr	svmlight file f	0.066667
procedure described in [rouseeuw1984]_ aiming at computing mcd	c step x n_support remaining_iterations initial_estimates	0.111111
best class label for each sample in x	vs one classifier predict x	1.000000
voting classifier valid parameter keys	voting classifier set	0.037037
whether the file was opened for writing	externals joblib binary zlib file writable	0.250000
hierarchical clustering on	cluster feature agglomeration	0.125000
multiple files	files files n_features	0.500000
on x	dbscan fit predict x	0.333333
and component wise scale	robust scale	0.125000
to split data into	model_selection time series split split	0.250000
the models computed by 'path' parameters	linear_model path residuals x y train test	0.250000
n_labels	n_labels	1.000000
compute minimum and maximum	utils min max axis	0.500000
for the	empirical covariance	0.125000
l1 distances between	manhattan distances	0.083333
the leaf that each sample is predicted as	tree base decision tree apply x check_input	0.500000
each input data	predict estimator	0.045455
log	linear_model logistic regression predict log	0.500000
the sigmoid	metrics sigmoid	0.333333
scale input vectors individually to unit	axis copy	0.166667
possible outcomes for samples in	svm base svc predict	0.222222
each input	cross val predict estimator	0.045455
number of splitting iterations in the cross-validator	group out get n splits x	0.111111
truncated	n_oversamples n_iter	1.000000
accuracy of a classification	y_true y_pred labels sample_weight	0.125000
callable case	metrics pairwise callable x	0.083333
dictionary learning finds a dictionary a	mini batch dictionary learning	0.142857
for a fit across one fold	feature_selection rfe single fit	0.200000
the number of splitting iterations in	leave one out get n splits x y	0.111111
linkage agglomerative	linkage	0.166667
the k-neighbors of a point	kneighbors mixin kneighbors	0.100000
factory	name factory	1.000000
the binary classification task	y_true y_score pos_label sample_weight	0.066667
function varies for mono and	y	0.002674
online computation	partial fit	0.111111
reduced likelihood function for the given autocorrelation	gaussian_process gaussian process reduced likelihood function	0.047619
mixin class for	mixin	0.037037
call wrapped function cache result and	joblib memorized func call	0.200000
the number of splitting iterations in	base kfold get n splits x y	0.111111
hash	memorized	0.015873
compute the l1	paired manhattan	0.333333
the kernel k	gaussian_process pairwise kernel call	0.333333
of csgraph inputs	sparsetools validate graph csgraph	0.250000
load the kddcup99 dataset downloading it	datasets fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
parameters	x y estimator parameters	0.500000
computes the free energy f v =	neural_network bernoulli rbm free energy	0.066667
exception types	base get	0.066667
search over parameters	base search cv	0.026316
linear	base sgdregressor	0.200000
scale back the data to the	robust scaler inverse transform	0.066667
feature	feature	0.666667
and compute prediction of init	ensemble base gradient boosting init decision function	0.142857
handle the callable case	metrics pairwise callable x y metric	0.083333
we don't store the timestamp when pickling	func reduce	0.050000
svmlight format this function	svmlight	0.050000
compute non-negative matrix factorization	negative factorization x	0.043478
meant to be cached by a joblib wrapper	index_file_path data_folder_path slice_ color	0.033333
a	spectral	0.026316
function and cache result or read cache	func	0.011364
user provided precisions	check precisions precisions covariance_type	0.250000
makes sure that	copy	0.062500
exception types to	parallel backend base	0.037037
locally linear embedding	manifold locally linear embedding x	0.071429
according to the given training data	y sample_weight	0.035714
project data to vectors and cluster the	project and cluster data vectors n_clusters	0.333333
indices corresponding to test sets	partition iterator iter test indices	0.333333
and variance along an axix on a	variance axis x axis last_mean last_var	0.142857
to data matrix x and target y	neural_network base multilayer perceptron partial	0.166667
regularization	regularization	1.000000
for the lfw pairs dataset this operation is	lfw pairs	0.018868
the callable case for pairwise_{distances	pairwise callable	0.083333
a transform	transform	0.011236
x	call x	0.392857
regression cv aka logit maxent	regression cv	0.200000
by using matrix product with the random matrix	random projection transform x	0.333333
an exception in an unfitted estimator	estimators unfitted name estimator	0.142857
of two clusterings of a	score labels_true labels_pred sparse	0.047619
by scaling each feature to	minmax scale	0.142857
neighbors within a given radius of	neighbors x radius	0.142857
as a sparse combination of the dictionary atoms	sparse coding mixin transform x y	0.333333
this classification dataset is constructed by taking	datasets make	0.015625
transforms features by scaling each feature to a	minmax scale x feature_range axis copy	0.200000
the solution to a sparse coding problem	sparse encode x dictionary gram	0.333333
project to the distortion introduced by a random	n_samples	0.058824
number of splitting iterations in the cross-validator	leave one group out get n splits	0.111111
input validation	accept_sparse dtype	0.200000
x y as training	x y copy_x	0.333333
coverage error measure	coverage error y_true	0.166667
print verbose message on	mixture print verbose msg init	0.666667
the similarity of two sets	score a b similarity	0.125000
context manager and decorator to ignore warnings	utils ignore warnings obj category	1.000000
this is the time it take to	externals joblib squeeze time t	0.200000
solve the isotonic regression model : min sum	isotonic regression	0.055556
actual fitting performing the search over	core base search cv fit x y parameter_iterable	0.333333
memory is inefficient to train	x y classes	0.027778
step in pipeline after transforms	core pipeline fit	0.166667
scale back the data	scaler inverse transform	0.058824
locally linear embedding analysis on	manifold locally linear embedding x n_neighbors n_components reg	0.071429
for full cases	mstep full gmm x responsibilities weighted_x_sum	0.250000
of exception types	parallel backend base	0.037037
projection to the	spectral	0.026316
to fit an estimator within	fit estimator estimator x y sample_weight	0.071429
normalize	scale normalize	1.000000
the minimum covariance determinant matrix	covariance fast mcd	0.250000
[rouseeuw1984]_ aiming at computing	n_support remaining_iterations initial_estimates	0.111111
dense array	mixin densify	0.100000
store the timestamp when pickling to	reduce	0.034483
of estimators within a job	estimators n_estimators	0.083333
the svmlight / libsvm	svmlight file f n_features dtype multilabel	0.066667
run fit on the estimator with randomly drawn	randomized search cv fit x y	0.500000
fit the model	model fit	1.000000
with categories as subfolder names	container_path description categories load_content	0.500000
dataset is constructed	datasets	0.015152
fit the model using x as training	manifold tsne fit x skip_num_points	0.500000
fit linear model with passive aggressive algorithm	passive aggressive regressor fit x y coef_init	1.000000
tied covariance matrix	covariances tied resp x	1.000000
assumes x contains only categorical features	one hot encoder fit transform x	1.000000
replace=true p=none) generates a random sample from	size replace p	0.125000
or thread	joblib	0.007299
the normalized	spectral	0.026316
filters the given args and kwargs using a	args func ignore_lst args kwargs	0.333333
best found parameters	model_selection base search cv	0.120000
that for c in (l1_min_c infinity) the model	c x y	0.030303
median absolute error regression loss read more	median absolute error y_true	0.166667
shutdown the process or thread	base terminate	0.500000
em algorithm and	gmmbase	0.062500
paired cosine distances between x and y	paired cosine distances x y	0.333333
check that predict is invariant of	utils check clusterer compute labels predict	0.250000
1 if dtype of x and y is	x y	0.002155
minimum and maximum	min max	0.250000
shortest path length	shortest path length graph	0.333333
fit	fit parameter	1.000000
reproducibility flips the sign of elements	deterministic vector sign flip	0.066667
indices to split data into training and test	split split x y groups	0.200000
a binary classifier	binary	0.031250
log probability for	mixture log multivariate normal density	0.250000
for tests involving both blas calls and	with blas func	0.333333
return the directory in which are persisted	get output dir	0.047619
fit the rfe model and then the	rfe fit x y	0.250000
estimates for each input data point	val	0.037037
should be used when memory is inefficient to	y classes	0.027778
data x with ability to accept precomputed	precomp distr x	0.333333
right fileobject from a filename	joblib read fileobject fileobj filename mmap_mode	0.250000
the maximizer	gaussian process arg max	0.047619
component wise scale	preprocessing robust scale	0.125000
labeled faces in the wild lfw pairs dataset	fetch lfw pairs	0.018868
low rank matrix with bell-shaped singular values most	low rank matrix	0.083333
grid	grid	0.400000
estimate sample weights by class for	utils compute sample	0.100000
for full covariance	multivariate normal density full x means covars min_covar	0.166667
aggressive regressor read more	aggressive regressor	0.166667
function for the given	function	0.021277
gaussian and label	make gaussian	0.125000
dispatch	dispatch	0.666667
to make cache size fit	reduce size	0.083333
locally linear embedding analysis	manifold locally linear embedding	0.062500
find two non-negative matrices w h	w h n_components	0.038462
format	sparse coef	0.071429
scale back the data to	scaler inverse transform	0.058824
outlier on the training	outlier factor fit predict	0.200000
model from data in	manifold spectral	0.111111
cv in a user friendly	check cv cv x y classifier	0.031250
func to be run	joblib parallel backend base apply async func	0.250000
score for a fit	fit rfe estimator	0.166667
to the cache for the function	memorized func	0.016949
build a batch of estimators within a job	parallel build estimators n_estimators	0.166667
estimator with the best found	model_selection base search cv predict	0.076923
svr	svr	0.714286
the cache for the	joblib memorized	0.015625
terminal regions (=leaves)	function update terminal region tree terminal_regions leaf x	0.200000
cross-validated least angle regression model read more	lars cv	0.333333
generate a random multilabel classification problem	make multilabel classification n_samples n_features	0.500000
the weighted graph of	graph	0.063830
regression or lasso path using lars algorithm [1]	linear_model lars path	0.100000
check initial parameters of	mixture base mixture check parameters x	0.200000
maximizer of the reduced likelihood function	arg max reduced likelihood function	0.333333
first prime element in the specified row returns	prime in row row	0.333333
k	gaussian_process pairwise	0.333333
wild lfw pairs dataset this dataset	lfw pairs	0.018868
time	time t	0.125000
cross-validated estimates for each	cross val predict estimator x y cv	0.071429
w to	w	0.035714
the kddcup99 dataset downloading it	brute kddcup99 subset data_home download_if_missing random_state	0.111111
point	val predict estimator x y	0.045455
free energy f v	bernoulli rbm free energy	0.066667
evaluation of the graph-lasso objective function the objective	objective	0.076923
sigmoid kernel between x and y : k	metrics sigmoid kernel	1.000000
find the median across axis 0	median axis 0 x	0.333333
evaluates the reduced likelihood function for the	gaussian_process gaussian process reduced likelihood function	0.047619
features	x features feature_names	1.000000
persist an arbitrary python object	externals joblib dump value filename compress protocol	0.250000
estimates for each input data	y	0.002674
and predicted probabilities for a calibration curve	calibration curve y_true y_prob normalize n_bins	0.142857
the hash	memorized	0.015873
reconfigure the	configure n_jobs	0.500000
store the timestamp when pickling to avoid the	joblib memory reduce	0.030303
last step in pipeline after transforms	core pipeline fit predict x y	0.166667
the binary classification	score y_true y_score average sample_weight	0.076923
input data	x y	0.002155
scale each feature by its maximum absolute value	max abs scaler	0.250000
a cv in a user	core check cv cv x y	0.031250
number of splitting iterations in the	model_selection leave pgroups out get n splits x	0.111111
apply decision function	analysis decision function x	0.500000
back the data to the original	scaler inverse transform x copy	0.066667
approximates the range of a	randomized range finder a	0.166667
matrices w h whose product	w h	0.031250
list of feature name -> indices mappings	feature_extraction dict vectorizer fit x y	0.250000
given arguments and persist	func call	0.047619
of exception types	joblib parallel backend	0.045455
private function used to fit a single tree	build trees tree forest x y	0.142857
faces in the wild lfw	datasets fetch lfw	0.041667
stratified k-fold cross	stratified kfold	0.200000
dense array format	densify	0.066667
gaussian process regression model	gaussian_process gaussian process regressor	0.058824
store	joblib	0.014599
with the generative model	base	0.014286
leaf that each sample is predicted as	apply x check_input	0.500000
classification used in hastie	make hastie	0.125000
an array of array from list of	array of	1.000000
perform affinity	affinity	0.142857
log probabilities	log proba	0.090909
to the average path length	average path length	0.090909
compute	pca	0.047619
20 newsgroups data and stored it as	20newsgroups	0.055556
the loss is	loss	0.027027
function output for x	x y	0.002155
for c such that for c in (l1_min_c	c x y loss fit_intercept	0.030303
generate indices to split data into	predefined split split x	0.250000
remove cache folders	externals	0.005747
partially fit	partial fit x	0.500000
to the given training data and	x y	0.002155
for the precisions	precisions x z	0.250000
precision ap from prediction scores this	precision	0.016667
clustering for the subclusters obtained after	clustering x	0.142857
long type introduces an 'l' suffix when using	utils	0.009709
number of splitting iterations in the cross-validator parameters	model_selection predefined split get n splits x	0.111111
the callable case for	callable x	0.083333
with x	x	0.003384
model using	linear_model lasso lars	1.000000
project data to vectors and cluster	cluster spectral biclustering project and cluster data vectors	0.333333
the solution to a sparse coding problem	decomposition sparse encode x	0.333333
of a cross-validated	x y cv	0.050000
area under the curve auc using the	auc x	0.040000
area under the curve auc using the trapezoidal	auc x	0.040000
compute the euclidean or frobenius norm of x	utils norm x	0.333333
the lfw pairs dataset this operation	datasets fetch lfw pairs	0.018868
locally linear embedding analysis on the	locally linear embedding	0.050000
area under the curve auc	auc x y	0.040000
set the parameters of this estimator	set params	0.250000
and intercepts from	neural_network base multilayer perceptron unpack	0.250000
fitted model	fit x y	0.005988
reconstruct the image from all of	feature_extraction reconstruct from	0.333333
data loading for the lfw pairs dataset	lfw pairs	0.018868
types	backend base get	0.066667
the callable case for	callable x y metric	0.083333
linkage agglomerative clustering	cluster linkage tree	1.000000
probabilities of possible	proba	0.029412
generate random samples from	neighbors kernel density sample n_samples random_state	1.000000
handle the callable case for	metrics pairwise callable x y metric	0.083333
fit the model	core multi output estimator fit x y sample_weight	0.200000
matrix m	m k k_skip	1.000000
and intercept for specified layer	grad layer	0.166667
regression randomized logistic regression works by subsampling the	randomized logistic regression	0.166667
matrix factorization nmf	negative factorization	0.043478
the gaussian process regression model we can	gaussian process regressor	0.055556
discriminant	discriminant	1.000000
of the memory	externals joblib memory	0.016949
compute the unnormalized posterior log probability of	core base nb joint log likelihood	0.166667
the number of free parameters in the model	mixture gaussian mixture n parameters	1.000000
used by logistic regression and cv and	x y	0.002155
under the curve auc using the trapezoidal rule	metrics auc x y	0.040000
the gradient	base gradient	0.500000
timestamp when pickling	memorized func reduce	0.050000
elastic net optimization function varies for	l1_ratio	0.030303
parameter value indexing	model_selection index param value x	0.200000
check x format check	allocation check	0.062500
model using	linear_model lasso lars ic	1.000000
c in	c x	0.030303
full covariance	multivariate normal density full x means	0.166667
true and false positives per binary	metrics binary clf curve y_true	0.090909
to compute log probabilities within a job	parallel predict log proba estimators estimators_features	0.250000
is	cfsubcluster	0.111111
leave one group out cross-validator provides train/test indices	leave one group out	0.200000
dependence	dependence	1.000000
single binary estimator	binary estimator	0.363636
compute the log probability under the model	mixture gmmbase score	0.500000
global clustering	cluster birch global clustering	0.142857
in multiplicative update	decomposition multiplicative update h x w	0.500000
returns false for indices increasingly apart the	joblib verbosity filter index	0.055556
estimator is using a precomputed gram matrix	core	0.015385
with the generative	pca get	0.076923
number of splitting iterations in	model_selection cviterable wrapper get n splits	0.111111
median absolute error regression loss	metrics median absolute error	0.166667
the hash depending	joblib memory	0.016949
lfw pairs	datasets fetch lfw pairs	0.037736
the callable case	metrics pairwise callable	0.083333
and evaluates the reduced likelihood	reduced likelihood	0.100000
a contingency matrix describing the	contingency matrix labels_true	0.166667
last element of numpy	last element arr	0.142857
to compute log probabilities within a	parallel predict log proba	0.058824
normalize x by scaling rows and columns independently	scale normalize x	0.142857
the weighted graph of neighbors	mixin radius neighbors graph	0.066667
convert coefficient matrix to	coef mixin	0.090909
convert	sparse	0.025000
enet	enet	1.000000
trace of np dot x	trace dot x	1.000000
user provided precisions	mixture check precisions precisions covariance_type n_components	0.250000
descent the elastic net optimization function	l1_ratio	0.030303
partially fit a single binary	partial fit binary	1.000000
a platform independent	shape	0.011765
non-negative matrices w h whose	x w h n_components	0.038462
of x (as bigger is better	decision function x	0.018868
h whose product	h	0.041667
voting classifier valid	voting classifier	0.035714
load dataset from multiple files in svmlight	datasets load svmlight files files	0.500000
on x and y	x y	0.002155
check x format check x format and	check	0.017857
global clustering	birch global clustering	0.142857
the oracle approximating shrinkage	oas	0.100000
arbitrary python object into one	filename	0.050000
predict multi-class targets using	core output code classifier predict	0.250000
lowest bound for c such that for c	c x	0.030303
continuous target variable	y discrete_features n_neighbors	0.500000
generate an array with	make	0.083333
according to x y	x y sample_weight	0.025974
contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true labels_pred eps sparse	0.200000
of u such that	flip u	0.047619
sign	sign flip	0.066667
csgraph inputs	validate graph csgraph	0.250000
nonnegative	nonnegative	1.000000
split data into training and test	model_selection base shuffle split split x y groups	0.200000
if suitable step length is not found and	line search	0.029412
resolv_alias	resolv_alias	1.000000
number of splitting iterations in the cross-validator parameters	model_selection base kfold get n splits x y	0.111111
the number of splitting iterations in the cross-validator	split get n splits	0.111111
cf tree for the input data	cluster birch	0.090909
non-negative matrix factorization nmf find	negative factorization x	0.043478
back	inverse transform x copy	0.066667
linear model with passive aggressive algorithm	linear_model passive aggressive regressor	0.250000
model parameters with the em algorithm	mixture gmmbase fit x y do_prediction	0.250000
for validation and conversion of csgraph	graph csgraph directed dtype csr_output	0.166667
compute the mean silhouette coefficient	metrics cluster silhouette score x	0.250000
generate	cross	0.037037
decision tree regressor	decision tree regressor	1.000000
in multiplicative update	decomposition multiplicative update	0.666667
generate indices to split data into	base kfold split x	0.250000
type introduces an 'l' suffix when using	repr	0.012500
fit the model to	fit x y sample_weight	0.040000
memmap	memmap	0.466667
the number of splitting iterations in the	model_selection leave pgroups out get n splits x	0.111111
transform function to portion of selected features parameters	preprocessing transform selected x transform selected	0.333333
set the sample weight array	linear_model base sgd validate sample weight sample_weight	0.333333
of the log of the determinant of	log	0.018868
c such that for c in (l1_min_c	c x y loss	0.030303
the long	shape repr	0.013699
for each input data	val predict	0.045455
return staged predictions for	ensemble ada boost classifier staged predict	1.000000
and compute prediction of init	base gradient boosting init decision	0.142857
k-means clustering	cluster kmeans fit	1.000000
be merged if	merge subcluster nominee_cluster threshold	0.250000
datasets in the	datasets	0.015152
number of splitting iterations in the cross-validator parameters	kfold get n splits	0.111111
the lfw	lfw	0.068966
for the lfw people dataset this	fetch lfw people	0.040000
avoid the	joblib memorized	0.015625
the precision is the ratio	precision	0.016667
compute the boolean mask	get mask	0.333333
compute	score x y	0.030303
number of splitting iterations in the	model_selection predefined split get n splits x y	0.111111
perform a locally linear embedding analysis on the	manifold locally linear embedding x n_neighbors n_components reg	0.071429
oracle approximating shrinkage covariance model according to	covariance oas	0.083333
a random multilabel classification	datasets make multilabel classification	0.166667
ind	ind	1.000000
the model from data	manifold spectral embedding	0.111111
a tolerance which is independent of	tolerance x	0.058824
density model	neighbors kernel density	0.090909
handle the callable case for	pairwise callable	0.083333
vectorized	vectorized	1.000000
y and optionally its gradient	y eval_gradient	1.000000
estimate mutual information	feature_selection mutual info regression x	0.500000
utility function opening the right fileobject	fileobject fileobj	0.200000
underlying estimators	core one vs one classifier	0.222222
generate cross-validated estimates for each input data	core cross val predict estimator x y cv	0.071429
the actual data loading for the lfw pairs	fetch lfw pairs	0.018868
with non-overlapping labels	label	0.045455
and concatenate	y	0.002674
the score on the	score	0.010101
of	joblib parallel backend base get	0.066667
of determination regression score	r2 score y_true	0.125000
count and smooth feature	bernoulli nb count x y	0.250000
make a scorer from a	make scorer	1.000000
weighted graph of neighbors	radius neighbors graph	0.066667
the user of a test that was skipped	skip test	0.200000
fit the model by computing truncated	pca fit truncated	1.000000
estimates for each input	x y	0.002155
of a classification	y_true y_pred labels sample_weight	0.125000
estimator on training subsets incrementally and compute	core incremental fit estimator estimator x y classes	0.200000
of determination regression score function	r2 score y_true y_pred	0.125000
init	shuffle split init	1.000000
is described by its spectrum spectrum	spectrum n_samples	0.166667
restricted to the binary classification	y_true y_score	0.054054
sizes	sizes	0.300000
directory in which are	output dir	0.047619
symmetric	symmetric	1.000000
number of splitting iterations in	model_selection cviterable wrapper get n splits x	0.111111
utility for building a cv in a user	cv cv x y classifier	0.031250
k-neighbors for	neighbors kneighbors mixin kneighbors	0.100000
fit the model to data	estimator fit x y sample_weight	0.200000
log-det of the cholesky decomposition	cholesky matrix_chol	0.500000
validation and conversion of csgraph	validate graph csgraph directed dtype csr_output	0.166667
the kl	kl	0.125000
the average path	ensemble average path	0.142857
the submatrix corresponding	mixin get submatrix	0.166667
handle the callable case for pairwise_{distances kernels}	pairwise callable x y	0.083333
probabilities of possible outcomes for samples	ensemble voting classifier predict proba	0.200000
derivative of the logistic sigmoid	inplace logistic derivative z delta	0.166667
0 to n	n	0.050000
generate	val	0.037037
class versus all others	multiclass x	0.166667
parameters theta as the maximizer of	arg max	0.047619
absolute error regression loss read more in the	absolute error y_true	0.142857
to build a batch of estimators within	ensemble parallel build estimators n_estimators	0.166667
create all the covariance	covar matrix to match covariance	0.250000
dict_type	dict_type	1.000000
curve auc from prediction scores note this implementation	metrics roc auc score	0.166667
reconfigure	base configure	0.500000
with	cv	0.018018
new points into	manifold locally linear	0.250000
of a csc/csr matrix in-place	utils inplace swap row x m	0.250000
row scaling of a	row	0.066667
transforms and	y	0.002674
the function call with the given arguments	joblib format call func	0.100000
for c in (l1_min_c infinity) the	c x y	0.030303
average	ensemble average	0.125000
to by some authors as	preprocessing label binarizer	0.071429
for each input data point	predict estimator x	0.045455
to avoid the hash depending from it	memorized	0.015873
utility for building a cv	core check cv cv x y classifier	0.031250
and return the content as a	externals joblib	0.004762
download_if_missing	download_if_missing	1.000000
class or regression value for	check_input	0.100000
predict_proba on the estimator with the best found	base search cv	0.052632
handle the callable case for pairwise_{distances	callable	0.058824
is restricted to the binary classification task	metrics precision recall curve y_true	0.142857
filters the given args and kwargs	externals joblib filter args func ignore_lst args kwargs	0.333333
linear model with stochastic gradient descent	linear_model base sgdregressor partial	0.333333
the data home cache	datasets clear data home data_home	0.076923
don't store the	joblib	0.014599
l1_ratio	l1_ratio	0.151515
log probabilities within	ensemble parallel predict log proba	0.058824
w to minimize	x w ht	0.250000
fit an estimator within a	fit estimator estimator	0.055556
build a text report showing the main	report	0.047619
value	value x v indices	1.000000
predict class at each stage	classifier staged predict	0.333333
kernel between	kernel	0.015625
function for factorizing common classes param logic estimators	partial fit first call clf classes	0.058824
__validate_parameters__	__validate_parameters__	1.000000
base class for forests of	base forest	0.333333
neighbors within a given radius of a	neighbors x radius	0.142857
for each input data point	cross val	0.038462
the parameters for the voting classifier valid	voting classifier set	0.037037
curve auc	metrics auc x y	0.040000
nonzero componentwise l1 cross-distances between the vectors in	gaussian_process l1 cross distances	0.111111
split data into	split split	0.250000
a continuous tie-breaking ovr decision function	utils ovr decision function predictions confidences n_classes	0.333333
fit the model to data	output estimator fit	0.200000
neighbors within	neighbors lshforest radius neighbors x	0.166667
possible outcomes	voting	0.066667
be used when memory is inefficient to train	y classes	0.027778
inplace row scaling of a	inplace row scale x scale	0.142857
split data into	model_selection predefined split split x	0.250000
the case method='lasso' is :	y xy gram	0.090909
run fit on the estimator with randomly drawn	randomized search cv fit x	0.500000
lfw pairs dataset this	fetch lfw pairs	0.037736
with	with	1.000000
routine for validation and conversion of csgraph inputs	csgraph directed dtype csr_output	0.166667
numerator	numerator	1.000000
to dense array format	densify	0.066667
predict using the trained model	base multilayer perceptron predict	0.333333
the number of splitting iterations in the cross-validator	base kfold get n splits x y groups	0.111111
check to make sure weights are valid	check weights weights	1.000000
least squares	least squares	1.000000
online	partial fit x y	0.500000
and perform dimensionality reduction on	y	0.002674
used when memory is inefficient to train	y classes	0.027778
sparsify	sparsify	1.000000
inplace column scaling	utils inplace column scale	0.166667
the wild lfw pairs dataset this dataset	lfw pairs	0.018868
x and dot w h	divergence x w h	0.500000
dense dictionary factor in	dictionary	0.071429
a given radius of a point	radius	0.045455
from data	manifold spectral embedding	0.111111
don't	externals joblib	0.009524
buffered	externals joblib buffered	0.333333
sample from a given 1-d array	utils choice a size replace p	0.250000
the search over	core base search cv fit x y	0.166667
new data by linear interpolation	core sigmoid calibration	1.000000
self	x_test y	0.142857
transform function to portion of selected features	transform selected x transform selected	0.333333
matrix shrunk on the diagonal read more in	shrunk	0.043478
can also predict based on	predict x	0.011765
number of splitting iterations in the cross-validator	out get n splits x	0.111111
fits the graphlasso covariance model to	covariance graph lasso cv	0.111111
the sign	sign	0.050000
seeds for	bin seeds x	0.250000
each input data	val predict estimator	0.045455
of observations in x according to the	x	0.001692
restricted to the binary classification task	y_true y_score average sample_weight	0.076923
return parametergrid instance for the given param_grid	get param iterator	0.166667
indices corresponding to test sets	core partition iterator iter test indices	0.333333
compute class covariance	core class cov x y priors shrinkage	0.250000
the california housing dataset	datasets fetch california housing	0.083333
list	joblib	0.007299
compute	decomposition base pca	0.071429
prediction of init	gradient boosting init decision function x	0.142857
to be used for later	fit x	0.006410
score with	score	0.010101
get the	ensemble voting classifier get	0.200000
residues on left-out data	residues x_train y_train x_test y_test	0.083333
for a fit across one fold	feature_selection rfe single fit rfe	0.200000
store the timestamp when pickling to avoid	reduce	0.034483
compute the decision function of	gradient boosting classifier decision function	0.166667
project data to vectors and cluster	spectral biclustering project and cluster data vectors n_clusters	0.333333
a batch of estimators within	estimators n_estimators ensemble x y	0.083333
california housing	datasets fetch california housing	0.083333
a fit	fit rfe estimator x	0.166667
to the cache for the	externals joblib memorized func	0.013158
is restricted to the binary classification task	y_true y_score average	0.076923
error regression loss read more in the	error y_true y_pred	0.125000
we don't store the	joblib	0.014599
number of splitting iterations in	kfold get n splits x y	0.111111
construct a featureunion from the given	make union	0.250000
isotonic regression model : min	isotonic regression y	0.066667
a reducer function to a given type in	externals joblib customizable pickler register type	0.083333
the scaler	abs scaler	0.333333
back the data to the	robust scaler inverse transform	0.066667
not found and raise an exception	search	0.019231
helper function to output a function	externals joblib function called str function_name	0.250000
generate an array with block	make	0.041667
opposite of the local outlier factor of	neighbors local outlier factor	0.125000
compute elastic net path with coordinate descent the	linear_model enet path	0.050000
undo the scaling of x according	min max scaler inverse transform x	0.250000
with the given arguments	externals joblib memorized func get	0.125000
computes the maximum likelihood covariance estimator parameters	covariance empirical covariance	0.071429
decision function of x	ensemble gradient boosting classifier decision function x	0.333333
back the data to the original representation parameters	standard scaler inverse transform x copy	0.066667
friedman [1] and breiman [2]	friedman3 n_samples noise	0.166667
compute the decision	ensemble gradient boosting classifier decision	0.333333
leave-p-label_out cross-validation iterator	leave plabel out	1.000000
linear assignment problem using the hungarian algorithm	utils linear assignment	0.090909
lfw pairs dataset	lfw pairs subset	0.035714
sine	sine	1.000000
function output for x relative	metrics threshold scorer call clf x y sample_weight	0.058824
huber loss and	linear_model huber loss and	0.166667
hash	joblib memory	0.016949
fit a binary classifier on x and y	sgdclassifier fit binary x y	1.000000
mmap	mmap	0.833333
the number of splitting iterations in the cross-validator	leave pgroups out get n splits x y	0.111111
the validity of the input	x metric p metric_params	0.100000
the lfw pairs dataset	datasets fetch lfw pairs	0.018868
the specified row returns	row row	0.166667
detects the soft boundary of the	svm one class svm fit	0.125000
estimates the minimum covariance	covariance	0.014493
random matrix given	random	0.058824
features by scaling each feature to	min max scaler	0.083333
train test indices	iter indices	0.250000
regularization	pos_class cs	0.166667
mixin	mixin	0.259259
data augmented with n_zeros for the given rank	elem at rank rank data n_negative n_zeros	1.000000
perform dbscan clustering from	cluster dbscan x eps min_samples metric	0.200000
estimate the precisions parameters of the precision	mixture bayesian gaussian mixture estimate precisions	0.166667
nothing and return the estimator unchanged	feature_extraction	0.037037
read an array using numpy memmap	numpy array wrapper read mmap unpickler	1.000000
n_x	n_x	1.000000
fit the	svc fit x	0.333333
to the data x which should contain a	x y	0.002155
log-transformed bounds on the	exponentiation bounds	0.333333
compute prediction of init	base gradient boosting init decision function x	0.142857
rand index adjusted for chance	metrics cluster adjusted rand	0.333333
the number of splitting iterations in the	leave one group out get n splits	0.111111
class at each stage for	boosting classifier staged	0.500000
the 20 newsgroups dataset and	datasets fetch 20newsgroups	0.333333
r^2 coefficient of determination regression score function	metrics r2 score y_true	0.125000
whether y	y	0.002674
iterate over	feature_selection iterate columns x	1.000000
the descriptors of a memmap instance to reopen	joblib reduce memmap a	0.050000
count and smooth	nb count x y	0.250000
of a cross-validated score	score estimator x y cv	0.083333
utility function opening the right fileobject from a	joblib read fileobject	0.100000
estimates for each input data point	predict estimator	0.045455
base class for logging messages	logger	1.000000
dual gap convergence criterion the	dual gap emp_cov	0.071429
convert coefficient matrix to	coef	0.058824
seekable	seekable	0.833333
incrementally fit the model to	core multi output regressor partial fit x	0.200000
the timestamp when pickling to avoid the	externals joblib memory reduce	0.030303
multinomial loss and class	multinomial loss w x y	0.333333
test	iter	0.050000
hash	joblib	0.014599
returns the coefficient of determination r^2	multi output regressor score x	0.200000
to	externals joblib parallel backend base	0.034483
names from	names	0.090909
under a	externals	0.005747
store	externals joblib memorized func	0.013158
model fitting method	fit x	0.006410
on an array list sparse matrix or similar	array array accept_sparse dtype order	0.500000
gaussian and label samples	gaussian	0.029412
spectral biclustering	spectral	0.026316
the logistic	neural_network inplace logistic	0.333333
the lfw people dataset	lfw people	0.040000
for the lfw pairs dataset this operation	datasets fetch lfw pairs	0.018868
of x and dot	beta divergence x	0.250000
skip test	check skip	0.500000
function used to fit an estimator within a	fit estimator estimator x y	0.071429
referred to by some authors	preprocessing label binarizer	0.071429
function used to partition estimators between jobs	ensemble partition estimators n_estimators n_jobs	0.200000
implement a single boost	boosting boost iboost	1.000000
of estimators within a	estimators n_estimators ensemble x	0.083333
described in [rouseeuw1984]_ aiming at computing	c step x n_support remaining_iterations initial_estimates	0.111111
for building a cv in a user	core check cv cv	0.031250
return the disk	externals joblib disk	1.000000
signature from the given	externals signature	0.050000
isomap	isomap	0.833333
for	core	0.015385
the coefficients and intercepts from packed_parameters	neural_network base multilayer perceptron unpack packed_parameters	0.250000
of classification this function returns posterior probabilities	cv predict proba x	0.034483
a mask to edges weighted	mask edges weights	0.333333
for the means	means	0.076923
matern	matern	0.833333
compute elastic net path with coordinate descent	enet path x	0.050000
rank matrix with bell-shaped singular values most	rank matrix	0.166667
for the one-vs-one multi class	svm one vs one	0.050000
using thresholding	thresholding	0.142857
a nicely formatted statement displaying the function call	joblib format call func args kwargs object_name	0.333333
types to	joblib parallel backend base	0.058824
to update terminal regions	update terminal regions tree x	0.500000
position of the	mds fit x	0.066667
impute all missing values in	preprocessing imputer transform	0.500000
format check x	latent dirichlet allocation check	0.062500
false positives per binary classification	metrics binary clf curve y_true	0.090909
estimators that	utils check	0.023810
estimator and	y sample_weight	0.017857
the kddcup99 dataset downloading it if	kddcup99 subset data_home download_if_missing random_state	0.111111
the model from data in	manifold spectral	0.111111
float32 then dtype float32 is returned	metrics return float dtype	0.250000
a sparse random matrix	utils random	0.333333
for c such that for c in	c x y loss	0.030303
the long type introduces an 'l' suffix when	shape	0.011765
on the training	fit	0.003257
determination r^2	multi output regressor score x	0.200000
shortest path length	source shortest path length graph	0.333333
of last step in pipeline after transforms	core pipeline fit predict x	0.166667
the maximum absolute value to be used	max abs	0.047619
called with the given arguments	memorized func get	0.125000
callable case for	callable	0.058824
compute the brier score	metrics brier score loss y_true y_prob sample_weight pos_label	0.333333
perform classification on test vectors	core dummy classifier predict	1.000000
func to be run	backend base apply async func	0.250000
function used to build a batch of	parallel build	0.047619
full covariance	multivariate normal density full x means covars	0.166667
finds indices	matching indices	0.250000
the right fileobject from a filename	externals joblib read fileobject fileobj filename mmap_mode	0.250000
homogeneity metric of a cluster labeling given a	metrics cluster homogeneity score labels_true	0.500000
the model parameters of the	mixture base mixture	0.111111
transform binary labels back to multi-class labels	binarizer inverse transform y threshold	0.333333
the search over	base search cv fit	0.166667
unnormalized	unnormalized	1.000000
coverage error measure compute how	coverage error y_true y_score sample_weight	0.166667
to the cache for the function	memorized	0.015873
sigmoid	metrics sigmoid	0.333333
minimum and	utils min	0.250000
for	estimator	0.014706
filters the given args and	filter args func ignore_lst args	1.000000
generate train test	core base shuffle split	0.166667
varies for mono and	y	0.002674
predict labels for	predict	0.006849
theta as the maximizer of	arg max	0.047619
concentration	concentration	0.625000
compute the recall the recall is the	recall	0.028571
and dispatch them	parallel dispatch one	0.250000
other and transforms the	x y	0.002155
the curve auc from prediction	auc score	0.052632
set the	core pipeline set	0.250000
graph of the	feature_extraction grid to graph	0.333333
content of the data home	clear data home	0.076923
passive	passive	0.529412
utility for building a cv in	cv cv	0.031250
measure the similarity of two clusterings	fowlkes mallows score labels_true labels_pred sparse	0.333333
predict multi-class targets using	output code classifier predict	0.250000
transform on the	transform x	0.016949
fit gaussian naive bayes according to x y	gaussian nb fit x y sample_weight	1.000000
fit the kernel density model on	neighbors kernel density fit x y	0.250000
the number of splitting iterations in	leave pgroups out get n splits x y	0.111111
evaluate a score	score estimator x y scoring	0.333333
build	build	0.259259
patches of	extract patches	0.083333
randomized svd parameters	randomized svd	0.500000
to split data into	series split split	0.250000
locally linear embedding analysis on the	locally linear embedding x n_neighbors n_components	0.071429
function to portion of selected features parameters	selected copy	0.500000
fit	output estimator fit x y sample_weight	0.200000
elastic net path with coordinate descent	linear_model enet path	0.050000
the data in the given file as	data compress	0.100000
data	core cross val predict estimator x y	0.045455
of a memmap instance to reopen on	reduce memmap a	0.050000
the function called with the given arguments	memorized func get	0.125000
maximizer of the reduced likelihood	arg max reduced likelihood	0.250000
detects the soft	svm one class svm fit	0.125000
y_discrete	y_discrete	1.000000
samples from	sample	0.032258
store	func	0.011364
matrices	covariance_type	0.166667
sizes of training subsets and validate 'train_sizes'	core translate train sizes train_sizes n_max_training_samples	0.500000
estimates for each input data point	core cross val predict estimator x	0.045455
meta estimators in	meta estimator	0.062500
analyzer	analyzer	1.000000
the array corresponding to	externals joblib numpy array	0.250000
function opening the right fileobject from a	joblib read fileobject	0.100000
estimate the precisions parameters of the precision distribution	mixture bayesian gaussian mixture estimate precisions nk	0.166667
random multilabel	multilabel	0.111111
we don't store the	externals joblib memorized func	0.013158
estimator on training subsets incrementally and compute scores	core incremental fit estimator estimator x y classes	0.200000
like d	d	0.166667
estimates the shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage x	0.125000
indices to split data into	time series split split x	0.250000
is the solution to a sparse coding problem	sparse encode	0.333333
for c in	c x y loss	0.030303
kernel k x y and	call x y	0.142857
fit	svm linear svr fit x	0.333333
covertype dataset downloading it if necessary	datasets fetch covtype data_home download_if_missing random_state shuffle	0.333333
isolation forest algorithm return the	isolation forest	0.200000
net path with	linear_model enet path x	0.050000
scale back the	robust scaler inverse transform x	0.066667
normalize x	normalize x	0.076923
least squares projection of the data onto	x ridge_alpha	0.071429
the kernel is	kernel is	0.400000
fit the rfe model	rfe fit x	1.000000
threshold value	importances threshold	0.500000
for c in (l1_min_c infinity)	c x y loss fit_intercept	0.030303
input	core	0.015385
initialize the univariate feature selection	base filter	0.333333
estimate model parameters	fit x	0.006410
building a cv in a user friendly	check cv cv x	0.031250
labeled faces in the wild lfw pairs	datasets fetch lfw pairs	0.018868
log-likelihood of a gaussian data set	score	0.010101
or thread pool	backend	0.033898
generate cross-validated estimates for each input data	cv	0.009009
minimum distances between one point and	pairwise distances argmin x y axis	0.333333
loss for regression	loss y_true y_pred	1.000000
recall is the ratio tp /	metrics recall	0.033333
covariance matrices from a given template	match covariance type tied_cv covariance_type n_components	0.333333
classification this function returns posterior probabilities of classification	calibrated classifier cv predict proba	0.200000
oracle approximating shrinkage covariance model according	covariance oas	0.083333
update h	x w h beta_loss	0.500000
back the data to the original representation	preprocessing standard scaler inverse transform x copy	0.066667
the leaf	decision tree apply x	0.166667
template method for updating terminal	function update terminal	0.200000
to avoid the hash depending from	memory	0.015625
log-det of the	matrix_chol	0.125000
handle the callable case	metrics pairwise callable	0.083333
gaussian mixture model	gmmbase	0.062500
number of splitting iterations in	leave pgroups out get n splits	0.111111
mean shift clustering	cluster mean shift	0.500000
for	core cross	0.045455
measure the similarity of two clusterings of a	fowlkes mallows score labels_true labels_pred sparse	0.333333
fit the model to	core multi output estimator fit x y	0.200000
signature from the given list of parameter	signature	0.047619
at each stage for	staged	0.250000
factorization nmf find	factorization	0.035714
fit the rfe model	rfe fit	1.000000
number of jobs for the computation	n jobs n_jobs	0.142857
the covariance matrices from a given template	to match covariance type tied_cv covariance_type n_components	0.333333
private function used to partition	ensemble partition	0.200000
build a	parallel build	0.047619
reporter	verbose reporter	0.500000
and a set of points	x y axis metric	1.000000
shutdown the process or thread	externals joblib multiprocessing backend terminate	0.166667
to be used for later scaling	scaler fit x y	0.200000
fetch an mldata	datasets fetch mldata	1.000000
data home cache	clear data home data_home	0.076923
mse for the models computed by 'path' parameters	linear_model path residuals x y train test	0.250000
number of splitting iterations in the cross-validator parameters	one out get n splits x y groups	0.111111
filters the given args and	externals joblib filter args func ignore_lst args	1.000000
center	robust scaler	1.000000
predictions using a single binary estimator	predict binary estimator	0.200000
function called with the given arguments	memorized func get output	0.125000
precisions parameters of	precisions nk xk sk	0.166667
input	estimator x y	0.038462
datasets	datasets	0.090909
compute labels and inertia using	labels inertia precompute dense x x_squared_norms centers distances	0.250000
a locally	locally	0.111111
tolerance which is independent of the	tolerance	0.045455
locally linear embedding	locally linear embedding x	0.071429
get the directory corresponding to the cache for	joblib memorized func get func dir mkdir	0.500000
the wild lfw pairs dataset this dataset	fetch lfw pairs subset	0.035714
shortest path length from source	shortest path length graph source	0.111111
x using	x y	0.002155
covariance matrices from a given template	covar matrix to match covariance type tied_cv covariance_type	0.333333
x y and	product call x y	0.200000
for a given dataset split	y scorer	0.111111
to capture the arguments of a	check_pickle	0.040000
corresponding to the	mkdir	0.125000
list of	externals joblib parallel	0.014085
compute the recall the recall is	recall	0.028571
x y as	x y copy_x	0.333333
x with ability	x	0.001692
to avoid the hash	externals joblib	0.009524
thresholding of array-like or scipy sparse matrix	preprocessing binarize x	0.083333
sequential	sequential	0.833333
normalized laplacian	spectral	0.026316
:ref user guide <sparse_inverse_covariance>	emp_cov alpha cov_init mode	0.200000
samples from the model	mixture gmmbase sample	1.000000
returns posterior probabilities of classification	classifier cv predict proba	0.200000
an extremely randomized tree classifier	extra tree classifier	1.000000
fit x into an embedded space	manifold tsne fit x	1.000000
store the timestamp when pickling to avoid the	joblib memorized func reduce	0.050000
the training set according to	factor fit predict	0.066667
swaps two columns of a csc/csr matrix in-place	utils inplace swap column x m	0.250000
func to	manager mixin apply async func	0.250000
range	range	0.352941
creates a	spectral fit	0.250000
computes an orthonormal matrix	size n_iter power_iteration_normalizer	0.166667
a random	n_samples	0.058824
leave one group out cross-validator provides train/test	leave one group out	0.200000
this dataset is	datasets make	0.031250
the log of the determinant of a wishart	wishart log	0.500000
coverage error measure compute how far we	metrics coverage error y_true y_score	0.166667
for unbalanced datasets	weight class_weight classes	0.333333
lad updates terminal regions	ensemble least absolute error update terminal	0.200000
the range of	randomized range finder	0.083333
reduce x to	transform x	0.016949
within a given radius of a	radius	0.045455
fit the model by computing	fit	0.003257
model evidence based on x and	x	0.001692
global clustering for	cluster birch global clustering x	0.142857
fit the model according to the	svm linear svr fit x y sample_weight	0.250000
matrix factorization nmf find two	negative factorization	0.043478
optimal batch size	auto batching mixin compute batch size	0.333333
a cv	core check cv cv x y	0.031250
by definition a confusion matrix :math c	confusion matrix	0.500000
step length is not found and raise an	line search	0.029412
ward clustering based on a feature	cluster ward tree x connectivity n_clusters	0.250000
apply clustering	clustering	0.050000
operation is meant	index_file_path data_folder_path slice_ color	0.033333
gaussian	make gaussian	0.125000
absolute error of the kl divergence	kl divergence error	0.100000
parallel processing this method is meant	joblib parallel	0.028571
from the meta-information and	externals joblib zndarray wrapper read unpickler	0.043478
the minimum covariance determinant matrix	covariance fast mcd x support_fraction cov_computation_method	0.250000
local outlier factor of	local outlier factor decision	0.125000
values for a given dataset	x y train	0.166667
determinant	det fit x y	0.333333
the directory in which are	dir	0.038462
the range	range finder	0.083333
compute the silhouette coefficient	metrics cluster silhouette	0.333333
update and a	utils	0.009709
undo the scaling of x according to feature_range	preprocessing min max scaler inverse transform x	0.250000
computes the exponential chi-squared kernel x	metrics chi2 kernel x	0.333333
class covariance	core class cov x y priors	0.250000
the pairwise matrix in n_jobs even	parallel pairwise x y func n_jobs	0.111111
explained variance regression score function	metrics explained variance	0.166667
bias vectors	backprop	0.166667
tolerance which is	tolerance x tol	0.058824
k-fold iterator variant with	group kfold	0.250000
the right fileobject from a	externals joblib read fileobject	0.100000
the kernel k x	product call x	0.200000
with respect to coefs and	n_samples activations deltas	0.166667
c such that for c in	c x y loss	0.030303
and then the underlying	y	0.002674
the training set according	fit	0.003257
collection of text documents to a	vectorizer	0.022222
predict	predict t	1.000000
find two non-negative matrices w h	x w h n_components	0.038462
returns cluster	cluster dbscan	0.125000
for each input	x	0.001692
the logistic	inplace logistic	0.333333
of points that will be sampled	model_selection parameter sampler len	0.333333
directory in which are persisted the	output dir	0.047619
of init	ensemble base gradient boosting init decision	0.142857
add documentation to a function	externals add doc func doc	0.500000
outcomes for samples	ensemble voting classifier predict	0.100000
c in (l1_min_c	c	0.022222
the dense dictionary	dictionary	0.071429
orthogonal matching pursuit step on a precomputed gram	omp gram xy n_nonzero_coefs tol_0	1.000000
weighted graph of neighbors for points in	radius neighbors mixin radius neighbors graph	0.066667
this dataset is described in friedman [1] and	datasets make	0.015625
local outlier factor of x (as bigger is	neighbors local outlier factor decision function x	0.100000
multi-task elasticnet model trained with l1/l2 mixed-norm as	multi task elastic net	0.250000
long type introduces	shape	0.011765
delete all the content of the data home	datasets clear data home data_home	0.076923
true and false positives per binary classification threshold	metrics binary clf curve y_true y_score pos_label	0.090909
the training set according	factor fit predict	0.066667
discrete_features	discrete_features	0.714286
build from the c and cpp	build from c and cpp	0.500000
derivative of the logistic sigmoid function	inplace logistic derivative z delta	0.166667
to each parameter weights and bias vectors	backprop x y	0.200000
getter for the	get	0.012048
precompute	precompute	1.000000
classification task	y_true	0.043478
from training	fit	0.003257
returns the index of the leaf	decision tree apply x	0.166667
ensure	ensure	0.833333
platform independent representation of	shape repr	0.013699
loader for the california housing	datasets fetch california housing data_home	0.250000
fit the model by computing truncated	fit truncated x	1.000000
generative	base	0.014286
predict using the kernel ridge model parameters	core kernel ridge predict	1.000000
fit ridge regression	ridge classifier fit x y sample_weight	1.000000
linear embedding read	linear embedding	0.083333
sign correction	flip u v u_based_decision	0.166667
data precision matrix with	decomposition base pca get precision	0.066667
grid of alpha values for	linear_model alpha grid x y	0.166667
a single tree	build trees tree forest x	0.142857
for factorizing common classes param logic	fit first call clf classes	0.058824
a single boost	classifier boost	0.100000
write a byte string to the file	externals joblib binary zlib file write data	1.000000
neighbors within a given radius of	neighbors lshforest radius neighbors x radius	0.142857
ledoit-wolf covariance	covariance ledoit wolf shrinkage x	0.125000
returns the score on the given data if	score x	0.033333
x y as training	x y	0.004310
base stochastic gradient descent optimizer parameters	base optimizer	1.000000
gaussian process regression model we can also	gaussian_process gaussian process regressor	0.058824
type introduces an 'l' suffix when	utils	0.009709
of edges for a 3d image	edges 3d n_x n_y n_z	0.250000
using thresholding	thresholding y output_type classes threshold	0.500000
classifier valid parameter keys can	classifier	0.013699
decision function output for x	x y sample_weight	0.012987
all the covariance	covariance	0.014493
cache and return	externals joblib memorized	0.013699
data augmented with n_zeros for the given rank	at rank rank data n_negative n_zeros	1.000000
run fit on one set of	model_selection fit grid point	0.500000
number of splitting iterations in the cross-validator	model_selection base kfold get n splits x	0.111111
fit a binary classifier on x and y	fit binary x y alpha	1.000000
edges for	edges	0.047619
nmf find two non-negative matrices w h	w h n_components	0.038462
in a multilabel format	multilabel	0.111111
shrunk on the diagonal read more	shrunk	0.043478
base	base	0.085714
python object into one	value filename	0.083333
validation of	validate	0.142857
number of splitting iterations in the cross-validator	cviterable wrapper get n splits	0.111111
parallelbackend which will execute all batches sequentially	sequential backend	0.500000
precisions parameters of	precisions nk xk	0.166667
compute elastic net path with	linear_model enet path x	0.050000
indices to split data into training and test	shuffle split split x y groups	0.200000
from source	graph source	0.200000
position	mds fit x y	0.066667
log-likelihood	decomposition pca score	1.000000
returns the bound	mixture dpgmmbase bound	0.166667
binarization transformation using thresholding	binarize thresholding y output_type classes threshold	1.000000
agglomerate features	feature agglomeration	0.333333
net path with coordinate	linear_model enet path x	0.050000
factorization nmf find two	decomposition non negative factorization	0.043478
import path as a list	resolv_alias win_characters	0.166667
the generative model	decomposition base pca	0.071429
force the execution of the function with the	externals joblib memorized	0.013699
median of data with n_zeros additional zeros	median data n_zeros	0.500000
binary classification	score y_true y_score	0.025000
extractor	extractor	1.000000
logistic regression cv aka logit maxent	logistic regression cv	0.200000
gaussian process classification based	gaussian process	0.083333
of the local outlier factor of	local outlier factor	0.125000
model according to the given training data	y sample_weight	0.035714
compute prediction of init	gradient boosting init decision function	0.142857
the posterior log probability of the samples	bernoulli nb joint log likelihood	0.083333
the samples x to the separating hyperplane	svm base lib svm decision function x	0.250000
feature names ordered by their	feature names	0.090909
memmap	mmap unpickler	1.000000
run fit with	grid search cv fit x y	0.333333
a grid of points based on the	grid from	0.166667
from data in	manifold spectral	0.111111
factor of x (as	factor decision function x	0.166667
h whose product approximates the	h	0.041667
for the	empirical	0.055556
estimate the spherical wishart distribution	bayesian gaussian mixture estimate wishart spherical	0.333333
generates boolean masks corresponding to test sets	base cross validator iter test masks x	1.000000
kernel matrix	kernel	0.015625
callable that handles preprocessing and tokenization	mixin build analyzer	0.333333
directory in	dir	0.038462
is restricted to the binary classification task	y_true y_score pos_label sample_weight	0.066667
number of splitting iterations in the cross-validator	one out get n splits	0.111111
to	coef mixin	0.090909
factorizing common classes param logic estimators	fit first call clf classes	0.058824
coverage error measure compute	coverage error y_true y_score	0.166667
class weights	class	0.071429
curve auc from prediction scores note this	roc auc	0.166667
this dataset is described in	datasets	0.030303
submatrix corresponding	submatrix	0.090909
cf tree for the input data	cluster birch fit	0.200000
the position of the points in	mds	0.050000
workers requested by the callers	effective	0.090909
extracts patches	feature_extraction extract patches	0.083333
as subfolder names	container_path description	1.000000
gradient of the loss	loss	0.027027
of the kl divergence	manifold kl divergence	0.083333
of exception types to	joblib parallel	0.028571
apply a mask to edges weighted	mask edges weights mask edges weights	0.333333
as the maximizer of the reduced	gaussian_process gaussian process arg max reduced	0.200000
param logic estimators that	utils check partial	0.038462
test	base shuffle	0.166667
shutdown the	externals joblib multiprocessing backend terminate	0.166667
predict the	predict	0.027397
the model to the data x which	x y	0.002155
non-negative matrix factorization	decomposition non negative factorization x	0.043478
in	joblib	0.007299
scale back the	inverse transform x copy	0.066667
learn empirical variances from x	feature_selection variance threshold fit x y	1.000000
run fit on the estimator	fit x y	0.005988
split data according	split	0.027778
compute the grid of alpha values for	alpha grid	0.166667
binary labels back to	binarizer inverse	0.166667
search	base search cv fit x	0.166667
of the data home cache	datasets clear data home data_home	0.076923
x	gaussian_process matern call x	0.200000
the score	score	0.050505
fit the hierarchical clustering on the data	cluster feature agglomeration fit x	0.250000
inplace column scaling	inplace column scale	0.166667
computation of max absolute value of	preprocessing max abs	0.050000
x and dot	beta divergence x	0.250000
data parameters	y	0.002674
we don't store the timestamp when pickling to	memory reduce	0.030303
string	string	1.000000
regression problem this dataset is	datasets	0.015152
update	update h x	0.500000
timestamp	timestamp	1.000000
all transformers	core feature union	0.500000
two non-negative matrices w	w	0.035714
last element of numpy array or	last element arr	0.142857
find the null	null	0.125000
func to be	backend base apply async func	0.250000
mutual information between two clusterings	metrics cluster mutual info score labels_true labels_pred contingency	1.000000
in the specified	in	0.090909
the long	utils shape repr	0.013699
the similarity of two clusterings of a	score labels_true labels_pred sparse	0.047619
embedding analysis on the	embedding x n_neighbors n_components	0.200000
angle regression or lasso path using lars algorithm	linear_model lars path	0.100000
factorization nmf find two	factorization	0.035714
in a one-vs-all fashion several regression and	y classes neg_label pos_label	0.111111
test indices	base shuffle split iter indices	0.250000
function	function x raw_values	0.250000
confusion	confusion	1.000000
this score corresponds to the area	score y_true y_score	0.025000
target_names	target_names	1.000000
return a platform independent	utils shape	0.013699
of csgraph inputs	validate graph csgraph	0.250000
predict the target of new samples	predict x	0.011765
as training	isotonic regression	0.055556
shutdown the process	backend terminate	0.166667
em update	decomposition latent dirichlet allocation em step x	0.500000
skip test	skip	0.142857
continuous	n_neighbors	0.250000
types to be captured	externals joblib parallel backend base get exceptions	0.166667
decision function for	classifier decision function	1.000000
back the data to the original representation parameters	preprocessing standard scaler inverse transform x copy	0.066667
target values for x	call estimator x	0.166667
the long type introduces an	utils shape	0.013699
the samples x to the separating hyperplane	lib svm decision function x	0.250000
randomized svd parameters	randomized svd m	0.500000
given radius of a point	radius	0.045455
predict using the	predict	0.013699
multiple files in svmlight format	svmlight files files n_features dtype multilabel	0.200000
points in	model_selection parameter	0.500000
rows of u such that	u	0.032258
the similarity of two sets of	score a b similarity	0.125000
and return the	externals joblib parallel	0.014085
descent fit is on grid	fit	0.003257
graph is connected true or not false	manifold graph is connected graph	0.500000
types	joblib parallel backend base	0.058824
cross-validated estimates for each input	y cv	0.050000
initial parameters of	parameters x	0.125000
r^2 coefficient of determination regression	metrics r2	0.125000
linear regression with combined l1 and l2 priors	elastic net	0.111111
number of points that will be sampled	parameter sampler len	0.333333
the objective function iterating once over all	coordinate descent	0.333333
scale back	robust scaler inverse transform x	0.066667
back the data	inverse transform	0.062500
continuous	y n_neighbors	1.000000
split data into training and test	kfold split x y groups	0.200000
data	y	0.010695
to dense array	sparse coef mixin densify	0.100000
local	local outlier factor local	0.500000
the proper format	base weight boosting validate x predict	1.000000
percentiles of x	x x percentiles grid_resolution	0.500000
parameters and evaluates the reduced likelihood function for	gaussian_process gaussian process reduced likelihood function	0.047619
private helper function for factorizing common classes param	partial fit first call clf classes	0.058824
sparse and dense inputs	y sample_weight random_state	0.166667
a which this function is called	externals joblib memorized func	0.013158
parallel processing this	parallel	0.019231
the wild lfw pairs dataset this dataset is	fetch lfw pairs subset	0.035714
store the timestamp when pickling to	externals joblib memory reduce	0.030303
function opening the right fileobject from a	read fileobject fileobj	0.100000
using matrix product with the random matrix parameters	core base random projection transform x	0.500000
matrix to	mixin	0.037037
n-class	n_informative n_redundant	0.333333
the decision function of x	ensemble gradient boosting classifier decision function x	0.333333
tied_cv	tied_cv	1.000000
descriptors of a memmap instance to reopen	reduce memmap a	0.050000
the content of the data home cache	clear data home	0.076923
for parallel processing this method is meant	externals joblib parallel	0.014085
the approximate feature map to	core rbfsampler transform	0.333333
the number of splitting iterations in the	get n splits x y groups	0.111111
for the lfw people dataset this operation	datasets fetch lfw people	0.040000
shrunk	covariance shrunk	0.066667
to build from the c and	build from c and	0.500000
the kernel is stationary	is stationary	0.142857
base class for random projections	base random projection	1.000000
a new subcluster into	cf subcluster subcluster	0.500000
precisions parameters	precisions nk xk sk	0.166667
the parameters for the voting	voting	0.066667
parameters	random	0.058824
computes the	y alpha	0.222222
and then the underlying estimator on	x y	0.002155
covariance model according to the	covariance	0.014493
any axis center to the	x axis	0.030769
[1] and breiman [2]	friedman3	0.090909
normalize x	cluster scale normalize x	0.142857
get the values	sgdoptimizer get	0.125000
predict	tree base decision tree predict	0.500000
an unsuccessful bst search since	n_samples_leaf	0.111111
cache folders to make cache size fit	reduce size	0.083333
the final estimator parameters	core pipeline	0.076923
data point	predict	0.006849
default backend	backend backend n_jobs	0.333333
sparse components	decomposition sparse pca transform	0.500000
the current file position	externals joblib binary zlib file tell	0.333333
em update for	decomposition latent dirichlet allocation em step x	0.500000
the model from data	manifold spectral	0.111111
long type	shape	0.011765
split data in train/test	split	0.027778
the significance of a cross-validated score with permutations	core permutation test score estimator x y cv	0.166667
m step for diagonal cases	mstep diag gmm x responsibilities weighted_x_sum	0.250000
perform dbscan clustering from vector array or	cluster dbscan	0.125000
estimate the precisions parameters of the precision	mixture bayesian gaussian mixture estimate precisions nk xk	0.166667
a regressor that makes predictions using simple rules	dummy regressor	0.333333
fit an estimator within a	fit estimator estimator x	0.055556
estimate class weights for	compute class	0.166667
curve auc using	metrics auc x	0.040000
number of splitting iterations in the cross-validator parameters	cviterable wrapper get n splits x	0.111111
the pairwise matrix	pairwise x y func	0.166667
of patch data	feature_extraction patch extractor transform	0.200000
the k-neighbors of	neighbors kneighbors mixin kneighbors x	0.125000
a single tree	build trees tree forest	0.142857
probabilities p(h=1|v)	neural_network bernoulli rbm mean hiddens v	0.333333
function code hash	get func code func	1.000000
input data	cross val predict estimator x	0.045455
api and hence	preprocessing binarizer fit x y	0.142857
linear model	linear model	0.090909
get	base randomized linear model get	0.500000
generate a random regression problem	regression n_samples n_features n_informative n_targets	1.000000
data loading for the lfw	datasets fetch lfw	0.083333
gaussian random matrix	core gaussian random matrix n_components	1.000000
gaussian process regression model we	gaussian process regressor	0.055556
helper class to repeatedly solve m*x=b	iter inv	0.200000
cache for the function	externals joblib memorized	0.013699
suffix when using the	utils shape repr	0.013699
explained by a bell-shaped curve of width	effective_rank tail_strength	0.125000
estimates the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage	0.125000
this kernel	deep	0.076923
dictionary	dictionary y	0.111111
array with block checkerboard structure for	checkerboard shape n_clusters noise minval	0.066667
the logistic loss	linear_model logistic loss w x y	0.500000
for c	c x y loss fit_intercept	0.030303
estimator with the best found parameters	base search cv predict proba x	0.076923
a random multilabel	multilabel	0.111111
inverse the transformation	agglomeration transform inverse transform xred	1.000000
pairwise	pairwise	0.333333
this implementation is restricted to the binary classification	y_true y_score pos_label	0.066667
are going to run	externals joblib multiprocessing	0.052632
updates terminal regions	terminal region tree	0.100000
estimate the precisions parameters of the	mixture bayesian gaussian mixture estimate precisions nk	0.166667
of the samples x to the separating hyperplane	base lib svm decision function x	0.250000
squared	utils squared	1.000000
generates boolean masks corresponding to test sets	model_selection base cross validator iter test masks	1.000000
compute non-negative matrix factorization nmf find two	decomposition non negative factorization x	0.043478
abstract base class for gradient	base gradient	0.250000
generate cross-validated estimates for each	x y cv	0.050000
a tolerance which is independent of the	cluster tolerance x tol	0.058824
by class for unbalanced datasets	weight class_weight	0.200000
reconstruct	externals joblib ndarray wrapper read unpickler	0.333333
to the average	average	0.066667
the oracle approximating shrinkage covariance model according	covariance oas	0.083333
the timestamp when pickling to avoid	reduce	0.034483
of x (as bigger is	x	0.001692
search over	base search cv fit	0.166667
perform dbscan clustering from vector array or	cluster dbscan x eps	0.200000
in a one-vs-all fashion several regression and binary	y classes neg_label pos_label	0.111111
list of exception	backend base get	0.066667
that x	x	0.001692
columns of a matrix	columns x columns	0.250000
generate cross-validated estimates for each input data	predict estimator x y cv	0.071429
check the	check	0.017857
an estimator implements the necessary methods	estimator estimator	0.052632
fits the graphlasso	graph lasso cv fit	0.333333
step length is not found and	line search	0.029412
the solution to a sparse coding problem	sparse encode	0.333333
estimate the precisions parameters of	bayesian gaussian mixture estimate precisions nk xk sk	0.166667
compute the median and	x y	0.002155
perceptron	perceptron	1.000000
transform a sequence of instances to a	feature_extraction feature hasher transform raw_x y	0.333333
partially fit underlying estimators should be used when	core one vs one classifier partial fit x	0.166667
display the message on stout or	parallel print	0.142857
to split data into	model_selection base shuffle split split	0.250000
class covariance matrix	class cov x y	0.250000
generate an array with block checkerboard structure for	make checkerboard shape n_clusters noise minval	0.500000
compute the gradient of loss	multilayer perceptron compute loss grad	1.000000
apply clustering to a projection to the	spectral clustering	0.142857
performs inductive inference across	semi_supervised base label propagation predict	1.000000
check	core check	0.222222
classification this function returns posterior probabilities of classification	classifier cv predict proba	0.200000
input data	predict estimator x y	0.045455
embedding	embedding x	0.200000
home	home data_home	0.500000
the long	repr	0.012500
step in pipeline after transforms	core pipeline fit predict x	0.166667
linear system of equations	b damp atol	0.200000
transform binary labels back to	preprocessing label binarizer inverse transform y	0.500000
p=none) generates a random sample from a	a size	0.142857
for reproducibility flips the sign of elements	utils deterministic vector sign	0.066667
compute incremental mean	incr mean	0.166667
covariances	covariances	1.000000
generates indices to split data into	repeated splits split x	1.000000
log of	logistic regression predict log	0.500000
update reporter	ensemble verbose reporter update	1.000000
infer	infer	1.000000
learn vocabulary and idf return term-document	fit transform raw_documents y	0.100000
return the kernel k x	gaussian_process dot product call x	0.200000
is not found	utils line search	0.029412
range approximates the range of a	randomized range finder a	0.166667
distortion introduced by a random	n_samples	0.058824
for the concentration parameter	concentration	0.125000
by scaling each feature to a given	minmax scale x	0.142857
finds indices in	indices	0.055556
read up	read	0.052632
update the bound	mixture bound	0.500000
the dual gap convergence criterion the specific	covariance dual gap emp_cov precision_ alpha	0.071429
model from data	manifold spectral	0.111111
the maximum absolute value	preprocessing max abs	0.050000
non-negative matrices w h whose	w h	0.031250
residual (=	ensemble binomial	0.250000
fit the	fit x y sample_weight	0.040000
undo the scaling of x according to	preprocessing min max scaler inverse transform x	0.250000
lfw pairs dataset	datasets fetch lfw pairs subset	0.035714
non-negative matrix factorization nmf find two	factorization	0.035714
factorization nmf find two non-negative matrices	non negative factorization x	0.043478
and	x y classes	0.055556
are selected	feature_selection selector mixin	0.142857
neighbors	lshforest radius neighbors	0.166667
labels_true	labels_true	1.000000
df	df	1.000000
log probability for full	mixture log multivariate normal density full x	0.333333
back the data to the original representation	scaler inverse transform x	0.052632
types	parallel backend base get	0.066667
to	externals	0.022989
brute	brute	1.000000
for each	estimator x	0.030303
in	externals	0.005747
and dense inputs	x y sample_weight random_state	0.166667
of transform is	transform	0.011236
factorize density check	check density density n_features	0.166667
names and a name for the	func name	0.047619
dummy	dummy	0.600000
return the score for a fit	fit rfe estimator x y	0.166667
generate cross-validated estimates for each input data point	val predict estimator x y cv	0.071429
message on the end	msg init end ll	0.333333
fit linear	sgdclassifier fit x y	0.333333
categories	categories	0.857143
fit estimator using ransac algorithm	linear_model ransacregressor fit x	1.000000
found	model_selection base search	0.250000
for building a cv in a user	core check cv cv x y	0.031250
reproducibility flips the sign of elements of all	deterministic vector sign	0.066667
class weights for unbalanced datasets	compute class weight class_weight classes	0.500000
the given file as a	externals	0.005747
fit estimator and compute scores for a	core fit and score estimator x y	0.333333
x y and scale if the scale	scale xy x y scale	0.500000
quantile this classification dataset is constructed by taking	datasets make	0.015625
the best found	search cv predict proba	0.076923
model to the training set x	predict x	0.011765
return probability estimates for	classifier predict proba	0.250000
a covariance matrix shrunk on	covariance shrunk covariance	0.090909
training and test set	y groups	0.500000
the laplacian kernel between x and y	laplacian kernel x y	0.333333
sort features by name	feature_extraction count vectorizer sort features x	1.000000
patches	feature_extraction extract patches	0.083333
loss and the gradient	loss and gradient w x	1.000000
the number of splitting iterations in the	predefined split get n splits x y	0.111111
generative	get	0.012048
backend and	parallel backend base	0.037037
kl divergence of	manifold kl divergence	0.083333
make cache size fit in bytes_limit	memory reduce size	0.083333
x y and optionally its gradient	x y eval_gradient	0.578947
data	x	0.006768
value_to_mask	value_to_mask	0.833333
coefficient matrix to	coef	0.058824
break the pairwise matrix	parallel pairwise x	0.166667
absolute sizes of training subsets and validate 'train_sizes'	core translate train sizes train_sizes n_max_training_samples	0.500000
object	filename	0.050000
decision tree regressor from	tree decision tree regressor	0.166667
used to capture the arguments of a function	delayed function check_pickle	0.333333
curve auc from prediction scores note this	roc auc score	0.166667
check initial parameters	mixture base mixture check parameters x	0.200000
estimate the precisions parameters	gaussian mixture estimate precisions nk xk sk	0.166667
used to build a batch of estimators within	build estimators n_estimators ensemble x	0.166667
predict class probabilities at each stage for	classifier staged predict proba	0.500000
check a precision vector is positive-definite	check precision positivity precision covariance_type	1.000000
restricted to the binary classification task	precision recall curve y_true	0.142857
low rank matrix with bell-shaped singular	datasets make low rank matrix	0.083333
the case method='lasso' is :	xy gram	0.090909
beta-divergence	beta	0.090909
input	val predict estimator x y	0.045455
provided precisions	check precisions precisions covariance_type	0.250000
apply decision function to	analysis decision function	0.500000
returns the number of splitting iterations in	pgroups out get n splits x y	0.111111
class_weight	targets	0.166667
grid of alpha values for	linear_model alpha grid x	0.166667
model using	linear_model lars cv	1.000000
customized copy	replace name kind annotation default	1.000000
spherical wishart	wishart spherical nk xk	0.333333
pairs dataset this dataset is a collection of	pairs	0.055556
fit the	output estimator fit	0.200000
w/ cross-validated choice of	cv	0.009009
manifold	manifold	0.500000
reconstruct	reconstruct	0.750000
initialization of the mixture parameters	mixture bayesian gaussian mixture initialize x	1.000000
beta-divergence of x and dot w h	divergence x w h beta	0.500000
calculate the posterior log probability of	bernoulli nb joint log likelihood	0.083333
transform data	transform x y	0.031250
build from	build from	0.250000
to compute log probabilities within a job	ensemble parallel predict log proba estimators estimators_features	0.250000
the number of splitting iterations in the cross-validator	get n splits x y groups	0.111111
c such that for c in (l1_min_c infinity)	c x y loss	0.030303
or thread	multiprocessing	0.045455
graph of neighbors for	radius neighbors mixin radius neighbors graph	0.066667
user of a test that was skipped	skip test	0.200000
all hyperparameter specifications	kernel hyperparameters	0.333333
get parameters of this kernel	gaussian_process kernel get params deep	1.000000
sign of elements of	sign flip	0.066667
shortest path length from source to all	single source shortest path length graph source	0.111111
estimate sample weights by class	utils compute sample	0.100000
insert	insert cf	0.500000
of feature name -> indices mappings	dict vectorizer fit x	0.250000
the california housing	datasets fetch california housing	0.083333
fit estimator and compute scores	core fit and score estimator	0.333333
factorization nmf find two non-negative matrices w h	decomposition non negative factorization x w h n_components	1.000000
make and configure a	make estimator append random_state	0.166667
x and y is float32	x y	0.002155
of equations	b damp atol	0.200000
non-negative matrix factorization nmf find two non-negative	negative factorization	0.043478
returns the index of the leaf	tree base decision tree apply	0.166667
under the model	score x	0.033333
dataset along any axis center to the mean	x axis	0.015385
random matrix	random	0.058824
non-negative matrix factorization nmf find	negative factorization	0.043478
random_state and	random_state	0.076923
coefficient score the	score	0.010101
a memmap instance to	externals joblib reduce memmap a	0.050000
fit the model according to the given	linear svc fit x y sample_weight	0.250000
of classification	calibrated classifier	0.083333
read more in the :ref user guide <sparse_inverse_covariance>	emp_cov alpha cov_init mode	0.200000
on the estimator with the best found parameters	model_selection base search cv predict	0.076923
input	cross val	0.038462
with given gradients	updates grads	0.076923
of patch	feature_extraction patch extractor	0.200000
or	joblib multiprocessing backend	0.052632
orthogonal matching pursuit	y n_nonzero_coefs tol	0.250000
tolerance which is independent of	cluster tolerance x tol	0.058824
distances between x and y read more	distances x y	0.142857
specified layer	layer n_samples	0.166667
between x and y	x y gamma	0.333333
return staged predictions for x	boost classifier staged predict x	1.000000
function for the	function	0.021277
from source to all	source	0.100000
the gaussian process regression model we	gaussian_process gaussian process regressor	0.058824
private function used	parallel decision function estimators	0.333333
suffix when using	repr	0.012500
the given args and kwargs	args kwargs	0.100000
used to capture the arguments of a	check_pickle	0.040000
and score with the final estimator parameters	core pipeline score x y sample_weight	0.500000
the time it take	squeeze time t	0.166667
of x from y	x	0.001692
for a full lars path	path	0.025641
get the parameters	voting classifier get	0.200000
an array shape under python 2 the	repr shape	0.166667
perform a locally linear embedding analysis on	manifold locally linear embedding x n_neighbors n_components reg	0.071429
function best possible score is	score y_true y_pred sample_weight	0.062500
dictionary learning finds a dictionary a set of	dictionary learning	0.142857
the kl	manifold kl	0.333333
lad updates terminal regions to median estimates	least absolute error update terminal	0.200000
the best class label for each sample in	core one vs one classifier predict	0.500000
samples by quantile this classification dataset is constructed	datasets make	0.015625
log probabilities within a job	parallel predict log proba estimators estimators_features	0.250000
an 'l' suffix	shape	0.011765
pipeline after transforms	core pipeline fit predict x y	0.166667
transform binary labels	transform	0.011236
load and return the boston house-prices dataset	datasets load boston return_x_y	0.500000
fit label encoder parameters	preprocessing label encoder fit	0.333333
all the covariance matrices from a given template	to match covariance type tied_cv covariance_type n_components	0.333333
the number of splitting iterations in	out get n splits x y groups	0.111111
number of splitting iterations in the	predefined split get n splits x y	0.111111
data if the estimator has been refit	base search cv	0.026316
from features	x y sample_weight	0.012987
shortest path length from source to all reachable	shortest path length graph source	0.111111
compute area under the curve auc using the	metrics auc	0.040000
computes the free energy f	free energy	0.066667
performance metric or loss function	score_func greater_is_better needs_proba needs_threshold	1.000000
fit on the	fit x	0.006410
the isotonic regression model : min sum	isotonic regression y	0.066667
by scaling each	preprocessing minmax scale x	0.142857
nominee_cluster	nominee_cluster	1.000000
c such that for c	c x y	0.030303
total_samples	total_samples	1.000000
for samples	classifier mixin decision function x	1.000000
dot product-based euclidean norm implementation see http //fseoane	decomposition norm x	1.000000
and inertia using	inertia precompute dense x x_squared_norms centers	0.250000
disk usage in	joblib disk used path	0.250000
estimate model parameters with the em algorithm	mixture gmmbase fit x	0.250000
score on the given data	score x	0.033333
kernel k x y and	gaussian_process constant kernel call x y	0.333333
returns the score on the	score x	0.033333
estimates for each	estimator x y	0.038462
the unnormalized posterior log probability of	nb joint log likelihood	0.033333
six moves urllib namespace that resembles	module six moves urllib	0.333333
dispatch	dispatch one	0.250000
modified weiszfeld step	linear_model modified weiszfeld step x x_old	1.000000
for building a cv in a user friendly	cv cv x	0.031250
absolute sizes of training subsets and validate 'train_sizes'	translate train sizes train_sizes n_max_training_samples	0.500000
check initial parameters of the derived	mixture base mixture check parameters	0.200000
two clusterings	score labels_true labels_pred	0.047619
to bicluster	bicluster	0.100000
with block	joblib	0.007299
fit the model to data matrix x	fit x	0.006410
factor	factor decision function	0.500000
is inefficient	x y classes	0.027778
graph of neighbors for points in x	radius neighbors graph x	0.500000
score the	score y_true y_pred normalize sample_weight	0.125000
spreading	spreading	1.000000
data onto the	x ridge_alpha	0.071429
estimators within a job	estimators n_estimators ensemble	0.083333
neighbors	neighbors lshforest radius neighbors x	0.166667
generate	val predict estimator	0.045455
terminal regions to	terminal region tree terminal_regions leaf x	0.066667
check if estimator adheres to	utils check estimator estimator	0.250000
number of patches	n patches	0.500000
to hash	hash	0.083333
scale back the data	robust scaler inverse transform	0.066667
number of splitting iterations in the cross-validator	one out get n splits x y	0.111111
used to compute log probabilities within	ensemble parallel predict log proba	0.058824
point	core cross	0.045455
index of the leaf	tree base decision tree apply x	0.166667
cache for the	externals joblib memorized	0.013699
gap	gap emp_cov precision_	1.000000
blup parameters and evaluates the reduced likelihood	gaussian process reduced likelihood	0.142857
all the covariance matrices from a given template	covar matrix to match covariance type tied_cv covariance_type	0.333333
mean update and a youngs and	utils incremental mean and	0.500000
break the pairwise matrix	pairwise x y	0.166667
of the decision functions of the base classifiers	ensemble bagging classifier decision function	0.333333
a mask	weights mask	0.333333
later	fit x	0.006410
fit a	sgdclassifier fit	0.153846
evaluate the accuracy of a classification	y_true y_pred	0.037037
estimators that implement the partial_fit	utils check	0.023810
hence	preprocessing binarizer fit x	1.000000
fit an estimator within a job	ensemble parallel fit estimator estimator	0.333333
csgraph	csgraph	0.666667
score corresponds	score y_true y_score	0.025000
squares projection of the data onto the	ridge_alpha	0.052632
matrices w h whose product approximates the	w h n_components	0.038462
determine the optimal batch	externals joblib parallel backend base compute batch	1.000000
approximate nearest neighbors	neighbors lshforest kneighbors x	0.500000
classifier for	classifier	0.013699
calculate approximate	decomposition latent dirichlet allocation	1.000000
compute minimum and	utils min	0.250000
lib	lib	1.000000
implement a single boost	classifier boost iboost x	1.000000
voting classifier valid parameter keys can	voting classifier	0.035714
median absolute	median absolute	1.000000
a	utils svds a	0.166667
train	core base shuffle split iter	0.166667
update w	w x w	0.500000
are going to	externals joblib multiprocessing	0.052632
in multiplicative update nmf	decomposition multiplicative update	0.333333
hungarian algorithm	utils hungarian cost_matrix	1.000000
fits a minimum	min	0.045455
loader for the california housing dataset from	datasets fetch california housing data_home	0.250000
the kernel	kernel	0.234375
stability	stability	1.000000
back the	preprocessing standard scaler inverse transform x copy	0.066667
returns the number of splitting iterations in the	one group out get n splits x	0.111111
update h in multiplicative update	decomposition multiplicative update h x w h	0.250000
the data x	x y	0.002155
the given training data and parameters	y	0.010695
in parallel	parallel	0.019231
compute the gradient of	multilayer perceptron compute	0.250000
model to the data x	x y	0.002155
classification dataset is constructed by	datasets make	0.015625
projection to the normalized laplacian	spectral	0.026316
the hash depending from it	externals	0.011494
routine for validation and conversion of csgraph inputs	validate graph csgraph directed dtype csr_output	0.166667
search over parameters	core base search cv fit	0.166667
with the given arguments and persist the	func call	0.047619
when memory is inefficient to	y classes	0.027778
the maximizer of the	arg max	0.047619
to split data into training and test set	model_selection predefined split split x y groups	0.200000
global clustering for the subclusters obtained after	global clustering x	0.142857
the hash depending	externals joblib memorized func	0.013158
error regression loss read	error y_true	0.111111
perform classification on an array of test vectors	nearest centroid predict	0.142857
in x as a mini-batch	mini batch dictionary learning partial fit x	1.000000
a youngs	utils incremental	0.166667
a cv	core check cv cv x y classifier	0.031250
perform mean shift clustering	cluster mean shift x bandwidth	0.500000
classification task	y_true y_score	0.027027
patches of any n-dimensional	feature_extraction extract patches	0.083333
latent dirichlet allocation with online variational bayes algorithm	latent dirichlet allocation	0.500000
x and y read more in the	x y	0.002155
with	base	0.014286
edges	make edges	0.066667
memmap instance	memmap	0.066667
list of edges	feature_extraction make edges	0.066667
polynomial kernel between x and y : k	metrics polynomial kernel	1.000000
function returns posterior probabilities of classification	core calibrated classifier cv predict proba x	0.200000
function used to build a batch	build	0.037037
the	utils shape repr	0.027397
measure the similarity of two clusterings	cluster fowlkes mallows score labels_true labels_pred	0.333333
get the	base randomized linear model get	0.500000
binomial deviance loss function for binary classification	binomial deviance	0.250000
lad updates terminal	ensemble least absolute error update terminal	0.200000
and false positives per binary classification threshold	metrics binary clf curve y_true	0.090909
f-beta score is the weighted harmonic mean of	metrics fbeta score y_true y_pred	0.333333
a locally linear embedding analysis	locally linear embedding x n_neighbors n_components reg	0.071429
found and raise an exception if	utils line search	0.029412
boosted regressor from the training set x y	ensemble ada boost regressor fit x y sample_weight	1.000000
affinity matrix for x using the	x y	0.002155
quadratic	quadratic	1.000000
the default backend used by parallel inside	parallel backend backend	0.166667
lfw pairs dataset this dataset is a	lfw pairs	0.018868
the laplacian matrix and convert it to a	laplacian	0.034483
getter	get	0.012048
apply clustering to a	spectral clustering	0.142857
labeled faces in the wild lfw	datasets fetch lfw	0.041667
of the kernel	kernel	0.078125
the local outlier factor	neighbors local outlier factor decision function	0.125000
number of splitting iterations in	get n splits x	0.111111
backend	parallel backend base	0.037037
updates terminal regions to median estimates	terminal region tree terminal_regions	0.100000
a	externals	0.091954
in multiplicative update nmf	multiplicative update h x w	0.500000
nmf find two non-negative matrices w h whose	w h	0.031250
the voting classifier	voting classifier set params	0.037037
the number of splitting iterations in	predefined split get n splits x	0.111111
initial centroids parameters	cluster init centroids x k	0.166667
a sparse matrix	a	0.018182
depending from it	joblib memory	0.016949
output of transform is sometimes referred to	transform	0.011236
memory is inefficient to train all data	classes	0.025641
run a	externals joblib	0.004762
perform dbscan clustering	cluster dbscan fit	1.000000
for	val	0.037037
features	features feature_names	1.000000
transforms features by scaling each feature to a	scale x feature_range axis copy	0.200000
shrunk ledoit-wolf	ledoit wolf shrinkage x assume_centered block_size	0.250000
kl divergence	manifold kl divergence	0.083333
the parameters for the voting classifier	ensemble voting classifier	0.031250
cross-validated estimates for each input data	x y cv	0.050000
feature	dict vectorizer get feature	0.200000
the arguments of a function	joblib delayed function	0.200000
the logistic loss and gradient	linear_model logistic loss and grad w x y	0.500000
outlier factor of x (as bigger	outlier factor decision function x	0.200000
force the execution of the	externals joblib memorized	0.013699
but fall back to line_search_wolfe2 if	wolfe12 f fprime xk pk	0.028571
pairwise matrix in	pairwise x y func	0.166667
depending from it	externals joblib memorized	0.013699
building a cv in a	cv cv x	0.031250
value	value x	1.000000
m step for full	full	0.055556
in the wild lfw pairs	datasets fetch lfw pairs subset	0.035714
each sample	one vs	1.000000
number of splitting iterations in the cross-validator parameters	out get n splits x y groups	0.111111
minimum covariance determinant matrix	covariance fast mcd x support_fraction	0.250000
the posterior log probability of the samples	multinomial nb joint log likelihood	0.083333
array is 2d square and symmetric	symmetric array tol raise_warning raise_exception	1.000000
scale back the data to	inverse transform	0.062500
the weighted graph of neighbors for points in	radius neighbors graph	0.066667
predict based on an unfitted model by using	predict x	0.011765
the number of splitting iterations in the cross-validator	leave one out get n splits	0.111111
fast	fast	1.000000
cache result and return	joblib memorized	0.015625
indices corresponding to test sets	iterator iter test indices	0.333333
diagonal of the kernel k	kernel mixin diag	1.000000
of a cross-validated	y cv	0.050000
fit label encoder	preprocessing label encoder fit	0.666667
the leaf that each sample is predicted as	base decision tree apply x check_input	0.500000
of the local	local	0.090909
returns first and last element	core first and last element arr	0.250000
byte string to the file	file	0.035714
utility for building a cv in	check cv cv x y	0.031250
return a platform	shape repr	0.013699
private function used to fit an estimator within	fit estimator estimator x	0.055556
computes the position of the points in	mds fit x y init	0.066667
apply dimensionality reduction on x	decomposition randomized pca transform x	1.000000
clustering for the subclusters obtained	clustering	0.050000
implement a single boost	boosting boost iboost x	1.000000
matrix factorization nmf find two non-negative	factorization	0.035714
two rows of a csc/csr matrix in-place	utils inplace swap row	0.250000
default backend used by parallel inside	parallel backend backend	0.166667
scale back the	preprocessing robust scaler inverse transform	0.066667
hash depending from	memorized	0.015873
usage in a directory	used path	0.250000
performs clustering on x and returns cluster	core cluster mixin fit predict x y	0.500000
graph of neighbors for points in	radius neighbors graph	0.066667
curve auc using the trapezoidal	metrics auc x y	0.040000
estimate model parameters with	fit x y do_prediction	0.166667
one after the other and transforms the	x y	0.002155
kernel k x y and	gaussian_process product call x y	1.000000
vectors in x and y	x y	0.004310
"news" format strip the headers by removing everything	strip newsgroup	0.090909
computing truncated	truncated x	0.200000
reconfigure the	base configure	0.500000
y	y	0.090909
directory in which	output dir	0.047619
estimate sample weights	compute sample	0.100000
break the pairwise matrix in n_jobs even slices	pairwise x y func n_jobs	0.111111
on the estimator with the best found parameters	search cv predict proba	0.076923
read the z-file and return the content	externals joblib read zfile file_handle	0.333333
for the voting classifier valid parameter keys can	voting classifier	0.035714
a	a	0.327273
initialization of the mixture parameters	gaussian mixture initialize	1.000000
the	parallel backend base	0.037037
greater_is_better	greater_is_better	1.000000
of data under each gaussian in the	mixture gmmbase	0.034483
perform mean shift clustering	cluster mean shift x bandwidth seeds bin_seeding	0.500000
diagonal of the kernel k x x	normalized kernel mixin diag x	1.000000
build a batch of estimators within a job	ensemble parallel build estimators n_estimators	0.166667
the output of transform is sometimes referred to	transform y	0.023256
the wishart	mixture wishart	0.125000
checker utility for building a cv in a	cv cv	0.031250
predict_log_proba on the estimator with	predict log proba	0.029412
biclustering	biclustering	0.833333
clustering on x	fit predict x	0.250000
the long type introduces an 'l'	utils shape repr	0.013699
dual gap convergence criterion the specific	covariance dual gap emp_cov precision_	0.071429
meta	meta estimator	0.062500
with_centering	with_centering	1.000000
binary classifier predicts one class versus all others	multiclass	0.076923
number of splitting iterations in the	split get n splits x y groups	0.111111
too common features	count vectorizer limit features x	1.000000
initializes the concentration parameters	mixture dpgmmbase initialize gamma	0.333333
to the binary classification	y_true y_score	0.054054
the cache for the	joblib memorized func	0.014706
cross-validated estimates	predict estimator x y cv	0.071429
break the pairwise matrix in	parallel pairwise x	0.166667
extracts patches of any n-dimensional array	feature_extraction extract patches	0.083333
scale back the data to	robust scaler inverse transform x	0.066667
process classification based on laplace approximation	process classifier laplace	1.000000
for the california	fetch california	0.333333
row scaling of a csr or	row	0.066667
is equal to the average path length of	average path length	0.090909
make and configure a copy of	base ensemble make estimator append random_state	0.166667
for binary classification used in hastie	datasets make hastie	0.125000
don't store the timestamp when pickling	joblib memorized func reduce	0.050000
fit a binary classifier on x and y	sgdclassifier fit binary x y alpha	1.000000
the number of splitting iterations in	pgroups out get n splits x y groups	0.111111
process of the parallel execution only a	externals joblib parallel	0.014085
test indices	split iter indices	0.250000
using x as training data and y	x y	0.004310
the data under the model	mixture dpgmmbase score samples x	0.200000
range approximates the range	randomized range	0.083333
given training data and parameters	y	0.010695
import path as a list of	resolv_alias win_characters	0.166667
prediction of init	ensemble base gradient boosting init	0.142857
reconstruct the array from the meta-information and	joblib zndarray wrapper read unpickler	0.043478
utility for building a cv	check cv cv x y classifier	0.031250
best found parameters	base search cv	0.052632
validate x whenever one tries to predict apply	decision tree validate x predict x check_input	0.500000
convert coefficient matrix	linear_model sparse coef mixin	0.090909
returns the number of splitting iterations in the	model_selection base kfold get n splits	0.111111
parallel processing this method	externals joblib parallel	0.014085
in array-like	utils num	1.000000
introduced by a random projection p only changes	core johnson lindenstrauss min dim n_samples eps	0.142857
call predict on the estimator with the best	cv predict	0.041667
generate indices to split data into	model_selection base shuffle split split	0.250000
fit the model using x as training data	neighbors unsupervised mixin fit x y	0.500000
function the absolute error of the kl divergence	manifold kl divergence error	0.100000
in the proper format	ensemble base weight boosting validate x	1.000000
precision matrices are symmetric and positive-definite	precisions full precisions covariance_type	1.000000
back the data to	inverse transform x	0.051282
deviance loss function for binary	deviance	0.062500
the f-beta score is the weighted harmonic mean	metrics fbeta score y_true y_pred	0.333333
cache folders to make cache size fit	joblib memory reduce size	0.083333
for each	val predict estimator x	0.045455
residual	residual	1.000000
logistic loss	logistic loss	0.500000
the position of the	mds	0.050000
avoid the	memory	0.015625
find the least-squares solution to a large	lsqr a	0.037037
fit linear model with stochastic gradient descent	fit x y coef_init intercept_init	0.076923
the backend and	parallel backend base	0.037037
with block checkerboard structure	checkerboard shape n_clusters noise minval	0.066667
run fit on one set	model_selection fit grid point	0.500000
from data	manifold isomap	0.333333
estimate model parameters with the em	base mixture fit	0.200000
estimates the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf x assume_centered	0.125000
memmap instance to reopen on same	memmap	0.066667
y_min	y_min	1.000000
x and dot	divergence x	0.250000
x which should contain	x	0.001692
found parameters	search	0.038462
perform dbscan clustering from vector	cluster dbscan x eps	0.200000
the timestamp when pickling to avoid the	reduce	0.034483
to the distortion introduced by a random	n_samples eps	0.125000
multinomial deviance loss function for multi-class classification	multinomial deviance	0.250000
undo the scaling of x	min max scaler inverse transform x	0.250000
input data point	y	0.002674
fit the model to	fit x y	0.011976
fit	svm linear svr fit x y	0.333333
of the kernel k	pairwise kernel	0.250000
based on	connectivity n_clusters return_distance	0.250000
data point	val predict	0.045455
returns the number of splitting iterations in the	split get n splits x y	0.111111
the synchronous ascending phase	neighbors lshforest get candidates query max_depth bin_queries n_neighbors	0.333333
x to unit norm parameters	preprocessing normalizer transform x y	0.250000
process regression model	process regressor	0.166667
people dataset this operation	people	0.100000
compute non-negative matrix factorization nmf	negative factorization	0.043478
depending from	externals joblib memory	0.016949
function used to fit a single tree	build trees tree forest	0.142857
utility for building a cv in a user	cv cv x y	0.031250
np arange(n_samples)	permutation	0.166667
for c in	c	0.022222
function used to fit a single	trees	0.083333
cosine lsh fingerprint	gaussian random projection hash	0.166667
fit ridge regression	ridge gcv fit x y	1.000000
compute prediction of init	gradient boosting init decision	0.142857
return the kernel k x y and	x y	0.008621
gradient of loss	loss	0.027027
it	memorized func	0.016949
gradient of the loss is	loss	0.027027
determine the optimal	auto batching mixin compute	0.333333
measure the similarity of	fowlkes mallows	0.250000
hash to identify uniquely python objects containing	joblib hash obj hash_name coerce_mmap	0.250000
n_max_training_samples	n_max_training_samples	1.000000
the leaves of the cf node	cluster birch get leaves	0.333333
constructs signature from the given list	externals signature	0.050000
l1	manhattan	0.125000
the weighted	base mixture	0.111111
generate a mostly low rank matrix with	make low rank matrix n_samples n_features	0.500000
and dispatch them	dispatch	0.111111
normalize x according to kluger's log-interactions scheme	log normalize x	0.200000
n_bins	n_bins	1.000000
compute non-negative matrix factorization nmf find	non negative factorization x	0.043478
consensus	consensus	0.555556
for parallel processing	externals joblib parallel	0.014085
polynomial	metrics polynomial	0.333333
get a list of all estimators from sklearn	utils all estimators include_meta_estimators include_other type_filter include_dont_test	0.500000
build a batch of estimators within a job	parallel build estimators	0.166667
estimators within	estimators n_estimators ensemble	0.083333
cache for	joblib memorized func	0.014706
check that	utils check clusterer compute	1.000000
are going	multiprocessing	0.045455
area under the curve auc from	auc score	0.052632
k-neighbors of	kneighbors mixin kneighbors x	0.125000
points in the	parameter	0.083333
the number of jobs	effective n jobs n_jobs	0.333333
of two clusterings of a	score labels_true labels_pred	0.047619
batch_size elements	batch_size	0.100000
single	build trees	0.142857
fit linear	sgdclassifier fit	0.076923
the median of	get median	0.166667
least-squares solution to a large sparse	utils lsqr a	0.037037
under the curve auc from prediction	auc score	0.052632
estimator	name estimator	1.000000
for specified layer	layer n_samples	0.166667
number of splitting iterations in	leave pgroups out get n splits x	0.111111
trace	trace	0.857143
of all the vectors rows of u such	u	0.032258
timestamp when pickling to	joblib memory reduce	0.030303
binarizer	binarizer	0.857143
transform binary labels back to	binarizer inverse transform y	0.500000
all tokens in the raw documents	feature_extraction count vectorizer	0.125000
a	externals joblib memorized	0.013699
the percentiles	percentiles grid_resolution	0.250000
decision function output for x relative	metrics threshold scorer call clf x	0.058824
dual gap convergence criterion the specific definition	dual gap emp_cov	0.071429
the gaussian process regression	gaussian_process gaussian process regressor	0.058824
perform a locally linear embedding analysis on	locally linear embedding x n_neighbors	0.071429
for later	fit x y	0.005988
for a sparse	svds a	0.166667
x using the	x y	0.002155
transform binary	transform y	0.023256
mean shift	mean shift	0.125000
the lfw people dataset this operation is	fetch lfw people	0.040000
cfnode	cfnode	1.000000
verbose message on the end	verbose msg init end ll	0.333333
uncompressed bytes from the	externals joblib binary zlib	0.125000
list of regularization parameters	path x y pos_class cs	0.166667
length	length	0.875000
isotonic regression model : min sum	isotonic regression	0.055556
is a general function given points on a	reorder	0.071429
print the dictionary 'params' parameters	core pprint params offset printer	0.250000
perform classification on test vectors	core dummy regressor predict	0.250000
of feature	get feature	0.125000
test	core base shuffle	0.166667
compute the decision function of x	gradient boosting classifier decision function x	0.333333
area under the curve auc	metrics auc x y	0.040000
fit a single tree	build trees tree	0.142857
scale back the data to the original representation	robust scaler inverse transform	0.066667
oracle approximating shrinkage	oas x	0.500000
in the svmlight / libsvm	svmlight file f n_features dtype	0.066667
search	search cv	0.018182
approximates the range	utils randomized range finder	0.083333
outlier factor of x	outlier factor decision function x	0.200000
private function used to fit a single tree	build trees tree	0.142857
model with passive	linear_model passive	0.200000
of neighbors	neighbors mixin radius neighbors	0.125000
covariance m step for	covar	0.153846
for c in (l1_min_c	c x y	0.030303
a platform independent representation of	shape	0.011765
a dataset along any axis center	axis	0.028169
compute the decision function of the given observations	covariance outlier detection mixin decision function	0.333333
cv in a user	cv cv x y	0.031250
of x from y along the first axis	x z	0.050000
max_depth	max_depth	1.000000
returns the index of the leaf	apply x	0.166667
compute the l1	metrics manhattan	0.333333
index of the leaf	decision tree apply	0.166667
error of the kl divergence of	manifold kl divergence error	0.100000
detects the	svm one class svm fit	0.125000
in svmlight format this	svmlight	0.050000
count	bernoulli nb count x	1.000000
list	externals joblib parallel	0.014085
a cv in	check cv cv x	0.031250
estimates for each input data	x y	0.002155
the number of splitting iterations in	cross validator get n splits x y groups	0.125000
c in (l1_min_c infinity) the	c x y loss	0.030303
classification	score y_true	0.058824
the reduced likelihood function for the	process reduced likelihood function	0.047619
missing_values	missing_values	1.000000
number of splitting iterations in the	leave one out get n splits x y	0.111111
generate a grid of	ensemble grid	0.111111
the average log-likelihood	decomposition factor analysis score x	0.333333
to check the test_size and train_size at init	validate shuffle split init test_size train_size	0.250000
a for a	a	0.018182
predictions using a single binary estimator	predict binary estimator x	0.200000
array corresponding to	joblib numpy array	0.250000
function to a given type	externals joblib customizable pickler register type	0.083333
with the best found parameters	base search cv predict	0.076923
diagonal of the laplacian matrix and convert it	diag laplacian	0.111111
estimate the spherical wishart distribution parameters	bayesian gaussian mixture estimate wishart spherical nk	0.333333
a set of points	metric	0.071429
loader for the california housing dataset from statlib	datasets fetch california housing data_home download_if_missing	0.250000
classifier valid parameter keys can	classifier set params	0.125000
compute elastic net path with coordinate	linear_model enet path x	0.050000
estimate the precisions parameters of the	bayesian gaussian mixture estimate precisions	0.166667
tp + fn where tp is the	score y_true y_pred labels pos_label	0.027778
covariance model	covariance	0.028986
block diagonal structure for	biclusters shape n_clusters noise minval	0.058824
coverage file from an open file object	coverage f header_length dtype	1.000000
exception	base get	0.066667
and return encoded labels parameters	y	0.002674
the boolean mask indicating which features	support mask	0.125000
a reducer function to a	externals joblib customizable	0.200000
* np dot	linear_model intercept dot	1.000000
exception types to	externals joblib parallel backend	0.029412
gradient	gradient w x	0.500000
matrix whose range approximates the range	utils randomized range	0.083333
rand index adjusted for chance	cluster adjusted rand score labels_true labels_pred	0.333333
read array from unpickler file handle	joblib numpy array wrapper read array unpickler	1.000000
the parallel	parallel	0.019231
to build a batch of estimators within a	build estimators n_estimators ensemble x y	0.166667
net path with coordinate descent the	path	0.025641
predict_log_proba on the estimator with the	predict log proba x	0.045455
any axis center to the median and component	axis	0.014085
posterior probabilities of classification	core calibrated classifier cv predict proba x	0.200000
computes the	empirical covariance	0.125000
computation of min	preprocessing min	0.166667
x	x x	0.333333
measure the similarity of two clusterings of a	fowlkes mallows score labels_true labels_pred	0.333333
the logistic loss and gradient	linear_model logistic loss and grad w	0.500000
build a contingency matrix describing the relationship	cluster contingency matrix labels_true labels_pred eps sparse	0.333333
probability calibration with sigmoid method platt 2000	sigmoid calibration df	0.500000
to the data x	x y	0.002155
randomized logistic regression works by subsampling the	randomized logistic regression	0.166667
matrix whose range approximates the range	randomized range	0.083333
a particular sample is an outlier or not	ensemble isolation forest	0.500000
fetch an	fetch	0.142857
matrix in-place	inplace swap	1.000000
apply a mask to edges weighted	mask edges weights	0.333333
add an item to	externals add move move	0.333333
write	numpy array wrapper write	1.000000
dual gap convergence criterion the specific definition is	covariance dual gap emp_cov precision_	0.071429
and the	w x y	0.133333
function for parameter value indexing	model_selection index param value x v indices	0.200000
locally linear embedding analysis on	locally linear embedding x	0.071429
input	predict	0.006849
scale back the data to the	preprocessing standard scaler inverse transform x	0.066667
new samples can be different from	calibrated classifier	0.083333
such that for c in (l1_min_c	c x y loss fit_intercept	0.030303
fit a multi-class	linear_model base sgdclassifier fit	0.076923
return posterior probabilities of classification	core quadratic discriminant analysis predict proba	0.333333
updates terminal regions to median estimates	terminal region	0.100000
fit the model	rbm fit	0.333333
finds the k-neighbors	kneighbors mixin kneighbors x n_neighbors return_distance	0.250000
avoid the	externals joblib memory	0.016949
predict labels for	predict x	0.011765
function used to fit an estimator within	fit estimator estimator x	0.055556
axis center to the mean and	axis	0.014085
determine the optimal batch size	externals joblib parallel backend base compute batch size	1.000000
with passive aggressive algorithm	passive aggressive classifier	0.125000
filters the given args and kwargs using a	func ignore_lst args kwargs	0.333333
check initial parameters of the derived class	base mixture check parameters	0.200000
also predict based on an unfitted	predict x	0.011765
used to capture the	check_pickle	0.040000
print verbose message on the end of	base mixture print verbose msg init end ll	0.333333
trace of np dot x y	decomposition trace dot x y	0.500000
exception types	externals joblib parallel backend	0.029412
is guaranteed not to be empty	svm l1 min	0.333333
em update for 1 iteration	em step x total_samples batch_update parallel	1.000000
then	core transformer	1.000000
split data into	kfold split	0.250000
initialization	initialize	0.272727
should be used when memory is inefficient to	x y classes	0.027778
pure	pure	1.000000
compute the maximum absolute value	preprocessing max abs	0.050000
the kernel density	neighbors kernel density	0.090909
incremental mean and variance	mean variance	0.166667
to run a	externals joblib parallel	0.014085
break the pairwise matrix in n_jobs even slices	metrics parallel pairwise x y func n_jobs	0.111111
that for c in (l1_min_c infinity) the	c x y loss fit_intercept	0.030303
scale back the data to the original	standard scaler inverse transform x copy	0.066667
with respect to	n_samples activations deltas	0.166667
mean and component wise scale to unit variance	preprocessing scale	0.090909
the log	log	0.018868
raw file object e g created	raw file	0.200000
to split time series data samples	time series split	0.250000
and false positives per binary classification threshold	binary clf curve y_true y_score pos_label	0.090909
depending from	memory	0.015625
a sparse combination of the dictionary atoms	decomposition sparse coding mixin transform x	0.333333
depending from	memorized	0.015873
load the kddcup99 dataset	brute kddcup99	0.166667
a single boost	boost classifier boost	0.100000
coefficients and intercepts from	neural_network base multilayer perceptron unpack	0.250000
t-sne objective function the absolute error	error	0.020000
using thresholding	thresholding y	0.500000
the dimension of a dataset of	dimension	0.050000
print	mixture base mixture print	1.000000
get	mixin get	0.500000
average path length	ensemble average path length	0.090909
gaussian data set with self	x_test y	0.142857
generates a random sample from a	a size replace p	0.142857
likelihood function	likelihood function	0.285714
kwargs using a list of arguments	kwargs	0.076923
the search over parameters	search	0.019231
used to compute log probabilities within a job	parallel predict log proba estimators estimators_features x n_classes	0.250000
a cv in a user	cv cv x y classifier	0.031250
of x for	fit x	0.006410
checker utility for building a cv in a	cv cv x	0.031250
matrices from a given template	type tied_cv covariance_type	1.000000
a job	estimators estimators_features x n_classes	0.333333
sparse and dense	x y	0.002155
given radius of a point or points	radius	0.045455
model	fit x	0.051282
single binary estimator	predict binary estimator x	0.200000
list of exception	joblib parallel backend base	0.058824
get the values used	neural_network sgdoptimizer get	0.125000
regression score function	score y_true y_pred	0.038462
of the final estimator parameters	core pipeline	0.076923
shutdown the process	terminate	0.090909
and breiman [2]	friedman3 n_samples noise random_state	0.166667
and scale the data parameters	transform x y	0.031250
two non-negative matrices w h whose product	w h n_components	0.038462
the	multiprocessing backend	0.038462
a logistic regression model	linear_model logistic regression	0.333333
which are going to run in parallel	externals joblib multiprocessing backend	0.035714
kernel k x y and optionally its gradient	gaussian_process pairwise kernel call x y eval_gradient	1.000000
loader for the california housing	datasets fetch california housing data_home download_if_missing	0.250000
wb	wb	1.000000
in the svmlight / libsvm format into sparse	svmlight file f n_features	0.066667
which are going to run	externals joblib multiprocessing backend	0.035714
for each input data	predict	0.006849
float32 is returned	metrics return float	1.000000
underlying	one vs one classifier	0.125000
sign correction to ensure deterministic output from svd	utils svd flip u v u_based_decision	1.000000
mp	mp	1.000000
custom warning to capture convergence problems	convergence warning	1.000000
apply feature map to x	core nystroem transform x	1.000000
can be different from the	calibrated classifier cv	0.071429
indices to split data into	base shuffle split split x	0.250000
data into the already fitted lsh forest	neighbors lshforest partial fit x y	0.200000
for building a cv in a user	cv cv x y classifier	0.031250
compute minimum and maximum	min max axis x	0.500000
from it	externals joblib memorized	0.013699
check the	neighbors check	0.333333
in x according to the fitted model	x	0.001692
number of splitting iterations in	model_selection cviterable wrapper get n splits x y	0.111111
evaluate decision function output for x relative	metrics threshold scorer call clf x y sample_weight	0.058824
predict class	tree predict	0.500000
to compute log probabilities within a job	ensemble parallel predict log proba estimators estimators_features x	0.250000
1d	1d y	1.000000
file was opened for writing	zlib file writable	0.250000
instance for the given param_grid	search cv get param iterator	0.166667
the leaf that each sample is predicted as	decision tree apply x check_input	0.500000
folders to make cache size fit in	reduce size	0.083333
and dispatch	parallel dispatch one	0.250000
absolute sizes of training	train sizes	0.066667
input data point	val predict estimator x	0.045455
number of splitting iterations in the cross-validator parameters	model_selection base kfold get n splits x	0.111111
patches	patches	0.388889
factorization nmf	non negative factorization x	0.043478
a tolerance which is independent of the dataset	cluster tolerance x	0.058824
product with the	projection	0.071429
be used when memory is inefficient	y classes	0.027778
largest k singular values/vectors for a sparse	utils svds a k ncv tol	0.166667
to pickler file handle	pickler	0.083333
measure	metrics cluster fowlkes mallows	0.250000
fit the model with x	chi2sampler fit x	1.000000
with self	x_test y	0.142857
hessian in the case of a multinomial loss	multinomial grad hess w	1.000000
function to output a function	externals joblib function called str function_name	0.250000
images for image manipulation	images	0.111111
of x according to feature_range	inverse transform x	0.025641
remove	memory reduce	0.030303
to the training set x and	x y	0.002155
elastic net parameter search parameters	l1_ratio	0.030303
fit the model to data	core multi output estimator fit x y sample_weight	0.200000
the shortest path length	shortest path length graph	0.333333
apply clustering to a projection to the	cluster spectral clustering affinity n_clusters n_components	0.166667
a single	trees	0.083333
format check	decomposition latent dirichlet allocation check	0.062500
the svmlight / libsvm format into	svmlight file f n_features dtype multilabel	0.066667
the time it take to	joblib squeeze time	0.200000
boundary of the set of samples x	x y	0.002155
returns the score on the given data	score x y	0.030303
concatenates results of multiple transformer	feature union	0.142857
partially fit a single binary estimator one-vs-one	core partial fit ovo binary estimator x y	1.000000
handle the callable case	metrics pairwise callable x y	0.083333
base class for adaboost estimators	base weight boosting	1.000000
fit linear	base sgdclassifier fit x	0.333333
the posterior log probability	bernoulli nb joint log likelihood	0.083333
callable case for	pairwise callable x y metric	0.083333
avoid the hash depending	joblib memorized	0.015625
propagation classifier read more in	propagation	0.076923
precisions	precisions x z	0.250000
get the weights from an array of	neighbors get	0.125000
this dataset is described in friedman	datasets make	0.015625
for reproducibility flips the	deterministic vector	0.076923
read the array corresponding to this wrapper	externals joblib numpy array wrapper read unpickler	1.000000
the given estimator	estimator	0.014706
which are going to	externals joblib multiprocessing backend effective	0.250000
the em algorithm and return the cluster weights	mixture gmmbase	0.034483
similarity of two clusterings of	score labels_true labels_pred sparse	0.047619
when numpy is loaded	numpy	0.083333
score by cross-validation read more in	cross val score estimator x	0.166667
the output of transform is sometimes referred	transform y	0.023256
class covariance matrix	core class cov x y priors shrinkage	0.250000
for factorizing common	partial fit first call clf	0.200000
the objective	objective	0.076923
the kernel k x	gaussian_process white kernel call x	0.333333
a score	score	0.020202
and false positives per binary classification	binary clf curve	0.090909
memmap instance	externals joblib reduce memmap	0.142857
compute non-negative matrix factorization	non negative factorization x	0.043478
min	preprocessing min	0.166667
selected features	feature_selection selector mixin	0.142857
check if there is	utils check	0.023810
reproducibility flips the sign of elements of	deterministic vector sign flip	0.066667
the position of	mds	0.050000
returns the index of the leaf	tree apply x	0.166667
and component wise scale	preprocessing robust scale x	0.125000
handle the callable case for pairwise_{distances kernels}	pairwise callable	0.083333
validate x whenever one tries to predict apply	ensemble base forest validate x predict x	0.500000
suffix when	utils shape	0.013699
oas	oas	0.500000
fit linear model	fit	0.016287
in [rouseeuw1984]_ aiming at computing	covariance c step x n_support remaining_iterations initial_estimates	0.111111
product	projection transform	0.333333
helper to workaround python 2 limitations of	utils parallel helper obj methodname	0.333333
the solution to a sparse coding problem	decomposition sparse encode x dictionary	0.333333
fit linear model with passive aggressive algorithm	linear_model passive aggressive regressor fit x y coef_init	1.000000
of feature names ordered by their	feature_extraction dict vectorizer get feature names	0.142857
shuffle-group s -out cross-validation iterator provides randomized train/test	group shuffle	1.000000
the recall is the ratio	recall	0.028571
estimators that implement	utils check	0.023810
for elastic net parameter search	x y xy l1_ratio	0.250000
convert coefficient matrix	coef	0.058824
for a	means	0.153846
fit the model to data matrix x	fit	0.003257
fit the gradient boosting model	base gradient boosting fit x y	1.000000
the similarity of two sets	a b similarity	0.125000
a score by cross-validation read more in the	cross val score estimator x y	0.166667
compute the decision	ensemble ada boost classifier decision	0.333333
estimates	val predict estimator x y	0.045455
for each input	predict	0.006849
recall	recall	0.171429
independent	utils	0.009709
random multilabel classification	make multilabel classification	0.166667
computes the exponential chi-squared	metrics chi2	0.333333
precision is	precision	0.016667
perform dbscan clustering from vector array	cluster dbscan x	0.200000
return whether the file	binary zlib file	0.125000
empty the	externals joblib memorized func clear warn	0.250000
sparse random matrix given	random choice	0.166667
the kernel k x	call x	0.142857
the wild lfw pairs dataset this	fetch lfw pairs	0.018868
score is the	score y_true y_pred beta labels	0.500000
this implementation is restricted to the binary	y_score	0.083333
introduces an 'l' suffix	repr	0.012500
residues on left-out data for a full	residues x_train y_train x_test y_test	0.083333
data home cache	datasets clear data home	0.076923
find two non-negative matrices w h whose product	w h	0.031250
zero row of x	x y	0.002155
log probability for	mixture log multivariate normal	0.500000
compute the recall the recall is the ratio	metrics recall	0.033333
for	cross val	0.038462
func to be	pool manager mixin apply async func	0.250000
perform classification on test vectors	core dummy regressor	0.200000
receiver operating characteristic roc note this implementation is	metrics roc curve	0.142857
capture the arguments of	check_pickle	0.040000
to n	n	0.050000
the median	median	0.133333
matrix to dense array format	linear_model sparse coef mixin densify	0.100000
reconstruct the image from all of	reconstruct from	0.333333
right fileobject from a	joblib read fileobject fileobj	0.100000
effective	effective	0.454545
the hash depending from it	externals joblib	0.009524
back the data to	inverse transform	0.062500
indices increasingly apart the distance	joblib verbosity filter index	0.055556
types	joblib parallel	0.028571
data samples in x	x y	0.002155
creating a class with	add	0.071429
lower bound on model evidence	dpgmmbase lower bound	0.071429
estimates for each input data	predict estimator x	0.045455
multi-class targets using	output code	0.200000
estimator on training subsets incrementally	incremental fit estimator estimator	0.500000
abc for	base	0.014286
mask to edges weighted	weights mask edges weights	0.333333
to make cache size fit in	externals joblib memory reduce size	0.083333
a cv in a user	core check cv cv x y classifier	0.031250
validate the provided 'means'	mixture check means means n_components n_features	0.333333
regression target	boosting regressor	0.500000
totally random trees	random trees embedding	0.250000
parameters and evaluates the reduced likelihood function	gaussian process reduced likelihood function	0.047619
boolean mask x	preprocessing get mask x	0.333333
on the estimator with the best	cv	0.045045
kernel density estimation read more in	kernel density	0.083333
fit the	multi output estimator fit x y sample_weight	0.200000
estimate the precisions parameters of	mixture bayesian gaussian mixture estimate precisions nk	0.166667
labels back	inverse	0.055556
estimate the precisions parameters of the precision distribution	bayesian gaussian mixture estimate precisions nk xk	0.166667
the test/test sizes are meaningful wrt	validate shuffle split n_samples test_size train_size	0.111111
fit all transformers transform	union fit transform x	0.333333
constant	constant	0.857143
a	make	0.041667
linear model parameters	linear model	0.090909
np dot x y t	dot x y	0.250000
sample_weight	sample_weight	0.129630
approximation of the	n_samples n_subsamples	0.333333
leaf that each sample is predicted as	tree apply x check_input	0.500000
clustering on x and	fit predict x y sample_weight	0.333333
mean squared logarithmic	mean squared	1.000000
to a projection to the normalized laplacian	eigen_solver	0.090909
determinant with the fastmcd algorithm	cov det fit	1.000000
lars path parameters	linear_model omp path	0.100000
the process	externals joblib multiprocessing	0.052632
apply a mask	mask edges weights mask	0.333333
return_x_y	return_x_y	1.000000
sure centering is not enabled for sparse matrices	robust scaler check array	0.250000
measure the similarity of two clusterings	cluster fowlkes mallows score labels_true labels_pred sparse	0.333333
of classification this function returns posterior probabilities of	cv predict proba	0.034483
predict	classifier predict	0.200000
to a	externals joblib customizable	0.200000
build a text report showing	report	0.047619
private function used to build a batch	ensemble parallel build	0.047619
can be different from	calibrated classifier cv	0.071429
quantile this classification dataset is constructed by taking	datasets	0.015152
the shrunk ledoit-wolf	ledoit wolf shrinkage x assume_centered	0.250000
similarity coefficient score	score	0.010101
the one-vs-one multi class libsvm in the	svm one vs one	0.050000
input checker utility for building a cv	check cv cv x y classifier	0.031250
generate an array with constant	datasets make	0.015625
in the :ref user guide <image_feature_extraction>	patch extractor	0.090909
sparse matrix	dtype	0.062500
cache result	joblib memorized	0.015625
remove cache folders	memory	0.015625
a platform independent representation of	shape repr	0.013699
a helper class for automagically batching jobs	auto batching mixin	0.333333
compute the largest k singular values/vectors	k ncv tol	0.166667
compute the median	median	0.066667
key	key	1.000000
fit the model to data	core multi output estimator fit x	0.200000
for each input	cross val predict	0.045455
the number of splitting iterations in	cviterable wrapper get n splits x y	0.111111
all the content of the data home	data home	0.076923
of module names and a name for	func name	0.047619
restricted to the binary classification	score y_true y_score average sample_weight	0.076923
are going to run	joblib multiprocessing	0.052632
predictions using a single binary estimator	binary estimator	0.090909
boundary of the set of samples x	x	0.001692
check x format	dirichlet allocation check	0.062500
by class for unbalanced datasets	weight class_weight y	0.200000
class priors from multioutput-multiclass target data parameters	class distribution y sample_weight	1.000000
center kernel matrix	preprocessing kernel centerer transform k	0.500000
of	externals	0.005747
array-like or scipy sparse matrix	preprocessing binarize x threshold	0.083333
and intercept for specified layer	layer	0.090909
fit the	embedding fit	0.333333
a list of	utils	0.009709
the precision matrix	covariance get precision	0.250000
similarity of two sets of biclusters	consensus score a b similarity	0.500000
labels in a	preprocessing label	0.166667
the callable case for pairwise_{distances	metrics pairwise callable x	0.083333
to line_search_wolfe2 if suitable step length is	wolfe12 f fprime xk pk	0.028571
generates indices to split data into	model_selection repeated splits split x	1.000000
bound for c such that for c	c x y	0.030303
partially fit a single binary	core partial fit binary	1.000000
exp-sine-squared kernel	exp sine squared	1.000000
for building a cv	check cv cv x y	0.031250
loader	data_home	0.166667
boolean thresholding of array-like or scipy	preprocessing binarize	0.083333
windows this is the time	time	0.047619
a platform	utils shape repr	0.013699
predict class log-probabilities for	ensemble forest classifier predict log proba	0.500000
using x as	x	0.003384
compute the median of data	get median data	0.333333
the covariance matrices from a given template	covariance type tied_cv covariance_type n_components	0.333333
given param_grid	search cv get param iterator	0.166667
avoid the hash depending from	externals joblib memorized	0.013699
build a batch of estimators within a	parallel build estimators n_estimators	0.166667
remove cache folders	memory reduce	0.030303
voting classifier	voting classifier set	0.037037
the recall the recall is	metrics recall	0.033333
the least-squares solution to a large sparse linear	a	0.018182
h whose product approximates the	h n_components	0.166667
validation of y and class_weight	svm base lib svm validate targets y	1.000000
quantiles	quantiles	1.000000
normalized	spectral	0.026316
full covariance	multivariate normal density full x	0.166667
classification score	score y_true	0.058824
fit the model	manifold tsne fit	0.333333
make predictions using a single binary estimator	core predict binary estimator	0.200000
the submatrix corresponding to bicluster i	core bicluster mixin get submatrix i	0.333333
regression target at each stage for	ensemble gradient boosting regressor staged	0.500000
default backend	backend backend	0.333333
best found	base search cv predict	0.076923
checker utility for building a cv	core check cv cv x	0.031250
of module names and a name	get func name	0.047619
the diagonal of the laplacian matrix and convert	diag laplacian	0.111111
pairs dataset	pairs subset	0.125000
number of splitting iterations in the cross-validator parameters	base kfold get n splits x	0.111111
fowlkes	fowlkes	1.000000
gaussian process regression model	gaussian process regressor	0.055556
input data	core cross val predict estimator x y	0.045455
for different probability thresholds note	probas_pred pos_label sample_weight	0.066667
with n_zeros for the	n_zeros	0.111111
build from the	build from	0.250000
outlier on the training set	outlier factor fit	0.200000
a large	a	0.018182
lad updates terminal regions to median estimates	least absolute error update terminal region tree terminal_regions	0.200000
transform binary labels back	inverse transform	0.031250
the decision function of	ensemble gradient boosting classifier decision function	0.166667
y is monotonically correlated with x	core check increasing x y	0.333333
l1	metrics paired manhattan	0.333333
fit linear model with passive aggressive algorithm	passive aggressive classifier fit x y coef_init intercept_init	1.000000
that for c	c x y loss fit_intercept	0.030303
weighted graph of neighbors for points in	neighbors radius neighbors mixin radius neighbors graph	0.066667
estimates for each	cross val predict estimator	0.045455
to a given type in the	externals joblib customizable pickler register type	0.083333
build a batch of estimators within	build estimators n_estimators ensemble x	0.166667
of estimators within a	estimators n_estimators	0.083333
the cholesky decomposition	log det cholesky	0.166667
false positives per binary classification	metrics binary clf curve	0.090909
the curve auc using the trapezoidal rule	metrics auc x y	0.040000
finds seeds for mean_shift	seeds x bin_size min_bin_freq	0.500000
estimate the spherical variance values	mixture estimate gaussian covariances spherical	1.000000
split data into	model_selection base kfold split	0.250000
as a sparse combination of the dictionary atoms	decomposition sparse coding mixin transform x	0.333333
function used to fit an estimator within a	fit estimator estimator	0.055556
avoid the hash depending from it	memory	0.015625
of the kernel k	constant kernel	0.250000
of the leaf	tree base decision tree apply x	0.166667
the search over	core base search cv fit x	0.166667
estimate sample weights by	sample	0.032258
non-negative matrices w h	x w h n_components	0.038462
an unfitted	estimators unfitted	0.142857
bootstrap	bootstrap	1.000000
compute the l1 distances between the	metrics manhattan distances	0.083333
get parameters of this	operator get params deep	1.000000
normalize	normalize	0.600000
to build a batch of estimators within a	ensemble parallel build estimators n_estimators ensemble x y	0.166667
k eigenvalues and eigenvectors of the square matrix	k m sigma	1.000000
stacklevel is	stacklevel	0.125000
outliers detection with covariance	outlier detection	1.000000
a fit	fit rfe estimator x y	0.166667
fit all transformers transform	feature union fit transform x	0.333333
swaps two rows of a csr matrix in-place	utils inplace swap row csr x	0.250000
generative model	decomposition base pca	0.071429
along an axix on	axis x axis last_mean last_var	0.200000
an 'l' suffix when	utils	0.009709
the covertype dataset	datasets fetch covtype	0.333333
the long type introduces an	utils	0.009709
by the	backend base	0.032258
global clustering for the	cluster birch global clustering	0.142857
derivative of the logistic sigmoid	neural_network inplace logistic derivative z delta	0.166667
binary classification task	metrics precision recall curve y_true	0.142857
independent representation	utils shape	0.013699
the linear	linear	0.062500
binary classification	metrics binary	0.333333
best found	search cv predict	0.074074
and false positives per binary classification threshold	metrics binary clf curve y_true y_score pos_label	0.090909
of samples in array-like	utils num samples	0.250000
given file as a	externals joblib	0.004762
input and compute prediction of init	base gradient boosting init decision	0.142857
hastie et al	make hastie 10 2 n_samples random_state	0.166667
list of edges for a	edges	0.047619
folders	externals joblib	0.004762
of the unpickler to unpickle our numpy pickles	zip numpy unpickler	0.333333
the model parameters of the	base mixture	0.111111
which is equal to the average path	average path	0.142857
generate a random n-class classification	datasets make classification n_samples n_features n_informative n_redundant	0.500000
check that y_true and y_pred belong	metrics check reg targets y_true y_pred multioutput	0.333333
contingency matrix describing	contingency matrix labels_true labels_pred	0.166667
and smooth	y	0.005348
for the one-vs-one	svm one vs one	0.050000
sure that an estimator implements the	check estimator estimator	0.142857
for each	val predict	0.045455
list sparse matrix or similar	dtype order	1.000000
the number of splitting iterations in	model_selection cviterable wrapper get n splits	0.111111
each input data point	cross	0.037037
an mldata	mldata	0.142857
calculate true and false positives per binary	metrics binary clf curve y_true y_score pos_label	0.090909
multiple files	files files	0.500000
trace of	trace	0.142857
prob	prob	1.000000
propagation	propagation fit x	1.000000
sizes of training subsets and	train sizes	0.066667
estimates for each input data	core cross	0.045455
eval function func with arguments	eval func	0.166667
outlier factor	outlier factor	0.333333
norm vector length	preprocessing normalize x norm axis	1.000000
to split data into training and test set	shuffle split split x y groups	0.200000
compute gaussian log-density at x for a spherical	log multivariate normal density spherical x means covars	1.000000
apply clustering to a projection	clustering	0.050000
learn vocabulary and idf return term-document matrix	feature_extraction tfidf vectorizer fit transform raw_documents y	0.250000
indices to split data in train/test	split	0.027778
predict class for	ensemble forest classifier predict	0.333333
csgraph	sparsetools validate graph csgraph directed	0.250000
x y and	sine squared call x y	1.000000
voting classifier valid parameter keys can be	ensemble voting classifier set params	0.037037
the isotonic regression model : min sum w[i]	isotonic regression y	0.066667
n_oversamples	n_oversamples	1.000000
back the data	scaler inverse transform x copy	0.066667
of	parallel	0.019231
measure the similarity of	cluster fowlkes mallows	0.250000
score for a fit across one fold	feature_selection rfe single fit rfe estimator x	0.200000
n_jobs even slices	n_jobs	0.023256
radial-basis function kernel aka squared-exponential kernel	rbf	0.166667
and y read more in the :ref user	y	0.002674
position of	mds	0.050000
the process	backend	0.016949
and returns	y w	0.500000
probabilities of possible outcomes for samples in	ensemble voting classifier predict proba	0.200000
return a callable that handles preprocessing and tokenization	build analyzer	0.333333
full covariance matrices	normal density full x means covars min_covar	0.166667
fit the	linear svc fit	0.333333
this implementation is restricted to the binary	y_score average sample_weight	0.142857
message	message	1.000000
computes the log-likelihood of	covariance empirical covariance score	0.166667
path length from source to all reachable	path length graph source cutoff	0.200000
the pairwise matrix in	parallel pairwise x	0.166667
least squares solver	linear discriminant analysis solve lsqr	1.000000
scale	scale	0.233333
the elastic net	l1_ratio	0.030303
returns the number of splitting iterations in the	model_selection base cross validator get n splits	0.125000
a byte string to	externals joblib binary zlib	0.125000
fp	fp	1.000000
regularization parameters	pos_class cs	0.166667
the number of splitting iterations in	model_selection predefined split get n splits x	0.111111
number of splitting iterations in the cross-validator	out get n splits x y	0.111111
the shortest path length	utils single source shortest path length graph	0.333333
x format check x format	decomposition latent dirichlet allocation check	0.062500
one set of	point	0.200000
priors from multioutput-multiclass target data parameters	distribution y sample_weight	1.000000
maximizer	process arg max	0.047619
for a fit across one fold	feature_selection rfe single fit rfe estimator x y	0.200000
row of x	transform x	0.016949
perform standardization by centering and scaling parameters	preprocessing standard scaler transform x y copy	0.333333
given radius of a point	x radius	0.058824
suffix when using	shape	0.011765
extracts patches of any n-dimensional	extract patches	0.083333
on test vectors	core dummy regressor	0.200000
project data to vectors and cluster the	cluster spectral biclustering project and cluster data vectors	0.333333
factorization nmf find two non-negative	negative factorization x	0.043478
number of splitting iterations in the	get n splits x	0.111111
the number of splitting iterations in the cross-validator	group out get n splits x y groups	0.111111
input validation for standard estimators	y x y accept_sparse dtype	0.250000
logistic regression	logistic regression	0.400000
estimate the precisions parameters of the precision	bayesian gaussian mixture estimate precisions nk xk	0.166667
single tree	trees tree forest x	0.142857
cache result and	externals joblib memorized	0.013699
as a sparse combination of the dictionary atoms	sparse coding mixin transform	0.333333
and	y	0.160428
collision between names of functions	job lib collision warning	0.333333
weighted graph of neighbors for points in	neighbors radius neighbors graph	0.066667
and transform x	fit transform x y	0.250000
number of splitting iterations in the cross-validator parameters	leave one out get n splits x	0.111111
number of splitting iterations in the	cviterable wrapper get n splits x y groups	0.111111
fn where tp is the	score y_true y_pred labels pos_label	0.027778
feature	feature x value	1.000000
read an array using	array wrapper read	0.500000
apply clustering	clustering affinity n_clusters	0.166667
c and cpp	c and cpp	0.500000
same as line_search_wolfe1 but fall back to line_search_wolfe2	wolfe12 f fprime xk pk	0.028571
for updating terminal	terminal	0.047619
a list of feature	feature_extraction dict vectorizer get feature	0.200000
get feature	get feature	0.125000
in parallel n_jobs is	n_jobs	0.023256
the recall is the ratio	metrics recall	0.033333
reconstruct the image from all	reconstruct from	0.333333
test	core base shuffle split	0.166667
tolerance which is independent	cluster tolerance x	0.058824
the time under windows this is the time	time	0.047619
the best found	base search cv predict	0.076923
silhouette coefficient	metrics cluster silhouette score x labels	0.250000
estimate model parameters with the	mixture base mixture fit x	0.200000
matrix for x using the	x	0.001692
outlier	outlier	0.800000
the gradient	gradient w	0.500000
objective function the absolute error	error	0.020000
line_search_wolfe1 but fall back to line_search_wolfe2 if	wolfe12 f fprime xk pk	0.028571
get the directory corresponding to the cache for	memorized func get func dir mkdir	0.500000
isotonic regression model :	isotonic regression	0.055556
extract the	joblib extract	0.500000
used to build	build	0.037037
kernel k x y and	gaussian_process pairwise kernel call x y	0.333333
for full	density full x means covars min_covar	0.166667
is not found	line search	0.029412
return a tolerance which is independent of	cluster tolerance	0.058824
shift clustering using a flat kernel	shift	0.100000
net path with coordinate descent	path x	0.045455
posterior log probability of the	nb joint log likelihood	0.066667
wrap	batched calls	1.000000
a score by cross-validation	core cross val score estimator	0.333333
analysis fa	analysis	0.045455
for building a cv in	cv cv x y	0.031250
a filename	filename mmap_mode	0.500000
curve auc	auc	0.040816
of length dimensions	dimensions rng	0.333333
depending from	memorized func	0.016949
on the training set according to	factor fit	0.062500
faces in the wild lfw	fetch lfw	0.041667
a locally linear embedding	manifold locally linear embedding x n_neighbors n_components	0.071429
getter for	covariance empirical	0.100000
compute the unnormalized posterior log probability of x	core base nb joint log likelihood x	0.200000
compute labels and inertia using a full distance	labels inertia precompute dense x x_squared_norms centers	0.250000
mean and	mean	0.107143
initialization of the mixture parameters	mixture bayesian gaussian mixture initialize x resp	1.000000
operation is meant to be cached by a	data_folder_path slice_ color resize	0.033333
kwargs using a list of arguments to ignore	kwargs	0.076923
as the maximizer	arg max	0.047619
one set	point x	0.500000
modifier	modifier	1.000000
stacklevel is the depth	check previous func code stacklevel	1.000000
lfw people	datasets fetch lfw people	0.040000
list of exception types	externals joblib parallel backend base	0.034483
fit	core rbfsampler fit	0.250000
extracts patches	extract patches	0.083333
test_size and train_size at init	validate shuffle split init test_size train_size	0.250000
iteration	iteration	1.000000
evaluate a score by cross-validation	core cross val score	0.333333
contingency matrix describing the	contingency matrix labels_true labels_pred	0.166667
reconstruct	read unpickler	0.200000
with the given arguments	joblib memorized func get output	0.125000
list of exception	joblib parallel backend base get	0.066667
fit the	estimator fit	0.200000
samples from	mixture sample	0.500000
apply clustering	cluster spectral clustering affinity n_clusters	0.166667
two non-negative matrices w h	x w h n_components	0.038462
cohen's kappa a statistic that measures inter-annotator agreement	metrics cohen kappa score y1 y2 labels weights	0.500000
factorization nmf find two non-negative matrices w h	negative factorization x w h n_components	1.000000
the accuracy of a classification	y_true y_pred	0.037037
the directory in	dir	0.038462
check x format check x	dirichlet allocation check	0.062500
point	core	0.015385
list of exception types to	backend base	0.032258
returns the number of splitting iterations in	leave pgroups out get n splits x	0.111111
check if vocabulary is empty or	vectorizer mixin check vocabulary	0.250000
estimators within	estimators n_estimators ensemble x y	0.083333
validate x whenever one tries to predict apply	base decision tree validate x predict x check_input	0.500000
coverage error measure compute how far	coverage error y_true	0.166667
delete all the content of the data home	clear data home data_home	0.076923
return the shortest path length	source shortest path length graph	0.333333
dataset along any axis center	x axis	0.030769
the number of splitting iterations in the	cviterable wrapper get n splits	0.111111
fit the model according to	svc fit x y sample_weight	0.250000
an array shape under python 2	shape repr shape	0.166667
iterate over	iter	0.100000
x format check	dirichlet allocation check	0.062500
the best found parameters	search cv predict	0.074074
array is 2d square and	array tol raise_warning raise_exception	1.000000
a locally linear embedding analysis	manifold locally linear embedding x n_neighbors	0.071429
the free energy f v = - log	neural_network bernoulli rbm free energy	0.066667
hide warnings	utils ignore warnings	0.142857
reduced likelihood function for the given autocorrelation	gaussian process reduced likelihood function	0.047619
estimates for each input data	estimator	0.014706
input data	predict estimator x	0.045455
vectors rows of u	u	0.032258
exception types	backend base	0.032258
and gradient	and grad w x y	1.000000
for a full lars path	linear_model omp path	0.100000
of	repr	0.012500
the lfw people dataset this	lfw people	0.040000
in parallel	ensemble parallel build	0.047619
pairwise matrix in n_jobs even slices	metrics parallel pairwise x y func n_jobs	0.111111
predict_log_proba on the	predict log proba x	0.045455
the number of splitting iterations in	one group out get n splits	0.111111
remove cache folders to	externals joblib memory	0.016949
updates terminal regions to	terminal region tree terminal_regions	0.100000
means	means	0.461538
mean and variance along an axix on a	incr mean variance axis x axis last_mean last_var	0.333333
standardize a dataset along any axis	axis with_mean with_std	0.333333
indicating which features	support	0.125000
grid of alpha values	linear_model alpha grid x y xy	0.166667
and return encoded	y	0.002674
find the median	median	0.066667
a subcluster	subcluster new_subcluster1	0.166667
print verbose	base mixture print verbose	1.000000
calculate the posterior log probability of the samples	multinomial nb joint log likelihood	0.083333
estimates for each input data	cross val predict	0.045455
the barycenter weighted graph of k-neighbors	barycenter kneighbors graph	0.333333
mask to edges weighted	mask edges weights	0.333333
implement a single boost	boost iboost x	1.000000
for	cross	0.037037
of vectors for reproducibility flips	deterministic vector	0.076923
more in the :ref user guide <image_feature_extraction>	patch extractor	0.090909
parameters	compute_sources	0.166667
ridge regression model	linear_model ridge classifier	1.000000
probabilities for a calibration	core calibration	0.125000
gmm	gmm	0.833333
loading for the lfw pairs dataset	fetch lfw pairs	0.018868
partially fit underlying estimators should be used	core one vs one classifier partial fit	0.166667
labels in	label	0.045455
platform	utils shape repr	0.013699
search over	base search cv	0.026316
ledoit-wolf	ledoit wolf shrinkage x assume_centered block_size	0.250000
the kernel k x y and	rbf call x y	0.200000
for each	predict estimator x	0.045455
in pipeline after transforms	pipeline fit predict x	0.166667
area under the curve auc using the	metrics auc x	0.040000
for specified layer	layer	0.090909
approximates the range	randomized range finder	0.083333
solve the linear assignment problem using the	utils linear assignment x	0.090909
class covariance	class cov x y priors shrinkage	0.250000
persist an arbitrary python object	joblib dump value filename compress protocol	0.250000
not found and raise an	search	0.019231
returns the number of splitting iterations in the	leave one out get n splits x	0.111111
predict	classifier predict x	0.500000
ignore_lst	ignore_lst	0.714286
types to	parallel	0.019231
given dataset split	scorer	0.045455
a contingency matrix describing	contingency matrix labels_true	0.166667
observations in x according to the	x	0.001692
note this	sample_weight	0.018519
minimum and maximum	utils min max axis x	0.500000
the callers	joblib effective	0.200000
evaluate the significance of a cross-validated score	score estimator x y cv	0.083333
node and update it with the split subclusters	cluster cfnode update split subclusters	1.000000
single binary estimator one-vs-one	ovo binary estimator	0.500000
the number of splitting iterations in	model_selection leave one out get n splits	0.111111
decision functions	decision function	0.025000
updates terminal	terminal region tree terminal_regions	0.100000
true and false positives per binary classification threshold	binary clf curve y_true y_score	0.090909
a covariance matrix shrunk on	shrunk covariance	0.090909
predicted target values for x	x	0.001692
evaluate the density model	density score samples x	0.250000
header_length	header_length	1.000000
kernel k x y and	gaussian_process exponentiation call x y	0.200000
of parameters	parameters	0.111111
data x with ability to accept precomputed doc_topic_distr	precomp distr x doc_topic_distr	0.500000
decision tree regressor from the training set x	tree decision tree regressor fit x	1.000000
that for c in	c x y	0.030303
timestamp when pickling	memory reduce	0.030303
get the parameters	ensemble voting classifier get	0.200000
cosine	cosine	1.000000
return_as	return_as	1.000000
long type introduces an	utils shape repr	0.013699
wishart the	mixture wishart	0.125000
for building a cv in a user friendly	core check cv cv x	0.031250
cv in a	cv cv x	0.031250
the search over	core base search cv	0.033333
callable case for	callable x y	0.083333
fit the model	core rbfsampler fit	0.250000
loader for	subset data_home	0.125000
list of exception types to	externals joblib parallel	0.014085
rare or too common features	feature_extraction count vectorizer limit features	0.250000
scores this score corresponds to	score y_true y_score	0.025000
graph of	feature_extraction img to graph	0.333333
gaussian distribution	bayesian gaussian	1.000000
locally linear embedding analysis on	manifold locally linear embedding	0.062500
predict regression target at each stage for x	ensemble gradient boosting regressor staged predict x	1.000000
underlying estimators should be used when	core one vs one classifier	0.111111
loss and gradient	loss and grad w	1.000000
whether the file was opened for writing	file writable	0.250000
list of exception types to	externals joblib parallel backend base get	0.066667
described in friedman [1] and breiman [2]	make friedman3 n_samples noise random_state	0.166667
faces in the wild lfw pairs	lfw pairs	0.018868
inplace column scaling of	utils inplace column scale x scale	0.166667
linear support vector classification	linear svc	1.000000
tolerance which is independent of the dataset	tolerance x tol	0.058824
estimate sample weights by class for unbalanced datasets	compute sample weight class_weight y	0.500000
used to build	parallel build	0.047619
before tokenization	feature_extraction vectorizer mixin build preprocessor	0.142857
each	core cross val predict estimator	0.045455
pairwise matrix	pairwise	0.066667
c such that for c in (l1_min_c	c x y	0.030303
names for estimators	core name estimators estimators	0.500000
estimate model parameters with the	base mixture fit x y	0.200000
possible outcomes for samples in	ensemble voting classifier predict	0.100000
for indices increasingly apart	verbosity filter index	0.055556
compute incremental mean and	utils incr mean	0.166667
axis center to the median	axis	0.014085
w	x w	0.250000
check initial parameters of	check parameters x	0.200000
center kernel	preprocessing kernel centerer transform k	0.500000
getter for the	covariance empirical covariance get	0.166667
shortest path length from source to all	shortest path length graph source	0.111111
building a cv in a user friendly way	check cv cv	0.031250
computes the log-likelihood of a gaussian	empirical covariance score	0.166667
handle the callable case for pairwise_{distances kernels}	pairwise callable x y metric	0.083333
pixel-to-pixel gradient connections edges are weighted	img mask return_as dtype	0.166667
probabilities for a calibration curve	core calibration curve y_true y_prob normalize n_bins	0.142857
of max	max	0.071429
c and cpp files	c and cpp files	0.500000
cv in a user friendly	check cv cv x	0.031250
loader for the california housing dataset	datasets fetch california housing data_home	0.250000
returns the number of splitting iterations in	leave pgroups out get n splits	0.111111
inverse the	cluster agglomeration transform inverse	0.500000
to the cache for the	externals joblib memorized	0.013699
a cv in a	cv cv x	0.031250
the first prime element	prime	0.111111
and false positives per	clf curve y_true y_score pos_label	0.250000
a conditional property using	iff has attr descriptor	0.083333
make and configure a copy of the base_estimator_	ensemble make estimator append random_state	0.166667
dataset along any axis center to the	axis	0.028169
returns the number of splitting iterations in	one out get n splits x y	0.111111
function used to fit an estimator within	fit estimator estimator x y sample_weight	0.071429
like assert_all_finite but only	utils assert all finite x	0.333333
kernel ridge model	kernel ridge	0.250000
generate the random projection matrix	random projection make random matrix n_components	1.000000
project	spectral biclustering project	1.000000
fit a single tree	trees tree forest x y	0.142857
logistic regression model	linear_model logistic regression path	0.333333
the gradient and the	x y	0.004310
and class probabilities	w x y	0.066667
used to compute log probabilities	parallel predict log proba	0.058824
and compute prediction of init	ensemble base gradient boosting init decision	0.142857
locally linear embedding	locally linear embedding x n_neighbors n_components	0.071429
format	coef	0.117647
the polynomial	polynomial	0.111111
by the callers	backend base effective	0.250000
building a cv in a user friendly way	core check cv cv x	0.031250
generate an array with constant block	datasets make	0.015625
for each input data	x	0.001692
project data to vectors and cluster the	project and cluster data vectors	0.333333
dispatch them	dispatch one	0.250000
regression score function best possible score is	score y_true y_pred sample_weight multioutput	0.062500
estimates the shrunk ledoit-wolf covariance	covariance ledoit wolf shrinkage x assume_centered	0.125000
local structure is retained	x_embedded n_neighbors precomputed	0.200000
tolerance which is independent of	tolerance x tol	0.058824
estimate model parameters with	fit x y	0.005988
number of splitting iterations in the cross-validator	get n splits x y groups	0.111111
compute the gradient of loss	compute loss	1.000000
check	decomposition latent dirichlet allocation check	0.062500
of the laplacian matrix and convert it	laplacian	0.034483
the bound	dpgmmbase bound	0.333333
the	decomposition	0.047619
return log-probability estimates for	base nb predict log proba	0.500000
count	core bernoulli nb count	1.000000
embedding read more in the :ref user guide	embedding	0.040000
edges for	make edges	0.066667
the timestamp when pickling to	memorized func reduce	0.050000
determinant with the	det fit	0.333333
fit the	multi output estimator fit	0.200000
format check x format	latent dirichlet allocation check	0.062500
create all the covariance	mixture distribute covar matrix to match covariance	0.250000
if y is in a multilabel format	is multilabel y	0.333333
quantile this classification dataset is constructed by	datasets make	0.015625
init	init decision	0.142857
transpose_data	transpose_data	1.000000
undo the scaling of x	preprocessing min max scaler inverse transform x	0.250000
and columns of x	x	0.001692
anova f-value for the provided sample	feature_selection f classif x y	0.200000
data loading for the lfw people dataset this	fetch lfw people	0.040000
linear embedding	linear embedding	0.166667
parameters of	params	0.114286
model with x	x	0.003384
construct a featureunion from the	core make union	0.250000
reduced likelihood function	gaussian_process gaussian process reduced likelihood function	0.047619
precision the precision is the ratio tp	precision	0.016667
path parameters	path	0.025641
according to	x y sample_weight	0.025974
the dual gap convergence criterion	dual gap emp_cov	0.071429
the number of splitting iterations in	one out get n splits x y	0.111111
this operation is meant to be cached	index_file_path data_folder_path slice_ color	0.033333
log-det	matrix_chol	0.125000
oracle approximating shrinkage algorithm	oas	0.100000
the file was opened for writing	joblib binary zlib file writable	0.250000
shutdown the process or thread	backend terminate	0.166667
single tree	trees tree forest x y	0.142857
constructs signature from the given list of	externals signature	0.050000
matrices w h whose	x w h n_components	0.038462
median absolute error regression loss read more	median absolute error y_true y_pred	0.166667
the shortest path	source shortest path	0.333333
that implement the partial_fit api need to be	utils check	0.023810
calibration curve	core calibration curve y_true	0.142857
l1 and l2 regularization coefficients	regularization alpha l1_ratio regularization	0.250000
for building a cv in a user friendly	core check cv cv x y	0.031250
on test vectors	core dummy regressor predict	0.250000
transform with	transform	0.011236
= - log sum_h exp(-e v h	v	0.052632
compute gaussian log-density at x for a spherical	log multivariate normal density spherical x means	1.000000
non-negative matrix factorization nmf find two	negative factorization x	0.043478
inplace column scaling of a	inplace column scale x	0.166667
transforms features by scaling each feature to	min max scaler	0.083333
private function used to compute decisions within a	ensemble parallel decision function estimators estimators_features x	0.500000
total log probability under the model	neighbors kernel density score	0.333333
filters the	ignore_lst	0.142857
hamming	hamming	1.000000
of a csr matrix in-place	utils inplace swap row csr x	0.250000
vectorizer	vectorizer	0.111111
and the gradient	and gradient w x y	1.000000
transform on the estimator with the	transform	0.011236
support_vectors	support_vectors	1.000000
representation of	shape	0.011765
a parallelbackend must implement	backend base	0.032258
the weighted graph of neighbors for points in	mixin radius neighbors graph	0.066667
x	gaussian_process exponentiation call x	0.200000
sure centering is not enabled for sparse matrices	robust scaler check array x	0.250000
sparse uncorrelated design this dataset is	datasets make sparse uncorrelated n_samples	0.166667
of exception	externals	0.005747
generate a	n_features	0.083333
of array-like or scipy sparse	preprocessing binarize x threshold copy	0.083333
predict using the kernel ridge model	kernel ridge predict x	1.000000
back the data to the	inverse transform	0.062500
updates terminal regions to median estimates	terminal	0.047619
more in the :ref user guide <mean_absolute_error>	y_true y_pred sample_weight multioutput	0.100000
the long type introduces	shape repr	0.013699
locally linear embedding analysis on the data	locally linear embedding x	0.071429
don't	memorized func	0.016949
based on laplace approximation	classifier laplace	0.333333
x for	x means covars	0.500000
the local outlier factor of	neighbors local outlier factor	0.125000
warning used when	warning	0.083333
compute the gradient of loss	multilayer perceptron compute loss	1.000000
p_ij	desired_perplexity verbose	0.500000
private function used to compute decisions within a	parallel decision function estimators estimators_features	0.500000
local	neighbors local	0.250000
handle the callable case for pairwise_{distances kernels}	callable	0.058824
autocorrelation parameters theta as the maximizer of	arg max	0.047619
to be captured	joblib parallel backend base get exceptions	0.166667
the l1 distances between	metrics manhattan distances	0.083333
compute area under the curve auc from	auc	0.020408
determination regression score	r2 score y_true y_pred sample_weight multioutput	0.125000
based on a	tree x connectivity n_clusters return_distance	0.250000
a subcluster from	subcluster new_subcluster1 new_subcluster2	0.166667
a contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true labels_pred	0.200000
input and compute prediction of init	gradient boosting init decision function	0.142857
from	externals joblib memorized func	0.013158
called with the given arguments	joblib memorized func	0.014706
computes the barycenter weighted graph of k-neighbors	barycenter kneighbors graph	0.333333
the one-vs-one multi class libsvm	svm one vs one	0.050000
of init	gradient boosting init decision function x	0.142857
function called with the given arguments	memorized func	0.016949
function call with the	format call func	0.100000
introduces an 'l'	shape	0.011765
fitted model	fit x y sample_weight	0.020000
thresholding	thresholding	0.857143
an estimator that simply predicts zero	zero estimator	1.000000
randomized svd	randomized svd m n_components	0.500000
windows cannot encode some characters in	joblib clean win chars string	0.333333
classifier on x	x	0.001692
a contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true	0.200000
a grid	grid	0.040000
outlyingness of observations	outlier detection mixin predict	0.250000
compute the weighted log probabilities	mixture base mixture score	0.111111
strip lines beginning with the	datasets strip	0.076923
model fitting method	fit x y	0.005988
best class label for each sample in	one vs one classifier predict	0.500000
finds seeds	seeds	0.111111
a dataset along any axis	axis	0.028169
directory in which are	get output dir	0.047619
training set according to	factor fit	0.062500
determines the blup parameters and evaluates the reduced	gaussian process reduced	0.125000
number of splitting iterations in	pgroups out get n splits	0.111111
feature	feature_extraction dict vectorizer get feature	0.200000
process regression model we can also predict based	process regressor predict	0.500000
project data to vectors and cluster the	spectral biclustering project and cluster data vectors n_clusters	0.333333
partially fit a	partial fit	0.222222
private function used to fit	fit	0.003257
fit the kernel density model on the	neighbors kernel density fit x y	0.250000
evaluate the accuracy of a classification	y_true y_pred labels	0.125000
centerer	centerer	1.000000
blobs for	make blobs	0.333333
list of	parallel backend	0.030303
inplace row scaling of a csr matrix	utils inplace csr row scale x scale	1.000000
of init	gradient boosting init decision	0.142857
this is the time it take	squeeze time t	0.166667
the dual gap convergence criterion the specific	covariance dual gap	0.071429
solve the linear	linear	0.062500
normalize rows and columns of	normalize	0.100000
k-neighbors	kneighbors mixin kneighbors x n_neighbors	0.125000
log of probability estimates	predict log proba x	0.045455
compute the unnormalized posterior log probability of x	base nb joint log likelihood x	0.200000
list of exception types	externals joblib parallel backend	0.029412
based on	tree x connectivity n_clusters return_distance	0.250000
embedding analysis	embedding x n_neighbors n_components	0.200000
exponentiate kernel by given exponent	exponentiation	0.166667
coefficient of determination regression score	metrics r2 score y_true y_pred sample_weight multioutput	0.125000
number	len	0.038462
evaluate the density model on	density score samples x	0.250000
the image from all of its patches	from patches 2d patches image_size	0.333333
to	externals joblib memory reduce	0.030303
predict class	ensemble forest classifier predict	0.666667
to be used for later scaling	scaler fit x	0.076923
predict confidence scores for samples	linear_model linear classifier mixin decision function x	0.500000
using	linear_model base	0.750000
scale	scale x	0.086957
back the data	preprocessing robust scaler inverse transform	0.066667
to split data into training and test	model_selection predefined split split x y groups	0.200000
for	x	0.001692
csgraph	validate graph csgraph	0.250000
between jobs	n_estimators n_jobs	1.000000
to compute log probabilities within	predict log proba	0.029412
initialization of the mixture parameters	gaussian mixture initialize x	1.000000
a contingency matrix describing the relationship	cluster contingency matrix labels_true labels_pred eps sparse	0.333333
class_probability	class_probability	1.000000
async	async	1.000000
right compressor file object in write mode	externals joblib write fileobject filename compress	0.500000
projection	projection	0.428571
neighbors within a given radius of a	neighbors lshforest radius neighbors x radius	0.142857
fit the model using x y	linear_model base randomized linear model fit x y	0.333333
get	sgdoptimizer get	0.125000
strategy	output	0.142857
generative	base pca get	0.076923
graphlasso covariance	covariance graph lasso cv	0.111111
model	base pca	0.071429
computes the free energy f v	neural_network bernoulli rbm free energy	0.066667
generator on parameters sampled from given distributions	parameter sampler	1.000000
the hash depending	memorized	0.015873
x y and	matern call x y	0.200000
the estimator with the best found parameters	base search cv predict proba	0.076923
full covariance	full x means covars min_covar	0.166667
the absolute error of the kl divergence of	kl divergence error	0.100000
linear assignment problem using	utils linear assignment	0.090909
for ridge and	x y	0.002155
predict class probabilities	ensemble bagging classifier predict proba	0.500000
inplace row scaling of	inplace row scale x	0.142857
c_step procedure described in [rouseeuw1984]_ aiming at computing	step x n_support remaining_iterations initial_estimates	0.111111
the solution to a sparse coding problem	decomposition sparse encode x dictionary gram cov	0.333333
fit on the estimator	fit x y	0.005988
generate isotropic gaussian blobs	make blobs n_samples n_features centers cluster_std	0.333333
verbose	verbose	0.562500
make and configure a copy	base ensemble make estimator append random_state	0.166667
a hash to	externals joblib hash	1.000000
class for	ensemble gradient boosting classifier	0.200000
for	covariance empirical covariance	0.071429
index of the leaf	apply x	0.166667
free energy f v =	neural_network bernoulli rbm free energy	0.066667
truncated svd	truncated x n_components	0.200000
the precision the precision is the ratio tp	precision	0.016667
voting classifier valid	ensemble voting classifier set	0.037037
opposite of the local outlier factor of	local outlier factor	0.125000
and dispatch them	joblib parallel dispatch one	0.250000
fits the oracle approximating shrinkage	oas fit	0.333333
random projection p only changes the	johnson lindenstrauss min dim n_samples	0.142857
omp solves n_targets orthogonal	linear_model orthogonal mp x	0.250000
the array from the meta-information and the z-file	joblib zndarray wrapper read unpickler	0.043478
a memmap instance to reopen on same	reduce memmap a	0.050000
model to data matrix x	x	0.001692
actual data loading for the lfw people dataset	datasets fetch lfw people	0.040000
number of splitting iterations in the cross-validator parameters	cviterable wrapper get n splits	0.111111
coefficient matrix to dense array format	coef mixin densify	0.100000
used in hastie et al	hastie 10 2 n_samples random_state	0.166667
more in the :ref user guide <mean_squared_log_error>	y_true y_pred sample_weight multioutput	0.100000
a random projection p only changes	core johnson lindenstrauss min dim n_samples eps	0.142857
windows this is the time it take to	joblib squeeze time	0.200000
indices	ensemble generate indices	1.000000
estimate model parameters with	base mixture fit x y	0.200000
the shrunk covariance model according	covariance shrunk covariance fit x	0.083333
number of splitting iterations in	model_selection base cross validator get n splits x	0.125000
and	beta divergence	0.500000
process classification	process	0.090909
to split data into	model_selection predefined split split x	0.250000
vector	w	0.035714
metrics should use this function first to assert	metrics	0.043478
compute mean and variance along an axix on	mean variance axis x axis	0.142857
log-det of the cholesky decomposition of matrices	cholesky matrix_chol covariance_type	1.000000
values for a given	train	0.117647
to	backend	0.016949
on	predict	0.006849
classification dataset is constructed by taking a	datasets make	0.015625
mlp loss function and its corresponding	multilayer perceptron	0.071429
leaf that each sample is predicted as	decision tree apply x check_input	0.500000
to catch and hide warnings without visual nesting	ignore warnings call fn	0.200000
classification dataset is constructed by taking	datasets	0.015152
vectors for reproducibility flips the sign	deterministic vector sign flip	0.066667
such that for c in	c x	0.030303
online computation	partial	0.086957
maximum to be used for later scaling	max scaler fit x	1.000000
generate	estimator	0.014706
matrix factorization nmf	non negative factorization x	0.043478
a large sparse	utils lsqr a	0.037037
the feature importances	decision tree feature importances	0.333333
an 'l' suffix when using the	shape repr	0.013699
search over	search	0.019231
each input data point	x y	0.002155
using just nearest	nn	0.166667
creates	spectral fit	0.250000
described in [rouseeuw1984]_ aiming at computing mcd	step x n_support remaining_iterations initial_estimates	0.111111
with the given arguments and persist	func call	0.047619
remove cache	externals joblib memory	0.016949
to this	externals joblib	0.004762
compute prediction of init	init decision function x	0.142857
the data onto the	x ridge_alpha	0.071429
mini-batch dictionary learning finds a dictionary a set	mini batch dictionary learning	0.142857
for each input data	estimator x y	0.038462
corresponding derivatives with respect to each	activations deltas	0.032258
wild lfw pairs	lfw pairs	0.018868
the training set x and returns	predict x y	0.043478
pickle the descriptors of a memmap instance to	memmap a	0.050000
a matrix of patch	patch extractor	0.090909
a general function given points on a curve	reorder	0.071429
gradient the gradient of the loss is	loss	0.027027
the directory in which	get output dir	0.047619
x (as	x	0.001692
the separating hyperplane	svm base lib svm decision function	0.333333
transformer from an arbitrary callable	function transformer	0.333333
generative model	pca get	0.076923
the paired cosine distances between x	paired cosine distances x	0.333333
subcluster	subcluster new_subcluster1 new_subcluster2	0.166667
dtype of x and y is	x y	0.002155
the reduced likelihood function for the given autocorrelation	process reduced likelihood function	0.047619
from	externals joblib memory	0.016949
extracts patches of any n-dimensional array in	patches	0.055556
generate	split iter	0.166667
logic estimators that implement	utils check partial	0.038462
cf tree for the input	cluster birch fit x	0.200000
and smooth feature	x y	0.004310
a func	externals joblib pool manager mixin apply async func	0.250000
fit with all sets	search cv fit x	0.111111
x and target y	neural_network base multilayer perceptron partial	0.166667
a calibration curve	core calibration curve y_true y_prob	0.142857
model by computing truncated	truncated x	0.200000
cache	joblib memorized func	0.014706
with likelihood terms for standard covariance types	x initial_bound precs means	1.000000
estimate the precisions	mixture bayesian gaussian mixture estimate precisions	0.166667
soft voting/majority rule classifier for unfitted estimators	voting classifier	0.035714
assert	assert	1.000000
validation	validate	0.142857
svmlight / libsvm format into	svmlight file f n_features dtype multilabel	0.066667
laplacian	laplacian	0.241379
the log probability under the model	mixture gmmbase score x	0.500000
leave one group out	leave one group out	0.200000
data augmented with n_zeros for the given rank	get elem at rank rank data n_negative n_zeros	1.000000
line_search_wolfe2 if suitable	wolfe12 f fprime xk pk	0.028571
in the wild lfw pairs dataset this	lfw pairs subset	0.035714
a lower bound on model evidence based	dpgmmbase lower bound	0.071429
bin_seeding	bin_seeding	1.000000
thread	externals joblib	0.004762
3d image	3d n_x n_y n_z	0.333333
compute the decision function of	ensemble ada boost classifier decision function	0.166667
filter select	select	0.200000
move	move	1.000000
the free energy f v = - log	rbm free energy	0.066667
loss	loss w x	1.000000
score is	score y_true y_pred beta	0.500000
decision_tree	decision_tree	1.000000
specified row returns	row row	0.166667
return	get	0.036145
fitted model	fit	0.003257
dataset of shape (n_samples n_features)	decomposition infer	0.200000
oracle approximating shrinkage covariance model according to the	covariance oas	0.083333
predict class	tree base decision tree predict	0.500000
given param_grid	get param iterator	0.166667
to avoid the hash depending	joblib	0.014599
all tokens in the raw documents	feature_extraction count vectorizer fit raw_documents y	0.125000
regularization	y pos_class cs	0.166667
curve auc using the trapezoidal rule this	auc x	0.040000
the model from	manifold spectral	0.111111
test	split iter	0.166667
predict class or regression value for x	decision tree predict x check_input	1.000000
does not need to	tree x y residual	0.500000
the pairwise matrix in	pairwise	0.066667
free energy f v	free energy	0.066667
sizes of training subsets	core translate train sizes	0.066667
the precision the precision is the	precision	0.016667
determine the optimal	parallel backend base compute	1.000000
lfw people dataset this operation is meant to	lfw people data_folder_path slice_ color resize	0.333333
estimate model parameters with the	mixture base mixture fit	0.200000
estimate model parameters	mixture base mixture fit x y	0.200000
the	joblib multiprocessing	0.052632
scaling of	scaler inverse transform	0.029412
dimension of	dimension	0.050000
computes the weighted graph of neighbors for points	radius neighbors graph	0.066667
derivatives with respect to each	activations deltas	0.032258
of transform is sometimes referred to by	transform	0.011236
is inefficient to train all	y classes	0.027778
in the wild lfw pairs	fetch lfw pairs	0.018868
bound term related to proportions	dpgmmbase bound proportions z	0.333333
score by cross-validation read more in	cross val score	0.166667
input	cross val predict	0.045455
in [rouseeuw1984]_ aiming at computing	step x n_support remaining_iterations initial_estimates	0.111111
std to be used	preprocessing standard	0.250000
is the depth	joblib memorized func check previous func code	0.333333
introduces an 'l' suffix when using	utils shape	0.013699
position	mds fit	0.066667
implement a single boost	weight boosting boost iboost x	1.000000
and false positives per binary	metrics binary clf curve y_true	0.090909
propagation classifier read more in the	propagation	0.076923
long type introduces an 'l' suffix when	shape repr	0.013699
finds seeds for	cluster get bin seeds x	0.250000
break the pairwise matrix	parallel pairwise	0.166667
checkerboard structure for biclustering	checkerboard shape n_clusters noise minval	0.066667
the grid of alpha values	alpha grid x y	0.166667
is restricted to the binary classification	score y_true y_score	0.025000
the covariance m	mixture covar	0.125000
private function used to build	ensemble parallel build	0.047619
learn empirical variances	feature_selection variance threshold	1.000000
generate	iter	0.050000
returns the bound term related to proportions	mixture dpgmmbase bound proportions z	0.333333
return the number of	externals joblib parallel	0.014085
the dual gap convergence criterion the	dual gap emp_cov precision_ alpha	0.071429
k-fold	kfold	0.058824
clustering on x	dbscan fit predict x	0.333333
is restricted to the binary classification	score y_true y_score average sample_weight	0.076923
laplacian kernel between x and y	laplacian kernel x y	0.333333
returns whether the kernel is	gaussian_process stationary kernel mixin is	1.000000
y2	y2	1.000000
input data	estimator	0.014706
score by cross-validation	core cross val score estimator x	0.333333
weighted graph of neighbors for points	mixin radius neighbors graph	0.066667
the median across axis 0	median axis 0	0.333333
to the average path	average path	0.142857
the sign of elements	sign flip	0.066667
for a	utils svds a	0.166667
used to fit an estimator within a job	ensemble parallel fit estimator estimator x y sample_weight	0.333333
locally linear embedding analysis on the	locally linear embedding x	0.071429
batch_size elements from	batch_size	0.100000
end	end	1.000000
model parameters with the em	mixture gmmbase fit x	0.250000
used by parallel	parallel	0.019231
diagonal	diag resp x nk	1.000000
the training set x and returns the labels	x y	0.002155
the process or thread	backend	0.016949
varies for mono and	x y	0.002155
when	utils shape repr	0.013699
the absolute error of the	error	0.020000
logz	logz	1.000000
private function used to partition estimators between jobs	ensemble partition estimators n_estimators n_jobs	0.200000
to	externals joblib memory	0.033898
fit the rfe model and then the underlying	rfe fit x y	0.250000
and a	y axis	0.250000
net model	net	0.166667
class or regression value	check_input	0.100000
predictions	predictions	1.000000
zfile	zfile	1.000000
collection of	vectorizer	0.066667
initialization of the gaussian mixture parameters	mixture gaussian mixture initialize x resp	1.000000
for	empirical	0.055556
train estimator on training subsets incrementally and compute	incremental fit estimator estimator x y	0.200000
single binary estimator one-vs-one	ovo binary estimator x y	0.500000
the ardregression model according	linear_model ardregression	0.100000
problem this dataset is described in friedman [1]	datasets	0.015152
the callable case for pairwise_{distances kernels}	metrics pairwise callable x	0.083333
error between two covariance estimators	covariance error	0.500000
estimate mutual information	feature_selection mutual info	0.500000
kernel between x and y : k x	kernel x	0.500000
long type	shape repr	0.013699
the callable case for pairwise_{distances kernels}	pairwise callable x y	0.083333
returns a list of edges for	feature_extraction make edges	0.066667
approximate nearest neighbors	neighbors lshforest kneighbors	0.500000
decision functions of the base classifiers	ensemble bagging classifier decision function	0.333333
path with coordinate	path	0.025641
count and smooth feature	core multinomial nb count x y	0.250000
patch_shape	patch_shape	1.000000
predict based	predict	0.006849
nb	nb	0.555556
type	utils shape	0.013699
unnormalized posterior log probability of x	core base nb joint log likelihood x	0.200000
transform the data and	transform x y	0.031250
of the data onto the	x ridge_alpha	0.071429
fit the model with x	core rbfsampler fit x	1.000000
the isotonic regression model	core isotonic regression y	0.066667
to avoid the hash depending from	externals joblib memorized	0.013699
number of splitting iterations in the cross-validator	split get n splits x y groups	0.111111
by scaling each	minmax scale x	0.142857
pairwise matrix in	pairwise x y	0.166667
gaussian process model fitting	gaussian_process gaussian process fit x	0.250000
estimate model parameters with	mixture base mixture fit x	0.200000
matrix factorization nmf find two	decomposition non negative factorization x	0.043478
loader for the labeled faces in	subset data_home	0.125000
component wise scale to unit	preprocessing scale	0.090909
func to be	backend apply async func	0.250000
don't	memory	0.015625
posterior probability of	proba x	0.111111
fit linear model with stochastic gradient descent	linear_model base sgdclassifier fit x y coef_init intercept_init	1.000000
all the content of the data home cache	clear data home	0.076923
factorization nmf find two non-negative matrices	negative factorization	0.043478
factor analysis fa	factor analysis	0.166667
scores note this implementation is restricted	roc	0.033333
the dual gap convergence criterion the	covariance dual gap emp_cov precision_	0.071429
whether the file	binary zlib file	0.125000
number of splitting iterations in the cross-validator	kfold get n splits x	0.111111
set the sample weight array	base sgd validate sample weight sample_weight n_samples	0.333333
private function used to	decision function estimators	0.333333
calibration curve	core calibration curve y_true y_prob normalize n_bins	0.142857
filters the	joblib filter args func ignore_lst	0.500000
of biclusters	cluster consensus	0.250000
largest k singular values/vectors for a sparse matrix	svds a k ncv tol	0.166667
probas_pred	probas_pred	1.000000
of x and dot	decomposition beta divergence x	0.250000
back the data	preprocessing standard scaler inverse transform	0.066667
dual	dual	1.000000
the shortest	utils single source shortest	0.333333
the data onto the	transform x ridge_alpha	0.071429
scores note this implementation is restricted to	roc	0.033333
compute cosine similarity between samples in	metrics cosine similarity	0.333333
evaluate a score by cross-validation	core cross val score estimator x	0.333333
format check x format	check	0.017857
last step in pipeline after transforms	core pipeline fit predict x	0.166667
and configure a copy of the base_estimator_ attribute	estimator append random_state	0.142857
update w	w	0.035714
likelihood of the data under the model	mixture dpgmmbase score samples x	0.200000
fit on	fit x	0.006410
batch and dispatch them	externals joblib parallel dispatch one batch	0.500000
cluster each sample in	cluster	0.021277
boolean mask indicating which features are selected	support mask	0.125000
x y and optionally its gradient	gaussian_process matern call x y eval_gradient	0.333333
two non-negative matrices w h whose	x w h	0.035714
to the binary classification task	y_true y_score	0.054054
list of edges for	edges	0.047619
multiple files in svmlight format this function is	svmlight files files n_features dtype multilabel	0.200000
samples x to the	x	0.003384
to update	error update	0.500000
we can also predict	predict	0.006849
checker utility for building a cv in a	core check cv cv x	0.031250
fix	fix	1.000000
isotonic regression model : min sum w[i]	isotonic regression y	0.066667
types to	externals	0.005747
concatenates results of	feature union	0.142857
returns distinct binary samples of length dimensions	datasets generate hypercube samples dimensions rng	1.000000
the right fileobject from a	externals joblib read fileobject fileobj	0.100000
callable case for pairwise_{distances	pairwise callable	0.083333
finds the k-neighbors of a	kneighbors mixin kneighbors x n_neighbors return_distance	0.250000
a locally linear embedding	manifold locally linear embedding	0.062500
for	classifier mixin	0.333333
function used to build a batch	parallel build	0.047619
the content of the data	clear data	0.142857
species distribution dataset from phillips et al 2006	species distributions	1.000000
under each gaussian in the	mixture gmmbase	0.034483
fit label encoder and return	label encoder fit transform y	0.200000
creating a class with a metaclass	externals add metaclass metaclass	0.166667
a size	externals	0.005747
header	header	0.454545
fit the model according to	svr fit x y sample_weight	0.250000
optimal	joblib auto batching mixin compute	0.333333
the query based on include_self param	neighbors query include self x include_self	0.333333
of the dual gap convergence criterion the	covariance dual gap	0.071429
the svmlight / libsvm	svmlight file f	0.066667
include_meta_estimators	include_meta_estimators	1.000000
normalize x according to	log normalize x	0.200000
estimates	cross val predict estimator x y	0.045455
timestamp when pickling to avoid	externals joblib memory reduce	0.030303
local outlier factor of	neighbors local outlier factor decision function	0.125000
a list of module names and a name	func name	0.047619
svmlight / libsvm format into sparse csr	svmlight file f n_features dtype multilabel	0.066667
for x relative to	metrics threshold scorer call clf x	0.058824
computes the free energy f v = -	rbm free energy	0.066667
number of splitting iterations in the	base kfold get n splits x y groups	0.111111
coverage error measure compute how far we	coverage error y_true y_score sample_weight	0.166667
nicely formatted statement displaying the function call	externals joblib format call func args kwargs object_name	0.333333
if dtype of x	x	0.001692
row of x	transform x y	0.031250
estimators that implement the	utils check partial fit	0.038462
fit	svm linear svc fit x y	0.333333
the generative	pca	0.047619
predict is invariant of	compute labels predict	0.250000
partially fit a single binary estimator one-vs-one	partial fit ovo binary estimator x	1.000000
compute the grid of alpha values	alpha grid x	0.166667
information	info	0.285714
get the parameters	classifier get	0.200000
using matrix product with the random matrix parameters	core base random projection transform x y	0.500000
base class for decision trees	base decision tree	1.000000
constructor store the useful information for	externals joblib ndarray wrapper init filename subclass allow_mmap	0.200000
temporary folder if still existing	delete folder folder_path warn	0.250000
fit subclasses should implement this method!	decomposition base pca fit	0.333333
predict multi-class targets using underlying	output code classifier predict	0.250000
the least-squares solution to a large	a	0.018182
object	joblib concurrency safe write to_write filename	1.000000
a mask to edges weighted	weights mask edges weights	0.333333
with the given arguments	externals joblib memorized func get output	0.125000
memmap instance to reopen on	reduce memmap	0.166667
the callable case for pairwise_{distances	metrics pairwise callable x y metric	0.083333
free energy f	bernoulli rbm free energy	0.066667
transforms and	x y	0.002155
path	enet path x	0.050000
inverse	cluster agglomeration transform inverse	0.500000
column class distributions	classes class_probability	0.166667
load the covertype	covtype	0.125000
probability calibration with sigmoid method platt 2000 parameters	core sigmoid calibration df y	0.500000
generate isotropic gaussian blobs	datasets make blobs n_samples n_features centers cluster_std	0.333333
friedman [1] and breiman [2]	make friedman3 n_samples	0.166667
mean update and a	utils incremental mean	0.250000
solution to a large sparse linear system	utils lsqr a	0.037037
type introduces an 'l' suffix when using the	utils shape repr	0.013699
of vectors for reproducibility flips the sign	deterministic vector sign flip	0.066667
lfw pairs dataset this dataset	datasets fetch lfw pairs	0.018868
undo the scaling of x according to	min max scaler inverse transform x	0.250000
the transformed	h	0.041667
the median and component wise scale	robust scale	0.125000
the kernel k	compound kernel	0.333333
spectrum spectrum	spectrum	0.090909
data precision matrix	get precision	0.052632
factor in place	code verbose	0.333333
x_squared_norms	x_squared_norms	1.000000
is invalid	undefined	1.000000
predict labels	predict	0.006849
cf tree for the input	cluster birch	0.090909
arguments	memorized func get output	0.125000
labels in a	label	0.045455
learn and	x y	0.002155
under the curve auc from	auc score	0.052632
return the feature	feature	0.055556
k x	matern call x	0.200000
tolerance which is independent of the dataset	cluster tolerance x	0.058824
number of splitting iterations in the cross-validator parameters	predefined split get n splits x y	0.111111
of vectors for reproducibility flips the sign	deterministic vector sign	0.066667
the log-likelihood of a gaussian data	score	0.010101
type suitable for	int	0.100000
a tolerance which is independent	tolerance x tol	0.058824
private function used to compute decisions within	parallel decision function estimators estimators_features	0.500000
epsilon-support vector regression	svr	0.142857
function for _fit_coordinate_descent update	update	0.035714
range	range finder	0.083333
loading for the lfw	fetch lfw	0.083333
for a full lars path parameters	omp path	0.100000
the neighbors within	radius neighbors	0.043478
check the precision matrices are symmetric and positive-definite	mixture check precisions full precisions covariance_type	1.000000
hash depending from	func	0.011364
optimal batch size	joblib auto batching mixin compute batch size	0.333333
getter	covariance get	0.166667
'l' suffix when using	shape repr	0.013699
too common	count vectorizer limit	1.000000
maximum absolute value to be used for	preprocessing max abs	0.050000
compute elastic net path with	path x	0.045455
estimates for each input data	predict estimator x y	0.045455
for updating terminal regions (=leaves)	loss function update terminal region tree terminal_regions leaf	0.200000
of regularization parameters	x y pos_class cs	0.166667
with the given arguments	func get output	0.125000
for each input	y	0.002674
setup	setup	1.000000
disk usage in a directory	joblib disk used path	0.250000
write array bytes to pickler	wrapper write array array pickler	0.333333
batch and	batch	0.111111
generate the	make	0.125000
ndarray	ndarray	1.000000
reduced likelihood function for the given	process reduced likelihood function	0.047619
computes the log-likelihood of a gaussian	covariance empirical covariance score	0.166667
the function with the given arguments and persist	func call	0.047619
print	base mixture print	1.000000
log-likelihood of a gaussian	covariance score	0.071429
solve the linear assignment problem using	linear assignment	0.090909
to multi-class	threshold	0.076923
scores note this implementation	metrics roc	0.040000
report showing the main classification metrics	metrics classification report	0.166667
the number of splitting iterations in the cross-validator	one out get n splits x y groups	0.111111
fit ridge regression model	linear_model ridge classifier fit x y sample_weight	1.000000
co-clustering algorithm dhillon 2001	coclustering	1.000000
samples	samples	0.368421
log-transformed bounds on	kernel bounds	0.333333
the decision boundary for each class	one vs rest classifier decision function x	0.250000
kwargs using a list	kwargs	0.076923
kernel k x y and optionally its gradient	gaussian_process dot product call x y eval_gradient	0.333333
the maximizer of the reduced	gaussian_process gaussian process arg max reduced	0.200000
mini-batch dictionary learning finds a dictionary a	batch dictionary learning	0.142857
determine the optimal batch	joblib auto batching mixin compute batch	0.333333
write the function code and the	func write func code	0.500000
local outlier factor	neighbors local outlier factor decision function	0.125000
and compute prediction of init	base gradient boosting init	0.142857
predict the	cv predict x	0.125000
be used when memory is inefficient to train	x y classes	0.027778
evaluate predicted target values for x	estimator x	0.030303
compute the initial centroids parameters	cluster init centroids x	0.166667
the maximum likelihood covariance estimator	covariance empirical covariance x assume_centered	0.166667
a regressor	regressor	0.027027
and evaluates the reduced likelihood function for	process reduced likelihood function	0.047619
loading for the lfw pairs dataset	datasets fetch lfw pairs	0.018868
regression score function	score y_true y_pred sample_weight	0.062500
in the raw documents	feature_extraction count vectorizer fit raw_documents	0.125000
list	get	0.012048
types to	externals joblib parallel backend base get	0.066667
and a	x y	0.002155
the linear assignment problem using	utils linear assignment x	0.090909
perform dbscan clustering from vector	cluster dbscan x eps min_samples metric	0.200000
get parameters of this	exponentiation get params deep	0.500000
as a sparse combination of the dictionary atoms	sparse coding mixin transform x	0.333333
cov	cov	0.500000
a tolerance which is independent of the dataset	cluster tolerance x tol	0.058824
to avoid the hash depending from it	externals joblib memory	0.016949
fit the model	fit transform	0.100000
utility function opening the right fileobject from a	read fileobject	0.100000
factorizing common classes	first call clf classes	0.058824
implement a single boost	ensemble base weight boosting boost iboost	1.000000
spherical	spherical	0.727273
the content of the data home	datasets clear data home data_home	0.076923
single tree	build trees tree forest	0.142857
a cv in a user	cv cv	0.031250
reduced	gaussian_process gaussian process reduced	0.125000
used to partition estimators	ensemble partition estimators	0.200000
cross-validated estimates for each input data point	predict estimator x y cv	0.071429
scale back the data to the	inverse transform	0.062500
raw_values	raw_values	1.000000
check initial parameters of the	base mixture check parameters x	0.200000
**kwargs, in the context of the memory	joblib memory	0.016949
that x	x predict x	0.333333
shift	shift x bandwidth	1.000000
search	core base search cv fit	0.166667
first blank line	header	0.090909
scale back the data to	preprocessing robust scaler inverse transform	0.066667
types to	joblib	0.007299
fits the graphlasso covariance model	covariance graph lasso cv	0.111111
usual api and hence	preprocessing binarizer fit x y	0.142857
log probabilities of possible outcomes for samples in	svm base svc predict log proba	1.000000
of exception	backend	0.016949
configure a copy of the base_estimator_ attribute	estimator append	0.142857
cross-validated	cv	0.045045
by scaling each feature to a given range	scale x	0.043478
fit label binarizer	preprocessing label binarizer fit y	0.500000
are selected returns	feature_selection selector	0.142857
returns the number of splitting iterations in	get n splits	0.111111
arbitrary python object into one file	joblib dump value filename	0.083333
dispatch	parallel dispatch one	0.250000
the data in	data compress	0.100000
and false positives per binary classification threshold	binary clf curve y_true y_score	0.090909
estimates for each input data	val	0.037037
bound for c such that for c in	c x y loss	0.030303
the score for a fit across one fold	feature_selection rfe single fit rfe estimator x y	0.200000
each input data	val	0.037037
w to minimize the	x w ht	0.250000
check a precision vector	check precision positivity precision	0.500000
don't store the	externals joblib	0.009524
the generative	base pca get	0.076923
projection	spectral	0.026316
contains only categorical features	one hot encoder fit	1.000000
fit a multi-class classifier by combining	linear_model base sgdclassifier fit	0.076923
progress	progress	0.500000
display the message on	joblib parallel print	0.166667
locally linear embedding analysis on	manifold locally linear embedding x	0.071429
for each input data point	core cross val predict estimator x	0.045455
utility function opening the right fileobject from a	externals joblib read fileobject fileobj	0.100000
factor analysis fa a simple	factor analysis	0.166667
absolute sizes of training	core translate train sizes	0.066667
tolerance which	tolerance x	0.058824
returns the number of splitting iterations in the	model_selection base cross validator get n splits x	0.125000
the curve auc using the trapezoidal rule this	auc x y	0.040000
reproducibility flips the sign of elements of	deterministic vector sign	0.066667
implement a single boost	ensemble base weight boosting boost iboost x	1.000000
returns distinct binary samples of	datasets generate hypercube samples	0.333333
precision ap from	precision	0.016667
number of splitting iterations in the	cviterable wrapper get n splits	0.111111
voting classifier valid parameter keys can be listed	voting classifier set params	0.037037
theta as the maximizer	arg max	0.047619
check x format check x format	dirichlet allocation check	0.062500
resize	resize	1.000000
cache	joblib	0.007299
the number of splitting iterations in	cviterable wrapper get n splits x y groups	0.111111
call	format call	0.200000
process or thread pool	multiprocessing	0.045455
the diagonal of the laplacian matrix	diag laplacian	0.111111
get parameters of	gaussian_process exponentiation get params	0.500000
we don't store the	func	0.011364
used in hastie	make hastie	0.125000
local outlier factor	local outlier factor decision	0.125000
model	decomposition base	0.076923
operation is meant	data_folder_path slice_ color resize	0.033333
to split data into	model_selection base kfold split	0.250000
of exception types to	backend base	0.032258
scale back the data to the original	scaler inverse transform x copy	0.066667
the model according to the given training	y sample_weight	0.035714
wise scale	robust scale	0.125000
given training data and	y	0.002674
needs_threshold	needs_threshold	1.000000
the hash depending from it	externals joblib memory	0.016949
aggressive classifier read more in the	aggressive classifier	0.166667
init	init decision function	0.142857
fit the model	core multi output estimator fit	0.200000
for all meta estimators in	meta estimator	0.062500
a name for	get func name	0.047619
signature from the given list of parameter	externals signature	0.050000
and evaluates the reduced likelihood function for	gaussian process reduced likelihood function	0.047619
mean shift	mean shift x bandwidth seeds	0.500000
the normalized laplacian	n_components eigen_solver	0.166667
a locally linear embedding analysis on	manifold locally linear embedding x n_neighbors	0.071429
return the shortest path	source shortest path	0.333333
then dtype	dtype	0.062500
estimate model parameters with	mixture base mixture fit x y	0.200000
regression score function best possible score is	score y_true y_pred	0.038462
compute non-negative matrix factorization nmf find two	factorization x	0.043478
integer indices corresponding to test sets	iterator iter test indices	0.333333
x and dot w	decomposition beta divergence x w	0.500000
indices in	indices	0.055556
precision matrix with the generative model	base pca get precision	0.066667
score on the given data if the	score x	0.033333
self	x_test	0.083333
estimates for each input data	cross val	0.038462
compute log probabilities within a job	predict log proba estimators estimators_features	0.250000
the pairwise matrix in	pairwise x y func	0.166667
computes the exponential chi-squared kernel x and y	metrics chi2 kernel x y	0.333333
binary classification task	curve y_true	0.125000
the callable case for	metrics pairwise callable x y	0.083333
don't store the timestamp when pickling	func reduce	0.050000
gaussian process classification based on	gaussian process	0.083333
data for binary classification used in hastie	datasets make hastie	0.125000
text report showing	report	0.047619
generate train test	base	0.014286
breast cancer wisconsin	breast cancer return_x_y	0.250000
the mean silhouette coefficient	metrics cluster silhouette score x labels metric	0.250000
last step in pipeline after transforms	pipeline fit predict x y	0.166667
can be different from the	calibrated classifier	0.083333
models for feature selection this	model	0.058824
cross-validated estimates for each input	cv	0.009009
degrees_of_freedom	degrees_of_freedom	1.000000
sample weights by	compute sample	0.100000
in an unfitted	estimators unfitted name	0.142857
compute	metrics	0.043478
sparse format well suited for eigenvalue decomposition	value norm_laplacian	0.142857
restricted to the binary classification task	y_true y_score average	0.076923
and	transform y	0.023256
predict using the gaussian	gaussian_process gaussian	0.250000
for full	normal density full x means covars	0.166667
the best found	model_selection base search cv predict	0.076923
func to	backend base apply async func	0.250000
model using x	x	0.006768
the loss of prediction pred and y	ensemble loss function call y pred sample_weight	0.333333
for the voting classifier valid	voting classifier set params	0.037037
input validation for standard estimators	y accept_sparse dtype	0.250000
a projection	spectral	0.026316
output of transform	transform y	0.023256
patches of	patches	0.055556
fit the model to	estimator fit	0.200000
input data point	core cross val	0.043478
y is of a	y	0.002674
cosine distances between	cosine distances	1.000000
of theta	theta	0.076923
func to	sequential backend apply async func	0.250000
convert coefficient	linear_model	0.025641
data x with ability to	x	0.001692
generate	cross val predict estimator x	0.045455
em	allocation em	1.000000
of	joblib	0.007299
filters the given args and kwargs using	filter args func ignore_lst args kwargs	0.333333
transform binary labels back	inverse transform y	0.500000
function opening the right fileobject from a	externals joblib read fileobject fileobj	0.100000
min sum w[i] (y[i] - y_[i]) ** 2	sample_weight y_min y_max	0.166667
in-place	swap	0.166667
from the training set x y	fit x y	0.023952
cross-validated estimates for each input data	predict estimator x y cv	0.071429
the log-likelihood of a gaussian data set	covariance score	0.071429
x y and optionally its gradient	call x y eval_gradient	0.550000
the descriptors of a memmap instance to	reduce memmap a	0.050000
bayesian	bayesian	0.714286
a tolerance which is	tolerance x	0.058824
last step in pipeline after transforms	pipeline fit	0.166667
fit linear	fit	0.016287
k x	x	0.006768
matrices w	x w	0.083333
non-negative matrix factorization nmf find	factorization x	0.043478
dense array format	coef mixin densify	0.100000
strip lines beginning with the	strip	0.055556
for factorizing common classes param logic estimators	first call clf classes	0.058824
the posterior log probability of the samples x	bernoulli nb joint log likelihood x	0.500000
rcv1 multilabel	rcv1	0.125000
for the voting classifier valid parameter keys	voting classifier	0.035714
lower bound on model evidence based on	dpgmmbase lower bound	0.071429
hierarchical clustering	cluster feature agglomeration	0.125000
lad updates terminal	least absolute error update terminal region tree	0.200000
used for later	fit	0.003257
is not found and raise	utils line search	0.029412
shutdown	multiprocessing backend terminate	0.166667
cv and	x y	0.002155
n_jobs even slices	y func n_jobs	0.166667
fit	rbfsampler fit	0.250000
for parallel processing this	parallel	0.019231
single tree	build trees tree	0.142857
call predict_log_proba on the estimator with	predict log proba	0.029412
sample image parameters	sample image image_name	0.166667
diagonal of the kernel k x x	pairwise kernel diag x	1.000000
to compute log probabilities	parallel predict log proba	0.058824
median	median	0.533333
em update	allocation em step x	0.500000
one set of parameters	point x y estimator parameters	1.000000
check that y_true and y_pred belong to the	metrics check reg targets y_true y_pred multioutput	0.333333
feature importances	decision tree feature importances	0.333333
for the one-vs-one multi class libsvm in the	one vs one	0.050000
a	externals joblib parallel	0.028169
scaler	standard scaler	0.333333
disk usage	disk used path	0.250000
computes the position of the	mds fit	0.066667
et al	10 2 n_samples random_state	1.000000
cv in a user friendly	cv cv x	0.031250
model to data matrix x and target y	base multilayer perceptron partial	0.166667
full	normal density full x means covars min_covar	0.166667
regression score	score y_true y_pred sample_weight multioutput	0.062500
for the voting classifier	voting classifier set params	0.037037
return the directory in which	output dir	0.047619
implement a single boost	ensemble ada boost classifier boost iboost x	1.000000
x and	x	0.001692
:ref user guide <mean_squared_log_error>	y_true y_pred sample_weight multioutput	0.100000
the parameters for the voting classifier valid	ensemble voting classifier set	0.037037
similarity	similarity	1.000000
the boolean mask	mask	0.142857
est weight tuples excluding none transformers	core feature union iter	0.333333
don't store the timestamp when pickling to	func reduce	0.050000
centroids_	cluster birch	0.090909
data loading for the lfw people	datasets fetch lfw people	0.040000
local outlier factor	local outlier factor	0.125000
deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call	0.333333
check according to li et	core check	0.111111
rows of a csc matrix in-place	utils inplace swap row csc x m n	0.250000
fit	svm linear svc fit	0.333333
be used when memory is inefficient to	classes	0.025641
w h	x w h	0.035714
step for diagonal	diag	0.031250
predict class or regression value for x	tree predict x check_input	1.000000
line	line	1.000000
the shortest path length from source to	single source shortest path length graph source	0.111111
median absolute	metrics median absolute	1.000000
coefficient	sparse	0.025000
into an embedded space	manifold tsne	1.000000
spherical wishart distribution parameters	wishart spherical nk	0.333333
name for	func name	0.047619
and dispatch them	joblib parallel dispatch	0.250000
relative	metrics predict scorer call	1.000000
file supports seeking	file seekable	0.250000
for all meta estimators in	meta	0.043478
scale back the data to the original representation	inverse transform x	0.051282
collect results from clf predict calls	classifier collect probas x	1.000000
global clustering for the subclusters	cluster birch global clustering	0.142857
fit_predict of last step in pipeline after transforms	pipeline fit predict x	0.166667
samples by quantile this classification dataset is	datasets	0.015152
tokens in the raw documents	feature_extraction count vectorizer	0.125000
a memmap instance	memmap a	0.050000
detects the soft boundary of	svm one class svm fit	0.125000
of exception types	backend base	0.032258
calculate mean update and a youngs and	utils incremental mean and	0.500000
grid of points	ensemble grid	0.111111
area under the curve auc	metrics auc x	0.040000
a	finder a	0.500000
metrics should use this function first	metrics	0.043478
of the data home cache	data home data_home	0.055556
swaps two columns of a csc/csr matrix in-place	utils inplace swap column	0.250000
a pickler to persist big data efficiently	numpy pickler	0.500000
compute the initial centroids parameters	cluster init centroids	0.166667
solve the isotonic regression model : min sum	core isotonic regression	0.055556
list_of_arrays	list_of_arrays	1.000000
load sample	load sample	1.000000
whose range approximates the range of a	range finder a	0.166667
the number of splitting iterations in the	base kfold get n splits x	0.111111
types to	parallel backend base	0.037037
the posterior log probability of the samples	nb joint log likelihood	0.066667
a transform	x transform	0.333333
transform binary labels back to multi-class labels	preprocessing label binarizer inverse transform y threshold	0.333333
weighted graph of neighbors for points in	neighbors graph	0.066667
check initial parameters of the derived class	mixture check parameters	0.166667
the model according to the given	y sample_weight	0.035714
the maximum likelihood estimator covariance model according to	covariance empirical covariance fit x	0.166667
terminal regions to	terminal region	0.100000
output for x relative to	metrics threshold scorer call clf x	0.058824
find the first prime element in the	utils hungarian state find prime in	0.333333
estimator and predict values for a given	and predict estimator x y train	0.200000
fit a single tree in parallel	ensemble parallel build trees tree forest x y	0.200000
a mostly low rank matrix with	make low rank matrix	0.083333
of init	boosting init decision function	0.142857
for full covariance matrices	full x	0.166667
the free energy f v = -	rbm free energy	0.066667
display the process of the parallel execution only	externals joblib parallel print	0.125000
hash depending from it	joblib memorized	0.015625
classification task	metrics precision recall curve y_true	0.142857
opposite of the local outlier	local outlier	0.250000
that implement	utils check	0.023810
the model to the	neural_network bernoulli	0.333333
random forest regressor	random forest regressor	1.000000
group out	group out	0.142857
variational bayesian estimation	bayesian	0.142857
inplace column scaling of a csc/csr	inplace column scale x scale	0.166667
on x y and get	fit x y	0.005988
signature from	externals signature	0.050000
perform a locally linear embedding analysis	locally linear embedding	0.050000
of the cf	birch get	0.333333
elastic net path with coordinate	path x	0.045455
back the data to the original	robust scaler inverse transform	0.066667
process	joblib multiprocessing	0.052632
fit linear model with passive aggressive algorithm	passive aggressive classifier fit x	1.000000
byte string to the file	zlib file	0.076923
factorization nmf	factorization	0.035714
sign of elements	sign	0.050000
the lrd of a sample is the	distances_x neighbors_indices	0.047619
or	or	1.000000
estimate model parameters	fit x y	0.005988
actually run in parallel	joblib parallel	0.028571
learning rate and potentially other states at	neural_network base optimizer iteration ends time_step	0.142857
and	transform x y	0.093750
of exception	externals joblib parallel backend base	0.034483
spatial independence correlation model pure	gaussian_process pure	1.000000
fileobj	fileobj	0.777778
each input	core	0.015385
shortest path length from source to all	source shortest path length graph source	0.111111
the l1 distances between	manhattan distances	0.083333
and compute	x y classes	0.055556
determine absolute sizes of training	sizes	0.050000
function used to fit an estimator within	fit estimator estimator x y	0.071429
pickle the descriptors of a memmap instance	memmap a	0.050000
false positives per binary	metrics binary clf curve y_true y_score	0.090909
don't store the timestamp when pickling to avoid	joblib memorized func reduce	0.050000
kernel k	gaussian_process exp sine	1.000000
tolerance which is independent	tolerance x tol	0.058824
of a	a	0.036364
multiple files in svmlight format this function	svmlight files files n_features dtype multilabel	0.200000
model using x	x y	0.002155
single boost using	boost	0.062500
loader for the labeled faces in the	subset data_home	0.125000
fits the oracle approximating shrinkage covariance model according	covariance oas fit x	0.083333
descent fit is on grid of	fit	0.003257
on x y and	fit x y	0.005988
case of a multinomial	linear_model multinomial	0.200000
directed	directed	1.000000
single binary estimator	binary estimator x y classes	0.500000
helper	utils parallel helper	0.500000
of the leaf	decision tree apply	0.166667
representation of an array shape under python 2	shape	0.011765
training	cv	0.009009
in a one-vs-all fashion several regression	classes neg_label pos_label	0.333333
linear model with passive aggressive algorithm	passive aggressive classifier	0.125000
suffix when using the	shape	0.011765
full covariance	normal density full x	0.166667
the score of the underlying	score	0.010101
on x by	fit x y	0.005988
each input data	cross val predict	0.045455
introduces an 'l' suffix when	repr	0.012500
each input data	val predict	0.045455
a cv in	core check cv cv	0.031250
training set x and returns the labels	predict x y	0.043478
compute class covariance matrix	core class cov x y	0.250000
function call	externals joblib format call func	0.100000
a large sparse linear system of equations	a b damp atol	0.500000
matching pursuit omp solves n_targets orthogonal matching pursuit	linear_model orthogonal mp x y n_nonzero_coefs tol	0.200000
det	det	0.357143
bicluster	core bicluster mixin get	0.500000
n_past	n_past	1.000000
input data	val predict estimator x	0.045455
mean and	utils mean	0.250000
- log sum_h exp(-e v h	v	0.052632
lower bound for the concentration parameter	bound concentration	1.000000
shutdown the process	multiprocessing backend terminate	0.166667
y_true	y_true	0.152174
a dataset along any axis center to	x axis	0.030769
for the case method='lasso' is :	y xy gram	0.090909
avoid the hash depending from	externals joblib memory	0.016949
the function	func	0.011364
outlyingness of observations in x according to the	covariance outlier detection mixin predict x	0.250000
the file	joblib binary zlib file	0.200000
to fit an estimator within a	fit estimator estimator x y sample_weight	0.071429
h	h beta_loss	0.333333
classification dataset is constructed	datasets	0.015152
signature from the	externals signature	0.050000
mean and component wise scale	scale x	0.043478
based on a feature matrix	connectivity n_clusters	0.250000
a platform	shape	0.011765
scores note this implementation is restricted	metrics roc	0.040000
list of module names and a name	get func name	0.047619
binary classification	y_true y_score	0.054054
of the parallel execution only	externals joblib parallel	0.014085
a lower bound on model evidence based	lower bound	0.071429
returns cluster labels	cluster	0.021277
model according to the given	y sample_weight	0.035714
compute the mean silhouette coefficient	metrics cluster silhouette score x labels	0.250000
n_jobs	func n_jobs	0.166667
cross-validated estimates for each input	cross val predict estimator x y cv	0.071429
prediction of init	ensemble base gradient boosting init decision function x	0.142857
mcd from	covariance select candidates x n_support	1.000000
kddcup99 dataset downloading it	kddcup99 subset data_home download_if_missing random_state	0.111111
precision is	metrics precision	0.033333
x	function x	0.060606
k x y and optionally its gradient	dot product call x y eval_gradient	0.333333
returns the number of splitting iterations in	leave one group out get n splits x	0.111111
decision function output for x	x	0.001692
make	base ensemble make	1.000000
matrix shrunk on the diagonal read more in	covariance shrunk	0.066667
used to fit an estimator within a	fit estimator estimator	0.055556
optimization objective for the case method='lasso' is	y xy gram	0.090909
method for updating terminal regions (=leaves)	loss function update terminal region tree terminal_regions leaf	0.200000
introduces an	utils	0.009709
linkage agglomerative clustering based on a feature matrix	cluster linkage tree x connectivity n_components n_clusters	1.000000
a sparse random matrix given	utils random choice csc	0.333333
mostly low	datasets make low	0.333333
back the data	scaler inverse transform	0.058824
net path	enet path x	0.050000
func	pool manager mixin apply async func	0.250000
number	n	0.050000
global clustering for the subclusters obtained	birch global clustering x	0.142857
samples x	x y	0.002155
returns the score on	score x	0.033333
function for factorizing common classes param logic	fit first call clf classes	0.058824
apply transforms and predict_log_proba	predict log proba x	0.045455
list of	externals joblib	0.004762
parameters for the voting classifier valid parameter	ensemble voting classifier set params	0.037037
compute log probabilities	predict log proba	0.029412
compute the	base multilayer perceptron compute	0.250000
scaling of x according to	scaler inverse transform x	0.026316
read up to	read	0.052632
two rows of a csr matrix in-place	utils inplace swap row csr x	0.250000
trained model parameters	multilayer perceptron	0.071429
a buffered version of a read file object	externals joblib buffered read file fobj	0.500000
compute the l1 distances between the vectors in	metrics manhattan distances	0.083333
calculate the posterior log probability of	core bernoulli nb joint log likelihood	0.083333
kernel k x	gaussian_process compound kernel call x	0.333333
returns a lower bound on model	dpgmmbase lower bound	0.071429
convert coefficient matrix to dense array format	coef mixin densify	0.100000
project data to maximize class separation	core linear discriminant analysis transform	0.250000
weights are valid	weights weights	1.000000
the number of splitting iterations in the	pgroups out get n splits x y groups	0.111111
the decision function of the given observations	covariance outlier detection mixin decision function x	0.333333
gaussian process regression model we can also predict	gaussian_process gaussian process regressor predict	1.000000
x and y is float32 then	x y	0.002155
scale back the data to the original representation	preprocessing standard scaler inverse transform	0.066667
returns the coefficient of determination r^2	multi output regressor score	0.200000
the linear assignment problem using	linear assignment x	0.090909
fit the	fit	0.074919
a list of feature name -> indices mappings	dict vectorizer fit	0.250000
used to compute log probabilities within a job	ensemble parallel predict log proba estimators estimators_features x	0.250000
the model and transform with	transform	0.011236
loss	loss grad	0.250000
boosted regressor from the training set x	ensemble ada boost regressor fit x	1.000000
cf tree	cluster birch fit x	0.200000
of the decision functions of the base	decision function	0.025000
matrix product with the random matrix parameters	core base random projection transform x y	0.500000
compute the maximum absolute value to be used	max abs	0.047619
x y and	x y	0.028017
the grid of alpha values	linear_model alpha grid x y xy	0.166667
on the estimator with the best found	search cv predict	0.074074
fit label encoder and return encoded	preprocessing label encoder fit transform y	0.200000
x_embedded	x_embedded	1.000000
recall is	metrics recall	0.033333
backend	backend base	0.032258
weighted graph	graph	0.085106
fit the model using x as	neighbors local outlier factor fit x y	0.333333
arbitrary python object	dump value filename	0.083333
posterior probabilities of	cv predict proba x	0.034483
list of feature name -> indices mappings	dict vectorizer fit	0.250000
collection of text documents	vectorizer	0.044444
an arbitrary python object into one file	filename	0.050000
linear assignment problem using the hungarian	linear assignment	0.090909
returns the number of splitting iterations in	model_selection leave one out get n splits x	0.111111
estimates	core cross val predict	0.045455
wild lfw	lfw	0.034483
transform binary labels back to multi-class	label binarizer inverse transform y threshold	0.333333
estimates the minimum covariance determinant matrix	covariance fast mcd x support_fraction cov_computation_method random_state	0.250000
compute non-negative matrix factorization nmf find two	negative factorization	0.043478
we don't	joblib memorized func	0.014706
trace of	decomposition trace	0.500000
partially fit a single binary estimator	partial fit binary estimator x	1.000000
number of	len	0.038462
return a	repr	0.012500
quantiles to be used for scaling	preprocessing robust scaler fit	1.000000
the hash	externals joblib memory	0.016949
factorization nmf find two non-negative matrices w	decomposition non negative factorization x w	0.500000
outlyingness of observations in	covariance outlier detection mixin predict	0.250000
extent the local structure is retained	manifold trustworthiness x x_embedded n_neighbors precomputed	0.200000
representation	utils shape repr	0.013699
to avoid the hash	memorized	0.015873
build a contingency matrix describing	contingency matrix labels_true	0.166667
the	backend	0.016949
used when memory is inefficient to train	classes	0.025641
passive aggressive regressor read more in the :ref	passive aggressive regressor	0.125000
generate train test	base shuffle split iter	0.166667
local outlier factor of	local outlier factor	0.125000
score on the given data	score	0.010101
the lrd of a sample	distances_x neighbors_indices	0.047619
number of splitting iterations in	pgroups out get n splits x y groups	0.111111
timestamp when pickling to avoid	joblib memory reduce	0.030303
reducer function to a	externals joblib customizable pickler register	0.200000
suffix when	repr	0.012500
calculate approximate log-likelihood as score	decomposition latent dirichlet allocation score	1.000000
isotonic regression model : min sum w[i] (y[i]	isotonic regression	0.055556
one-vs-one multi class libsvm in the	one vs one	0.050000
best class label for each sample in	core one vs one classifier predict	0.500000
arguments	memorized func get	0.125000
rand index adjusted for	adjusted rand score labels_true	0.333333
tokens in the raw documents	feature_extraction count vectorizer fit	0.125000
type introduces an	repr	0.012500
lfw people dataset this operation is meant to	fetch lfw people data_folder_path slice_ color resize	0.333333
building a cv in a user friendly	cv cv x	0.031250
restricted to the binary classification	y_true y_score average	0.076923
lfw people dataset	datasets fetch lfw people	0.040000
retrieve or aggregate feature importances from estimator	feature importances estimator norm_order	1.000000
number of splitting iterations in the	model_selection predefined split get n splits	0.111111
meta estimators	meta	0.043478
least squares	ensemble least squares	1.000000
report showing the main classification metrics read more	metrics classification report	0.166667
fit linear model with passive aggressive algorithm	linear_model passive aggressive classifier partial fit x y	1.000000
used to fit an estimator within	fit estimator estimator x y sample_weight	0.071429
compute the sigmoid	sigmoid	0.111111
convert coefficient matrix to dense array format	densify	0.066667
kernel between x	kernel x	0.250000
estimator adheres to	estimator estimator	0.052632
don't	func	0.011364
svmlight / libsvm	svmlight file f n_features dtype	0.066667
e-step in em update	decomposition latent dirichlet allocation e step x	1.000000
callable case	metrics pairwise callable x y metric	0.083333
finds indices in sorted array of integers	indices tree bin_x left_mask right_mask	0.166667
func to be	joblib sequential backend apply async func	0.250000
threshold	threshold estimator importances threshold	0.500000
read	read	0.578947
lasso	lasso	0.875000
data and y	y	0.005348
persist an arbitrary python object into one	externals joblib dump value filename compress protocol	0.250000
transform a sequence of instances to	feature hasher transform raw_x	0.333333
rare or	feature_extraction	0.037037
when using the	shape repr	0.013699
fit a	fit	0.016287
return the kernel k x y and	dot product call x y	0.200000
to capture the arguments of a function	function check_pickle	0.333333
we	externals joblib	0.009524
full covariance	normal density full	0.166667
global clustering for the subclusters	cluster birch global clustering x	0.142857
checker utility for building a cv in	cv cv x y classifier	0.031250
scale each feature by	scaler	0.031250
uncorrelated design	uncorrelated n_samples	1.000000
on x by chunking it into mini-batches	cluster mini batch kmeans fit x y	0.500000
returns the number of splitting iterations in the	pgroups out get n splits x y	0.111111
predict_log_proba on the estimator with the	predict log proba	0.029412
models computed by 'path' parameters	linear_model path residuals x y train	0.250000
back the data to the original representation	scaler inverse transform	0.058824
random sample from a given 1-d array	utils choice a size replace p	0.250000
sparse random	random choice	0.166667
as training	core isotonic regression	0.055556
also predict based on an unfitted	predict	0.006849
linear models	linear model	0.090909
sparse inverse covariance estimation with an l1-penalized estimator	graph lasso	0.500000
directory in which	dir	0.038462
by scaling	scale x	0.043478
best	cv predict	0.083333
base class for	base	0.185714
parameters	core base	0.416667
pickle the descriptors of a memmap instance to	reduce memmap a	0.050000
is	y_true	0.021739
like assert_all_finite but only for ndarray	utils assert all finite x	0.333333
predict	base decision tree predict	0.500000
each	core cross val predict estimator x y	0.045455
recall is the ratio tp / tp	recall	0.028571
compute class covariance	class cov	0.250000
precisions parameters of the precision	precisions nk xk sk	0.166667
new parallel backend factory	parallel backend name factory make_default	1.000000
a platform independent representation	shape repr	0.013699
h whose product approximates the non-	h n_components	0.166667
opening the right fileobject from a filename	externals joblib read fileobject fileobj filename	0.250000
an estimator finds all parameters ending	ensemble set random states estimator	0.333333
in the svmlight / libsvm format	svmlight file f n_features dtype multilabel	0.066667
in place	code verbose	0.333333
this dataset is described in	datasets make	0.031250
factor of x (as bigger is	factor decision function x	0.166667
predict is invariant	clusterer compute labels predict	0.250000
and scaling parameters	transform x y	0.031250
axis center to the median and	x axis	0.015385
the shortest	source shortest	0.333333
read more in the :ref user guide <mean_absolute_error>	y_true y_pred sample_weight multioutput	0.100000
compute the log probability under the model	mixture gmmbase score x	0.500000
load and return the breast cancer wisconsin dataset	datasets load breast cancer return_x_y	1.000000
number of splitting iterations in the cross-validator	leave pgroups out get n splits x y	0.111111
pairs dataset this operation	pairs	0.055556
arr	arr	1.000000
the initial centroids parameters	cluster init centroids x k	0.166667
stratified shufflesplit cross-validator provides train/test indices	stratified shuffle	0.333333
store the	externals joblib memorized	0.013699
array from the meta-information and the	zndarray wrapper read unpickler	0.043478
is inefficient to train	x y classes	0.027778
object representing	memorized result	0.500000
for full covariance matrices	multivariate normal density full x	0.166667
which are going to	joblib multiprocessing backend effective	0.250000
the unpickler to unpickle our numpy pickles	numpy unpickler	0.333333
net path	path	0.025641
and maximum along an axis on a	max axis x axis	0.142857
other and transforms	y	0.002674
found and raise	search	0.019231
incrementally fit the model to data	core multi output regressor partial fit	0.200000
ridge regression model parameters	linear_model base ridge cv	1.000000
compute the polynomial	metrics polynomial	0.333333
estimate the precisions parameters	bayesian gaussian mixture estimate precisions	0.166667
evaluates the reduced likelihood function for the	process reduced likelihood function	0.047619
calculate the posterior log probability of the	core bernoulli nb joint log likelihood	0.083333
model using x y	x y sample_weight	0.012987
return a tolerance which is	cluster tolerance x	0.058824
subsets incrementally	core incremental	1.000000
is a	joblib is	0.500000
for the california housing	datasets fetch california housing	0.083333
coefficient matrix	linear_model sparse	0.076923
all methods a parallelbackend must implement	parallel backend base	0.037037
fit to data then transform it	core transformer mixin fit transform x	0.500000
the k-neighbors of a	neighbors kneighbors mixin kneighbors	0.100000
length from source to all reachable	length graph source	0.200000
set x	predict x	0.011765
the number of splitting iterations in	model_selection leave one out get n splits x	0.111111
generate cross-validated estimates for each	val predict estimator x y cv	0.071429
transform is	transform	0.011236
columns of a csc/csr matrix in-place	utils inplace swap column x	0.250000
to avoid the hash depending from	joblib memory	0.016949
reachability	reachability	1.000000
center to the median and component wise scale	robust scale x	0.125000
boolean thresholding of array-like or scipy sparse matrix	binarize	0.045455
shutdown the process or thread	joblib multiprocessing backend terminate	0.166667
computes the weighted graph of	graph	0.063830
and false positives per binary	binary clf curve y_true y_score pos_label	0.090909
full lars path parameters	path	0.025641
data	cross val predict estimator x	0.045455
make predictions using a single binary estimator	binary estimator x	0.090909
which are going to run in	externals joblib multiprocessing backend	0.035714
precision the precision is the	metrics precision	0.033333
train	iter	0.050000
folders	externals	0.005747
distances and a parameter weights	weights dist weights	0.142857
gaussian process regression model we can	gaussian process regressor	0.055556
objective for the case method='lasso' is	x y xy gram	0.090909
indices to split data into	model_selection predefined split split	0.250000
matrices w h whose product approximates the	w h	0.031250
object	externals	0.005747
of feature names ordered by their indices	vectorizer get feature names	0.125000
parameters for an estimator finds all parameters ending	ensemble set random states estimator	0.333333
get parameters of this kernel	gaussian_process kernel operator get params deep	1.000000
estimates the shrunk ledoit-wolf	ledoit wolf x assume_centered block_size	0.250000
predict using the trained model	multilayer perceptron predict	0.333333
predict class probabilities	classifier predict proba	0.500000
partially fit underlying estimators should be	core one vs one classifier partial fit	0.166667
get the values used to update	sgdoptimizer get	0.125000
independent	shape	0.011765
logistic loss and gradient	logistic loss and grad w x	0.500000
itree which is equal to the average	ensemble average	0.125000
and false positives per	clf curve	0.250000
note	metrics roc	0.040000
transform feature->value dicts to array or sparse	feature_extraction dict vectorizer transform	0.200000
for each input data point	cross val predict	0.045455
store the	externals	0.011494
and dispatch	dispatch one	0.250000
backend and return the number	externals joblib parallel backend base	0.034483
perform the	responsibilities	0.142857
a platform independent representation of	repr	0.012500
get number of	utils get n	0.500000
cv and linearsvc	liblinear x y c fit_intercept	0.142857
on the training	fit predict	0.055556
a cv in a user	cv cv x	0.031250
computes the weighted graph of k-neighbors for	neighbors kneighbors mixin kneighbors graph	0.250000
load the kddcup99 dataset downloading it if necessary	kddcup99 subset data_home download_if_missing random_state	0.111111
coverage error measure	metrics coverage error y_true y_score	0.166667
loader for the california housing dataset	fetch california housing data_home	0.250000
init	split init	1.000000
prediction scores note this implementation is restricted	metrics roc	0.040000
to the average	ensemble average	0.125000
time it take to	joblib squeeze time t	0.200000
the number of splitting iterations in	get n splits x	0.111111
theta as the maximizer of the reduced likelihood	gaussian_process gaussian process arg max reduced likelihood	0.250000
outcomes	voting	0.066667
an 'l' suffix when using	utils shape	0.013699
this dataset is described	datasets make	0.031250
dimension of a dataset	dimension	0.050000
scale back the data	scaler inverse transform x	0.052632
private function used to	ensemble parallel decision function estimators	0.333333
the laplacian kernel between	metrics laplacian kernel	0.166667
in the	compress	0.100000
a cv	cv cv	0.031250
predicted target values for x relative to y_true	metrics predict scorer call estimator x y_true sample_weight	0.200000
descriptors of a memmap instance to reopen on	joblib reduce memmap a	0.050000
different probability thresholds note this implementation	probas_pred pos_label sample_weight	0.066667
ledoit-wolf	ledoit wolf x assume_centered	0.250000
the cf node	cluster birch get	0.333333
generate an	datasets make	0.031250
returns posterior probabilities of classification	classifier cv predict proba x	0.200000
the array corresponding to this	joblib numpy array	0.250000
the posterior log probability of the samples	core multinomial nb joint log likelihood	0.083333
fits the oracle approximating shrinkage covariance	covariance oas	0.083333
stop	stop	1.000000
the array from the meta-information and	externals joblib zndarray wrapper read unpickler	0.043478
implement a single boost	base weight boosting boost iboost x	1.000000
we don't store the timestamp when pickling	externals joblib memorized func reduce	0.050000
workers requested by the	backend base	0.032258
dir	dir	0.192308
called with the given arguments	externals joblib memorized func get output	0.125000
a binary metric for multilabel classification parameters	metrics average binary score binary_metric y_true y_score	0.500000
a cv in a	cv cv x y	0.031250
to sparse format	sparse coef	0.071429
the voting classifier	ensemble voting classifier set params	0.037037
data x	x	0.003384
of	shape	0.011765
as a	externals joblib	0.004762
model parameters	fit x y sample_weight	0.020000
determine	externals joblib parallel backend	0.029412
signature from the given list	signature	0.047619
normalize x according to kluger's log-interactions	cluster log normalize x	0.200000
output a function	externals joblib function called str function_name	0.250000
solve the isotonic regression model : min	isotonic regression	0.055556
is the	y_true y_pred	0.037037
predict multi-class targets using underlying	output code classifier predict x	0.250000
x and returns the	transform x y w	0.500000
set the diagonal of the laplacian matrix	set diag laplacian	0.333333
bound	dpgmmbase bound	0.333333
rows of a csr matrix in-place	utils inplace swap row csr	0.250000
targets	targets	0.833333
the right fileobject from a	read fileobject fileobj	0.100000
onto the	transform x ridge_alpha	0.071429
y = (gamma <x y> + coef0)^degree	y degree gamma	1.000000
lfw pairs dataset this	datasets fetch lfw pairs	0.037736
search over	base search cv fit x	0.166667
wild lfw pairs dataset this dataset	datasets fetch lfw pairs subset	0.035714
update h in multiplicative update nmf	multiplicative update h x w h beta_loss	0.250000
folders to	externals joblib memory	0.016949
with the best found parameters	model_selection base search cv predict proba x	0.076923
routine for validation and conversion of csgraph inputs	graph csgraph directed dtype csr_output	0.166667
the given arguments and persist the output values	func call	0.047619
load and return the boston house-prices	load boston return_x_y	1.000000
is restricted to the binary classification task	curve y_true	0.125000
projection of the data onto the	x ridge_alpha	0.071429
utility for building a cv in	cv cv x y	0.031250
for detecting outliers in a gaussian distributed dataset	elliptic envelope	0.166667
regression problem with sparse uncorrelated design	make sparse uncorrelated n_samples n_features random_state	0.166667
the process or thread	externals joblib multiprocessing	0.052632
estimator	core	0.030769
matrix	linear_model	0.025641
the search over parameters	search cv fit	0.111111
return the kernel k x y and	matern call x y	0.200000
with the generative model	get	0.012048
and transform with the	transform	0.011236
build a	sample_weight check_input	0.500000
of estimators within a job	estimators n_estimators ensemble x y	0.083333
list of exception types	get	0.012048
the bound	bound	0.083333
the model to the data x	x y	0.002155
set of samples x	x y	0.002155
svmlight / libsvm format into	svmlight file f n_features dtype	0.066667
a helper class for managing pool	pool manager mixin	0.500000
set the	linear_model linear model set	0.500000
collection of raw documents	vectorizer	0.022222
with the generative	pca	0.047619
scale back the	inverse transform	0.062500
the position of the	mds fit x y init	0.066667
the long type	utils shape	0.013699
compute the precision the precision is	precision	0.016667
specified row	row row	0.166667
format	linear_model sparse coef	0.076923
to the data	y	0.002674
input data	cross	0.037037
covariance	cov x y priors shrinkage	0.500000
a tolerance which is independent of the	cluster tolerance	0.058824
reduced likelihood function for the given autocorrelation	reduced likelihood function	0.041667
position of the points in	mds	0.050000
returns the number of splitting iterations in	pgroups out get n splits x	0.111111
one set of	point x y	0.500000
the paired	paired	0.200000
generate cross-validated	val predict estimator x y cv	0.071429
distances between the vectors	distances	0.100000
generate a grid of points	grid	0.040000
python object into one file	joblib dump value filename	0.083333
of a memmap instance	externals joblib reduce memmap a	0.050000
computes multidimensional scaling using the smacof algorithm	smacof dissimilarities metric n_components init	1.000000
absolute sizes of training subsets and	train sizes	0.066667
to	joblib memory	0.033898
prediction scores this score corresponds	score y_true y_score	0.025000
a callable that handles preprocessing and tokenization	build analyzer	0.333333
determine the number of jobs	n jobs n_jobs	0.142857
on x for	fit x	0.006410
names	name	0.033333
user provided precisions	mixture check precisions precisions	0.250000
em algorithm	gmmbase	0.187500
boolean masks	masks	0.125000
number of splitting iterations in	cviterable wrapper get n splits x	0.111111
pickler	pickler	0.583333
is the time	time	0.047619
row scaling of a csr or csc matrix	row	0.066667
return feature names for	get feature names	0.090909
submatrix corresponding	get submatrix	0.166667
the score on the given data if the	score	0.010101
compute the log probability under the model	mixture gmmbase score x y	0.500000
cross-validated estimates	cross val predict estimator x y cv	0.071429
call transform on the estimator with the	transform x	0.016949
max absolute value	max abs	0.047619
makes sure that whenever	copy	0.062500
to split data into	split split	0.250000
the median of data with n_zeros additional zeros	get median data n_zeros	0.500000
does not need	y residual	0.500000
to build a batch of estimators within a	parallel build estimators	0.166667
binomial	binomial	1.000000
coverage error measure compute how far we need	metrics coverage error	0.166667
error regression loss read more in	error	0.060000
classification by definition a confusion matrix :math c	confusion matrix y_true y_pred labels sample_weight	1.000000
rate this uses the benjamini-hochberg procedure	fdr	0.142857
compute joint probabilities	joint probabilities	1.000000
private function used to fit an estimator within	fit estimator estimator	0.055556
get number of jobs for	utils get n jobs n_jobs	0.250000
fit the model to data	output estimator fit x y	0.200000
data home	clear data home	0.076923
and last element of numpy array or	and last element arr	0.166667
binarize labels in	preprocessing label binarize	0.333333
returns the number of splitting iterations in	base cross validator get n splits	0.125000
of observations in x according to	x	0.001692
approximates the range of a	range finder a	0.166667
scale back the	preprocessing standard scaler inverse transform x	0.066667
memmap instance to reopen on same file	externals joblib reduce memmap	0.142857
function returns posterior probabilities	cv predict proba x	0.034483
svmlight format this function is	svmlight	0.050000
a set of points	axis metric	0.250000
conversion	csr_output	0.111111
validate user provided precisions	mixture check precisions precisions covariance_type n_components	0.250000
the dual gap convergence criterion the	covariance dual gap emp_cov	0.071429
labeled faces in the wild lfw pairs dataset	datasets fetch lfw pairs	0.018868
wikipedia page	utils step1 state	0.142857
check to make sure	neighbors check	0.333333
patches that will be extracted in an image	patches i_h i_w p_h p_w	0.250000
of x from y along the first	x	0.001692
based on a feature matrix	connectivity n_clusters return_distance	0.250000
updates terminal regions	terminal	0.047619
corresponds to the area under the	y_true y_score	0.027027
the value in data	data	0.038462
dirichlet process gaussian mixture models	dpgmm	1.000000
type introduces an 'l' suffix	utils	0.009709
random	n_features	0.083333
update h in multiplicative update	multiplicative update h x w h	0.250000
covariance matrix shrunk	covariance shrunk covariance	0.090909
objective for the case method='lasso' is :	xy gram	0.090909
identify uniquely python objects	obj hash_name coerce_mmap	0.200000
the free energy f	bernoulli rbm free energy	0.066667
versus all others	multiclass x y alpha c	0.166667
cache key	cache key	0.250000
percentiles of	percentiles grid_resolution	0.250000
linear embedding analysis on the	linear embedding	0.083333
the voting classifier valid	voting classifier	0.035714
an 'l' suffix when	shape repr	0.013699
isotonic regression model : min sum w[i] (y[i]	core isotonic regression	0.055556
getter	empirical	0.055556
with categories	categories load_content	0.500000
checker utility for building a cv in a	check cv cv x y classifier	0.031250
the coefficient of determination r^2	multi output regressor score x	0.200000
estimates the shrunk ledoit-wolf	ledoit wolf shrinkage x	0.250000
compound	compound	0.833333
the training set	factor fit predict	0.066667
sign of	sign flip	0.066667
the dual gap convergence criterion the specific definition	dual gap emp_cov	0.071429
a which this function is called	externals joblib memorized func check	0.125000
matrix factorization nmf find two non-negative	factorization x	0.043478
at	at	1.000000
rand index adjusted	adjusted rand score labels_true	0.333333
and scale the	x y	0.002155
for building a cv in a user	cv cv x	0.031250
a score by cross-validation read more in	cross val score estimator x	0.166667
dual gap convergence criterion the	dual gap emp_cov precision_ alpha	0.071429
each sample	samples x	0.428571
apply the derivative	derivative	0.125000
of the samples x	svc decision function x	0.200000
the average path length of	average path length	0.090909
compute the laplacian	metrics laplacian	0.333333
on the estimator with the best found parameters	search cv predict proba x	0.076923
predefined	predefined	1.000000
pred	pred	0.857143
predict class at each stage	gradient boosting classifier staged predict	0.500000
leave-p-out cross validation iterator	leave pout	1.000000
of regularization	pos_class cs	0.166667
squares projection of the data onto the	transform x ridge_alpha	0.071429
estimate class weights for unbalanced datasets	utils compute class weight class_weight classes y	0.500000
collision between names	job lib collision warning	0.333333
x for later	fit x	0.006410
fit to data then transform	transformer mixin fit transform x y	0.500000
flattened log-transformed non-fixed hyperparameters	gaussian_process exponentiation theta theta	0.333333
each class	classifier	0.013699
returns the number of splitting iterations in the	split get n splits	0.111111
generate isotropic gaussian blobs for	datasets make blobs n_samples n_features centers cluster_std	0.333333
density model on the data	neighbors kernel density	0.090909
list of exception types to be captured	parallel backend base get exceptions	0.166667
apply dimensionality reduction to x using the	decomposition factor analysis transform x	0.250000
boosted regressor from the	ensemble ada boost regressor	0.333333
compute the gradient of	neural_network base multilayer perceptron compute	0.250000
evaluate predicted target values for x	x	0.001692
introduces an 'l'	utils	0.009709
in the svmlight / libsvm format into	svmlight file f n_features dtype	0.066667
the file supports seeking	binary zlib file seekable	0.250000
function used to compute log probabilities within a	ensemble parallel predict log proba	0.058824
predict_log_proba	predict log proba x	0.045455
curve auc using	auc x	0.040000
from data in	manifold isomap	0.333333
and scale the data	x y	0.002155
non-negative matrix factorization nmf find two non-negative	factorization	0.035714
score by cross-validation read more in the :ref	cross val score estimator x	0.166667
class covariance	core class cov x	0.250000
k x y and	matern call x y	0.200000
msg_args	msg_args	1.000000
lfw pairs	fetch lfw pairs	0.037736
pipeline after transforms	pipeline fit predict x	0.166667
return the feature importances	tree base decision tree feature importances	0.333333
of the dual gap convergence criterion the specific	covariance dual gap emp_cov	0.071429
download the 20 newsgroups	datasets download 20newsgroups	0.200000
given arguments	memorized func get output	0.125000
the model according to the	sample_weight	0.037037
returns the huber	linear_model huber	0.333333
scale back the data to the original representation	scaler inverse transform x	0.052632
class weights for unbalanced datasets	compute class weight class_weight classes y	0.500000
generate indices to split data into	model_selection base kfold split	0.250000
return	utils	0.009709
regression model we	regressor	0.027027
computes the maximum likelihood covariance estimator	covariance empirical covariance x assume_centered	0.166667
the california housing	fetch california housing	0.083333
sparse combination of the dictionary atoms	decomposition sparse coding mixin transform	0.333333
such that for c in (l1_min_c infinity)	c	0.022222
for building a cv in	core check cv cv x y	0.031250
update the bound	mixture bound state log lik	0.500000
laplacian	metrics laplacian	0.333333
returns the number of splitting iterations in	model_selection base cross validator get n splits x	0.125000
decision function of the given observations	covariance outlier detection mixin decision function x raw_values	0.333333
the blup parameters and evaluates the reduced likelihood	gaussian_process gaussian process reduced likelihood	0.142857
em update	latent dirichlet allocation em step x	0.500000
the data onto	transform x ridge_alpha	0.071429
estimator with the best found parameters	search cv predict	0.074074
the number of splitting iterations in the cross-validator	model_selection cviterable wrapper get n splits x	0.111111
kernel k	gaussian_process	0.031250
warnings without visual nesting	warnings call fn	0.200000
selected	feature_selection selector	0.142857
inefficient to train all data	classes	0.025641
data	estimator	0.014706
the long type	shape	0.011765
regression model we can also predict based on	regressor predict	0.200000
the reduced likelihood	process reduced likelihood	0.142857
x relative to y_true	metrics threshold scorer call clf x	0.058824
and return encoded labels	y	0.002674
estimates for	cross val predict estimator x	0.045455
descent fit is on	fit x	0.006410
estimator on training subsets incrementally and compute scores	model_selection incremental fit estimator estimator x y classes	0.200000
free energy f v = -	rbm free energy	0.066667
the number of splitting iterations in	split get n splits x y	0.111111
problem with sparse uncorrelated design	make sparse uncorrelated n_samples	0.166667
we don't	externals joblib memory	0.016949
exception	joblib parallel backend base	0.058824
computes the paired cosine	metrics paired cosine	0.333333
estimate model	fit x y do_prediction	0.166667
or thread pool	externals joblib multiprocessing	0.052632
thread pool	multiprocessing backend	0.076923
x y and optionally its gradient	squared call x y eval_gradient	1.000000
iterator	iterator	0.833333
the content of the data home	clear data home	0.076923
single tree	build trees tree forest x y	0.142857
store the timestamp when pickling to avoid the	reduce	0.034483
check initial parameters	mixture base mixture check parameters	0.200000
x and returns	predict x y	0.043478
of csgraph	utils sparsetools validate graph csgraph	0.250000
n_features	n_features	0.416667
dimension	dimension	0.300000
fit estimator using ransac algorithm	linear_model ransacregressor fit x y sample_weight	1.000000
predict class	classifier predict	0.600000
detects the soft boundary of	one class svm fit	0.125000
the linear assignment problem using the hungarian algorithm	linear assignment	0.090909
parallel backend factory	parallel backend name factory	1.000000
shrunk on the	covariance shrunk	0.066667
computes the free energy f v	free energy	0.066667
graph of neighbors	neighbors radius neighbors mixin radius neighbors graph	0.066667
value of the log of	log	0.018868
to a given type in the dispatch table	externals joblib customizable pickler register type reduce_func	1.000000
log of	linear_model logistic regression predict log	0.500000
locally linear embedding analysis on the data	manifold locally linear embedding	0.062500
compute the decision function	ensemble gradient boosting classifier decision function	0.166667
nonzero componentwise l1 cross-distances between	gaussian_process l1 cross distances	0.111111
extracts patches of any	patches	0.055556
backend	backend	0.152542
features parameters	features	0.100000
to x and perform dimensionality reduction on x	transform x y	0.031250
leaf	decision tree apply	0.166667
defines all methods a parallelbackend must implement	parallel backend	0.030303
in	joblib memory	0.016949
building a cv	check cv cv x y	0.031250
estimator	estimator x y classes	1.000000
collect	ensemble voting classifier collect	1.000000
implement randomized linear models	randomized linear model	0.076923
get parameters of	exponentiation get params	0.500000
be used for scaling	scaler fit	0.076923
introduces an 'l' suffix when using the	utils shape repr	0.013699
the recall	recall	0.028571
shutdown the process or thread pool	joblib parallel backend base terminate	0.500000
fit the calibrated model	calibrated classifier cv fit	1.000000
given type in	register type	0.333333
layer	layer	0.454545
building a cv in a user friendly	cv cv	0.031250
k_skip	k_skip	1.000000
scores note this implementation is	roc	0.033333
returns	gaussian_process normalized	1.000000
all estimators from sklearn	all estimators include_meta_estimators include_other type_filter include_dont_test	1.000000
sizes of training subsets	sizes	0.050000
search over parameters	base search	0.100000
hessian in the case of a multinomial loss	multinomial grad hess	1.000000
is inefficient to train all	classes	0.025641
dual gap convergence criterion the specific	covariance dual gap emp_cov precision_ alpha	0.071429
scikit-learn notes	estimator	0.014706
kernel density	neighbors kernel density	0.090909
true and predicted probabilities for a calibration curve	core calibration curve	0.142857
with coordinate descent the elastic net optimization function	l1_ratio	0.030303
fit a single binary estimator one-vs-one	fit ovo binary estimator x	1.000000
std to be used for	preprocessing standard	0.250000
fit a multi-class classifier	fit	0.003257
indices increasingly apart the distance depending on the	verbosity filter index	0.055556
don't store the	externals joblib memory	0.016949
should contain a partial	partial	0.043478
number of jobs which are going to run	externals joblib multiprocessing backend effective n jobs n_jobs	0.333333
return a platform independent representation	utils shape repr	0.013699
connectivity	x connectivity	1.000000
dense array	linear_model sparse coef mixin densify	0.100000
probabilities for a calibration curve	core calibration curve y_true y_prob normalize	0.142857
predict class at each stage for x	ensemble gradient boosting classifier staged predict x	1.000000
random	random	0.647059
the shrunk ledoit-wolf covariance matrix	covariance ledoit wolf x assume_centered	0.125000
getter	empirical covariance	0.125000
isolation forest algorithm return	isolation forest	0.200000
ridge classifier	linear_model ridge classifier cv	1.000000
the kernel k	white kernel	0.250000
in the svmlight / libsvm format into sparse	svmlight file f n_features dtype multilabel	0.066667
ledoit-wolf	ledoit wolf x	0.250000
median and component wise scale	robust scale x	0.125000
x from y along the first	x	0.001692
least squares solver	linear discriminant analysis solve lsqr x	1.000000
the laplacian matrix and convert it	laplacian	0.034483
apply clustering to a	spectral clustering affinity n_clusters	0.166667
compute the	neural_network base	1.000000
constructor store the	externals joblib zndarray wrapper init filename init_args state	0.200000
perform	predict	0.006849
that will be extracted in an image	i_h i_w p_h p_w	0.250000
hash depending from it	memorized	0.015873
generate an array with constant block diagonal structure	datasets make biclusters shape n_clusters noise minval	1.000000
deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call y pred	0.333333
well suited for eigenvalue decomposition	value norm_laplacian	0.142857
input data	core cross val predict estimator	0.045455
a cv in	core check cv cv x	0.031250
as	neighbors unsupervised mixin	1.000000
n_samples itree which is equal to the average	ensemble average	0.125000
transform feature->value dicts to array	feature_extraction dict vectorizer transform	0.200000
discrete target variable	x y discrete_features n_neighbors	0.500000
and returns	y	0.002674
samples can be different from the	calibrated classifier	0.083333
factorization	non negative factorization	0.043478
load and return the iris	load iris	1.000000
k-neighbors of	neighbors kneighbors mixin kneighbors x n_neighbors	0.125000
prediction scores this score corresponds to	score y_true y_score	0.025000
log probabilities for each sample	samples x	0.142857
rand index adjusted for	metrics cluster adjusted rand score	0.333333
of exception types to	parallel	0.019231
fitted model	fit x	0.006410
the deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call y pred sample_weight	0.333333
svmlight / libsvm file format	svmlight file	1.000000
exception	externals	0.005747
binarize labels	label binarize	0.333333
operation is meant to	data_folder_path slice_ color resize	0.033333
and quantiles to be used for scaling	preprocessing robust scaler fit x y	0.500000
break the pairwise matrix in n_jobs	parallel pairwise x y func n_jobs	0.111111
for full covariance	full x	0.166667
l1 and l2 regularization coefficients for	regularization alpha l1_ratio regularization	0.250000
for binary classification used in hastie et al	hastie 10 2 n_samples random_state	0.166667
for each input data	x y	0.002155
number of splitting iterations in the cross-validator parameters	group out get n splits x	0.111111
to the average path length of	average path length	0.090909
a logistic regression	logistic regression path	0.333333
mean absolute error regression	metrics mean absolute error	0.166667
change the default backend used by parallel	parallel backend backend n_jobs	0.166667
replace=true p=none) generates a random sample from a	a size replace	0.142857
evaluate decision function output for x relative	metrics threshold scorer call clf x y	0.058824
image samples in x into a	x	0.001692
autocorrelation parameters theta as the maximizer	arg max	0.047619
decorate function	utils deprecated decorate	1.000000
input data	cross val predict estimator x y	0.045455
compute data precision matrix with	pca get precision	0.066667
implement randomized linear models for feature	randomized linear model	0.076923
cartesian	cartesian	1.000000
of array-like or scipy sparse matrix	preprocessing binarize x	0.083333
blup parameters and evaluates the reduced likelihood function	process reduced likelihood function	0.047619
input validation for	y x y accept_sparse dtype	0.250000
and raise valueerror if not	base gradient boosting	0.100000
of feature names ordered by their indices	get feature names	0.090909
the best	cv predict proba	0.068966
posterior probability of data	proba	0.029412
indices to split data into	model_selection base kfold split	0.250000
get number of jobs for the	utils get n jobs n_jobs	0.250000
the parameters for the voting classifier	ensemble voting classifier set params	0.037037
locally linear embedding analysis on	manifold locally linear embedding x n_neighbors	0.071429
the density model on the	neighbors kernel density	0.090909
for factorizing common classes param logic estimators	fit first call clf classes	0.058824
fit the model to	estimator fit x y	0.200000
load	datasets load	0.500000
function to	function	0.042553
parameters for the voting classifier	voting classifier set	0.037037
check the validity of	check params x metric p metric_params	0.200000
and configure	append random_state	0.142857
hierarchical	feature agglomeration	0.333333
inverse covariance w/ cross-validated choice	cv	0.009009
pseudo-inverse of a hermetian matrix	utils pinvh a cond rcond lower	0.200000
transform feature->value dicts to array	feature_extraction dict vectorizer transform x y	0.200000
mixin for k-neighbors searches	kneighbors mixin	1.000000
fit label encoder and	preprocessing label encoder fit transform y	0.200000
to x using the model	transform x	0.016949
shrunk ledoit-wolf covariance matrix	covariance ledoit wolf shrinkage x assume_centered block_size	0.125000
fit the model according to	svm linear svc fit x y sample_weight	0.250000
parameters and evaluates the reduced	process reduced	0.125000
probabilistic	probabilistic	1.000000
clear all covered	state clear	1.000000
process regression model we can also predict based	process regressor predict x	0.500000
order	order	1.000000
number of splitting iterations in the	model_selection predefined split get n splits x	0.111111
models computed by 'path' parameters	linear_model path residuals x	0.250000
the number of splitting iterations in the	model_selection cviterable wrapper get n splits	0.111111
x according to feature_range	transform x	0.016949
single boost	classifier boost	0.100000
givens rotation	utils sym ortho a b	0.250000
the number of splitting iterations in	kfold get n splits x	0.111111
matrix given column class distributions parameters	classes class_probability	0.166667
on the estimator with the best found	base search cv predict proba	0.076923
process of the parallel execution only a fraction	externals joblib parallel	0.014085
a single binary estimator one-vs-one	ovo binary estimator	0.500000
train estimator on training subsets incrementally and compute	model_selection incremental fit estimator estimator x y classes	0.200000
should use this function first to assert that	check	0.017857
and component wise scale to unit	preprocessing scale	0.090909
single binary estimator	core predict binary estimator	0.200000
the free energy f	neural_network bernoulli rbm free energy	0.066667
fit the model according to the	fit x y sample_weight	0.040000
parameters and evaluates the reduced	gaussian process reduced	0.125000
confidence scores for samples	linear_model linear classifier mixin decision function x	0.500000
computes	metric	0.071429
multi-class targets using underlying estimators	output code	0.200000
build a batch of estimators within a job	parallel build estimators n_estimators ensemble	0.166667
reconstruct the	ndarray wrapper read unpickler	0.333333
binary labels the output of transform is	transform y	0.023256
for	val predict	0.045455
weighted graph of neighbors for points in x	neighbors graph x	0.500000
learn vocabulary and idf return term-document	vectorizer fit transform raw_documents y	0.100000
values for a	y train	0.166667
argument checking for random matrix generation	input size n_components n_features	0.200000
using the	utils shape repr	0.013699
base class for sgd classification and regression	base sgd	1.000000
iterate	iterate columns x	1.000000
evaluate the density model on the	kernel density score samples	0.250000
y as training data	y copy_x	0.333333
the cache for the	externals joblib memorized	0.013699
convert	linear_model sparse coef	0.076923
as a	externals	0.005747
lindenstrauss	lindenstrauss	1.000000
return the kernel k x	x	0.006768
the sparse components	sparse pca transform x	0.500000
in friedman [1] and breiman [2]	make friedman3 n_samples noise random_state	0.166667
classification dataset is constructed by taking	datasets make	0.015625
estimates the shrunk ledoit-wolf	ledoit wolf shrinkage	0.250000
est weight tuples excluding none	iter	0.050000
models for feature selection this implements the strategy	model	0.058824
returns the number of splitting iterations in	model_selection base kfold get n splits x	0.111111
the per-sample	mixture base mixture	0.111111
fit a single tree	trees tree forest x	0.142857
matrix	linear_model sparse coef	0.076923
number of splitting iterations in	cviterable wrapper get n splits	0.111111
estimates for each input	cross val predict estimator x	0.045455
update h	h x w h beta_loss	0.500000
moved objects in six moves urllib_parse	module six moves urllib parse	0.333333
an open file object	f header_length dtype	0.333333
the decision function of x	gradient boosting classifier decision function x	0.333333
handle the callable case for	callable x y metric	0.083333
needs_proba	needs_proba	1.000000
to preprocess the text before tokenization	feature_extraction vectorizer mixin build preprocessor	0.142857
predict labels for data	predict x	0.011765
call transform on the estimator	transform	0.011236
if y is in a multilabel format	utils is multilabel y	0.333333
compute the gradient of loss	neural_network base multilayer perceptron compute loss	1.000000
names	core name	0.250000
creates	base spectral fit	0.250000
predict class log-probabilities	ensemble gradient boosting classifier predict log proba	0.500000
compute the unnormalized posterior log probability of	nb joint log likelihood	0.033333
input data point	predict estimator x y	0.045455
in the wild lfw pairs dataset this	fetch lfw pairs	0.018868
argument_hash	argument_hash	1.000000
mean_shift	x bin_size min_bin_freq	0.500000
helper function for factorizing common classes param	first call clf classes	0.058824
the number of splitting iterations in the cross-validator	base kfold get n splits x y	0.111111
the em algorithm and return the cluster	mixture gmmbase	0.034483
step length is not found	search	0.019231
log probability for full	log multivariate normal density full x	0.333333
for building a cv in a user friendly	check cv cv	0.031250
to sparse	linear_model sparse	0.076923
graph of k-neighbors for points in x	kneighbors mixin kneighbors graph x n_neighbors mode	0.333333
lfw pairs dataset this dataset is a collection	fetch lfw pairs	0.018868
pairwise matrix	parallel pairwise x	0.166667
input checker utility for building a cv in	core check cv cv x y classifier	0.031250
compute mean and variance along an	utils mean variance axis x axis	0.142857
autocorrelation parameters theta as the maximizer of	process arg max	0.047619
partially fit underlying estimators should	core one vs one classifier partial fit x	0.166667
the diagonal of	diag	0.031250
is the solution to a sparse coding problem	decomposition sparse encode x dictionary gram cov	0.333333
number of splitting iterations in the cross-validator	split get n splits x	0.111111
compute the precision the precision	precision	0.016667
predict class log-probabilities for x	ensemble forest classifier predict log proba x	1.000000
best	cv	0.045045
avoid the hash	externals joblib memorized func	0.013158
predict using the gaussian process regression model we	gaussian process regressor	0.055556
the l1 distances between the vectors in x	manhattan distances x	0.500000
false positives per binary	binary clf curve y_true	0.090909
lad updates terminal	ensemble least absolute error update terminal region	0.200000
directory in	output dir	0.047619
catch and hide warnings without visual nesting	ignore warnings call fn	0.200000
center	cross_decomposition center scale	1.000000
compute the mlp loss function and	neural_network base multilayer perceptron	0.083333
windows this is the time it take	squeeze time t	0.166667
parameter_iterable	parameter_iterable	0.714286
of neighbors for points in	neighbors radius neighbors mixin radius neighbors	0.125000
the estimator with the best found	core base search cv predict proba x	0.076923
file from an open file object	f header_length dtype	0.333333
generate primal coefficients from dual coefficients for the	coef dual_coef n_support support_vectors	0.333333
fit linear model with stochastic gradient descent	linear_model base sgdregressor partial fit x y	1.000000
two sets of biclusters	cluster consensus score	0.250000
logistic loss and	logistic loss and	0.500000
used to compute log probabilities	ensemble parallel predict log proba	0.058824
perform classification on	nearest centroid predict	0.142857
fit the model according to the given	svm linear svr fit x y sample_weight	0.250000
of two clusterings of	score labels_true labels_pred	0.047619
distribution	mixture bayesian	1.000000
based on a feature	x connectivity n_clusters	0.250000
an affinity matrix for x using	x y	0.002155
each	core cross val	0.043478
covariance determinant mcd : robust estimator of	cov det	0.200000
the number of splitting iterations in the	base cross validator get n splits x	0.125000
checker utility for building a cv in a	check cv cv	0.031250
computation of max	preprocessing max	0.166667
perform a locally linear embedding analysis on the	locally linear embedding	0.050000
private function used to fit a single	trees	0.083333
are selected	feature_selection	0.066667
similarity of two clusterings of a set of	score labels_true labels_pred sparse	0.047619
matrices from	covariance_type	0.083333
returns n_neighbors of	x n_neighbors return_distance	0.250000
a	externals joblib pool manager mixin apply async	1.000000
log	logistic regression predict log	0.500000
point	cross val	0.038462
is not found and	search	0.019231
w	x w ht l1_reg	0.250000
minimum covariance determinant mcd : robust estimator	min cov det	0.500000
non-negative matrix factorization nmf find two	factorization x	0.043478
regression problem this dataset is described	datasets	0.015152
the lfw people dataset this operation is	datasets fetch lfw people	0.040000
setting the parameters for the voting	ensemble voting	0.142857
note this implementation is restricted	metrics roc	0.040000
a cv in a user friendly way	core check cv cv x	0.031250
loader for the california housing dataset from	fetch california housing data_home	0.250000
array-like or scipy	preprocessing binarize x	0.083333
generate a random regression	datasets make regression n_samples n_features	1.000000
fit a binary classifier	base sgdclassifier fit binary	0.333333
split data into training and test	model_selection predefined split split x y groups	0.200000
be captured	exceptions	0.083333
factorizing common	partial fit first call clf	0.200000
theta as the maximizer of the reduced	arg max reduced	0.200000
model to x	fit x	0.006410
beta_loss	beta_loss	0.833333
store the timestamp when pickling	joblib memorized func reduce	0.050000
based on x and	x	0.001692
model from data	manifold spectral embedding	0.111111
don't store the	func	0.011364
of np dot x y	dot x y	0.250000
function call with the given arguments	format call func	0.100000
similarity of	a b similarity	0.125000
of last step in pipeline after transforms	pipeline fit predict x	0.166667
the time it take	squeeze time	0.166667
laplacian kernel between	laplacian kernel	0.166667
the lfw people	datasets fetch lfw people	0.040000
values for x relative to y_true	predict scorer call estimator x y_true sample_weight	0.200000
truncated	m n_components n_oversamples n_iter	1.000000
hash depending	memory	0.015625
compute the laplacian	laplacian	0.034483
indices to split data into	shuffle split split	0.250000
kernel k	pairwise kernel	0.250000
memmap instance to reopen	joblib reduce memmap	0.166667
as training	orthogonal matching pursuit cv	0.200000
corresponding to	joblib numpy	0.250000
found	utils line search	0.029412
ledoit-wolf	ledoit wolf shrinkage x assume_centered	0.250000
cholesky decomposition of	log det cholesky	0.166667
single boost using the	boost classifier boost	0.100000
building a cv	cv cv	0.031250
fit the gradient boosting model	base gradient boosting fit x y sample_weight monitor	1.000000
for each input data	core cross val predict	0.045455
predicting	prior probability	1.000000
conditional property using the	iff has attr descriptor	0.083333
that for c in	c	0.022222
compute the weighted log probabilities for	mixture base mixture score	0.111111
compute mutual information between two variables	compute mi	1.000000
minimum and maximum	min max axis	0.500000
empirical	empirical	0.277778
function output for x relative to y_true	metrics threshold scorer call clf x y	0.058824
the scaler	preprocessing max abs scaler	0.333333
one set of	point x	0.500000
compute mutual information between continuous and discrete variables	compute mi cd c d n_neighbors	1.000000
the raw documents	feature_extraction count vectorizer fit	0.125000
implement the usual api and	y	0.008021
process or	externals joblib multiprocessing	0.052632
pursuit model omp	pursuit	0.181818
estimator's fit method supports the	utils has fit	0.500000
compute the median and	y	0.002674
an 'l' suffix	shape repr	0.013699
the sign of elements of all the vectors	sign	0.050000
axis center to the median and component	axis	0.014085
a transform function to portion of selected features	selected x transform selected	0.333333
compute_labels	name clusterer	0.250000
check that the parameters are well defined	gaussian mixture check parameters	1.000000
step1	step1	1.000000
memmap backed arrays	memmap backed	0.333333
loss function for	loss function	0.666667
perform dbscan clustering from vector	cluster dbscan x	0.200000
classifier valid parameter keys can be	classifier	0.013699
calibration curve	core calibration curve y_true y_prob normalize	0.142857
for the california housing dataset from statlib	datasets fetch california housing	0.083333
is restricted to the binary	y_score average sample_weight	0.142857
test vectors x	core dummy regressor predict x	0.250000
class for	ensemble forest classifier	0.333333
with sparse uncorrelated design this dataset is described	datasets make sparse uncorrelated	0.166667
to split data into training and test	kfold split x y groups	0.200000
parallel n_jobs	n_jobs	0.023256
for specified layer	grad layer	0.166667
to cleanup a temporary folder if still existing	folder folder_path	0.250000
precision matrix with the generative model	decomposition base pca get precision	0.066667
described in friedman [1] and breiman [2]	friedman3	0.090909
of x for	x y	0.002155
implementation is restricted to the binary	y_score	0.083333
online	partial	0.130435
on the estimator with the best found parameters	core base search cv predict	0.076923
inverse	agglomeration transform inverse	0.500000
depending	joblib memorized func	0.014706
update h	w h beta_loss	0.500000
the shrunk covariance model according to	covariance shrunk covariance fit	0.083333
number of splitting iterations in the	cviterable wrapper get n splits x y	0.111111
a cross-validated	cv	0.009009
estimate the spherical wishart distribution parameters	bayesian gaussian mixture estimate wishart spherical	0.333333
not found and raise an exception if	line search	0.029412
is	joblib is	0.500000
cache key	externals joblib cache key	0.250000
clustering for the subclusters	clustering	0.050000
persist an arbitrary python object	filename compress protocol	0.250000
the best found parameters	core base search cv predict	0.076923
decision tree	tree decision tree	0.250000
residual (= negative gradient)	ensemble binomial deviance negative gradient	0.333333
a hash	hash	0.083333
the kernel	stationary kernel mixin	0.333333
free energy f v =	rbm free energy	0.066667
hash depending	externals joblib memory	0.016949
fit a single binary estimator	fit binary estimator x	1.000000
and dense	y sample_weight random_state	0.166667
lrd of a sample	distances_x neighbors_indices	0.047619
message on initialization	msg init beg n_init	1.000000
to check the test_size and train_size at init	model_selection validate shuffle split init test_size train_size	0.250000
dtype float32 is returned	metrics return float dtype	0.250000
probability estimates	proba x	0.111111
estimators within a	estimators n_estimators ensemble	0.083333
elastic net path with coordinate descent the	path x	0.045455
r^2 coefficient of determination regression score function	r2 score y_true y_pred sample_weight multioutput	0.125000
constructs signature from the given list of	signature	0.047619
computes the paired cosine distances between	paired cosine distances	0.333333
build a batch of estimators within	build estimators n_estimators ensemble x y	0.166667
don't store	externals joblib memory	0.016949
file	file x y f zero_based	1.000000
cached	cached	1.000000
check x format check	latent dirichlet allocation check	0.062500
of jpeg pictures of famous people	funneled resize	0.142857
terminal	terminal region tree terminal_regions leaf	0.066667
a cv in a user	cv cv x y	0.031250
warning used to notify	warning	0.166667
fit the	svc fit x y	0.333333
an	utils shape repr	0.013699
of exception	joblib parallel	0.028571
wild lfw pairs	datasets fetch lfw pairs subset	0.035714
meta-information	externals joblib zndarray wrapper read unpickler	0.043478
p=none) generates a random sample from	size	0.032258
predict	tree predict	0.500000
building a cv in a user friendly	check cv cv x y classifier	0.031250
the process or thread pool	externals joblib	0.004762
or too common features	feature_extraction count vectorizer limit features x vocabulary	0.250000
back	preprocessing standard scaler inverse transform x	0.066667
updates terminal regions to	terminal region tree	0.100000
called with the given arguments	func get output	0.125000
implement randomized linear models for feature selection	randomized linear model	0.076923
transform is sometimes referred to by some authors	preprocessing label binarizer transform y	1.000000
average path	average path	0.142857
the neighbors within a given radius of a	lshforest radius neighbors x radius	0.142857
fit	fit rfe estimator	0.166667
be used for later scaling	scaler fit x	0.153846
the 20 newsgroups	20newsgroups	0.055556
estimates for each input data	core cross val	0.043478
evaluate the density model on the data	density score samples x	0.250000
problem with sparse uncorrelated design this dataset is	datasets make sparse uncorrelated n_samples n_features random_state	0.166667
from	manifold spectral	0.111111
generate n colors with equally spaced hues	tree color brew n	1.000000
clustering for the subclusters	clustering x	0.142857
spectral embedding	spectral embedding	0.200000
descriptors of a memmap instance	reduce memmap a	0.050000
score	score estimator	0.375000
estimator with the best found	core base search cv predict proba x	0.076923
return whether the file was opened for writing	file writable	0.250000
the directory in which are persisted the	get output dir	0.047619
compute non-negative matrix factorization nmf find two non-negative	decomposition non negative factorization	0.043478
x format check	latent dirichlet allocation check	0.062500
returns a lower bound on model evidence based	dpgmmbase lower bound	0.071429
suffix when	shape	0.011765
moved objects in six moves urllib_error	module six moves urllib error	0.333333
normalization constant	logz v s dets n_features	0.200000
validity of	x metric p metric_params	0.100000
and dense	x y sample_weight	0.012987
number of splitting iterations in	model_selection leave one out get n splits	0.111111
nk	nk	1.000000
scale back	preprocessing robust scaler inverse transform	0.066667
scale back the data to the original representation	standard scaler inverse transform	0.066667
of array-like or scipy sparse matrix	binarize x threshold copy	0.083333
cf node	cluster birch	0.090909
building a cv in	check cv cv x y	0.031250
search	search cv fit x y	0.111111
fit the model to data	core multi output estimator fit	0.200000
build a text report	report	0.047619
partially	core partial	1.000000
estimates	val predict estimator	0.045455
for building a cv	core check cv cv x y classifier	0.031250
a	externals joblib parallel backend	0.058824
model to x	x y	0.002155
the function call	joblib format call func	0.100000
given type in	pickler register type	0.333333
capture the arguments of a	check_pickle	0.040000
generative model	decomposition	0.047619
matching pursuit problems	x y n_nonzero_coefs tol	0.250000
parallel backend	parallel backend	0.030303
return the kernel k x	gaussian_process rbf call x	0.200000
of possible outcomes for samples in	ensemble voting classifier predict	0.100000
a single tree in parallel	ensemble parallel build trees tree forest x y	0.200000
the nonzero componentwise l1 cross-distances between the	gaussian_process l1 cross distances	0.111111
the cf	birch get	0.333333
estimate the tied covariance matrix	mixture estimate gaussian covariances tied resp	1.000000
such that for c in (l1_min_c infinity) the	c x y	0.030303
the wild lfw pairs dataset this dataset is	datasets fetch lfw pairs	0.018868
the position	mds fit x y	0.066667
model and	y	0.002674
gradient the gradient of the loss	loss	0.027027
x format check x	decomposition latent dirichlet allocation check	0.062500
the lfw pairs dataset this operation	fetch lfw pairs	0.018868
array with constant block diagonal structure for biclustering	biclusters shape n_clusters noise minval	0.058824
transform binary labels back to multi-class labels parameters	inverse transform y threshold	0.333333
to	selector mixin transform	1.000000
c such that for c in (l1_min_c	c	0.022222
true and predicted probabilities for a calibration	core calibration	0.125000
fit and then predict labels for data	mixture gmmbase fit predict x y	0.333333
full	full	0.388889
a given	scorer	0.136364
a classifier that makes predictions using simple rules	dummy classifier	0.500000
brew	brew	1.000000
e-step in em update	decomposition latent dirichlet allocation e step	1.000000
cross-validated estimates for each	val predict estimator x y cv	0.071429
ngrams	ngrams	1.000000
3d	3d	1.000000
compute the unnormalized posterior log probability	core base nb joint log likelihood	0.166667
feature_selection	feature_selection	0.333333
for c such that for c	c x y loss	0.030303
center	preprocessing robust scaler transform	1.000000
the relationship between labels	metrics cluster	0.142857
predict class probabilities for x	ensemble bagging classifier predict proba x	1.000000
repr	externals joblib safe repr	1.000000
optionally its gradient	eval_gradient	0.687500
list of module names and a name	func name	0.047619
with respect to each parameter weights and	x y activations deltas	0.500000
random	core dummy	0.285714
a locally linear embedding analysis on the data	manifold locally linear embedding x n_neighbors n_components reg	0.071429
shift	shift	0.600000
boost	weight boosting boost	1.000000
the first prime element in	prime in	0.166667
covariance	to match covariance	0.250000
based on x and membership	x z	0.050000
x return leaf	x	0.001692
feature names	feature names	0.090909
returns the number of splitting iterations in	group out get n splits x y groups	0.111111
given format	format spmatrix accept_sparse	1.000000
in the wild lfw pairs dataset	lfw pairs	0.018868
randomized logistic regression works by subsampling the training	randomized logistic regression	0.166667
for later scaling	scaler fit x y	0.200000
in bytes_limit	externals joblib memory reduce	0.030303
the k-neighbors of	kneighbors mixin kneighbors x n_neighbors	0.125000
meant to be cached	data_folder_path slice_ color resize	0.033333
whence	whence	1.000000
score on	score	0.010101
to a given type	externals joblib customizable pickler register type	0.083333
epsilon	epsilon	1.000000
multiple files in svmlight	svmlight files files n_features	0.200000
fit with all sets	cv fit x	0.250000
by scaling each feature to a	minmax scale	0.142857
k	compound	0.166667
get	model get	0.500000
compute the recall the recall is	metrics recall	0.033333
the l1 distances between the vectors in	paired manhattan distances	0.083333
the function called with the given arguments	externals joblib memorized func	0.013158
the pairwise matrix	metrics parallel pairwise x y func	0.166667
opposite of the local outlier factor	neighbors local outlier factor decision	0.125000
returns the number of splitting iterations in the	one out get n splits	0.111111
to avoid	externals joblib	0.009524
handle the callable case for	callable x	0.083333
and hide warnings without visual nesting	warnings call fn	0.200000
end	end ll	0.166667
cholesky decomposition	log det cholesky	0.166667
x y = tanh(gamma <x y> + coef0)	x y gamma coef0	1.000000
samples x	x	0.010152
extract the	extract	0.142857
shortest path length from source to	shortest path length graph source	0.111111
format check x format and make sure no	dirichlet allocation check non neg array	0.500000
compute minimum distances between one point	metrics pairwise distances argmin	1.000000
samples of length dimensions	samples dimensions	1.000000
the data home cache	clear data home	0.076923
to delete to keep the	to delete root_path bytes_limit	0.500000
module names and a name for	get func name	0.047619
neighbors for points in	radius neighbors	0.086957
construct a featureunion from	core make union	0.250000
x	fit x y	0.023952
for the precision matrix	covariance get precision	0.250000
variance along an axix on a	variance axis x axis last_mean last_var	0.142857
kernel k x y and optionally its gradient	rbf call x y eval_gradient	0.333333
estimates for each input data point	val predict estimator x	0.045455
compute a logistic regression	logistic regression path x	0.333333
training set according	fit predict	0.055556
estimates for each	estimator	0.014706
back the data to the	inverse transform x	0.051282
absolute sizes of training subsets and	core translate train sizes	0.066667
maximum	preprocessing max	0.166667
the search over parameters	search cv fit x	0.111111
input	x	0.001692
of a memmap instance to reopen on	externals joblib reduce memmap a	0.050000
by arpack or randomized	svd_solver	0.166667
estimates for	cross	0.037037
back	standard scaler inverse transform	0.066667
path length	path length	0.666667
remove	joblib	0.007299
windows cannot encode some characters in filename	externals joblib clean win chars string	0.333333
labels the output of transform	transform	0.011236
mmap_mode	mmap_mode	1.000000
build a batch of estimators within a job	build estimators n_estimators ensemble x	0.166667
a temporary folder if still existing	externals joblib delete folder folder_path	0.250000
the recall the recall is the ratio tp	recall	0.028571
lars using	lars	0.090909
samples in x into a matrix	x	0.001692
of determination regression score	metrics r2 score y_true y_pred	0.125000
embedding analysis on	embedding x n_neighbors n_components	0.200000
and scale the data	y	0.002674
execution only a fraction of	externals joblib	0.004762
check x	decomposition latent dirichlet allocation check	0.062500
estimate the precisions parameters of	gaussian mixture estimate precisions nk xk	0.166667
joint	joint	0.714286
largest k singular values/vectors for a sparse	svds a k ncv tol	0.166667
this operation is meant to be	index_file_path data_folder_path slice_ color	0.033333
scaling features	scaler	0.031250
a diagonal model	diag	0.031250
gaussian process regression	gaussian_process gaussian process regressor	0.058824
types to be captured	joblib parallel backend base get exceptions	0.166667
coef0	coef0	1.000000
data	predict estimator x y	0.045455
of the dual gap convergence criterion the	covariance dual gap emp_cov precision_	0.071429
propagation classifier read more in the :ref	propagation	0.076923
paired distances between x	metrics paired distances x	0.500000
find two non-negative matrices w	x w	0.083333
the deviance (= 2 * negative log-likelihood)	ensemble binomial deviance call y pred	0.333333
number of splitting iterations in	one out get n splits x	0.111111
integer indices corresponding to test sets	iter test indices	0.333333
find two non-negative matrices w h	w h	0.031250
names ordered by their	names	0.090909
compute mutual information between two variables	feature_selection compute mi x y	1.000000
parallel processing this method is	parallel	0.019231
get the boolean mask indicating which	get support mask	0.333333
of the samples x to the separating hyperplane	lib svm decision function x	0.250000
if y - pred	y pred	0.250000
documents	feature_extraction count vectorizer fit raw_documents y	0.125000
the position	mds fit x	0.066667
transform the data and concatenate results	transform x y	0.031250
in the svmlight / libsvm format	svmlight file f	0.066667
a new ndarray with aligned	utils aligned	0.500000
list of exception	backend	0.016949
interpret	feature_selection calculate threshold	1.000000
a random regression problem with sparse uncorrelated design	make sparse uncorrelated n_samples	0.166667
*	linear_model intercept	1.000000
range approximates the range of	randomized range finder	0.083333
predict multi-output variable	core multi output estimator predict x	0.166667
it	joblib memorized	0.015625
a which this function is called to issue	externals joblib memorized func check	0.125000
the binary classification	score y_true y_score	0.025000
validate x whenever one tries to predict apply	decision tree validate x predict x	0.500000
data x with ability to accept precomputed doc_topic_distr	precomp distr x doc_topic_distr sub_sampling	0.500000
fit the model from data	manifold isomap fit	0.333333
the graph-lasso objective function the objective	covariance objective	0.125000
for the	covariance empirical covariance get	0.166667
matrix x and target y	multilayer perceptron partial	0.166667
truncated	truncated	0.600000
the maximizer of the reduced likelihood	gaussian_process gaussian process arg max reduced likelihood	0.250000
nugget	nugget	1.000000
to avoid the hash depending	joblib memorized func	0.014706
ortho	ortho	1.000000
constructor store the useful information for later	externals joblib ndarray wrapper init filename subclass allow_mmap	0.200000
write the function code and the filename	memorized func write func code filename func_code first_line	1.000000
apply clustering to a projection to	clustering	0.050000
as training data	neighbors local outlier	0.142857
fit	multi output estimator fit x y sample_weight	0.200000
determination regression score function	metrics r2 score y_true y_pred sample_weight multioutput	0.125000
number of splitting iterations in the cross-validator parameters	base kfold get n splits	0.111111
matrix shrunk on the	covariance shrunk	0.066667
pairs for different probability thresholds	probas_pred pos_label	0.200000
estimates	val	0.037037
the function called with the given arguments	func	0.011364
length from source to	length graph source	0.200000
cf tree for the	cluster birch fit	0.200000
wrapper	wrapper	0.875000
for reproducibility flips the sign of elements of	deterministic vector sign flip	0.066667
checkerboard structure for	checkerboard shape n_clusters noise minval	0.066667
standardize a	with_centering with_scaling	0.200000
neighbors within	radius neighbors	0.043478
the logistic	linear_model logistic	0.222222
to	joblib memorized func	0.014706
build a contingency matrix describing the relationship	cluster contingency matrix labels_true labels_pred	0.333333
long type introduces an 'l'	repr	0.012500
of array-like or scipy sparse	preprocessing binarize x threshold	0.083333
covariance matrix	cov x y	0.500000
of the derived class	x resp	0.166667
set of points	axis metric	0.250000
boolean mask x	get mask x	0.333333
make predictions using a single binary	predict binary	0.200000
download the 20 newsgroups data	download 20newsgroups	0.200000
indices increasingly apart the distance depending	verbosity filter index	0.055556
persist an arbitrary python object into	externals joblib dump value filename compress protocol	0.250000
graphlasso covariance model to	covariance graph lasso cv fit	0.111111
nicely formatted statement displaying	args kwargs object_name	0.166667
prediction of init	ensemble base gradient boosting init decision	0.142857
this dataset is described	datasets	0.030303
for the voting classifier valid parameter keys	ensemble voting classifier	0.031250
expression of the dual gap convergence criterion	covariance dual gap emp_cov precision_	0.071429
inplace column scaling of	inplace column scale x	0.166667
to fit an estimator within a	fit estimator estimator	0.055556
coefficient matrix	coef	0.058824
get	classifier get	0.200000
sample	utils compute sample	0.100000
a decision tree regressor from the training	tree decision tree regressor fit	0.250000
x as	x	0.005076
recall is the	recall	0.028571
regions (=leaves)	region tree terminal_regions leaf	1.000000
introduces an 'l' suffix when using the	shape	0.011765
call	call args kwargs	1.000000
for a fit across one fold	feature_selection rfe single fit rfe estimator x	0.200000
a score	score estimator x y groups	0.333333
the depth a which this function is called	externals joblib memorized func check previous func code	0.055556
case method='lasso' is :	y xy gram	0.090909
checker utility for building a cv in	cv cv x y	0.031250
return the path	get	0.012048
regression target at each stage for	boosting regressor staged	0.500000
method for updating terminal regions (=leaves)	terminal region tree terminal_regions leaf	0.066667
returns the number of splitting iterations in	one out get n splits x y groups	0.111111
x as	fit x	0.006410
non-negative matrix factorization nmf find two	negative factorization	0.043478
that implement the partial_fit api need to	utils check partial	0.038462
the 20 newsgroups data and stored it as	20newsgroups	0.055556
the paired cosine distances between x and y	paired cosine distances x y	0.333333
zero row of x to unit norm parameters	preprocessing normalizer transform x y copy	0.250000
binarization transformation for	binarize	0.045455
get the values used to update params	get	0.012048
average path	ensemble average path	0.142857
x relative	metrics predict scorer call estimator x	0.166667
kernel	kernel call	0.333333
outlier on	outlier	0.100000
leave one group out cross-validator	leave one group out	0.200000
is the	y_true y_pred beta	0.500000
a platform independent representation	repr	0.012500
and false positives per binary	binary clf curve y_true y_score	0.090909
the recall is the ratio tp /	recall	0.028571
proper format	weight boosting validate	1.000000
compute gaussian log-density at x for a diagonal	mixture log multivariate normal density diag x means	1.000000
determine the optimal batch	externals joblib auto batching mixin compute batch	0.333333
utility for building a cv in	core check cv cv x y	0.031250
paired	paired	0.700000
function best possible score is 1	score y_true y_pred	0.038462
indices	generate indices	1.000000
thresholding of array-like or scipy	binarize	0.045455
samples in x into a	x	0.001692
laplacian kernel between x and y	laplacian kernel x y gamma	0.333333
the least-squares solution to a large sparse	utils lsqr a	0.037037
samples from a gaussian	sample gaussian	1.000000
indicate if wrapped	one vs one classifier pairwise	1.000000
estimator on training subsets incrementally and	core incremental fit estimator estimator x y	0.200000
a projection to the	spectral	0.026316
density lrd the lrd of a	density distances_x neighbors_indices	0.200000
the trained	neural_network base multilayer perceptron	0.083333
each	estimator x y	0.038462
from	manifold isomap	0.333333
loss of	loss	0.027027
a job	parallel	0.019231
partially fit a single binary estimator one-vs-one	partial fit ovo binary estimator x y	1.000000
make cache size fit	size	0.032258
for a sparse matrix	svds a	0.166667
decorate function fun	utils deprecated decorate fun fun	1.000000
fit the model	estimator fit x y sample_weight	0.200000
vector is	positivity	0.142857
fit a multi-class classifier by combining binary classifiers	fit	0.003257
covariance matrices from a given template	to match covariance type tied_cv covariance_type n_components	0.333333
locally linear embedding	locally linear embedding x n_neighbors	0.071429
class	classifier	0.095890
cross-validated estimates	core cross val predict estimator x y cv	0.071429
downloading it if necessary	data_home download_if_missing random_state shuffle	1.000000
for the california housing dataset from	datasets fetch california housing	0.083333
people dataset this operation is meant to be	people data_folder_path slice_ color resize	0.333333
points in x	x n_neighbors mode	0.500000
classification task	y_true y_score pos_label	0.066667
vectors rows of u such	u	0.032258
compute non-negative matrix factorization nmf find two	factorization	0.035714
input validation for standard	x y x y accept_sparse dtype	0.250000
score on the given data	score x y	0.030303
the decision function of	gradient boosting classifier decision function	0.166667
cv and linearsvc	fit liblinear x y c fit_intercept	0.142857
kwargs using	kwargs	0.076923
collect	classifier collect	1.000000
seeds	seeds x	0.250000
build a batch of estimators within	ensemble parallel build estimators n_estimators ensemble x	0.166667
mostly low rank matrix with bell-shaped singular values	datasets make low rank matrix	0.083333
p	p	1.000000
of x from y along the first axis	x	0.001692
get feature names from all transformers	core feature union get feature names	1.000000
that for c in (l1_min_c	c x y	0.030303
the scaling	scaler	0.031250
c such that for c in (l1_min_c infinity)	c x	0.030303
check the validity of the	neighbors check params x metric p metric_params	0.200000
raw file object e g created with	raw file	0.200000
from	x y sample_weight	0.012987
friedman [1] and breiman [2]	friedman3 n_samples	0.166667
an	utils	0.009709
right fileobject from	joblib read fileobject fileobj	0.100000
run fit	fit	0.003257
to	linear_model sparse	0.076923
check that predict raises an	utils check estimators	0.142857
apply transforms and predict_log_proba of	predict log proba x	0.045455
that implement the partial_fit api	utils check partial fit	0.038462
mini-batch dictionary learning finds a dictionary a	dictionary learning	0.142857
back to line_search_wolfe2 if	wolfe12 f fprime xk pk	0.028571
compute	decomposition compute	1.000000
deviance	deviance call y pred sample_weight	0.333333
for	core cross val predict estimator x	0.045455
a single binary estimator one-vs-one	ovo binary estimator x	0.500000
input	val predict	0.045455
similarity of two	score a b similarity	0.125000
check initial parameters of the derived	check parameters x	0.200000
seeds for mean_shift	bin seeds x bin_size min_bin_freq	0.500000
the maximum likelihood estimator covariance model according	covariance empirical covariance fit	0.166667
metric for multilabel classification parameters	score binary_metric y_true y_score	1.000000
don't store	memory	0.015625
transform	preprocessing transform selected x transform	0.333333
estimate sample weights by class for unbalanced datasets	sample weight class_weight y indices	0.500000
project data to vectors and cluster	biclustering project and cluster data vectors n_clusters	0.333333
the maximum likelihood estimator	empirical	0.055556
equal to the average	ensemble average	0.125000
the model using	linear_model base randomized linear model	0.500000
class versus all others	multiclass x y alpha	0.166667
from prediction scores this score corresponds	score y_true y_score	0.025000
the proper format	base weight boosting validate x	1.000000
fit the model using x y as training	core isotonic regression fit x y sample_weight	1.000000
fit linear	base sgdclassifier fit x y	0.333333
predict multi-class targets	output code classifier predict	0.250000
delete	delete	1.000000
of the base classifiers	ensemble bagging classifier	0.200000
center to the mean and component wise scale	scale	0.033333
log of	linear_model sgdclassifier predict log	0.500000
check x format	check	0.017857
described in [rouseeuw1984]_ aiming at computing mcd	c step x n_support remaining_iterations initial_estimates	0.111111
fit on the estimator with	fit x y	0.005988
that can actually run in parallel n_jobs is	n_jobs	0.023256
sizes of	sizes	0.050000
loader for	data_home download_if_missing	0.400000
homogeneity metric of a cluster labeling given	metrics cluster homogeneity	0.500000
function to output a function call	externals joblib function called str function_name args kwargs	0.250000
each input	predict	0.006849
back the data to the original	inverse transform x	0.051282
log probability for full covariance	mixture log multivariate normal density full x means	0.333333
utility for building a cv	check cv cv x	0.031250
finds radius neighbors from the candidates obtained	lshforest get radius neighbors query max_depth bin_queries radius	1.000000
the median of data	get median data	0.333333
to run in parallel	joblib	0.007299
index of the leaf	decision tree apply x	0.166667
svmlight / libsvm format into sparse	svmlight file f n_features dtype	0.066667
unit	axis copy	0.166667
of all tokens in the raw documents	feature_extraction count vectorizer fit raw_documents y	0.125000
concurrency	concurrency	1.000000
width	effective_rank tail_strength	0.125000
standardize a	with_mean with_std	0.200000
input_features	input_features	1.000000
labels back	binarizer inverse	0.166667
the elastic net optimization function varies for	l1_ratio	0.030303
a single binary estimator one-vs-one	ovo binary estimator x y	0.500000
evaluate a score	score estimator	0.250000
group out cross-validator provides train/test	group out	0.142857
the complete cache directory	memory clear warn	0.333333
method for updating terminal	loss function update terminal	0.200000
of jobs for the	jobs n_jobs	0.100000
compute the grid of alpha values for	alpha grid x	0.166667
when memory is inefficient to train all data	y classes	0.027778
convert coefficient matrix to	sparse	0.025000
input validation	check x y x y accept_sparse dtype	0.250000
number of splitting iterations in the	one group out get n splits x y	0.111111
remove cache folders	reduce	0.017241
training set x	x	0.001692
covertype dataset	datasets fetch covtype	0.333333
cv in	cv cv	0.031250
decides whether it is time to stop	neural_network base optimizer trigger stopping msg verbose	0.250000
gradient and	y	0.005348
faces in the wild lfw pairs	fetch lfw pairs subset	0.035714
count and	core bernoulli nb count x y	0.250000
for later	fit x	0.006410
for building a cv in a user	check cv cv x y classifier	0.031250
extracts patches of any	feature_extraction extract patches	0.083333
matrix given column class distributions	classes class_probability random_state	0.166667
mean and variance along an	mean variance axis x axis	0.142857
for the voting classifier valid parameter keys	ensemble voting classifier set	0.037037
is restricted to the binary classification task	recall curve y_true	0.142857
swaps two rows of a csc matrix in-place	utils inplace swap row csc x m n	0.250000
from source	source cutoff	0.200000
according to the given	y sample_weight	0.035714
store the	joblib memorized func	0.014706
exceptions	message exceptions	1.000000
generalized	generalized	1.000000
and variance	variance	0.176471
the function call with	joblib format call func	0.100000
transformed real-valued array into a	projection to	0.166667
convert coefficient matrix to dense array	mixin densify	0.100000
build	parallel build	0.047619
the dual gap convergence criterion the	covariance dual gap emp_cov precision_ alpha	0.071429
derivatives with respect to each parameter weights and	y activations deltas	0.500000
apply dimensionality reduction on	decomposition randomized pca transform	1.000000
timestamp when pickling to	externals joblib memory reduce	0.030303
keep	root_path bytes_limit	0.500000
hessian in the case of a multinomial loss	linear_model multinomial grad hess w	1.000000
lrd of a sample is	distances_x neighbors_indices	0.047619
observations in x	x	0.001692
for the voting classifier valid parameter	ensemble voting classifier set params	0.037037
estimate the precisions	bayesian gaussian mixture estimate precisions nk xk	0.166667
for c in (l1_min_c infinity)	c	0.022222
of the mixture parameters	mixture bayesian gaussian mixture	0.333333
construct a pipeline from the	core make pipeline	0.250000
connectivity matrix	x connectivity n_components affinity	1.000000
all of its patches	patches 2d patches image_size	0.333333
and compute	y classes	0.055556
calculate mean update and	incremental mean	0.166667
pickle-protocol - set state of	core isotonic regression setstate state	0.250000
decisions within a job	estimators_features	0.071429
or thread pool	multiprocessing backend	0.076923
data loading for the lfw people dataset	datasets fetch lfw people	0.040000
infers the dimension of a	dimension	0.050000
memmap instance to	reduce memmap	0.166667
and predicted probabilities for a calibration curve	calibration curve y_true y_prob	0.142857
a logistic regression	logistic regression path x	0.333333
wrapped function cache result	externals joblib memorized func	0.013158
actually run in parallel	externals joblib parallel backend	0.029412
set x and returns the labels	predict x y	0.043478
function to output	called str function_name	0.250000
given arguments	func get output	0.125000
collect results from clf predict calls	voting classifier collect probas x	1.000000
memory	joblib memory	0.016949
maximizer of the reduced	arg max reduced	0.200000
adjusted	adjusted	0.750000
vectors rows of u such	flip u	0.047619
graph of neighbors for	neighbors graph	0.066667
along an axis on	x axis	0.015385
func to be	mixin apply async func	0.250000
estimates	cross val predict	0.045455
of	backend	0.016949
of the data home cache	clear data home data_home	0.076923
make and configure a copy of	ensemble base ensemble make estimator append random_state	0.166667
boston house-prices	boston return_x_y	0.250000
store the timestamp when pickling	memory reduce	0.030303
compute decision function	decision function	0.050000
and breiman [2]	make friedman3 n_samples noise random_state	0.166667
voting classifier valid parameter keys can	voting classifier set params	0.037037
to the cache for	externals joblib memorized func	0.013158
build a batch of estimators within a	ensemble parallel build estimators	0.166667
type introduces	repr	0.012500
estimator on	estimator estimator x	0.181818
to	reduce	0.017241
x	x n_neighbors reg	0.500000
classifier valid parameter keys	classifier set params	0.125000
of	backend base	0.032258
computes the position of the points	mds fit	0.066667
terminal regions to	terminal region tree	0.100000
using the gp prior	return_std return_cov	0.142857
the position of the points	mds fit x y init	0.066667
actually run in parallel n_jobs is the is	n_jobs	0.023256
the estimator with the best found	search cv predict proba x	0.076923
coordinate	coordinate	1.000000
boosting for classification	boosting classifier	0.250000
train estimator on training subsets incrementally and	incremental fit estimator estimator x y classes	0.200000
matching pursuit model	matching pursuit	0.333333
the reduced likelihood function	gaussian_process gaussian process reduced likelihood function	0.047619
call	call args	1.000000
linear embedding	linear embedding x n_neighbors	0.200000
estimate the precisions parameters of the	gaussian mixture estimate precisions	0.166667
model with	decomposition pca	1.000000
absolute	absolute	1.000000
the kernel k	constant kernel	0.250000
set the	set	0.240000
hash	externals	0.011494
from a	externals joblib read	0.333333
patch data	patch extractor transform	0.200000
returns a list of edges for a	make edges	0.066667
perform a locally linear embedding analysis on the	locally linear embedding x	0.071429
ward clustering based on a feature	cluster ward tree x connectivity n_clusters return_distance	0.250000
exception in an unfitted estimator	unfitted name estimator	0.142857
computes the position of the	mds fit x y	0.066667
on x	fit predict x	0.250000
run fit on one set	model_selection fit grid point x y	0.500000
predict on the estimator with the	predict x	0.011765
and false positives per binary classification threshold	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
and linearsvc	svm fit liblinear x y c fit_intercept	0.142857
best found	search cv predict proba x	0.076923
estimate class weights for unbalanced datasets	compute class weight class_weight classes y	0.500000
fit the	core rbfsampler fit	0.250000
labels back	label binarizer inverse	0.166667
empty the function's cache	joblib memorized func clear warn	0.250000
transforms the image samples in x into a	x	0.001692
split	split	0.138889
model	model	0.411765
mse for the models computed by 'path' parameters	linear_model path residuals x	0.250000
parameters of this estimator	params	0.028571
a single binary estimator	predict binary estimator	0.200000
estimator on training subsets incrementally	model_selection incremental fit estimator estimator x	0.500000
returns the score	score	0.020202
fit linear model with passive aggressive algorithm	passive aggressive classifier fit x y	1.000000
binomial deviance loss function for binary	binomial deviance	0.250000
shutdown the	terminate	0.090909
and cv and linearsvc	liblinear x y c fit_intercept	0.142857
residual (= negative gradient)	ensemble binomial deviance negative gradient y	0.333333
local	factor local	0.500000
and make sure no negative value in x	non neg array x whom	0.500000
data home cache	data home	0.076923
linear model	linear_model	0.051282
determinant matrix	fast mcd x support_fraction cov_computation_method	1.000000
check that	utils check	0.071429
names and a name	get func name	0.047619
x according to feature_range	x	0.001692
similarity coefficient score	score y_true y_pred	0.038462
linear model	sgdregressor	0.181818
generate cross-validated estimates	predict estimator x y cv	0.071429
the reduced	gaussian_process gaussian process reduced	0.125000
fixes	cluster fix connectivity	1.000000
make and configure a copy of the	ensemble base ensemble make estimator append random_state	0.166667
classifier predicts one class versus all others	multiclass x y alpha c	0.166667
perform dbscan clustering from vector array	cluster dbscan x eps min_samples	0.200000
free energy f v = - log sum_h	bernoulli rbm free energy	0.066667
n_jobs even	n_jobs	0.023256
the data x which should contain a	x y	0.002155
regression problem with sparse uncorrelated design	sparse uncorrelated n_samples n_features random_state	0.166667
sum_h exp(-e v h	v	0.052632
to	backend base	0.032258
along an axis on	axis x axis	0.083333
cache folders to make cache size fit in	reduce size	0.083333
the covariance matrices from a given template	covar matrix to match covariance type tied_cv covariance_type	0.333333
func_name	func_name	1.000000
samme	samme	1.000000
the voting classifier valid parameter	voting classifier set	0.037037
extracts patches of any n-dimensional array in	feature_extraction extract patches	0.083333
training set x y	fit x y sample_weight	0.060000
type introduces an 'l'	utils shape	0.013699
factorizing common classes param logic estimators that implement	utils check partial fit first call clf classes	0.333333
to bicluster	core bicluster mixin	0.500000
then return the score	rfe score	0.200000
transformer on	preprocessing imputer	0.333333
estimate the precisions parameters	gaussian mixture estimate precisions nk xk	0.166667
of moved objects in six moves urllib_robotparser	module six moves urllib robotparser	0.333333
getter for	empirical covariance	0.125000
input data point	val predict estimator	0.045455
the best	cv predict	0.083333
to a large sparse linear	utils lsqr a	0.037037
the k-neighbors of a point	kneighbors mixin kneighbors x	0.125000
transform on	transform	0.011236
precision matrix with the generative model	pca get precision	0.066667
for	val predict estimator	0.045455
calculate mean	mean	0.035714
perform a locally linear	locally linear	0.200000
the	shape	0.023529
the number of splitting iterations in the	out get n splits x	0.111111
used in hastie	datasets make hastie	0.125000
score by cross-validation read more in the	cross val score estimator x y groups	0.166667
l1 distances between the	manhattan distances	0.083333
cache items to delete to keep the cache	cache items to delete root_path bytes_limit	1.000000
the number of splitting iterations in	base kfold get n splits x y groups	0.111111
check values of the basic	mixture base mixture check initial	1.000000
the best found parameters	core base search cv predict proba x	0.076923
the "friedman \#3" regression problem this dataset is	datasets	0.015152
the :ref user guide	y_true y_pred	0.111111
the variational distributions for	mixture dpgmmbase	0.166667
evaluate a score by cross-validation	core cross val score estimator x y scoring	0.333333
matrix factorization	factorization	0.035714
incremental mean and variance	utils incr mean variance	0.333333
fit a single binary estimator	fit binary estimator x y classes	1.000000
file	zlib file	0.230769
a given radius of a	radius	0.045455
kernels	kernel	0.015625
returns the number of estimators	len	0.038462
from file-like object until size bytes are	bytes fp size error_template	0.333333
kernel k	gaussian_process product	1.000000
covariance determinant matrix	covariance fast mcd x support_fraction	0.250000
compute l1 and l2 regularization coefficients for w	decomposition compute regularization alpha l1_ratio regularization	0.333333
tree	tree	0.785714
wild lfw pairs dataset	lfw pairs subset	0.035714
fit the model according	quadratic discriminant analysis fit	1.000000
the model from	manifold spectral embedding	0.111111
it	externals joblib memorized func	0.013158
a multinomial	linear_model multinomial	0.200000
equal to the average path length	average path length	0.090909
compute the loss	ensemble loss	0.166667
predicted probabilities for a calibration	core calibration	0.125000
binary classification task	y_true y_score pos_label	0.066667
matrix factorization nmf find two non-negative	negative factorization	0.043478
compute area under the curve auc using	auc	0.020408
array-like or scipy sparse matrix	binarize x threshold	0.083333
in pipeline after transforms	core pipeline fit predict	0.166667
points	metric	0.071429
to implement the usual api and	y	0.008021
log-likelihood of a gaussian data	score	0.010101
modified weiszfeld	modified weiszfeld	1.000000
for	predict estimator	0.045455
initial	initial	0.833333
fit ridge regression model parameters	linear_model ridge gcv fit x y	1.000000
of neighbors	radius neighbors mixin radius neighbors	0.125000
reduced likelihood function for the given autocorrelation parameters	gaussian process reduced likelihood function	0.047619
is monotonically correlated with	core check increasing	1.000000
initialize the model parameters of the derived	base mixture initialize x resp	0.500000
sparse inverse covariance w/ cross-validated choice of	cv	0.009009
the posterior log probability of the samples x	core multinomial nb joint log likelihood x	0.500000
bytes_limit	joblib memory reduce	0.030303
loss and the gradient	loss and gradient w	1.000000
for each input data point	cross val predict estimator x y	0.045455
predict using the	predict x	0.023529
the initial centroids	cluster init centroids x k init random_state	0.166667
of init	ensemble base gradient boosting init	0.142857
the unnormalized posterior log probability of x i	base nb joint log likelihood x	0.200000
of exception types to	backend base get	0.066667
r^2 coefficient of determination regression score	metrics r2 score y_true y_pred sample_weight multioutput	0.125000
covariance matrices from a given template	to match covariance type tied_cv covariance_type	0.333333
the number of	effective n	0.333333
for	empirical covariance	0.125000
of new samples can be different from the	core calibrated classifier cv	0.111111
for building a cv in a	check cv cv x y classifier	0.031250
densify	densify	0.333333
compute the laplacian kernel between x and y	metrics laplacian kernel x y	0.333333
warnings	ignore warnings	0.166667
as line_search_wolfe1 but fall back to line_search_wolfe2 if	wolfe12 f fprime xk pk	0.028571
scale back the data to the	standard scaler inverse transform	0.066667
eigs	eigs	1.000000
dataset	datasets	0.045455
returns the number	len	0.038462
load sample images for image	load sample images	0.250000
boosted regressor from the training set	ensemble ada boost regressor fit	0.500000
list of exception types	parallel backend base get	0.066667
input data	val	0.037037
tolerance which is independent	cluster tolerance	0.058824
fit kernelcenterer parameters	preprocessing kernel centerer fit k y	1.000000
then return the score of	rfe score	0.200000
result	result	0.833333
is not found	search	0.019231
blup parameters and evaluates the reduced	gaussian process reduced	0.125000
do nothing and return the estimator unchanged	feature_extraction	0.037037
labels	voting	0.066667
random	n_samples eps	0.125000
models computed by 'path' parameters	linear_model path residuals x y	0.250000
evaluate a score by cross-validation read more in	model_selection cross val score estimator x	0.166667
the timestamp when pickling	memorized func reduce	0.050000
process or thread pool	joblib	0.007299
compute non-negative matrix factorization nmf	factorization x	0.043478
the weighted graph of neighbors for points	neighbors radius neighbors mixin radius neighbors graph	0.066667
tolerance which is independent of the	cluster tolerance x	0.058824
estimators that implement the partial_fit api	utils check	0.023810
detects the	one class svm fit	0.125000
a given mapping	y class_mapping	0.333333
coefficient score	score y_true	0.058824
build a batch of estimators within a job	ensemble parallel build estimators n_estimators ensemble	0.166667
full	density full x means covars	0.166667
case method='lasso' is	x y xy gram	0.090909
described in friedman [1] and breiman [2]	friedman3 n_samples	0.166667
predict class probabilities for	ensemble ada boost classifier staged predict proba	1.000000
matrix factorization nmf find two non-negative matrices w	factorization x w	0.500000
x and transform x	transform x y	0.093750
each input data	cross val	0.038462
the shrunk covariance model according to the	covariance shrunk covariance fit x	0.083333
the callable case for pairwise_{distances	pairwise callable x y	0.083333
x	squared call x	1.000000
returns the number of splitting iterations in	cross validator get n splits x	0.125000
reconfigure the backend and return the number	externals joblib parallel backend base configure	0.333333
the number of splitting iterations in	predefined split get n splits x y	0.111111
get parameters of this	gaussian_process exponentiation get params deep	0.500000
get parameters of	get params	0.400000
output of transform is sometimes	transform y	0.023256
the flattened log-transformed non-fixed hyperparameters	gaussian_process compound kernel theta theta	0.333333
data point	predict estimator x	0.045455
unnormalized posterior log probability of x i	nb joint log likelihood x	0.111111
a score by cross-validation	core cross val score estimator x	0.333333
for a calibration curve	calibration curve	0.142857
the shortest path length from source to	shortest path length graph source	0.111111
the precision matrix	precision	0.016667
update reporter with new iteration	verbose reporter update j est	1.000000
iterate over the points in the grid	core parameter grid iter	1.000000
the dual gap convergence criterion the specific definition	dual gap	0.071429
problem this dataset is	datasets make	0.015625
computes the log-likelihood	empirical covariance score	0.166667
with non-overlapping	label	0.045455
the best found parameters	model_selection base search cv predict proba	0.076923
to x	fit x y	0.005988
to x return leaf	x	0.001692
zero-one classification loss	metrics zero one loss y_true y_pred normalize sample_weight	1.000000
the voting classifier valid parameter keys can be	ensemble voting classifier set	0.037037
clusterings	labels_true labels_pred contingency	1.000000
train	base	0.014286
decision functions of the base	decision function	0.025000
mean squared logarithmic error regression	metrics mean squared log error	0.200000
regression based on neighbors	neighbors regressor	1.000000
true and predicted probabilities for a calibration	calibration	0.071429
posterior probabilities of classification	classifier cv predict proba x	0.200000
matching pursuit problems	y n_nonzero_coefs	0.500000
for a	means covars	0.500000
wild lfw pairs dataset	datasets fetch lfw pairs subset	0.035714
the mean squared error	error norm comp_cov norm scaling squared	0.166667
the number of splitting iterations in	model_selection predefined split get n splits x y	0.111111
compute non-negative matrix factorization	decomposition non negative factorization x	0.043478
stacklevel is the depth	func check previous func code stacklevel	1.000000
an array of array from list of arrays	array of arrays	1.000000
coefficient matrix to	sparse coef mixin	0.083333
function with the given arguments and	func call	0.047619
multi-task l1/l2 elasticnet with built-in cross-validation	multi task elastic net cv	1.000000
regression problem with sparse	sparse	0.025000
center x y and	cross_decomposition center scale xy x y	1.000000
of the data	clear data	0.142857
features are selected	feature_selection	0.066667
prediction of init	base gradient boosting init decision function	0.142857
factorization nmf find	non negative factorization x	0.043478
classification task	recall curve y_true	0.142857
apply decision function	analysis decision function	0.500000
the dual gap convergence criterion the specific definition	covariance dual gap emp_cov	0.071429
for the lfw people dataset	datasets fetch lfw people	0.040000
to avoid the hash depending	externals joblib	0.009524
mstep	do mstep	0.500000
returns whether the kernel is	gaussian_process kernel operator is	1.000000
compute mean and	mean	0.035714
in the wild lfw pairs dataset	fetch lfw pairs subset	0.035714
to y_true	y_true sample_weight	1.000000
estimator adheres to scikit-learn conventions	estimator estimator	0.052632
for building a cv in a user	check cv cv	0.031250
pairwise matrix in	pairwise	0.066667
data precision matrix with the	get precision	0.052632
store the timestamp when pickling to avoid the	memory reduce	0.030303
run in parallel	externals joblib parallel backend	0.029412
from source to all reachable nodes	source cutoff	0.200000
persist an arbitrary python object into one file	value filename compress protocol	0.250000
the timestamp when pickling to avoid	memory reduce	0.030303
fit a single binary estimator	fit binary estimator	1.000000
from	joblib memorized func	0.014706
returns first and last element of	core first and last element arr	0.250000
fit	rbm fit	0.333333
of a memmap instance to	joblib reduce memmap a	0.050000
fit gaussian process classification model	gaussian_process gaussian process classifier fit	0.500000
generate names for	name	0.033333
binary classifier predicts one class versus all others	multiclass x y	0.166667
when	repr	0.012500
found and raise	utils line search	0.029412
solution to a sparse coding problem	decomposition sparse encode x	0.333333
inverse label binarization transformation using thresholding	preprocessing inverse binarize thresholding	1.000000
number of splitting iterations in	one out get n splits x y groups	0.111111
compute class covariance matrix	class cov	0.250000
two non-negative matrices w h	x w h	0.035714
of max	preprocessing max	0.166667
probabilities p_ij from distances	probabilities distances desired_perplexity verbose	1.000000
the model using x as	x	0.003384
fit estimator and compute scores	core fit and score estimator x y	0.333333
em update	em step x	0.500000
clustering on x and	fit predict x y	0.250000
rows of a csr matrix in-place	utils inplace swap row csr x m n	0.250000
vocabulary dictionary and return term-document	transform raw_documents y	0.100000
check the gaussian mixture parameters are well defined	mixture gaussian mixture check parameters	1.000000
scaling	scaler fit x	0.076923
of determination regression score function	metrics r2 score y_true	0.125000
k x y and	exponentiation call x y	0.200000
arbitrary python object into	joblib dump value filename	0.083333
test_size and train_size at	test_size train_size	0.200000
consistent	consistent	1.000000
types	backend base	0.032258
fit the kernel density model on	neighbors kernel density fit x	0.250000
helper class for managing	manager mixin	0.500000
total log probability under	neighbors kernel density score	0.333333
perform mean shift	mean shift x bandwidth seeds	0.500000
file-like object until size bytes are read	joblib read bytes fp size error_template	0.500000
indices in sorted array	matching indices tree bin_x left_mask right_mask	0.166667
sparse inverse covariance w/ cross-validated	cv	0.009009
determine the optimal batch size	backend base compute batch size	1.000000
decorator used to capture the arguments of a	check_pickle	0.040000
the time under windows this is the time	time t	0.125000
isotonic	isotonic	0.833333
used to fit an estimator	fit estimator estimator x y sample_weight	0.071429
step in pipeline after transforms	core pipeline fit predict x y	0.166667
and transform with	transform x	0.016949
place using strides	arr patch_shape extraction_step	0.166667
compute the laplacian kernel between x and y	laplacian kernel x y	0.333333
- y_[i]) ** 2	sample_weight y_min y_max	0.166667
transformer	transformer	0.500000
fit all transformers using x	core feature union fit x y	1.000000
multiple files in svmlight format this	svmlight files files	0.200000
provided data parameters	neighbors radius neighbors	0.100000
squared euclidean norm of	squared	0.083333
absolute error	absolute error	0.142857
a memmap instance to	memmap a	0.050000
from	size	0.032258
the pairwise matrix in n_jobs even slices	parallel pairwise x y func n_jobs	0.111111
scores note this	metrics roc	0.040000
error	error y_true y_score sample_weight	1.000000
fit ridge regression	ridge gcv fit x y sample_weight	1.000000
of parameters and raise valueerror if not	base gradient boosting	0.100000
decorator used to capture the arguments of	check_pickle	0.040000
the benjamini-hochberg procedure	fdr	0.142857
partially fit underlying	core one vs one classifier partial fit x	0.166667
of determination regression score function	metrics r2 score y_true y_pred sample_weight	0.125000
returns the number of splitting iterations in	leave one out get n splits x y	0.111111
any negative	non negative x whom	0.200000
graph of neighbors for points in x	neighbors radius neighbors mixin radius neighbors graph x	0.500000
kddcup99 dataset downloading it if	fetch brute kddcup99 subset data_home download_if_missing random_state	0.111111
find the first prime element	state find prime	0.500000
the oracle approximating shrinkage covariance	covariance oas fit x	0.083333
lower bound on model evidence based	dpgmmbase lower bound	0.071429
with the generative model	decomposition	0.047619
get the directory	get func dir	1.000000
param logic estimators that implement the	utils check	0.023810
estimate class weights for unbalanced datasets	class weight class_weight classes	0.500000
implement a single boost	boost iboost x y	1.000000
process regression model we can also	process regressor	0.166667
along any axis center to the mean and	axis	0.014085
for	predict estimator x y	0.045455
estimate	estimate	1.000000
the score on the	score x y	0.030303
neighbors	neighbors radius neighbors mixin radius neighbors	0.125000
california	california	0.750000
using a single binary estimator	binary estimator x	0.090909
the maximum absolute value to be	preprocessing max abs	0.050000
for the voting	ensemble voting	0.142857
number of splitting iterations in	pgroups out get n splits x y	0.111111
compute elastic net path with coordinate descent the	linear_model enet path x	0.050000
c in (l1_min_c	c x y loss	0.030303
encode	encode	1.000000
calibrated	calibrated classifier cv	0.071429
by cross-validation read more in	cross val	0.038462
make cache size fit in bytes_limit	joblib memory reduce size	0.083333
the other and transforms the	y	0.002674
kddcup99 dataset downloading it if necessary	kddcup99 subset data_home download_if_missing random_state	0.111111
a binary metric for multilabel classification parameters	binary score binary_metric y_true y_score	0.500000
of x for later	fit x y	0.005988
back the data	standard scaler inverse transform	0.066667
score	core score	0.166667
param logic estimators that implement	utils check	0.023810
the recall the recall	recall	0.028571
platform independent representation of	repr	0.012500
and y read	y	0.002674
and dot	decomposition beta divergence	0.500000
compute directory associated with	to dir cachedir func argument_hash	0.250000
not enabled for sparse	robust	0.090909
compute the residues on left-out data	residues x_train y_train x_test y_test	0.083333
data onto the sparse components	decomposition sparse pca transform x ridge_alpha	0.200000
class at each stage	ensemble gradient boosting classifier staged	0.333333
parameter names	param names cls	1.000000
'l'	shape repr	0.013699
linear embedding analysis	linear embedding	0.083333
of last step in pipeline after transforms	pipeline fit predict x y	0.166667
according to the given training data and parameters	y	0.002674
in multiplicative update	decomposition multiplicative update w x	1.000000
estimator on training subsets incrementally and	incremental fit estimator estimator x y classes	0.200000
features	features	0.900000
its corresponding derivatives with respect to each	activations deltas	0.032258
the maximizer of the reduced likelihood function	gaussian process arg max reduced likelihood function	0.333333
neighbors within a given radius of a point	neighbors lshforest radius neighbors x radius	0.142857
don't store the	memorized func	0.016949
the arguments of a function	delayed function	0.200000
the pairwise matrix in	parallel pairwise	0.166667
compute the k-way softmax function inplace	neural_network softmax	1.000000
set x	x	0.008460
number of splitting iterations in the	get n splits	0.111111
which are going to	externals joblib multiprocessing backend	0.035714
classification task	y_true y_score pos_label sample_weight	0.066667
linear model with stochastic gradient descent	linear_model base sgdregressor	0.250000
predict class probabilities for x	ensemble gradient boosting classifier predict proba x	1.000000
be used for later	fit x	0.006410
likelihood of the data	x	0.003384
estimator on training subsets incrementally and	core incremental fit estimator estimator x y classes	0.200000
number of splitting iterations in the cross-validator	out get n splits	0.111111
non-negative matrices w h whose	w h n_components	0.038462
build a contingency matrix describing the relationship between	metrics cluster contingency matrix labels_true labels_pred	0.200000
to the file	file	0.035714
apply the derivative of the logistic sigmoid	inplace logistic derivative z delta	0.166667
helper function to	helper	0.100000
generates integer indices corresponding to test sets	iterator iter test indices	0.333333
add	add	0.428571
zero row of x to unit norm parameters	preprocessing normalizer transform x	0.250000
utility for building a cv in a	core check cv cv x y	0.031250
pickle the descriptors of a memmap instance	externals joblib reduce memmap a	0.050000
returns the number of estimators in	len	0.038462
median absolute error	median absolute error	0.166667
a single binary estimator one-vs-one	ovo binary estimator x y i	0.500000
largest k singular values/vectors	k ncv tol	0.166667
false for indices increasingly apart	externals joblib verbosity filter index	0.055556
number of points that will be sampled	core parameter sampler len	0.333333
when the dot operation does not use blas	non blasdot	1.000000
function for factorizing common	fit first call clf	0.200000
returns the number of splitting iterations in the	model_selection base kfold get n splits x y	0.111111
any axis center to the median and	axis	0.014085
decision functions	decision function x	0.018868
return a tolerance which is independent	tolerance x tol	0.058824
the posterior log probability	nb joint log likelihood	0.066667
call predict_proba on the estimator with the best	cv predict	0.083333
return whether the file	file	0.071429
shift	shift x bandwidth seeds	1.000000
the models computed by 'path' parameters	linear_model path residuals x y	0.250000
transforms and score	score x y sample_weight	0.250000
is the depth a	externals joblib memorized func check previous func code	0.055556
neighbors for points in	neighbors radius neighbors	0.100000
a lower bound on model evidence	lower bound	0.071429
from the meta-information	externals joblib zndarray wrapper read unpickler	0.043478
write array bytes to pickler	array wrapper write array array pickler	0.333333
call wrapped function	func call	0.047619
list of	externals joblib parallel backend	0.029412
arbitrary python object into	externals joblib dump value filename	0.083333
initialize the model parameters of	mixture base mixture initialize x	0.333333
the number of splitting iterations in the	get n splits	0.111111
lasso_stability_path	linear_model lasso stability path x y mask	1.000000
transform data back to its original space	decomposition nmf inverse transform	1.000000
the data	data	0.076923
input and compute prediction of init	boosting init decision	0.142857
which is equal to the average path length	average path length	0.090909
batch and	one batch	0.500000
pairs dataset this dataset	pairs subset	0.125000
convert a collection of text documents to	vectorizer	0.022222
ensemble of totally random trees	random trees embedding	0.250000
the average	ensemble average	0.125000
function used to build	ensemble parallel build	0.047619
file was opened for writing	joblib binary zlib file writable	0.250000
x	x y	0.056034
normalize x according to kluger's log-interactions scheme	cluster log normalize x	0.200000
helper function for _fit_coordinate_descent update	update	0.035714
the index of the leaf	apply x	0.166667
function used to compute log probabilities	ensemble parallel predict log proba	0.058824
housing	housing	0.666667
a single tree	build trees tree	0.142857
to fit a single tree	build trees tree	0.142857
fit the model with	core rbfsampler fit	0.250000
graph of neighbors	mixin radius neighbors graph	0.066667
incr	incr	1.000000
underlying estimators should be used	core one vs one classifier	0.111111
lasso path using lars algorithm [1] the	linear_model lars path x	0.100000
fit to data then transform it	transformer mixin fit transform x y	0.500000
packed_parameters	packed_parameters	1.000000
of edges for a	edges	0.047619
function call with the given arguments	joblib format call func	0.100000
error of the kl divergence of p_ijs and	manifold kl divergence error	0.100000
connectivity	connectivity x connectivity	1.000000
fit label binarizer parameters	preprocessing label binarizer fit	0.500000
utility function opening the right fileobject from	read fileobject	0.100000
'l' suffix when	utils shape repr	0.013699
transforms and score	score x y	0.030303
the image samples in x into a matrix	x	0.001692
fit the calibrated model	calibrated classifier cv fit x y sample_weight	1.000000
parameters for the voting	ensemble voting	0.142857
in an unfitted	unfitted name	0.142857
the pairwise matrix	pairwise x	0.166667
least-squares solution to a	a	0.018182
be used for later	fit	0.003257
precision_	precision_	0.833333
train	core base shuffle	0.166667
choice	choice	1.000000
fit linear	base sgdclassifier fit	0.076923
updates terminal	terminal	0.047619
in bytes_limit	joblib memory	0.016949
to avoid	joblib memorized	0.015625
fit linear model with passive aggressive algorithm	passive aggressive regressor fit x y coef_init intercept_init	1.000000
by scaling each feature	preprocessing minmax scale	0.142857
computes the	w x y alpha	0.250000
search over	search cv fit x y	0.111111
target variable	x y discrete_features	1.000000
california housing dataset from statlib	california housing	0.083333
non-negative matrices w h whose product approximates	w h n_components	0.038462
files with	files	0.100000
to check the test_size and train_size at init	shuffle split init test_size train_size	0.250000
voting classifier valid parameter keys can be listed	ensemble voting classifier set	0.037037
exception	externals joblib parallel backend base get	0.066667
for full covariance matrices	normal density full x means	0.166667
with the given arguments and persist the output	func call	0.047619
coverage file	coverage	0.125000
voting classifier valid parameter	ensemble voting classifier set params	0.037037
absolute error regression loss read	absolute error y_true	0.142857
data for binary classification used in hastie	hastie	0.076923
birch	birch	0.625000
the callable case	callable x y	0.083333
strip lines beginning	strip	0.055556
the svmlight / libsvm format into	svmlight file f n_features	0.066667
of the pixel-to-pixel gradient connections edges are weighted	img mask return_as dtype	0.166667
input	estimator x	0.030303
learn and	y	0.002674
decorator used to capture	check_pickle	0.040000
module names and a name	get func name	0.047619
clustering algorithm	cluster	0.021277
evaluate	score samples x	0.500000
boosted classifier from the training set x y	ensemble ada boost classifier fit x y sample_weight	1.000000
matrix product with the random matrix	random projection transform x	0.333333
compute probabilities of possible outcomes for samples in	svm base svc predict proba	0.333333
decision function	gradient boosting classifier decision function	0.166667
partially fit underlying estimators	core one vs one classifier partial fit x	0.166667
transform data to polynomial features parameters	preprocessing polynomial features transform x	0.500000
the number of splitting iterations in	get n splits x y groups	0.111111
utility for building a cv in a	check cv cv x	0.031250
the covariance	covar matrix to match covariance	0.250000
a list of feature name -> indices mappings	feature_extraction dict vectorizer fit x	0.250000
the callable case for pairwise_{distances kernels}	metrics pairwise callable	0.083333
class covariance	class cov x y priors	0.250000
scaling of x	scaler inverse transform x	0.026316
determine the	joblib parallel backend base	0.058824
transforms features by scaling each feature to a	min max scaler	0.083333
of x (as bigger is better i	decision function x	0.018868
for indices increasingly apart the	joblib verbosity filter index	0.055556
compute	manifold	0.100000
graph matrix for label spreading	semi_supervised label spreading build	0.250000
makes sure centering is not enabled	preprocessing robust scaler check array x copy	0.333333
for the voting classifier	ensemble voting classifier set params	0.037037
the two clusterings matching 1d integer arrays	clusterings labels_true labels_pred	0.500000
parameters and evaluates the reduced likelihood function	process reduced likelihood function	0.047619
to fit an estimator within a job	ensemble parallel fit estimator estimator x	0.333333
boosted	ensemble ada boost	1.000000
read the array corresponding to this wrapper	externals joblib numpy array wrapper read	0.500000
cross-validated estimates for each input data	cross val predict estimator x y cv	0.071429
the median and	y	0.002674
dim	dim	1.000000
k-neighbors	kneighbors mixin kneighbors	0.100000
returns posterior probabilities of	cv predict proba	0.034483
lfw pairs dataset this dataset is a collection	lfw pairs	0.018868
factorize density check according to li	core check density density n_features	0.166667
of the reduced likelihood	reduced likelihood	0.100000
fit ridge regression model parameters	linear_model ridge fit	1.000000
california housing dataset	datasets fetch california housing	0.083333
samples can be different from the	core calibrated classifier	0.083333
by a random projection p only changes	core johnson lindenstrauss min dim n_samples eps	0.142857
quantiles to be used for scaling	preprocessing robust scaler fit x	1.000000
compute the grid of alpha values for	linear_model alpha grid x	0.166667
multiple files in svmlight format	svmlight files files	0.200000
clustering for	clustering x	0.142857
count and smooth feature	nb count x y	0.250000
back the data to the original representation parameters	standard scaler inverse transform	0.066667
indices to split data into training and test	kfold split x y groups	0.200000
long type introduces an 'l' suffix	utils shape repr	0.013699
two non-negative matrices w	x w	0.083333
a name	func name	0.047619
covariance	cov x	0.500000
fit the hierarchical	feature agglomeration fit x	1.000000
cholesky decomposition	cholesky omp	1.000000
median absolute error regression loss	median absolute error	0.166667
classification	calibrated classifier	0.083333
the model to the training set x	predict x	0.011765
exception types to	backend base	0.032258
an arbitrary python object into one file	dump value filename	0.083333
place	y code verbose	0.333333
the grid of alpha values for	linear_model alpha grid x	0.166667
print verbose	mixture print verbose	1.000000
the median and	fit x y	0.005988
of test vectors x	x	0.001692
equal to the average path length of	ensemble average path length	0.090909
not found and	search	0.019231
scale to unit variance	preprocessing scale x	0.090909
and hide warnings	ignore warnings	0.166667
of exception	parallel backend	0.030303
posterior log probability	core bernoulli nb joint log likelihood	0.083333
sparse and dense inputs	x y sample_weight random_state	0.166667
the gradient of loss	loss grad	0.250000
from	joblib memorized	0.015625
prediction scores note this	metrics roc	0.040000
function used to build a	ensemble parallel build	0.047619
calculate true and false positives per	clf curve y_true y_score pos_label sample_weight	0.250000
with the generative model	pca	0.047619
get the values used	get	0.012048
coverage error measure compute how far we need	coverage error y_true y_score sample_weight	0.166667
from source to	source cutoff	0.200000
sign of vectors for reproducibility flips	utils deterministic vector	0.076923
of points on the grid	model_selection parameter grid len	0.333333
get a signature object for the passed	externals signature obj	0.200000
loading for the lfw people dataset this operation	datasets fetch lfw people	0.040000
the shrunk ledoit-wolf	ledoit wolf	0.111111
the number of splitting iterations in	leave one out get n splits x	0.111111
inplace row scaling of a csr	inplace row scale	0.142857
norm vector length	normalize x norm	1.000000
data x which should contain	x y	0.002155
mean update and	incremental mean	0.166667
calculate true and false positives per binary classification	metrics binary clf curve y_true y_score	0.090909
for the given param_grid	search cv get param iterator	0.166667
as	orthogonal matching pursuit	0.250000
fit the gradient boosting model	ensemble base gradient boosting fit	1.000000
types to	parallel backend base get	0.066667
the median	get median	0.166667
absolute error regression loss read more in the	absolute error y_true y_pred	0.142857
handle the callable case for pairwise_{distances	pairwise callable x y metric	0.083333
fit_binary	linear_model prepare fit binary est y i	1.000000
deviance loss function for multi-class classification	deviance	0.062500
arguments	externals joblib memorized func get	0.125000
solve the isotonic regression model :	core isotonic regression	0.055556
and component wise scale to unit variance	preprocessing scale	0.090909
position	binary zlib	0.333333
mono and multi-outputs	y eps n_alphas	0.250000
spherical	spherical nk xk sk	1.000000
fit the model to	estimator fit x y sample_weight	0.200000
k x	gaussian_process exponentiation call x	0.200000
return the kernel	kernel call	0.333333
in parallel n_jobs is the is the	n_jobs	0.023256
data in the given	data compress	0.100000
cache	externals joblib memorized	0.027397
generate cross-validated	predict estimator x y cv	0.071429
the kernel k x	matern call x	0.200000
hash to identify uniquely python	joblib hash obj hash_name coerce_mmap	0.250000
given format	format	0.166667
of the leaf	apply x	0.166667
total log probability under the	neighbors kernel density score	0.333333
any axis center to the mean and component	axis	0.014085
validation and conversion	directed dtype csr_output	0.166667
data point	val predict estimator x	0.045455
returns the number of splitting iterations in the	one out get n splits x y	0.111111
training set x	fit x	0.025641
true if the given estimator	estimator	0.014706
evaluate the accuracy of a classification	y_true y_pred labels sample_weight	0.125000
whether the kernel	stationary kernel	0.333333
wise squaring of array-likes and sparse matrices	utils safe sqr x copy	0.125000
to compute log probabilities	predict log proba	0.029412
return a tolerance which	cluster tolerance x	0.058824
precision matrix	precision	0.033333
the estimator with the best found	base search cv	0.052632
two sets of biclusters	metrics cluster consensus	0.250000
oracle approximating shrinkage covariance	covariance oas fit	0.083333
apply decision	analysis decision	1.000000
the curve auc from prediction	auc	0.020408
timestamp when pickling to avoid the	externals joblib memory reduce	0.030303
of exception types to	base get	0.066667
find the least-squares solution to a large	a	0.018182
recall the recall is the ratio	recall	0.028571
modified weiszfeld step	modified weiszfeld step x x_old	1.000000
median absolute error regression loss read	median absolute error y_true y_pred	0.166667
inplace row scaling of a	inplace row scale x	0.142857
finds the neighbors within a given radius	lshforest radius neighbors x radius return_distance	0.500000
base class for decision	base decision	1.000000
on the training set according to the	fit predict	0.055556
fit the	output estimator fit x y sample_weight	0.200000
integer indices corresponding to test sets	test indices	0.333333
covariance m	covar	0.153846
online	partial fit	0.222222
first and last element of	first and last element arr	0.200000
the shortest path length from source to all	source shortest path length graph source cutoff	0.111111
fit a single	build trees	0.142857
arbitrary python object	value filename	0.083333
fit the	fit x	0.044872
initialize the model parameters of the	mixture base mixture initialize	0.333333
sparse random matrix	random choice csc	0.166667
minimum distances between one point and a set	pairwise distances argmin x y axis	0.333333
linear embedding analysis	linear embedding x n_neighbors n_components reg	0.200000
get feature names from all transformers	feature union get feature names	1.000000
that for c in (l1_min_c infinity)	c x y loss fit_intercept	0.030303
the significance of a cross-validated score	score estimator x y cv	0.083333
of the data	x	0.003384
a which this function is called to issue	externals joblib	0.004762
model parameters with	fit	0.003257
a func to	externals joblib sequential backend apply async func	0.250000
reproducibility flips the sign of	deterministic vector sign	0.066667
predict the target	regressor predict	0.400000
predict new data by linear interpolation	sigmoid calibration predict t	1.000000
k x y and optionally its gradient	x y eval_gradient	0.210526
the median of	median	0.066667
from	fit x y sample_weight	0.020000
uncorrelated design	uncorrelated n_samples n_features	1.000000
predict_proba on the estimator with the best found	core base search cv predict proba	0.076923
x and y read more in	x y	0.002155
and gradient	and grad w	1.000000
exception	parallel	0.019231
fit with all	cv fit x y	0.250000
score for a fit	fit rfe	0.166667
evaluate the significance of a cross-validated	cv	0.009009
prediction scores note this implementation	metrics roc	0.040000
number of splitting iterations in the	model_selection base kfold get n splits x y	0.111111
lfw pairs dataset this operation is meant to	datasets fetch lfw pairs index_file_path data_folder_path slice_ color	0.333333
modified weiszfeld step	linear_model modified weiszfeld step	1.000000
returns the number of splitting iterations in the	leave one group out get n splits x	0.111111
logic estimators that implement the partial_fit api need	utils check partial	0.038462
mean shift	mean shift x bandwidth	0.500000
csr	csr	1.000000
exception	joblib	0.007299
the pairwise matrix in n_jobs	parallel pairwise x y func n_jobs	0.111111
predict using the gaussian process regression	gaussian_process gaussian process regressor	0.058824
array-like or scipy sparse matrix	binarize	0.045455
binary classification	binary	0.031250
return the feature importances	base decision tree feature importances	0.333333
dictionary factor in	dict dictionary y	0.111111
a byte string to the	externals	0.005747
for c in (l1_min_c infinity) the model	c	0.022222
columns of a matrix	columns	0.111111
compute the recall the recall	recall	0.028571
position of the points in	mds fit	0.066667
the position of the	mds fit	0.066667
recall is the ratio tp /	recall	0.028571
lfw pairs dataset this operation is	datasets fetch lfw pairs	0.018868
generate a random multilabel classification	datasets make multilabel classification n_samples n_features n_classes	0.500000
data	cross val predict	0.045455
or regression value for	check_input	0.100000
faces in the wild lfw pairs dataset this	lfw pairs subset	0.035714
elastic net path with coordinate descent the	path	0.025641
in the :ref user guide <sparse_inverse_covariance>	emp_cov alpha cov_init mode	0.200000
distribution p(v|h)	visibles h rng	0.500000
lower bound on model	lower bound	0.071429
or too common features	feature_extraction count vectorizer limit features x	0.250000
median absolute error regression loss read more	metrics median absolute error y_true y_pred	0.166667
pickle-protocol - set state of the estimator	core isotonic regression setstate state	0.250000
axis	x axis	0.030769
iboost	iboost	0.714286
linear model	base sgdregressor	0.200000
of regularization	path x y pos_class cs	0.166667
representation	repr	0.012500
the minimum covariance determinant matrix	covariance fast mcd x support_fraction	0.250000
best found parameters	core base search cv	0.066667
depth	func check previous func code	0.333333
generate a random regression problem	datasets make regression n_samples n_features n_informative n_targets	1.000000
for each	cross	0.037037
generate	n_samples n_components n_features n_nonzero_coefs	0.500000
values for a given dataset	train	0.117647
model to the data x which should	x y	0.002155
linear model with stochastic gradient descent	base sgdregressor partial	0.333333
which are going to run	externals joblib multiprocessing backend effective	0.250000
that array is 2d square and symmetric	utils check symmetric array tol raise_warning raise_exception	0.500000
models for feature selection	model	0.058824
p=none) generates a random sample from	size replace	0.125000
the moore-penrose pseudo-inverse of a hermetian matrix	utils pinvh a cond rcond lower	0.200000
all methods a parallelbackend must implement	backend	0.016949
a list of feature names ordered by their	feature_extraction dict vectorizer get feature names	0.142857
sample from a given 1-d array	utils choice a size	0.250000
are selected returns	feature_selection	0.066667
with the given arguments	externals joblib memorized func	0.013158
the lfw pairs dataset this operation	lfw pairs	0.018868
what extent the local structure is retained	manifold trustworthiness x x_embedded n_neighbors precomputed	0.200000
the hash depending	externals	0.011494
of approximate nearest neighbors	neighbors lshforest kneighbors x	0.500000
scale	preprocessing max abs scaler transform x y	1.000000
a cv in	core check cv cv x y	0.031250
indices in sorted array of	indices tree bin_x left_mask right_mask	0.166667
arbitrary python object into one	joblib dump value filename	0.083333
and scale	transform x y	0.031250
gmmbase	gmmbase	0.312500
generate a	n_samples n_features	0.250000
covariance estimator read	covariance	0.014493
tags	tags	1.000000
of a	finder a	0.500000
perform dbscan clustering from vector array	cluster dbscan x eps min_samples metric	0.200000
the array corresponding	numpy array	0.500000
the density model on the data	neighbors kernel density	0.090909
write	wrapper write	1.000000
make sure that an estimator implements the	check estimator estimator	0.142857
generate	core cross val predict estimator x y	0.045455
diagonal of the	diag	0.156250
store the timestamp when pickling to	memorized func reduce	0.050000
list	externals joblib parallel backend base	0.034483
note this implementation is restricted to the	roc	0.033333
retrieve the leaves of the	get leaves	0.111111
of edges for a	feature_extraction make edges	0.066667
step in pipeline after transforms	core pipeline	0.076923
message on the end of	msg init end ll	0.333333
the weighted graph of neighbors for	mixin radius neighbors graph	0.066667
position of the	mds fit x y init	0.066667
fit the	linear svr fit	0.333333
pickle-protocol - return state of	core isotonic regression getstate	0.250000
matrix to evaluate the accuracy of a classification	y_true y_pred labels	0.125000
for each input data	core cross val predict estimator	0.045455
step for full	full	0.055556
perform mean shift	mean shift x	0.500000
by a random projection p only changes the	core johnson lindenstrauss min dim n_samples eps	0.142857
a full lars path parameters	linear_model omp path	0.100000
problem with sparse uncorrelated design	sparse uncorrelated n_samples n_features random_state	0.166667
a gaussian distribution	gaussian mean covar covariance_type	0.500000
evaluate a score by cross-validation read more in	model_selection cross val score estimator x y	0.166667
parameters theta as the maximizer of	process arg max	0.047619
initial centroids	cluster init centroids x	0.166667
of x according	x	0.001692
tests involving both blas calls and multiprocessing	utils if safe multiprocessing with blas func	0.500000
the score of the underlying estimator	score	0.010101
the covariance m step	mixture covar	0.125000
position of the points	mds fit x y init	0.066667
split data into training and test	model_selection cviterable wrapper split x y groups	0.200000
labeled faces in the wild lfw pairs dataset	fetch lfw pairs subset	0.035714
function func	func	0.011364
people dataset this operation is meant to	people data_folder_path slice_ color resize	0.333333
the long type introduces an 'l' suffix when	utils	0.009709
linear model with passive aggressive algorithm	linear_model passive aggressive regressor partial	1.000000
the binary classification task	metrics precision recall curve y_true	0.142857
lower bound for	bound	0.083333
the long type	utils shape repr	0.013699
fit estimator and predict	fit and predict estimator x	1.000000
setting the parameters for the voting classifier valid	voting classifier set params	0.037037
sure centering is not enabled	preprocessing robust scaler check array	0.250000
the svmlight / libsvm format into sparse	svmlight file f n_features	0.066667
avoid	joblib memorized	0.015625
returns the number of splitting iterations in	cross validator get n splits	0.125000
for the	get	0.012048
perform dbscan clustering from	cluster dbscan x eps min_samples	0.200000
word	word	1.000000
opening the right fileobject from a filename	externals joblib read fileobject fileobj filename mmap_mode	0.250000
a func to be run	externals joblib sequential backend apply async func	0.250000
a random	n_samples eps	0.125000
implement a single boost	base weight boosting boost iboost x y	1.000000
compute true and predicted probabilities for a calibration	core calibration	0.125000
estimate the precisions parameters of the	bayesian gaussian mixture estimate precisions nk xk sk	0.166667
shortest path length from source to all reachable	single source shortest path length graph source	0.111111
log probability for full covariance	log multivariate normal density full x	0.333333
patches of any n-dimensional array in	patches	0.055556
multiple files in svmlight format	svmlight files files n_features	0.200000
the hash depending from	joblib memorized	0.015625
data under each gaussian in	mixture gmmbase	0.034483
reconstruct the	read unpickler	0.200000
cluster is	cluster cfsubcluster	0.250000
memmap instance to reopen on same	externals joblib reduce memmap	0.142857
initialize the model parameters	mixture base mixture initialize parameters	1.000000
perform dbscan clustering from	cluster dbscan	0.125000
kernel k x	rbf call x	0.200000
estimator with the best	cv predict	0.083333
general function given points on	y reorder	0.111111
return the kernel k x y and	product call x y	0.200000
compute prediction of init	boosting init decision function x	0.142857
load the kddcup99 dataset downloading it if	brute kddcup99 subset data_home download_if_missing random_state	0.111111
objects and 'return_annotation'	init parameters return_annotation __validate_parameters__	0.125000
using x y	x y sample_weight	0.012987
computes the weighted graph of neighbors for points	neighbors graph	0.066667
x format check x format	dirichlet allocation check	0.062500
submatrix corresponding to bicluster i	bicluster mixin get submatrix i data	0.333333
return whether the file was opened for writing	externals joblib binary zlib file writable	0.250000
generate cross-validated estimates for	estimator x y cv	0.050000
bound for c such that for c	c x y loss fit_intercept	0.030303
the	externals joblib parallel backend base	0.034483
code	code	1.000000
huber loss and the	huber loss and	0.166667
of csgraph	csgraph directed	0.250000
computes the nonzero componentwise l1 cross-distances between the	gaussian_process l1 cross distances	0.111111
absolute value to be used for	abs	0.083333
the huber loss and the gradient	huber loss and gradient w	0.333333
matrix	k k_skip	1.000000
boosted classifier/regressor from the	ensemble base weight boosting	0.333333
the optimal batch	joblib auto batching mixin compute batch	0.333333
init	base gradient boosting init decision function	0.142857
mean and component wise scale to unit variance	preprocessing scale x	0.090909
computes the	empirical	0.111111
compute the decision function of the given	decision function x	0.018868
folders to make cache size fit	memory reduce size	0.083333
input data point	cross val predict estimator	0.045455
compute directory associated	to dir cachedir func argument_hash	0.250000
classification this function returns posterior probabilities of classification	calibrated classifier cv predict proba x	0.200000
negative value	negative	0.090909
for	get	0.012048
indices in sorted array	find matching indices tree bin_x left_mask right_mask	0.166667
> 0 0 else -1 0	ensemble least absolute error negative gradient	1.000000
the median	utils get median	0.166667
fit the model using x as training data	manifold tsne fit x skip_num_points	0.500000
compute the score of	core score	0.166667
matrix whose range approximates the range of a	randomized range finder a	0.166667
the graphlasso covariance model to	covariance graph lasso cv fit	0.111111
type introduces an 'l' suffix	utils shape	0.013699
lad	ensemble least absolute error update	0.500000
of vectors for reproducibility flips the	utils deterministic vector	0.076923
measure the similarity of two clusterings of a	metrics cluster fowlkes mallows score labels_true labels_pred sparse	0.333333
evaluate decision function output for x relative to	metrics threshold scorer call clf x y	0.058824
precision-recall pairs for different probability thresholds note	probas_pred pos_label sample_weight	0.066667
x to	x	0.003384
to avoid the hash depending from	func	0.011364
build a batch of estimators within a job	build estimators	0.166667
the lrd of	distances_x neighbors_indices	0.047619
tp + fp where tp is	score y_true y_pred labels pos_label	0.027778
by scaling each feature to a given	preprocessing minmax scale	0.142857
feature name -> indices mappings	feature_extraction dict vectorizer fit x y	0.250000
input checker utility for building a cv in	check cv cv x y	0.031250
of the logistic	logistic	0.047619
warning class used to notify the user	warning	0.083333
10	10	1.000000
using	utils	0.009709
compute log probabilities within a job	parallel predict log proba estimators estimators_features	0.250000
exception types	backend base get	0.066667
arrays	arrays out	1.000000
predict on the estimator with the best found	search cv predict x	1.000000
used for scaling	scaler	0.031250
low rank matrix with bell-shaped singular values most	make low rank matrix	0.083333
create subset of dataset and	x y	0.002155
global clustering for	global clustering x	0.142857
for c such that for c in (l1_min_c	c	0.022222
estimate the spherical wishart distribution parameters	mixture bayesian gaussian mixture estimate wishart spherical	0.333333
the covariance	match covariance	0.250000
fit linear model with	fit x y	0.017964
generate cross-validated	y cv	0.050000
checker utility for building a cv	cv cv x	0.031250
function the absolute error of the kl divergence	kl divergence error	0.100000
get number of	get n	0.500000
x format check x format and	latent dirichlet allocation check	0.062500
cross-validated estimates for	core cross val predict estimator x y cv	0.071429
and y	y	0.032086
classifier valid parameter	classifier set	0.125000
compute class priors from multioutput-multiclass target data parameters	utils class distribution y sample_weight	1.000000
logistic loss	logistic loss w x y	0.500000
dispatch	joblib parallel dispatch one	0.250000
reduce x to	mixin transform x	0.500000
tokens	word ngrams tokens	1.000000
and inertia using	inertia precompute dense x x_squared_norms centers distances	0.250000
iterate	iterate	1.000000
build a batch of estimators within	ensemble parallel build estimators n_estimators ensemble x y	0.166667
label	label	0.454545
distortion introduced by a random	n_samples eps	0.125000
whether the file was opened for writing	binary zlib file writable	0.250000
of a gaussian data set with self	x_test y	0.142857
false positives per	clf curve	0.250000
to avoid the	joblib memory	0.016949
don't store the timestamp when pickling to	joblib memorized func reduce	0.050000
normalized probabilities from unnormalized log-probabilites	mixture log normalize v axis	1.000000
returns whether the kernel is stationary	gaussian_process pairwise kernel is stationary	1.000000
the log-likelihood of a gaussian data set with	score	0.010101
of the log of	log	0.018868
number of splitting iterations in the cross-validator	one out get n splits x	0.111111
the parallel execution only a	externals joblib parallel	0.014085
weighted graph of neighbors for points in	neighbors mixin radius neighbors graph	0.066667
of x and y is float32 then dtype	dtype x y	0.500000
the bound	mixture dpgmmbase bound	0.166667
for a fit	fit rfe estimator x	0.166667
handle the callable case for pairwise_{distances kernels}	metrics pairwise callable x y	0.083333
call wrapped function cache	joblib memorized func call	0.200000
process or thread pool	joblib multiprocessing backend	0.052632
c in (l1_min_c infinity) the	c x	0.030303
relu	relu	1.000000
x according to the fitted model	x	0.001692
compute the beta-divergence of	beta	0.090909
voting classifier	voting classifier	0.035714
the means	means	0.076923
the generative model	decomposition base pca get	0.071429
terminal	update terminal	0.142857
finds the k-neighbors of a point	kneighbors mixin kneighbors x n_neighbors return_distance	0.250000
compute the l1 distances between the vectors in	paired manhattan distances	0.083333
of points on the	len	0.038462
fit the hierarchical clustering on the data	cluster agglomerative clustering fit x y	0.250000
log-likelihood of	covariance score	0.071429
check if vocabulary is empty or missing not	check vocabulary	0.250000
class weights for unbalanced datasets	utils compute class weight class_weight classes	0.500000
private function used to build a	build	0.037037
h in multiplicative update	multiplicative update h x w h	0.250000
predict if a	predict x	0.011765
mutual information	feature_selection mutual info regression x	0.500000
python object	value filename	0.083333
hence	patch extractor fit x	1.000000
check if vocabulary is empty or missing not	vectorizer mixin check vocabulary	0.250000
of the function called with the given arguments	memorized func	0.016949
class	ensemble gradient boosting classifier	0.800000
construct a pipeline from the given estimators	core make pipeline	0.250000
format and make sure no	non neg array	0.250000
aggressive classifier read more in the :ref	aggressive classifier	0.166667
get parameters for this	get params deep	0.200000
number of splitting iterations in	cross validator get n splits	0.125000
local outlier factor	neighbors local outlier factor decision	0.125000
of x from	x z reg	0.066667
line_search_wolfe2 if suitable step length is not	wolfe12 f fprime xk pk	0.028571
back	standard scaler inverse transform x copy	0.066667
"news" format strip lines beginning with the	strip newsgroup	0.090909
predict class at each stage for	boosting classifier staged predict	0.500000
breakdown point	breakdown point	0.333333
index	index	1.000000
update	update	0.607143
avoid the hash depending from it	memorized func	0.016949
compute the rectified linear unit function inplace	neural_network relu	1.000000
an 'l'	shape repr	0.013699
absolute sizes of training subsets and	sizes	0.050000
remove cache	externals joblib memory reduce	0.030303
the vocabulary dictionary and return term-document matrix	vectorizer fit transform raw_documents y	0.100000
array-like or scipy	binarize	0.045455
dtype	dtype	0.375000
parameters for the voting classifier valid parameter	ensemble voting classifier	0.031250
in-place	swap row x m	1.000000
train test	core	0.015385
pls	pls	1.000000
model with passive aggressive algorithm	passive aggressive regressor	0.125000
path length from source	path length graph source	0.200000
fit ridge regression	ridge fit	1.000000
computation	output_dir func_name timestamp metadata	1.000000
string to the file	joblib binary zlib file	0.066667
return the shortest path length from source to	utils single source shortest path length graph source	0.111111
return the path of the	get	0.012048
sample weights by class for unbalanced datasets	sample weight class_weight y indices	0.500000
compute	compute log det	1.000000
a random projection p only changes the	johnson lindenstrauss min dim n_samples	0.142857
two rows of a csr matrix in-place	utils inplace swap row csr x m	0.250000
of the cholesky decomposition of	det cholesky	0.166667
for c	c x	0.030303
t-sne objective function the absolute error of the	error	0.020000
the boolean mask x	mask x	0.333333
paired distances between x and y	metrics paired distances x y	0.500000
the number of splitting iterations in the	one out get n splits x y	0.111111
root_path	root_path	1.000000
sure that an estimator implements the necessary	core check estimator estimator	0.142857
for the one-vs-one multi class libsvm	one vs one	0.050000
lrd of a sample is the	distances_x neighbors_indices	0.047619
check values of the basic parameters	mixture check initial parameters	1.000000
private function used to compute decisions within	function estimators estimators_features	0.500000
semi_supervised	semi_supervised	1.000000
samples in x	x	0.005076
used to fit a single tree	trees tree forest	0.142857
the weighted graph of neighbors for	neighbors radius neighbors mixin radius neighbors graph	0.066667
logarithm of the normalization constant	logz v s dets	0.200000
structure for biclustering	shape n_clusters noise minval	0.333333
filters the given args and	joblib filter args func ignore_lst args	1.000000
input validation for standard estimators	utils check x y x y accept_sparse dtype	0.250000
split data into training and test set	model_selection time series split split x y groups	0.200000
the model is guaranteed not to be empty	svm l1 min	0.333333
types	joblib parallel backend base get	0.066667
initialization of the mixture parameters	mixture bayesian gaussian mixture initialize	1.000000
thresholding of array-like or scipy	preprocessing binarize x threshold	0.083333
a func to be run	externals joblib parallel backend base apply async func	0.250000
compute the initial centroids parameters	cluster init centroids x k	0.166667
boolean thresholding of array-like or scipy sparse matrix	binarize x threshold copy	0.083333
point	estimator x y	0.038462
list of exception types	joblib parallel	0.028571
fun	fun	1.000000
set the parameters of this estimator	core base estimator set params	1.000000
rows of u such	u	0.032258
convert a collection	vectorizer	0.044444
returns the number of splitting iterations in	out get n splits x y groups	0.111111
c	c	0.266667
a cross-validated	x y cv	0.050000
estimate the precisions parameters of	bayesian gaussian mixture estimate precisions nk xk	0.166667
x and returns the transformed	x y w h	0.500000
the number of splitting iterations in the cross-validator	kfold get n splits	0.111111
shrunk ledoit-wolf	ledoit wolf x assume_centered	0.250000
path length from source to	path length graph source cutoff	0.200000
of exception types	parallel backend base get	0.066667
loader for the california housing	fetch california housing data_home download_if_missing	0.250000
determinant	det fit x	0.333333
jobs that can actually run	jobs	0.111111
in multiplicative update	multiplicative update h	0.500000
the best found	search cv	0.090909
log probabilities	parallel predict log proba	0.058824
estimator with the best found parameters	base search cv	0.052632
blup parameters and evaluates the reduced likelihood	gaussian_process gaussian process reduced likelihood	0.142857
of a classification	y_true y_pred	0.037037
expression of the dual gap convergence criterion the	dual gap emp_cov precision_	0.071429
for reproducibility flips the sign of	utils deterministic vector sign flip	0.066667
decision path	decision path x	0.333333
for each iteration	ensemble gradient boosting classifier staged	0.333333
incrementally fit the model to data	core multi output regressor partial fit x y	0.200000
learn vocabulary and idf return	feature_extraction tfidf vectorizer fit transform raw_documents y	0.250000
update and a youngs	utils incremental	0.166667
lad updates terminal	least absolute error update terminal region	0.200000
w h whose product approximates the non-	x w h	0.035714
parallel n_jobs is the is the	n_jobs	0.023256
closest cluster each sample	cluster	0.021277
median absolute error regression loss read	metrics median absolute error y_true	0.166667
and compute prediction of init	base gradient boosting init decision function	0.142857
sigmoid	core sigmoid	1.000000
initialization of the mixture parameters	bayesian gaussian mixture initialize x resp	1.000000
estimates for each input data	val predict estimator	0.045455
nicely formatted statement displaying the function call	joblib format call func args kwargs object_name	0.333333
whom	whom	0.833333
in the raw documents	feature_extraction count vectorizer fit	0.125000
this operation is meant	index_file_path data_folder_path slice_ color	0.033333
compute incremental mean and	mean	0.035714
set the diagonal of the laplacian matrix and	manifold set diag laplacian	0.333333
build a batch of estimators within	parallel build estimators	0.166667
types to	base get	0.066667
array-like or scipy sparse	binarize	0.045455
x according to	x	0.003384
for parallel processing this method is	externals joblib parallel	0.014085
backed arrays	backed a m	1.000000
to split data into	shuffle split split	0.250000
the median across axis 0	median axis 0 x	0.333333
the l1 distances between the vectors	metrics manhattan distances	0.083333
em update for	latent dirichlet allocation em step x	0.500000
used to partition	ensemble partition	0.200000
models for feature	model	0.058824
max	max	0.428571
of the logistic	inplace logistic	0.333333
getter for	get	0.012048
standardize	with_mean with_std	0.200000
in an unfitted estimator	estimators unfitted name estimator	0.142857
factorization nmf find two non-negative matrices	non negative factorization	0.043478
fit the	linear svc fit x y	0.333333
clear the state of the gradient boosting model	ensemble base gradient boosting clear state	1.000000
for factorizing common classes param logic estimators that	utils check partial fit first call clf classes	0.333333
pipeline after transforms	pipeline fit predict	0.166667
eval function func with	eval func	0.166667
x	sine squared call x	1.000000
the sample	sample	0.032258
curve auc from prediction	auc	0.020408
convert coefficient matrix	mixin sparsify	0.500000
two continuous	cc x y n_neighbors	1.000000
estimator with the best found parameters	model_selection base search cv predict proba	0.076923
transform function to portion of selected features	preprocessing transform selected x transform selected	0.333333
a process or thread pool and	multiprocessing backend	0.038462
precisions parameters of the	precisions nk xk sk	0.166667
metrics read more in	metrics	0.043478
data precision matrix with	base pca get precision	0.066667
with sparse uncorrelated design	make sparse uncorrelated n_samples	0.166667
of the data home cache	datasets clear data home	0.076923
shrunk on the diagonal read more in the	shrunk	0.043478
sure centering is not enabled for	preprocessing robust scaler check array	0.250000
call with the given	joblib format call	0.200000
check x format check x format and	decomposition latent dirichlet allocation check	0.062500
of init	boosting init decision	0.142857
non-negative matrices w h whose product approximates the	w h	0.031250
decision	ada boost classifier decision	0.333333
generate cross-validated estimates for each input	y cv	0.050000
all the content of the data home cache	data home data_home	0.055556
check x	check	0.017857
x to	mixin transform x	0.500000
performs a 1-way anova	feature_selection f oneway	0.333333
score	model_selection score	0.166667
parameters with the em	mixture gmmbase	0.034483
building a cv	core check cv cv x y classifier	0.031250
with passive aggressive algorithm	passive aggressive classifier partial	1.000000
for each input data point	core cross val predict	0.045455
file object	file	0.107143
labels back to	preprocessing label binarizer inverse	0.166667
under the curve auc using the trapezoidal rule	auc x y	0.040000
nu support vector regression	nu svr	1.000000
coverage error measure compute how far we need	coverage error y_true y_score	0.166667
number of splitting iterations in the cross-validator parameters	one out get n splits x y	0.111111
to split data into	shuffle split split x	0.250000
estimate sample weights by class for unbalanced datasets	compute sample weight class_weight	0.500000
get the weights from	neighbors get	0.125000
metrics should use this function	metrics	0.043478
callable case for pairwise_{distances kernels}	metrics pairwise callable x	0.083333
func	joblib parallel backend base apply async func	0.250000
matrix factorization nmf find	negative factorization	0.043478
kernel	stationary kernel mixin	0.333333
the reduced likelihood function for the given	gaussian_process gaussian process reduced likelihood function	0.047619
classifier	classifier set	0.125000
tolerance which	cluster tolerance	0.058824
encode the data as a sparse	sparse	0.025000
estimator	base estimator	1.000000
estimates for each	core cross val predict	0.045455
pairwise matrix in n_jobs even slices	parallel pairwise x y func n_jobs	0.111111
call transform on	transform x	0.016949
the generative	decomposition base pca get	0.071429
the number of splitting iterations in the cross-validator	leave pgroups out get n splits	0.111111
to avoid the hash	joblib	0.014599
building a cv in a user friendly way	cv cv x y classifier	0.031250
sparse uncorrelated design	sparse uncorrelated n_samples	0.166667
compute	x	0.001692
return the current	tell	0.125000
and dispatch	externals joblib parallel dispatch	0.250000
to avoid the hash depending	joblib memorized	0.015625
arguments of a function	delayed function	0.200000
the isotonic regression model : min	isotonic regression	0.055556
median absolute error regression	metrics median absolute error y_true	0.166667
kernel k x y and	x y	0.008621
loss and	loss and	0.666667
utility for building a cv in a	cv cv x y classifier	0.031250
back the data to	inverse transform x copy	0.066667
information for reducing the	items root_path	0.066667
of neighbors for points	radius neighbors	0.086957
implement a single boost	boosting boost iboost x y	1.000000
fit a multi-class classifier by combining binary classifiers	base sgdclassifier fit	0.076923
lars using bic or aic	lars ic	0.250000
a minimum covariance determinant with the fastmcd algorithm	covariance min cov det	0.500000
finds seeds	seeds x	0.250000
update parameters with given gradients parameters	base optimizer update params grads	1.000000
the huber loss and the gradient	linear_model huber loss and gradient w	0.333333
label samples by quantile this classification dataset is	datasets make	0.015625
the transformation	transform xred	0.500000
and then	y	0.002674
apply clustering to a projection to the	cluster spectral clustering affinity n_clusters	0.166667
the hash depending from it	memorized func	0.016949
back	binarizer inverse	0.166667
vocabulary dictionary and return term-document	fit transform raw_documents y	0.100000
reconstruct	wrapper read unpickler	0.333333
returns posterior probabilities of classification	core calibrated classifier cv predict proba x	0.200000
with the best found	core base search cv predict proba	0.076923
the derivative	derivative z	0.333333
func to	apply async func	0.250000
bound for c such that for c in	c x y loss fit_intercept	0.030303
true and false positives per binary classification threshold	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
representing a fully connected graph	graph	0.021277
compute the l1	metrics paired manhattan	0.333333
evaluates the reduced likelihood function for the	gaussian process reduced likelihood function	0.047619
used in hastie et al	make hastie 10 2 n_samples random_state	0.166667
return a tolerance which	cluster tolerance	0.058824
kernel k	gaussian_process exp	1.000000
of the leaf	base decision tree apply x	0.166667
number of splitting iterations in	get n splits x y groups	0.111111
perform mean shift	mean shift	0.125000
shrunk on the diagonal read more in	covariance shrunk	0.066667
back the data to the	preprocessing standard scaler inverse transform	0.066667
the initial centroids	cluster init centroids x	0.166667
to unit norm vector length	x norm axis copy	0.200000
the binary classification task	y_true y_score pos_label	0.066667
that	utils check	0.071429
for each input	predict estimator x	0.045455
convert a sparse matrix to a given format	utils ensure sparse format spmatrix accept_sparse	1.000000
a binary metric for multilabel classification parameters	binary score binary_metric y_true	0.500000
internal unpickling function	externals joblib unpickle fobj filename mmap_mode	1.000000
unnormalized posterior log probability of	base nb joint log likelihood	0.166667
models computed by 'path' parameters	linear_model path residuals	0.250000
k x	exponentiation call x	0.200000
the least-squares solution to a large sparse	a	0.018182
a sparse random projection matrix parameters	core base random projection fit	0.333333
loading for the lfw people dataset this operation	lfw people	0.040000
call transform on the estimator with the best	cv transform	0.500000
function used to build a	build	0.037037
the number of splitting iterations in the	one group out get n splits x	0.111111
after the other and	y	0.002674
returns the number of splitting iterations in	leave pgroups out get n splits x y	0.111111
parameters for the voting classifier valid	voting classifier	0.035714
unnormalized posterior log probability	nb joint log likelihood	0.033333
data point	core cross val predict estimator x	0.045455
x	transform x y copy	0.142857
the search	search cv fit x	0.111111
inplace row scaling of a csr or csc	inplace row scale x scale	0.142857
x to	selector mixin transform x	0.500000
introduces an 'l' suffix when	utils shape repr	0.013699
values for x	x	0.001692
cache	memorized	0.015873
c in (l1_min_c infinity)	c x y loss	0.030303
pairwise matrix	parallel pairwise	0.166667
call predict on the	predict x	0.011765
locally linear embedding analysis	locally linear embedding x	0.071429
run fit on one set of	model_selection fit grid point x y	0.500000
called with the given arguments	externals joblib memorized func	0.013158
projection p only changes	johnson lindenstrauss min dim	0.500000
an affinity matrix for x using	x	0.001692
check values of the basic parameters	mixture check initial parameters x	1.000000
doc_topic_distr	doc_topic_distr sub_sampling	1.000000
with randomly drawn parameters	core randomized search cv	0.200000
w h whose product approximates	w h n_components	0.038462
finds indices in sorted array of	indices tree bin_x left_mask right_mask	0.166667
the graphlasso covariance	covariance graph lasso cv	0.111111
content of the data home cache	datasets clear data home data_home	0.076923
estimate the spherical wishart distribution	gaussian mixture estimate wishart spherical	0.333333
thresholding of array-like or scipy sparse matrix	binarize x	0.083333
locs	locs	1.000000
estimate the spherical wishart	mixture bayesian gaussian mixture estimate wishart spherical	0.333333
embedding analysis on	embedding	0.040000
partially fit underlying estimators should	one vs one classifier partial fit x y	0.166667
estimate the precisions parameters of the	bayesian gaussian mixture estimate precisions nk	0.166667
compute the l1 distances between the vectors in	metrics paired manhattan distances	0.083333
one group out cross-validator provides train/test indices	one group out	0.166667
detects the soft boundary	class svm fit	0.125000
the optimal batch size	auto batching mixin compute batch size	0.333333
a func	externals joblib parallel backend base apply async func	0.250000
input checker utility for building a cv in	cv cv	0.031250
squared logarithmic	squared	0.083333
convert string beta_loss to float	decomposition beta loss to float beta_loss	1.000000
to evaluate the accuracy of a classification	y_true y_pred labels	0.125000
a text report	report	0.047619
fit ridge regression	ridge fit x y sample_weight	1.000000
and y read more in the	y	0.002674
for updating terminal regions (=leaves)	update terminal region tree terminal_regions leaf	0.200000
for full	density full x	0.166667
the pairwise matrix in n_jobs even slices	pairwise x y func n_jobs	0.111111
base class for label propagation module	base label propagation	1.000000
descriptors of a memmap instance to reopen on	memmap a	0.050000
file descriptor for the underlying file	binary zlib file fileno	0.333333
fit	linear svr fit x y	0.333333
sample weights by class for unbalanced datasets	utils compute sample weight class_weight y	0.500000
of	joblib parallel backend	0.045455
with n_zeros additional zeros	n_zeros	0.111111
persist	compress protocol	0.333333
to the binary classification task	y_true y_score pos_label sample_weight	0.066667
two non-negative matrices w h whose product approximates	x w h n_components	0.038462
building a cv in a user	check cv cv x	0.031250
check initial parameters of the	base mixture check parameters	0.200000
on the estimator with the best found	core base search cv predict proba	0.076923
a fit	fit	0.003257
content of the data home	clear data home data_home	0.076923
check a precision vector	mixture check precision positivity precision	0.500000
precisions parameters of the precision distribution	precisions	0.066667
in multiplicative update	multiplicative update h x	0.500000
the right fileobject from	joblib read fileobject	0.100000
given arguments and	func call	0.047619
on the estimator with the best found	core base search cv	0.033333
determination regression score	r2 score y_true	0.125000
building a cv in a user	cv cv x	0.031250
returns posterior probabilities of classification	calibrated classifier cv predict proba x	0.200000
helper function to test error messages in	message function	1.000000
estimates for each input data point	estimator	0.014706
elasticnet model	elastic net	0.111111
estimators within a job	estimators n_estimators ensemble x	0.083333
sample from a	a size replace	0.142857
can actually run in parallel n_jobs is the	n_jobs	0.023256
a logistic regression model	linear_model logistic regression path x	0.333333
on x and	dbscan fit predict x y	0.333333
and validate 'train_sizes'	train_sizes n_max_training_samples	0.200000
estimates for each input data	core cross val predict estimator	0.045455
lasso path using lars algorithm [1] the optimization	linear_model lars path x y	0.100000
log-det of the cholesky decomposition of	det cholesky matrix_chol	0.500000
numpy array of a single sample	sample	0.032258
reconfigure the backend and return the number of	externals joblib parallel backend base configure	0.333333
minimum covariance determinant matrix	covariance fast mcd	0.250000
the trained model	base multilayer perceptron	0.142857
parallel processing	parallel	0.019231
generate	base	0.014286
w/ cross-validated	cv	0.009009
a cv	core check cv cv	0.031250
that for c in (l1_min_c infinity)	c	0.022222
and then the underlying	x y	0.002155
a single tree	trees tree	0.142857
type introduces an 'l' suffix when	shape	0.011765
binary classification used in hastie et al	datasets make hastie 10 2 n_samples random_state	0.166667
returns whether the kernel	gaussian_process kernel operator	1.000000
creates a biclustering for x	cluster base spectral fit x	1.000000
log-likelihood of a gaussian data set with self	covariance score x_test y	1.000000
parameters for the voting classifier valid parameter	voting classifier	0.035714
avoid	memorized func	0.016949
used when memory is inefficient	classes	0.025641
roc	roc	0.166667
using x	x	0.006768
perform dbscan clustering	cluster dbscan fit x y	1.000000
fit linear	linear_model base sgdclassifier fit x	0.333333
perform classification on test vectors x	core dummy classifier predict x	1.000000
isotonic regression model	core isotonic regression y	0.066667
non-negative matrix factorization	non negative factorization x	0.043478
the estimator with the best found	model_selection base search cv predict proba x	0.076923
estimate model parameters	fit x y do_prediction	0.166667
adjusted for chance	cluster adjusted	0.333333
cholesky decomposition	cholesky omp x y	1.000000
python object into one	dump value filename	0.083333
defines all methods a parallelbackend must implement	backend	0.016949
x (as bigger is better i	x	0.001692
fits the shrunk covariance model according	covariance shrunk covariance fit x	0.083333
the hash depending	joblib memorized func	0.014706
number of splitting iterations in the cross-validator	cross validator get n splits x y groups	0.125000
elastic net model with iterative fitting along	elastic net cv	0.333333
optimizer	optimizer	0.714286
the hierarchical	feature agglomeration	0.333333
the index of the leaf	apply	0.083333
a conditional property using the	iff has attr descriptor	0.083333
vectors	dummy	0.200000
breiman [2]	friedman3 n_samples	0.166667
estimates for each input	predict estimator x y	0.045455
y as training	y xy	0.333333
the submatrix corresponding to bicluster i	bicluster mixin get submatrix i	0.333333
the parameters for the voting classifier valid	ensemble voting classifier	0.031250
dataset this operation is meant to be	data_folder_path slice_ color resize	0.033333
trace of np dot x	decomposition trace dot x	1.000000
private function used to compute log probabilities	predict log proba	0.029412
a batch of estimators within a job	estimators n_estimators	0.083333
number of splitting iterations in the cross-validator parameters	leave one out get n splits x y	0.111111
a read	read	0.052632
decision function of the given	decision function x	0.018868
the laplacian matrix and convert	laplacian	0.034483
of the data onto	ridge_alpha	0.052632
a temporary folder if still existing	folder folder_path	0.250000
returns the number of splitting iterations in the	cviterable wrapper get n splits x y groups	0.111111
n_jobs is the is	n_jobs	0.023256
train estimator on training subsets incrementally and	core incremental fit estimator estimator x y classes	0.200000
precision matrix with the generative	pca get precision	0.066667
load and return the breast	load breast	1.000000
probabilities for a calibration curve	core calibration curve y_true y_prob	0.142857
fit the gradient boosting model	ensemble base gradient boosting fit x y	1.000000
the dual gap convergence criterion the	dual gap emp_cov	0.071429
matrix factorization	negative factorization	0.043478
or regression value	check_input	0.100000
all	all	1.000000
error regression loss read	error y_true y_pred	0.125000
accuracy classification score	metrics accuracy score y_true y_pred normalize sample_weight	1.000000
global clustering for	birch global clustering	0.142857
representing each class with a	classifier	0.013699
the given arguments	func get	0.100000
of biclusters	consensus score	0.250000
linear regression with combined l1 and	elastic net	0.111111
set the parameters of this estimator	estimator set params	1.000000
single boost using the	ada boost classifier boost	0.100000
performs clustering on x and	dbscan fit predict x y sample_weight	0.333333
matrix shrunk on	shrunk	0.043478
updates terminal	terminal region tree	0.100000
in friedman [1] and breiman [2]	friedman3 n_samples noise random_state	0.166667
component wise scale	scale x	0.086957
and dispatch	joblib parallel dispatch	0.250000
of jobs for	jobs n_jobs	0.100000
the voting classifier valid parameter keys can	ensemble voting classifier	0.031250
handle the callable case for	metrics pairwise callable x	0.083333
labeled faces in the wild lfw	fetch lfw	0.041667
in 'soft' voting	ensemble voting	0.142857
list of parameter objects and 'return_annotation'	init parameters return_annotation __validate_parameters__	0.125000
items to delete to keep the	items to delete root_path bytes_limit	0.500000
unpack	unpack	1.000000
calculate true and false positives per binary classification	binary clf curve y_true y_score pos_label sample_weight	0.090909
faces in the wild lfw pairs	datasets fetch lfw pairs subset	0.035714
fit onehotencoder to x	preprocessing one hot encoder fit x	1.000000
raises an exception in an unfitted	unfitted	0.083333
filters the given args and	args func ignore_lst args	1.000000
least squares projection of the data onto the	x ridge_alpha	0.071429
compute the boolean mask x	preprocessing get mask x	0.333333
point	x	0.001692
sum w[i] (y[i] - y_[i]) ** 2	sample_weight y_min y_max	0.166667
a single binary estimator	binary estimator	0.363636
actual data loading for the lfw pairs	datasets fetch lfw pairs	0.018868
a	externals joblib parallel backend base apply async	1.000000
fit underlying estimators	core one vs one classifier fit x	1.000000
an 'l' suffix when using the	repr	0.012500
the dual gap convergence criterion the specific definition	dual gap emp_cov precision_	0.071429
variance regression score function best possible score is	variance score y_true y_pred sample_weight	1.000000
data in x as a mini-batch	decomposition mini batch dictionary learning partial fit x	1.000000
range approximates the range	utils randomized range	0.083333
shortest path	single source shortest path	0.333333
loss function for quantile regression	quantile loss function	1.000000
for a calibration curve	core calibration curve y_true y_prob	0.142857
we don't store	externals joblib memorized	0.013699
a single boost using	classifier boost	0.100000
measure the similarity of two clusterings of	cluster fowlkes mallows score labels_true labels_pred	0.333333
compute the largest k singular values/vectors for a	svds a k ncv tol	0.166667
decision function output for x relative to y_true	metrics threshold scorer call clf x y	0.058824
estimate sample	compute sample	0.100000
indices corresponding to test sets	iter test indices	0.333333
mostly low rank matrix with bell-shaped	datasets make low rank matrix	0.083333
parameters theta as the maximizer	gaussian_process gaussian process arg max	0.047619
coefficient of determination r^2	multi output regressor score x y	0.200000
distributions	distributions	0.833333
of feature name -> indices mappings	dict vectorizer fit x y	0.250000
warnings	warnings obj category	1.000000
joblib	joblib	0.036496
best found parameters	core base search cv predict proba x	0.076923
train estimator on training subsets incrementally and compute	incremental fit estimator estimator x y classes	0.200000
the search over parameters	base search cv fit	0.166667
to the given training data and	y	0.002674
w to minimize	x w ht l1_reg	0.250000
the hierarchical clustering	cluster feature agglomeration	0.125000
'l' suffix when	shape repr	0.013699
the reduced likelihood function	gaussian process reduced likelihood function	0.047619
fit a single tree	trees tree	0.142857
the meta-information and the	externals joblib zndarray wrapper read unpickler	0.043478
for reproducibility flips	deterministic vector	0.076923
type introduces	utils shape repr	0.013699
for x relative to y_true	metrics threshold scorer call clf x y	0.058824
k-fold iterator variant with	kfold	0.117647
the directory	dir	0.038462
elastic net path	linear_model enet path x	0.050000
a locally linear embedding analysis on	locally linear embedding x n_neighbors	0.071429
kernel k x	gaussian_process pairwise kernel call x	0.333333
the loss	loss	0.054054
shift clustering using a	shift	0.100000
median absolute error	metrics median absolute error y_true y_pred	0.166667
performs clustering on x and	fit predict x y sample_weight	0.333333
k-means	k means x n_clusters init precompute_distances	1.000000
last step in pipeline after transforms	pipeline fit predict x	0.166667
compute the median of data	median data	0.333333
create a base	meta	0.086957
of moved objects in six moves urllib_response	module six moves urllib response	0.333333
of the breakdown	breakdown	0.125000
get the	neural_network sgdoptimizer get	0.125000
to avoid the hash depending from it	joblib memorized	0.015625
generate train	shuffle	0.083333
linear model parameters	sgdregressor	0.181818
base class	base	0.028571
directory in which are persisted	dir	0.038462
x by scaling rows and	x	0.001692
lfw pairs dataset this operation is meant to	lfw pairs index_file_path data_folder_path slice_ color	0.333333
a callable that handles preprocessing and tokenization	mixin build analyzer	0.333333
for each class	classifier	0.013699
used to fit an estimator within a job	parallel fit estimator estimator	0.333333
zhu et al [1]	ensemble samme proba estimator n_classes	1.000000
the current file position	joblib binary zlib file tell	0.333333
compute the residues on left-out data for	residues x_train y_train x_test y_test	0.083333
the actual data loading for the lfw pairs	datasets fetch lfw pairs	0.018868
generate a	n_samples n_components	0.500000
curve auc from prediction scores note this implementation	roc auc	0.166667
single binary estimator	predict binary estimator	0.200000
parameters for the voting classifier valid	voting classifier set	0.037037
the intercept_	intercept x_offset y_offset x_scale	0.500000
introduces an 'l' suffix	shape repr	0.013699
factor	factor decision	0.500000
elastic net optimization function varies for mono and	x y l1_ratio	1.000000
check input and compute prediction of init	gradient boosting init	0.142857
one-vs-one multi class libsvm	svm one vs one	0.050000
force the execution of the function	joblib memorized	0.015625
dispatch them	dispatch	0.111111
the significance of a cross-validated	estimator x y cv	0.050000
a mostly low rank matrix with	low rank matrix	0.083333
multi-class targets using underlying estimators	output code classifier	0.250000
for parameter value indexing	model_selection index param value x	0.200000
svmlight format this function is equivalent	svmlight	0.050000
generate indices to split data into	split split x	0.250000
a sparse random matrix given	utils random	0.333333
x by scaling rows and columns	x	0.001692
don't	joblib	0.014599
true and false positives per binary	metrics binary clf curve y_true y_score pos_label	0.090909
run fit on one set of	core fit grid point	0.500000
an array	utils check array array accept_sparse	0.250000
incremental fit on a batch of	discrete nb partial fit x y classes sample_weight	0.166667
computes the weighted graph of neighbors for	radius neighbors graph	0.066667
cross-validated estimates for each input	val predict estimator x y cv	0.071429
list of exception	parallel	0.019231
function for factorizing common classes param logic	partial fit first call clf classes	0.058824
compute minimum and maximum along an axis on	utils min max axis x axis	0.333333
data	decomposition	0.095238
ridge regression	ridge cv	0.500000
back the data to the original representation	preprocessing standard scaler inverse transform x	0.066667
routine for validation and conversion of csgraph inputs	sparsetools validate graph csgraph directed dtype csr_output	0.166667
locally linear embedding analysis on the data	locally linear embedding x n_neighbors n_components reg	0.071429
coverage error measure compute how far we need	coverage error	0.166667
precisions	mixture check precisions precisions covariance_type	0.250000
long type introduces an 'l'	utils shape repr	0.013699
do basic checks on matrix covariance sizes	mixture validate covars covars covariance_type n_components	0.250000
factorizing common	fit first call clf	0.200000
count and smooth feature occurrences	nb count x y	0.250000
step5	step5	1.000000
values for	y train	0.166667
is a general function given points on a	x y reorder	0.111111
count	count x	1.000000
k-neighbors	neighbors kneighbors mixin kneighbors x n_neighbors	0.125000
transform binary labels back to	label binarizer inverse transform	0.500000
kernel k x y and	gaussian_process white kernel call x y	0.333333
for each input data point	val predict	0.045455
strip the	strip	0.055556
test/test sizes are meaningful wrt to	model_selection validate shuffle split n_samples test_size train_size	0.111111
used to build a batch of estimators within	parallel build estimators	0.166667
for factorizing common	first call clf	0.200000
oracle approximating shrinkage	oas fit x	0.333333
the svmlight / libsvm format	svmlight file f n_features dtype	0.066667
the em algorithm and return	mixture gmmbase	0.034483
and return it	externals	0.005747
function used to fit an estimator	fit estimator estimator	0.055556
generate isotropic gaussian blobs	blobs n_samples n_features centers cluster_std	0.333333
generate a random multilabel classification	multilabel classification n_samples n_features n_classes	0.500000
using a one-hot aka one-of-k scheme	one hot encoder	0.200000
the one-vs-one multi	one vs one	0.050000
abort	joblib parallel backend base abort	1.000000
fit the model by computing truncated	pca fit truncated x	1.000000
x into	x	0.001692
binary classification task	score y_true y_score average sample_weight	0.076923
scores this score	score	0.010101
number of splitting iterations in the cross-validator	model_selection leave pgroups out get n splits	0.111111
neighbors	mixin radius neighbors	0.125000
for factorizing common classes	first call clf classes	0.058824
an array list sparse matrix or similar	utils check array array accept_sparse dtype order	0.500000
the elastic net optimization	l1_ratio	0.030303
functions of the base	function	0.021277
the largest k singular values/vectors for a	svds a k ncv tol	0.166667
along any axis center to the	x axis	0.030769
local outlier factor of	local outlier factor decision function	0.125000
a memmap instance	externals joblib reduce memmap a	0.050000
into the already fitted lsh forest	neighbors lshforest partial fit	0.200000
global	global	1.000000
estimators between jobs	estimators n_estimators n_jobs	1.000000
length is not found and raise an exception	utils line search	0.029412
the validity of the input parameters	metric p metric_params	0.100000
inplace row	inplace row scale x scale	0.142857
homogeneity metric of a cluster labeling given	metrics cluster homogeneity score labels_true labels_pred	0.500000
finds the neighbors within a given radius	neighbors x radius return_distance	0.500000
private function used to compute decisions within a	ensemble parallel decision function estimators estimators_features	0.500000
determinant matrix	fast mcd x support_fraction	1.000000
wild lfw pairs dataset this dataset	lfw pairs subset	0.035714
apply	analysis	0.090909
cross_decomposition	cross_decomposition	1.000000
to split data into training and test set	model_selection base kfold split x y groups	0.200000
for a fit across one fold	feature_selection rfe single fit rfe estimator	0.200000
gaussian and label samples	make gaussian	0.125000
finds indices in sorted array of integers	neighbors find matching indices tree bin_x left_mask right_mask	0.166667
center kernel	preprocessing kernel centerer transform k y	0.500000
all meta	meta	0.043478
a platform independent representation of	utils shape	0.013699
and maximum	max axis x	0.500000
function call with the	externals joblib format call func	0.100000
memmap instance to reopen on same file	joblib reduce memmap	0.166667
fit the model to data	estimator fit	0.200000
the covariance	distribute covar matrix to match covariance	0.250000
online learning prevents rebuilding of cftree from scratch	cluster birch partial fit x	1.000000
wise scale to unit	preprocessing scale	0.090909
row-wise	row norms	1.000000
x	product call x	0.200000
masks for	masks	0.125000
the directory corresponding to the	func dir mkdir	0.166667
for a given	scorer	0.045455
search over parameters	search cv	0.018182
folders to make cache size fit in bytes_limit	externals joblib memory reduce size	0.083333
dtype of x and y is float32 then	x y	0.002155
in the svmlight / libsvm format	svmlight file f n_features dtype	0.066667
implement a single boost	boost classifier boost iboost x y sample_weight	1.000000
more in the :ref user guide <classification_report>	y_true y_pred labels target_names	0.200000
intercept for specified layer	layer n_samples	0.166667
of new samples can be different from	core calibrated classifier cv	0.111111
collect results from clf predict calls	ensemble voting classifier collect probas	1.000000
to make cache size fit in	joblib memory reduce size	0.083333
trained	neural_network base multilayer perceptron	0.083333
x parameters	x n_neighbors reg	0.500000
score by cross-validation read more in	model_selection cross val score estimator x	0.166667
under the curve auc from prediction scores note	roc auc	0.166667
catch and hide warnings without visual nesting	warnings call fn	0.200000
according to the given training data and parameters	fit x y	0.005988
returns the number of splitting iterations in	split get n splits x y	0.111111
of the	ensemble base	0.166667
the number of jobs which are going to	externals joblib multiprocessing backend effective n jobs n_jobs	0.333333
class versus all others	multiclass	0.076923
detects the soft boundary	svm one class svm fit	0.125000
the data home	clear data home	0.076923
svd	utils svd	0.166667
and hide warnings without visual nesting	ignore warnings call fn	0.200000
a cv in a user friendly	core check cv cv x y classifier	0.031250
evaluate predicted target values for x relative	predict scorer call estimator x	0.166667
is a general function given points on	reorder	0.071429
shortest path length from source to	source shortest path length graph source	0.111111
train test	base shuffle	0.166667
for samples	classifier mixin decision	1.000000
class covariance matrix	class cov x	0.250000
eigenvalues and eigenvectors of	m sigma	0.250000
generate train	base	0.014286
false positives per binary classification	metrics binary clf curve y_true y_score pos_label sample_weight	0.090909
returns the number of splitting iterations in	predefined split get n splits	0.111111
the gaussian process model fitting method	gaussian_process gaussian process fit x y	0.250000
filters the	externals joblib filter args func ignore_lst	0.500000
of the data onto the sparse components	decomposition sparse pca transform x ridge_alpha	0.200000
find the first prime	hungarian state find prime	0.500000
the shortest path length from source to	source shortest path length graph source	0.111111
a locally linear embedding analysis on	manifold locally linear embedding	0.062500
gaussian distributed dataset	elliptic envelope	0.166667
set the parameters of this estimator	pipeline set params	0.500000
python object	filename	0.050000
estimators within	estimators n_estimators ensemble x	0.083333
distances between the vectors in x and y	distances x y sum_over_features size_threshold	1.000000
points in	n_neighbors mode	1.000000
data precision matrix with the generative model	base pca get precision	0.066667
x for	fit x	0.006410
weights by class for unbalanced datasets	weight class_weight y	0.200000
minimum and maximum along an axis on a	min max axis x axis	0.333333
do nothing and return the	feature_extraction	0.037037
with coordinate descent the elastic net	l1_ratio	0.030303
hessian in the case of a logistic loss	logistic grad hess w x	1.000000
of x from y along the	x	0.001692
fit an estimator	fit estimator estimator x y sample_weight	0.071429
data precision matrix with the	precision	0.016667
the reduced	gaussian process reduced	0.125000
lower bound on model evidence based on	mixture dpgmmbase lower bound	0.071429
random regression problem with sparse uncorrelated design	sparse uncorrelated n_samples n_features	0.166667
the huber loss and	huber loss and	0.166667
get the values used to update params with	neural_network sgdoptimizer get	0.125000
precision matrix	empirical covariance get precision	0.250000
with iterative fitting along a	cv	0.018018
building a cv in a user friendly way	core check cv cv	0.031250
from the decision	decision	0.027778
fits the oracle approximating shrinkage covariance	covariance oas fit x	0.083333
backend and return the number of	externals joblib parallel backend base	0.034483
compute log probabilities	log proba	0.090909
also predict based on	predict	0.006849
of x from y along	x z	0.050000
the huber	linear_model huber	0.333333
given type in the dispatch table	customizable pickler register type reduce_func	1.000000
compute the per-sample average log-likelihood of the	mixture base mixture score	0.111111
set the intercept_	linear_model linear model set intercept x_offset y_offset x_scale	1.000000
read an array using numpy memmap	numpy array wrapper read mmap	1.000000
the lfw pairs	lfw pairs	0.018868
corresponding derivatives with respect to	activations deltas	0.032258
friedman [1] and breiman [2]	friedman3	0.090909
low rank matrix with bell-shaped	low rank matrix	0.083333
according to the given training	x y sample_weight	0.025974
transform array or sparse matrix x back to	inverse transform x	0.025641
estimate the precisions parameters of	gaussian mixture estimate precisions nk	0.166667
contingency matrix describing the relationship	cluster contingency matrix labels_true	0.333333
lfw pairs dataset this dataset is	lfw pairs subset	0.035714
kwargs using a	kwargs	0.076923
the given arguments	func	0.022727
compute log	log	0.037736
two columns of a csc/csr matrix in-place	utils inplace swap column	0.250000
least-squares solution to a large sparse linear system	lsqr a	0.037037
of a function	function	0.021277
sgd	sgd	0.833333
in the svmlight / libsvm	svmlight file f n_features dtype multilabel	0.066667
apply	apply	0.416667
prefetch the tasks for the next batch and	batch iterator	0.500000
module names and a name for the	name	0.033333
the flattened log-transformed non-fixed hyperparameters	gaussian_process kernel operator theta theta	0.333333
avoid the hash	externals joblib memory	0.016949
to implement the usual api and hence	decomposition sparse coder fit x y	0.142857
exception in an unfitted	unfitted	0.083333
count	nb count x	1.000000
elastic net path with	enet path	0.050000
returns distinct binary samples	datasets generate hypercube samples	0.333333
parameter value indexing	model_selection index param value x v	0.200000
restricted to the binary classification task	y_true y_score pos_label	0.066667
estimator on training subsets incrementally	core incremental fit estimator estimator	0.500000
for building a cv	cv cv	0.031250
store the timestamp when pickling to avoid	memory reduce	0.030303
set the diagonal of the laplacian matrix	manifold set diag laplacian	0.333333
estimator on training subsets incrementally and compute	model_selection incremental fit estimator estimator x y classes	0.200000
class for all meta estimators	meta estimator	0.062500
the free energy f	free energy	0.066667
case the hasher	hasher	0.111111
types	base	0.014286
of feature names ordered by their indices	dict vectorizer get feature names	0.142857
return a	utils shape repr	0.013699
fit onehotencoder to	preprocessing one hot encoder fit	0.500000
the l1 distances between the	manhattan distances	0.083333
number of estimators	len	0.038462
model	decomposition	0.095238
and compute prediction of init	boosting init decision function	0.142857
used to build a batch of estimators within	ensemble parallel build estimators n_estimators ensemble x y	0.166667
binary classifier	binary	0.031250
of the leaf	tree base decision tree apply	0.166667
fit	output estimator fit x y	0.200000
biclusters	cluster consensus	0.250000
type introduces an 'l' suffix when	utils shape	0.013699
returns the index of the leaf	base decision tree apply	0.166667
file	file x y f	1.000000
for parallel processing	parallel	0.019231
evaluate the significance of a cross-validated score with	score estimator x y cv	0.083333
em update	decomposition latent dirichlet allocation em step	0.500000
fit	core multi output estimator fit	0.200000
attempts to retrieve a reliable	externals joblib get	0.142857
of the set of samples x	x	0.001692
dictionary learning finds a dictionary a set	dictionary learning	0.142857
absolute sizes of training subsets and validate 'train_sizes'	sizes train_sizes n_max_training_samples	0.500000
em algorithm and return	gmmbase	0.062500
sizes	core translate train sizes	0.066667
apply a transform	x transform	0.333333
the number of splitting iterations in the cross-validator	out get n splits	0.111111
number of splitting iterations in the	group out get n splits x y groups	0.111111
loading for the lfw people	fetch lfw people	0.040000
the labeled faces in the wild lfw pairs	fetch lfw pairs	0.018868
timestamp when pickling	reduce	0.034483
of the dual gap convergence criterion the	dual gap	0.071429
used to capture the arguments of a function	externals joblib delayed function check_pickle	0.333333
w h whose product	x w h	0.035714
from	memorized func	0.016949
returns	gaussian_process stationary	0.333333
model	get	0.012048
get parameters	gaussian_process exponentiation get params	0.500000
function used to fit an estimator	fit estimator estimator x y	0.071429
return the shortest path	single source shortest path	0.333333
the dual gap convergence criterion	covariance dual gap emp_cov precision_	0.071429
apply clustering to a projection to the	clustering affinity n_clusters	0.166667
x by scaling rows and columns independently	x	0.001692
types	externals joblib parallel backend	0.029412
text report showing the main classification metrics	metrics classification report	0.166667
list of exception types to	joblib parallel backend base get	0.066667
used to build a batch	parallel build	0.047619
display	externals joblib parallel print	0.125000
random regression problem with sparse	sparse	0.025000
k-neighbors of a point	neighbors kneighbors mixin kneighbors	0.100000
log-probabilities for x	log proba x	0.666667
log-likelihood	covariance score	0.071429
wild lfw pairs dataset this dataset is	fetch lfw pairs	0.018868
check that the parameters are well defined	mixture check parameters	0.166667
instance for the given param_grid	model_selection grid search cv get param iterator	0.166667
job	estimators estimators_features x n_classes	0.333333
likelihood of theta	likelihood theta	1.000000
y is of	y	0.002674
