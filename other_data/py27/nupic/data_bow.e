construct the filter parameters
testregion command that returns the identity policy instance that was associated with this testregion instance via setidentitypolicyinstance()
add labels from the anomaly classifier within this model
see the function description in base py
get a specific item by name out of the results dict
semi-private method for retrieving the jobid
returns the number of synapses
see comments in base class
inspects the control task and updates any stream sources it finds that are not absolute paths into paths generated by pkg_resources relative to
returns the indices of the cells passed in
@internal return the random number state
returns the index of the column that a cell belongs to
testregion command that sets identity policy instance the instance
update the inference active state from the last set of predictions and the current bottom-up
performs inhibition this method calculates the necessary values needed to
(from backtracking_tm py)
updates synapses on segment
turn inference off for the current model
returns storage stats like min and max values of the fields
this function gives the future predictions for <nsteps> timesteps starting from the current tm state
@param c column index @param i cell index in column
non-equality operator for temporalmemory instances
sets the permanence increment
generate the simulated output from a spatial pooler that's sitting on top of another spatial pooler / temporal memory pair
emits a set of inputs data inferences and metrics from a model resulting from a single record
marks the stream completed true or false
return the total # of models we have in our database if swarmid is none or in a specific swarm
emits periodic metrics metrics a list of prediction_metrics_manager
remove labels from each record with record rowid in range from start to end noninclusive of end
generate a set of records reflecting a set of probabilities
return a pattern for a number
this gets called on every compute it determines if it's time to
returns the permanence increment amount for columns that have not been
generates requested number of records and saves in a csv file
returns the next value of the disribution using knowledge about the current state of the distribution as stored in numvalues
returns all field names
sets the current model as orphaned this is called when the scheduler is
similar to cross(), but generates output dictionaries instead of tuples
pick a value according to the provided distribution
return the name of the region
perform an internal optimization step that speeds up inference if we know learning will not be performed anymore
makes directory for the given directory path if it doesn't already exist in the filesystem
returns the index of the cell
fetch the values of 1 or more fields from a sequence of job records
sets the permanence increment amount for columns that have not been
capnp deserialization method for the anomaly likelihood object
check for concurrency violation and add self to _clsoutstandinginstances
re- initialize the loging directory for the calling application that uses initlogging() for logging configuration
set the current state of this particle this is counterpart to getstate
update the boost factors for all columns the boost factors are used to
this causes the variable to jiggle away from its current position
gets a neighborhood of inputs
return the list of active swarms in the given sprint these are swarms
return a dict that can be used to create an anomaly model via opf's modelfactory
get the runtime statistics specific to the model
change the given swarm's state to 'newstate' if 'newstate' is
modify the data in place adding noise
generate the initial first order and second order transition probabilities for 'model0'
returns a string representing a numpy array of 0's and 1's
returns the activity duty cycles for all columns 'activedutycycles'
returns the sdr for jth value at column i
gets the value of a given field from the input record
takes a record and returns true if record meets filter criteria
disables learning in the htmpredictionmodel's temporal pooler while retaining the ability to re-enable tm learning in the future
updates the final aggregated score error given the prediction and the ground truth
return {value : <current measurement>, "stats" : {<stat> : <value>
helper function used by averageontimepertimestep 'durations' is a vector
update the results string and last-update-time fields of a model
enable the diagnostic feature for debugging unexpected concurrency in acquiring connectionwrapper instances
run the given model
run this worker
pretty-print the encoded output using ascii art
sets the update period
public api for returning the category list
pretty print the connections in the temporal memory
save the current metric value and see if the model's performance has 'leveled off
search the configuration path (specified via the nta_conf_path environment variable) for the given filename
sets the autodetectwaitrecords
this method will be called only when the node is used in nupic 2
initialize all ephemeral members after being restored to a pickled state
computes the average on-time of the outputs that are on at each time step and then averages this over all time steps
resets stats collected so far
pretty print a sequence
returns an array of length size and type dtype that is everywhere 0 except in the indices listed in sequence pos
auto forwarding of properties to get methods of internal region
returns the model creation parameters based on the settings in the config dictionary
converts the control element from nupic format to a default opf format with 1 task
:returns a sequence of :class ~ fieldmetainfo
see the function description in base py
performs initialization that is necessary upon entry to the phase must
set the read timeout in seconds int or floating point
loads all the parameters for this dummy model for any paramters
generate stream definition based on
computes the number of models that are expected to complete as part of this instances's hypersearch
populate the output array with the category indices
set the radius resolution and range these values are updated when minval
initialize all ephemerals used by derived classes
add the label labelname to each record with record rowid in range from start to end noninclusive of end
adds a histogram to the plot's figure
see nupic encoders base encoder for more information
return the metric value
this is the tail of every params file we generate between the head and the tail
sets which metrics should be written to the prediction log
set the value of a spec parameter most parameters are handled
translate coordinates into an index using the given coordinate system
@returns moving average of learned sequence length
recursively applies f to the values in dict d
returns the overlap duty cycles for all columns 'overlapdutycycles'
instantiate the hypersearch worker parameters
generate and return the following encoder related substitution variables encoderspecsstr
return a dict containing all of the configuration properties parameters
sets the max new synapse count
return the default spatial pooler implementation for this region
returns true if the inference from this timestep is predicted the input for the next timestep
allows ids to be assigned a category and subsequently enables users to use - :meth ~
returns an array of length size and type dtype that is everywhere 0 except in the index in pos
choose a new position that is as far away as possible from all 'othervars', where 'othervars' is a list of permutevariable instances
the range of connectedsynapses per column averaged for each dimension
compute the number of eigenvectors singularvalues to keep
test that we can converge on the right answer
sets the activation threshold
return the total number of segments
raises nupicjobfailexception by mapping from another exception that is being handled in the caller's scope and preserves the current exception's
force a switch back to learning mode not normally supported
disable writing of output tap files
public api for returning the category list this is a required api of the nearestneighbor inspector
creates and returns the _iterationphase-based instance corresponding
search the configuration path (specified via the nta_conf_path environment variable) for the given filename
creates an iterator that returns modelinfo elements for the given modelids warning the order of modelinfo elements returned by the iterator
convert a list of numbers to a string of space-separated numbers
generate requested statistics for a dataset and cache to a file
return a numpy array containing the complete set of hyperplanes used by the trained svm classifier
loads a json value from a file and converts it to the corresponding python object
see method description in base py
should return the output width in bits
train an svm model
get the collection of regions in a network this is a tricky one
returns the synapses on a segment
extracts the predicitonkind temporal vs nontemporal from the given
returns the permanence decrement amount for inactive synapses
[virtual method override] this method is called during deserialization (after __setstate__) with an external directory path that can be used to
translate parameters and initialize member variables specific to backtracking_tm py
build the additional specs in three groups for the inspector use the type of the default argument to set the spec type defaulting
@return trace trace of predicted => active cells
initialize tables if needed parameters
get the particle state as a dict this is enough information to
converts a category number into a list of labels
return the list of swarms in the given sprint that were not killed
writer non-temporal prediction log writer conforming to predictionwriteriface interface
return the learning state of the current model
generates the non-default metrics specified by the expgenerator params
@doc place_holder network initialize
generate the filename for aggregated dataset the filename is based on the input filename and the
get the maximum number of synapses per segment
process one input sample
returns the periodic checks to see if the model should continue running
:param network nupic engine network
get cap'n proto schema note this is an abstract method
emits a set of inputs data inferences and metrics from a model resulting from a single record
updates a duty cycle estimate with a new value this is a helper
feeds input record through tm performing inference and learning
initializes the permanences of a column the method
called when the stream is completed
put us back at the beginning of the file again)
@todo implement this it is used by the node's getparameter() call
return the generation index of the first generation in the given swarm that does not have numparticles particles in it either still in the
release database connection and cursor passed as a callback to
instantiate a metricsiface-based module
the main function of the hypersearchworker script this parses the command
return serializable state this function will return a version of the
see comments in base class
returns an array of length size and type dtype that is everywhere 0 except in the indices listed in sequence pos
metric a metric type name that identifies which metrics module is to be constructed by the metrics factory method
recursively updates the values in original with the values from updates
performs initial setup activities including 'setup' callbacks this
encode a record as a sparse distributed representation
returns list of default metrics to be overridden
requestedactivities a sequence of periodicactivityrequest elements
return the position of this particle this returns a dict() of key
see the function description in base py
recursively copies a dict and returns the result
decorator for specifying tests that only run when --long is specified
find the user's "documents" directory os x "my documents" directory windows or home directory unix
return a sequence of matching rows with the requested field values from a table or empty sequence if nothing matched
@returns the total number of segments
fetch the values of 1 or more fields from a sequence of model records
initialize the random seed
parses and validates the --descriptionfromfile option and executes the
read the data out of the given category file returning a tuple categorycount listofcategories
translate parameters and initialize member variables specific to backtracking_tm py
patch __getattr__ so that we can catch the first access to 'cells' and load
run one iteration of spregion's compute profiling it if requested
list of our member variables that we don't need to be saved
prints a listing of experiments that would take place without actually executing them
retrives a dictionary of metrics that combines all report and optimization metrics
context guard - exit ensures that the file is always closed at the end of the 'with' block
just return the inference value from one input sample the actual
mangles the given mangled private member name a mangled member name is one whose name begins with two or more underscores and ends with one
returns the segments that belong to a cell
generates the clientjobs database name for the current version of the database "semi-private" class method for use by friends of the class
process one input sample
returns a dict of all temporary values in custom configuration file
return serializable state this function will return a version of the
takes an encoded output and does its best to work backwards and generate the input that would have generated it
writes out a new combined file containing weather data
trivial init method that just calls base class's __init__() this method is attached to classes that don't define __init__()
stores the current model results in the manager's internal store
return the total number of segments in cell c i
this "backtracks" our inference state trying to see if we can lock onto the current set of inputs by assuming the sequence started up to n steps
print out what test we are running
convert a database internal column name to a public name this
returns model completion message
delete's the output cache associated with the given modelid this actually
return a dict containing all custom configuration properties parameters
return index of the 'timestamp' field
return the classified labeling of record
construct a _htmclassificationrecord based on the current state of the htm_prediction_model of this classifier
constructor label a distinguishing string that will be used to distinguish
creates the required metrics modules
:returns a list containing unique non-none partition ids just the keys
recursively applies f to the values in dict d
returns an array of field names associated with the data
return a dict that can be used to construct this encoder this dict
fetch the values of 1 or more fields from a job record here 'fields'
add noise to the given input
see the function description in base py
intercept the _readstdconfigfiles call from our base config class to read in base and custom configuration settings
is called once by nupic before the first call to compute()
returns the number of output elements
returns combined data from all sources values only
returns true if the inference from this timestep is predicted the input for the next timestep
potentially change the minval and maxval using input
retrieve the requested property and return it as a bool if property
loads a description file and returns it as a module
constructor streamdefdict stream definition as defined in
init all of our variable positions velocities and optionally the best result and best position from the given particle
compute a log scale representation of the likelihood value since the
returns list of all ephemeral members
initialize a field with various parameters such as n w flag datatype encodertype and tag predicted field
change the value of 1 field in a job to 'newvalue', but only if the current value matches 'curvalue'
close the policy instance and its shared database connection
return the index of a cell in this column which is a good candidate for adding a new segment
returns the metadata specifying the format of the model's output
returns a dictionary of permutation variables
@returns the total number of cells
find weakly activated cell in column with at least minthreshold active synapses
wraps getrow() such that instances may be indexed by columnindex
returns reference to the network's tm region
get the results string and other status fields for a set of models
look through the jobs table and get the demand - minimum and maximum number of workers requested if new workers are to be allocated if there
returns next available data record from the storage as a dict with the keys being the field names
serializes model using capnproto and writes data to checkpointdir
print out a banner
@param segment object segment object that the synapse is synapsed to
runs the given opf task against the given model instance
helper function to create a logger object for the current object with
@returns the total number of synapses
returns all the records
[_iterationphase method implementation] performs initialization that is necessary upon entry to the phase
returns information about the distribution of segments synapses and permanence values in the current tm
initialize all ephemerals used by derived classes
returns the learning iteration number
returns information about the distribution of segments synapses and permanence values in the current tm
return the distances from inputpattern to all stored patterns
this method overrides valuegetterbase's "pure virtual" method it
calculate distances in the original input space pre-svm post-pca
return the value of skiprecords for passing to estimateanomalylikelihoods if windowsize is very large bigger than the amount of data then this
list of attributes to not save with serialized state
punishes the segments that incorrectly predicted a column to be active
get the labels on classified points within range start to end not inclusive
metricmulti constructor using metricspec is not allowed
enables learning in the htmpredictionmodel's temporal pooler see also htmpredictionmodelcontroldisabletplearningcb
return the offset and length of a given field within the encoded output
adds an image to the plot's figure
finds the category that best matches the input pattern returns the
resets stats collected so far
print the list of [column cellidx] indices for each of the active cells in state
returns the permanence increment amount for active synapses
constructor fields a non-empty sequence of nupic
reset the state of all cells
[overrides nupic encoders scalar scalarencoder getbucketindices]
get all info about a job parameters
@return trace trace of predicted => inactive cells
disables learning in the htmpredictionmodel's spatial pooler while retaining the ability to re-enable sp learning in the future
starts hypersearch as a worker or runs it inline for the "dryrun" action
update boost factors when local inhibition is used
run a named function specified by a filesystem path module name and function name
utility function to get information about function callers the information is the tuple (function/method name filename class)
return the base spec for testregion
resets stats collected so far
returns instance of the underlying spatialpooler algorithm object
generates a "persistentjobguid" value
get the permanence decrement
returns the duty cycle period
see method description in base py
retrives a dictionary of metrics designated for report parameters
@param monitor monitormixinbase monitor mixin instance that generated
returns an array containing the sub-field bucket indices for each sub-field of the inputdata
print the parameter settings for the tm
return a dict containing all of the configuration properties parameters
compute the learning active state given the predicted state and the bottom-up input
this "backtracks" our learning state trying to see if we can lock onto the current set of inputs by assuming the sequence started up to n steps
retrieve the requested property as a string if property does not exist
for all models that modified their results since last time this method was called send their latest results to the hypersearch implementation
@doc place_holder region getspecfromtype
list available checkpoints for the specified experiment
serialize python list object containing only 0's and 1's to string
get the value of a parameter most parameters are handled automatically by
not implemented csv file is always considered completed
update the inference state called from compute() on every iteration
returns the cumulative w for all the fields in the dataset
returns the max new synapse count
map category indices internal to category ids external
see nupic encoders base encoder for more information
sets the potential percent
create our state object
return list of models ids that completed with errors
return the inference value from one input sample the actual
return the list of labels for the metrics that are being calculated
open the data file and write the header row
return the list of paths to search for configuration files
equivalent to the category inference of zeta1 toplevel
unittest testcase assertfalse override adds extra log items to msg
returns a sequence of nupic data fieldmeta fieldmetainfo
:returns int the index of the record that will be read next from :meth ~
return a single matching row with the requested field values from the the requested table or none if nothing matched
returns a randomly generated permanence value for a synapses that is initialized in a connected state
return a json representation of obj with sorted keys on any embedded dicts
reset the learning and inference stats this will usually be called by
perform cross-validation to measure the recognition accuracy of an svm
returns a subset of the keys that match any of the given patterns
validates control dictionary for the experiment context
write state to proto object
test to see if the metrics manager correctly shifts records for multistep
return true iff checkpointdir appears to be a checkpoint directory
given a synapse and a list of synapses check whether this synapse exist in the list
returns list of all ephemeral class members
generate the initial first order and second order transition probabilities for 'model1'
setup our resultsperchoice history based on the passed in resultsperchoice
see comments in base class
initialize ephemeral instance variables (those that aren't serialized)
merge sorted chunk files into a sorted output file
there are two caveats first this is a potentially slow operation second
grows the histogram to have rows rows and cols columns
override this method to set the default tm params for self tm
generate and log a 32-bit compatible seed value
convert datetime timedelta to seconds in floating point
this method is called during network serialization with an external filename that can be used to bypass pickle for saving large binary states
get labels from the anomaly classifier within this model
see comments in base class
activity tick handler services all activities
see method description in base py
removes any stored records within the range from start to end
(from backtracking_tm py)
get the sensor input element that corresponds to the given inference element
get serializable state
update the results string and/or num_records fields of a model
calculate error signal
@param connections object connections for the tm
removes any update that would be for the given col cellidx segidx
returns the task instances of the experiment description
returns the stability for the population averaged over multiple time steps
network for the region
generate a set of simple and hub sequences a simple sequence contains
updates counter instance variables each round
returns a description of the dataset
[context manager protocol method] permit a connectionwrapper instance to be used in a context manager expression (with
returns the indices of the winner cells
given an encoder name get the key
creates the model's predictionlogger object which is an interface to write
initialize the encoders
this method increases the permanence values of synapses of columns whose activity level has been too low
returns list of all ephemeral members
implemented in backtrackingtmcpp backtrackingtmcpp loadfromfile
[scalarencoder class method override]
get the beginning part of the database name for the given database version
return a pretty print string representing the return value from :meth
@return trace trace of # synapses
function for running binary search on a sorted list
return serializable state this function will return a version of the
same as writerecord above but emits multiple rows in one shot
niter number of iterations to remain in this phase
set the value of the parameter
starts a swarm given a path to a json file containing configuration
add 'value' to the field i
returns the number of segments
form of distribution must be an array of counts in order of self keys
create a checkpoint from the current model and store it in a dir named
returns sensor region's encoder for the given network
choose a new position based on results obtained so far from all other particles
returns the nth encoding
creates a fieldmetainfo list from the a list of tuples basically runs
used as optparse callback for reaping a variable number of option args
returns descriptions for all fields
do one iteration of inference and/or learning and return the result parameters
:param sampledata :type sampledata numpy array
utility function for creating enumerations in python example usage
exports a network as a networkx multidigraph intermediate representation suitable for visualization
return all the prototype distances from all computes available
process the attendance data of one club if the club already exists in the list update its data
get the points in the neighborhood of a point
get the value of the parameter
construct a _htmclassificationrecord based on the state of the model passed in through the inputs
return index of the 'category' field
add values to the field
retrieve the engine-level model params from a swarm model
returns the data for a segment
returns the metadata specifying the format of the model's output
generates a list of model names that are expected to complete as part of this instances's hypersearch
dictkeychain one or more strings the first string is a key that will eventually be defined in the dictionary that will be passed
enables learning in the htmpredictionmodel's spatial pooler see also htmpredictionmodelcontroldisablesplearningcb
load cells4 state from this file
for the given column return the cell with the fewest number of segments
insert a new entry or update an existing one if this is an update
returns the permanence amount that qualifies a synapse as
see comments in base class
generates set of consecutive patterns
gets fields from all models in a job that have been checkpointed this is
process one input sample
parse a string of space-separated numbers returning a python list
maps the coordinate to a bit in the sdr
return the inferencetype of this model
export all the records into a csv file in numenta format
process one input sample
see the function description in base py
initialize all ephemeral members after being restored to a pickled state
implement this method to specify the temporal memory class
returns the classified labeling of record
[overrides nupic encoders scalar scalarencoder encodeintoarray]
returns the number of connected synapses for all columns
instantiates a _hypersearchjob instance from info saved in file
get the path of the custom configuration file
handles a "warning signal" from the scheduler this is received when the
context guard - enter
see nupic encoders base encoder for more information
get the connected permanence
sets the local area density invalidates the 'numactivecolumnsperinharea'
calculate dendrite segment activity using the current active cells
return the absolute path of the model's checkpoint file
apply pre-encoding filters
generates a random sample from the discrete probability distribution and returns its value and the log of the probability of sampling that value
@param connections object connections for the tm
log 'msg % args' with severity 'debug'
@return trace trace of predictive cells
see the function description in base py
hash a coordinate to a 64 bit integer
computes the predictive ability of a temporal memory tm this routine returns
returns a sequence of field types corresponding to the elements in the decoded output field array
check if the cancelation flag has been set for this model
back up a file
add the label labelname to each record with record rowid in range from start to end noninclusive of end
called at the end of inference to print out various diagnostic information based on the current verbosity level
not implemented csv file is always considered completed nothing to do
@param c column index
return the interal _topdownmappingm matrix used for handling the bucketinfo() and topdowncompute() methods
writes the results of one iteration of a model the results are written to
begin writing output tap files
see the function description in base py
remove labels from the anomaly classifier within this model
n is the total bits in input
writes the contents of this model's in-memory prediction cache to a permanent
returns the active segments
retrives a dictionary of metrics designagted for optimization parameters
return the spec for tmregion
@param cell int index of the cell that this segment is on
get the path of the custom configuration file
a list of row indices to remove there are two caveats first this is
returns most common value seen in the non-none elements of the list
get the value of the parameter
bring a top-level window with a given title
places the model in a permanent "finished learning" mode
return the best model id and it's errscore from the given swarm
updates the accumulated error given the prediction and the ground truth
for the given cell find the segment with the largest number of active synapses
get the beginning part of the database name for the current version of the database
change the status on the given job to completed parameters
gets the current metric values returns a dictionary of metric values
called from the scope of the region's pyregion initialize() method
return serializable state this function will return a version of the
adds one encoder
get the default arguments from the function and assign as instance vars
return the absolute path to the directory where the model's own "extra data" are stored (i
performs local inhibition local inhibition is performed on a column by
see comments in base class
sets the learning iteration number
represents the sum of the widths of each fields encoding
see comments in base class
returns the aggregation period of the record stream as a dict containing 'months' and 'seconds'
create a sdr classifier factory
passes the "finish learning" command to the model note upon completion
sets the path of the custom configuration file
returns true if all elements of the sequence satisfy true and x
returns the experiment description schema this implementation loads it in
@param numsequences int number of sequences to return
returns the min threshold
get a connection instance
:param outp file-like obj to which rendered graph is written (defaults to sys
copies a range of values to a new location in the data set
initialize this metric if the params contains the key 'errormetric', then that is the name of
like _getonematchingrownoretries(), but with retries on transient mysql
get the logger created by this subclass
set the queue of upcoming partition ids this can be used instead of the
handle the cla classifier compute logic when implementing multi-step prediction
get the labels of the previously computed record
the average number of columns per input taking into account the topology of the inputs and columns
flush the file to disk
deerializes model from checkpointdir using capnproto
tell the writer which metrics should be written
returns sum of the elements in the list missing items are replaced with
return a dict containing all custom configuration properties parameters
returns the update period
given the normal distribution specified by the mean and standard deviation in distributionparams return the probability of getting samples further
close the policy instance and its shared database connection
write state to proto object
incremet the value of 1 field in a job by increment the 'fieldname' is
return the distances between the input pattern and all other stored patterns
reset the velocity to be some fraction of the total distance this
returns the boosted overlap score for each column
this is a toy example to show the basic functionality the dataset is
sets the value that will be encoded when this region does a compute
@internal seed the random number generator
loads the experiment description file from the path
look through the jobs table and count the running jobs whose cancel field is true
parse the given xml file and return a dict describing the file
returns the inhibition radius
called by our unittest testcase assertxxxxxx overrides to construct a
list of our member variables that we don't need to be saved
retrieve the requested property as a string if property does not exist
returns a list of labels that correspond to metrics being computed
get current average
construct the filter parameters
returns true if the inference type is 'temporal', i e requires a
turn learning off for the current model
resumes processing of an existing job that is presently in the status_completed state
returns data types for all fields
destructor note this is not guaranteed to be called bugs like circular references could prevent it from being called
set whether learning is enabled
check whether any experiments failed in our latest hypersearch
returns current maximum value for the field 'fieldname'
sets the path of the custom configuration file
set flag for field at index flags are special characters such as 's' for
@return trace trace of active columns
set the random seeds helpful to make unit tests repeatable
helper function to return a scalar value representing the expected
change the category associated with this vector s
this method will remove the given records from the classifier
see if we can kill off some speculative swarms if an earlier sprint
hash a region
reclassifies given state
set the value of a parameter
initialize interpreter with blacklisted nodes removed from supported nodes
run scanning inference and store the results
returns the total number of inputs
accepts log-values as input exponentiates them sums down the rows first dimension normalizes then converts the sum back to
sets the inhibition radius
returns reference to the network's sp region
perform global inhibition performing global inhibition entails picking the
returns the percent of the outputs that remain completely stable over n time steps
returns pretty-printed table of traces
makes directory for the given directory path with default permissions
transfer the permanences from source to dest sp's this is used in test
add one instance consisting of ground truth and a prediction
returns args dictionary from the calling method
returns the dimensions of the columns in the region
see method description in base py
save a checkpoint of the prediction output stream the checkpoint
generate a coincidence matrix this is used to generate random inputs to the
periodically check to see if we should remove a certain field combination from evaluation because it is doing so poorly or move on to the next
@return trace trace of resets
seeks to numrecords from the end and returns a bookmark to the new position
function that compares two spatial pooler instances compares the
run one iteration of tmregion's compute
sets the initial permanence
verify the validity of the node spec object the type of each sub-object is verified and then
@internal patch __getattr__ so that we can catch the first access to 'cells' and load
given the name of an aggregation function returns the function pointer and param
returns encodings for all the records
get the value of the parameter
set a parameter of the anomaly classifier within this model
checks to see if the model should exit based on the exitafter dummy
reset the state of all cells
compute normalization constants for each feature dimension based on the collected training samples
propagate field statistics to the model in case some of its machinery needs it
see method description in base py
returns coordinates around given coordinate within given radius
mark the passage of time this information is used during segment
reads the current "best model" for the job and returns whether or not the
instantiate a clientjobsdao instance
(from backtracking_tm py)
return a map from number to matching on bits for all numbers that match a set of bits
sets the minimum tolerated activity duty cycle given as percent of
a segment is active if it has >= activationthreshold connected synapses that are active due to activestate
set the state of this object from a serialized state
@param monitor monitormixinbase monitor mixin instance that generated
this calls phase 2 of inference used in multistep prediction
generates a worker completion message that is suitable for the
unwraps self __rawinfo params into the equivalent python dictionary
does nothing kept here for api compatibility
return the base spec for tmregion
close the policy instance and its database connection pool
this method is called during network deserialization with an external filename that can be used to bypass pickle for loading large binary states
acquire a connectionwrapper instance that represents a connection to the sql server per nupic
raises an error if cell index is invalid
return the closest training pattern that is *not* of the given category "cat"
return index of the 'sequenceid' field
get all info for a set of models warning!!!: the order of the results are not necessarily in the same order as
compute area under the curve auc using the trapezoidal rule parameters
places the model in a permanent "finished learning" mode
loads a saved jobid from file parameters
generates the clientjobs database name for the given version of the
returns the total number of columns
sets the min threshold
returns the average on-time averaged over all on-time runs
return the pycapnp proto type that the class uses for serialization
gets the specified fields for all the models for a single job this is
print a message to the console
converts a category number into a list of labels
returns the dimensions of the input vector
generate a figure that shows each output over time time goes left to right
generate a dataset of aggregated values parameters
return the overlap between two representations rep1 and rep2 are lists of
modelconfig a dictionary object which holds user-defined settings for model
generate a set of simple sequences the elements of the sequences will be
returns the width of dataout
attaches an 'anomalyclassifier' region to the network will remove current
return segment number segidx on cell c i
returns whether there are more records from current position bookmark
returns true if all records are already in the storage or false if more records is expected
returns mean of non-none elements of the list
get the value of a nodespec parameter most parameters are handled
store a new item in our history
callback that returns a list of all "ephemeral" members (i e data members
add distribution to row row
loads the experiment description python script from the given experiment directory
parses a textual datetime format and return a python datetime object
non-equality operator for temporalmemory instances
routine for computing a moving average
generate a filepath for the calling app
allocate the spatial pooler instance
construct the tm @param pamlength number of time steps to remain in "pay attention mode" after
weighted mean uses params must be the same size as inlist and
get tuple actually a generator of indices where the max value of array x occurs
use the c++ implementation to build an svm model
from http //book opensourceproject org cn/lamp/python/pythoncook2/opensource/0596007973/pythoncook2-chp-19-sect-9 html
resets the region's sequence states
displays command schema to stdout and exit program
return the dictionary of output values note that these are normal python
generates set of random patterns
see the function description in base py
return a clipped version of obj suitable for printing this is useful when generating log messages by printing data structures but
creates and returns the _iterationphase-based instance corresponding
given the complete set of results generated by an experiment (passed in 'results'), filter out and return only the ones the caller wants as
constructs a new empty histogram with no rows or columns
gets a training pattern either by index or category number
marks the stream completed true or false
converts all of the non-numeric fields from spatialoutput and temporaloutput into their scalar equivalents and records them in the output dictionary
saves multiple records in the underlying storage
compute the column confidences given the cell confidences if
niters number of iterations must be greater than 0
close connectionfactory's connection policy typically there is no need
[override] turn learning on for the current model
return a numeric key for sorting this segment
since the knn classifier stores categories as numbers we must store each label as a number
returns pretty-printed table of metrics
@doc place_holder network enableprofiling
protected method that is called during deserialization (after __setstate__) with an external directory path
set all the dynamic state variables from the <tpdynamicstate> dict
set the state of ourself from a serialized state
add an item to the log items list for the currently running session
return a segmentupdate data structure containing a list of proposed changes to segment s
returns log(exp a + exp b a and b are numpy arrays
attempt to insert a row with the given parameters into the jobs table
print segment information for verbose messaging and debugging
return the pycapnp proto type that the class uses for serialization
returns a range of records starting from the bookmark if 'bookmark'
return the field contributions statistics
given an encoder dictionary key get the encoder name
this method will remove any stored records within the range from start to end
returns a state's anomaly vertor converting it from spare to dense
compute the singular value decomposition svd the svd is a factorization
build the additional specs in three groups for the inspector use the type of the default argument to set the spec type defaulting
helper function to return a scalar value representing the most
a "pure virtual" method the derived class must override this method
edits the xml configuration file with the parameters specified by
given model params figure out the correct parameters for the randomdistributed encoder
return the overlap between bucket indices i and j
get the logger for this object
implement the iterator protocol
return the pycapnp proto type that the class uses for serialization
validates control dictionary for the nupic engine context
two-gram model constructor
returns log(exp a - exp b a and b are numpy arrays values in a should be
add a single field to the dataset
initialize field using relevant encoder parameters
helper function there are three different ways of thinking about the representation
returns list of output names in spec
[encoder class virtual method override]
[encoder class virtual method override]
reset the sensor to beginning of data
consruct an instance the instance's open() method must be
@return trace trace of sequence labels
utility function that saves the passed in groundtruth into a local history buffer and returns the groundtruth from self
returns a bookmark to the current position
given two tm instances list the difference between them and returns false if there is a difference
return the best model id and it's errscore from the given sprint
updates the minimum duty cycles the minimum duty cycles are determined
perform the main computation this method is called in each iteration for each phase the node supports
log 'msg % args' with severity 'critical'
handle one compute possibly learning
returns plot of the cell activity note that if many timesteps of
support for the iterator protocol return itself
set the random seed and the numpy seed
returns the number of cells in this layer
sort in memory chunk of records records - a list of records read from the original dataset
look through the jobs table and reactivate all that are already in the running state by setting their _eng_allocate_new_workers fields to true
return the spec for spregion
for use only by nupic scheduler also known as clientjobmanager look through the jobs table and see if any new job requests have been
returns count of all lines in dataset including header lines
return the current active state this is called by the node to
get the predicted segment decrement
@return trace trace of predicted => inactive columns
sets the permanence trim threshold
prints all available results in the given hypersearch job and emits model information to the permutations report csv
:returns list field names associated with the data
[virtual method override] emits a single prediction as input versus predicted
close the policy instance
unwraps self __rawinfo results and caches it in self __cachedresults
fetch jobids for jobs in the table with optional fields given a
maps a column to its input bits this method encapsulates the topology of
run one iteration of spregion's compute
flushes the file
return index of the field matching the field meta special value
unittest testcase assertequal override adds extra log items to msg
this method takes a list of labels and returns a unique category number
sets the predicted segment decrement
see the function description in base py
returns whether global inhibition is enabled
generate a record each value is stored in its respective field
returns 3 things for a vector * the total on time
abbreviate the given text to threshold chars and append an ellipsis if its
generate a non overlapping coincidence matrix this is used to generate random
get parameter value
make a two-dimensional clone map mapping columns to clone master
returns radius for given speed
determines which cells in a predicted column should be added to winner cells list and learns on the segments that correctly predicted this column
generate a set of hub sequences these are sequences which contain a hub
get the label for the metric being optimized this function also caches
this method goes through a list of segments for a given cell and deletes all synapses whose permanence is less than minpermanence and deletes
flush the file to disk
@return countstrace a new trace made up of cumulative counts of this trace's indices
@internal set the state of ourself from a serialized state
like _getmatchingrowsnoretries(), but with retries on transient mysql
returns the number of records that elapse between when an inference is made and when the corresponding input record will appear
returns the top w coordinates by order
deletes temporary system objects/files
:param verbosity integer controlling extent of printouts for debugging
return the best model id and it's errscore from the given sprint which may still be in progress
choose a new position based on results obtained so far from other particles and the passed in globalbestposition
find weakly activated cell in column returns index and segment of most
returns the index of the record that will be read next from
gets a value of w for use in generating a pattern
validate a python value against json schema validate value schemapath
generate multiple records refer to definition for generaterecord
generate the aggregated output record
initialize prngs that may be used by other modules in the experiment stack
modify the data in place adding noise
translate an index into coordinates using the given coordinate system
return the list of labels for the metrics that are being calculated
set the value of the parameter
run a bunch of iterations on a permutevar and collect which positions were visited
returns a tuple successcode recordsarray where successcode - if the stream had enough records to return true/false
get the sensor input element that corresponds to the given inference element
validate a python object against an opf json schema file target target python object to validate typically a dictionary
writes serialized data to proto object
this function will replace checkprediction
returns the verbosity level
get the value of a parameter
get the initial permanence
[virtual method override] save a checkpoint of the prediction output stream
deletes all rows from the table if any data was found
write obj instance to cap'n proto object note this is an abstract method
give the timestamp of a record a datetime object compute the record's timestamp index - this is the timestamp divided by the aggregation period
loads custom configuration settings from their persistent storage
from http //book opensourceproject org cn/lamp/python/pythoncook2/opensource/0596007973/pythoncook2-chp-19-sect-9 html
returns a concatenated list of both the standard base class ephemeral members as well as any additional ephemeral members
insert a new unique model based on params into the model table in the "running" state
returns the cumulative n for all the fields in the dataset
generates centre offsets and spread offsets for block-mode based training regimes - star cross block
add an entry to the jobs table for a new job request this is called by
removes the given records from the classifier
see method description in base py
@doc place_holder network addregion
compute each segment's number of active synapses for a given input
return the inference value from one input sample the actual
returns last non-none element in the list or none if all are none
initilize nupic logging by reading in from the logging configuration file the
encodes the given input row as a dict with the keys being the field names
@doc place_holder network link
closes connect to output store and cleans up any resources associated
log 'msg % args' with the integer severity 'level'
capnp serialization method for the anomaly likelihood object
accepts log-values as input exponentiates them computes the sum then converts the sum back to log-space and returns the result
change the status on the given job parameters
given a bucket index return the list of non-zero bits if the bucket
get a record from the datasource and encode it
returns the nth record
returns a numpy array containing the sub-field scalar value s for each sub-field of the inputdata
get field metadate information for inferences that are of dict type
signal that the input record is the start of a new sequence
creates ndesirednewsynapes synapses on the segment passed in if possible choosing random cells from the previous winner cells that are
set the value of the parameter
encodes inputdata and puts the encoded value into the numpy output array which is a 1-d array of length returned by :meth
helper function for querying the models table including relevant job info where the job type matches the specified jobtype
generate and return the following encoder related substitution variables encoderspecsstr
metric for number of sequences each predicted => active cell appears in note this metric is flawed when it comes to high-order sequences
returns a closure suitable for use as function/method decorator for retrying a function being decorated
called by finalizelearning() this will project all the patterns onto the
cancel the given job this will update the cancel field in the
creates the inference output directory for the given experiment experimentdir experiment directory path that contains description
generates a random sample from the discrete probability distribution and returns its value the log of the probability of sampling that value and the
pretty print a pattern
returns list of default traces to be overridden
compute/update and return the positive activations duty cycle of this segment
translates the given metrics value to json string metrics a list of dictionaries per opftaskdriver
this method ensures that each column has enough connections to input bits to allow it to become active
initialize the dataset generator with a random seed and a name
returns next available data record from the storage if usecache is
sets the iteration number
return the interal _topdownmappingm matrix used for handling the bucketinfo() and topdowncompute() methods
updates the duty cycles for each column the overlap duty cycle is a moving
return the set of pattern numbers that match a bit
returns the potential mapping for a given column 'potential' size
returns true if all records have been read
return info on all of the models that are in already in the models table for a given job
advance to the next iteration cycle phase
helper function check if the settings are reasonable for sp to work
iterates through stream to calculate total records after aggregation
intercept the _readstdconfigfiles call from our base config class to read in base and custom configuration settings
run one iteration of this model
place the given job in status_running mode the job is expected to be status_notstarted
saves the given _hypersearchjob instance's jobid to file
sets global inhibition
get the runtime statistics specific to the model
compute receiver operating characteristic roc note this implementation is restricted to the binary classification task
perform final activities including 'finish' callbacks this
initialize all ephemeral data members and give the derived class the opportunity to do the same by invoking the
model model instance niters number of iterations must be greater than 0
this method deletes all synapses where permanence value is strictly less than self
compute the saturation for a continuous level this breaks the level into
escape commas tabs newlines and dashes in a string
see nupic encoders base encoder for more information
choose n random cells to learn from
this method updates the permanence matrix with a column's new permanence values
generate two sets of sequences the first set of sequences is used to train
generates the metrics for a given inferencetype
generate description from a text description of the ranges
returns the input in the same format as is returned by :meth
chooses the best model for a given job
start a new hypersearch job and monitor it to completion
@param n int number of available bits in pattern @param w (int/list) number of on bits in pattern
activates all of the cells in an unpredicted active column chooses a winner cell and if learning is turned on learns on one segment growing a new
returns a closure suitable for use as function/method decorator for logging entry/exit of function/method
return a dict of the errors obtained on models that were run with each value from a permutechoice variable
return a closure suitable for use as a decorator for retrying a pymysql dao function on certain failures that warrant retries
clear all custom configuration settings and delete the persistent custom configuration store
emit a input/prediction pair if possible
sets the boost factors for all columns 'boostfactors' size must match
requests a job to be suspended note this is primarily for suspending production jobs do not use
remove entries with 0 likelihood or likelihood less than minlikelihoodthreshold but don't leave an empty dict
turn learning on for the current model
protected method that is called during serialization with an external directory path
adds a new segment on a cell
construct a variable that permutes over floating point values using the particle swarm optimization pso algorithm
sets the duty cycle period
special row id is 0xff ffff ffff ffff ffff 9 bytes of 0xff
see method description in base py
get what the inferred value for this field was
resolves the referenced value if the result is already cached
store a dated potential segment update the "date" iteration index is used
add a delta field to the data
@doc place_holder network disableprofiling
return isint intvalue for a given floating point number
returns current minimum value for the field 'fieldname'
close the stream
advances the iteration returns true if more iterations remain false if this is the final
writer non-temporal prediction log writer conforming to predictionwriteriface interface
[virtual method override] returns the sequence of fieldmetainfo objects specifying this
removes trailing whitespace on each line
loads custom configuration settings from their persistent storage
pretty print a numpy matrix using the given format string for each value
method which returns a dictionary of field statistics received from the input source
retrieves the runner's _hypersearchjob instance note only available after run()
creates and returns the _iterationphase-based instance corresponding
get a parameter of the anomaly classifier within this model
set the state of ourself from a serialized state
constructor - initialize the internal engine_internal
returns a list of all known feature groups
report usage error and exit program with error indication
setting this to true freezes the state of the encoder this is separate from the learning state which affects changing parameters
computes the raw anomaly score
return reasonable min/max values to use given the data
retrieves the current results and updates the model's record in the model database
see the function description in base py
sets the number of active columns per inhibition area invalidates the
get the value of the parameter
this is the primary public method of the spatialpooler class this
calls runtimeelement interpret expression and wraps the result
[overrides nupic encoders scalar scalarencoder topdowncompute]
return the best score and position for a given particle the position
model model instance niters number of iterations must be greater than 0
returns the permanence values for a given column 'permanence' size
:returns int count of data rows in dataset excluding header lines
see method description in base py
there are two caveats first this is a potentially slow operation second
generate the string that defines the permutations to apply for a given encoder
see the function description in base py
return the result from dividing two dicts that represent date and time
[override] turn learning off for the current model
this method deletes all synapses whose permanence is less than minpermanence and deletes any segments that have less than
@doc place_holder network resetprofiling
adds partition id for pattern index
returns a tuple of dataset field metadata descriptors that are arranged in the same order as the columns in the dataset
dimensions of the region
used for batch scenarios this method needs to be called between learning
see method description in :meth ~ nupic encoders base encoder getscalars
add multiple fields to the dataset
reads deserialized data from proto object
returns plot of the cell activity
writes serialized data to proto object
returns instance of the underlying sdrclassifier algorithm object
compute the new metrics values given the next inference/ground-truth values parameters
construct a dimensions object
find this cell's segment that was least recently used
fetch jobids for jobs in the table with optional fields given a
model model instance
phase 2 for the inference state the computes the predicted state then
update the state in the job record with our local changes if any
does a bitwise compare of the two bitmaps and returns a fractonal value between 0 and 1 of how similar they are
[virtual method override] this method is called during serialization with an external directory path that can be used to bypass pickle for saving
sets the connected permanence
module imported description py module
initializes internal filter variables for further processing
read state from proto object
@returns the average number of synapses per segment
saves multiple records in the underlying storage
construct an aggregator instance params
run one iteration of tmregion's compute profiling it if requested
parse the given xml file and store all properties it describes
see method description in base py
set the read timeout
accepts log-values as input exponentiates them sums down the rows first dimension then converts the sum back to log-space and returns the result
returns a dictionary of arguments for dbutils steadydb steadydbconnection
returns a range of records starting from the bookmark if 'bookmark'
returns next available data record from the file
see the function description in base py
this is the primary method of the class that will return true or false based on the current environment and user
delete all models from the models table
parses a string containing only 0's and 1's and return a python list object
store a training sample and associated category label
returns a state's anomaly vertor converting it from spare to dense
pick up the latest search from a saved jobid and monitor it to completion
computes the amount of time if any to delay the run of this model
set parameter value
returns the dimensions of the columns in the region
create a particle
overrides the python logging facility's handler handleerror function to
@doc place_holder network save
return true if our local copy of the state has changed since the last time we read from the db
returns a list of :class encoderresult namedtuples describing the inputs
run one iteration of this model
gets a logger for the given class in this module
get the needed length for a list to hold a value for every segment's flatidx
return our connection id this can be used for worker identification
add an entry to the jobs table for a new job request but only if the same job by the same client is not already running
log 'msg % args' with severity 'error'
this is the first portion of every sub-experiment params file we generate between
@return trace trace of predicted => active columns
get the segment with the specified flatidx
create and start a swarm job
set the state of ourself from a serialized state
set the value of the parameter
explicitly run inference on a vector that is passed in and return the category id
[context manager protocol method] release resources
see nupic encoders base encoder for more information
return the total number of synapses
replaces the iteration cycle phases
create a new model instance given a description dictionary
generate the initial first order and second order transition probabilities for 'model2'
returns list of input names in spec
implement this method to provide the pattern machine
todo describe filterdict schema
base class constructor performs common initialization parameters
find this segment's synapse with the smallest permanence
updates the permanence for a synapse
returns the indices of the predictive cells
returns the index of the record that will be read next from
delete the stored checkpoint for the specified modelid this function is
process the consumption a club - skip the header line
read state from proto object
return the nodespec for this pynode
shift the model result and return the new instance
initialize the bucket map assuming the given number of maxbuckets
:returns true if p is a valid estimator params as might be returned by estimateanomalylikelihoods() or updateanomalylikelihoods,
set state from serialized state
returns a concatenated list of both the standard base class ephemeral members as well as any additional ephemeral members
@doc place_holder network getcallbacks
see comments in base class
returns mode evaluation end time
clear out the entire configuration
this is usually used to display a histogram of the on-times encountered in a particular output
see comments in base class
see the function description in base py
generates a file by applying token replacements to the given template file
parse the given xml file and store all properties it describes
accumulate a list of values 'values' into the frequency counts 'freqcounts', and return the updated frequency counts
convenience method to compute a metric over an indices trace excluding resets
returns sensor region's encoder that is sent only to the classifier
retrieve the requested property and return it as an int if property
returns reference to the network's sensor region
clear all configuration properties from in-memory cache but do not alter the custom configuration file
change the values of 1 or more fields in a job here 'fields' is a
note if you set the kwarg "mmname", then pretty-printing of traces and metrics will include the name you specify as a tag before every title
loads a json value from a file and converts it to the corresponding python object
retrives the optimization key name and optimization function
generate a new and unique representation returns a numpy array
run one iteration of the region's compute
compute the anomaly score as the percent of active columns not predicted
[encoder class virtual method override]
a segment is active if it has >= activationthreshold connected synapses that are active due to infactivestate
retrieve the requested property and return it as a float if property
return the degree of overlap between an input pattern and each category stored in the classifier
get the maximum number of segments per cell
like 'neighborhood', except that the neighborhood isn't truncated when it's near an edge
get the logger for this object
return the position of a particle given its state dict
called at the end of learning and inference this routine will update a number of stats in our _internalstats dictionary including our computed
generate a set of possible report keys for an experiment's results
returns the next record from the dataset the returned record object
saves specified error in the storage
set the state of ourself from a serialized state
return our moving average of learned sequence length
return serializable state this function will return a version of the
return the current state of this particle this is used for
update a set of synapses in the segment
emit model info to csv file
return a list of possible encoder parameter combinations for the given field and the default aggregation function to use
requestedactivities a sequence of periodicactivityrequest elements
creates a new synapse on a segment
compute the probability that the current value plus anomaly score represents an anomaly given the historical distribution of anomaly scores
translate parameters and initialize member variables
@doc place_holder region compute
returns the potential percent
look through the models table for an orphaned model which is a model that is not completed yet whose _eng_last_update_time is more than
periodic check to see if this is the best model this should only have an
do one iteration of top-down inference
returns whether or not the object is a string
override of getstats() in basestatscollector
initialize the random seed
load saved model
see the function description in base py
returns the indices of the active cells
save the model in the given directory
get the default arguments from the function and assign as instance vars
not implemented csv file version does not provide storage for the error
handle legacy options temporary
return true if no aggregation will be performed either because the aggregationinfo was none or all aggregation params within it were 0
main loop of the opf model runner
returns the shared cjdao clientjobsdao instance
- backs up old report csv file - opens the report csv file in append or overwrite mode (per
gets the current metric values returns a dictionary where each key is the metric-name and the values are
print up to maxcols number from a flat floating point array
if state is allocated in cpp copy over the data into our numpy arrays
this method will add the record to the knn classifier
log 'msg % args' with severity 'warning'
initialize class properties from stored values
get a connection instance
run final activities after a model has run these include recording and
see the function description in base py
[_iterationphase method implementation] performs initialization that is necessary upon entry to the phase
samples n rows
returns true if any element of the sequence satisfies true
sets the autodetectthreshold
train the classifier to associate specified input pattern with a particular category
this creates an experiment directory with a base py description file
add the aggregation period to the input time t and return a datetime object years and months are handled as aspecial case due to leap years
creates and returns a periodicactivitymgr instance initialized with
validate a python value against json schema validate value schemapath
set a single custom setting and persist it to the custom configuration store
returns the index of the pattern that is closest to inputpattern the distances of all patterns to inputpattern and the indices of the k
return total number of models that completed
[virtual method override] returns a tuple of dataset field metadata descriptors that are
convert a list of sequences of pattern indices and a pattern lookup table into a an array of patterns
returns the task instances of the experiment description
edits the xml configuration file with the parameters specified by
semi-private method for retrieving the job-specific params parameters
:returns index of the 'reset' field none if no such field
create and seed random number generator
called to indicate the start of a new sequence
constructor taskcontrol dictionary conforming to opftaskcontrolschema
removes the set of columns who have never been active from the set of active columns selected in the inhibition round
given a series of anomaly scores compute the likelihood for each score this
creates the model's predictionlogger object which is an interface to write
given two tm instances list the difference between them and returns false if there is a difference
return the field names for each of the scalar values returned by getscalars
returns the next n values for the distribution as a list
converts an int to a packed byte array with left most significant byte
perform recursive latin hypercube sampling
converts a byte array into an integer
processes the given record according to the current phase inputrecord record object formatted according to
get the actual value for this field
@param categoriesout -- the maximum number of distinct category labels that can be learned
return a list of namedtuples from the result of a join query a
return the absolute path of the model's pickle file
return the pycapnp proto type that the class uses for serialization
emits final metrics
returns the synapses for the source cell that they synapse on
instantiate the hyperseachv2 instance
returns flags for all fields
sets the permanence increment amount for active synapses
read state from proto object
debugging/profiling utility method to allow tools to simulate the presentation of training sample
return the model id of the model with the best result so far and it's score on the optimize metric
return the autodetectwaitrecords
initialize svm engine use the swig bindings to initialize an instance of an svm classifier engine
deprecated use @ref runwithconfig
resumes processing of an existing job that is presently in the status_completed state
save cells4 state to this file
returns true if the inference type is 'temporal', i e requires a
emits final metrics
retrieve the requested property and return it as a float if property
@doc place_holder network removeregion
return the inference state of the current model
return a list of swarm generations that have completed and the best minimal errscore seen for each of them
encoder class virtual method override
returns instance of the underlying claclassifier algorithm object
print a floating point array that is the same shape as activestate
saves the record in the underlying storage
get what the inferred value for this field was
set a single custom setting and persist it to the custom configuration store
return serializable state this function will return a version of the
filepath path of file where sp __init__ args are to be saved
this function determines each column's overlap with the current input vector
adds a value over a range of rows
spec of the region
returns traceback namedtuple for our caller's caller
return the list of all swarms in the given sprint
see the function description in base py
helper method that generates a unique label for a metricspec / inferencetype pair
[virtual method override] resets the model's sequence states normally
returns the data for a synapse
compute anomaly score if required
see method description in base py
return a numpy array predictedcells representing the current predicted state
returns a segment object of the specified segment using data from the self
reclassifies all internal state
get the permanence increment
write inputs to output tap file
place the model in a permanent "finished learning" mode
generates a lookup dictionary of permutation variables whose values are too complex for labels so that artificial labels have to be generated
consruct an instance the instance's open() method must be
non-equality operator for connections instances
return number of models that completed with errors
see nupic encoders base encoder for more information
prediction function for determining whether a function is a categorical variable or a scalar variable
