construct the filter parameters
testregion command that returns the identity policy instance that was associated with this testregion instance via setidentitypolicyinstance()
add labels from the anomaly classifier within this model
see the function description in base py
get a specific item by name out of the results dict
semi-private method for retrieving the jobid
returns the number of synapses
see comments in base class
inspects the control task and updates any stream sources it finds that are not absolute paths into paths generated by pkg_resources relative to
returns the indices of the cells passed in
@internal return the random number state
returns the index of the column that a cell belongs to
testregion command that sets identity policy instance the instance
update the inference active state from the last set of predictions and the current bottom-up
performs inhibition this method calculates the necessary values needed to
(from backtracking_tm py)
updates synapses on segment
turn inference off for the current model
returns storage stats like min and max values of the fields
this function gives the future predictions for <nsteps> timesteps starting from the current tm state
@param c column index @param i cell index in column
non-equality operator for temporalmemory instances
sets the permanence increment
generate the simulated output from a spatial pooler that's sitting on top of another spatial pooler / temporal memory pair
emits a set of inputs data inferences and metrics from a model resulting from a single record
marks the stream completed true or false
return the total # of models we have in our database if swarmid is none or in a specific swarm
emits periodic metrics metrics a list of prediction_metrics_manager
remove labels from each record with record rowid in range from start to end noninclusive of end
generate a set of records reflecting a set of probabilities
return a pattern for a number
this gets called on every compute it determines if it's time to
returns the permanence increment amount for columns that have not been
generates requested number of records and saves in a csv file
returns the next value of the disribution using knowledge about the current state of the distribution as stored in numvalues
returns all field names
sets the current model as orphaned this is called when the scheduler is
similar to cross(), but generates output dictionaries instead of tuples
pick a value according to the provided distribution
return the name of the region
perform an internal optimization step that speeds up inference if we know learning will not be performed anymore
makes directory for the given directory path if it doesn't already exist in the filesystem
returns the index of the cell
fetch the values of 1 or more fields from a sequence of job records
sets the permanence increment amount for columns that have not been
capnp deserialization method for the anomaly likelihood object
check for concurrency violation and add self to _clsoutstandinginstances
re- initialize the loging directory for the calling application that uses initlogging() for logging configuration
set the current state of this particle this is counterpart to getstate
update the boost factors for all columns the boost factors are used to
this causes the variable to jiggle away from its current position
gets a neighborhood of inputs
return the list of active swarms in the given sprint these are swarms
return a dict that can be used to create an anomaly model via opf's modelfactory
get the runtime statistics specific to the model
change the given swarm's state to 'newstate' if 'newstate' is
modify the data in place adding noise
generate the initial first order and second order transition probabilities for 'model0'
returns a string representing a numpy array of 0's and 1's
returns the activity duty cycles for all columns 'activedutycycles'
returns the sdr for jth value at column i
gets the value of a given field from the input record
takes a record and returns true if record meets filter criteria
disables learning in the htmpredictionmodel's temporal pooler while retaining the ability to re-enable tm learning in the future
updates the final aggregated score error given the prediction and the ground truth
return {value : <current measurement>, "stats" : {<stat> : <value>
helper function used by averageontimepertimestep 'durations' is a vector
update the results string and last-update-time fields of a model
enable the diagnostic feature for debugging unexpected concurrency in acquiring connectionwrapper instances
run the given model
run this worker
pretty-print the encoded output using ascii art
sets the update period
public api for returning the category list
pretty print the connections in the temporal memory
save the current metric value and see if the model's performance has 'leveled off
search the configuration path (specified via the nta_conf_path environment variable) for the given filename
sets the autodetectwaitrecords
this method will be called only when the node is used in nupic 2
initialize all ephemeral members after being restored to a pickled state
computes the average on-time of the outputs that are on at each time step and then averages this over all time steps
resets stats collected so far
pretty print a sequence
returns an array of length size and type dtype that is everywhere 0 except in the indices listed in sequence pos
auto forwarding of properties to get methods of internal region
returns the model creation parameters based on the settings in the config dictionary
converts the control element from nupic format to a default opf format with 1 task
:returns a sequence of :class ~ fieldmetainfo
see the function description in base py
performs initialization that is necessary upon entry to the phase must
set the read timeout in seconds int or floating point
loads all the parameters for this dummy model for any paramters
generate stream definition based on
computes the number of models that are expected to complete as part of this instances's hypersearch
populate the output array with the category indices
set the radius resolution and range these values are updated when minval
initialize all ephemerals used by derived classes
add the label labelname to each record with record rowid in range from start to end noninclusive of end
adds a histogram to the plot's figure
see nupic encoders base encoder for more information
return the metric value
this is the tail of every params file we generate between the head and the tail
sets which metrics should be written to the prediction log
set the value of a spec parameter most parameters are handled
translate coordinates into an index using the given coordinate system
@returns moving average of learned sequence length
recursively applies f to the values in dict d
returns the overlap duty cycles for all columns 'overlapdutycycles'
instantiate the hypersearch worker parameters
generate and return the following encoder related substitution variables encoderspecsstr
return a dict containing all of the configuration properties parameters
sets the max new synapse count
return the default spatial pooler implementation for this region
returns true if the inference from this timestep is predicted the input for the next timestep
allows ids to be assigned a category and subsequently enables users to use - :meth ~
returns an array of length size and type dtype that is everywhere 0 except in the index in pos
choose a new position that is as far away as possible from all 'othervars', where 'othervars' is a list of permutevariable instances
the range of connectedsynapses per column averaged for each dimension
compute the number of eigenvectors singularvalues to keep
test that we can converge on the right answer
sets the activation threshold
return the total number of segments
raises nupicjobfailexception by mapping from another exception that is being handled in the caller's scope and preserves the current exception's
force a switch back to learning mode not normally supported
disable writing of output tap files
public api for returning the category list this is a required api of the nearestneighbor inspector
creates and returns the _iterationphase-based instance corresponding
search the configuration path (specified via the nta_conf_path environment variable) for the given filename
creates an iterator that returns modelinfo elements for the given modelids warning the order of modelinfo elements returned by the iterator
convert a list of numbers to a string of space-separated numbers
generate requested statistics for a dataset and cache to a file
return a numpy array containing the complete set of hyperplanes used by the trained svm classifier
loads a json value from a file and converts it to the corresponding python object
see method description in base py
should return the output width in bits
train an svm model
get the collection of regions in a network this is a tricky one
returns the synapses on a segment
extracts the predicitonkind temporal vs nontemporal from the given
returns the permanence decrement amount for inactive synapses
[virtual method override] this method is called during deserialization (after __setstate__) with an external directory path that can be used to
translate parameters and initialize member variables specific to backtracking_tm py
build the additional specs in three groups for the inspector use the type of the default argument to set the spec type defaulting
@return trace trace of predicted => active cells
initialize tables if needed parameters
get the particle state as a dict this is enough information to
converts a category number into a list of labels
return the list of swarms in the given sprint that were not killed
writer non-temporal prediction log writer conforming to predictionwriteriface interface
return the learning state of the current model
generates the non-default metrics specified by the expgenerator params
@doc place_holder network initialize
generate the filename for aggregated dataset the filename is based on the input filename and the
get the maximum number of synapses per segment
process one input sample
returns the periodic checks to see if the model should continue running
:param network nupic engine network
get cap'n proto schema note this is an abstract method
emits a set of inputs data inferences and metrics from a model resulting from a single record
updates a duty cycle estimate with a new value this is a helper
feeds input record through tm performing inference and learning
initializes the permanences of a column the method
called when the stream is completed
put us back at the beginning of the file again)
@todo implement this it is used by the node's getparameter() call
return the generation index of the first generation in the given swarm that does not have numparticles particles in it either still in the
release database connection and cursor passed as a callback to
instantiate a metricsiface-based module
the main function of the hypersearchworker script this parses the command
return serializable state this function will return a version of the
see comments in base class
returns an array of length size and type dtype that is everywhere 0 except in the indices listed in sequence pos
metric a metric type name that identifies which metrics module is to be constructed by the metrics factory method
recursively updates the values in original with the values from updates
performs initial setup activities including 'setup' callbacks this
encode a record as a sparse distributed representation
returns list of default metrics to be overridden
requestedactivities a sequence of periodicactivityrequest elements
return the position of this particle this returns a dict() of key
see the function description in base py
recursively copies a dict and returns the result
decorator for specifying tests that only run when --long is specified
find the user's "documents" directory os x "my documents" directory windows or home directory unix
return a sequence of matching rows with the requested field values from a table or empty sequence if nothing matched
@returns the total number of segments
fetch the values of 1 or more fields from a sequence of model records
initialize the random seed
parses and validates the --descriptionfromfile option and executes the
read the data out of the given category file returning a tuple categorycount listofcategories
translate parameters and initialize member variables specific to backtracking_tm py
patch __getattr__ so that we can catch the first access to 'cells' and load
run one iteration of spregion's compute profiling it if requested
list of our member variables that we don't need to be saved
prints a listing of experiments that would take place without actually executing them
retrives a dictionary of metrics that combines all report and optimization metrics
context guard - exit ensures that the file is always closed at the end of the 'with' block
just return the inference value from one input sample the actual
mangles the given mangled private member name a mangled member name is one whose name begins with two or more underscores and ends with one
returns the segments that belong to a cell
generates the clientjobs database name for the current version of the database "semi-private" class method for use by friends of the class
process one input sample
returns a dict of all temporary values in custom configuration file
return serializable state this function will return a version of the
takes an encoded output and does its best to work backwards and generate the input that would have generated it
writes out a new combined file containing weather data
trivial init method that just calls base class's __init__() this method is attached to classes that don't define __init__()
stores the current model results in the manager's internal store
return the total number of segments in cell c i
this "backtracks" our inference state trying to see if we can lock onto the current set of inputs by assuming the sequence started up to n steps
print out what test we are running
convert a database internal column name to a public name this
returns model completion message
delete's the output cache associated with the given modelid this actually
return a dict containing all custom configuration properties parameters
return index of the 'timestamp' field
return the classified labeling of record
construct a _htmclassificationrecord based on the current state of the htm_prediction_model of this classifier
constructor label a distinguishing string that will be used to distinguish
creates the required metrics modules
:returns a list containing unique non-none partition ids just the keys
recursively applies f to the values in dict d
returns an array of field names associated with the data
return a dict that can be used to construct this encoder this dict
fetch the values of 1 or more fields from a job record here 'fields'
add noise to the given input
see the function description in base py
intercept the _readstdconfigfiles call from our base config class to read in base and custom configuration settings
is called once by nupic before the first call to compute()
returns the number of output elements
returns combined data from all sources values only
returns true if the inference from this timestep is predicted the input for the next timestep
potentially change the minval and maxval using input
retrieve the requested property and return it as a bool if property
loads a description file and returns it as a module
constructor streamdefdict stream definition as defined in
init all of our variable positions velocities and optionally the best result and best position from the given particle
compute a log scale representation of the likelihood value since the
returns list of all ephemeral members
initialize a field with various parameters such as n w flag datatype encodertype and tag predicted field
change the value of 1 field in a job to 'newvalue', but only if the current value matches 'curvalue'
close the policy instance and its shared database connection
return the index of a cell in this column which is a good candidate for adding a new segment
returns the metadata specifying the format of the model's output
returns a dictionary of permutation variables
@returns the total number of cells
find weakly activated cell in column with at least minthreshold active synapses
wraps getrow() such that instances may be indexed by columnindex
returns reference to the network's tm region
get the results string and other status fields for a set of models
look through the jobs table and get the demand - minimum and maximum number of workers requested if new workers are to be allocated if there
returns next available data record from the storage as a dict with the keys being the field names
serializes model using capnproto and writes data to checkpointdir
print out a banner
@param segment object segment object that the synapse is synapsed to
runs the given opf task against the given model instance
helper function to create a logger object for the current object with
@returns the total number of synapses
returns all the records
[_iterationphase method implementation] performs initialization that is necessary upon entry to the phase
returns information about the distribution of segments synapses and permanence values in the current tm
initialize all ephemerals used by derived classes
returns the learning iteration number
returns information about the distribution of segments synapses and permanence values in the current tm
return the distances from inputpattern to all stored patterns
this method overrides valuegetterbase's "pure virtual" method it
calculate distances in the original input space pre-svm post-pca
return the value of skiprecords for passing to estimateanomalylikelihoods if windowsize is very large bigger than the amount of data then this
list of attributes to not save with serialized state
punishes the segments that incorrectly predicted a column to be active
get the labels on classified points within range start to end not inclusive
metricmulti constructor using metricspec is not allowed
enables learning in the htmpredictionmodel's temporal pooler see also htmpredictionmodelcontroldisabletplearningcb
return the offset and length of a given field within the encoded output
adds an image to the plot's figure
finds the category that best matches the input pattern returns the
resets stats collected so far
print the list of [column cellidx] indices for each of the active cells in state
returns the permanence increment amount for active synapses
constructor fields a non-empty sequence of nupic
reset the state of all cells
[overrides nupic encoders scalar scalarencoder getbucketindices]
get all info about a job parameters
@return trace trace of predicted => inactive cells
disables learning in the htmpredictionmodel's spatial pooler while retaining the ability to re-enable sp learning in the future
starts hypersearch as a worker or runs it inline for the "dryrun" action
update boost factors when local inhibition is used
run a named function specified by a filesystem path module name and function name
utility function to get information about function callers the information is the tuple (function/method name filename class)
return the base spec for testregion
resets stats collected so far
returns instance of the underlying spatialpooler algorithm object
generates a "persistentjobguid" value
get the permanence decrement
returns the duty cycle period
see method description in base py
retrives a dictionary of metrics designated for report parameters
@param monitor monitormixinbase monitor mixin instance that generated
returns an array containing the sub-field bucket indices for each sub-field of the inputdata
print the parameter settings for the tm
return a dict containing all of the configuration properties parameters
compute the learning active state given the predicted state and the bottom-up input
this "backtracks" our learning state trying to see if we can lock onto the current set of inputs by assuming the sequence started up to n steps
retrieve the requested property as a string if property does not exist
for all models that modified their results since last time this method was called send their latest results to the hypersearch implementation
@doc place_holder region getspecfromtype
list available checkpoints for the specified experiment
serialize python list object containing only 0's and 1's to string
get the value of a parameter most parameters are handled automatically by
not implemented csv file is always considered completed
update the inference state called from compute() on every iteration
returns the cumulative w for all the fields in the dataset
returns the max new synapse count
map category indices internal to category ids external
see nupic encoders base encoder for more information
sets the potential percent
create our state object
return list of models ids that completed with errors
return the inference value from one input sample the actual
return the list of labels for the metrics that are being calculated
open the data file and write the header row
return the list of paths to search for configuration files
equivalent to the category inference of zeta1 toplevel
unittest testcase assertfalse override adds extra log items to msg
returns a sequence of nupic data fieldmeta fieldmetainfo
:returns int the index of the record that will be read next from :meth ~
return a single matching row with the requested field values from the the requested table or none if nothing matched
returns a randomly generated permanence value for a synapses that is initialized in a connected state
return a json representation of obj with sorted keys on any embedded dicts
reset the learning and inference stats this will usually be called by
perform cross-validation to measure the recognition accuracy of an svm
returns a subset of the keys that match any of the given patterns
validates control dictionary for the experiment context
write state to proto object
test to see if the metrics manager correctly shifts records for multistep
return true iff checkpointdir appears to be a checkpoint directory
given a synapse and a list of synapses check whether this synapse exist in the list
returns list of all ephemeral class members
generate the initial first order and second order transition probabilities for 'model1'
setup our resultsperchoice history based on the passed in resultsperchoice
see comments in base class
initialize ephemeral instance variables (those that aren't serialized)
merge sorted chunk files into a sorted output file
there are two caveats first this is a potentially slow operation second
grows the histogram to have rows rows and cols columns
override this method to set the default tm params for self tm
generate and log a 32-bit compatible seed value
convert datetime timedelta to seconds in floating point
this method is called during network serialization with an external filename that can be used to bypass pickle for saving large binary states
get labels from the anomaly classifier within this model
see comments in base class
activity tick handler services all activities
see method description in base py
removes any stored records within the range from start to end
(from backtracking_tm py)
get the sensor input element that corresponds to the given inference element
get serializable state
update the results string and/or num_records fields of a model
calculate error signal
@param connections object connections for the tm
removes any update that would be for the given col cellidx segidx
returns the task instances of the experiment description
returns the stability for the population averaged over multiple time steps
network for the region
generate a set of simple and hub sequences a simple sequence contains
updates counter instance variables each round
returns a description of the dataset
[context manager protocol method] permit a connectionwrapper instance to be used in a context manager expression (with
returns the indices of the winner cells
given an encoder name get the key
creates the model's predictionlogger object which is an interface to write
initialize the encoders
this method increases the permanence values of synapses of columns whose activity level has been too low
returns list of all ephemeral members
implemented in backtrackingtmcpp backtrackingtmcpp loadfromfile
[scalarencoder class method override]
get the beginning part of the database name for the given database version
return a pretty print string representing the return value from :meth
@return trace trace of # synapses
function for running binary search on a sorted list
return serializable state this function will return a version of the
same as writerecord above but emits multiple rows in one shot
niter number of iterations to remain in this phase
set the value of the parameter
starts a swarm given a path to a json file containing configuration
add 'value' to the field i
returns the number of segments
form of distribution must be an array of counts in order of self keys
create a checkpoint from the current model and store it in a dir named
returns sensor region's encoder for the given network
choose a new position based on results obtained so far from all other particles
returns the nth encoding
creates a fieldmetainfo list from the a list of tuples basically runs
used as optparse callback for reaping a variable number of option args
returns descriptions for all fields
do one iteration of inference and/or learning and return the result parameters
:param sampledata :type sampledata numpy array
utility function for creating enumerations in python example usage
exports a network as a networkx multidigraph intermediate representation suitable for visualization
return all the prototype distances from all computes available
process the attendance data of one club if the club already exists in the list update its data
get the points in the neighborhood of a point
get the value of the parameter
construct a _htmclassificationrecord based on the state of the model passed in through the inputs
return index of the 'category' field
add values to the field
retrieve the engine-level model params from a swarm model
returns the data for a segment
returns the metadata specifying the format of the model's output
generates a list of model names that are expected to complete as part of this instances's hypersearch
dictkeychain one or more strings the first string is a key that will eventually be defined in the dictionary that will be passed
enables learning in the htmpredictionmodel's spatial pooler see also htmpredictionmodelcontroldisablesplearningcb
load cells4 state from this file
for the given column return the cell with the fewest number of segments
insert a new entry or update an existing one if this is an update
returns the permanence amount that qualifies a synapse as
see comments in base class
generates set of consecutive patterns
gets fields from all models in a job that have been checkpointed this is
process one input sample
parse a string of space-separated numbers returning a python list
maps the coordinate to a bit in the sdr
return the inferencetype of this model
export all the records into a csv file in numenta format
process one input sample
see the function description in base py
initialize all ephemeral members after being restored to a pickled state
implement this method to specify the temporal memory class
returns the classified labeling of record
[overrides nupic encoders scalar scalarencoder encodeintoarray]
returns the number of connected synapses for all columns
instantiates a _hypersearchjob instance from info saved in file
get the path of the custom configuration file
handles a "warning signal" from the scheduler this is received when the
context guard - enter
see nupic encoders base encoder for more information
get the connected permanence
sets the local area density invalidates the 'numactivecolumnsperinharea'
calculate dendrite segment activity using the current active cells
return the absolute path of the model's checkpoint file
apply pre-encoding filters
generates a random sample from the discrete probability distribution and returns its value and the log of the probability of sampling that value
@param connections object connections for the tm
log 'msg % args' with severity 'debug'
@return trace trace of predictive cells
see the function description in base py
hash a coordinate to a 64 bit integer
computes the predictive ability of a temporal memory tm this routine returns
returns a sequence of field types corresponding to the elements in the decoded output field array
check if the cancelation flag has been set for this model
back up a file
add the label labelname to each record with record rowid in range from start to end noninclusive of end
called at the end of inference to print out various diagnostic information based on the current verbosity level
not implemented csv file is always considered completed nothing to do
@param c column index
return the interal _topdownmappingm matrix used for handling the bucketinfo() and topdowncompute() methods
writes the results of one iteration of a model the results are written to
begin writing output tap files
see the function description in base py
remove labels from the anomaly classifier within this model
n is the total bits in input
writes the contents of this model's in-memory prediction cache to a permanent
returns the active segments
retrives a dictionary of metrics designagted for optimization parameters
return the spec for tmregion
@param cell int index of the cell that this segment is on
get the path of the custom configuration file
a list of row indices to remove there are two caveats first this is
returns most common value seen in the non-none elements of the list
get the value of the parameter
bring a top-level window with a given title
places the model in a permanent "finished learning" mode
return the best model id and it's errscore from the given swarm
updates the accumulated error given the prediction and the ground truth
for the given cell find the segment with the largest number of active synapses
get the beginning part of the database name for the current version of the database
change the status on the given job to completed parameters
gets the current metric values returns a dictionary of metric values
called from the scope of the region's pyregion initialize() method
return serializable state this function will return a version of the
adds one encoder
get the default arguments from the function and assign as instance vars
return the absolute path to the directory where the model's own "extra data" are stored (i
performs local inhibition local inhibition is performed on a column by
see comments in base class
sets the learning iteration number
represents the sum of the widths of each fields encoding
see comments in base class
returns the aggregation period of the record stream as a dict containing 'months' and 'seconds'
create a sdr classifier factory
passes the "finish learning" command to the model note upon completion
sets the path of the custom configuration file
returns true if all elements of the sequence satisfy true and x
returns the experiment description schema this implementation loads it in
@param numsequences int number of sequences to return
returns the min threshold
get a connection instance
:param outp file-like obj to which rendered graph is written (defaults to sys
copies a range of values to a new location in the data set
initialize this metric if the params contains the key 'errormetric', then that is the name of
like _getonematchingrownoretries(), but with retries on transient mysql
get the logger created by this subclass
set the queue of upcoming partition ids this can be used instead of the
handle the cla classifier compute logic when implementing multi-step prediction
get the labels of the previously computed record
the average number of columns per input taking into account the topology of the inputs and columns
flush the file to disk
deerializes model from checkpointdir using capnproto
tell the writer which metrics should be written
returns sum of the elements in the list missing items are replaced with
return a dict containing all custom configuration properties parameters
returns the update period
given the normal distribution specified by the mean and standard deviation in distributionparams return the probability of getting samples further
close the policy instance and its shared database connection
write state to proto object
incremet the value of 1 field in a job by increment the 'fieldname' is
return the distances between the input pattern and all other stored patterns
reset the velocity to be some fraction of the total distance this
returns the boosted overlap score for each column
this is a toy example to show the basic functionality the dataset is
sets the value that will be encoded when this region does a compute
@internal seed the random number generator
loads the experiment description file from the path
look through the jobs table and count the running jobs whose cancel field is true
parse the given xml file and return a dict describing the file
returns the inhibition radius
called by our unittest testcase assertxxxxxx overrides to construct a
list of our member variables that we don't need to be saved
retrieve the requested property as a string if property does not exist
returns a list of labels that correspond to metrics being computed
get current average
construct the filter parameters
returns true if the inference type is 'temporal', i e requires a
turn learning off for the current model
resumes processing of an existing job that is presently in the status_completed state
returns data types for all fields
destructor note this is not guaranteed to be called bugs like circular references could prevent it from being called
set whether learning is enabled
check whether any experiments failed in our latest hypersearch
returns current maximum value for the field 'fieldname'
sets the path of the custom configuration file
set flag for field at index flags are special characters such as 's' for
@return trace trace of active columns
set the random seeds helpful to make unit tests repeatable
helper function to return a scalar value representing the expected
change the category associated with this vector s
this method will remove the given records from the classifier
see if we can kill off some speculative swarms if an earlier sprint
hash a region
reclassifies given state
set the value of a parameter
initialize interpreter with blacklisted nodes removed from supported nodes
run scanning inference and store the results
returns the total number of inputs
accepts log-values as input exponentiates them sums down the rows first dimension normalizes then converts the sum back to
sets the inhibition radius
returns reference to the network's sp region
perform global inhibition performing global inhibition entails picking the
returns the percent of the outputs that remain completely stable over n time steps
returns pretty-printed table of traces
makes directory for the given directory path with default permissions
transfer the permanences from source to dest sp's this is used in test
add one instance consisting of ground truth and a prediction
returns args dictionary from the calling method
returns the dimensions of the columns in the region
see method description in base py
save a checkpoint of the prediction output stream the checkpoint
generate a coincidence matrix this is used to generate random inputs to the
periodically check to see if we should remove a certain field combination from evaluation because it is doing so poorly or move on to the next
@return trace trace of resets
seeks to numrecords from the end and returns a bookmark to the new position
function that compares two spatial pooler instances compares the
run one iteration of tmregion's compute
sets the initial permanence
verify the validity of the node spec object the type of each sub-object is verified and then
@internal patch __getattr__ so that we can catch the first access to 'cells' and load
given the name of an aggregation function returns the function pointer and param
returns encodings for all the records
get the value of the parameter
set a parameter of the anomaly classifier within this model
checks to see if the model should exit based on the exitafter dummy
reset the state of all cells
compute normalization constants for each feature dimension based on the collected training samples
propagate field statistics to the model in case some of its machinery needs it
see method description in base py
returns coordinates around given coordinate within given radius
mark the passage of time this information is used during segment
reads the current "best model" for the job and returns whether or not the
instantiate a clientjobsdao instance
(from backtracking_tm py)
return a map from number to matching on bits for all numbers that match a set of bits
sets the minimum tolerated activity duty cycle given as percent of
a segment is active if it has >= activationthreshold connected synapses that are active due to activestate
set the state of this object from a serialized state
@param monitor monitormixinbase monitor mixin instance that generated
this calls phase 2 of inference used in multistep prediction
generates a worker completion message that is suitable for the
unwraps self __rawinfo params into the equivalent python dictionary
does nothing kept here for api compatibility
return the base spec for tmregion
close the policy instance and its database connection pool
this method is called during network deserialization with an external filename that can be used to bypass pickle for loading large binary states
acquire a connectionwrapper instance that represents a connection to the sql server per nupic
raises an error if cell index is invalid
return the closest training pattern that is *not* of the given category "cat"
return index of the 'sequenceid' field
get all info for a set of models warning!!!: the order of the results are not necessarily in the same order as
compute area under the curve auc using the trapezoidal rule parameters
places the model in a permanent "finished learning" mode
loads a saved jobid from file parameters
generates the clientjobs database name for the given version of the
returns the total number of columns
sets the min threshold
returns the average on-time averaged over all on-time runs
return the pycapnp proto type that the class uses for serialization
gets the specified fields for all the models for a single job this is
print a message to the console
converts a category number into a list of labels
returns the dimensions of the input vector
generate a figure that shows each output over time time goes left to right
generate a dataset of aggregated values parameters
return the overlap between two representations rep1 and rep2 are lists of
modelconfig a dictionary object which holds user-defined settings for model
generate a set of simple sequences the elements of the sequences will be
returns the width of dataout
attaches an 'anomalyclassifier' region to the network will remove current
return segment number segidx on cell c i
returns whether there are more records from current position bookmark
returns true if all records are already in the storage or false if more records is expected
returns mean of non-none elements of the list
get the value of a nodespec parameter most parameters are handled
store a new item in our history
callback that returns a list of all "ephemeral" members (i e data members
add distribution to row row
loads the experiment description python script from the given experiment directory
parses a textual datetime format and return a python datetime object
non-equality operator for temporalmemory instances
routine for computing a moving average
generate a filepath for the calling app
allocate the spatial pooler instance
construct the tm @param pamlength number of time steps to remain in "pay attention mode" after
weighted mean uses params must be the same size as inlist and
get tuple actually a generator of indices where the max value of array x occurs
use the c++ implementation to build an svm model
from http //book opensourceproject org cn/lamp/python/pythoncook2/opensource/0596007973/pythoncook2-chp-19-sect-9 html
resets the region's sequence states
displays command schema to stdout and exit program
return the dictionary of output values note that these are normal python
generates set of random patterns
see the function description in base py
return a clipped version of obj suitable for printing this is useful when generating log messages by printing data structures but
creates and returns the _iterationphase-based instance corresponding
given the complete set of results generated by an experiment (passed in 'results'), filter out and return only the ones the caller wants as
constructs a new empty histogram with no rows or columns
gets a training pattern either by index or category number
marks the stream completed true or false
converts all of the non-numeric fields from spatialoutput and temporaloutput into their scalar equivalents and records them in the output dictionary
saves multiple records in the underlying storage
compute the column confidences given the cell confidences if
niters number of iterations must be greater than 0
close connectionfactory's connection policy typically there is no need
[override] turn learning on for the current model
return a numeric key for sorting this segment
since the knn classifier stores categories as numbers we must store each label as a number
returns pretty-printed table of metrics
@doc place_holder network enableprofiling
protected method that is called during deserialization (after __setstate__) with an external directory path
set all the dynamic state variables from the <tpdynamicstate> dict
set the state of ourself from a serialized state
add an item to the log items list for the currently running session
return a segmentupdate data structure containing a list of proposed changes to segment s
returns log(exp a + exp b a and b are numpy arrays
attempt to insert a row with the given parameters into the jobs table
print segment information for verbose messaging and debugging
return the pycapnp proto type that the class uses for serialization
returns a range of records starting from the bookmark if 'bookmark'
return the field contributions statistics
given an encoder dictionary key get the encoder name
this method will remove any stored records within the range from start to end
returns a state's anomaly vertor converting it from spare to dense
compute the singular value decomposition svd the svd is a factorization
build the additional specs in three groups for the inspector use the type of the default argument to set the spec type defaulting
helper function to return a scalar value representing the most
a "pure virtual" method the derived class must override this method
edits the xml configuration file with the parameters specified by
given model params figure out the correct parameters for the randomdistributed encoder
return the overlap between bucket indices i and j
get the logger for this object
implement the iterator protocol
return the pycapnp proto type that the class uses for serialization
validates control dictionary for the nupic engine context
two-gram model constructor
returns log(exp a - exp b a and b are numpy arrays values in a should be
add a single field to the dataset
initialize field using relevant encoder parameters
helper function there are three different ways of thinking about the representation
returns list of output names in spec
[encoder class virtual method override]
[encoder class virtual method override]
reset the sensor to beginning of data
consruct an instance the instance's open() method must be
@return trace trace of sequence labels
utility function that saves the passed in groundtruth into a local history buffer and returns the groundtruth from self
returns a bookmark to the current position
given two tm instances list the difference between them and returns false if there is a difference
return the best model id and it's errscore from the given sprint
updates the minimum duty cycles the minimum duty cycles are determined
perform the main computation this method is called in each iteration for each phase the node supports
log 'msg % args' with severity 'critical'
handle one compute possibly learning
returns plot of the cell activity note that if many timesteps of
support for the iterator protocol return itself
set the random seed and the numpy seed
returns the number of cells in this layer
sort in memory chunk of records records - a list of records read from the original dataset
look through the jobs table and reactivate all that are already in the running state by setting their _eng_allocate_new_workers fields to true
return the spec for spregion
for use only by nupic scheduler also known as clientjobmanager look through the jobs table and see if any new job requests have been
returns count of all lines in dataset including header lines
return the current active state this is called by the node to
get the predicted segment decrement
@return trace trace of predicted => inactive columns
sets the permanence trim threshold
prints all available results in the given hypersearch job and emits model information to the permutations report csv
:returns list field names associated with the data
[virtual method override] emits a single prediction as input versus predicted
close the policy instance
unwraps self __rawinfo results and caches it in self __cachedresults
fetch jobids for jobs in the table with optional fields given a
maps a column to its input bits this method encapsulates the topology of
run one iteration of spregion's compute
flushes the file
return index of the field matching the field meta special value
unittest testcase assertequal override adds extra log items to msg
this method takes a list of labels and returns a unique category number
sets the predicted segment decrement
see the function description in base py
returns whether global inhibition is enabled
generate a record each value is stored in its respective field
returns 3 things for a vector * the total on time
abbreviate the given text to threshold chars and append an ellipsis if its
generate a non overlapping coincidence matrix this is used to generate random
get parameter value
make a two-dimensional clone map mapping columns to clone master
returns radius for given speed
determines which cells in a predicted column should be added to winner cells list and learns on the segments that correctly predicted this column
generate a set of hub sequences these are sequences which contain a hub
get the label for the metric being optimized this function also caches
this method goes through a list of segments for a given cell and deletes all synapses whose permanence is less than minpermanence and deletes
flush the file to disk
@return countstrace a new trace made up of cumulative counts of this trace's indices
@internal set the state of ourself from a serialized state
like _getmatchingrowsnoretries(), but with retries on transient mysql
returns the number of records that elapse between when an inference is made and when the corresponding input record will appear
returns the top w coordinates by order
deletes temporary system objects/files
:param verbosity integer controlling extent of printouts for debugging
return the best model id and it's errscore from the given sprint which may still be in progress
choose a new position based on results obtained so far from other particles and the passed in globalbestposition
find weakly activated cell in column returns index and segment of most
returns the index of the record that will be read next from
gets a value of w for use in generating a pattern
validate a python value against json schema validate value schemapath
generate multiple records refer to definition for generaterecord
generate the aggregated output record
initialize prngs that may be used by other modules in the experiment stack
modify the data in place adding noise
translate an index into coordinates using the given coordinate system
return the list of labels for the metrics that are being calculated
set the value of the parameter
run a bunch of iterations on a permutevar and collect which positions were visited
returns a tuple successcode recordsarray where successcode - if the stream had enough records to return true/false
get the sensor input element that corresponds to the given inference element
validate a python object against an opf json schema file target target python object to validate typically a dictionary
writes serialized data to proto object
this function will replace checkprediction
returns the verbosity level
get the value of a parameter
get the initial permanence
[virtual method override] save a checkpoint of the prediction output stream
deletes all rows from the table if any data was found
write obj instance to cap'n proto object note this is an abstract method
give the timestamp of a record a datetime object compute the record's timestamp index - this is the timestamp divided by the aggregation period
loads custom configuration settings from their persistent storage
from http //book opensourceproject org cn/lamp/python/pythoncook2/opensource/0596007973/pythoncook2-chp-19-sect-9 html
returns a concatenated list of both the standard base class ephemeral members as well as any additional ephemeral members
insert a new unique model based on params into the model table in the "running" state
returns the cumulative n for all the fields in the dataset
generates centre offsets and spread offsets for block-mode based training regimes - star cross block
add an entry to the jobs table for a new job request this is called by
removes the given records from the classifier
see method description in base py
@doc place_holder network addregion
compute each segment's number of active synapses for a given input
return the inference value from one input sample the actual
returns last non-none element in the list or none if all are none
initilize nupic logging by reading in from the logging configuration file the
encodes the given input row as a dict with the keys being the field names
@doc place_holder network link
closes connect to output store and cleans up any resources associated
log 'msg % args' with the integer severity 'level'
capnp serialization method for the anomaly likelihood object
accepts log-values as input exponentiates them computes the sum then converts the sum back to log-space and returns the result
change the status on the given job parameters
given a bucket index return the list of non-zero bits if the bucket
get a record from the datasource and encode it
returns the nth record
returns a numpy array containing the sub-field scalar value s for each sub-field of the inputdata
get field metadate information for inferences that are of dict type
signal that the input record is the start of a new sequence
creates ndesirednewsynapes synapses on the segment passed in if possible choosing random cells from the previous winner cells that are
set the value of the parameter
encodes inputdata and puts the encoded value into the numpy output array which is a 1-d array of length returned by :meth
helper function for querying the models table including relevant job info where the job type matches the specified jobtype
generate and return the following encoder related substitution variables encoderspecsstr
metric for number of sequences each predicted => active cell appears in note this metric is flawed when it comes to high-order sequences
returns a closure suitable for use as function/method decorator for retrying a function being decorated
called by finalizelearning() this will project all the patterns onto the
cancel the given job this will update the cancel field in the
creates the inference output directory for the given experiment experimentdir experiment directory path that contains description
generates a random sample from the discrete probability distribution and returns its value the log of the probability of sampling that value and the
pretty print a pattern
returns list of default traces to be overridden
compute/update and return the positive activations duty cycle of this segment
translates the given metrics value to json string metrics a list of dictionaries per opftaskdriver
this method ensures that each column has enough connections to input bits to allow it to become active
initialize the dataset generator with a random seed and a name
returns next available data record from the storage if usecache is
sets the iteration number
return the interal _topdownmappingm matrix used for handling the bucketinfo() and topdowncompute() methods
updates the duty cycles for each column the overlap duty cycle is a moving
return the set of pattern numbers that match a bit
returns the potential mapping for a given column 'potential' size
returns true if all records have been read
return info on all of the models that are in already in the models table for a given job
advance to the next iteration cycle phase
helper function check if the settings are reasonable for sp to work
iterates through stream to calculate total records after aggregation
intercept the _readstdconfigfiles call from our base config class to read in base and custom configuration settings
run one iteration of this model
place the given job in status_running mode the job is expected to be status_notstarted
saves the given _hypersearchjob instance's jobid to file
sets global inhibition
get the runtime statistics specific to the model
compute receiver operating characteristic roc note this implementation is restricted to the binary classification task
perform final activities including 'finish' callbacks this
initialize all ephemeral data members and give the derived class the opportunity to do the same by invoking the
model model instance niters number of iterations must be greater than 0
this method deletes all synapses where permanence value is strictly less than self
compute the saturation for a continuous level this breaks the level into
escape commas tabs newlines and dashes in a string
see nupic encoders base encoder for more information
choose n random cells to learn from
this method updates the permanence matrix with a column's new permanence values
generate two sets of sequences the first set of sequences is used to train
generates the metrics for a given inferencetype
generate description from a text description of the ranges
returns the input in the same format as is returned by :meth
chooses the best model for a given job
start a new hypersearch job and monitor it to completion
@param n int number of available bits in pattern @param w (int/list) number of on bits in pattern
activates all of the cells in an unpredicted active column chooses a winner cell and if learning is turned on learns on one segment growing a new
returns a closure suitable for use as function/method decorator for logging entry/exit of function/method
return a dict of the errors obtained on models that were run with each value from a permutechoice variable
return a closure suitable for use as a decorator for retrying a pymysql dao function on certain failures that warrant retries
clear all custom configuration settings and delete the persistent custom configuration store
emit a input/prediction pair if possible
sets the boost factors for all columns 'boostfactors' size must match
requests a job to be suspended note this is primarily for suspending production jobs do not use
remove entries with 0 likelihood or likelihood less than minlikelihoodthreshold but don't leave an empty dict
turn learning on for the current model
protected method that is called during serialization with an external directory path
adds a new segment on a cell
construct a variable that permutes over floating point values using the particle swarm optimization pso algorithm
sets the duty cycle period
special row id is 0xff ffff ffff ffff ffff 9 bytes of 0xff
see method description in base py
get what the inferred value for this field was
resolves the referenced value if the result is already cached
store a dated potential segment update the "date" iteration index is used
add a delta field to the data
@doc place_holder network disableprofiling
return isint intvalue for a given floating point number
returns current minimum value for the field 'fieldname'
close the stream
advances the iteration returns true if more iterations remain false if this is the final
writer non-temporal prediction log writer conforming to predictionwriteriface interface
[virtual method override] returns the sequence of fieldmetainfo objects specifying this
removes trailing whitespace on each line
loads custom configuration settings from their persistent storage
pretty print a numpy matrix using the given format string for each value
method which returns a dictionary of field statistics received from the input source
retrieves the runner's _hypersearchjob instance note only available after run()
creates and returns the _iterationphase-based instance corresponding
get a parameter of the anomaly classifier within this model
set the state of ourself from a serialized state
constructor - initialize the internal engine_internal
returns a list of all known feature groups
report usage error and exit program with error indication
setting this to true freezes the state of the encoder this is separate from the learning state which affects changing parameters
computes the raw anomaly score
return reasonable min/max values to use given the data
retrieves the current results and updates the model's record in the model database
see the function description in base py
sets the number of active columns per inhibition area invalidates the
get the value of the parameter
this is the primary public method of the spatialpooler class this
calls runtimeelement interpret expression and wraps the result
[overrides nupic encoders scalar scalarencoder topdowncompute]
return the best score and position for a given particle the position
model model instance niters number of iterations must be greater than 0
returns the permanence values for a given column 'permanence' size
:returns int count of data rows in dataset excluding header lines
see method description in base py
there are two caveats first this is a potentially slow operation second
generate the string that defines the permutations to apply for a given encoder
see the function description in base py
return the result from dividing two dicts that represent date and time
[override] turn learning off for the current model
this method deletes all synapses whose permanence is less than minpermanence and deletes any segments that have less than
@doc place_holder network resetprofiling
adds partition id for pattern index
returns a tuple of dataset field metadata descriptors that are arranged in the same order as the columns in the dataset
dimensions of the region
used for batch scenarios this method needs to be called between learning
see method description in :meth ~ nupic encoders base encoder getscalars
add multiple fields to the dataset
reads deserialized data from proto object
returns plot of the cell activity
writes serialized data to proto object
returns instance of the underlying sdrclassifier algorithm object
compute the new metrics values given the next inference/ground-truth values parameters
construct a dimensions object
find this cell's segment that was least recently used
fetch jobids for jobs in the table with optional fields given a
model model instance
phase 2 for the inference state the computes the predicted state then
update the state in the job record with our local changes if any
does a bitwise compare of the two bitmaps and returns a fractonal value between 0 and 1 of how similar they are
[virtual method override] this method is called during serialization with an external directory path that can be used to bypass pickle for saving
sets the connected permanence
module imported description py module
initializes internal filter variables for further processing
read state from proto object
@returns the average number of synapses per segment
saves multiple records in the underlying storage
construct an aggregator instance params
run one iteration of tmregion's compute profiling it if requested
parse the given xml file and store all properties it describes
see method description in base py
set the read timeout
accepts log-values as input exponentiates them sums down the rows first dimension then converts the sum back to log-space and returns the result
returns a dictionary of arguments for dbutils steadydb steadydbconnection
returns a range of records starting from the bookmark if 'bookmark'
returns next available data record from the file
see the function description in base py
this is the primary method of the class that will return true or false based on the current environment and user
delete all models from the models table
parses a string containing only 0's and 1's and return a python list object
store a training sample and associated category label
returns a state's anomaly vertor converting it from spare to dense
pick up the latest search from a saved jobid and monitor it to completion
computes the amount of time if any to delay the run of this model
set parameter value
returns the dimensions of the columns in the region
create a particle
overrides the python logging facility's handler handleerror function to
@doc place_holder network save
return true if our local copy of the state has changed since the last time we read from the db
returns a list of :class encoderresult namedtuples describing the inputs
run one iteration of this model
gets a logger for the given class in this module
get the needed length for a list to hold a value for every segment's flatidx
return our connection id this can be used for worker identification
add an entry to the jobs table for a new job request but only if the same job by the same client is not already running
log 'msg % args' with severity 'error'
this is the first portion of every sub-experiment params file we generate between
@return trace trace of predicted => active columns
get the segment with the specified flatidx
create and start a swarm job
set the state of ourself from a serialized state
set the value of the parameter
explicitly run inference on a vector that is passed in and return the category id
[context manager protocol method] release resources
see nupic encoders base encoder for more information
return the total number of synapses
replaces the iteration cycle phases
create a new model instance given a description dictionary
generate the initial first order and second order transition probabilities for 'model2'
returns list of input names in spec
implement this method to provide the pattern machine
todo describe filterdict schema
base class constructor performs common initialization parameters
find this segment's synapse with the smallest permanence
updates the permanence for a synapse
returns the indices of the predictive cells
returns the index of the record that will be read next from
delete the stored checkpoint for the specified modelid this function is
process the consumption a club - skip the header line
read state from proto object
return the nodespec for this pynode
shift the model result and return the new instance
initialize the bucket map assuming the given number of maxbuckets
:returns true if p is a valid estimator params as might be returned by estimateanomalylikelihoods() or updateanomalylikelihoods,
set state from serialized state
returns a concatenated list of both the standard base class ephemeral members as well as any additional ephemeral members
@doc place_holder network getcallbacks
see comments in base class
returns mode evaluation end time
clear out the entire configuration
this is usually used to display a histogram of the on-times encountered in a particular output
see comments in base class
see the function description in base py
generates a file by applying token replacements to the given template file
parse the given xml file and store all properties it describes
accumulate a list of values 'values' into the frequency counts 'freqcounts', and return the updated frequency counts
convenience method to compute a metric over an indices trace excluding resets
returns sensor region's encoder that is sent only to the classifier
retrieve the requested property and return it as an int if property
returns reference to the network's sensor region
clear all configuration properties from in-memory cache but do not alter the custom configuration file
change the values of 1 or more fields in a job here 'fields' is a
note if you set the kwarg "mmname", then pretty-printing of traces and metrics will include the name you specify as a tag before every title
loads a json value from a file and converts it to the corresponding python object
retrives the optimization key name and optimization function
generate a new and unique representation returns a numpy array
run one iteration of the region's compute
compute the anomaly score as the percent of active columns not predicted
[encoder class virtual method override]
a segment is active if it has >= activationthreshold connected synapses that are active due to infactivestate
retrieve the requested property and return it as a float if property
return the degree of overlap between an input pattern and each category stored in the classifier
get the maximum number of segments per cell
like 'neighborhood', except that the neighborhood isn't truncated when it's near an edge
get the logger for this object
return the position of a particle given its state dict
called at the end of learning and inference this routine will update a number of stats in our _internalstats dictionary including our computed
generate a set of possible report keys for an experiment's results
returns the next record from the dataset the returned record object
saves specified error in the storage
set the state of ourself from a serialized state
return our moving average of learned sequence length
return serializable state this function will return a version of the
return the current state of this particle this is used for
update a set of synapses in the segment
emit model info to csv file
return a list of possible encoder parameter combinations for the given field and the default aggregation function to use
requestedactivities a sequence of periodicactivityrequest elements
creates a new synapse on a segment
compute the probability that the current value plus anomaly score represents an anomaly given the historical distribution of anomaly scores
translate parameters and initialize member variables
@doc place_holder region compute
returns the potential percent
look through the models table for an orphaned model which is a model that is not completed yet whose _eng_last_update_time is more than
periodic check to see if this is the best model this should only have an
do one iteration of top-down inference
returns whether or not the object is a string
override of getstats() in basestatscollector
initialize the random seed
load saved model
see the function description in base py
returns the indices of the active cells
save the model in the given directory
get the default arguments from the function and assign as instance vars
not implemented csv file version does not provide storage for the error
handle legacy options temporary
return true if no aggregation will be performed either because the aggregationinfo was none or all aggregation params within it were 0
main loop of the opf model runner
returns the shared cjdao clientjobsdao instance
- backs up old report csv file - opens the report csv file in append or overwrite mode (per
gets the current metric values returns a dictionary where each key is the metric-name and the values are
print up to maxcols number from a flat floating point array
if state is allocated in cpp copy over the data into our numpy arrays
this method will add the record to the knn classifier
log 'msg % args' with severity 'warning'
initialize class properties from stored values
get a connection instance
run final activities after a model has run these include recording and
see the function description in base py
[_iterationphase method implementation] performs initialization that is necessary upon entry to the phase
samples n rows
returns true if any element of the sequence satisfies true
sets the autodetectthreshold
train the classifier to associate specified input pattern with a particular category
this creates an experiment directory with a base py description file
add the aggregation period to the input time t and return a datetime object years and months are handled as aspecial case due to leap years
creates and returns a periodicactivitymgr instance initialized with
validate a python value against json schema validate value schemapath
set a single custom setting and persist it to the custom configuration store
returns the index of the pattern that is closest to inputpattern the distances of all patterns to inputpattern and the indices of the k
return total number of models that completed
[virtual method override] returns a tuple of dataset field metadata descriptors that are
convert a list of sequences of pattern indices and a pattern lookup table into a an array of patterns
returns the task instances of the experiment description
edits the xml configuration file with the parameters specified by
semi-private method for retrieving the job-specific params parameters
:returns index of the 'reset' field none if no such field
create and seed random number generator
called to indicate the start of a new sequence
constructor taskcontrol dictionary conforming to opftaskcontrolschema
removes the set of columns who have never been active from the set of active columns selected in the inhibition round
given a series of anomaly scores compute the likelihood for each score this
creates the model's predictionlogger object which is an interface to write
given two tm instances list the difference between them and returns false if there is a difference
return the field names for each of the scalar values returned by getscalars
returns the next n values for the distribution as a list
converts an int to a packed byte array with left most significant byte
perform recursive latin hypercube sampling
converts a byte array into an integer
processes the given record according to the current phase inputrecord record object formatted according to
get the actual value for this field
@param categoriesout -- the maximum number of distinct category labels that can be learned
return a list of namedtuples from the result of a join query a
return the absolute path of the model's pickle file
return the pycapnp proto type that the class uses for serialization
emits final metrics
returns the synapses for the source cell that they synapse on
instantiate the hyperseachv2 instance
returns flags for all fields
sets the permanence increment amount for active synapses
read state from proto object
debugging/profiling utility method to allow tools to simulate the presentation of training sample
return the model id of the model with the best result so far and it's score on the optimize metric
return the autodetectwaitrecords
initialize svm engine use the swig bindings to initialize an instance of an svm classifier engine
deprecated use @ref runwithconfig
resumes processing of an existing job that is presently in the status_completed state
save cells4 state to this file
returns true if the inference type is 'temporal', i e requires a
emits final metrics
retrieve the requested property and return it as a float if property
@doc place_holder network removeregion
return the inference state of the current model
return a list of swarm generations that have completed and the best minimal errscore seen for each of them
encoder class virtual method override
returns instance of the underlying claclassifier algorithm object
print a floating point array that is the same shape as activestate
saves the record in the underlying storage
get what the inferred value for this field was
set a single custom setting and persist it to the custom configuration store
return serializable state this function will return a version of the
filepath path of file where sp __init__ args are to be saved
this function determines each column's overlap with the current input vector
adds a value over a range of rows
spec of the region
returns traceback namedtuple for our caller's caller
return the list of all swarms in the given sprint
see the function description in base py
helper method that generates a unique label for a metricspec / inferencetype pair
[virtual method override] resets the model's sequence states normally
returns the data for a synapse
compute anomaly score if required
see method description in base py
return a numpy array predictedcells representing the current predicted state
returns a segment object of the specified segment using data from the self
reclassifies all internal state
get the permanence increment
write inputs to output tap file
place the model in a permanent "finished learning" mode
generates a lookup dictionary of permutation variables whose values are too complex for labels so that artificial labels have to be generated
consruct an instance the instance's open() method must be
non-equality operator for connections instances
return number of models that completed with errors
see nupic encoders base encoder for more information
prediction function for determining whether a function is a categorical variable or a scalar variable
neighbors
neighbors
neighbors
neighbors
neighbors
expValues
expValues
expValues
expValues
expValues
all
all
all
all
all
totalw
totalw
totalw
totalw
totalw
encoderParams
encoderParams
encoderParams
encoderParams
encoderParams
skip
skip
skip
skip
skip
global
global
global
global
global
pick
pick
pick
pick
pick
results
results
results
results
results
likelihoodsDict
likelihoodsDict
likelihoodsDict
likelihoodsDict
likelihoodsDict
exp_generator
exp_generator
exp_generator
exp_generator
exp_generator
iterationLearnNum
iterationLearnNum
iterationLearnNum
iterationLearnNum
iterationLearnNum
metricResult
metricResult
metricResult
metricResult
metricResult
synapses
synapses
synapses
synapses
synapses
dutyCyclePeriod
dutyCyclePeriod
dutyCyclePeriod
dutyCyclePeriod
dutyCyclePeriod
flattenedChosenValues
flattenedChosenValues
flattenedChosenValues
flattenedChosenValues
flattenedChosenValues
dirty
dirty
dirty
dirty
dirty
nPatterns
nPatterns
nPatterns
nPatterns
nPatterns
saved
saved
saved
saved
saved
nSteps
nSteps
nSteps
nSteps
nSteps
row
row
row
row
row
computeInfOutput
computeInfOutput
computeInfOutput
computeInfOutput
computeInfOutput
calculate
calculate
calculate
calculate
calculate
filterDict
filterDict
filterDict
filterDict
filterDict
segments
segments
segments
segments
segments
statistics
statistics
statistics
statistics
statistics
globalBestPosition
globalBestPosition
globalBestPosition
globalBestPosition
globalBestPosition
totaln
totaln
totaln
totaln
totaln
inputData
inputData
inputData
inputData
inputData
to
to
to
to
to
init
init
init
init
init
tm
tm
tm
tm
tm
vec
vec
vec
vec
vec
stable
stable
stable
stable
stable
retries
retries
retries
retries
retries
through
through
through
through
through
spec
spec
spec
spec
spec
cellConfidences
cellConfidences
cellConfidences
cellConfidences
cellConfidences
activities
activities
activities
activities
activities
updated
updated
updated
updated
updated
labels
labels
labels
labels
labels
string
string
string
string
string
connectedPerm
connectedPerm
connectedPerm
connectedPerm
connectedPerm
filePath
filePath
filePath
filePath
filePath
particleState
particleState
particleState
particleState
particleState
SP1
SP1
SP1
SP1
SP1
SP2
SP2
SP2
SP2
SP2
reader
reader
reader
reader
reader
multi
multi
multi
multi
multi
choice
choice
choice
choice
choice
tmclass
tmclass
tmclass
tmclass
tmclass
sparseForm
sparseForm
sparseForm
sparseForm
sparseForm
roccurve
roccurve
roccurve
roccurve
roccurve
extraDataDir
extraDataDir
extraDataDir
extraDataDir
extraDataDir
updates
updates
updates
updates
updates
join
join
join
join
join
dbname
dbname
dbname
dbname
dbname
tpdynamic
tpdynamic
tpdynamic
tpdynamic
tpdynamic
getstate
getstate
getstate
getstate
getstate
fieldsInfo
fieldsInfo
fieldsInfo
fieldsInfo
fieldsInfo
level
level
level
level
level
stimulus
stimulus
stimulus
stimulus
stimulus
positions
positions
positions
positions
positions
recursionIndex
recursionIndex
recursionIndex
recursionIndex
recursionIndex
distn
distn
distn
distn
distn
list
list
list
list
list
prefix
prefix
prefix
prefix
prefix
permanenceIncrement
permanenceIncrement
permanenceIncrement
permanenceIncrement
permanenceIncrement
random
random
random
random
random
p
p
p
p
p
vector
vector
vector
vector
vector
spEnable
spEnable
spEnable
spEnable
spEnable
pooler
pooler
pooler
pooler
pooler
scores
scores
scores
scores
scores
indices
indices
indices
indices
indices
lscsum0
lscsum0
lscsum0
lscsum0
lscsum0
joiner
joiner
joiner
joiner
joiner
setup
setup
setup
setup
setup
dir
dir
dir
dir
dir
claclassifier
claclassifier
claclassifier
claclassifier
claclassifier
coordinate
coordinate
coordinate
coordinate
coordinate
peek
peek
peek
peek
peek
htm
htm
htm
htm
htm
columnMatchingSegments
columnMatchingSegments
columnMatchingSegments
columnMatchingSegments
columnMatchingSegments
numRecords
numRecords
numRecords
numRecords
numRecords
prediction
prediction
prediction
prediction
prediction
hs
hs
hs
hs
hs
likely
likely
likely
likely
likely
cross
cross
cross
cross
cross
outputFilename
outputFilename
outputFilename
outputFilename
outputFilename
pass
pass
pass
pass
pass
l1Hubs
l1Hubs
l1Hubs
l1Hubs
l1Hubs
outputCloningWidth
outputCloningWidth
outputCloningWidth
outputCloningWidth
outputCloningWidth
permutations
permutations
permutations
permutations
permutations
predictedState
predictedState
predictedState
predictedState
predictedState
append
append
append
append
append
diff2
diff2
diff2
diff2
diff2
index
index
index
index
index
errors
errors
errors
errors
errors
timeoutSec
timeoutSec
timeoutSec
timeoutSec
timeoutSec
frameworks
frameworks
frameworks
frameworks
frameworks
contributions
contributions
contributions
contributions
contributions
sum
sum
sum
sum
sum
resume
resume
resume
resume
resume
lookback
lookback
lookback
lookback
lookback
thru
thru
thru
thru
thru
ptr
ptr
ptr
ptr
ptr
current
current
current
current
current
public
public
public
public
public
deleteOldVersions
deleteOldVersions
deleteOldVersions
deleteOldVersions
deleteOldVersions
version
version
version
version
version
templates
templates
templates
templates
templates
modelInfo
modelInfo
modelInfo
modelInfo
modelInfo
fractional
fractional
fractional
fractional
fractional
seqs
seqs
seqs
seqs
seqs
new
new
new
new
new
learned
learned
learned
learned
learned
coordinates
coordinates
coordinates
coordinates
coordinates
boost
boost
boost
boost
boost
method
method
method
method
method
counters
counters
counters
counters
counters
segList
segList
segList
segList
segList
completion
completion
completion
completion
completion
full
full
full
full
full
nodeType
nodeType
nodeType
nodeType
nodeType
presynaptic
presynaptic
presynaptic
presynaptic
presynaptic
multinomial
multinomial
multinomial
multinomial
multinomial
enabled
enabled
enabled
enabled
enabled
learn
learn
learn
learn
learn
modify
modify
modify
modify
modify
indicator
indicator
indicator
indicator
indicator
ranges
ranges
ranges
ranges
ranges
hubs
hubs
hubs
hubs
hubs
learnState
learnState
learnState
learnState
learnState
groups
groups
groups
groups
groups
active
active
active
active
active
path
path
path
path
path
aState
aState
aState
aState
aState
strong
strong
strong
strong
strong
wait
wait
wait
wait
wait
convert
convert
convert
convert
convert
moduleName
moduleName
moduleName
moduleName
moduleName
synapse
synapse
synapse
synapse
synapse
shift
shift
shift
shift
shift
vectors
vectors
vectors
vectors
vectors
wcoordinates
wcoordinates
wcoordinates
wcoordinates
wcoordinates
decodeResults
decodeResults
decodeResults
decodeResults
decodeResults
periodic
periodic
periodic
periodic
periodic
useConnectionID
useConnectionID
useConnectionID
useConnectionID
useConnectionID
maxRows
maxRows
maxRows
maxRows
maxRows
aggr
aggr
aggr
aggr
aggr
ignoreUnchanged
ignoreUnchanged
ignoreUnchanged
ignoreUnchanged
ignoreUnchanged
topDownInput
topDownInput
topDownInput
topDownInput
topDownInput
exc_tb
exc_tb
exc_tb
exc_tb
exc_tb
activeState
activeState
activeState
activeState
activeState
control
control
control
control
control
recordsToDelete
recordsToDelete
recordsToDelete
recordsToDelete
recordsToDelete
inference
inference
inference
inference
inference
verbose
verbose
verbose
verbose
verbose
classification
classification
classification
classification
classification
extra
extra
extra
extra
extra
timestep
timestep
timestep
timestep
timestep
serialize
serialize
serialize
serialize
serialize
dbresults
dbresults
dbresults
dbresults
dbresults
private
private
private
private
private
jobId
jobId
jobId
jobId
jobId
idToCategorize
idToCategorize
idToCategorize
idToCategorize
idToCategorize
alpha
alpha
alpha
alpha
alpha
spreadShape
spreadShape
spreadShape
spreadShape
spreadShape
sys
sys
sys
sys
sys
names
names
names
names
names
viz
viz
viz
viz
viz
apply
apply
apply
apply
apply
counts
counts
counts
counts
counts
nodeSet
nodeSet
nodeSet
nodeSet
nodeSet
byte
byte
byte
byte
byte
outputVal
outputVal
outputVal
outputVal
outputVal
saveModelDir
saveModelDir
saveModelDir
saveModelDir
saveModelDir
plot
plot
plot
plot
plot
confidence
confidence
confidence
confidence
confidence
attendance
attendance
attendance
attendance
attendance
from
from
from
from
from
stream
stream
stream
stream
stream
consumption
consumption
consumption
consumption
consumption
usageStr
usageStr
usageStr
usageStr
usageStr
pred
pred
pred
pred
pred
chooser
chooser
chooser
chooser
chooser
tail
tail
tail
tail
tail
perm
perm
perm
perm
perm
singularValues
singularValues
singularValues
singularValues
singularValues
predict
predict
predict
predict
predict
minVal
minVal
minVal
minVal
minVal
call
call
call
call
call
useCache
useCache
useCache
useCache
useCache
memory
memory
memory
memory
memory
bottomUpNZ
bottomUpNZ
bottomUpNZ
bottomUpNZ
bottomUpNZ
parameterName
parameterName
parameterName
parameterName
parameterName
coinc
coinc
coinc
coinc
coinc
type
type
type
type
type
metric
metric
metric
metric
metric
sort
sort
sort
sort
sort
flat
flat
flat
flat
flat
unlearned
unlearned
unlearned
unlearned
unlearned
upcoming
upcoming
upcoming
upcoming
upcoming
tm1
tm1
tm1
tm1
tm1
club
club
club
club
club
outputs
outputs
outputs
outputs
outputs
ylabel
ylabel
ylabel
ylabel
ylabel
tm2
tm2
tm2
tm2
tm2
initialPermanence
initialPermanence
initialPermanence
initialPermanence
initialPermanence
isSparse
isSparse
isSparse
isSparse
isSparse
std
std
std
std
std
depth
depth
depth
depth
depth
tmcpp
tmcpp
tmcpp
tmcpp
tmcpp
fieldInfo
fieldInfo
fieldInfo
fieldInfo
fieldInfo
connectedPct
connectedPct
connectedPct
connectedPct
connectedPct
modelIDs
modelIDs
modelIDs
modelIDs
modelIDs
anomalyScore
anomalyScore
anomalyScore
anomalyScore
anomalyScore
states
states
states
states
states
identityPolicyObj
identityPolicyObj
identityPolicyObj
identityPolicyObj
identityPolicyObj
locked
locked
locked
locked
locked
localAreaDensity
localAreaDensity
localAreaDensity
localAreaDensity
localAreaDensity
err
err
err
err
err
f
f
f
f
f
mm
mm
mm
mm
mm
inputWidth
inputWidth
inputWidth
inputWidth
inputWidth
fieldStats
fieldStats
fieldStats
fieldStats
fieldStats
prediction2
prediction2
prediction2
prediction2
prediction2
newSerialization
newSerialization
newSerialization
newSerialization
newSerialization
cat
cat
cat
cat
cat
column
column
column
column
column
testRegionObj
testRegionObj
testRegionObj
testRegionObj
testRegionObj
allResults
allResults
allResults
allResults
allResults
values
values
values
values
values
convergence
convergence
convergence
convergence
convergence
useStartCells
useStartCells
useStartCells
useStartCells
useStartCells
rowHint
rowHint
rowHint
rowHint
rowHint
under
under
under
under
under
dec
dec
dec
dec
dec
pickle
pickle
pickle
pickle
pickle
def
def
def
def
def
history
history
history
history
history
doForeground
doForeground
doForeground
doForeground
doForeground
overlaps
overlaps
overlaps
overlaps
overlaps
traceback
traceback
traceback
traceback
traceback
tap
tap
tap
tap
tap
serializable
serializable
serializable
serializable
serializable
scan
scan
scan
scan
scan
process
process
process
process
process
varName
varName
varName
varName
varName
distributions
distributions
distributions
distributions
distributions
nL1SimpleSequences
nL1SimpleSequences
nL1SimpleSequences
nL1SimpleSequences
nL1SimpleSequences
searchParams
searchParams
searchParams
searchParams
searchParams
decode
decode
decode
decode
decode
sample
sample
sample
sample
sample
outputFile
outputFile
outputFile
outputFile
outputFile
inputFilename
inputFilename
inputFilename
inputFilename
inputFilename
chunkIndex
chunkIndex
chunkIndex
chunkIndex
chunkIndex
categoryColumn
categoryColumn
categoryColumn
categoryColumn
categoryColumn
node_attrs
node_attrs
node_attrs
node_attrs
node_attrs
absolute
absolute
absolute
absolute
absolute
normalize
normalize
normalize
normalize
normalize
filter
filter
filter
filter
filter
scalar
scalar
scalar
scalar
scalar
spaceShape
spaceShape
spaceShape
spaceShape
spaceShape
end
end
end
end
end
parentFieldName
parentFieldName
parentFieldName
parentFieldName
parentFieldName
pluggable
pluggable
pluggable
pluggable
pluggable
regions
regions
regions
regions
regions
newVal
newVal
newVal
newVal
newVal
progressStart
progressStart
progressStart
progressStart
progressStart
rewind
rewind
rewind
rewind
rewind
n_dims
n_dims
n_dims
n_dims
n_dims
parameter
parameter
parameter
parameter
parameter
activation
activation
activation
activation
activation
classify
classify
classify
classify
classify
A
A
A
A
A
inputs
inputs
inputs
inputs
inputs
javaparameters
javaparameters
javaparameters
javaparameters
javaparameters
description
description
description
description
description
timedelta
timedelta
timedelta
timedelta
timedelta
includeZeros
includeZeros
includeZeros
includeZeros
includeZeros
clone
clone
clone
clone
clone
after
after
after
after
after
lscsum
lscsum
lscsum
lscsum
lscsum
maxValue
maxValue
maxValue
maxValue
maxValue
maxElementSize
maxElementSize
maxElementSize
maxElementSize
maxElementSize
csvfile
csvfile
csvfile
csvfile
csvfile
add2darray
add2darray
add2darray
add2darray
add2darray
disable
disable
disable
disable
disable
streamDef
streamDef
streamDef
streamDef
streamDef
activeColumns
activeColumns
activeColumns
activeColumns
activeColumns
date
date
date
date
date
svmclassifier
svmclassifier
svmclassifier
svmclassifier
svmclassifier
data
data
data
data
data
grow
grow
grow
grow
grow
types
types
types
types
types
sprint
sprint
sprint
sprint
sprint
sequenceLength
sequenceLength
sequenceLength
sequenceLength
sequenceLength
nodeParams
nodeParams
nodeParams
nodeParams
nodeParams
includeIndices
includeIndices
includeIndices
includeIndices
includeIndices
mgr
mgr
mgr
mgr
mgr
sp
sp
sp
sp
sp
buVectors
buVectors
buVectors
buVectors
buVectors
timeout
timeout
timeout
timeout
timeout
computeScores
computeScores
computeScores
computeScores
computeScores
callback
callback
callback
callback
callback
switch
switch
switch
switch
switch
algorithms
algorithms
algorithms
algorithms
algorithms
saveOutput
saveOutput
saveOutput
saveOutput
saveOutput
enter
enter
enter
enter
enter
first
first
first
first
first
tpDynamicState
tpDynamicState
tpDynamicState
tpDynamicState
tpDynamicState
documents
documents
documents
documents
documents
matured
matured
matured
matured
matured
accumulatedError
accumulatedError
accumulatedError
accumulatedError
accumulatedError
activeInput
activeInput
activeInput
activeInput
activeInput
clientInfo
clientInfo
clientInfo
clientInfo
clientInfo
vars
vars
vars
vars
vars
numbers
numbers
numbers
numbers
numbers
over
over
over
over
over
inferenceArgs
inferenceArgs
inferenceArgs
inferenceArgs
inferenceArgs
find
find
find
find
find
cols
cols
cols
cols
cols
feature
feature
feature
feature
feature
stability
stability
stability
stability
stability
runner
runner
runner
runner
runner
frequency
frequency
frequency
frequency
frequency
minLikelihoodThreshold
minLikelihoodThreshold
minLikelihoodThreshold
minLikelihoodThreshold
minLikelihoodThreshold
filters
filters
filters
filters
filters
labelList
labelList
labelList
labelList
labelList
interpreter
interpreter
interpreter
interpreter
interpreter
dividend
dividend
dividend
dividend
dividend
cellTrace
cellTrace
cellTrace
cellTrace
cellTrace
patternNZ
patternNZ
patternNZ
patternNZ
patternNZ
write
write
write
write
write
activationThreshold
activationThreshold
activationThreshold
activationThreshold
activationThreshold
group
group
group
group
group
monitor
monitor
monitor
monitor
monitor
stepSize
stepSize
stepSize
stepSize
stepSize
del
del
del
del
del
temporal
temporal
temporal
temporal
temporal
pct
pct
pct
pct
pct
generateFlag
generateFlag
generateFlag
generateFlag
generateFlag
bottomUpInput
bottomUpInput
bottomUpInput
bottomUpInput
bottomUpInput
categories
categories
categories
categories
categories
kwargs
kwargs
kwargs
kwargs
kwargs
policy
policy
policy
policy
policy
numberOfCols
numberOfCols
numberOfCols
numberOfCols
numberOfCols
paths
paths
paths
paths
paths
main
main
main
main
main
lx
lx
lx
lx
lx
cls
cls
cls
cls
cls
curve
curve
curve
curve
curve
tableInfo
tableInfo
tableInfo
tableInfo
tableInfo
non
non
non
non
non
completionReason
completionReason
completionReason
completionReason
completionReason
env
env
env
env
env
predictedSegmentDecrement
predictedSegmentDecrement
predictedSegmentDecrement
predictedSegmentDecrement
predictedSegmentDecrement
reportKeys
reportKeys
reportKeys
reportKeys
reportKeys
timestamp
timestamp
timestamp
timestamp
timestamp
sdrcategory
sdrcategory
sdrcategory
sdrcategory
sdrcategory
dao
dao
dao
dao
dao
initialPerm
initialPerm
initialPerm
initialPerm
initialPerm
initialRetryDelaySec
initialRetryDelaySec
initialRetryDelaySec
initialRetryDelaySec
initialRetryDelaySec
outputCloningHeight
outputCloningHeight
outputCloningHeight
outputCloningHeight
outputCloningHeight
records
records
records
records
records
pooled
pooled
pooled
pooled
pooled
accumulate
accumulate
accumulate
accumulate
accumulate
sorted
sorted
sorted
sorted
sorted
day
day
day
day
day
overlap
overlap
overlap
overlap
overlap
segUpdate
segUpdate
segUpdate
segUpdate
segUpdate
cellsPerColumn
cellsPerColumn
cellsPerColumn
cellsPerColumn
cellsPerColumn
name
name
name
name
name
edit
edit
edit
edit
edit
config
config
config
config
config
dbVersion
dbVersion
dbVersion
dbVersion
dbVersion
exc_val
exc_val
exc_val
exc_val
exc_val
merge
merge
merge
merge
merge
weather
weather
weather
weather
weather
newInput
newInput
newInput
newInput
newInput
getitem
getitem
getitem
getitem
getitem
maxCols
maxCols
maxCols
maxCols
maxCols
likelihoods
likelihoods
likelihoods
likelihoods
likelihoods
nSeq
nSeq
nSeq
nSeq
nSeq
traces
traces
traces
traces
traces
classifier
classifier
classifier
classifier
classifier
renderer
renderer
renderer
renderer
renderer
reset
reset
reset
reset
reset
r
r
r
r
r
l2sequences
l2sequences
l2sequences
l2sequences
l2sequences
metrics
metrics
metrics
metrics
metrics
map
map
map
map
map
bump
bump
bump
bump
bump
chunk
chunk
chunk
chunk
chunk
logTraceback
logTraceback
logTraceback
logTraceback
logTraceback
targetValue
targetValue
targetValue
targetValue
targetValue
numeric
numeric
numeric
numeric
numeric
used
used
used
used
used
xLabel
xLabel
xLabel
xLabel
xLabel
iter
iter
iter
iter
iter
encodings
encodings
encodings
encodings
encodings
includeHidden
includeHidden
includeHidden
includeHidden
includeHidden
operation
operation
operation
operation
operation
inList
inList
inList
inList
inList
extract
extract
extract
extract
extract
message
message
message
message
message
special
special
special
special
special
out
out
out
out
out
category
category
category
category
category
jsondump
jsondump
jsondump
jsondump
jsondump
minOverlapPct
minOverlapPct
minOverlapPct
minOverlapPct
minOverlapPct
statsInfo
statsInfo
statsInfo
statsInfo
statsInfo
matrix
matrix
matrix
matrix
matrix
modelParams
modelParams
modelParams
modelParams
modelParams
factors
factors
factors
factors
factors
truth
truth
truth
truth
truth
relaxSegmentTests
relaxSegmentTests
relaxSegmentTests
relaxSegmentTests
relaxSegmentTests
encoderSpec
encoderSpec
encoderSpec
encoderSpec
encoderSpec
factory
factory
factory
factory
factory
load
load
load
load
load
recordTS
recordTS
recordTS
recordTS
recordTS
geospatial
geospatial
geospatial
geospatial
geospatial
weighted
weighted
weighted
weighted
weighted
adapt
adapt
adapt
adapt
adapt
precision
precision
precision
precision
precision
max
max
max
max
max
print
print
print
print
print
adaptive
adaptive
adaptive
adaptive
adaptive
sensor
sensor
sensor
sensor
sensor
math
math
math
math
math
common
common
common
common
common
patterns
patterns
patterns
patterns
patterns
phaseSpecs
phaseSpecs
phaseSpecs
phaseSpecs
phaseSpecs
offsets
offsets
offsets
offsets
offsets
discrete
discrete
discrete
discrete
discrete
args
args
args
args
args
given
given
given
given
given
argv
argv
argv
argv
argv
mapping
mapping
mapping
mapping
mapping
terminationFunc
terminationFunc
terminationFunc
terminationFunc
terminationFunc
progressCB
progressCB
progressCB
progressCB
progressCB
base
base
base
base
base
lastUsedIteration
lastUsedIteration
lastUsedIteration
lastUsedIteration
lastUsedIteration
members
members
members
members
members
initialize
initialize
initialize
initialize
initialize
spregion
spregion
spregion
spregion
spregion
estimate
estimate
estimate
estimate
estimate
likelihood
likelihood
likelihood
likelihood
likelihood
generate
generate
generate
generate
generate
additional
additional
additional
additional
additional
advance
advance
advance
advance
advance
training
training
training
training
training
logging
logging
logging
logging
logging
reactivate
reactivate
reactivate
reactivate
reactivate
alreadyRunning
alreadyRunning
alreadyRunning
alreadyRunning
alreadyRunning
starts
starts
starts
starts
starts
script
script
script
script
script
to8byte
to8byte
to8byte
to8byte
to8byte
modules
modules
modules
modules
modules
times
times
times
times
times
rfInput
rfInput
rfInput
rfInput
rfInput
inputRow
inputRow
inputRow
inputRow
inputRow
length
length
length
length
length
choices
choices
choices
choices
choices
termination
termination
termination
termination
termination
w
w
w
w
w
activePresynapticCells
activePresynapticCells
activePresynapticCells
activePresynapticCells
activePresynapticCells
listObj
listObj
listObj
listObj
listObj
calc
calc
calc
calc
calc
deltaField
deltaField
deltaField
deltaField
deltaField
dbName
dbName
dbName
dbName
dbName
inputPattern
inputPattern
inputPattern
inputPattern
inputPattern
user
user
user
user
user
origField
origField
origField
origField
origField
suspend
suspend
suspend
suspend
suspend
updatePeriod
updatePeriod
updatePeriod
updatePeriod
updatePeriod
mixin
mixin
mixin
mixin
mixin
probability
probability
probability
probability
probability
encoding
encoding
encoding
encoding
encoding
inputCategory
inputCategory
inputCategory
inputCategory
inputCategory
variables
variables
variables
variables
variables
activeDutyCycles
activeDutyCycles
activeDutyCycles
activeDutyCycles
activeDutyCycles
parser
parser
parser
parser
parser
number
number
number
number
number
one
one
one
one
one
clear
clear
clear
clear
clear
labelFilter
labelFilter
labelFilter
labelFilter
labelFilter
construct
construct
construct
construct
construct
prng
prng
prng
prng
prng
neighborhood
neighborhood
neighborhood
neighborhood
neighborhood
array
array
array
array
array
seqLength
seqLength
seqLength
seqLength
seqLength
size
size
size
size
size
has
has
has
has
has
properties
properties
properties
properties
properties
cortical
cortical
cortical
cortical
cortical
compat
compat
compat
compat
compat
splearning
splearning
splearning
splearning
splearning
bookmark
bookmark
bookmark
bookmark
bookmark
requireAll
requireAll
requireAll
requireAll
requireAll
top
top
top
top
top
burst
burst
burst
burst
burst
least
least
least
least
least
includeClassifierOnlyField
includeClassifierOnlyField
includeClassifierOnlyField
includeClassifierOnlyField
includeClassifierOnlyField
checkpoint
checkpoint
checkpoint
checkpoint
checkpoint
swarm
swarm
swarm
swarm
swarm
breakOnResets
breakOnResets
breakOnResets
breakOnResets
breakOnResets
params
params
params
params
params
monitor_mixin
monitor_mixin
monitor_mixin
monitor_mixin
monitor_mixin
htmPredictionModel
htmPredictionModel
htmPredictionModel
htmPredictionModel
htmPredictionModel
bucketIdx
bucketIdx
bucketIdx
bucketIdx
bucketIdx
bits
bits
bits
bits
bits
final
final
final
final
final
store
store
store
store
store
schema
schema
schema
schema
schema
shifted
shifted
shifted
shifted
shifted
expJsonFilePath
expJsonFilePath
expJsonFilePath
expJsonFilePath
expJsonFilePath
activity
activity
activity
activity
activity
hub
hub
hub
hub
hub
groundTruth
groundTruth
groundTruth
groundTruth
groundTruth
punish
punish
punish
punish
punish
gym
gym
gym
gym
gym
acquire
acquire
acquire
acquire
acquire
fractionOfMax
fractionOfMax
fractionOfMax
fractionOfMax
fractionOfMax
emit
emit
emit
emit
emit
task
task
task
task
task
transaction
transaction
transaction
transaction
transaction
agitate
agitate
agitate
agitate
agitate
simulate
simulate
simulate
simulate
simulate
signum
signum
signum
signum
signum
particleId
particleId
particleId
particleId
particleId
copy
copy
copy
copy
copy
knnclassifier
knnclassifier
knnclassifier
knnclassifier
knnclassifier
getLoggerCallback
getLoggerCallback
getLoggerCallback
getLoggerCallback
getLoggerCallback
population
population
population
population
population
performLowerBoundCheck
performLowerBoundCheck
performLowerBoundCheck
performLowerBoundCheck
performLowerBoundCheck
connections
connections
connections
connections
connections
partitionId
partitionId
partitionId
partitionId
partitionId
jobs
jobs
jobs
jobs
jobs
pprint
pprint
pprint
pprint
pprint
timeStep
timeStep
timeStep
timeStep
timeStep
lnsum0
lnsum0
lnsum0
lnsum0
lnsum0
iteration
iteration
iteration
iteration
iteration
second
second
second
second
second
columnIndex
columnIndex
columnIndex
columnIndex
columnIndex
savedModelDir
savedModelDir
savedModelDir
savedModelDir
savedModelDir
delay
delay
delay
delay
delay
xlabel
xlabel
xlabel
xlabel
xlabel
baseDescription
baseDescription
baseDescription
baseDescription
baseDescription
str
str
str
str
str
exp
exp
exp
exp
exp
privateMemberName
privateMemberName
privateMemberName
privateMemberName
privateMemberName
rowsToRemove
rowsToRemove
rowsToRemove
rowsToRemove
rowsToRemove
files
files
files
files
files
false
false
false
false
false
collectActiveData
collectActiveData
collectActiveData
collectActiveData
collectActiveData
decoded
decoded
decoded
decoded
decoded
spatialImp
spatialImp
spatialImp
spatialImp
spatialImp
numSteps
numSteps
numSteps
numSteps
numSteps
generation
generation
generation
generation
generation
arg
arg
arg
arg
arg
consoleLevel
consoleLevel
consoleLevel
consoleLevel
consoleLevel
raw
raw
raw
raw
raw
seed
seed
seed
seed
seed
tapPath
tapPath
tapPath
tapPath
tapPath
increment
increment
increment
increment
increment
close
close
close
close
close
maxC
maxC
maxC
maxC
maxC
optStr
optStr
optStr
optStr
optStr
element
element
element
element
element
seek
seek
seek
seek
seek
any
any
any
any
any
waitRecords
waitRecords
waitRecords
waitRecords
waitRecords
option
option
option
option
option
opftask
opftask
opftask
opftask
opftask
segIdx
segIdx
segIdx
segIdx
segIdx
iface
iface
iface
iface
iface
datafile
datafile
datafile
datafile
datafile
jobHash
jobHash
jobHash
jobHash
jobHash
min
min
min
min
min
permanence
permanence
permanence
permanence
permanence
inputVector
inputVector
inputVector
inputVector
inputVector
minC
minC
minC
minC
minC
hyperplanes
hyperplanes
hyperplanes
hyperplanes
hyperplanes
specs
specs
specs
specs
specs
instance
instance
instance
instance
instance
learningEnabled
learningEnabled
learningEnabled
learningEnabled
learningEnabled
potential
potential
potential
potential
potential
build
build
build
build
build
presynapticCell
presynapticCell
presynapticCell
presynapticCell
presynapticCell
dbConn
dbConn
dbConn
dbConn
dbConn
flags
flags
flags
flags
flags
colConfidence
colConfidence
colConfidence
colConfidence
colConfidence
inValue
inValue
inValue
inValue
inValue
completed
completed
completed
completed
completed
entryExitLogLevel
entryExitLogLevel
entryExitLogLevel
entryExitLogLevel
entryExitLogLevel
selectFieldNames
selectFieldNames
selectFieldNames
selectFieldNames
selectFieldNames
connectedCounts
connectedCounts
connectedCounts
connectedCounts
connectedCounts
usage
usage
usage
usage
usage
datafiles
datafiles
datafiles
datafiles
datafiles
hypersearch
hypersearch
hypersearch
hypersearch
hypersearch
populate
populate
populate
populate
populate
centre
centre
centre
centre
centre
track
track
track
track
track
outputLabel
outputLabel
outputLabel
outputLabel
outputLabel
topKCategories
topKCategories
topKCategories
topKCategories
topKCategories
minPermanence
minPermanence
minPermanence
minPermanence
minPermanence
windowSize
windowSize
windowSize
windowSize
windowSize
columnsShape
columnsShape
columnsShape
columnsShape
columnsShape
most
most
most
most
most
sphering
sphering
sphering
sphering
sphering
connected
connected
connected
connected
connected
otherVars
otherVars
otherVars
otherVars
otherVars
entry
entry
entry
entry
entry
phase
phase
phase
phase
phase
stats
stats
stats
stats
stats
idsToRemove
idsToRemove
idsToRemove
idsToRemove
idsToRemove
histogram
histogram
histogram
histogram
histogram
segment
segment
segment
segment
segment
msg
msg
msg
msg
msg
opfJsonSchemaFilename
opfJsonSchemaFilename
opfJsonSchemaFilename
opfJsonSchemaFilename
opfJsonSchemaFilename
dummy
dummy
dummy
dummy
dummy
y_true
y_true
y_true
y_true
y_true
checkpointSink
checkpointSink
checkpointSink
checkpointSink
checkpointSink
maxRetryDelaySec
maxRetryDelaySec
maxRetryDelaySec
maxRetryDelaySec
maxRetryDelaySec
average
average
average
average
average
partition
partition
partition
partition
partition
steady
steady
steady
steady
steady
cjDAO
cjDAO
cjDAO
cjDAO
cjDAO
callers
callers
callers
callers
callers
RecordSensorFilters
RecordSensorFilters
RecordSensorFilters
RecordSensorFilters
RecordSensorFilters
clean
clean
clean
clean
clean
deepCopy
deepCopy
deepCopy
deepCopy
deepCopy
outputFilePath
outputFilePath
outputFilePath
outputFilePath
outputFilePath
numSequences
numSequences
numSequences
numSequences
numSequences
swarming
swarming
swarming
swarming
swarming
width
width
width
width
width
dot
dot
dot
dot
dot
metricSpecs
metricSpecs
metricSpecs
metricSpecs
metricSpecs
templateFileNames
templateFileNames
templateFileNames
templateFileNames
templateFileNames
checkpointed
checkpointed
checkpointed
checkpointed
checkpointed
hyper
hyper
hyper
hyper
hyper
sequenceIdFieldName
sequenceIdFieldName
sequenceIdFieldName
sequenceIdFieldName
sequenceIdFieldName
shifter
shifter
shifter
shifter
shifter
visualizer
visualizer
visualizer
visualizer
visualizer
show
show
show
show
show
violation
violation
violation
violation
violation
text
text
text
text
text
slidingWindow
slidingWindow
slidingWindow
slidingWindow
slidingWindow
csvwriter
csvwriter
csvwriter
csvwriter
csvwriter
rep2
rep2
rep2
rep2
rep2
rep1
rep1
rep1
rep1
rep1
funcName
funcName
funcName
funcName
funcName
paramRange
paramRange
paramRange
paramRange
paramRange
bring
bring
bring
bring
bring
modelConfig
modelConfig
modelConfig
modelConfig
modelConfig
numActiveColumnsPerInhArea
numActiveColumnsPerInhArea
numActiveColumnsPerInhArea
numActiveColumnsPerInhArea
numActiveColumnsPerInhArea
threshold
threshold
threshold
threshold
threshold
filename
filename
filename
filename
filename
exc_type
exc_type
exc_type
exc_type
exc_type
backtracking
backtracking
backtracking
backtracking
backtracking
ground
ground
ground
ground
ground
slot
slot
slot
slot
slot
cell
cell
cell
cell
cell
distributionParams
distributionParams
distributionParams
distributionParams
distributionParams
slow
slow
slow
slow
slow
parameters
parameters
parameters
parameters
parameters
title
title
title
title
title
front
front
front
front
front
winner
winner
winner
winner
winner
distributed
distributed
distributed
distributed
distributed
state
state
state
state
state
sparseBinaryForm
sparseBinaryForm
sparseBinaryForm
sparseBinaryForm
sparseBinaryForm
reinit
reinit
reinit
reinit
reinit
cmdLine
cmdLine
cmdLine
cmdLine
cmdLine
only
only
only
only
only
experiment
experiment
experiment
experiment
experiment
dict
dict
dict
dict
dict
pretty
pretty
pretty
pretty
pretty
newStatus
newStatus
newStatus
newStatus
newStatus
local
local
local
local
local
l1SeqLength
l1SeqLength
l1SeqLength
l1SeqLength
l1SeqLength
columns
columns
columns
columns
columns
minPctOverlapDutyCycles
minPctOverlapDutyCycles
minPctOverlapDutyCycles
minPctOverlapDutyCycles
minPctOverlapDutyCycles
info
info
info
info
info
minNumParticles
minNumParticles
minNumParticles
minNumParticles
minNumParticles
handle
handle
handle
handle
handle
aggregator
aggregator
aggregator
aggregator
aggregator
modelParamsHash
modelParamsHash
modelParamsHash
modelParamsHash
modelParamsHash
get
get
get
get
get
rgen
rgen
rgen
rgen
rgen
closeness
closeness
closeness
closeness
closeness
de
de
de
de
de
andValues
andValues
andValues
andValues
andValues
repr
repr
repr
repr
repr
centerInput
centerInput
centerInput
centerInput
centerInput
hsVersion
hsVersion
hsVersion
hsVersion
hsVersion
next
next
next
next
next
fieldName
fieldName
fieldName
fieldName
fieldName
checkpoints
checkpoints
checkpoints
checkpoints
checkpoints
import
import
import
import
import
report
report
report
report
report
td
td
td
td
td
banner
banner
banner
banner
banner
gen
gen
gen
gen
gen
descriptionPyPath
descriptionPyPath
descriptionPyPath
descriptionPyPath
descriptionPyPath
errorCode
errorCode
errorCode
errorCode
errorCode
synPermBelowStimulusInc
synPermBelowStimulusInc
synPermBelowStimulusInc
synPermBelowStimulusInc
synPermBelowStimulusInc
metricValue
metricValue
metricValue
metricValue
metricValue
settings
settings
settings
settings
settings
fields
fields
fields
fields
fields
verbosity
verbosity
verbosity
verbosity
verbosity
progressEnd
progressEnd
progressEnd
progressEnd
progressEnd
inpt
inpt
inpt
inpt
inpt
optimized
optimized
optimized
optimized
optimized
remove
remove
remove
remove
remove
calling
calling
calling
calling
calling
otherPositions
otherPositions
otherPositions
otherPositions
otherPositions
sprintIdx
sprintIdx
sprintIdx
sprintIdx
sprintIdx
predicted
predicted
predicted
predicted
predicted
minGamma
minGamma
minGamma
minGamma
minGamma
release
release
release
release
release
x
x
x
x
x
sourceSP
sourceSP
sourceSP
sourceSP
sourceSP
cycles
cycles
cycles
cycles
cycles
valid
valid
valid
valid
valid
generations
generations
generations
generations
generations
predictive
predictive
predictive
predictive
predictive
arr
arr
arr
arr
arr
set
set
set
set
set
concurrency
concurrency
concurrency
concurrency
concurrency
seq
seq
seq
seq
seq
backtrack
backtrack
backtrack
backtrack
backtrack
float
float
float
float
float
colIdx
colIdx
colIdx
colIdx
colIdx
frame
frame
frame
frame
frame
hsObj
hsObj
hsObj
hsObj
hsObj
orphan
orphan
orphan
orphan
orphan
columnActiveSegments
columnActiveSegments
columnActiveSegments
columnActiveSegments
columnActiveSegments
module
module
module
module
module
seg
seg
seg
seg
seg
num
num
num
num
num
radius
radius
radius
radius
radius
result
result
result
result
result
optimizeKey
optimizeKey
optimizeKey
optimizeKey
optimizeKey
fail
fail
fail
fail
fail
hash
hash
hash
hash
hash
resultsPerChoice
resultsPerChoice
resultsPerChoice
resultsPerChoice
resultsPerChoice
temporalImp
temporalImp
temporalImp
temporalImp
temporalImp
averagingWindow
averagingWindow
averagingWindow
averagingWindow
averagingWindow
inputTSRecordIdx
inputTSRecordIdx
inputTSRecordIdx
inputTSRecordIdx
inputTSRecordIdx
best
best
best
best
best
modelID
modelID
modelID
modelID
modelID
columnDimensions
columnDimensions
columnDimensions
columnDimensions
columnDimensions
detect
detect
detect
detect
detect
spimp
spimp
spimp
spimp
spimp
sharedRange
sharedRange
sharedRange
sharedRange
sharedRange
outputsShape
outputsShape
outputsShape
outputsShape
outputsShape
exhaustedSwarmId
exhaustedSwarmId
exhaustedSwarmId
exhaustedSwarmId
exhaustedSwarmId
pattern
pattern
pattern
pattern
pattern
away
away
away
away
away
meta
meta
meta
meta
meta
propose
propose
propose
propose
propose
cellIdx
cellIdx
cellIdx
cellIdx
cellIdx
label
label
label
label
label
getattr
getattr
getattr
getattr
getattr
score
score
score
score
score
features
features
features
features
features
fieldParams
fieldParams
fieldParams
fieldParams
fieldParams
invariant
invariant
invariant
invariant
invariant
discardNoneKeys
discardNoneKeys
discardNoneKeys
discardNoneKeys
discardNoneKeys
previous
previous
previous
previous
previous
checks
checks
checks
checks
checks
generators
generators
generators
generators
generators
cancelling
cancelling
cancelling
cancelling
cancelling
C
C
C
C
C
profiling
profiling
profiling
profiling
profiling
recently
recently
recently
recently
recently
particleHash
particleHash
particleHash
particleHash
particleHash
do
do
do
do
do
wrapping
wrapping
wrapping
wrapping
wrapping
buckets
buckets
buckets
buckets
buckets
weak
weak
weak
weak
weak
topDownIn
topDownIn
topDownIn
topDownIn
topDownIn
unittesthelpers
unittesthelpers
unittesthelpers
unittesthelpers
unittesthelpers
modelResult
modelResult
modelResult
modelResult
modelResult
replaceReport
replaceReport
replaceReport
replaceReport
replaceReport
job
job
job
job
job
maxUpdateInterval
maxUpdateInterval
maxUpdateInterval
maxUpdateInterval
maxUpdateInterval
key
key
key
key
key
interface
interface
interface
interface
interface
filterInfo
filterInfo
filterInfo
filterInfo
filterInfo
configuration
configuration
configuration
configuration
configuration
fieldsToMatch
fieldsToMatch
fieldsToMatch
fieldsToMatch
fieldsToMatch
phase1
phase1
phase1
phase1
phase1
c
c
c
c
c
phase2
phase2
phase2
phase2
phase2
last
last
last
last
last
maxval
maxval
maxval
maxval
maxval
table2d
table2d
table2d
table2d
table2d
categoryIndices
categoryIndices
categoryIndices
categoryIndices
categoryIndices
region
region
region
region
region
gram
gram
gram
gram
gram
escape
escape
escape
escape
escape
equal
equal
equal
equal
equal
forced
forced
forced
forced
forced
minNumSyns
minNumSyns
minNumSyns
minNumSyns
minNumSyns
cache
cache
cache
cache
cache
s
s
s
s
s
item
item
item
item
item
paramName
paramName
paramName
paramName
paramName
inputFile
inputFile
inputFile
inputFile
inputFile
attributes
attributes
attributes
attributes
attributes
bookeeping
bookeeping
bookeeping
bookeeping
bookeeping
expression
expression
expression
expression
expression
col
col
col
col
col
duty
duty
duty
duty
duty
and
and
and
and
and
extent
extent
extent
extent
extent
bool
bool
bool
bool
bool
simple
simple
simple
simple
simple
cb
cb
cb
cb
cb
period
period
period
period
period
doBackground
doBackground
doBackground
doBackground
doBackground
spinit
spinit
spinit
spinit
spinit
exception
exception
exception
exception
exception
trueCatIndex
trueCatIndex
trueCatIndex
trueCatIndex
trueCatIndex
runtimeElement
runtimeElement
runtimeElement
runtimeElement
runtimeElement
learning
learning
learning
learning
learning
sequences
sequences
sequences
sequences
sequences
cancel
cancel
cancel
cancel
cancel
diff
diff
diff
diff
diff
allocated
allocated
allocated
allocated
allocated
toBeAdded
toBeAdded
toBeAdded
toBeAdded
toBeAdded
centerIndex
centerIndex
centerIndex
centerIndex
centerIndex
actValues
actValues
actValues
actValues
actValues
flattenedPermuteVars
flattenedPermuteVars
flattenedPermuteVars
flattenedPermuteVars
flattenedPermuteVars
spatialOutput
spatialOutput
spatialOutput
spatialOutput
spatialOutput
initProb
initProb
initProb
initProb
initProb
raise
raise
raise
raise
raise
console
console
console
console
console
svddims
svddims
svddims
svddims
svddims
newCategory
newCategory
newCategory
newCategory
newCategory
create
create
create
create
create
velocity
velocity
velocity
velocity
velocity
add
add
add
add
add
confidences
confidences
confidences
confidences
confidences
inertia
inertia
inertia
inertia
inertia
tplearning
tplearning
tplearning
tplearning
tplearning
parameterValue
parameterValue
parameterValue
parameterValue
parameterValue
options
options
options
options
options
maxSamples
maxSamples
maxSamples
maxSamples
maxSamples
skipRecords
skipRecords
skipRecords
skipRecords
skipRecords
basic
basic
basic
basic
basic
maxNewSynapseCount
maxNewSynapseCount
maxNewSynapseCount
maxNewSynapseCount
maxNewSynapseCount
proposal
proposal
proposal
proposal
proposal
prototype
prototype
prototype
prototype
prototype
anomaly
anomaly
anomaly
anomaly
anomaly
abstract
abstract
abstract
abstract
abstract
killed
killed
killed
killed
killed
choose
choose
choose
choose
choose
engine
engine
engine
engine
engine
field
field
field
field
field
enable
enable
enable
enable
enable
clipped
clipped
clipped
clipped
clipped
prevActiveCells
prevActiveCells
prevActiveCells
prevActiveCells
prevActiveCells
sdrfor
sdrfor
sdrfor
sdrfor
sdrfor
learningOffAt
learningOffAt
learningOffAt
learningOffAt
learningOffAt
retryExceptions
retryExceptions
retryExceptions
retryExceptions
retryExceptions
worker
worker
worker
worker
worker
search
search
search
search
search
assert
assert
assert
assert
assert
catIndex
catIndex
catIndex
catIndex
catIndex
zero
zero
zero
zero
zero
sensed
sensed
sensed
sensed
sensed
debugging
debugging
debugging
debugging
debugging
func
func
func
func
func
demand
demand
demand
demand
demand
consecutive
consecutive
consecutive
consecutive
consecutive
logger
logger
logger
logger
logger
wrapper
wrapper
wrapper
wrapper
wrapper
divisor
divisor
divisor
divisor
divisor
case
case
case
case
case
inhibit
inhibit
inhibit
inhibit
inhibit
inhibitionRadius
inhibitionRadius
inhibitionRadius
inhibitionRadius
inhibitionRadius
connectedPermanence
connectedPermanence
connectedPermanence
connectedPermanence
connectedPermanence
minThreshold
minThreshold
minThreshold
minThreshold
minThreshold
numSamples
numSamples
numSamples
numSamples
numSamples
as
as
as
as
as
value
value
value
value
value
n
n
n
n
n
cursor
cursor
cursor
cursor
cursor
dendrites
dendrites
dendrites
dendrites
dendrites
decoder
decoder
decoder
decoder
decoder
optimization
optimization
optimization
optimization
optimization
categoryColumns
categoryColumns
categoryColumns
categoryColumns
categoryColumns
error
error
error
error
error
tmshim
tmshim
tmshim
tmshim
tmshim
exist
exist
exist
exist
exist
removeList
removeList
removeList
removeList
removeList
property
property
property
property
property
mangle
mangle
mangle
mangle
mangle
adapter
adapter
adapter
adapter
adapter
loop
loop
loop
loop
loop
unlikely
unlikely
unlikely
unlikely
unlikely
bin
bin
bin
bin
bin
stop
stop
stop
stop
stop
labelName
labelName
labelName
labelName
labelName
helper
helper
helper
helper
helper
pickup
pickup
pickup
pickup
pickup
experimentDir
experimentDir
experimentDir
experimentDir
experimentDir
is
is
is
is
is
j
j
j
j
j
tests
tests
tests
tests
tests
parse
parse
parse
parse
parse
pState
pState
pState
pState
pState
terminator
terminator
terminator
terminator
terminator
in
in
in
in
in
decrement
decrement
decrement
decrement
decrement
avg
avg
avg
avg
avg
id
id
id
id
id
conn
conn
conn
conn
conn
if
if
if
if
if
null
null
null
null
null
paramsHash
paramsHash
paramsHash
paramsHash
paramsHash
whois
whois
whois
whois
whois
open
open
open
open
open
proto
proto
proto
proto
proto
cellCount
cellCount
cellCount
cellCount
cellCount
recursion
recursion
recursion
recursion
recursion
make
make
make
make
make
inputRef
inputRef
inputRef
inputRef
inputRef
header
header
header
header
header
connection
connection
connection
connection
connection
same
same
same
same
same
member
member
member
member
member
writer
writer
writer
writer
writer
reasonable
reasonable
reasonable
reasonable
reasonable
complex
complex
complex
complex
complex
auto
auto
auto
auto
auto
prevWinnerCells
prevWinnerCells
prevWinnerCells
prevWinnerCells
prevWinnerCells
fill
fill
fill
fill
fill
infer
infer
infer
infer
infer
setstate
setstate
setstate
setstate
setstate
ordinal
ordinal
ordinal
ordinal
ordinal
finish
finish
finish
finish
finish
y
y
y
y
y
timeFieldName
timeFieldName
timeFieldName
timeFieldName
timeFieldName
closest
closest
closest
closest
closest
hyperSearchJob
hyperSearchJob
hyperSearchJob
hyperSearchJob
hyperSearchJob
keys
keys
keys
keys
keys
boosted
boosted
boosted
boosted
boosted
permutation
permutation
permutation
permutation
permutation
driver
driver
driver
driver
driver
syn
syn
syn
syn
syn
metricData
metricData
metricData
metricData
metricData
maxGenIdx
maxGenIdx
maxGenIdx
maxGenIdx
maxGenIdx
v1
v1
v1
v1
v1
running
running
running
running
running
levels
levels
levels
levels
levels
moving
moving
moving
moving
moving
two
two
two
two
two
aggregate
aggregate
aggregate
aggregate
aggregate
vectorIndices
vectorIndices
vectorIndices
vectorIndices
vectorIndices
validate
validate
validate
validate
validate
globalInhibition
globalInhibition
globalInhibition
globalInhibition
globalInhibition
newValue
newValue
newValue
newValue
newValue
identity
identity
identity
identity
identity
cycle
cycle
cycle
cycle
cycle
knn
knn
knn
knn
knn
safe
safe
safe
safe
safe
raisePerm
raisePerm
raisePerm
raisePerm
raisePerm
database
database
database
database
database
i
i
i
i
i
itemName
itemName
itemName
itemName
itemName
fieldType
fieldType
fieldType
fieldType
fieldType
replace
replace
replace
replace
replace
opf
opf
opf
opf
opf
deferred
deferred
deferred
deferred
deferred
divide
divide
divide
divide
divide
numIngested
numIngested
numIngested
numIngested
numIngested
client
client
client
client
client
command
command
command
command
command
chunkCount
chunkCount
chunkCount
chunkCount
chunkCount
whichVars
whichVars
whichVars
whichVars
whichVars
position
position
position
position
position
model
model
model
model
model
htmprediction
htmprediction
htmprediction
htmprediction
htmprediction
test1
test1
test1
test1
test1
distances
distances
distances
distances
distances
tmparams
tmparams
tmparams
tmparams
tmparams
partitionIds
partitionIds
partitionIds
partitionIds
partitionIds
kill
kill
kill
kill
kill
common_models
common_models
common_models
common_models
common_models
ephemeral
ephemeral
ephemeral
ephemeral
ephemeral
activityType
activityType
activityType
activityType
activityType
replacementDict
replacementDict
replacementDict
replacementDict
replacementDict
db
db
db
db
db
sources
sources
sources
sources
sources
inactive
inactive
inactive
inactive
inactive
completionMsg
completionMsg
completionMsg
completionMsg
completionMsg
includedFields
includedFields
includedFields
includedFields
includedFields
comput
comput
comput
comput
comput
dutyCycles
dutyCycles
dutyCycles
dutyCycles
dutyCycles
speed
speed
speed
speed
speed
permute
permute
permute
permute
permute
abbreviate
abbreviate
abbreviate
abbreviate
abbreviate
tables
tables
tables
tables
tables
modelDescription
modelDescription
modelDescription
modelDescription
modelDescription
monitored
monitored
monitored
monitored
monitored
freqCounts
freqCounts
freqCounts
freqCounts
freqCounts
generator
generator
generator
generator
generator
density
density
density
density
density
dbargs
dbargs
dbargs
dbargs
dbargs
skipCheck
skipCheck
skipCheck
skipCheck
skipCheck
percent
percent
percent
percent
percent
logged
logged
logged
logged
logged
prevPredictedColumns
prevPredictedColumns
prevPredictedColumns
prevPredictedColumns
prevPredictedColumns
orphaned
orphaned
orphaned
orphaned
orphaned
requestedActivities
requestedActivities
requestedActivities
requestedActivities
requestedActivities
logLevel
logLevel
logLevel
logLevel
logLevel
enableLearn
enableLearn
enableLearn
enableLearn
enableLearn
spread
spread
spread
spread
spread
combine
combine
combine
combine
combine
samples
samples
samples
samples
samples
dtype
dtype
dtype
dtype
dtype
recordNum
recordNum
recordNum
recordNum
recordNum
input
input
input
input
input
cum
cum
cum
cum
cum
save
save
save
save
save
minResolution
minResolution
minResolution
minResolution
minResolution
match
match
match
match
match
useless
useless
useless
useless
useless
knnanomaly
knnanomaly
knnanomaly
knnanomaly
knnanomaly
sigFigs
sigFigs
sigFigs
sigFigs
sigFigs
saturation
saturation
saturation
saturation
saturation
finalize
finalize
finalize
finalize
finalize
format
format
format
format
format
read
read
read
read
read
workerID
workerID
workerID
workerID
workerID
encoderDict
encoderDict
encoderDict
encoderDict
encoderDict
historyBuffer
historyBuffer
historyBuffer
historyBuffer
historyBuffer
rng
rng
rng
rng
rng
aggregation
aggregation
aggregation
aggregation
aggregation
pos
pos
pos
pos
pos
callbacks
callbacks
callbacks
callbacks
callbacks
readOnly
readOnly
readOnly
readOnly
readOnly
inhibition
inhibition
inhibition
inhibition
inhibition
overlapDutyCycles
overlapDutyCycles
overlapDutyCycles
overlapDutyCycles
overlapDutyCycles
ephemerals
ephemerals
ephemerals
ephemerals
ephemerals
guid
guid
guid
guid
guid
cancelation
cancelation
cancelation
cancelation
cancelation
bit
bit
bit
bit
bit
unique
unique
unique
unique
unique
reap
reap
reap
reap
reap
scalars
scalars
scalars
scalars
scalars
insert
insert
insert
insert
insert
fixup
fixup
fixup
fixup
fixup
d
d
d
d
d
particle
particle
particle
particle
particle
taskControl
taskControl
taskControl
taskControl
taskControl
should
should
should
should
should
signal
signal
signal
signal
signal
sampleData
sampleData
sampleData
sampleData
sampleData
inputFilePath
inputFilePath
inputFilePath
inputFilePath
inputFilePath
synPermActiveInc
synPermActiveInc
synPermActiveInc
synPermActiveInc
synPermActiveInc
api
api
api
api
api
steps
steps
steps
steps
steps
t
t
t
t
t
sparse
sparse
sparse
sparse
sparse
output
output
output
output
output
outDir
outDir
outDir
outDir
outDir
categoriesOut
categoriesOut
categoriesOut
categoriesOut
categoriesOut
opfdummy
opfdummy
opfdummy
opfdummy
opfdummy
inputRecord
inputRecord
inputRecord
inputRecord
inputRecord
old
old
old
old
old
sequence
sequence
sequence
sequence
sequence
conditional
conditional
conditional
conditional
conditional
nL1Patterns
nL1Patterns
nL1Patterns
nL1Patterns
nL1Patterns
minValue
minValue
minValue
minValue
minValue
tpregion
tpregion
tpregion
tpregion
tpregion
clubs
clubs
clubs
clubs
clubs
printer
printer
printer
printer
printer
export
export
export
export
export
expected
expected
expected
expected
expected
flush
flush
flush
flush
flush
opfmodel
opfmodel
opfmodel
opfmodel
opfmodel
multiple
multiple
multiple
multiple
multiple
extended
extended
extended
extended
extended
collector
collector
collector
collector
collector
ntime
ntime
ntime
ntime
ntime
matching
matching
matching
matching
matching
testMethod
testMethod
testMethod
testMethod
testMethod
outp
outp
outp
outp
outp
encoderName
encoderName
encoderName
encoderName
encoderName
total
total
total
total
total
dimensions
dimensions
dimensions
dimensions
dimensions
for
for
for
for
for
bottom
bottom
bottom
bottom
bottom
numCategories
numCategories
numCategories
numCategories
numCategories
propertyName
propertyName
propertyName
propertyName
propertyName
normal
normal
normal
normal
normal
validationSets
validationSets
validationSets
validationSets
validationSets
months
months
months
months
months
per
per
per
per
per
prop
prop
prop
prop
prop
maxBuckets
maxBuckets
maxBuckets
maxBuckets
maxBuckets
maxPredictionsPerStep
maxPredictionsPerStep
maxPredictionsPerStep
maxPredictionsPerStep
maxPredictionsPerStep
machine
machine
machine
machine
machine
exit
exit
exit
exit
exit
allocate
allocate
allocate
allocate
allocate
numSVDDims
numSVDDims
numSVDDims
numSVDDims
numSVDDims
sdrclassifier
sdrclassifier
sdrclassifier
sdrclassifier
sdrclassifier
newBest
newBest
newBest
newBest
newBest
mode
mode
mode
mode
mode
core
core
core
core
core
noise
noise
noise
noise
noise
run
run
run
run
run
nCoinc
nCoinc
nCoinc
nCoinc
nCoinc
flatIdx
flatIdx
flatIdx
flatIdx
flatIdx
anomalyScores
anomalyScores
anomalyScores
anomalyScores
anomalyScores
seconds
seconds
seconds
seconds
seconds
enum
enum
enum
enum
enum
debug
debug
debug
debug
debug
step
step
step
step
step
curValue
curValue
curValue
curValue
curValue
rep
rep
rep
rep
rep
offset
offset
offset
offset
offset
resetShading
resetShading
resetShading
resetShading
resetShading
categoryToRemove
categoryToRemove
categoryToRemove
categoryToRemove
categoryToRemove
by
by
by
by
by
_
_
_
_
_
lock
lock
lock
lock
lock
on
on
on
on
on
activeArray
activeArray
activeArray
activeArray
activeArray
clientKey
clientKey
clientKey
clientKey
clientKey
obj
obj
obj
obj
obj
idx
idx
idx
idx
idx
metricNames
metricNames
metricNames
metricNames
metricNames
nodeInfo
nodeInfo
nodeInfo
nodeInfo
nodeInfo
of
of
of
of
of
boostFactors
boostFactors
boostFactors
boostFactors
boostFactors
ids
ids
ids
ids
ids
topContainer
topContainer
topContainer
topContainer
topContainer
inputVal
inputVal
inputVal
inputVal
inputVal
range
range
range
range
range
estimator
estimator
estimator
estimator
estimator
streamDefDict
streamDefDict
streamDefDict
streamDefDict
streamDefDict
maxConcurrency
maxConcurrency
maxConcurrency
maxConcurrency
maxConcurrency
genIdx
genIdx
genIdx
genIdx
genIdx
overCategories
overCategories
overCategories
overCategories
overCategories
shared
shared
shared
shared
shared
B
B
B
B
B
runtime
runtime
runtime
runtime
runtime
backup
backup
backup
backup
backup
or
or
or
or
or
gamma
gamma
gamma
gamma
gamma
maturity
maturity
maturity
maturity
maturity
predictedColumns
predictedColumns
predictedColumns
predictedColumns
predictedColumns
logArgs
logArgs
logArgs
logArgs
logArgs
varClass
varClass
varClass
varClass
varClass
into
into
into
into
into
recreate
recreate
recreate
recreate
recreate
tmEnable
tmEnable
tmEnable
tmEnable
tmEnable
dataset
dataset
dataset
dataset
dataset
down
down
down
down
down
activate
activate
activate
activate
activate
dcross
dcross
dcross
dcross
dcross
metricSpec
metricSpec
metricSpec
metricSpec
metricSpec
strip
strip
strip
strip
strip
wrap
wrap
wrap
wrap
wrap
cmdLineArgs
cmdLineArgs
cmdLineArgs
cmdLineArgs
cmdLineArgs
jobID
jobID
jobID
jobID
jobID
permanences
permanences
permanences
permanences
permanences
outputName
outputName
outputName
outputName
outputName
rows
rows
rows
rows
rows
span
span
span
span
span
log
log
log
log
log
val
val
val
val
val
area
area
area
area
area
swarmId
swarmId
swarmId
swarmId
swarmId
support
support
support
support
support
initial
initial
initial
initial
initial
available
available
available
available
available
json
json
json
json
json
long
long
long
long
long
custom
custom
custom
custom
custom
filterList
filterList
filterList
filterList
filterList
start
start
start
start
start
modelResults
modelResults
modelResults
modelResults
modelResults
rawInput
rawInput
rawInput
rawInput
rawInput
var
var
var
var
var
manager
manager
manager
manager
manager
checkpointDir
checkpointDir
checkpointDir
checkpointDir
checkpointDir
translate
translate
translate
translate
translate
absDirPath
absDirPath
absDirPath
absDirPath
absDirPath
function
function
function
function
function
maxVal
maxVal
maxVal
maxVal
maxVal
head
head
head
head
head
recompute
recompute
recompute
recompute
recompute
resultsDB
resultsDB
resultsDB
resultsDB
resultsDB
sdr
sdr
sdr
sdr
sdr
unwrap
unwrap
unwrap
unwrap
unwrap
inputFields
inputFields
inputFields
inputFields
inputFields
nupic
nupic
nupic
nupic
nupic
descriptions
descriptions
descriptions
descriptions
descriptions
encoder
encoder
encoder
encoder
encoder
packed_data
packed_data
packed_data
packed_data
packed_data
encode
encode
encode
encode
encode
inferenceElement
inferenceElement
inferenceElement
inferenceElement
inferenceElement
link
link
link
link
link
delta
delta
delta
delta
delta
inferenceType
inferenceType
inferenceType
inferenceType
inferenceType
encoded
encoded
encoded
encoded
encoded
line
line
line
line
line
numIters
numIters
numIters
numIters
numIters
with
with
with
with
with
yupe
yupe
yupe
yupe
yupe
count
count
count
count
count
minval
minval
minval
minval
minval
iterationNum
iterationNum
iterationNum
iterationNum
iterationNum
compute
compute
compute
compute
compute
secondOrder
secondOrder
secondOrder
secondOrder
secondOrder
default
default
default
default
default
numpy
numpy
numpy
numpy
numpy
cells
cells
cells
cells
cells
caller
caller
caller
caller
caller
bucket
bucket
bucket
bucket
bucket
up
up
up
up
up
getter
getter
getter
getter
getter
record
record
record
record
record
below
below
below
below
below
fdr
fdr
fdr
fdr
fdr
critical
critical
critical
critical
critical
distribution
distribution
distribution
distribution
distribution
define
define
define
define
define
synPermTrimThreshold
synPermTrimThreshold
synPermTrimThreshold
synPermTrimThreshold
synPermTrimThreshold
impl
impl
impl
impl
impl
delete
delete
delete
delete
delete
trim
trim
trim
trim
trim
patternLen
patternLen
patternLen
patternLen
patternLen
expr
expr
expr
expr
expr
adopt
adopt
adopt
adopt
adopt
showReset
showReset
showReset
showReset
showReset
shim
shim
shim
shim
shim
int
int
int
int
int
model2
model2
model2
model2
model2
model1
model1
model1
model1
model1
model0
model0
model0
model0
model0
permWorkDir
permWorkDir
permWorkDir
permWorkDir
permWorkDir
persistent
persistent
persistent
persistent
persistent
v2
v2
v2
v2
v2
single
single
single
single
single
warning
warning
warning
warning
warning
inh
inh
inh
inh
inh
predictions
predictions
predictions
predictions
predictions
file
file
file
file
file
trace
trace
trace
trace
trace
destSP
destSP
destSP
destSP
destSP
temporalOutput
temporalOutput
temporalOutput
temporalOutput
temporalOutput
jobIDs
jobIDs
jobIDs
jobIDs
jobIDs
check
check
check
check
check
utils
utils
utils
utils
utils
inc
inc
inc
inc
inc
ignoreKilled
ignoreKilled
ignoreKilled
ignoreKilled
ignoreKilled
tick
tick
tick
tick
tick
encoders
encoders
encoders
encoders
encoders
raiseException
raiseException
raiseException
raiseException
raiseException
epsilon
epsilon
epsilon
epsilon
epsilon
no
no
no
no
no
eq
eq
eq
eq
eq
learningPeriod
learningPeriod
learningPeriod
learningPeriod
learningPeriod
nd
nd
nd
nd
nd
ne
ne
ne
ne
ne
forceUpdate
forceUpdate
forceUpdate
forceUpdate
forceUpdate
dbto
dbto
dbto
dbto
dbto
other
other
other
other
other
lookup
lookup
lookup
lookup
lookup
details
details
details
details
details
spatial
spatial
spatial
spatial
spatial
test
test
test
test
test
nIters
nIters
nIters
nIters
nIters
patternNZs
patternNZs
patternNZs
patternNZs
patternNZs
categoryList
categoryList
categoryList
categoryList
categoryList
addList
addList
addList
addList
addList
inputValue
inputValue
inputValue
inputValue
inputValue
node
node
node
node
node
tmregion
tmregion
tmregion
tmregion
tmregion
resetSignal
resetSignal
resetSignal
resetSignal
resetSignal
ncols
ncols
ncols
ncols
ncols
isBlocking
isBlocking
isBlocking
isBlocking
isBlocking
models
models
models
models
models
durations
durations
durations
durations
durations
swarms
swarms
swarms
swarms
swarms
update
update
update
update
update
testNumber
testNumber
testNumber
testNumber
testNumber
inferenceDict
inferenceDict
inferenceDict
inferenceDict
inferenceDict
push
push
push
push
push
generated
generated
generated
generated
generated
retry
retry
retry
retry
retry
aggregationInfo
aggregationInfo
aggregationInfo
aggregationInfo
aggregationInfo
sql
sql
sql
sql
sql
variable
variable
variable
variable
variable
resets
resets
resets
resets
resets
param
param
param
param
param
network
network
network
network
network
svm
svm
svm
svm
svm
e
e
e
e
e
algorithm
algorithm
algorithm
algorithm
algorithm
svd
svd
svd
svd
svd
mask
mask
mask
mask
mask
flag
flag
flag
flag
flag
jobType
jobType
jobType
jobType
jobType
strings
strings
strings
strings
strings
patternActivity
patternActivity
patternActivity
patternActivity
patternActivity
potentialPct
potentialPct
potentialPct
potentialPct
potentialPct
status
status
status
status
status
y_score
y_score
y_score
y_score
y_score
time
time
time
time
time
directory
directory
directory
directory
directory
mean
mean
mean
mean
mean
original
original
original
original
original
numSVDSamples
numSVDSamples
numSVDSamples
numSVDSamples
numSVDSamples
a record of the past data the algorithm has seen with an entry for each iteration
class implementing the temporal memory algorithm as described in the published cortical learning algorithm documentation
a structure that contains the input to a model and the resulting predictions as well as any related information related to the predictions
database connection policy base class/interface
the period of the duty cycle
record number of this input pattern. record numbers
wrapper that translates the data type and access code to a string the original values are an enumerated type in c++ that become
this class enumerates the types of search we can perform
utility class to help with the selection of the 'best' model during hypersearch for a particular job
a base class that must be subclassed by users in order to define the testregion instance's specialization
datetime representing the time being encoded
this class is responsible for running a single experiment task on the
the input data in the format it is received from the data
category label to remove
input values to encode to string
this exception may be raised by our error result reporting code when
manages iteration cycle phase drivers
pass an encoded sdr straight to the model
option parser with predefined test options
base sequence machine class
classifier-like object that diffs the output from different classifiers
dict of the classification information
bool
class that records the performane of swarms in a sprint and makes decisions about which swarms should stop running
each entry contains strings for example sequence labels
interface for iterationphasexxxxx classes
a single record to store data associated with a single prediction for the anomaly classifier
a list of row indices to remove
this class represents the learn-and-infer phase of the iteration cycle in the taskcontrol block of description
the sdr classifier accepts a binary input pattern from the level below (the "activationpattern") and information from the sensor and
this class implements logging of predictions to files as actual vs predicted values
encodes metric data input rows for consumption by opf models see
optional input of scalar names to convert. if none gets
many functions in spatialpooler operate on a columnindex but use an underlying storage implementation based on a sparse matrix in which cortical
raised when supplied ranges to a method are invalid
a multiencoder encodes a dictionary or object with multiple components a
array of actual values typically obtained from
numpy array of metric data. used to calculate minval
@private
public values for the field data types valid types are
an array of permanence values for a column. the array is
this exception may be raised in response to invalid or incompatible function arguments
to print
the encoded output that you want decode
the v2 hypersearch implementation this is one example of a hypersearch
a source of programmatically-generated data
this recordsensor filter adds noise to the input
@private
this class defines the interface for prediction writer implementation
if true the bitoffset is w.r.t. formatted output
index of the training pattern
this is the base class that all opf model implementations should subclass
this class implements the "learn-only" phase of the iteration cycle
a :class ~.nupic.frameworks.opf.opfutils.modelresult
how far to seek from end of file
region for computing the anomaly score
implements a stream reader this is a high level class that owns one or more
define a permutation variable which can take on floating point values
represent the topology of an n-dimensional region basically it is a list of integers such as [4 8 6]
this exception may be raised in response to invalid or incompatible command arguments/values
mixin for temporalmemory that stores a detailed history for inspection and debugging
factory for instantiating cla classifiers
pattern machine class that generates patterns with non-overlapping consecutive on bits
svmclassifiernode implements support vector machines svms which can be used to perform supervised learning by mapping a set of top-level groups beliefs onto
a boolean value indicating that boundaries should be
data to encode. this should be validated by the encoder
sequence of values corresponding to a single input metric
helper class for computing the average percent change for a best-fit exponential function given a set of x y points
private class to handle creation deletion and editing of the custom configuration file used by this implementation of configuration
the name of the encoder which is our parent. this
enum one of the following
helper class for computing windowed moving
basic class providing common implementation for
computes root-mean-square error
if a model raises this exception then the runmodelxxx code will mark the job as canceled so that all other workers exit immediately and mark
sparsematrix variant of _sparsematrixcorticalcolumnadapter use in cases
@private
integer controlling extent of printouts for debugging
this is not a metric but rather a facility for passing the predictions generated by a baseline metric through to the prediction output cache produced
knnanomalyclassifierregion wraps the knnclassifierregion to classify htm_prediction_model state
base class for all expgenerator-specific exceptions
this class implements the "infer-only" phase of the iteration cycle
a scalar encoder encodes a numeric floating point value into an array of bits
static factory class that produces a model based on a description dict
@private
base pattern machine class
a metrics module compares a prediction y to corresponding ground truth x and returns a single measure representing the "goodness" of the prediction
this class wraps the :class scalarencoder
list of new category indices. for example mapping=[2 0 1]
task phase driver implementation conceptually the client injects input records one at a time into
wrap an nupic :collection with a dict-like interface the optional valuewrapper is used to wrap values for adaptation purposes
this class is used by region _getparametermethods
each entry contains counts (for example # of predicted => active cells)
no effect
the fraction of columns to survive inhibition
printed before the header if specified
no effect
this connection policy establishes/breaks a new connection for every high-level transaction (i
this class implements nupic's k nearest neighbor classifier knn is very
(:class .encoder) the encoder to add
each entry contains metrics (for example metric for # of predicted => active cells)
this exception is raised when a worker tries to update a model record that belongs to another worker
a numpy array specifying the potential pool of the column
@private
this is an implementation of a delta encoder the delta encoder encodes
given a data file in standard numenta format and the types of weather you are interested in
this connection policy maintains a pool of connections that are doled out as needed for each transaction
this class implements the spatial pooler it is in charge of handling the
the indices of the columns whose permanences need to be
given a coordinate in an n-dimensional space and a radius around that coordinate the coordinate encoder returns an sdr representation
temporal memory => tm shim class
encodes a list of discrete categories described by strings that aren't related to each other
integer specifying number of records to skip when
class to hold data representing the connectivity of a collection of cells
a numpy array of 0's and 1's that comprises the input to
network visualization framework entry point
tm => monitored temporal memory shim class
an array whose size is equal to the number of columns
this is the base interface class for description api classes which provide opf configuration parameters
each entry contains indices (for example of predicted => active cells)
optional value of current input to encoders
the hypersearchworker is responsible for evaluating one or more models within a specific hypersearch job
@private
this class extends the configurationbase implementation with the ability to read and write custom persistent parameters
generates a sinewave of a given period amplitude and phase shift
integer number of records to average over
list of the active indices from the output below
minimum value of metric. used to set up encoders. if none
array of records as in
helper class to compute the slope of a best-fit line given a set of x y points
this class can be used to fetch nupic configuration settings which are stored in one or more xml files
tm => temporal memory shim class
array of columns indices predicted in prev step
pattern to check overlap of
extends the log message by appending custom parameters
partial implementation of metrics interface for metrics that accumulate an error and compute an aggregate score potentially
this class serves as a base or mixin for classes that want to enforce the locked attributes pattern (all attributes should be defined in __init__()
this class serves as an adapter for a client-instantiated non-temporal log writer
this connection policy maintains a single shared database connection
dictionary of dictionaries with the first level being
@private
@doc place_holder region description
computes normalized root-mean-square error
this is the interface class for a dataset readers
class containing cell information internal to the connections
serializable base class establishing read() and write() abstract methods readfromfile() and writetofile() concrete methods to support
this class represents the learn-only phase of the iteration cycle in the taskcontrol block of description
a pluggableencodersensor holds a value and encodes it into network output
todo move to shared script so that we can share it with run_opf_experiment
offset of the bit to get the description of
bool if true learn this sample
base class for "value getters" (e g class dictvaluegetter) that are used
given a gps coordinate and a speed reading the geospatial coordinate encoder returns an sdr representation
the base class of all permutexxx classes that can be used from within a permutation script
dict of the classification information where
a list of python objects that will be string-ified
represents the mapping of a given inputrecord by the sensor region's encoder
must contain name, type, and special
computes negative log-likelihood likelihood is the predicted probability of
class to store an activationpattern bit history
this recordsensor filter adds noise to the input
this class encapsulates the hypersearch state which we share with all other workers
computes error metric based on one-grams the groundtruth passed into
a new numerical value used to update the duty cycle
base class for monitormixin each subclass will be a mixin for a particular
this class captures standard output and writes it to a file
initial implementation of auto-reset is fairly simple you just give it a
the current anomaly score
@doc place_holder network description
numpy 1-d array of same length returned by
network visualization "renderer" implementation to render a network with graphviz
a value between 0 or 1 governing the chance for each
network visualization "renderer" implementation to render a network to a dot-formatted document suitable for use w/ graphviz
int size of sliding window of historical
class implementing the temporal memory algorithm as described in the published cortical learning algorithm documentation
data type used as return value type by htmpredictionmodel
raised on error creating the experiment directory
the index identifying a column in the permanence potential
defines the exception thrown when the input stream times out receiving new records
object capnp proto object specified in
array of columns indices predicted in this step
creates a "future" reference to a value within a top-level or a nested dictionary
validictory schemavalidator subclass to not accept nan values as numbers
string name of encoder should be unique
class containing minimal information to identify a unique synapse
int the number of reasonable anomaly scores
custom error metric class that handles user defined error metrics
spregion is designed to implement the spatial pooler compute for a given htm level
simple opf client
knnclassifierregion implements the k nearest neighbor classification algorithm
a cla classifier accepts a binary input from the level below (the "activationpattern") and information from the sensor and encoders (the
a scalar encoder encodes a numeric floating point value into an array of bits
int number of egeinvectors used for projection
clalearningperiod deprecated - int the number of
computes average absolute error
two-gram benchmark model
holds frequencies in a 2d grid of bins
a record sensor rs retrieves an information "record" and encodes it to be suitable as input to an htm
@private
this class represents a single metrics specification in the taskcontrol
a metric computed over a set of data (usually from a countstrace)
string candidate field data type
[optional] timestamp of the ocurrence
claclassifierregion implements a cla specific classifier that accepts a binary input from the level below (the "activationpattern") and information from the
index of partition
class implementing the temporal memory algorithm
an array containing one or more duty cycle values that need
this is an "uber" metric which is used to apply one of the other basic metrics to a specific step in a multi-step prediction
public values for the "special" field attribute valid values are
a sequence of field attribute tuples conforming to the format
int the category to be associated to the training
this class serves as an adapter for a client-instantiated temporal log writer
this is a class to handle the computation of metrics properly this class
this class holds all the information we have accumulated on completed models which particles were used etc
string candidate value
list a pattern to be classified
bool whether to apply svd to the input patterns
single index or list of indices. can also be a
saves sp initialization args
multi metric can combine multiple other sub metrics and weight them to provide combined score
if not none get the first pattern belonging to category cat. if
database connection factory
simply the inverse of the accuracy metric
the required category of closest neighbor
encodes a list of discrete categories described by strings that aren't related to each other so we never emit a mixture of categories
construct a particle each particle evaluates one or more models
a simple fifo stack add data when it's available and
each entry contains bools for example resets
region for computing the anomaly likelihoods
raised when a supplied value to a method is invalid
this is an "uber" metric which is used to apply one of the other basic metrics to a specific step in a multi-step prediction
define a permutation variable which can take on discrete choices
network visualization "renderer" implementation to render a network with graphviz
this is the interface for output of prediction metrics
string specifying type of temporal memory implementation
define a permutation variable which can take on integer values
computes error metric based on moving mode prediction
saves backtrackingtmcpp initialization args
computes simple accuracy for an enumerated type all inputs are treated as
int if 0 the input pattern is a dense representation. if
csv file based recordstream implementation each field is a 3-tuple (name, type, special or
the fraction of columns to survive inhibition. this
the encoded output. typically received from the topdown
int the number of samples to use for the svd
generates random categories
list of the active indices from the output below. when the
pattern to check distance with
this class can be used to fetch nupic configuration settings which are stored in one or more xml files
the pattern whose closest neighbor is sought
one of the special field attribute values from
the current metric ("raw") input value eg. "orange", or
single index or list of indices
creates a "future" reference to a value within an implicit dictionary that will be passed to applyvaluegetterstocontainer() in the future (typically
computes the "alternative" mean absolute percent error
numpy array of the weight matrix
utility class for generating anomaly scores in different ways
@private
this is a csv file-based implementation of datasetreaderiface
overrides for default temporal memory parameters
generates a gaussian distribution
int unknown
an encoder that can be used to permute the encodings through different spaces these include absolute value delta log space etc
int partitionid allows you to associate an id with each
computes the "classic" mean absolute percent error
if a model raises this exception then the runmodelxxx code will mark the job as canceled so that all other workers exit immediately and mark
the json dict returned by estimateanomalylikelihoods
this class defines the basic file-based implementation of
an encoder converts a value to a sparse distributed representation
this class acts as a container of meta-data for a single field column of a dataset
an array containing the overlap score for each column
enum class for actions that can be performed
reference to the corresponding input (not applicable
sparsebinarymatrix variant of _sparsematrixcorticalcolumnadapter use in
computes error metric based on moving mean prediction
@private
the name of the encoder which is our parent. this name
a list of records. each record is a list with the
here we wrap the various unittest testcase assert methods (that
this is essentially an static class that handles the logic for terminating bad models
this is the interface for the record input/output storage classes
this is the file-based implementation of the interface for output of
this class runs a 'dummy' opf experiment it will periodically update the
@private
function callback to report progress
cap'n proto obj
this class implements a record classifier used to classify prediction records
this class runs an a given model
a date encoder encodes a date according to encoding parameters specified in its constructor
class containing minimal information to identify a unique segment
this class provides context and methods for aggregating records the caller
list the pattern to be assigned a category. if
int how often we re-estimate the gaussian
helper class for computing moving average and sliding window
dictionary that allows attribute-like access to its elements
sdrclassifierregion implements a sdr classifier that accepts a binary input from the level below (the "activationpattern") and information from the
array of active column indices
the requested operation timed out
mixin class for printing to the console with different verbosity levels
a boolean value indicating whether the permanence values
array of expected scalar values typically obtained from
this class implements the "learn-and-infer" phase of the iteration cycle
name of the field this encoder is encoding provided by
bool if true perform inference
int if provided all training vectors with partitionid
helper class for running anomaly likelihood computation to use it simply
minimum resolution of metric. used to set up
if true returns a list of the indices of the
testregion is designed for testing and exploration of cla network mechanisms
a permutation variable that defines a field encoder this serves as
shifts time for :class ~ nupic frameworks opf opfutils modelresult objects
dict with 'mean' and 'stdev' of the distribution
a numpy array of 0's and 1's that comprises the input
previous value model
tmregion is designed to implement the temporal memory compute for a given cla level
input data to be encoded
convert a bitmap encoded as array indices to an sdr
this class defines the interface for opf prediction logger implementations
a boolean value indicating whether learning should be
base exception class for cla model exceptions
this exception is raised when g_max_concurrency is exceeded
name of the field
node attributes to apply to all nodes in graph
an instance of this class is returned by acquireconnection() methods of our database connection policy classes
this class can be used to retrieve available and calculated features for a given combination of running environment and user
the segment class is a container for all of the segment variables and the synapses it owns
this class represents the infer-only phase of the iteration cycle in the taskcontrol block of description
computes a metric against the ground truth n steps ago the metric to
the user passed an invalid value for a spatialpooler parameter
computes -1 * auc area under the curve of the roc receiver operator characteristics curve
optional date timestamp when the sample occured
this is an implementation of the scalar encoder that adapts the min and max of the scalar encoder dynamically
the datagenerator provides a framework for generating encoding saving and exporting records
this exception signals that the nupic job (e g hypersearch production
the data from the source. this is typically an object with
bool whether learning should be enabled
the list of bucket indices one for each sub-field encoder
sequence of nupic.data.fieldmeta.fieldmetainfo objects
the index identifying a column in the permanence
this metaclass makes objects attribute-locked by decorating their __init__() and __setstate__() methods with the _allow_new_attributes
this data access object dao is used for creating managing and updating the clientjobs database
maximum value of metric. used to set up input encoders. if
factory for instantiating sdr classifiers
